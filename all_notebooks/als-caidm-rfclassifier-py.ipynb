{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data vizualization\nimport seaborn as sns # data visualization\nfrom sklearn.model_selection import train_test_split # machine learning\nfrom sklearn.ensemble import RandomForestClassifier # machine learning\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current sessio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Import Data\nexample_clinical_data_path_1 = '/kaggle/input/end-als/end-als/clinical-data/filtered-metadata/metadata/clinical/Demographics.csv'\nexample_clinical_data_path_2 = '/kaggle/input/end-als/end-als/clinical-data/filtered-metadata/metadata/clinical/ALSFRS_R.csv'\nexample_transcriptomics_DESEQ2_data_path_1 = '/kaggle/input/end-als/end-als/transcriptomics-data/DESeq2/bulbar_vs_limb.csv'\nexample_transcriptomics_DESEQ2_data_path_2 = '/kaggle/input/end-als/end-als/transcriptomics-data/DESeq2/ctrl_vs_case.csv'\nexample_transcriptomics_3counts_data_path = '/kaggle/input/end-als/end-als/transcriptomics-data/L3_counts/CASE-NEUZX521TKK/CASE-NEUZX521TKK-5793-T/CASE-NEUZX521TKK-5793-T_P85.exon.txt'\n\ndemographics = pd.read_csv(example_clinical_data_path_1)\ndemographics.to_csv('/kaggle/working/demographics.csv')\nalsfrs_scores = pd.read_csv(example_clinical_data_path_2)\nalsfrs_scores.to_csv('/kaggle/working/alsfrs_scores.csv')\nbulbar_vs_limb = pd.read_csv(example_transcriptomics_DESEQ2_data_path_1)\nbulbar_vs_limb.to_csv('/kaggle/working/bulbar_vs_limb.csv')\nctrl_vs_case = pd.read_csv(example_transcriptomics_DESEQ2_data_path_2)\nctrl_vs_case.to_csv('/kaggle/working/ctrl_vs_case.csv')\nexample_transcriptomics_3counts_data = pd.read_csv(example_transcriptomics_3counts_data_path,delim_whitespace=True,skiprows=1,low_memory=False)\nexample_transcriptomics_3counts_data.to_csv('/kaggle/working/L3_counts.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define Functions\n#Code created by Aviv Benchorin\n\ndef sort_feature_importances(df, visualize = False):\n    '''\n    Adapted from https://github.com/WillKoehrsen/feature-selector\n    '''\n    #Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    #Normalise the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n    #Make a horizontal bar chart of feature importances\n    \n    if(visualize):\n        plt.figure(figsize = (10,6))\n        ax = plt.subplot()\n        #Need to reverse the index to plot most important on top\n        ax.barh(list(reversed(list(df.index[:15]))),\n               df['importance_normalized'].head(15),\n               align = 'center', edgecolor = 'k')\n        #Set the yticks and labels\n        ax.set_yticks(list(reversed(list(df.index[:15]))))\n        ax.set_yticklabels(df['feature'].head(15))\n        #Plot labeling\n        plt.xlabel('Normalized Importance'); plt.title('Feature Importance')\n        plt.show()\n    \n    return df\n\ndef important_clusters(XClusterLabels, Y, numClusters, threshold = 0.8, labelOfInterest = 1):\n    \"\"\" Check which clusters express a given class label in a ratio greater than a threshold\n    \n    Arguments:\n        XClusterLabels: ndarray of shape (n_samples,) cluster predictions for the training data\n        Y: ndarray of shape (n_samples,), training labels\n        numCluster: an integer representing the number of clusters\n        threshold: a float representing the ratio threshold for a cluster to be significant, defaults to 0.8\n        labelOfInterest: an integer representing the class label of interest\n    \n    Returns:\n        An ndarray containing 0 (not exceeding the threshold) or 1 (exceeding the threshold) for each cluster,\n        and an ndarray containing the ratio for each cluster\n    \"\"\"\n    meaningfulList = np.zeros((numClusters))\n    ratioList = np.zeros((numClusters))\n    \n    for i in np.arange(numClusters):\n        YClusterLabels = Y[XClusterLabels == i]\n        ratio = YClusterLabels[YClusterLabels == labelOfInterest].shape[0] / (YClusterLabels.shape[0])\n        if ratio >= threshold:\n            meaningfulList[i] = 1\n        ratioList[i] = ratio\n    return meaningfulList, ratioList\n\ndef extract_important_features(X, XClusterLabels, clusterOfInterest, numFeatures=5000, visualize=False):\n    \"\"\" Find which features are important in a random forest classifier with two classes: \n    being in the cluster of interest, and not being in it.\n\n    Arguments:\n        X: Pandas DataFrame containing the training data\n        XClusterLabels: ndarray of shape (n_samples,) cluster predictions for the training data\n        clusterOfInterest: an integer representing the cluster of interest\n        numFeatures: an integer representing the number of important features to return, defaults to 5000\n        visualize: a boolean representing whether to visualize the important features, defaults to False\n\n    Returns:\n        A Pandas DataFrame containing the top numFeatures most important features\n    \"\"\"\n    \n    clf = RandomForestClassifier()\n    newClusterLabels = np.zeros(XClusterLabels.shape)\n    newClusterLabels[XClusterLabels == clusterOfInterest] = 1\n    clf.fit(X, newClusterLabels)\n\n    feature_importance_values = clf.feature_importances_\n    features = list(X.columns)\n    feature_importances = pd.DataFrame({'feature': features, 'importance':feature_importance_values})\n    return sort_feature_importances(feature_importances, visualize)[:numFeatures]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PCA\n\n#Data Processing\npca_input = bulbar_vs_limb.copy()\nground_truth = pd.DataFrame(pca_input['SiteOnset_Class'])\npca_input = pca_input.drop(['SiteOnset_Class'], axis = 1)\npca_input = pca_input.set_index(\"Participant_ID\")\n\n#Model Creation and Fitting\npca = PCA(n_components = 64)\npca_output = pca.fit_transform(pca_input)\npca_df = pd.DataFrame(data= pca_output)\n\n#Post PCA Data Processing\nrow_names = list(pca_input.index)\npca_df.index = row_names\nground_truth.index = row_names","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#K-Means Clustering\n\n# Necessary Data Frames: Reduced Feauture Data Frame(pca_df), Ground Truth Data Frame (ground_truth), Original Feature Data Frame (pca_input)\n\n# reduced_df = Neural Network Output (Row Names = Patient ID's)\n# feuture_input = Neural Network Input (Row Names = Patient ID's)\n# row_names = list(Patient IDs)\n# ground_truth = ALS vs No ALS (Row Names = Patient ID's), format as One Column Data Frame\n\nreduced_df = pca_df\nfeauture_input = pca_input\n\ncluster_number = 2\nkmeans = KMeans(n_clusters= cluster_number).fit_predict(reduced_df)\nkmeans_df = pd.DataFrame(data = kmeans)\nkmeans_df.index = row_names\nkmeans_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check Cluster Importance\n\nground_truth_np = ground_truth.to_numpy()\nkmeans_numpy = kmeans_df.to_numpy()\nmeaning_list, ratio_list = important_clusters(kmeans_numpy, ground_truth_np, cluster_number, threshold = 0.8)\nprint(ratio_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Random Forest Classifier\n\n#Data Preprocessing\nforest_input = pd.concat([kmeans_df, feauture_input], axis = 1)\nX = forest_input[forest_input.columns[2:]]\ny = forest_input[forest_input.columns[0]]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n#Random Forest Model Creation and Fitting\nclf=RandomForestClassifier(n_estimators=100)\nclf.fit(X_train,y_train)\n\n#Check Model Accuracy\ny_pred=clf.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feature Importance and Extraction\n\nfor i in np.arange(4):\n    if meaning_list[i] == 1:\n        test = extract_important_features(X, kmeans_numpy, i, visualize=True)\n        print(test[:10])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}