{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"267fe3fd8c41f48877be4220796d584d6ec4451c"},"cell_type":"markdown","source":"**Input the datasets and see the shape etc**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a1d3505d95d67e9d0bb7b2ec774171c6454e4c5"},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb3c81e5a9133631c1ec4b9f8e0f55dab3f814ec","collapsed":true},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c6dfb12cf02b3e65c4bc7179acb43a204ba59b5"},"cell_type":"markdown","source":"Sales Price is the column that we need to predict"},{"metadata":{"trusted":true,"_uuid":"2830cd003692877cc9f60669a804019d9338cdc6"},"cell_type":"code","source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport seaborn\nprint(train.iloc[:,-1].describe())\nseaborn.distplot(train['SalePrice'])\ntrain.iloc[:,-1].isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51f32b6c40df559454f1ba4148656cbaad051414"},"cell_type":"code","source":"#qqplot\nfrom scipy import stats\nimport pylab\nstats.probplot(train.iloc[:,-1],dist=\"norm\",plot=pylab)\npylab.show","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a8e2892f6e4d9ca0fa3b3e73652a1f7f3a63037"},"cell_type":"markdown","source":"the above seaborn.distpot i found online :). Anyways, it can be seen that the saleprice is slightly right scewed but that was expected. most of the people take lower priced accomodations and there is a limit at price 0 too. They number of extremely high prices is quite low. They probably are the houses that rich people bought and arent exactly outliers. And there are no missing values atleast not in this column that needs to be predicted."},{"metadata":{"trusted":true,"_uuid":"8ff73adc7f5d230bd26c518f588a3386ac29e49e","collapsed":true},"cell_type":"code","source":"train.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9210829b577207bfdcbfeae6dbba10c78e150f55"},"cell_type":"markdown","source":"Finding and imputing all the columns with missing values"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"2b52b1d2c919f3b027eda58c8282b153e111f599","collapsed":true},"cell_type":"code","source":"#Finding all columns with null values\ntrain.columns[train.isnull().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0ff8c1f59e321de1e8adefa233a624dbd5494ed9"},"cell_type":"code","source":"#Function for Finding and replacing mmissing values with median\ndef num_miss(df_in, col_name):\n    m = df_in[col_name].describe()['50%']\n    df_in.loc[(df_in[col_name].isnull()),col_name] = m\n    return df_in\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7628fa7d1b133a8363d7ca6726ef32f52c9ec6a"},"cell_type":"code","source":"num_miss(train, 'LotFrontage')\nnum_miss(train, 'MasVnrArea')\nnum_miss(train, 'GarageYrBlt')\ntrain.columns[train.isnull().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bd2cb8b0335c4270613b24bdcc4f17bd5800d49","collapsed":true},"cell_type":"code","source":"#Function for Finding and replacing mmissing values with most appearing value\ndef str_miss(df_in, col_name):\n    m = df_in[col_name].describe()['top']\n    df_in.loc[(df_in[col_name].isnull()),col_name] = m\n    return df_in\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2226138b5c0a7c6e6327df9ca4e83c4ef3acd0e"},"cell_type":"code","source":"str_miss(train,'Alley')\nstr_miss(train,'MasVnrType')\nstr_miss(train,'BsmtQual')\nstr_miss(train,'BsmtCond')\nstr_miss(train,'BsmtExposure')\nstr_miss(train,'BsmtFinType1')\nstr_miss(train,'BsmtFinType2')\nstr_miss(train,'Electrical')\nstr_miss(train,'FireplaceQu')\nstr_miss(train,'GarageType')\nstr_miss(train,'GarageFinish')\nstr_miss(train,'GarageQual')\nstr_miss(train,'GarageCond')\nstr_miss(train,'PoolQC')\nstr_miss(train,'Fence')\nstr_miss(train,'MiscFeature')\n\ntrain.columns[train.isnull().any()].tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1072b54d803fee3c950985e4ae9d72fcf8407ec"},"cell_type":"markdown","source":"**At this point all the missing values have been imputed, however outliers still remain which we can deal with by creating another function.**"},{"metadata":{"trusted":true,"_uuid":"9eb746d314c6997e4b2ca1a78195bcb6d8a38c1b","collapsed":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4467328e413bfcd400c8b7cde8308f33085ecd3c"},"cell_type":"code","source":"#Function for Finding and replacing outliers with mean\ndef change_outlier(df_in, col_name):\n    q1 = df_in[col_name].describe()['25%']\n    q3 = df_in[col_name].describe()['75%']\n    m = df_in[col_name].describe()['mean']\n    iqr = q3-q1 #Interquartile range\n    fence_low  = q1-(1.5*iqr)\n    fence_high = q3+(1.5*iqr)\n    df_in.loc[(df_in[col_name] <= fence_low) | (df_in[col_name] >= fence_high),col_name] = m\n    return df_in","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8132597037460eb82ed06def324ed8eb457a720"},"cell_type":"markdown","source":"the only columns that outliers need to be changed are as follows-\nMasVnrArea, garageArea\n*only these values included as garage area even for bigger houses is nearly the same however other areas are not outliers but represent larger houses and their corresponding sales values are large too*"},{"metadata":{"trusted":true,"_uuid":"a41cc8f10ec9423b1eb95aacf4e13393b17c0b1e"},"cell_type":"code","source":"change_outlier(train,'GarageArea')\nchange_outlier(train,'MasVnrArea')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"993758cfea5cbd652ba3af98fd752aecfb4f4f8b"},"cell_type":"markdown","source":"**All the outliers and missing values have been taken care of and will now plot grpahs to find inferences but before that i'll transform the salesPrice with its log to make it more normal and reduce the skew**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7ba9c54a68c95fb29ac61ae25a9d07a9afcd7751"},"cell_type":"code","source":"#log transform the target \ntrain[\"SalePriceLog\"] = np.log1p(train[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fea26c5b67c4ebf474b6883cc7d6c8ea925605f3"},"cell_type":"code","source":"print(train.iloc[:,-1].describe())\nseaborn.distplot(train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d310b4be48067d564722a30acb76328016ae6a11","collapsed":true},"cell_type":"code","source":"#qqplot\nstats.probplot(train.iloc[:,-1],dist=\"norm\",plot=pylab)\npylab.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e03b09f02085420dfe1908ad10539f0577d426b4"},"cell_type":"markdown","source":"** seeing the qq plot we can clearly see that the SalesPriceLog is actually more normal than SalesPriceLog**"},{"metadata":{"trusted":true,"_uuid":"2bb0be5dd8adbb11ae3efdf914db6489c7ad5520","collapsed":true},"cell_type":"code","source":"#something else that i found online. It reports the skewness.\nprint(\"skewness of price: %f\" % train['SalePrice'].skew())\nprint(\"skewness of log of price: %f\" % train['SalePriceLog'].skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24f8d9915830cb7fb56bc8043e3129e453b0c452","collapsed":true},"cell_type":"code","source":"#this gives a good idea on which numerical parameters does the salesprice depend on\ntrain.corr().iloc[-1,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a35da21012aa7395b884ae2307fa2ca304e061d","collapsed":true},"cell_type":"code","source":"numer = train._get_numeric_data()\nnumername = numer.columns.values.tolist()\nfor name in numername:\n    plt.scatter(train.SalePrice,train[name])\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"98665466383e217b48065421a74cc3e9fbad9a52"},"cell_type":"markdown","source":"From these plots and correlation table one can conclude that the factors that affect Price most are OverallQual, GrLivArea, Basement area and 1st floor area. i.e the numerical quantities representing area and the overall quality"},{"metadata":{"trusted":true,"_uuid":"94d5a70f065d367ab3f144bd62d4a4c218cf9701","collapsed":true},"cell_type":"code","source":"allname = train.select_dtypes(include='object').columns.values.tolist()\nfor name in allname:\n    train.boxplot(column = 'SalePrice', by = name)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b5474364b330062b72d6b3a833c95ef7a4d6049f"},"cell_type":"markdown","source":"it again seems that condition parameters and central ac are more important than the others"},{"metadata":{"trusted":true,"_uuid":"b917becb214fc325ad40f292c0e398a92d257c57","collapsed":true},"cell_type":"markdown","source":"**Starting with the model building now**"},{"metadata":{"trusted":true,"_uuid":"f5643f78b579ac5440718a2714328ab3da804fbf"},"cell_type":"code","source":"targetLog = train.iloc[:,-1]\ntarget = train.iloc[:,-2]\ndel train['SalePrice']\ndel train['SalePriceLog']\ndel train['Id']\ntrain.shape\n#now  train has all the predictors and target and targetLog have the fianl values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63bcdd8726e954c41441dd580f156009b6fef444","scrolled":true,"collapsed":true},"cell_type":"code","source":"#splitting train into train and test to see how well model works\nfrom sklearn import datasets, linear_model\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.2)\n\n#First Baseline Model with SalePrice as Predictor\npredicted = y_train.mean()\nsize = y_test.size\nsum =0;\nfor i in range(size):\n    sum = sum + ((y_test.iloc[i] - predicted)*(y_test.iloc[i] - predicted))\nmse = sum/size\nrmse = mse**0.5\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d6af09d986aea2ba3f6bcf6f9d6547512b3caa1","collapsed":true},"cell_type":"code","source":"#splitting train into train and test to see how well model works\nX_train, X_test, y_train, y_test = train_test_split(train, targetLog, test_size=0.2)\n\n#Second Baseline Model with SalePriceLog as Predictor\nmean = y_train.mean()\npredicted = np.expm1(mean)\nsize = y_test.size\nsum =0;\nfor i in range(size):\n    sum = sum + ((y_test.iloc[i] - predicted)*(y_test.iloc[i] - predicted))\nmse = sum/size\nrmse = mse**0.5\nrmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1732e62cbda7f4f70be47f762899a9671133fa34"},"cell_type":"markdown","source":"Two baseline models were created using SalePrice and SalePriceLog and it seems that SalePrice is doing better with lower root mean squared error!! I personally thought log would do better but in this case it didnt!!"},{"metadata":{"_uuid":"cc88f3a3fedb6d480f81e290d29e00d2d3fb9760"},"cell_type":"markdown","source":"**Moving to Linear Regression**"},{"metadata":{"trusted":true,"_uuid":"e7e6f7357e1275055fe57d284f4048b1551855d9"},"cell_type":"code","source":"#Dummy coding for categorical Variables\ntrain = pd.get_dummies(train)\nprint(train.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.2)\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef313aeaf03c15ceedc4cdd2d5e1507cddf2a1f4","collapsed":true},"cell_type":"code","source":"#Making the first model\nfrom sklearn.linear_model import LinearRegression\nmodel1=LinearRegression()\nmodel1.fit(X_train,y_train)\nprint(\"Mean squared error in Test:\",np.mean((model1.predict(X_test) - y_test) ** 2))\nprint('R² of Test:',model1.score(X_test, y_test))\nprint(\"Mean squared error in train:\",np.mean((model1.predict(X_train) - y_train) ** 2))\nprint('R² of train:',model1.score(X_train, y_train))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"301508e7ec2758482ce8c1cca6f69ed574603bba"},"cell_type":"markdown","source":"This seems a problem of overfitting as test model has 94% R2 but the test counterpart has has very low R2. Anyways, let us make the secong model and then we will look at the assumptions followed by better models"},{"metadata":{"trusted":true,"_uuid":"3fb7bb97b82d8d8b05cc4503eaac867fb1ba52a6","scrolled":true,"collapsed":true},"cell_type":"code","source":"X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(train, targetLog, test_size=0.2)\nmodel2=LinearRegression()\nmodel2.fit(X_train_log,y_train_log)\nprint(\"Mean squared error in Test:\",np.mean((model2.predict(X_test_log) - y_test_log) ** 2))\nprint('R² of Test:',model2.score(X_test_log, y_test_log))\nprint(\"Mean squared error in train:\",np.mean((model2.predict(X_train_log) - y_train_log) ** 2))\nprint('R² of train:',model2.score(X_train_log, y_train_log))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5c304a154a94e6d725dcc84c700f897ecb059643"},"cell_type":"markdown","source":"This seems a much better model as the R2 on test is better and i feel it is because the SalePriceLog columnn is much more normal as seen from the qq plots above. This is a clear comparison that normal coulmns result in much better results!! :) (Ranges are there as it is selection biased and for every run there is a different R2)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2ab11272f0a0a0e53ed3547d8d02f65643b7ca57"},"cell_type":"markdown","source":"**Time to check if assumptions were satisfied**."},{"metadata":{"trusted":true,"_uuid":"78ed5a95fe1d6e4d244d84737825ca89c8ed912b","collapsed":true},"cell_type":"code","source":"#Residual vs fitted plot for training set for second model\nplot_lm_1 = plt.figure(1)\nplot_lm_1.set_figheight(8)\nplot_lm_1.set_figwidth(12)\n\nplot_lm_1.axes[0] = seaborn.residplot(model2.predict(X_train_log), y_train_log, lowess=True,scatter_kws={'alpha': 0.5}, line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_1.axes[0].set_title('Residuals vs Fitted - Train Model 2')\nplot_lm_1.axes[0].set_xlabel('Fitted values')\nplot_lm_1.axes[0].set_ylabel('Residuals')\nplt.show()\n\n#Residual vs fitted plot for test set for second model\nplot_lm_2 = plt.figure(1)\nplot_lm_2.set_figheight(8)\nplot_lm_2.set_figwidth(12)\n\nplot_lm_2.axes[0] = seaborn.residplot(model2.predict(X_test_log), y_test_log,lowess=True, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_2.axes[0].set_title('Residuals vs Fitted - Test Model 2')\nplot_lm_2.axes[0].set_xlabel('Fitted values')\nplot_lm_2.axes[0].set_ylabel('Residuals')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a89a65d481aa0dfe447fc46bb24faa70391f0a3","collapsed":true},"cell_type":"code","source":"#Residual vs fitted plot for training set for first model\nplot_lm_1 = plt.figure(1)\nplot_lm_1.set_figheight(8)\nplot_lm_1.set_figwidth(12)\n\nplot_lm_1.axes[0] = seaborn.residplot(model1.predict(X_train), y_train, lowess=True,scatter_kws={'alpha': 0.5}, line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_1.axes[0].set_title('Residuals vs Fitted - Train Model 1')\nplot_lm_1.axes[0].set_xlabel('Fitted values')\nplot_lm_1.axes[0].set_ylabel('Residuals')\nplt.show()\n\n#Residual vs fitted plot for test set for first model\nplot_lm_2 = plt.figure(1)\nplot_lm_2.set_figheight(8)\nplot_lm_2.set_figwidth(12)\n\nplot_lm_2.axes[0] = seaborn.residplot(model1.predict(X_test), y_test,lowess=True, scatter_kws={'alpha': 0.5}, line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\n\nplot_lm_2.axes[0].set_title('Residuals vs Fitted - Test Model 1')\nplot_lm_2.axes[0].set_xlabel('Fitted values')\nplot_lm_2.axes[0].set_ylabel('Residuals')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2638b7fa96b148ed9213c9fc67407175389ae3e1"},"cell_type":"markdown","source":"Time for my analysis.\nThe first Model, made with SalePrice did not have a very normal qq plot and its residual vs fitted plot also has a parabolic structrure which shows non-linearity. All of this contributes to a not very good model.\nHowever, the second model with SalePriceLog had a nearly normal qq plot and its residual vs fitted plot also has no pattern which means that all the assumptions of linear regression are satisfied. \n\nSo uptill this point our best Model is a MultipleLinearRegression plot using SalePriceLog as the targeted value.\n\n\nIn both the models, There is a problem of overfitting as R2 for train is much higher than that for test.\n\n\n**I will now be using Cross Vaidation to reduce overfitting as i feel that the Model fits the train model too well, i.e. i think there is a problem of overfitting.**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bd1f88a398a0a343be463352d783491a554aff4c"},"cell_type":"markdown","source":"**CROSS VALIDATION FOR FEATURE SELECTION AND REDUCE OVERFITTING**"},{"metadata":{"_uuid":"4d7d2259e05fd65584dec5f718fbf6e1d96ad078"},"cell_type":"markdown","source":"All right, i will quickly explain what i have done. I initially took a model that has all the columns as predictors and found its rmse. Now i remove 1 column at a time and again find rmse score using K- fold cross validation(here k is 10). If the score has increased then we were better off having that column, so i append it back into the included_cols list. After the code has run, i will have a list of all the predictors that contribute the best to Predicting correctly.\n\nAnd the icing over the cake, it is not selection biased."},{"metadata":{"trusted":true,"_uuid":"8026d13e6aaa2c2294f2a07a22ddd043b7556019","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nlm  = LinearRegression()\nscore = (np.sqrt(cross_val_score(lm, train, targetLog, cv=10, scoring='neg_mean_squared_error') * -1)).mean()\nprint (score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bf028d48c933a327be33ffc9b1b3011a8ae04a5","collapsed":true},"cell_type":"code","source":"all_cols = train.columns.values.tolist()\nincluded_cols = train.columns.values.tolist()\nfor i in all_cols:\n    prevscore = score\n    included_cols.remove(i)\n    score = (np.sqrt(cross_val_score(lm, train[included_cols], targetLog, cv=10, scoring='neg_mean_squared_error') * -1)).mean()\n    if (score>prevscore):\n        included_cols.append(i)\n        print('reverted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef4a93258e9ca922dda0f16e035adc2dbe9c5bfb","collapsed":true},"cell_type":"code","source":"final_score = (np.sqrt(cross_val_score(lm, train[included_cols], targetLog, cv=10, scoring='neg_mean_squared_error') * -1)).mean()\nfinal_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9073d18bba548b13bad5e89fe4dd1f17b9546d5c"},"cell_type":"markdown","source":"**The final score has reduced considerably, rather exponentially ;);). The score is much better than having all the predictors, therefore we can conclude that this model is better than the previous regression models and is also free from overfit!!**"},{"metadata":{"trusted":true,"_uuid":"22a356637af9d2f439b42f1f725bbc23fcf6c544","collapsed":true},"cell_type":"code","source":"#List of all the columns to be used to train for LinearRegression\nprint (len(included_cols))\nincluded_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c7dce9ef7db3277a3fa479572737ce2d8b2a937","collapsed":true},"cell_type":"code","source":"#Building the regression Model using the list of columns found using cross validation.\nto_use = train[included_cols]\nmodel3 = LinearRegression()\nmodel3.fit(to_use,targetLog)\nprint(\"Root Mean squared error:\",np.sqrt(np.mean((model3.predict(to_use) - targetLog) ** 2)))\nprint('R²:',model3.score(to_use, targetLog))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7b86685b9f5bca026662afe1e964639a05ae2b0c"},"cell_type":"markdown","source":"The R2 for this model is **89.35%** which is pretty good and its rmse is **0.1302955**. We will now try feature Selection techniques and build a model using them too. After thet we will compare and accept the best model"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6aee0d0605dca4977619cb94720827e942c8d41b"},"cell_type":"markdown","source":"**RIDGE REGRESSION**"},{"metadata":{"trusted":true,"_uuid":"95494bb40bdd877df4ea5591e64a700d0b512044","collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n## training the model\nalpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2,0.5,0.05, 1, 5, 10, 20]\nfor i in alpha_ridge:\n    ridgeReg = Ridge(alpha=i, normalize=True)\n    ridgeReg.fit(train,targetLog)\n    pred = ridgeReg.predict(train)\n    print(\"Root Mean squared error for \",i,\" is:\",np.sqrt(np.mean((pred - targetLog) ** 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dea792c211919e6cf07d25aaa6d087624d5e4ec2"},"cell_type":"markdown","source":"Minimum rmse is for alpha = 0.001.  Let us select that Ridge Regression Model."},{"metadata":{"trusted":true,"_uuid":"14fd8fc53bb30a1988789f8b42876c61b95d8610","collapsed":true},"cell_type":"code","source":"ridgeReg = Ridge(alpha=0.001, normalize=True)\nridgeReg.fit(train,targetLog)\npred = ridgeReg.predict(train)\nprint(\"Root Mean squared error is:\",np.sqrt(np.mean((pred - targetLog) ** 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9c6e171bec46816039c3b9ffe2e17cfa91eb18ae"},"cell_type":"markdown","source":"**LASSO REGRESSION**"},{"metadata":{"trusted":true,"_uuid":"69a79ada55280296171b7eeeca8df0afd4261f0f"},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso_ridge = [0.0005,1e-3,1e-2,0.5,0.05, 1, 5, 10, 20]\nfor i in lasso_ridge:\n    lassoReg = Lasso(alpha=i, normalize=True)\n    lassoReg.fit(train,targetLog)\n    pred = lassoReg.predict(train)\n    print(\"Root Mean squared error for \",i,\" is:\",np.sqrt(np.mean((pred - targetLog) ** 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dce16b8294e420e1393dbe10ceac1d84f79a0d14"},"cell_type":"markdown","source":"Minimum rmse is for alpha = 0.0005.  Let us select that Lasso Regression Model."},{"metadata":{"trusted":true,"_uuid":"f121db4a588b67ad31578cb1973d7f7174e230a9"},"cell_type":"code","source":"lassoReg = Lasso(alpha=0.0005, normalize=True)\nlassoReg.fit(train,targetLog)\npred = lassoReg.predict(train)\nprint(\"Root Mean squared error is:\",np.sqrt(np.mean((pred - targetLog) ** 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"86b2c7f6ac9541b720a62992a272415e4b785931"},"cell_type":"markdown","source":"**FINAL MODEL SELECTION**\n\nThe rmse of the LinearRegressionModel with Cross Validation was higher than the other two\n\nBetween Lasso and Ridge Regression, they have similar rmse, so i will choose Lasso Regression because when there are a large number f parameters, it is better to take Lasso as it cn actually make parameters = 0"},{"metadata":{"trusted":true,"_uuid":"6d80f375148985ab16936664a6378711cc642a0e"},"cell_type":"code","source":"#inputting the files again and concatenating\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\ndel train['SalePrice']\nprint(train.shape)\nprint(test.shape)\n\nframes = [test,train]\nconc  = pd.concat(frames)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"378c1d98dec005da867af1ec4fcfa5bba607b4be"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83b00a9e0f6f77e2ff505d7c1d57a1e77b027593","collapsed":true},"cell_type":"markdown","source":"conc has all the rows of test and train combined, now i will remove all the missing values."},{"metadata":{"trusted":true,"_uuid":"ebb5588dc4dd6ad0f5410f028b4d13b468e0ec4e","collapsed":true},"cell_type":"code","source":"#Finding columns with missing values and replacing using functions defined above\nmissing = conc.columns[conc.isnull().any()].tolist()\nfor i in missing:\n    types = conc[i].dtype\n    if (types == object):\n        str_miss(conc,i)\n    else:\n        num_miss(conc, i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d257c2ec4c2dc5a1c11824b3abbf187d14c30ebd"},"cell_type":"code","source":"#Creating dummy variables\nconc = pd.get_dummies(conc)\nprint(conc.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42d9cb2f71d427f0614b96518aefc91eb5a95ab4"},"cell_type":"code","source":"#Getting the test set back\ntest = conc.iloc[0:1459,:]\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27362cefc674017bb529c17993000ac6228e7fa2"},"cell_type":"code","source":"#Dropping the id column in test\ndel test['Id']\ntest.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9545f4cb6377290bbc781035a6dabafa3f106914"},"cell_type":"code","source":"LogPredicted = lassoReg.predict(test)\nPredicted = np.expm1(LogPredicted)\nPredicted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7240c313477ed044511658900d4a3e80ab6860e1"},"cell_type":"markdown","source":"***Predicted is the final answer that contains all the predicted values*** \n\nI am going to concatenate it to the test set."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ab3e08a0074b0ceeea03ee7270cbf127f553c1bd"},"cell_type":"code","source":"test = pd.read_csv(\"../input/test.csv\")\ntest['PredictedSalePrice'] = list(Predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5181fd918ae244f2f776118988c23fe50eeb1bfd"},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}