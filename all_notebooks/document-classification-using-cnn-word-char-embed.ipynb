{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%reload_ext tensorboard","metadata":{"id":"SzzjOu94qjeU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport pickle\nimport numpy as np\nimport random as rn\nimport pandas as pd\nfrom tqdm import tqdm\nimport nltk\nfrom nltk import ne_chunk, pos_tag, word_tokenize\nfrom nltk.tree import Tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, auc, roc_curve\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, Embedding, Conv1D, Concatenate, MaxPool1D, Flatten, Dropout, Dense, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, TerminateOnNaN, Callback\nfrom tensorflow.keras.optimizers import RMSprop, SGD, Adam\nfrom tensorflow.keras.utils import to_categorical","metadata":{"id":"Ce0pxQmm-3g1","execution":{"iopub.status.busy":"2021-06-13T10:07:04.412673Z","iopub.execute_input":"2021-06-13T10:07:04.413192Z","iopub.status.idle":"2021-06-13T10:07:11.641528Z","shell.execute_reply.started":"2021-06-13T10:07:04.413088Z","shell.execute_reply":"2021-06-13T10:07:11.640691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.isfile(\"text_label.csv\"):\n    files = os.listdir(\"./documents\")\n    label = []\n    text = []\n    num = []\n    \n    for file in tqdm(files):\n        f = open(\"./documents\"+\"/\"+file, \"r\")\n        text.append(f.read())\n        f.close()\n        label.append(file.split(\"_\")[0])\n        num.append(file.split(\"_\")[1].split(\".\")[0])\n        \n    d = {\"Text\": text, \"D_Number\": num, \"Label\": label}\n    data = pd.DataFrame(d)    \n    data.to_csv(\"text_label.csv\", index = False, header = True)\n    \nelse:\n    data = pd.read_csv(\"text_label.csv\", index_col = False)","metadata":{"id":"KfFt1R7s-3g2","execution":{"iopub.status.busy":"2021-06-13T10:07:11.642992Z","iopub.execute_input":"2021-06-13T10:07:11.643448Z","iopub.status.idle":"2021-06-13T10:07:12.011046Z","shell.execute_reply.started":"2021-06-13T10:07:11.643402Z","shell.execute_reply":"2021-06-13T10:07:12.009781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"id":"G39D2fq4-3g2","outputId":"282443d7-180c-42b9-b0c8-23f959266bf3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of classes: \", len(data.Label.value_counts()))\nprint(\"Number of documents of each class\\n\", data.Label.value_counts())","metadata":{"id":"iQ6qQzmQ-3g4","outputId":"8127ea2e-74c3-49ea-d7af-cb81ec56bd21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Preprocess(text_data):\n    \"\"\"\"\n    Function to preprocess text file, generate preprocessed email domain, subjects, and text\n    \"\"\"\n    \n    preprocessed_email = []\n    preprocessed_subject = []\n    preprocessed_text = []\n    \n    for sentence in tqdm(text_data):\n        \n        #preprocessing email\n        domain = re.findall(\"@[\\w.]+\", sentence)\n        email = \"\"\n        for items in domain:\n            items = items.replace(\"@\", \"\")    \n            items = items.split(\".\")\n            for i in set(items):\n                if((len(i) > 2) and i != \"com\" and i != \"COM\"):\n                    email += i + \" \"\n        preprocessed_email.append(email.strip())\n        \n        #preprocessing subject    \n        text_split = sentence.split(\"\\n\")\n        for item in text_split:\n            if(item.startswith(\"Subject:\")):\n                subject = \"\"\n                for word in item.split():\n                    if not word.endswith(\":\"):\n                        subject += word + \" \"\n                subject = re.sub(\"[^0-9a-zA-Z\\s]\", \" \", subject)\n                subject = \" \".join(subject.split()).strip()\n        preprocessed_subject.append(subject.lower())\n        \n        #preprocessing text\n        #https://towardsdatascience.com/how-i-preprocessed-text-data-using-regular-expressions-for-my-text-classification-task-cnn-cb206e7274ed\n\n        text = re.sub(r\"(.*)Subject:(.*?)(.*)\\n\", \" \", sentence)   #remove subject line\n        text = re.sub(r\"(.*)From:(.*?)(.*)\\n\", \" \", text)          #remove from line\n        text = re.sub(r\"(.*)Write to:(.*?)(.*)\\n\", \" \", text)      #remove write to line\n        text = re.sub(r\"(.*):(.*?)\", \" \", text)                    #remove words ending with :\n\n        #decontract\n        text = re.sub(r\"won't\", \"will not\", text)\n        text = re.sub(r\"can\\'t\", \"can not\", text)\n        text = re.sub(r\"n\\'t\", \" not\", text)\n        text = re.sub(r\"\\'re\", \" are\", text)\n        text = re.sub(r\"\\'s\", \" is\", text)\n        text = re.sub(r\"\\'d\", \" would\", text)\n        text = re.sub(r\"\\'ll\", \" will\", text)\n        text = re.sub(r\"\\'t\", \" not\", text)\n        text = re.sub(r\"\\'ve\", \" have\", text)\n        text = re.sub(r\"\\'m\", \" am\", text)\n\n        text = re.sub(r\"[\\w\\-\\.]+@[\\w\\.-]+\\b\", \" \", text)          #remove all emails\n        text = re.sub(r\"[\\n\\t]\",\" \", text)                         #remove line feeds and tabs\n        text = re.sub(r\"<.*>\", \" \", text)                          #remove text within angular brackets\n        text = re.sub(r\"\\(.*\\)\", \" \", text)                        #remove text within parantheses\n        text = re.sub(r\"\\[.*\\]\", \" \", text)                        #remove text within square brackets\n        text = re.sub(r\"\\{.*\\}\", \" \", text)                        #remove text within curly braces\n        text = re.sub(\"[0-9]+\", \" \", text)                         #remove all digts\n        text = re.sub(\"[^A-Za-z\\s]\", \" \", text)                    #remove all characters except alphabets and spaces\n\n        #https://towardsdatascience.com/how-i-preprocessed-text-data-using-regular-expressions-for-my-text-classification-task-cnn-cb206e7274ed\n        chunks = []\n        chunks = (list(ne_chunk(pos_tag(word_tokenize(text)))))\n\n        for i in chunks:\n            if(type(i) == Tree):\n                if i.label() == \"GPE\":\n                    j = i.leaves()\n                    if len(j)>1:   #if a city name has two or more words we combine it with underscore\n                        gpe = \"_\".join([term for term, pos in j])\n                        text = re.sub(rf\"{j[1][0]}\", gpe, text)\n                        text = re.sub(rf\"{j[0][0]}\", \" \", text)\n                if i.label() == \"PERSON\":\n                    for term, pos in i.leaves():\n                        text = re.sub(re.escape(term), \"\", text)\n                \n        #https://stackoverflow.com/questions/20802056/python-regular-expression-1\n        text = re.sub(r\"\\b_([a-zA-z]+)_\\b\", r\"\\1\", text) #replace _word_ to word\n        text = re.sub(r\"\\b_([a-zA-z]+)\\b\", r\"\\1\", text) #replace_word to word\n        text = re.sub(r\"\\b([a-zA-z]+)_\\b\", r\"\\1\", text) #replace word_ to word\n\n        text = re.sub(r\"\\b[a-zA-Z]{1}_([a-zA-Z]+)\", r\"\\1\", text) \n        text = re.sub(r\"\\b[a-zA-Z]{2}_([a-zA-Z]+)\", r\"\\1\", text)\n\n        text = text.lower()\n        text_split = text.split()\n        text = \"\"\n        for words in text_split:\n            if((len(words) > 2) and (len(words) < 15)):\n                text += words + \" \"\n        preprocessed_text.append(text.strip())\n    \n    return (preprocessed_email, preprocessed_subject, preprocessed_text)","metadata":{"id":"UO-aLWF4-3g4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not os.path.isfile(\"preprocessed_final.csv\"):\n    processed_email, processed_subject, processed_text = Preprocess(data.Text.values)\n    data[\"Email\"] = processed_email\n    data[\"Subject\"] = processed_subject\n    data[\"Processed_text\"] = processed_text\n    \n    data.to_csv(\"preprocessed_final.csv\", index = False, header = True)\n    \nelse:\n    data = pd.read_csv(\"preprocessed_final.csv\", index_col = False)","metadata":{"id":"fvJdjZFN-3g5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"id":"YK8M0f5S-3g5","outputId":"a5002743-33c4-4fc7-b367-74e58d3b9043"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking if preprocessing is correct\n#document 49960 is in index 0\n\nprint(\"Label: \", data.Label[0])\nprint(\"-\"*25)\nprint(\"Email: \", data.Email[0])\nprint(\"-\"*25)\nprint(\"Subject: \", data.Subject[0])\nprint(\"-\"*25)\nprint(\"Processed Text\\n\", data.Processed_text[0])\nprint(\"-\"*25)","metadata":{"id":"UeeWQr1f-3g7","outputId":"9c791a4f-264c-411c-d3f0-187246aa1f99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"combined_data\"] = data[\"Email\"].astype(str)+\" \"+data[\"Subject\"].astype(str)+\" \"+data[\"Processed_text\"].astype(str)\ndata = data.drop([\"Email\", \"Subject\", \"Processed_text\"], axis = 1)\ndata.head()","metadata":{"id":"VKBHFuPo-3g-","outputId":"2c7f8b06-c89c-4841-888d-fe121abb84c5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data[\"combined_data\"]\ny = data[\"Label\"]\ny = pd.get_dummies(data[\"Label\"].values)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify = y, random_state = 42)","metadata":{"id":"udWrRJVx-3hd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"id":"AgYAPSQm-3he","outputId":"04da8d39-ffe8-4341-9f63-fdebbc6ed490"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class F1_Score(Callback):\n\n    def on_train_begin(self, logs = {}):\n        self.f1 = []       \n\n    def on_epoch_end(self, epoch, logs = {}):\n        y_pred = []\n        y_true = []\n\n        val_predict = np.asarray(self.model.predict(padded_test))\n        for y in val_predict:\n            y_pred.append(np.argmax(y))\n        for y in np.asarray(y_test):\n            y_true.append(np.argmax(y))\n\n        val_f1 = f1_score(np.asarray(y_true), np.asarray(y_pred), average = \"micro\")\n        self.f1.append(val_f1)\n        print(\"\\t F1 Score: \", val_f1)","metadata":{"id":"iyRo0xF0-3hf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model-1 Using CNN with word embeddings","metadata":{"id":"HfbEyNm0qu5p"}},{"cell_type":"code","source":"!rm -rf ./logs/","metadata":{"id":"SmGYChF9q1Mf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"PYTHONHASHSEED\"] = \"0\"\ntf.keras.backend.clear_session()\nnp.random.seed(42)\nrn.seed(42)","metadata":{"id":"Vp287UxA8Lsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = Tokenizer(filters = \"_\")\nt.fit_on_texts(X_train)\nvocab_size = len(t.word_index) + 1\nencoded_train = t.texts_to_sequences(X_train)\nencoded_test = t.texts_to_sequences(X_test)\nmax_length = 100                   \npadded_train = pad_sequences(encoded_train, maxlen = max_length, padding = \"post\", truncating = \"post\")\npadded_test = pad_sequences(encoded_test, maxlen = max_length, padding = \"post\", truncating = \"post\")\n\nwith open(\"./glove_vectors\", \"rb\") as f:\n    glove = pickle.load(f)\n    glove_words = set(glove.keys())\n    \nembedding_matrix = np.zeros((vocab_size, 300))\nfor word, i in t.word_index.items():\n    if word in glove_words:\n        embedding_vector = glove[word]\n        embedding_matrix[i] = embedding_vector","metadata":{"id":"J5x8IRkI-3he"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_layer = Input(shape = (max_length, ), name = \"Input_Layer\")\n\nembed_layer = Embedding(input_dim = vocab_size, output_dim = 300, weights = [embedding_matrix], \n                        input_length = max_length, trainable = False, \n                        name = \"Embedding_Layer\")(input_layer)\n\nconv_1 = Conv1D(128, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_1\")(embed_layer)\n\nconv_2 = Conv1D(126, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_2\")(embed_layer)\n\nconv_3 = Conv1D(124, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_3\")(embed_layer)              \n\nconcat_layer_1 = Concatenate()([conv_1, conv_2, conv_3]) \n\n\nmax_pool_layer_1 = MaxPool1D(pool_size = 2, padding = \"valid\",\n                             name = \"MaxPooling_layer_1\")(concat_layer_1)\n\nconv_4 = Conv1D(68, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_4\")(max_pool_layer_1)\nconv_5 = Conv1D(64, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_5\")(max_pool_layer_1)\nconv_6 = Conv1D(62, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_6\")(max_pool_layer_1)\n\nconcat_layer_2 = Concatenate()([conv_4, conv_5, conv_6])\n\nmax_pool_layer_2 = MaxPool1D(pool_size = 2, padding = \"valid\",\n                             name = \"MaxPooling_layer_2\")(concat_layer_2)\n\ndrop_1 = Dropout(0.8, name = \"Dropout_layer_1\")(max_pool_layer_2)\n\nconv_7 = Conv1D(32, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_7\")(drop_1)\n\nflatten = Flatten(name = \"Flatten_layer\")(conv_7)\n\ndrop_2 = Dropout(0.1, name = \"Dropout_layer_2\")(flatten)\n\ndense_1 = Dense(100, activation = \"relu\", \n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Dense_Layer_1\")(drop_2)\n\ndrop_3 = Dropout(0.05, name = \"Dropout_layer_3\")(dense_1)\n\noutput = Dense(20, activation = \"softmax\", \n               name = \"Output_Layer\")(drop_3)\n\nmodel1 = Model(inputs = input_layer, outputs = output, \n               name = \"Model-1\")\n\nmodel1.summary()","metadata":{"id":"7wyu0bLB-3he","outputId":"be108739-2f32-4cc4-e197-e3ea74be3ac9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model1, show_shapes = True, to_file = \"model1.png\")","metadata":{"id":"c6vPDTm9-3he","outputId":"2a85d377-b4e6-491c-95f6-e14325bfeeab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(learning_rate = 0.001) \n\nmodel1.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n\nf1 = F1_Score()\n\ncheckpoint = ModelCheckpoint(filepath = \"best_model_1_{val_accuracy:.4f}.hdf5\",\n                             monitor='val_accuracy',  verbose = 1, save_best_only = True, mode = \"auto\")\n\nearlystop = EarlyStopping(monitor = \"val_accuracy\", patience = 1, verbose = 1, mode = \"auto\")\n\nlog_dir = \"logs/model1\"\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1, write_graph = True)\n\ncallback_list = [checkpoint, earlystop, tensorboard_callback, f1]\n\nmodel1.fit(padded_train, y_train, validation_data = (padded_test, y_test), batch_size = 512, epochs = 100, callbacks = callback_list)","metadata":{"id":"XjR7Lkpi-3hf","outputId":"65f26ec2-dad0-4b00-d581-f4dc9c8858ed"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%tensorboard --logdir logs/model1","metadata":{"id":"DoTODbuoHTic","outputId":"d40c1303-8e08-457e-8728-c293b290d8ae"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model-2 Using CNN with character embeddings","metadata":{"id":"SfZATDNkiFZs"}},{"cell_type":"code","source":"!rm -rf ./logs/","metadata":{"id":"O6k4fZNgPAd2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"PYTHONHASHSEED\"] = \"0\"\ntf.keras.backend.clear_session()\nnp.random.seed(42)\nrn.seed(42)","metadata":{"id":"IOQ7BMm78H1I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t = Tokenizer(filters = \"_\", char_level = True, oov_token = \"UNK\")\nt.fit_on_texts(X_train)\nvocab_size = len(t.word_index) + 1\nencoded_train = t.texts_to_sequences(X_train)\nencoded_test = t.texts_to_sequences(X_test)\nmax_length = 3000              \npadded_train = pad_sequences(encoded_train, maxlen = max_length, padding = \"post\", truncating = \"post\")\npadded_test = pad_sequences(encoded_test, maxlen = max_length, padding = \"post\", truncating = \"post\")\n\nembedding_index = dict()\nwith open(\"./glove.840B.300d-char.txt\", \"rb\") as f:\n    for line in f:\n        line_split = line.strip().split()\n        char = line_split[0]\n        coefs = np.asarray(line_split[1:])\n        embedding_index[char] = coefs\nf.close()\n\n#https://towardsdatascience.com/character-level-cnn-with-keras-50391c3adf33\nembedding_matrix = []\nembedding_matrix.append(np.zeros(vocab_size))\nfor char, i in t.word_index.items():\n    onehot = np.zeros(vocab_size)\n    onehot[i - 1] = 1\n    embedding_matrix.append(onehot)\n\nembedding_matrix = np.asarray(embedding_matrix)","metadata":{"id":"mN5-eP1Uunz6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_layer = Input(shape = (max_length, ), name = \"Input_Layer\")\n\nembed_layer = Embedding(input_dim = vocab_size, output_dim = 40, weights = [embedding_matrix], \n                        input_length = max_length, trainable = False, \n                        name = \"Embedding_Layer\")(input_layer)\n\nconv_1 = Conv1D(256, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeNormal(),\n                name = \"Convolution_layer_1\")(embed_layer)\n\nconv_2 = Conv1D(128, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeNormal(),\n                name = \"Convolution_layer_2\")(conv_1)\n\nmax_pool_layer_1 = MaxPool1D(pool_size = 4, padding = \"valid\",\n                             name = \"MaxPooling_layer_1\")(conv_2)\n\ndrop_1 = Dropout(0.4, name = \"Dropout_layer_1\")(max_pool_layer_1)\n\nconv_3 = Conv1D(64, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeNormal(),\n                name = \"Convolution_layer_3\")(drop_1)\n\nconv_4 = Conv1D(32, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeNormal(),\n                name = \"Convolution_layer_4\")(conv_3)\n\nmax_pool_layer_2 = MaxPool1D(pool_size = 4, padding = \"valid\",\n                             name = \"MaxPooling_layer_2\")(conv_4)\n\ndrop_2 = Dropout(0.4, name = \"Dropout_layer_2\")(max_pool_layer_2)\n\nflatten = Flatten(name = \"Flatten_layer\")(drop_2)\n\ndense_1 = Dense(100, activation = \"relu\", \n                kernel_initializer = tf.keras.initializers.HeNormal(),\n                name = \"Dense_Layer_1\")(flatten)\n\ndrop_3 = Dropout(0.1, name = \"Dropout_layer_3\")(dense_1)\n\noutput = Dense(20, activation = \"softmax\", \n               name = \"Output_Layer\")(drop_3)\n\nmodel2 = Model(inputs = input_layer, outputs = output, \n               name = \"Model-2\")\n\nmodel2.summary()             ","metadata":{"id":"eu9utEHUJ3F8","outputId":"061255d5-216b-4315-a39e-8567bf5c7c4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(model2, show_shapes = True, to_file = \"model2.png\")","metadata":{"id":"eziENYKWCxwq","outputId":"d8133fdd-108a-4dec-cc52-1dbb0d54f604"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(learning_rate = 0.01) \n\nmodel2.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n\nf1 = F1_Score()\n\ncheckpoint = ModelCheckpoint(filepath = \"best_model_2_{val_accuracy:.4f}.hdf5\",\n                             monitor='val_accuracy',  verbose = 1, save_best_only = True, mode = \"auto\")\n\nearlystop = EarlyStopping(monitor = \"val_accuracy\", patience = 2, verbose = 1, mode = \"auto\")\n\nlog_dir = \"logs/model2\"\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1, write_graph = True)\n\ncallback_list = [checkpoint, earlystop, tensorboard_callback, f1]\n\nmodel2.fit(padded_train, y_train, validation_data = (padded_test, y_test), batch_size = 512, epochs = 100, callbacks = callback_list)","metadata":{"id":"OZeMxTt9n7FD","outputId":"2f5d48ed-490e-421d-eb18-c6426b6c0434"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%tensorboard --logdir logs/model2","metadata":{"id":"qkpaQ_nFQhbv","outputId":"9349cc83-2ceb-4f46-d43b-0f746d1b02a2"},"execution_count":null,"outputs":[]}]}