{"cells":[{"metadata":{"_uuid":"c7e803bcb692553a80f9fdded4c1a4f9a7897428"},"cell_type":"markdown","source":"# Breast cancer prediction with Multi-layer Perceptron classifier\n\n### Author\nPiotr Tynecki  \nLast edition: May 16, 2018\n\n#### Alternative study with Logistic Regression\n\nIf you're interested about my previous study for breast cancer prediction using **Logistic Regression** feel free to go to that [kaggle link](https://www.kaggle.com/ptynecki/breast-cancer-prediction-with-lr-99).\n\n### About the Breast Cancer Wisconsin Diagnostic Dataset\nBreast Cancer Wisconsin Diagnostic Dataset (WDBC) consists of features which were computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. Those features describe the characteristics of the cell nuclei found in the image.\n\n![Diagnosing Breast Cancer from Image](https://kaggle2.blob.core.windows.net/datasets-images/180/384/3da2510581f9d3b902307ff8d06fe327/dataset-cover.jpg)\n\nThis dataset has 569 instances: 212 - Malignant and 357 - Benign. It consists of 31 attributes including the class attribute. The attributes description is ten real-valued features which are computed for each cell nucleus. These features include: Texture, Radius, Perimeter, Smoothness, Area, Concavity, Compactness, Symmetry, Concave points and Fractal dimension.\n\nIn this document I demonstrate an automated methodology to predict if a sample is benign or malignant."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"da9c6cd47b5c79bed433cc47618a56837322db5d"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\nfrom sklearn.preprocessing import Normalizer, MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, LabelEncoder\nfrom sklearn.pipeline import Pipeline","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"4bec5d24fe6bc971286ea5b327cc2bc723fdcc91"},"cell_type":"markdown","source":"### Step 1: Exploratory Data Analysis (EDA)\nEDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task. It let us to summarize data main characteristics."},{"metadata":{"trusted":false,"_uuid":"614080eef877a8cc3f436522abe889a14958d4dc"},"cell_type":"code","source":"breast_cancer = pd.read_csv('../input/data.csv')\nbreast_cancer.head()","execution_count":2,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"10d690e530f6ec976f763532c5cff1ea94ad95d4"},"cell_type":"code","source":"breast_cancer.info()","execution_count":3,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"158f5216c1537af8caaa25fa3fe7a5e0c790fa4d"},"cell_type":"code","source":"breast_cancer.shape","execution_count":4,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ddb2cd9defbef85b047cb467cbb7ca0600a16b7b"},"cell_type":"code","source":"breast_cancer.describe()","execution_count":5,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"38a3fe46d1084c96b07960b5bf4d103d71099d6b"},"cell_type":"code","source":"breast_cancer.groupby('diagnosis').size()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"23755bbbcc640b0ebb965b2f355d842eda344361"},"cell_type":"markdown","source":"#### Data quality checks"},{"metadata":{"trusted":false,"_uuid":"aa3d5c40f7a7efbf61238d9bd3944f1a2049b053"},"cell_type":"code","source":"breast_cancer.isnull().sum()","execution_count":7,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"87fdfc0e1694ca844752dac790c4d14b177e5d02"},"cell_type":"code","source":"for field in breast_cancer.columns:\n    amount = np.count_nonzero(breast_cancer[field] == 0)\n    \n    if amount > 0:\n        print('Number of 0-entries for \"{field_name}\" feature: {amount}'.format(\n            field_name=field,\n            amount=amount\n        ))","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"a06977de599e1a2094e6d0b74f7e0c7ceef02165"},"cell_type":"markdown","source":"### Step 2: Feature Engineering"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"438a3421517af2feab182ceca38e2396bb10f05e"},"cell_type":"code","source":"# Features \"id\" and \"Unnamed: 32\" are not useful \nfeature_names = breast_cancer.columns[2:-1]\nX = breast_cancer[feature_names]\n# \"diagnosis\" feature is our class which I wanna predict\ny = breast_cancer.diagnosis","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"ed083f7eaec1b4389b74fca255d68bc9930ab23b"},"cell_type":"markdown","source":"#### Transforming the prediction target"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4a9999cf49909c36cc65261cd345776b02a407b6"},"cell_type":"code","source":"class_le = LabelEncoder()\n# M -> 1 and B -> 0\ny = class_le.fit_transform(breast_cancer.diagnosis.values)","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"51e3b6ac955925377d2bc1c02b34e7768c006a88"},"cell_type":"markdown","source":"#### Correlation Matrix\nA matrix of correlations provides useful insight into relationships between pairs of variables."},{"metadata":{"trusted":false,"_uuid":"8e3b7648c9265377ab4c28578e150d5b1abbfbc7"},"cell_type":"code","source":"sns.heatmap(\n    data=X.corr(),\n    annot=True,\n    fmt='.2f',\n    cmap='RdYlGn'\n)\n\nfig = plt.gcf()\nfig.set_size_inches(20, 16)\n\nplt.show()","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"1edc4bd8a9f5feaabff05088ef522a447ee666f2"},"cell_type":"markdown","source":"### Step 3: Multi-layer Perceptron classifier evaluation after Pipeline and GridSearchCV usage\n\nFor this case study I decided to use [Multi-layer Perceptron classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) classifier.\n\n#### Model Parameter Tuning\n[GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) returns the set of parameters which have an imperceptible impact on model evaluation. Model parameter tuning with other steps like data preprocessing and cross-validation splitting strategy can be automated by [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class.\n\n#### Data standardization\n[Preprocessing data](http://scikit-learn.org/stable/modules/preprocessing.html) provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators."},{"metadata":{"_uuid":"0cbd6f0b9b62088889e15197674b2bdb2a1beb27"},"cell_type":"markdown","source":"Let's start with defining the Pipeline instance. In this case I used three different approach `Normalizer`, `MinMaxScaler`, `StandardScaler`, `RobustScaler`, `QuantileTransformer` for data preprocesing and `MLPClassifier` for classification."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"999c7631a90886fd36ab4034d1c1cf526343ce3e"},"cell_type":"code","source":"pipe = Pipeline(steps=[\n    ('preprocess', StandardScaler()),\n    ('classification', MLPClassifier())\n])","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"c3e5bacbf50ce62eec5588a5c399dbafcc75b83a"},"cell_type":"markdown","source":"Next, I needed to prepare attributes with values for above steps which wanna to check by the model parameter tuning process: `activation`, `solver`, `max_iter` and `alpha`."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d3c7aca7679b6f8f9d928225159a4c2c0b82f5e4"},"cell_type":"code","source":"random_state = 42\nmlp_activation = ['identity', 'logistic', 'tanh', 'relu']\nmlp_solver = ['lbfgs', 'sgd', 'adam']\nmlp_max_iter = range(1000, 10000, 1000)\nmlp_alpha = [1e-4, 1e-3, 0.01, 0.1, 1]\npreprocess = [Normalizer(), MinMaxScaler(), StandardScaler(), RobustScaler(), QuantileTransformer()]","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"f987af43fe9e91a6654b59cd8330a6320d544b33"},"cell_type":"markdown","source":"Next, I needed to prepare supported combinations for classifier parameters including above attributes. In Multi-layer Perceptron classifier case I decided to opt out of the PCA or any other feature selection techniques."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3a67d95ca305d741de9e8df9c226fecb74da9ef1"},"cell_type":"code","source":"mlp_param_grid = [\n    {\n        'preprocess': preprocess,\n        'classification__activation': mlp_activation,\n        'classification__solver': mlp_solver,\n        'classification__random_state': [random_state],\n        'classification__max_iter': mlp_max_iter,\n        'classification__alpha': mlp_alpha\n    }\n]","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"6a2409b0e0c50a1badb107a546ea923823004be6"},"cell_type":"markdown","source":"Next, I needed to prepare cross-validation splitting strategy object with `StratifiedKFold` and passed it with others to `GridSearchCV`. In that case for evaluation I used `f1 score` metric."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"76ecb54a1f9db741bebf725740b894d0707b9595"},"cell_type":"code","source":"# strat_k_fold = StratifiedKFold(\n#     n_splits=10,\n#     random_state=42\n# )\n\n# mlp_grid = GridSearchCV(\n#     pipe,\n#     param_grid=mlp_param_grid,\n#     cv=strat_k_fold,\n#     scoring='f1',\n#     n_jobs=-1,\n#     verbose=2\n# )\n\n# mlp_grid.fit(X, y)\n\n# # Best MLPClassifier parameters\n# print(mlp_grid.best_params_)\n# # Best score for MLPClassifier with best parameters\n# print('\\nBest F1 score for MLP: {:.2f}%'.format(mlp_grid.best_score_ * 100))\n\n# best_params = mlp_grid.best_params_","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"97e25b35b710f6776048a4c43a5db6566ce3c5cb"},"cell_type":"markdown","source":"#### Model evaluation\n\nFinally, after a few hours of computation, I established the best parameters values which I passed to new feature selection and classifier instances. `best_params` returned `StandardScaler` for data preprocessing and `1000`, `0.1`, `'logistic'` and `'adam'` values for `max_iter`, `alpha`, `activation` and `solver` classifier attributes.\n\nWhat else, I discovered that `train_test_split` function gave the best F1 score with split around 32% of data for training and 68% for testing."},{"metadata":{"trusted":false,"_uuid":"94e906e80e114711f0c19f8e676f1535b0a24f36"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    random_state=42,\n    test_size=0.32\n)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":16,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5da36760a2041c7bf74b9ecd1fba0def64889ed7"},"cell_type":"code","source":"scaler = StandardScaler()\n\nprint('\\nData preprocessing with {scaler}\\n'.format(scaler=scaler))\n\nX_train_scaler = scaler.fit_transform(X_train)\nX_test_scaler = scaler.transform(X_test)\n\nmlp = MLPClassifier(\n    max_iter=1000,\n    alpha=0.1,\n    activation='logistic',\n    solver='adam',\n    random_state=42\n)\nmlp.fit(X_train_scaler, y_train)\n\nmlp_predict = mlp.predict(X_test_scaler)\nmlp_predict_proba = mlp.predict_proba(X_test_scaler)[:, 1]\n\nprint('MLP Accuracy: {:.2f}%'.format(accuracy_score(y_test, mlp_predict) * 100))\nprint('MLP AUC: {:.2f}%'.format(roc_auc_score(y_test, mlp_predict_proba) * 100))\nprint('MLP Classification report:\\n\\n', classification_report(y_test, mlp_predict))\nprint('MLP Training set score: {:.2f}%'.format(mlp.score(X_train_scaler, y_train) * 100))\nprint('MLP Testing set score: {:.2f}%'.format(mlp.score(X_test_scaler, y_test) * 100))","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"0e04dceb3de94fe6623bb5f0d09caa5d1f72c061"},"cell_type":"markdown","source":"#### Confusion Matrix\n\nAlso known as an Error Matrix, is a specific table layout that allows visualization of the performance of an algorithm. The table have two rows and two columns that reports the number of False Positives (FP), False Negatives (FN), True Positives (TP) and True Negatives (TN). This allows more detailed analysis than accuracy."},{"metadata":{"trusted":false,"_uuid":"0012453b5d85288ee1ff20a9b78fadd38541e6fe"},"cell_type":"code","source":"outcome_labels = sorted(breast_cancer.diagnosis.unique())\n\n# Confusion Matrix for MLPClassifier\nsns.heatmap(\n    confusion_matrix(y_test, mlp_predict),\n    annot=True,\n    fmt=\"d\",\n    xticklabels=outcome_labels,\n    yticklabels=outcome_labels\n)","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"a3211f043c29717a4d6373d534438957e094a16b"},"cell_type":"markdown","source":"#### Receiver Operating Characteristic (ROC)\n\n[ROC curve](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied."},{"metadata":{"trusted":false,"_uuid":"1a07d045a1865b4e68881132c4ed678785fa67b0"},"cell_type":"code","source":"# ROC for MLPClassifier\nfpr, tpr, thresholds = roc_curve(y_test, mlp_predict_proba)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for MLPClassifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"d89212d5f80a12a9f238ad5e976c5aee262fdd5b"},"cell_type":"markdown","source":"#### F1-score after 10-fold cross-validation"},{"metadata":{"trusted":false,"_uuid":"620af960419de1fced2bff8d859f1c7e0e609930"},"cell_type":"code","source":"strat_k_fold = StratifiedKFold(\n    n_splits=10,\n    random_state=42\n)\n\nscaler = StandardScaler()\n\nX_std = scaler.fit_transform(X)\n\nfe_score = cross_val_score(\n    mlp,\n    X_std,\n    y,\n    cv=strat_k_fold,\n    scoring='f1'\n)\n\nprint(\"MLP: F1 after 10-fold cross-validation: {:.2f}% (+/- {:.2f}%)\".format(\n    fe_score.mean() * 100,\n    fe_score.std() * 2\n))","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"10124e98a79242c99e5a82a125564ae56bf962e2"},"cell_type":"markdown","source":"### Final step: Conclusions\n\nAfter the application of data standardization and tuning the classifier parameters I achieved the following results:\n\n* Accuracy: ~99.5%\n* F1-score: 99%\n* Precision: 99%\n* Recall: 99%\n\nF1 score after 10-fold cross-validation is a little lower (-0.01%) than in my previous study for [breast cancer prediction using Logistic Regression](https://www.kaggle.com/ptynecki/breast-cancer-prediction-with-lr-99).\n\nI would love to knows your comments and other tuning proposals for that study case."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}