{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv(os.path.join(dirname, filename),delim_whitespace=True, header=None)\ndata.columns = ['lever_position', 'ship_speed', 'gt_shaft', 'gt_rate', 'gg_rate', 'sp_torque', 'pp_torque',\n                     'hpt_temp', 'gt_c_i_temp', 'gt_c_o_temp', 'hpt_pressure', 'gt_c_i_pressure', 'gt_c_o_pressure',\n                     'gt_exhaust_pressure', 'turbine_inj_control', 'fuel_flow', 'gt_c_decay',  'gt_t_decay']\ndata = data.dropna()\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analyze data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.round(data.corr(),4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Eliminate unneccessary features\n* We can easily see that gt_c_i_temp has std=0 that means it is a constant variable not an important variable that effect on our final result => drop  'gt_c_i_temp' column\n* Correlation of \"gt_c_i_pressure\" is 0 to all others => we can drop gt_c_i_pressure"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop('gt_c_i_temp', axis=1)\ndata=data.drop('gt_c_i_pressure',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split data to X(Features) and Y(Responses)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=data[['lever_position', 'ship_speed', 'gt_shaft', 'gt_rate', 'gg_rate', 'sp_torque',\n        'pp_torque', 'hpt_temp', 'gt_c_o_temp', 'hpt_pressure', 'gt_c_o_pressure','gt_exhaust_pressure',\n        'turbine_inj_control', 'fuel_flow']]\nY1=data['gt_c_decay']\nY2=data['gt_t_decay']\nY=pd.DataFrame([Y1,Y2]).transpose()\nY","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further Correlation Analyze\n* Since all the features despite having high correlation between each other but very low correlation for the response, we should assume that the features and responses may have a non-linear relationship."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mat= np.round(data.corr(),4)\nplt.figure(figsize = (18,9))\nsns.heatmap(corr_mat, annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Further Correlation Analyze\n* Our response Y1 and Y2 have extremely small correlation => We can consider them independent and treat them as two difference response and affect by different features."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mat= Y.corr()\nplt.figure(figsize = (18,9))\nsns.heatmap(corr_mat, annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normality Test\nFrom normality test we are sure that our data do not follow normal distribution => Linear regression is not viable"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in data.columns:\n    stat,p= stats.kstest(data[col],'norm')\n    if p<=0.05:\n        print('Feature: %s is not normal'%col)\n    else:\n        print('Feature: %s is normal'%col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split data for training model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,Y)\nX_train1,X_test1,y_train1,y_test1=train_test_split(X,Y1)\nX_train2,X_test2,y_train2,y_test2=train_test_split(X,Y2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning library Import"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor #Ensemble using averaging method\nfrom xgboost import XGBRegressor #Ensemble using boosting method\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model selection\nBy training everything by it default setting, we will find out which model perform best in default setting\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"svr=SVR()\nknn= KNeighborsRegressor()\ntree=DecisionTreeRegressor()\nbagg=BaggingRegressor()\nxgb=XGBRegressor()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Y1 model selection\nBagging Regressor is the best model for Y1"},{"metadata":{"trusted":true},"cell_type":"code","source":"y1_train=[]\ny1_test=[]\nsvr.fit(X_train1, y_train1)\nknn.fit(X_train1, y_train1)\ntree.fit(X_train1, y_train1)\nbagg.fit(X_train1, y_train1)\nxgb.fit(X_train1, y_train1)\nprint('Accuracy of SVRegression on training set: {:.4f}'\n     .format(svr.score(X_train1, y_train1)))\ny1_train.append(svr.score(X_train1, y_train1))\nprint('Accuracy of SVRegression on test set: {:.4f}'\n     .format(svr.score(X_test1, y_test1)))\ny1_test.append(svr.score(X_test1, y_test1))\n\nprint('Accuracy of KNN Regressor on training set: {:.4f}'\n     .format(knn.score(X_train1, y_train1)))\ny1_train.append(knn.score(X_train1, y_train1))\nprint('Accuracy of KNN Regressor on test set: {:.4f}'\n     .format(knn.score(X_test1, y_test1)))\ny1_test.append(knn.score(X_test1, y_test1))\n\nprint('Accuracy of Decision Tree on training set: {:.4f}'\n     .format(tree.score(X_train1, y_train1)))\ny1_train.append(tree.score(X_train1, y_train1))\nprint('Accuracy of Decision Tree on test set: {:.4f}'\n     .format(tree.score(X_test1, y_test1)))\ny1_test.append(tree.score(X_test1, y_test1))\n\nprint('Accuracy of Bagging Regressor on training set: {:.4f}'\n     .format(bagg.score(X_train1, y_train1)))\ny1_train.append(bagg.score(X_train1, y_train1))\nprint('Accuracy of Bagging Regressor on test set: {:.4f}'\n     .format(bagg.score(X_test1, y_test1)))\ny1_test.append(bagg.score(X_test1, y_test1))\n\nprint('Accuracy of XG Boost Regressor on training set: {:.4f}'\n     .format(xgb.score(X_train1, y_train1)))\ny1_train.append(xgb.score(X_train1, y_train1))\nprint('Accuracy of XG Boost Regressor on test set: {:.4f}'\n     .format(xgb.score(X_test1, y_test1)))\ny1_test.append(xgb.score(X_test1, y_test1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Y2 model selection\nBagging Regressor is the best model for Y2"},{"metadata":{"trusted":true},"cell_type":"code","source":"y2_train=[]\ny2_test=[]\nsvr.fit(X_train2, y_train2)\nknn.fit(X_train2, y_train2)\ntree.fit(X_train2, y_train2)\nbagg.fit(X_train2, y_train2)\nxgb.fit(X_train2, y_train2)\nprint('Accuracy of SVRegression on training set: {:.4f}'\n     .format(svr.score(X_train2, y_train2)))\ny2_train.append(svr.score(X_train2, y_train2))\nprint('Accuracy of SVRegression on test set: {:.4f}'\n     .format(svr.score(X_test2, y_test2)))\ny2_test.append(svr.score(X_test2, y_test2))\n\nprint('Accuracy of KNN Regressor on training set: {:.4f}'\n     .format(knn.score(X_train2, y_train2)))\ny2_train.append(knn.score(X_train2, y_train2))\nprint('Accuracy of KNN Regressor on test set: {:.4f}'\n     .format(knn.score(X_test2, y_test2)))\ny2_test.append(knn.score(X_test2, y_test2))\n\nprint('Accuracy of Decision Tree on training set: {:.4f}'\n     .format(tree.score(X_train2, y_train2)))\ny2_train.append(tree.score(X_train2, y_train2))\nprint('Accuracy of Decision Tree on test set: {:.4f}'\n     .format(tree.score(X_test2, y_test2)))\ny2_test.append(tree.score(X_test2, y_test2))\n\nprint('Accuracy of Bagging Regressor on training set: {:.4f}'\n     .format(bagg.score(X_train2, y_train2)))\ny2_train.append(bagg.score(X_train2, y_train2))\nprint('Accuracy of Bagging Regressor on test set: {:.4f}'\n     .format(bagg.score(X_test2, y_test2)))\ny2_test.append(bagg.score(X_test2, y_test2))\n\nprint('Accuracy of XG Boost Regressor on training set: {:.4f}'\n     .format(xgb.score(X_train2, y_train2)))\ny2_train.append(xgb.score(X_train2, y_train2))\nprint('Accuracy of XG Boost Regressor on test set: {:.4f}'\n     .format(xgb.score(X_test2, y_test2)))\ny2_test.append(xgb.score(X_test2, y_test2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Conclusion\n* Bagging Regressor gives both model for Y1 and Y2 quite good accuracy with 99.4% and 98.3% respectively\n* Here I only consider some algorithm that represent a kind of method. There might be an algorithm with higher accuracy that I haven't discover."},{"metadata":{"trusted":true},"cell_type":"code","source":"model=['SVRegression','KNN Regressor','Decision Tree','Bagging Regressor','XG Boost Regressor']\nmod1=pd.DataFrame([model,y1_train,y1_test]).transpose()\nmod1.columns=['model','Train Accuracy','Test Accuracy']\nmod1.set_index('model')\nmod1.sort_values('Test Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=['SVRegression','KNN Regressor','Decision Tree','Bagging Regressor','XG Boost Regressor']\nmod2=pd.DataFrame([model,y2_train,y2_test]).transpose()\nmod2.columns=['model','Train Accuracy','Test Accuracy']\nmod2.set_index('model')\nmod2.sort_values('Test Accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}