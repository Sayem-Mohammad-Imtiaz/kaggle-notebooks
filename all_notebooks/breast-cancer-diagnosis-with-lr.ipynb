{"cells":[{"metadata":{"_uuid":"7a4e673e124e3b3f70e97858a9fbbc9da4f98229"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"173a76ae168cdda827ce331fa73b361b7b403e7e"},"cell_type":"markdown","source":"# Breast Cancer Diagnosis with Logistic Regression\n<br>\n![breast-cancer-word-cloud](https://www.rush.edu/sites/default/files/breast-cancer-word-cloud-og.jpg)\n\n## Contents\n* [Introduction](#0)\n* [Thanks](#99)\n* [Importing Libraries and Dataset](#1)\n* [Data Exploration and Cleaning](#2)\n* [Data Preparing](#3)\n* [Correlation](#31)\n* [Split Test and Train Variables](#4)\n* [Learning](#5)\n    * [Sigmoid Function](#6)\n    * [Loss Function](#7)\n    * [Optimizing Algorithm with Gradient Descent](#8)\n    * [Update Function](#9)\n    * [Predict Method](#10)\n    * [Coding Logistic Regression](#11)\n* [Conclusions](#12)"},{"metadata":{"trusted":true,"_uuid":"c291650b28ec73e8b2edf2786afacf1ade776831"},"cell_type":"markdown","source":"<a id=\"0\"></a> \n## Introduction\n\nI will be exploring [Breast Cancer Wisconsin (Diagnostic) Dataset](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data), and then going to explain the math behind Logistic Regression. Then I will be implementing all these techniques on real dataset step-by-step.  \nInstead of using libraries for learning, I will be coding all the functions and methods from scratch."},{"metadata":{"_uuid":"c0c218b54f6cd7a8ed099331e9b4bd5a443301ec"},"cell_type":"markdown","source":"<a id=\"99\"></a> \n## Thanks\nThis kernel is inspired by [DATAI](https://www.kaggle.com/kanncaa1)'s [kernel](https://www.kaggle.com/kanncaa1/deep-learning-tutorial-for-beginners) and course materials. I would like thank to DATAI Team for all their effort."},{"metadata":{"_uuid":"a65d41ea08cad562d5d88379367a0ebdd958b731"},"cell_type":"markdown","source":"<a id=\"1\"></a> \n## Importing Libraries and Dataset\n\nLet's start by importing necessary libraries:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0dda6b809c8375b1ef187e65575326c87b164e1"},"cell_type":"markdown","source":"Fetch the dataset into pandas dataframe:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6fe9d67a9e5f556d1e79bbef98f4fce8fd096aea"},"cell_type":"markdown","source":"<a id=\"2\"></a> \n## Data Exploration and Cleaning\n\nTo have a basic understanding of the data, we have to use pandas' `info` method."},{"metadata":{"trusted":true,"_uuid":"7069fba58686045b8de0384562664da0029b05d3"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54ebb867e61b3fb22f8b4331b2ca3cf264939fdf"},"cell_type":"markdown","source":"* There are 569 rows and 33 columns in the dataset.\n* `id` is an attribute with integer values, `diagnosis` is an object (string) type variable and the rest of the features are float numbers.\n* Luckily, there is no `NaN` values, excepting `Unnamed: 32` feature. All of the entries have `NaN` values for this column. So it has to be removed from the dataframe.  "},{"metadata":{"trusted":true,"_uuid":"1d7c69546683376b0be339ac7210ba5f71e7a7f9"},"cell_type":"code","source":"data.drop(['Unnamed: 32'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97bad6d4d68ded1660b1ffa641ded09ec6db16fe"},"cell_type":"markdown","source":"Now let's take a glimpse of data with the first five entries, by using `head` method."},{"metadata":{"trusted":true,"_uuid":"3329e3e901d95512aecb4eefee02e884da0579ae"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bceb435f6829719f4bea4221682c4f44137d05e"},"cell_type":"markdown","source":"As we can observe from the table above, `id` is a unique value per individual patient.  \nIt has nothing to do with diagnosis, so we have to eliminate it."},{"metadata":{"trusted":true,"_uuid":"57428ecbb9338983881e541db13b85cb6db67152"},"cell_type":"code","source":"data.drop(['id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47be60f05385094ee616570eee21eb9a32eac298"},"cell_type":"markdown","source":"<a id=\"3\"></a> \n## Data Preparation"},{"metadata":{"trusted":true,"_uuid":"a406c7452e1e9cf218efdd26f8a8f2f5aaff97d5"},"cell_type":"code","source":"data.diagnosis.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aebd041a7a8bbdbf2d92894d39f6081ce38aec2e"},"cell_type":"markdown","source":"`diagnosis` is the label (target) of the dataset.  \n* `B` indicates Benign  \n* `M` indicates Malignant\n\nWhen it comes to label, we do not prefer working with object-type variables. So let's change them:\n* `1` → `M`\n* `0` → `B`"},{"metadata":{"trusted":true,"_uuid":"50511def4f6910a75ad446a6371f3dd92495ad6c"},"cell_type":"code","source":"data.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a5bd0d979df7e235388e0cb9f7fb2b7bfcf749a"},"cell_type":"markdown","source":"Target feature will be held in `y`, and the rest of attributes the will be held in `x`."},{"metadata":{"trusted":true,"_uuid":"f99dde767d4a863509d5fa9878cd832d998ab554"},"cell_type":"code","source":"y = data.diagnosis.values\nx = data.drop(['diagnosis'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14e206d10209f6004e002586282211407630d7f1"},"cell_type":"markdown","source":"Normalizing variables is importing to avoid imbalanced weights during learning. We want all values to be scaled between 0 and 1."},{"metadata":{"trusted":true,"_uuid":"7d90f30339bf7ec344707dc5e3b827b3f565940f"},"cell_type":"code","source":"x = (x - np.min(x) ) / ( np.max(x) - np.min(x) ).values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"486a1852bf79d619dd6ef5b47232306b6ced9e97"},"cell_type":"markdown","source":"<a id=\"31\"></a> \n## Correlation\nLet's check the correlation among features before jumping into *splitting train/test variables* part:"},{"metadata":{"trusted":true,"_uuid":"68e602cefa7b5500d16a01065b71c07cd11b9004"},"cell_type":"code","source":"f, axis = plt.subplots(figsize = (18,18))\nsns.heatmap(data.corr(), annot = False, linewidths = .4, ax = axis)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21fe20a2afb1d895a2187135f89992ffb5791bd2"},"cell_type":"markdown","source":"There are some features that are strongly correlated with `diagnosis`. Some of them are:\n* `concave points_worst`\n* `area_worst`\n* `perimeter_worst`\n* `concave points_mean`  \nand etc."},{"metadata":{"_uuid":"77db9d889270e0e845e2aafe51cc51d8baca896d"},"cell_type":"markdown","source":"<a id=\"4\"></a> \n## Split Test and Train Variables\n\nWe have to split our data to create train and test variables. To do so, we will be using `sklearn`'s `train_test_split method`.  \n\nWe set the`test_size` parameter to `0.2`, so the train values will be randomly 80% of the data.  Let's briefly describe what all four values correspond to:\n* `x_train` : randomly 80% of data with features of `x` (`radius_mean`, `texture_mean`, etc.)\n* `x_test` : randomly 20% (the rest) of data with features of `x`\n* `y_train` : randomly 80% of data with feature of `y` (`diagnosis`, the target feature)\n* `y_test` : randomly 20% (the rest) of data with feature of `y`  \n\nLet's visualize what I mean:\n\n![train_test_split_image](http://i68.tinypic.com/2rf9dev.jpg)"},{"metadata":{"trusted":true,"_uuid":"536aa90caa8c0e7e1194f7d53c7c7c62d3d13814"},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 5)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1cd1705209f2ac3d021188faae24289d9b22360"},"cell_type":"markdown","source":"<a id=\"5\"></a> \n## Learning  \n\nWe will be using Logistic Regression for learning, but by coding everything from scratch instead of using library functions (like `sklearn`, etc).\n\nFirstly, we need the full understanding of the formula.\n\n$$\\large ý = b_0 + f_0w_0 + f_1w_1 + f_2w_2 + ... + f_nw_n$$\n\nHere $\\large ý$ is the predicted value, i.e. output. In our case, it corresponds to label (`diagnosis`).  \n$\\large b_0$ is the bias of the algorithm.  \n$\\large f_c$ correspons to cth feature, starting from 0. (eg: $\\large f_0$ is `radius_mean`)  \n$\\large w_c$ means the weight (coefficient) of the cth feature.  \n\n<br>\nLet's create a method to initialize the weights and bias of the algorithm:"},{"metadata":{"trusted":true,"_uuid":"bc4b56c3ffc788a797d079a963b70a8bc47ae5f6"},"cell_type":"code","source":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"391554ac4e93e0425e8b79622d08c6b212f80a9f"},"cell_type":"markdown","source":"We need the optimized values of bias and weights so that the algorithm will produce the outputs that are closest to real values.\n\nLet's visualize what we want to do.\n\n![learning-image](http://i67.tinypic.com/6i49vk.png)\n\nAfter the summation part, we get the output ( $\\large ý$ ) of our algorithm. We need a **threshold** to check whether the numeric output corresponds to `Malignant` or `Benign`. The **sigmoid function** comes handy here to do so."},{"metadata":{"_uuid":"ce8b32a6b4da9e8d45dd2ebf717ac07617f1073d"},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n### Sigmoid Function\n\nA sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. [Source of definition](https://en.wikipedia.org/wiki/Sigmoid_function)  \nThe formula of the sigmoid function is:\n\n$$\\LARGE f(x)=\\frac{1}{1+e^{-x}}$$\n\nThe figure below is how it looks like:\n\n![sigmoid-function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)\n\nWe will be using sigmoid function as threshold to determine whether output corresponds to 0 or 1. If value is below the threshold (< 0.5) it is going to be considered as 0, otherwise 1.\n\nAfter checking the output, we rearrange the bias and weights and go back to first step (and keep doing it until their optimized values).  \n\nLet's code our sigmoid function:"},{"metadata":{"trusted":true,"_uuid":"25ba9766779900f52f124422c4c7f748bba24ad2"},"cell_type":"code","source":"def sigmoid(z):\n    output = 1/( 1 + np.exp(-z) )\n    return output","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"127ca885936d2898503ad15fb10acd0a5790d4e4"},"cell_type":"markdown","source":"After defining the sigmoid function, all we have to do is passing our prediction ( $ \\large ý$ ) to sigmoid function, and getting the output as 0 or 1. In our case, as we defined before, `0` means Benign and `1` means Malignant.  \n\nGetting the output via sigmoid function is okay, but how can we know how well we predicted it?  \nThe answer is by using **Loss Functions**."},{"metadata":{"_uuid":"1e2dea7e49e2434ff85b28c5b085b931a6fad60c"},"cell_type":"markdown","source":"<a id=\"7\"><br></a>\n### Loss Function\n\nAt its core, a loss function is incredibly simple: it’s a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they’re pretty good, it’ll output a lower one. As you change pieces of your algorithm to try and improve your model, your loss function will tell you if you’re getting anywhere. \n[Source of definition](https://blog.algorithmia.com/introduction-to-loss-functions/)  \n\nLoss function is basicly calculating the difference between predicted value and real value of target feature. But here, we will be using a kind of specialized loss function, which is **log loss (cross entry loss)**.  \nLet's examine the formula of log loss function:  \n\n$$\\LARGE -( y log(ý) + (1-y)log(1-ý) ) $$\n\nAs you can observe from the formula above, if we predict the value as 0, and the real value of label is 0 too (which means we guessed right), then the output of loss function is 0. It is valid for vice versa (both predicted and real values are 1).  \nOtherwise if we cannot guess it right, then the output of the function would be so high.  \n\nAs a conclusion, *lower the output of loss function, better we predicted the label*.\n\nWe measured the error size of one algorithm via loss function.  We have to add all loss function outputs to get the **cost function**."},{"metadata":{"_uuid":"27ae3e4c235b1e3d78497269edfe145a88faaa65"},"cell_type":"markdown","source":"What we did so far (from configuring the algorithm to calculating its cost function output) is called **forward propagation**.  \n\nLet's put all the pieces together and code them as a function:"},{"metadata":{"trusted":true,"_uuid":"745b38df959c5b0c6f9e070947d0d427fb249db6"},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    y_ = np.dot(w.T,x_train) + b # numeric output of regression algorithm\n    y_pre = sigmoid(y_) # binary output of sigmoid function\n    loss = -y_train*np.log(y_pre)-(1-y_train)*np.log(1-y_pre) # output of loss function\n    cost = (np.sum(loss))/x_train.shape[1] # x_train.shape[1]  is for scaling","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94345a58007f52aa6aa62e58371a99c40bf15b72"},"cell_type":"markdown","source":"<a id=\"8\"><br></a>\n### Optimizing Algorithm with Gradient Descent\n\nWe measured how well our algorithms can predict via cost function. Now it is time to optimize algorithm's parameters (weights and bias) so that it can learn from data and make better predictions. It will be handled by **gradient descent** method.  \n\n**What is Gradient Descent?**  \nGradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. [Source of definition](https://en.wikipedia.org/wiki/Gradient_descent)  \n\n**How do we use it?**  \nWe've been through the definition, but what is under the hood?  \nLet's think of a feature's weight ( $\\large w_c$ ), we initialized it with some arbitrary value. But we know that there is an optimal value for it, and values higher or lower than that optimal value increases our cost function, and makes our prediction worse. So we have to find its optimal value iteratively, by using gradient descent method. \n\n$$\\LARGE w := w - \\alpha \\frac{\\partial J(w)}{\\partial w} $$\n\nHere $\\large J(w)$ is the *cost*, we divide its derivative by $\\large w$'s derivative, so we get a numerical result like $\\large 0.02$. And then subtract it from the initial weight, and then update our weight with its new value. \n\nP.S: The $\\large \\alpha$ is the *learning rate*, determines how fast we learn. It shouldn't be too big (jumps over the optimal point) or too small (may never converge).\n\nLet's take a look at the schema to understand what I mean:\n\n![gradiend_descent](https://cdn-images-1.medium.com/max/1600/0*rBQI7uBhBKE8KT-X.png)  \n\nAs you can see from the image above, we are updating our weight iteratively, until it converges to a local minimum. After convergence, the right part of the formula ( $\\large \\frac{\\partial J(w)}{\\partial w}$) will keep producing zero, which means weight is at the optimal point, and not about to change anymore.  \n\nIn fact, we will be using bias in the formula too, and it's gonna look like this:\n\n$$\\LARGE w := w - \\alpha \\frac{\\partial J(w,b)}{\\partial (w,b)} $$  \n\nWhat we did above is called **backward propagation**. Let's implement it with 'forward propagation' in one function:"},{"metadata":{"trusted":true,"_uuid":"ae5768a3897d6b6fab51c69a1ec0dc11599deb44"},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    y_ = np.dot(w.T,x_train) + b # numeric output of regression algorithm\n    y_pre = sigmoid(y_) # binary output of sigmoid function\n    loss = -y_train*np.log(y_pre)-(1-y_train)*np.log(1-y_pre) # output of loss function\n    cost = (np.sum(loss))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_pre-y_train).T)))/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_pre-y_train)/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4baca9891620569689f8a1d05dd395963653a7e2"},"cell_type":"markdown","source":"We may call one forward and one backward propagation as one iteration. It can take our algorithm to somewhere, but obviously it wouldn't be enough. So we have to pick an arbitrary number for iteration ($\\large n$), and update our algorithm $\\large n$ times for learning.  \n\n<a id=\"9\"><br></a>\n### Update Function\nLet's create a function `update` and use this function to call the methods above to train our algorithm, and give some feedback about costs by printing cost values and visualizing them."},{"metadata":{"trusted":true,"_uuid":"2257fc2d4e30a4a978fe65996dab1b4e549d24be"},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c19342c4160241c5402239edb40d8b7478b70a9f"},"cell_type":"markdown","source":"<a id=\"10\"><br></a>\n### Predict Method\nAnd it is time to code the `predict` method to create an interface between us and our algorithm."},{"metadata":{"trusted":true,"_uuid":"9eeddd44e9370f42274cdf484ecab9a847c83593"},"cell_type":"code","source":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95b5ed8cd1b4fc556b8272f246ae793cdcc04580"},"cell_type":"markdown","source":"<a id=\"11\"><br></a>\n### Coding Logistic Regression\nAnd finally let's put all these things together, i.e, code our custom logistic regression function!"},{"metadata":{"trusted":true,"_uuid":"c130ca1ad9823f96d603de037033b7bfc783c643"},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    \n    dimension =  x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 100) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bccd3d455917603e9e8914d9db9ac5b735252c3c"},"cell_type":"markdown","source":"<a id=\"12\"><br></a>\n## Conclusions\n* Logistic Regression is a good way to classify binary labels, concluded from a lot of numerical features.\n* Cost gets smaller as the iteration of update (forward and backward propagation) increases.\n* We got an accuracy of `~93.86` on the `test` dataset."},{"metadata":{"_uuid":"fa7d15b649fb116236a70da48e23be203ea98902"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}