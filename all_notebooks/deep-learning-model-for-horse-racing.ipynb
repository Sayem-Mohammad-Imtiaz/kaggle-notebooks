{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Classification with Deep Learning for Hong Kong Horse Racing\nFirst of all, I'd like to thank Graham for providing such an interesting dataset. It's really fun for me to explore the data that is related to my living place, Hong Kong.\n\nI think horse racing is predictable given all the feature parameters. It's different from Mark-6, which is totally random. I am going to train a Deep Neural Network to predict whether a horse can win a race. Deep Neural Network is able to learn any function within the massive data, which is not possible for a human to calculate manually or even with an excel.\n\nLet's see what we can find.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import packages\nVery common packages to be used, e.g. pandas, numpy, tensorflow, sklearn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport sklearn.preprocessing as preprocessing\nimport sklearn.model_selection as model_selection \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing - read inputs\nHere, I am going to select some features that I think useful. I will also join runs.csv and races.csv because they are related and each includes some features for the classification.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"races = pd.read_csv(r\"../input/hkracing/races.csv\", delimiter=\",\", header=0, index_col='race_id')\nraces_data = races[['venue', 'race_no', 'config', 'surface', 'distance', 'going', 'horse_ratings', 'race_class']]\nruns = pd.read_csv(r\"../input/hkracing/runs.csv\", delimiter=\",\", header=0)\nruns_data = runs[['race_id', 'result', 'won', 'horse_age', 'horse_country', 'horse_type', 'horse_rating',\n                  'declared_weight', 'actual_weight', 'draw', 'win_odds', 'trainer_id', 'jockey_id']]\ndata = runs_data.join(races_data, on='race_id')\n# drop race_id after join because it's not a feature\ndata = data.drop(columns=['race_id'])\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing - encoding\n- Deal with missing values, I simply dropped them because it's not much, just 2 rows was dropped.\n- Encode ordinal columns: config, going, horse_ratings.\n- Encode nominal columns: horse_country, horse_type, venue, trainer_id, jorkey_id.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove rows with NaN\nprint('data shape before drop NaN rows', data.shape)\ndata.dropna(inplace=True)\ndata.reset_index(drop=True, inplace=True)\nprint('data shape after drop NaN rows', data.shape)\n\n# encode ordinal columns\nencoder = preprocessing.OrdinalEncoder()\ndata['config'] = encoder.fit_transform(data['config'].values.reshape(-1, 1))\ndata['going'] = encoder.fit_transform(data['going'].values.reshape(-1, 1))\ndata['horse_ratings'] = encoder.fit_transform(data['horse_ratings'].values.reshape(-1, 1))\n\n# encode nominal columns\nhorse_countries = sorted(data['horse_country'].unique())\nhorse_types = sorted(data['horse_type'].unique())\ntrainer_ids = sorted(data['trainer_id'].unique())\njockey_ids = sorted(data['jockey_id'].unique())\nvenues = sorted(data['venue'].unique())\nonehot = preprocessing.OneHotEncoder(dtype=np.int, sparse=True)\nnominal_columns = ['horse_country', 'horse_type', 'venue', 'trainer_id', 'jockey_id']\nnominals = pd.DataFrame(onehot.fit_transform(data[nominal_columns]).toarray(),\n                        columns=np.concatenate((horse_countries, horse_types, venues, trainer_ids, jockey_ids)))\ndata = data.drop(columns=nominal_columns)\n\ndata = pd.concat([data, nominals], axis=1)\nprint('numberic data frame', data.shape)\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocessing - prepare train/test data\n- prepare X and y\n- standardization\n- split to train/test sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# result and won are outputs, the rest are inputs\nX = data.drop(columns=['result', 'won'])\ny = data['won']\n#y = pd.DataFrame(np.where(data['result'] <= 3, 1, 0), columns=['top_3'])\n\n# standardize the inputs to similar scale\nss = preprocessing.StandardScaler()\nX = pd.DataFrame(ss.fit_transform(X),columns = X.columns)\nprint(X.head())\n\n# split data into train and test sets\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=1)\nprint('X_train', X_train.shape)\nprint('y_train', y_train.shape)\nprint('X_test', X_test.shape)\nprint('y_test', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the NN model\n- 3 hidden layers\n- output layer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(120, activation='relu', input_shape=(402,)),\n    tf.keras.layers.Dense(80, activation='relu'),\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n              loss='binary_crossentropy',\n              metrics=[tf.keras.metrics.Precision(name='precision')])\n\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\ntrain_dataset = dataset.shuffle(len(X_train)).batch(1000)\ndataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\nvalidation_dataset = dataset.shuffle(len(X_test)).batch(1000)\n\nprint(\"Start training..\\n\")\nhistory = model.fit(train_dataset, epochs=200, validation_data=validation_dataset)\nprint(\"Done.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = history.history['precision']\nval_precision = history.history['val_precision']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(precision) + 1)\n\nplt.plot(epochs, precision, 'b', label='Training precision')\nplt.plot(epochs, val_precision, 'r', label='Validation precision')\nplt.title('Training and validation precision')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\n\nWith the 4-layer Deep Neural Network, training looked normal. Training loss decreased all the way, while validation loss decreased first then increased, which meant **overfitting** happened. \n\nI chose `precision` as an evaluation parameter because it's our interest to bet on win.\n\n$ precision = \\frac{TP}{TP + FP} $\n\nIf `precision = 0.2`, it mean we bet for winning horse 10 times and only 2 times is correct.\n\nAt the end, we could reach validation precision around 0.26. I am not sure `precision = 0.26` will let us win the market or not. However, the network generalizes poorly from Machine Learning point of view. \n\nI'd like to hear from you.\n\nThank you.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}