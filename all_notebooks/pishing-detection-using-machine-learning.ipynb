{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', None)\nplt.rcParams['figure.figsize'] = (12,6)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:09.811255Z","iopub.execute_input":"2021-07-05T08:42:09.811625Z","iopub.status.idle":"2021-07-05T08:42:10.496401Z","shell.execute_reply.started":"2021-07-05T08:42:09.811541Z","shell.execute_reply":"2021-07-05T08:42:10.495562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting Phishing Web Page Using Machine Learning\n\nPhishing is a method of trying to gather personal information using deceptive e-mails and websites.\n\nIn this notebook, we will read the data and look at what are the features that can give us information on what are the attributes of a phishing website","metadata":{}},{"cell_type":"markdown","source":"# Loading the data\n\nWe will start by loading the provided csv data using pandas read_csv method","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/phishing-dataset-for-machine-learning/Phishing_Legitimate_full.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:10.497842Z","iopub.execute_input":"2021-07-05T08:42:10.49821Z","iopub.status.idle":"2021-07-05T08:42:10.566797Z","shell.execute_reply.started":"2021-07-05T08:42:10.498174Z","shell.execute_reply":"2021-07-05T08:42:10.566049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert data \n\nIn this phase we will convert float64 and int64 data to type 32, by doing this we can save the memory usage and we can prepare the data for using with CuML later for training purpose\n\nAs we can see the data has 10k rows and 50 columns including labels","metadata":{}},{"cell_type":"code","source":"float_cols = data.select_dtypes('float64').columns\nfor c in float_cols:\n    data[c] = data[c].astype('float32')\n    \nint_cols = data.select_dtypes('int64').columns\nfor c in int_cols:\n    data[c] = data[c].astype('int32')\n    \ndata.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:10.56867Z","iopub.execute_input":"2021-07-05T08:42:10.569031Z","iopub.status.idle":"2021-07-05T08:42:10.631106Z","shell.execute_reply.started":"2021-07-05T08:42:10.568995Z","shell.execute_reply":"2021-07-05T08:42:10.630064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.rename(columns={'CLASS_LABEL': 'labels'}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:10.632626Z","iopub.execute_input":"2021-07-05T08:42:10.632978Z","iopub.status.idle":"2021-07-05T08:42:10.637929Z","shell.execute_reply.started":"2021-07-05T08:42:10.632942Z","shell.execute_reply":"2021-07-05T08:42:10.636698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# View the data\n\nLets look at random 5 rows from the dataset, it seems like we have mix of ranged data, some column have smaller range compared to others","metadata":{}},{"cell_type":"code","source":"data.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:10.639518Z","iopub.execute_input":"2021-07-05T08:42:10.639901Z","iopub.status.idle":"2021-07-05T08:42:10.676697Z","shell.execute_reply.started":"2021-07-05T08:42:10.639862Z","shell.execute_reply":"2021-07-05T08:42:10.67568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary Statistics\n\nBy using the describe method, we can see some of the columns have high variance and some have smaller variance, this is due to the fact that some of the column have bigger values and bigger ranges","metadata":{}},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:10.677903Z","iopub.execute_input":"2021-07-05T08:42:10.678238Z","iopub.status.idle":"2021-07-05T08:42:10.822397Z","shell.execute_reply.started":"2021-07-05T08:42:10.678205Z","shell.execute_reply":"2021-07-05T08:42:10.821476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Balance/Imbalanced Data\n\nThe data is balanced for non-phising and phising label","metadata":{}},{"cell_type":"code","source":"data['labels'].value_counts().plot(kind='bar')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:10.823799Z","iopub.execute_input":"2021-07-05T08:42:10.824129Z","iopub.status.idle":"2021-07-05T08:42:10.962383Z","shell.execute_reply.started":"2021-07-05T08:42:10.824094Z","shell.execute_reply":"2021-07-05T08:42:10.961456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Spearman Correlation\n\nBy looking the spearman correlation, we can find which features are linearly correlated in terms of predicting if a site is phising or not","metadata":{}},{"cell_type":"code","source":"def corr_heatmap(data, idx_s, idx_e):\n    y = data['labels']\n    temp = data.iloc[:, idx_s:idx_e]\n    if 'id' in temp.columns:\n        del temp['id']\n    temp['labels'] = y\n    sns.heatmap(temp.corr(), annot=True, fmt='.2f')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:10.965058Z","iopub.execute_input":"2021-07-05T08:42:10.965378Z","iopub.status.idle":"2021-07-05T08:42:10.970404Z","shell.execute_reply.started":"2021-07-05T08:42:10.965352Z","shell.execute_reply":"2021-07-05T08:42:10.969479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First 10 columns\n\nBy looking at the first 10 columns against labels, we can concluded that non of the features have strong correlation with the labels, however, NumDash has some significant negative effect towards the labels, which could mean if there is less number of dash then it is more likely to be phising site","metadata":{}},{"cell_type":"code","source":"corr_heatmap(data, 0, 10)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:10.972673Z","iopub.execute_input":"2021-07-05T08:42:10.973101Z","iopub.status.idle":"2021-07-05T08:42:11.908992Z","shell.execute_reply.started":"2021-07-05T08:42:10.973064Z","shell.execute_reply":"2021-07-05T08:42:11.90683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Columns 10 to 20\n\nThere are no strong or even medium level strength correlation features with labels","metadata":{}},{"cell_type":"code","source":"corr_heatmap(data, 10, 20)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:11.91008Z","iopub.execute_input":"2021-07-05T08:42:11.910387Z","iopub.status.idle":"2021-07-05T08:42:12.825238Z","shell.execute_reply.started":"2021-07-05T08:42:11.910354Z","shell.execute_reply":"2021-07-05T08:42:12.824419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Columns 20 to 30\n\nStill no strong correlation feature","metadata":{}},{"cell_type":"code","source":"corr_heatmap(data, 20, 30)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:12.826445Z","iopub.execute_input":"2021-07-05T08:42:12.826941Z","iopub.status.idle":"2021-07-05T08:42:13.502165Z","shell.execute_reply.started":"2021-07-05T08:42:12.826901Z","shell.execute_reply":"2021-07-05T08:42:13.501375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Columns 30 to 40\n\nWell here we have a few features that are linearly correlated to our dep variable\n\n* InsecureForms shows that as the value is higher so the probability of being a phising site\n* PctNullSelfRedirectHyperlinks shows the same positive correlation as InsecureForms\n* FequentDomainNameMismatch shows that it has medium linear correlation in positive direction\n* SubmitInfoToEmail seems to indicate that sites that ask users to submit their details to emails seems to be more high probability for phising","metadata":{}},{"cell_type":"code","source":"corr_heatmap(data, 30, 40)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:13.503411Z","iopub.execute_input":"2021-07-05T08:42:13.503777Z","iopub.status.idle":"2021-07-05T08:42:14.262504Z","shell.execute_reply.started":"2021-07-05T08:42:13.50374Z","shell.execute_reply":"2021-07-05T08:42:14.261683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Columsn 40 to 50\n\nThe only column in this group that has some correlation with labels is PctExtNullSelfRedirectHyperlinksRT and it has negative effect towards labels which could mean that when the number of percent of null self redirect hyperlinks occur hence the probabiliy of phising increases","metadata":{}},{"cell_type":"code","source":"corr_heatmap(data, 40, 50)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:14.26385Z","iopub.execute_input":"2021-07-05T08:42:14.264386Z","iopub.status.idle":"2021-07-05T08:42:15.061619Z","shell.execute_reply.started":"2021-07-05T08:42:14.264348Z","shell.execute_reply":"2021-07-05T08:42:15.060683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mutual Info\n\nWe will use mutual info classifier to find non linear and linear correlation betweem the features and labels","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:15.063027Z","iopub.execute_input":"2021-07-05T08:42:15.063413Z","iopub.status.idle":"2021-07-05T08:42:15.346597Z","shell.execute_reply.started":"2021-07-05T08:42:15.063375Z","shell.execute_reply":"2021-07-05T08:42:15.345772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data.drop(['id', 'labels'], axis=1)\ny = data['labels']","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:15.347863Z","iopub.execute_input":"2021-07-05T08:42:15.348225Z","iopub.status.idle":"2021-07-05T08:42:15.355132Z","shell.execute_reply.started":"2021-07-05T08:42:15.348188Z","shell.execute_reply":"2021-07-05T08:42:15.354193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discrete_features = X.dtypes == int","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:15.356476Z","iopub.execute_input":"2021-07-05T08:42:15.356835Z","iopub.status.idle":"2021-07-05T08:42:15.364824Z","shell.execute_reply.started":"2021-07-05T08:42:15.356799Z","shell.execute_reply":"2021-07-05T08:42:15.364099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we process the scores and we can see that now mutual info is showing a bit different list from spearman corr","metadata":{}},{"cell_type":"code","source":"mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\nmi_scores = pd.Series(mi_scores, name='MI Scores', index=X.columns)\nmi_scores = mi_scores.sort_values(ascending=False)\nmi_scores","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:15.366143Z","iopub.execute_input":"2021-07-05T08:42:15.366485Z","iopub.status.idle":"2021-07-05T08:42:17.684957Z","shell.execute_reply.started":"2021-07-05T08:42:15.366452Z","shell.execute_reply":"2021-07-05T08:42:17.684076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"MI Scores\")\n    \nplt.figure(dpi=100, figsize=(12,12))\nplot_mi_scores(mi_scores)","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:17.686453Z","iopub.execute_input":"2021-07-05T08:42:17.686842Z","iopub.status.idle":"2021-07-05T08:42:18.377569Z","shell.execute_reply.started":"2021-07-05T08:42:17.686807Z","shell.execute_reply":"2021-07-05T08:42:18.376771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction\n\nWe will first use logistic regression as for baseline, then try to beat the baseline using random forest classifer\n\nOur evaluation metrics will be accuracy, precision, recall and f1 score\n\nBelow we import all the required modules","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom cuml.ensemble import RandomForestClassifier as cuRfc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:18.378909Z","iopub.execute_input":"2021-07-05T08:42:18.379263Z","iopub.status.idle":"2021-07-05T08:42:21.561316Z","shell.execute_reply.started":"2021-07-05T08:42:18.379225Z","shell.execute_reply":"2021-07-05T08:42:21.560487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train logistic models\n\nThis method is to perform a repetative training process using logistic regression model, the purpose for this is to find the optimal number of features that can be used to find the best fitted model without adjusting much of the hyperparameters, hence the idea here is to go with Data-Centric training, basically the method takes number of top N features to be used for training the model and all the evaluation metrics are returned for evaluation purpose","metadata":{}},{"cell_type":"code","source":"def train_logistic(data, top_n):\n    top_n_features = mi_scores.sort_values(ascending=False).head(top_n).index.tolist()\n    X = data[top_n_features]\n    y = data['labels']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n    \n    lr = LogisticRegression(max_iter=10000)\n    lr.fit(X_train, y_train)\n    \n    y_pred = lr.predict(X_test)\n    \n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return precision, recall, f1, accuracy","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:21.562665Z","iopub.execute_input":"2021-07-05T08:42:21.563027Z","iopub.status.idle":"2021-07-05T08:42:21.571842Z","shell.execute_reply.started":"2021-07-05T08:42:21.562987Z","shell.execute_reply":"2021-07-05T08:42:21.569814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here the loop will be starting from 20 as we will start training with top 20 features up to all 50 features to find optimal number of features needed for this problem","metadata":{}},{"cell_type":"code","source":"arr = []\nfor i in range(20,51,1):\n    precision, recall, f1, accuracy = train_logistic(data, i)\n    print(\"Performance for Logistic Model with Top {} features is precision : {}, recall : {}, f1 score : {}, accuracy : {}\".format(i, precision, recall, f1, accuracy))\n    arr.append([i, precision, recall, f1, accuracy])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:42:21.573296Z","iopub.execute_input":"2021-07-05T08:42:21.573646Z","iopub.status.idle":"2021-07-05T08:43:43.91169Z","shell.execute_reply.started":"2021-07-05T08:42:21.5736Z","shell.execute_reply":"2021-07-05T08:43:43.910762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(arr, columns=['num_of_features', 'precision', 'recall', 'f1_score', 'accuracy'])\ndf","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:43:43.913216Z","iopub.execute_input":"2021-07-05T08:43:43.913788Z","iopub.status.idle":"2021-07-05T08:43:43.936083Z","shell.execute_reply.started":"2021-07-05T08:43:43.91375Z","shell.execute_reply":"2021-07-05T08:43:43.935195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Logistic Reg Performance\n\nAs we can see, the model had ups and downs during the training as more number of features were added, as our target is to maximize all the metrics we have to find the number of features that gives us the best of all metrics, from the figure below, we can see that recall is constantly performing good but our model tend to have problem with precision score, hence to choose the best N of features, we have to pick the area where all the metrics are performing and based on the figure I would say its around 39 features","metadata":{}},{"cell_type":"code","source":"sns.lineplot(x='num_of_features', y='precision', data=df, label='Precision Score')\nsns.lineplot(x='num_of_features', y='recall', data=df, label='Recall Score')\nsns.lineplot(x='num_of_features', y='f1_score', data=df, label='F1 Score')\nsns.lineplot(x='num_of_features', y='accuracy', data=df, label='Acc Score')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:43:43.939723Z","iopub.execute_input":"2021-07-05T08:43:43.940386Z","iopub.status.idle":"2021-07-05T08:43:44.223043Z","shell.execute_reply.started":"2021-07-05T08:43:43.940346Z","shell.execute_reply":"2021-07-05T08:43:44.222255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Random Forest Classifier on GPU\n\nIt is the same method as logistic reg, the only diff is that we are now using random forest classifier for training and trying to beat the logistic baseline","metadata":{}},{"cell_type":"code","source":"def train_rfc(data, top_n):\n    top_n_features = mi_scores.sort_values(ascending=False).head(top_n).index.tolist()\n    X = data[top_n_features]\n    y = data['labels']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n    \n    rfc = cuRfc(n_estimators=500, \n                split_criterion=1,  \n                max_depth=32, \n                max_leaves=-1,\n                max_features=1.0,\n                n_bins=128)\n    \n    rfc.fit(X_train, y_train)\n    \n    y_pred = rfc.predict(X_test, predict_model='CPU')\n    \n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    return precision, recall, f1, accuracy","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:43:44.224929Z","iopub.execute_input":"2021-07-05T08:43:44.225283Z","iopub.status.idle":"2021-07-05T08:43:44.233138Z","shell.execute_reply.started":"2021-07-05T08:43:44.225247Z","shell.execute_reply":"2021-07-05T08:43:44.232172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr = []\nfor i in range(20,51,1):\n    precision, recall, f1, accuracy = train_rfc(data, i)\n    print(\"Performance for RFC Model with Top {} features is precision : {}, recall : {}, f1 score : {}, accuracy : {}\".format(i, precision, recall, f1, accuracy))\n    arr.append([i, precision, recall, f1, accuracy])","metadata":{"execution":{"iopub.status.busy":"2021-07-05T08:43:44.234738Z","iopub.execute_input":"2021-07-05T08:43:44.235079Z","iopub.status.idle":"2021-07-05T09:33:07.835593Z","shell.execute_reply.started":"2021-07-05T08:43:44.235044Z","shell.execute_reply":"2021-07-05T09:33:07.834768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(arr, columns=['num_of_features', 'precision', 'recall', 'f1_score', 'accuracy'])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:33:07.837066Z","iopub.execute_input":"2021-07-05T09:33:07.837429Z","iopub.status.idle":"2021-07-05T09:33:07.852151Z","shell.execute_reply.started":"2021-07-05T09:33:07.837387Z","shell.execute_reply":"2021-07-05T09:33:07.85128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Random Forest Performance\n\nOur goal is to beat logistic regression baseline which is\n\n* accuracy = 0.947162\n* precision = 0.957468\n* recall = 0.952287\n* f1_score = 0.9515\n\nSo by visualizing the figure below, we can conclude that the best number of features for this model would be 32, one less than logistic regression, the reason why I chose 32 is because that is the number of features that allowed the model to perform the best across all the evaluation metric","metadata":{}},{"cell_type":"code","source":"sns.lineplot(x='num_of_features', y='precision', data=df, label='Precision Score')\nsns.lineplot(x='num_of_features', y='recall', data=df, label='Recall Score')\nsns.lineplot(x='num_of_features', y='f1_score', data=df, label='F1 Score')\nsns.lineplot(x='num_of_features', y='accuracy', data=df, label='Acc Score')","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:33:07.853445Z","iopub.execute_input":"2021-07-05T09:33:07.854049Z","iopub.status.idle":"2021-07-05T09:33:08.10344Z","shell.execute_reply.started":"2021-07-05T09:33:07.854009Z","shell.execute_reply":"2021-07-05T09:33:08.102674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final Random Forest Model\n\nLets train the final random forest model based on the optimal N number of features","metadata":{}},{"cell_type":"code","source":"top_n_features = mi_scores.sort_values(ascending=False).head(32).index.tolist()\nX = data[top_n_features]\ny = data['labels']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n\nrfc = cuRfc(n_estimators=500, \n            split_criterion=1,  \n            max_depth=32, \n            max_leaves=-1,\n            max_features=1.0,\n            n_bins=128)\n\nrfc.fit(X_train, y_train)\n\ny_pred = rfc.predict(X_test, predict_model='CPU')\n\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"Performance for RFC Model with Top {} features is precision : {}, recall : {}, f1 score : {}, accuracy : {}\".format(27, precision, recall, f1, accuracy))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:34:06.230252Z","iopub.execute_input":"2021-07-05T09:34:06.230571Z","iopub.status.idle":"2021-07-05T09:35:33.294599Z","shell.execute_reply.started":"2021-07-05T09:34:06.230541Z","shell.execute_reply":"2021-07-05T09:35:33.293732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance\n\nThe model is now capable of predicting at up to 98% accuracy and also precision and recall, this shows the model has high confidence in predicting phishing or non-phishing site","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-07-05T09:35:33.296001Z","iopub.execute_input":"2021-07-05T09:35:33.296506Z","iopub.status.idle":"2021-07-05T09:35:33.308756Z","shell.execute_reply.started":"2021-07-05T09:35:33.296466Z","shell.execute_reply":"2021-07-05T09:35:33.307814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}