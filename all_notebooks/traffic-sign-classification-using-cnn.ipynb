{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Intro","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The German Traffic Sign Benchmark is a multi-class, single-image classification challenge held at the International Joint Conference on Neural Networks (IJCNN) 2011. We cordially invite researchers from relevant fields to participate: The competition is designed to allow for participation without special domain knowledge. Our benchmark has the following properties:\n\n1. Single-image, \n2. Multi-class classification problem\n3. More than 40 classes\n4. More than 50,000 images in total\n5. Large, lifelike database","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Labels Overview","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.read_csv('../input/traffic-signs-classification/labels.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#   ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Collection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_path = '../input/gtsrb-german-traffic-sign'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(dir_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assigning the path for train and test images\n\ntrain_path = dir_path +'/Train'\ntest_path = dir_path + '/Test'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sorted(os.listdir(train_path)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"sorted(os.listdir(test_path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##   ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Visualization","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing 25 random sample images from test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nimages_path = os.listdir(test_path)\n\nplt.figure(figsize=(25,25))\n\n\nfor i in range(1,26):\n    \n    plt.subplot(5,5,i)\n    random_img_path = test_path +'/'+ random.choice(images_path)\n    rand_img = imread(random_img_path)\n    plt.imshow(rand_img)\n    plt.xlabel(rand_img.shape[1], fontsize = 20)#width of image\n    plt.ylabel(rand_img.shape[0], fontsize = 20)#height of image\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dimensions of the images are not fixed. \n\n#### Note:\nConvolutional neural networks cannot perform on images that have various dimensions.\nWe will resize these images during our model building.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"But first find the mean of the dimensions of all the images in training set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dim1 = []\ndim2 = []\n\nfor i in range(0,43):\n    labels = train_path + '/{0}'.format(i)\n    image_path = os.listdir(labels)\n    for x in image_path:\n        img = imread(labels + '/' + x)\n        dim1.append(img.shape[0])\n        dim2.append(img.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Exploring the dimensions with a jointplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(dim1,dim2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(dim1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(dim2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the mean of both dimensions is around 50 , we will use (50x50) as the shape of images.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_shape = (50,50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Importing the images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\n\nimages = []\nlabel_id = []\n\nfor i in range(43):\n    labels = train_path + '/{0}'.format(i)\n    image_path = os.listdir(labels)\n    for x in image_path:\n        img = Image.open(labels + '/' + x)\n        img = img.resize(image_shape)\n        img = np.array(img)\n        images.append(img)\n        label_id.append(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scaling the images so that the values remain between 0 and 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting images into numpy array\nimages = np.array(images)\n#The pixel value of each image ranges between 0 and 255\n#Dividing each image by 255 will scale the values between 0 and 1. This is also known as normalization.\nimages = images/255 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_id = np.array(label_id)\nlabel_id.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\nsns.countplot(label_id)\nplt.title('Distribution of images among different classes', fontsize = 15)\nplt.xlabel('Label_id', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saving the scaled images and labels for future use\nnp.save('Training_set', images)\nnp.save('Label_Id', label_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the train data into train and validation data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = np.load('Training_set.npy')\nlabel_id = np.load('Label_Id.npy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the data\nfrom sklearn.model_selection import  train_test_split\nx_train, x_val, y_train, y_val = train_test_split(images, label_id , test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Changing target labels to categorical  using one-hot encoding technique","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#keras has a built-in function for one-hot encoding.\nfrom tensorflow.keras.utils import to_categorical\n\ny_train_cat = to_categorical(y_train)\n\ny_val_cat = to_categorical(y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#    ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Model Building","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPool2D","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\n#1st layer\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), input_shape = x_train.shape[1:], activation = 'relu', padding = 'same'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.5))\n\n#2nd layer\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.5))\n\n#3rd layer\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\n\n#Dense layer\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.5))\n\n#Output layer\nmodel.add(Dense(43, activation = 'softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 2)\n\nmodel.fit(\n    \n    x_train, y_train,\n    epochs = 25,\n    batch_size = 64,\n    validation_data = (x_val, y_val),\n    callbacks = [early_stopping],\n    verbose = 2\n\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Achieved highest accuracy of 99.50% on validation data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saving the model\nmodel.save('Model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation = pd.DataFrame(model.history.history)\n\nevaluation[['accuracy', 'val_accuracy']].plot()\nevaluation[['loss', 'val_loss']].plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####   ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Testing on test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model\nmodel = load_model('Model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note: The test images folder in the original dataset has a blank csv file which cannot be opened with the above function. So i copied that folder and deleted that csv file and uploaded the test images again seperately. These test images are same as the test images in the original dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_path = '../input/test-images/Test'\ntest_img = sorted(os.listdir(test_path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### For some unknown reason , the images in kaggle kernel is not showing in the order they are in the Test folder. Upon inspection it is seen that the images are in sorted order. So using sorted() function to sort them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining a function that will scale images\nfrom PIL import Image\n\ndef scaling(test_images, test_path):\n    images = []\n\n    image_path = test_images\n    \n    for x in image_path:\n        img = Image.open(test_path + '/' + x)\n        img = img.resize((50,50))\n        img = np.array(img)\n        images.append(img)\n\n    #Converting images into numpy array\n    images = np.array(images)\n    #The pixel value of each image ranges between 0 and 255\n    #Dividing each image by 255 will scale the values between 0 and 1. This is also known as normalization.\n    images = images/255\n\n    return images","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above function can be used to scale any new traffic-sign images that can be predicted with our model. This is a general purpose function for code reusability.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images = scaling(test_img,test_path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test labels","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/gtsrb-german-traffic-sign/Test.csv')\n\ny_test = test['ClassId'].values\n\ny_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Testing on test images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict_classes(test_images)\n\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We achieved an overall accuracy of 97% on our model. This is pretty good and we can use this model for predicting some other Traffic signs as well in future.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}