{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw = pd.read_csv('../input/news-summary/news_summary_more.csv', encoding='iso-8859-1')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = raw.iloc[0,1]\ntext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport json \nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained('t5-small')\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\ndevice = torch.device('cpu')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocess_text = text.strip().replace(\"\\n\",\"\")\nt5_prepared_Text = \"summarize: \"+preprocess_text\nprint (\"original text preprocessed: \\n\", preprocess_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability.**\n\nTo know more about it visit this link - https://huggingface.co/blog/how-to-generate","metadata":{}},{"cell_type":"code","source":"summary_ids = model.generate(tokenized_text,\n                                    num_beams=14,\n                                    no_repeat_ngram_size=2,\n                                    min_length=30,\n                                    max_length=100,\n                                    early_stopping=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Summarized text: \\n\",output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary = pd.read_csv('../input/news-summary/news_summary.csv', encoding='iso-8859-1')\nsummary.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = summary.iloc[0,5]\ntext","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing the text**","metadata":{}},{"cell_type":"code","source":"preprocess_text = text\nt5_prepared_Text = \"summarize: \"+preprocess_text\nprint (\"original text preprocessed: \\n\", preprocess_text)\nprint (\"original length: \\n\", len(preprocess_text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generating the model**","metadata":{}},{"cell_type":"code","source":"summary_ids = model.generate(tokenized_text,\n                                    num_beams=10,\n                                    no_repeat_ngram_size=3,\n                                    min_length=100,\n                                    max_length=250,\n                                    early_stopping=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Summary of the above text**","metadata":{}},{"cell_type":"code","source":"output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (\"Summary length: \\n\", len(output))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}