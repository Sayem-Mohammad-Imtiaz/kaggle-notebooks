{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* Image denoising is to remove noise from a noisy image, so as to restore the true image\n* In this notebook FER2013 dataset is used which contains approx 35 thousand images of 7 different emotions\n* Image is grayscale of size 48*48","metadata":{}},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"from keras.datasets import fashion_mnist, mnist\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout\nfrom keras.models import Model\n\nimport os,cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20, 10\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd# Any results you write to the current directory are saved as output.\nfrom IPython.display import display, Image\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout\n\n# Any results you write to the current directory are saved as output.\nfrom IPython.display import display, Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract data from CSV","metadata":{}},{"cell_type":"code","source":"# get the data\nfilname = '../input/facial-expression/fer2013/fer2013.csv'\n\n#different labels of images(not useful known about for current problem)\nlabel_map = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n\n#different features names\nnames=['emotion','pixels','usage']\n\n#Reading data in dataframe\ndf=pd.read_csv('../input/facial-expression/fer2013/fer2013.csv',names=names, na_filter=False)\nim=df['pixels']\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding labels and images(pixel values) in respective array","metadata":{}},{"cell_type":"code","source":"#reading data and labels from dataset and appending in list\n\ndef getData(filname):\n    # images are 48x48\n    # N = 35887\n    Y = []\n    X = []\n    first = True\n    for line in open(filname):\n        if first:\n            first = False\n        else:\n            row = line.split(',')\n            Y.append(int(row[0]))\n            X.append([int(p) for p in row[1].split()])\n\n    X, Y = np.array(X), np.array(Y)\n    return X, Y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#extracting data from dataset\nX, Y = getData(filname)\nnum_class = len(set(Y))\n#print(num_class)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reshaping images","metadata":{}},{"cell_type":"code","source":"# keras with tensorflow backend\nN, D = X.shape\n\n#reshaping the dataset\nX = X.reshape(N, 48, 48, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting Data and splitting train and test ","metadata":{}},{"cell_type":"code","source":"#splitting data in train, test\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"# noise factor: 0.05","metadata":{}},{"cell_type":"code","source":"#NOrmalizing the images\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\n\n#reshaping the images\nx_train = np.reshape(x_train, (len(x_train), 48, 48, 1))  # adapt this if using `channels_first` image data format\nx_test = np.reshape(x_test, (len(x_test), 48, 48, 1))  # adapt this if using `channels_first` image data format\n\n\n#adding noise in data\nnoise_factor = 0.05\n\nfrom skimage.util import random_noise\n\n#noisy = random_noise(img, mode=\"poisson\")\n#just change the mode pf the noise to-->'gaussain', \nx_train_noisy = random_noise(x_train, mode=\"s&p\",clip=True, amount=noise_factor)\nx_test_noisy = random_noise(x_test, mode=\"s&p\",clip=True, amount=noise_factor)\n\n\n#x_train_noisy = x_train + noise_factor * np.random.poisson(lam=(0,1), size=x_train.shape) \n#x_test_noisy = x_test + noise_factor * np.random.poisson(lam=(0,1), size=x_test.shape) \n\n#clipping put data near to 0--->0 aand data near to 1-->1(eg=0.3-->0 or 0.7-->1)\n#x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n#x_test_noisy = np.clip(x_test_noisy, 0., 1.)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Taking 100 images seperate for final testing","metadata":{}},{"cell_type":"code","source":"x_test_final_noisy = x_test_noisy[-100:]\nx_test_noisy = x_test_noisy[:-100]\n\nx_test_final_original = x_test[-100:]\nx_test = x_test[:-100]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization of 10 Data","metadata":{}},{"cell_type":"code","source":"n = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(48, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_train_noisy[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# One of the way we can achieve our goal of removing noise is AutoEncoder\n\n**Copied from Keras Blog(https://blog.keras.io/building-autoencoders-in-keras.html):\n**\n* What are autoencoders good for?\n* Today two interesting practical applications of autoencoders are data denoising (which we feature later in this post), and dimensionality reduction for data visualization. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.","metadata":{}},{"cell_type":"markdown","source":"**Refer to Keras Blog for better idea : https://blog.keras.io/building-autoencoders-in-keras.html**","metadata":{}},{"cell_type":"markdown","source":"## AutoEncoder Architecture","metadata":{}},{"cell_type":"code","source":"display(Image(filename=\"/kaggle/input/images-architecture/images_architecture/autoencoder.png\"))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Construction of Model","metadata":{}},{"cell_type":"code","source":"input_img = Input(shape=(48, 48, 1))  # adapt this if using `channels_first` image data format\n\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Dropout(0.2)(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\n\n\n# at this point the representation is (7, 7, 32)\n\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Dropout(0.2)(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\n\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = Model(input_img, decoded)\nautoencoder.compile(optimizer='adam', loss='MSE')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AutoEncoder Summary","metadata":{}},{"cell_type":"code","source":"autoencoder.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(autoencoder, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"aaa","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model","metadata":{}},{"cell_type":"code","source":"autoencoder.fit(x_train_noisy, x_train,\n                epochs=35,\n                batch_size=64,\n                shuffle=True,\n                validation_data=(x_test_noisy, x_test))","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making Prediction","metadata":{}},{"cell_type":"code","source":"predict = autoencoder.predict(x_test_final_noisy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing the prediction","metadata":{}},{"cell_type":"markdown","source":"## Original Test images","metadata":{}},{"cell_type":"code","source":"n=5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(40, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_test_final_original[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Noised Test images","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(40, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(x_test_final_noisy[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generated Test images","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(40, 48))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(predict[i].reshape(48, 48))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AutoEncoder: Train Loss VS validation loss","metadata":{}},{"cell_type":"code","source":"autoencoder.history.history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(len(autoencoder.history.history['loss']))\n\nplt.plot(epochs,autoencoder.history.history['loss'],'r-o', label='Training Loss')\nplt.title('Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.figure()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(len(autoencoder.history.history['loss']))\n\nplt.plot(epochs,autoencoder.history.history['loss'],'r-o', label='Training Loss')\nplt.plot(epochs,autoencoder.history.history['val_loss'],'b-o', label='Validation Loss')\nplt.title('Training Loss vs Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.figure()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from skimage.measure import compare_ssim, compare_psnr, compare_mse\nfrom skimage import data, img_as_float","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MSE between Images\n\n* Compare MSE between two images","metadata":{}},{"cell_type":"code","source":"# Fourth Image\ncompare_mse(x_test_final_original[3], predict[3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fifth Image\ncompare_mse(x_test_final_original[4], predict[4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Third Image\ncompare_mse(x_test_final_original[2], predict[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# For Whole final_test Dataset","metadata":{}},{"cell_type":"code","source":"# whole dataset\ncompare_mse(x_test_final_original, predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Structural Similarity Index\n\n* When comparing images, the mean squared error (MSE)--while simple to implement--is not highly indicative of perceived similarity. Structural similarity aims to address this shortcoming by taking texture into account\n\n","metadata":{}},{"cell_type":"code","source":"#fouth Image\ncompare_ssim(x_test_final_original[3], predict[3], multichannel=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fifth Image\ncompare_ssim(x_test_final_original[4], predict[4], multichannel=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Third Image\ncompare_ssim(x_test_final_original[2], predict[2], multichannel=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# whole test dataset\ncompare_ssim(x_test_final_original, predict, multichannel=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PSNR\n\nSource: https://www.mathworks.com/help/vision/ref/psnr.html\n\n* The PSNR block computes the peak signal-to-noise ratio, in decibels, between two images. This ratio is used as a quality measurement between the original and a compressed image. The higher the PSNR, the better the quality of the compressed, or reconstructed image.\n\n* The mean-square error (MSE) and the peak signal-to-noise ratio (PSNR) are used to compare image compression quality. The MSE represents the cumulative squared error between the compressed and the original image, whereas PSNR represents a measure of the peak error. The lower the value of MSE, the lower the error.","metadata":{}},{"cell_type":"code","source":"#fouth image\ncompare_psnr(x_test_final_original[3], predict[3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fifth image\ncompare_psnr(x_test_final_original[4], predict[4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#third image\ncompare_psnr(x_test_final_original[2], predict[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#whole final dataset\ncompare_psnr(x_test_final_original, predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}