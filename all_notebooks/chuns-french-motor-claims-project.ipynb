{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n#  **Introduction**\nFrench Motor Claims:\n\nA dataset including \n\n* Policy No: ID\n* 9 Explanatory Variables: Driver Age, Vehicle Age, Vehicle Power, Density, Bonus Malus, Area, Vehicle Brand, Vehicle Gas, Region\n* Weight: Exposure\n* Response Variable: Claims Number\n\nObjectives:\n1. Data Exploration / Pre-Process\n    Want to understand our explanatory variables to make suitable grouped and banded variables\n2. Testing Models\n    Using different techniques such as xgBoost, GLM etc\n3. Finding the best model\n    "},{"metadata":{},"cell_type":"markdown","source":"\n### Importing packages and dataset. \n\nAlso Creating Frequency variable (Claims/Exposure)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport sklearn as sc\nimport os\nfrom bokeh import __version__ as bk_version\n\n\ndata = pd.read_csv('/kaggle/input/french-motor-claims-datasets-fremtpl2freq/freMTPL2freq.csv')\n\n\ndata['freq'] = data['ClaimNb']/data['Exposure']\ndata['LogDensity'] = np.log(data['Density'])\n\nprint(data.describe)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a subset"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_subset = data.sample(n=20000)\nsubset_summary = random_subset.describe()\nprint(subset_summary)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a Function to create visualisation of Exposure and Claim Frequency over factors"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotBarChart(data,x,ylimitFREQ):\n    EVY= data.groupby(x,as_index=False).agg({'Exposure': 'sum'})\n    plt.subplot(1, 2, 1)\n    plt.bar(EVY[x],EVY['Exposure'],align='center')\n    plt.xlabel(x)\n    plt.ylabel('Exposure')\n    plt.title(\"Exposure over \" + str(x))\n    plt.rcParams['figure.figsize'] = (10,6)\n    plt.xticks(rotation=50)\n    \n    Freq= data.groupby(x,as_index=False).agg({'freq': 'mean'})\n    plt.subplot(1, 2, 2)\n    plt.bar(Freq[x],Freq['freq'],align='center')\n    plt.xlabel(x)\n    plt.ylabel('Freq')\n    plt.title(\"Freq over \" + str(x))\n    plt.rcParams['figure.figsize'] = (10,6)\n    plt.ylim(top=ylimitFREQ)\n    plt.xticks(rotation=50)   \n    \n    plt.tight_layout()\nplt.show()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration\nTrying to understand each of the explanatory variables (based on the subset that we have).\n\nThese graphs will give us a good visualisation of how Frequency and Exposure behave on these variables. Whilst we have a rough idea how it might look like, we are looking at France and not the UK hence we might find something else.\n\nFrequency capped at 2.\n"},{"metadata":{},"cell_type":"markdown","source":"### Driver Age"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%time\n\nplotBarChart(data,'DrivAge',2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exposure\n* Median of 44.\n* Exposure largely concentrated from ~30 to ~60.\n\nFrequency \n* As expected, we see a rise in Frequency in young ages and old ages"},{"metadata":{},"cell_type":"markdown","source":"### Vehicle Power"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBarChart(data,'VehPower',2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exposure \n* largely concentrated around 4-7. With median of 6.\n\nFrequency\n* Does not give us a good idea of any trends or correlation"},{"metadata":{},"cell_type":"markdown","source":"### Vehicle Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBarChart(data,'VehAge',2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exposure\n* median of 6\n* Exposure is concentrated from 1-15\n\nFrequency\n* Frequency high at Vehicle age 0\n* Also high for some very high Vehicle Ages, though EVY is minimal"},{"metadata":{},"cell_type":"markdown","source":"### Bonus Malus"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBarChart(data,'BonusMalus',2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exposure\n* large amount of our data has a default Bonus Malus of 50, and the rest has minimal exposure.\n\nFrequency\n* Frequency low for Bonus Malus of 50, where most EVY is concentrated\n* Very high at some high values, but with small EVY"},{"metadata":{},"cell_type":"markdown","source":"### Density\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBarChart(data,'Density',2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The standard deviation is very high for density, it seems that there are a large amount of different density. So we want to create a new variable which takes log(density):"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBarChart(data,'LogDensity',2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have logDensity as a continous numeric variable. This is good to be an input for our model."},{"metadata":{},"cell_type":"markdown","source":"### Area"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBarChart(data,'Area',2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Area looks clean and already grouped up as well.\n\nExposure\n* Good amount of EVY for every Area except for F\n\nFrequency\n* Trend of higher frequency from A to F. F hinted to be a risky area"},{"metadata":{},"cell_type":"markdown","source":"### Vehicle Brand"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBarChart(data,'VehBrand',2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Vehicle Gas"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBarChart(data,'VehGas',2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Region"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotBarChart(data,'Region',2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import geopandas as gpd\n\n\n\nsf = gpd.read_file('/kaggle/input/france/departements-version-simplifiee.geojson')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking for a possible chance to merge Map data with the French Motor Claims Project, however it seems that the data is split into 'Department' areas,different to the French Claims data, where it is split into the 22 Regions (old Region mapping)."},{"metadata":{},"cell_type":"markdown","source":"# Models"},{"metadata":{},"cell_type":"markdown","source":"1. Check the data\n2. Split Train/Validation/Holdout Data\n3. Model \n4. Fitting\n5. Predict"},{"metadata":{},"cell_type":"markdown","source":"### Checking if there are any missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One hot encoding variables to make them suitable for XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_new=pd.get_dummies(data,prefix=['Area','VehBrand','VehGas','Region'])\n\nprint(data_new.describe)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting Data and construction DMatrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import modules specific for this section\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\n# Proportions we want to split in (must sum to 1)\nsplit_props = pd.Series({\n    'train': 0.7,\n    'validation': 0.15,\n    'holdout': 0.15\n})\n\n\n# Split out training data\ndf_train,df_not_train = train_test_split(\n    data_new, test_size=(1 - split_props['train']), random_state=51, shuffle=True\n)\n# Split remaining data between validation and holdout\ndf_validation, df_holdout= train_test_split(\n    df_not_train, test_size=(split_props['holdout'] / (1 - split_props['train'])), random_state=13, shuffle=True\n)\n\n\ny_train = df_train.filter(['ClaimNb'])\n\n\nx_train = df_train.drop(columns=['ClaimNb','IDpol','freq','Exposure'])\nx_train_weight = df_train.filter(['Exposure'])\n\ntrain_dmatrix = xgb.DMatrix(data=x_train,label=y_train,weight=x_train_weight)\n\ny_valid = df_validation.filter(['ClaimNb'])\n\nx_valid = df_validation.drop(columns=['ClaimNb','IDpol','freq','Exposure'])\nx_valid_weight = df_validation.filter(['Exposure'])\n\nvalid_dmatrix = xgb.DMatrix(data=x_valid,label=y_valid,weight=x_valid_weight)\n\n\nprint(df_validation.describe)\nprint(x_valid.describe)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XgBoost\n"},{"metadata":{},"cell_type":"markdown","source":"Default Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nparams={'eval_metric':'poisson-nloglik',\"objective\": \"count:poisson\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5,'min_child_weight':1, 'reg_alpha': 10}\n\nlog_exp= np.log(df_train.filter(['Exposure']))\n\ntrain_dmatrix.set_base_margin(log_exp)\n\n\nxgmodel = xgb.train(\n    params=params,\n    dtrain=train_dmatrix,\n    num_boost_round=999,\n    early_stopping_rounds=10,\n    evals=[(train_dmatrix, \"train\"),(valid_dmatrix,\"valid\")],\n    verbose_eval=False\n)\n\n\n    \nxgb.plot_tree(xgmodel,num_trees=0)\nplt.rcParams['figure.figsize'] = [10,10]\nplt.show()\n    \nxgb.plot_importance(xgmodel)\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\ny_train= df_train.iloc[:,1]\ny_valid = df_validation.iloc[:,1]\n# \"Learn\" the mean from the training data\nmean_train = np.mean(y_train)\n\n# Get predictions on the test set\nbaseline_predictions = np.ones(y_valid.shape)\n\n# Compute MAE\nmae_baseline = mean_absolute_error(y_valid, baseline_predictions)\n\nprint(\"Baseline MAE is {:.2f}\".format(mae_baseline))\n\nprint(\"Best Poisson Likelihood: {:.2f} with {} rounds\".format(\n                 xgmodel.best_score,\n                 xgmodel.best_iteration+1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"params={'eval_metric':'poisson-nloglik',\"objective\": \"count:poisson\",'colsample_bytree':0.9 ,'learning_rate':0.1 ,\n                'max_depth':3 ,'min_child_weight':1, 'reg_alpha':5, 'subsample':0.9 }\n\nlog_exp= np.log(df_train.filter(['Exposure']))\n\ntrain_dmatrix.set_base_margin(log_exp)\n\n\nxgmodel = xgb.train(\n    params=params,\n    dtrain=train_dmatrix,\n    num_boost_round=65,\n    early_stopping_rounds=10,\n    evals=[(train_dmatrix, \"train\"),(valid_dmatrix,\"valid\")],\n    verbose_eval=False\n)\n\n\n    \nxgb.plot_tree(xgmodel,num_trees=0)\nplt.rcParams['figure.figsize'] = [10,10]\nplt.show()\n    \nxgb.plot_importance(xgmodel)\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(xgmodel)\npredictions_valid=xgmodel.predict(valid_dmatrix)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGB Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"xg_reg = xgb.XGBRegressor(eval_metric='poisson-nloglik',objective= \"count:poisson\",colsample_bytree=0.9 ,learning_rate=0.1 ,\n                max_depth=3 ,min_child_weight=1, reg_alpha=5, subsample=0.9)\n\nxg_reg.fit(x_train,y_train)\n\nreg_pred = xg_reg.predict(x_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lift and Residual"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nimport scikitplot as skplt\n\n# Reg_Pred\nregpredictions=pd.Series(reg_pred)\nregpredictions.rename(columns={\"0\":\"ClaimNb\"})\nregpredictions = regpredictions.reset_index()\ndel regpredictions['index']\n\n# Prediction\npredictions=pd.Series(predictions_valid)\npredictions.rename(columns={\"0\":\"ClaimNb\"})\npredictions = predictions.reset_index()\ndel predictions['index']\n\n\n#Actual\n\nactual_valid = df_validation.iloc[:,8]\nactual=pd.Series(actual_valid)\nactual = actual.reset_index()\ndel actual['index']\n\n#Exposure \nexposure = df_validation.iloc[:,2]\nexposure=pd.Series(exposure)\nexposure = exposure.reset_index()\ndel exposure['index']\n\nprint(regpredictions)\nprint(predictions)\nprint(actual)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Residual"},{"metadata":{"trusted":true},"cell_type":"code","source":"nppred=np.array(predictions)\nnpactual=np.array(actual)\nnpexposure=np.array(exposure)\n\nnpregpred=np.array(regpredictions)\n\nresidual=npactual-nppred\nreg_residual = npactual-npregpred\n\n\noverall_res=np.average(residual, weights=npexposure)\noverall_reg_res=np.average(reg_residual, weights=npexposure)\n\nprint(overall_res)\nprint(overall_reg_res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Saving the model and predicted validation dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_validation.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfiltered_pred_valid_set=df_validation.filter(['IDpol','ClaimNb','Exposure','freq'])\n\nfiltered_pred_valid_set = filtered_pred_valid_set.reset_index()\ndel filtered_pred_valid_set['index']\n\ndf_validation = df_validation.reset_index()\ndel df_validation['index']\n\npredicted_validation_set = df_validation.merge(predictions, left_index=True, right_index=True)\npredicted_validation_set.columns.values[-1] = 'pred_ClaimNb'\n\nfilt_pred_valid_set = filtered_pred_valid_set.merge(predictions, left_index=True, right_index=True)\nfilt_pred_valid_set.columns.values[-1] = 'pred_ClaimNb'\n\nfilt_regpred_valid_set = filtered_pred_valid_set.merge(regpredictions, left_index=True, right_index=True)\nfilt_regpred_valid_set.columns.values[-1] = 'pred_ClaimNb'\n\npredicted_reg_validation_set = df_validation.merge(regpredictions, left_index=True, right_index=True)\npredicted_reg_validation_set.columns.values[-1] = 'pred_ClaimNb'\n\npredicted_validation_set.to_pickle('xgb_pred_valid_set_new.gzip')\nfilt_pred_valid_set.to_pickle('xgb_filtered_pred_valid_set_new.gzip')\nfilt_regpred_valid_set.to_pickle('xgb_filt_reg_pred_valid_set_new.gzip')\npredicted_reg_validation_set.to_pickle('xgb_pred_reg_valid_set_new.gzip')\n\nx_train.to_pickle('xtrain.gzip')\n\nprint(filt_pred_valid_set.describe)\nprint(predicted_validation_set.describe)\n\nimport pickle\nfile_name = \"regressorxgbmodel.pkl\"\nfile_name2 = \"xgbmodel.pkl\"\n\n\n# save\npickle.dump(xg_reg, open(file_name, \"wb\"))\npickle.dump(xgmodel, open(file_name2, \"wb\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(filt_pred_valid_set.describe())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}