{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"../input/human-activity-recognition-with-smartphones/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(os.path.join(path,'train.csv'))\ndf_test = pd.read_csv(os.path.join(path,'test.csv'))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train['Activity'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Human Activity Recognition\n\nIn this notebook, we are trying to predict the Activity of a user. As you can it is a Muliclassification Problem. This notebook is to build a model that can predict whether a person is `Laying`, `Standing` , `Sitting`, `Walking`, `Walking_upstairs`, or `Walking_downstairs`\n\nInitially, the information in this dataset is the measurements from the accelerometer, gyroscope, magnetometer, and GPS of the smartphone. \n\n#### Data Information \nFrom the website: \n\nhttp://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions\n\nThe experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (`WALKING`, `WALKING_UPSTAIRS`, `WALKING_DOWNSTAIRS`, `SITTING`, `STANDING`, `LAYING`) wearing a smartphone <b>(Samsung Galaxy S II) </b> on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data.\n\nThe sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain.\n\n### Let's talk about the features (columns)\n\nWe see, there are `563 individual features(columns)`. \n\n1. The features selected for this database come from the <b> accelerometer </b> and <b> gyroscope </b> 3-axial raw signals <b> tAcc-XYZ </b>. These time domain signals (prefix <b>'t'</b> to denote time) were captured at a constant rate of 50 Hz. Then they were filtered using a median filter and a 3rd order low pass Butterworth filter with a corner frequency of 20 Hz to remove noise. \n\n\n2. Similarly, the acceleration signal was then separated into body and gravity acceleration signals <b> (tBodyAcc-XYZ and tGravityAcc-XYZ) </b> using another low pass Butterworth filter with a corner frequency of 0.3 Hz. \n\n\n3. Subsequently, the body linear acceleration and angular velocity were derived in time to obtain <b> Jerk signals (tBodyAccJerk-XYZ </b> and <b> tBodyGyroJerk-XYZ) </b>. Also the magnitude of these three-dimensional signals were calculated using the Euclidean norm `(tBodyAccMag`, `tGravityAccMag`, `tBodyAccJerkMag`, `tBodyGyroMag`, `tBodyGyroJerkMag)`\n\n`jerk is the rate at which an object's acceleration changes with respect to time`\n\n\n4. Finally a Fast Fourier Transform (FFT) was applied to some of these signals producing\n\n`fBodyAcc-XYZ, fBodyAccJerk-XYZ, fBodyGyro-XYZ, fBodyAccJerkMag, fBodyGyroMag, fBodyGyroJerkMag. `\n\n(Note the 'f' to indicate frequency domain signals). \n\nThese signals were used to estimate variables of the feature vector for each pattern:  \n\n\n5. <b>'-XYZ' </b> is used to denote 3-axial signals in the X, Y and Z directions.\n\n    - tBodyAcc-XYZ\n    - tGravityAcc-XYZ\n    - tBodyAccJerk-XYZ\n    - tBodyGyro-XYZ\n    - tBodyGyroJerk-XYZ\n    - tBodyAccMag\n    - tGravityAccMag\n    - tBodyAccJerkMag\n    - tBodyGyroMag\n    - tBodyGyroJerkMag\n    - fBodyAcc-XYZ\n    - fBodyAccJerk-XYZ\n    - fBodyGyro-XYZ\n    - fBodyAccMag\n    - fBodyAccJerkMag\n    - fBodyGyroMag\n    - fBodyGyroJerkMag`\n    \n    \n\n6. The set of variables that were estimated from these signals are: \n\n    - `mean()`: Mean value\n    - `std()`: Standard deviation\n    - `mad()`: Median absolute deviation \n    - `max()`: Largest value in array\n    - `min()`: Smallest value in array\n    - `sma()`: Signal magnitude area\n    - `energy()`: Energy measure. Sum of the squares divided by the number of values. \n    - `iqr()`: Interquartile range \n    - `entropy()`: Signal entropy\n    - `arCoeff()`: Autorregresion coefficients with Burg order equal to 4\n    - `correlation()`: correlation coefficient between two signals\n    - `maxInds()`: index of the frequency component with largest magnitude\n    - `meanFreq()`: Weighted average of the frequency components to obtain a mean frequency\n    - `skewness()`: skewness of the frequency domain signal \n    - `kurtosis()`: kurtosis of the frequency domain signal \n    - `bandsEnergy()`: Energy of a frequency interval within the 64 bins of the FFT of each window.\n    - `angle()`: Angle between to vectors.\n    \n\n7. Additional vectors obtained by averaging the signals in a signal window sample. These are used on the angle() variable:\n\n    `gravityMean\n     tBodyAccMean\n     tBodyAccJerkMean\n     tBodyGyroMean\n     tBodyGyroJerkMean`\n \n That's too much information. "},{"metadata":{},"cell_type":"markdown","source":"."},{"metadata":{},"cell_type":"markdown","source":"## What's our Plan?\n\n\n### `Outline`\n\n- <b>1. Read Dataset </b>\n\n\n- <b>2. Datset Cleaning </b>\n    - 2.1 Outliers\n    - 2.2 Filling null values\n    - 2.3 Check for data imbalance\n    - 2.4 Correcting some feature names\n\n\n   \n- <b>3. Exploratory Data Analysis </b>\n\n\n- <b>4. Data Preprocessing </b>\n    - 4.1 Encoding categorical variables\n    - 4.2 Normalization\n    - 4.3 Split Training and testing\n    \n    \n    \n- <b>5. Models, Hyperparameter Tuning and Cross Validation</b>\n    - 5.1 Logistic Regression \n    - 5.2 Naive Bayes \n    - 5.3 K-Nearest Neighbor\n    - 5.4 Decision Tree\n    - 5.5 Random Forest\n    - 5.5 Support Vector Machine\n    \n    \n"},{"metadata":{},"cell_type":"markdown","source":"Since we have already observed the data and the features. So we will skip the part."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Dataset Cleaning\n\n- 2.1 Outliers\n- 2.2 Filling null values\n- 2.3 Check for data imbalance"},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Oultiers"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no any possibility of having Outliers. All the values are squeezed between -1 to 1. "},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Checking for NaN/null values and Duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checking for Duplicates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total Duplicates Train: {} \\n\".format(sum(df_train.duplicated())))\nprint(\"Total Duplicates in Test: {} \\n\".format(sum(df_test.duplicated())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Checking for null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total Null values in Train: {}\\n\".format(df_train.isnull().values.sum()))\nprint(\"Total Null values in Test: {} \\n\".format(df_test.isnull().values.sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Check for imbalanced dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,8))\nplt.title('Subjects')\nsns.countplot(x = 'subject', data = df_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,8))\nplt.title(\"Subject with Each Activity\")\nsns.countplot(hue = 'Activity', x='subject',data = df_train);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,8))\nsns.countplot(x = 'Activity', data = df_train);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see, each subjects has almost equal or less amount of data. There is no any huge amount of gap between them."},{"metadata":{},"cell_type":"markdown","source":"### 2.4 Correcting some feature names\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see, some () 'bracket' between the feature's name. We will remove all these brackets quickly. So it's easier for us to type correctly later."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = df_train.columns\n\n## Removing ()\n\ncolumns = columns.str.replace('[()]','')\ncolumns = columns.str.replace('[-]','')\ncolumns = columns.str.replace('[,]','')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns = columns\ndf_test.columns = columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Exploratory Data Analysis\n\n\n#### Static and Dynamic Activites\n\n- Static activities are (sit, stand, lie and down) thus there is no any motion of an object. \n- Dynamic activities (Walking, WalkingUpStairs, WalkingDownStairs) motion info will be significant\n\n"},{"metadata":{},"cell_type":"markdown","source":"#### 2. Stationary and Moving activities are completely different"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_palette(\"Set1\", desat=0.80)\nfacetgrid = sns.FacetGrid(df_train, hue='Activity', size=6,aspect=2)\nfacetgrid.map(sns.distplot,'tBodyAccMagmean', hist=False)\\\n    .add_legend()\nplt.annotate(\"Stationary Activities\", xy=(-0.956,17), xytext=(-0.9, 23), size=20,\\\n            va='center', ha='left',\\\n            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\n\nplt.annotate(\"Moving Activities\", xy=(0,3), xytext=(0.2, 9), size=20,\\\n            va='center', ha='left',\\\n            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a closer look at them"},{"metadata":{"trusted":true},"cell_type":"code","source":"## \n\nplt.figure(figsize = (12,8))\nplt.subplot(1,2,1)\nplt.title(\"Static Activities (closer view)\")\nsns.distplot(df_train[df_train[\"Activity\"]==\"SITTING\"]['tBodyAccMagmean'], hist = False, label = 'Sitting');\nsns.distplot(df_train[df_train[\"Activity\"]==\"STANDING\"]['tBodyAccMagmean'], hist = False, label = 'Standing');\nsns.distplot(df_train[df_train[\"Activity\"]==\"LAYING\"]['tBodyAccMagmean'], hist = False, label = 'Laying');\nplt.axis([-1.02, -0.5, 0, 35])\nplt.subplot(1,2,2)\nplt.title(\"Dynamic Activities (closer view)\")\nsns.distplot(df_train[df_train[\"Activity\"]==\"WALKING\"][\"tBodyAccMagmean\"], hist = False, label =\"Sitting\");\nsns.distplot(df_train[df_train[\"Activity\"]==\"WALKING_UPSTAIRS\"]['tBodyAccMagmean'], hist = False, label = 'Laying');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will also, use box plot to visulaize"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,7))\nsns.boxplot(x = 'Activity', y ='tBodyAccMagmean', data = df_train, showfliers = False);\nplt.ylabel('Body Acceleration Magnitude mean')\nplt.title('Boxplot of tBodyAccMagmean column across various activities')\nplt.axhline(y =- 0.7, xmin = 0.1, xmax = 0.9, dashes = (3,3))\nplt.axhline(y = 0.020, xmin = 0.4, dashes = (3,3))\nplt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using boxplot agian, we can come with conditions to seperate static activities from dynamic activities.\n\n`` if(tBodyAccMagmean <= -0.8):\n      Activity = \"static\"\n  if(tBodyAccMagmean >= -0.6):\n      Activity = \"dynamic\"\n ``\n "},{"metadata":{},"cell_type":"markdown","source":"Also, we can easily seperate WALKING_DOWNSTAIRS activity from others using boxplot.\n\n`` \nif (tBodyAccMagmean > 0.02):\n    Activity = \"WALKING_DOWNSTARIS\"\nelse:\n    Activity = \"others\"\n``\n\nBut still 25% of WALKING_DOWNSTAIRS observations are below 0.02 which are misclassified as others so this condition makes an error of 25% in classification."},{"metadata":{},"cell_type":"markdown","source":"#### 3.2 Analysing Angle between X-axis and gravityMean feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,7))\nsns.boxplot(x = 'Activity', y = 'angleXgravityMean', data = df_train, showfliers = False)\nplt.axhline(y = 0.08, xmin = 0.1 , xmax = 0.9, dashes = (3,3))\nplt.ylabel(\"Angle between X-axis and gravityMean\")\nplt.title(\"Box plot of angleXgravityMean column across various activities\")\nplt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Observation: </b>\n- If angleXgravityMean > 0.01 then Activity is <b> Laying </b>\n- We can classify all datapoints belonging to Laying activity with just a single if else statement\n\n"},{"metadata":{},"cell_type":"markdown","source":"#### 3.3 Analysing Angle between Y-axis and gravityMean feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,7))\nsns.boxplot(x = 'Activity', y = 'angleYgravityMean', data = df_train, showfliers = False)\nplt.ylabel(\"Angle between Y-axis and gravityMean\")\nplt.title(\"Box plot of angleYgravitymean column across various activities\")\nplt.xticks(rotation = 90)\nplt.axhline(y = -0.35, xmin = 0.01, dashes = (3,3))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.4 Visualizing data using t-SNE\n\nUsing t-SNE data can be visualized from a extermely high dimensional space to a low dimensional space and still it retains lots of actual information. Given training data has 561 unique featuers, using t-SNE let's visualze it to a 2D space."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.manifold import TSNE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_for_tsne = df_train.drop(['subject','Activity'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\ntsne = TSNE(random_state = 42, n_components = 2, verbose = 1, perplexity = 50, n_iter = 1000).fit_transform(X_for_tsne)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,8))\nsns.scatterplot(x = tsne[:,0], y = tsne[:,1], hue = df_train[\"Activity\"], palette = \"bright\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observations:</b>\n- Laying is totally different position\n- Walking, Walking_downstaris, Walking_upstairs are some kind of similar so they are clustered together\n- And, Standing and Sitting are also some kind of same position."},{"metadata":{},"cell_type":"markdown","source":"## 4. Data Preprocessing\n\n\n\n\n\n\n\n#### 4.1 Splitting training and testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.Activity\nX_train = df_train.drop(['subject','Activity'], axis = 1)\ny_test = df_test.Activity\nX_test = df_test.drop(['subject','Activity'], axis = 1)\nprint('Training data size:', X_train.shape)\nprint('Test data size:', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = pd.DataFrame(columns = (\"Model\",\"Score\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Models, HyperparamterTuning and Cross Validations\n- Logistic Regression \n- Linear SVM\n- Kernel SVM\n- Decision Tree\n- Random Forest\n\n"},{"metadata":{},"cell_type":"markdown","source":"#### 5.1 Logistic regression model with Hyperparameter tuning and cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'C':np.arange(10,61,10),'penalty':['l2','l1']}\nlr_classifier = LogisticRegression()\nlr_classifier_rs = RandomizedSearchCV(lr_classifier, param_distributions = parameters, cv = 5, random_state = 42)\nlr_classifier_rs.fit(X_train, y_train)\ny_pred = lr_classifier_rs.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\nprint(\"Accuracy using Logisitc Regression:\", lr_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(pd.DataFrame({'Model':[\"LogisticRegression\"],'Score':[lr_accuracy]}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_classifier_rs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## plotting confusion matrix\n\ndef plot_confusion_matrix(cm, lables):\n    fig, ax = plt.subplots(figsize = (12,8))\n    im = ax.imshow(cm, interpolation = 'nearest', cmap = plt.cm.Blues)\n    ax.figure.colorbar(im, ax = ax)\n    ax.set(xticks = np.arange(cm.shape[1]))\n    yticks = np.arange(cm.shape[0])\n    ylabel = 'True label'\n    xlabel = 'Predicted label'\n    plt.xticks(rotation = 90)\n    thresh = cm.max() / 2\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, int(cm[i,j]), ha = \"center\", va = \"center\", color = \"white\" if cm[i,j]> thresh else \"black\")\n            fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test.values, y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## function to get best random search attributes\n\ndef get_best_randomsearch_results(model):\n    print(\"Best estimator:\", model.best_estimator_)\n    print(\"Best set of parameters:\", model.best_params_)\n    print(\"Best score:\", model.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## getting best random search attributes\n\nget_best_randomsearch_results(lr_classifier_rs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2 Linear SVM model with Hyperparameter tuning and cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'C': np.arange(1,12,2)}\nlr_svm = LinearSVC(tol = 0.00005)\nlr_svm_rs = RandomizedSearchCV(lr_svm, param_distributions = parameters, random_state = 42)\nlr_svm_rs.fit(X_train, y_train)\ny_pred = lr_svm_rs.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_svm_accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\nprint(\"Accuracy using Linear SVM:\", lr_svm_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(pd.DataFrame({'Model':[\"LinearSVM\"],'Score':[lr_svm_accuracy]}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test.values, y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## getting best random search attributes\nget_best_randomsearch_results(lr_svm_rs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.3 Kernel SVM model with Hyperparameter tuning and cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.linspace(2,22, 6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'C':[2,4,8,16], 'gamma':[0.125, 0.250, 0.5, 1]}\nkernel_svm = SVC(kernel = 'rbf')\nkernel_svm_rs = RandomizedSearchCV(kernel_svm, param_distributions = parameters, random_state = 42)\nkernel_svm_rs.fit(X_train, y_train)\ny_pred = kernel_svm_rs.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kernel_svm_accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\nprint(\"Accuracy using Kernel SVM:\", kernel_svm_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(pd.DataFrame({'Model':[\"KernelSVM\"],'Score':[kernel_svm_accuracy]}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test.values, y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## getting best random search attributes\n\nget_best_randomsearch_results(kernel_svm_rs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.4 Decision tree model with Hyperparameter tuning and cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = {'max_depth':np.arange(2,10,2)}\ndt_classifier = DecisionTreeClassifier()\ndt_classifier_rs = RandomizedSearchCV(dt_classifier,param_distributions=parameters,random_state = 42)\ndt_classifier_rs.fit(X_train, y_train)\ny_pred = dt_classifier_rs.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_accuracy = accuracy_score(y_true = y_test, y_pred = y_pred)\nprint(\"Accuracy using Decision tree:\", dt_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(pd.DataFrame({'Model':[\"DecisionTrees\"],'Score':[dt_accuracy]}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test.values, y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## getting best estimators\n\nget_best_randomsearch_results(dt_classifier_rs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5.5 Random Forest model using Hyperparameter tuning and cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_estimators': np.arange(20,101,10), 'max_depth':np.arange(2,16,2)}\nrf_classifier = RandomForestClassifier()\nrf_classifier_rs = RandomizedSearchCV(rf_classifier, param_distributions=params,random_state = 42)\nrf_classifier_rs.fit(X_train, y_train)\ny_pred = rf_classifier_rs.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_accuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy using Random Forest:\", rf_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(pd.DataFrame({'Model':[\"RandomForest\"],'Score':[rf_accuracy]}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test.values, y_pred)\nplot_confusion_matrix(cm, np.unique(y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}