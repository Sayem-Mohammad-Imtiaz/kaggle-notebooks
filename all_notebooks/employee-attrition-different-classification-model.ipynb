{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Installing and Importing Packages and Importing the Data  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Installing pydotplus package that could provide a python interface to graphviz's dot language.\n# We will use this package later on to plot the feature importance.\n!pip install pydotplus","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"##Importing the packages\n#Data processing packages\nimport numpy as np \nimport pandas as pd \n\n#Visualization packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n#Machine Learning packages\nfrom sklearn.svm import SVC,NuSVC\n#from xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Importing the Dataset\ndataset = pd.read_csv(\"../input/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffle the Dataset.\ndataset = dataset.sample(frac=1,random_state=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if our Dataset has any nan values\ndataset.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = dataset['Age'], y=None, hue =dataset['Attrition']).set_title( \"Attrition rate with the Age\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = dataset['Department'], y=None, hue =dataset['Attrition']).set_title( \"Attrition distribution in different department\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = dataset['DistanceFromHome'], y=None, hue =dataset['Attrition']).set_title( \"Attrition distribution depending on distance from home to company\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = dataset['YearsAtCompany'], y=None, hue =dataset['Attrition']).set_title( \"Age distribution depending on years at the company\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build a histogram that shows the attrition in the company \nsns.countplot(x = 'Attrition',data= dataset,palette = \"Set2\").set_title('Attrition')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***************************"},{"metadata":{},"cell_type":"markdown","source":"# Cleaning the Data and Data visualization"},{"metadata":{},"cell_type":"markdown","source":"At first, we have to drop the unecessary and unsigneficant features such as: **EmployeeCount**, **employeeNumber**, **Over18**, **StandardHours** "},{"metadata":{"trusted":true},"cell_type":"code","source":"ToDrop = [\"EmployeeCount\", \"EmployeeNumber\", \"Over18\", \"StandardHours\"]\ndataset = dataset.drop(ToDrop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### WARNING\n\nTo Check: **NumCompaniesWorked** "},{"metadata":{},"cell_type":"markdown","source":"Now, we will devide the dataset into 3 datasets to visualize the correlation between the ontinuous values and the target value:\n\n**DisData:** for the discreete values\n\n**ContData:** for the continuous values\n\n**Y:** for the target values"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = dataset.loc[:, \"Attrition\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.loc[:, dataset.columns !=\"Attrition\"]\n\nContinuous = [\"Age\", \"DailyRate\", \"DistanceFromHome\", \"HourlyRate\", \"MonthlyIncome\", \"MonthlyRate\", \n              \"PercentSalaryHike\",\"TotalWorkingYears\", \"TrainingTimesLastYear\", \"YearsAtCompany\", \"YearsInCurrentRole\", \n              \"YearsSinceLastPromotion\", \"YearsWithCurrManager\"]\n\nContData = X[Continuous].copy()\nContData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Discreet = ['BusinessTravel', 'Department', 'Education', 'EducationField', 'EnvironmentSatisfaction',\n            'Gender', 'JobInvolvement', 'JobLevel', 'JobRole','JobSatisfaction', 'MaritalStatus',\n            'NumCompaniesWorked', 'OverTime', 'PerformanceRating', 'RelationshipSatisfaction', \n            'StockOptionLevel', 'WorkLifeBalance']\n\nDisData = X[Discreet].copy()\nDisData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(15,8))\nsns.heatmap(ContData.corr(),annot=True,cmap='viridis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test = DisData[\"Department\"]\nTest1 = pd.get_dummies(Test, drop_first= True)\nTest1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"************************"},{"metadata":{},"cell_type":"markdown","source":"# Preparing our datasets"},{"metadata":{},"cell_type":"markdown","source":"**First, we have to convert the categorical values to numerical ones with getdummies and seperate the features and the target value**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, Y = dataset.loc[:, dataset.columns !=\"Attrition\"], dataset.loc[:, \"Attrition\"]\nX = pd.get_dummies(X, drop_first= True) # X has 44 columns\nY = pd.get_dummies(Y, drop_first= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform the Y collumn to an array\nY = np.ravel(Y)\nY.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features' Selection\n**There are lots of methods to select features such as:**"},{"metadata":{},"cell_type":"markdown","source":"## Univariate feature selection\nUnivariate feature selection works by selecting the best features based on univariate statistical tests."},{"metadata":{},"cell_type":"markdown","source":"### SelectKBes(k_best), SelectPercentile(percentile), SelectFpr(fpr), SelectFdr(fdr), SelectFwe(fwe) ...\n\n* **SelectKBest:** Removes all but the K highest scoring features\n\n* **SelectPercentile:** Leaves a percentage of the features (Percentile is the Percent of features to keep)\n\n* **FPR(false positive rate):** Selects the pvalues below alpha based on a FPR test.\n\n    FPR test stands for False Positive Rate test. It controls the total amount of false detections.\n\n* **FDR(false discovery rate):** \n\n* **FWE(family wise error):** Selects the p-values corresponding to Family-wise error rate\n\n==> These objects take as input a **scoring function** that returns univariate scores and p-values (or only scores for SelectKBest and SelectPercentile)\n\nWe have as exemples of scoring functions: **Chi2, f-classif, mutual_info_classif**\n\n==> The scoring function takes two arrays X and y, and returning a pair of arrays **(scores, pvalues)** or a single array with scores. Default is f_classif (see below “See also”). The default function only works with classification tasks. its **attributes** are:\n    \n   * **scores_ : array-like, shape=(n_features,)** ==> Scores of features.\n\n   * **pvalues_ : array-like, shape=(n_features,)**  ==> p-values of feature scores, None if score_func returned scores only.\n\n"},{"metadata":{},"cell_type":"markdown","source":"#### Chi2: It's based on Chi-squared statistical test\nThe null hypothesis for chi2 test is that \"**two categorical variables are independent**\". So a **higher value of chi2** statistic means \"**two categorical variables are dependent**\" and MORE USEFUL for classification.\n\nIn other words, Chi2 test calculates the dependence between the features and the target value, then SelectKBest gives you the best features based on higher chi2 values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the necessary librairies\nfrom sklearn.feature_selection import GenericUnivariateSelect\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import chi2\n\n# printing the indexes of the selected features\n#V = GenericUnivariateSelect(chi2, 'k_best', param=40).fit(X,Y).get_support(indices=True)\n#X.iloc[:,V].columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the function that selects the best k features based on chi2 statistical test\ndef selectbest_chi2(X,Y,k):\n    #Selecting the best k features\n    X_new = GenericUnivariateSelect(chi2, 'k_best', param=k).fit_transform(X, Y)\n    ## X_new = GenericUnivariateSelect(chi2, 'percentile', param=k).fit_transform(X, Y)\n    return X_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### f_classif: It computes the ANOVA (Analysis of Variance) F-value for the provided sample.\n\nF-tests are named after its test statistic, F. The F-statistic is simply a ratio of two variances. Variances are a measure of dispersion, or how far the data are scattered from the mean. Larger values represent greater dispersion.\n\nThe F-statistic is this ratio:\n\n**F = variation between sample means / variation within the samples**\n\nIn other words, f_classif classify the features by the F value that means it prioritizes the features that have a big variation between their means."},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the necessary librairie\nfrom sklearn.feature_selection import GenericUnivariateSelect\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_classif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the function that selects the best k features based on F statistical test.\ndef selectbest_fclassif(X,Y,k):\n    #Selecting the best k features\n    X_new = GenericUnivariateSelect(f_classif, 'k_best', param=k).fit_transform(X, Y)\n    return X_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### mutual_info_classif: It's based on mutual information\nMutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\nso, the mutual_info_classif prioritizes the most independent features and eliminates the dependent features or leaves one of them."},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the necessary librairies\nfrom sklearn.feature_selection import GenericUnivariateSelect\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import mutual_info_classif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the function that selects the best k features based on mutual information between the features.\ndef selectbest_mutualinfoclassif (X,Y,k):\n    #Selecting the best k features\n    X_new = GenericUnivariateSelect(mutual_info_classif, 'k_best', param=k).fit_transform(X, Y)\n    return X_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"********************************"},{"metadata":{},"cell_type":"markdown","source":"## Recursive feature elimination\nRecursive feature elimination (RFE) is a feature selection method that fits a model and removes the weakest feature until the specified number of features is reached.\n\n**Remarque:** This function doesn't work with all the models."},{"metadata":{"trusted":true},"cell_type":"code","source":"def recursive_elimination(model,X,Y):\n    from sklearn.model_selection import StratifiedKFold\n    from sklearn.feature_selection import RFECV\n    from sklearn.datasets import make_classification\n\n    # The \"accuracy\" scoring is proportional to the number of correct\n    # classifications\n    rfecv = RFECV(estimator=model, step=1, cv=StratifiedKFold(2),scoring='accuracy')\n    rfecv.fit(X, Y)\n\n    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n\n    # Plot number of features VS. cross-validation scores\n    plt.figure()\n    plt.xlabel(\"Number of features selected\")\n    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**********************"},{"metadata":{},"cell_type":"markdown","source":"# Feature Ranking via the models\nWith this function we can see the features' importance for the models based on tree's model.\n\nWith **Ploty** package we can plot the features' importance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import statements required for Plotly \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Building functions that  tell us which features within our dataset has been given most importance through the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ranking function specified only to the models which are based on trees (models in DecisionTreeClassifier)\ndef Ranking(model,title):\n    trace = go.Scatter(\n        y = model.feature_importances_,\n        x = X.columns.values,\n        mode='markers',\n        marker=dict(\n            sizemode = 'diameter',\n            sizeref = 1,\n            size = 13,\n            #size= rf.feature_importances_,\n            #color = np.random.randn(500), #set color equal to a variable\n            color = model.feature_importances_,\n            colorscale='Portland',\n            showscale=True\n        ),\n        text = X.columns.values\n    )\n    data = [trace]\n\n    layout= go.Layout(\n        autosize= True,\n        hovermode= 'closest',\n         xaxis= dict(\n             ticklen= 5,\n             showgrid=False,\n            zeroline=False,\n            showline=False\n         ),\n        yaxis=dict(\n            title= 'Feature Importance',\n            showgrid=False,\n            zeroline=False,\n            ticklen= 5,\n            gridwidth= 2\n        ),\n        showlegend= False\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig,filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*********************"},{"metadata":{},"cell_type":"markdown","source":"# Let's make some predictions!! Then compare the different models"},{"metadata":{},"cell_type":"markdown","source":"The function below allows us to scale our dataset to standardize the range of independent variables or features of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"def scaling(X_new):\n    #feature scaling\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    X_new = sc.fit_transform(X_new)\n    return X_new","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After scaling the data, we will use the function below which allows us to split our database into training and testing datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"def splitting(X_new,Y): \n    # Splitting the dataset into the Training set and Test set\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X_new, Y, test_size = 0.25, random_state = 0)\n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below we built a function which takes as input the test set, the train set and the model used. Then, it trains the models on the train set and makes the prediction and gives us the results of the prediction and some values that allow us to know the perfomance of our model. \n\nThese values are:\n* **accuracy = ** (number of points classified correctly) / (total number of points in your test set)\n* **RocAUC score : **Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n* **Precision = :**TruePositive/(TruePositive+FalsePositive)\n* **Recall = :**TruePositive/(TruePositive+FalseNegative)\n* **F1_score = :**2*(Precision*Recall)/(Precision+Recall)\n* **Mean Square Error : **Mean Squared Error of an estimator measures the average of error squares i.e. the average squared difference between the estimated values and true value.It is always non negative and values close to zero are better.\n* **Time of execution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to Train and Test Machine Learning Model\ndef train_test_ml_model(X_train,y_train,X_test,model,Model):\n    import time\n    start = time.time()\n\n    model.fit(X_train,y_train) #Train the Model\n    y_pred = model.predict(X_test) #Use the Model for prediction\n    end = time.time()\n\n    # Test the Model\n    from sklearn.metrics import confusion_matrix\n    cm = confusion_matrix(y_test,y_pred)\n    \n    # Test the model accuracy\n    from sklearn.model_selection import cross_val_score\n    #accuracy = cross_val_score(estimator = model, X = X_new, y = y_train, cv = 4) #It gives you the accuracy of each training\n    accuracy = round(100*np.trace(cm)/np.sum(cm),1)\n    \n    # Test the RocAUC of the model\n    #from sklearn.metrics import roc_auc_score\n   # roc_value = roc_auc_score(y_test, y_pred)\n    \n    #Precision,Recall,F1_score\n    Precision = cm[0][0] / (cm[0][0] + cm[0][1])\n    Recall = cm[0][0] / (cm[0][0] + cm[1][0])\n    F1 = 2*(Precision*Recall)/(Precision+Recall)\n\n    #Mean square error\n    #from sklearn.metrics import mean_squared_error\n    #MSE = mean_squared_error(y_test, y_pred)\n    \n    #Plot/Display the results\n    cm_plot(cm,Model)\n    \n    print('The Wrong predicted points are:')\n    Wrong_Prediction(X_test,y_test,y_pred)\n\n    Results = pd.DataFrame(columns=['accuracy','Precision = TP/(TP+FP)','Recall = TP/(TP+FN)',\n                                    'F1_score','Time of execution'], index=[Model])\n    results = {'accuracy':accuracy,'Precision = TP/(TP+FP)':Precision,\n         'Recall = TP/(TP+FN)':Recall,'F1_score':F1,'Time of execution':end - start} \n    Results.loc[Model] = pd.Series(results)\n    return (Results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make a function to plot the confusion matrix. It's a square matrix that contains 4 blocs which are:\n* **True Positive (TP) : ** Real and predicted values are YES_Attrition\n* **False Positive (FP) : ** Real and Prredicted values are NO_Attrition\n* **True Negative (TN) : ** Real values are YES_Attrition and predicted ones are NO_Attrition\n* **False Negative (FN) : ** Real values are NO_Attrition and predicted ones are YES_Attrition"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to plot Confusion Matrix\ndef cm_plot(cm,Model):\n    plt.clf()\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.viridis)\n    classNames = ['Negative','Positive']\n    plt.title('Comparison of Prediction Result for '+ Model)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames, rotation=45)\n    plt.yticks(tick_marks, classNames)\n    s = [['TN','FP'], ['FN', 'TP']]\n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below, gives us the indices of the wrong predicted values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def Wrong_Prediction (X_test,y_test,y_pred):\n    WIndex = []\n    for i in range(len(y_test)):\n        if y_test[i] != y_pred[i] :\n            WIndex.append(i)\n    print (WIndex)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, let's test some models**"},{"metadata":{},"cell_type":"markdown","source":"First of all, we have to split our data into train data and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = splitting(X,Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"In this model we will use different scoring function of Univariate feature selection and compare their performance, then in the next models we will only use the scoring function that outperforms the others in this model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression  #Import package related to Model\nModel = \"LogisticRegression\"\nmodel= LogisticRegression(C=0.35000000000000003, solver=\"newton-cg\", max_iter=200) #Create the Model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Selectbest_chi2"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new = selectbest_chi2(X,Y,40)\nX_new = scaling(X_new)\nX_train, X_test, y_train, y_test = splitting(X_new,Y)\nResults = pd.DataFrame()\nResults = train_test_ml_model(X_train,y_train,X_test,model,Model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After getting our model results and to improve its prformance, we will use now the features' recursive elimination methods wich gives us the optimal number of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"recursive_elimination(model,X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we train our model with the new number of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = splitting(X,Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new = selectbest_chi2(X,Y,37)\nX_new = scaling(X_new)\nX_train, X_test, y_train, y_test = splitting(X_new,Y)\nresults_logistic = pd.DataFrame()\nresults_logistic = train_test_ml_model(X_train,y_train,X_test,model,Model)\n\nResults = Results.append(results_logistic.iloc[0,:])\nResults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"==> as we suspected, the performance of our model has improved"},{"metadata":{},"cell_type":"markdown","source":"We will do the same steps in the other models"},{"metadata":{},"cell_type":"markdown","source":"## Random Forest\nRandom Forests are called Bagging Meta-Estimators."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier  #Import package related to Model\nModel = \"RandomForestClassifier\"\nmodel= RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt') #Create the Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = splitting(X,Y)\nX_new = selectbest_chi2(X,Y,40)\nX_new = scaling(X_new)\nX_train, X_test, y_train, y_test = splitting(X_new,Y)\nresults = pd.DataFrame()\nresults = train_test_ml_model(X_train,y_train,X_test,model,Model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recursive_elimination(model,X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = splitting(X,Y)\nX_new = selectbest_chi2(X,Y,25)\nX_new = scaling(X_new)\nX_train, X_test, y_train, y_test = splitting(X_new,Y)\nresults_forest = pd.DataFrame()\nresults_forest = train_test_ml_model(X_train,y_train,X_test,model,Model)\n\nresults = results.append(results_forest.iloc[0,:])\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shown below is an Interactive Plotly diagram of the various feature importances for the Random Forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"title= 'Random Forest Feature Importance'\nRanking(model,title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Kernel SVM"},{"metadata":{},"cell_type":"markdown","source":"Linear Kernel"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm  #Import package related to Model\nModel = \"LinearSVM\"\nmodel = svm.SVC(kernel='linear', C=1.0)  # Create the Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = splitting(X,Y)\nX_new = selectbest_chi2(X,Y,40)\nX_new = scaling(X_new)\nX_train, X_test, y_train, y_test = splitting(X_new,Y)\n\nresults = pd.DataFrame()\nresults = train_test_ml_model(X_train,y_train,X_test,model,Model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recursive_elimination(model,X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = splitting(X,Y)\nX_new = selectbest_chi2(X,Y,27)\nX_new = scaling(X_new)\nX_train, X_test, y_train, y_test = splitting(X_new,Y)\nresults_linearSVM = pd.DataFrame()\nresults_linearSVM = train_test_ml_model(X_train,y_train,X_test,model,Model)\n\nresults = results.append(results_linearSVM.iloc[0,:])\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{},"cell_type":"markdown","source":"sequential decision trees (one after the other) but we build these trees with boosting techniques and with according weights to each point\nthese weights are initially equal, if a point get predicted wrongly, we increase its weight. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier  #Import package related to Model\nModel = \"XGBClassifier\"\nmodel=XGBClassifier() #Create the Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = splitting(X,Y)\nX_new = selectbest_chi2(X,Y,40)\nX_new = scaling(X_new)\nX_train, X_test, y_train, y_test = splitting(X_new,Y)\n\nresults = pd.DataFrame()\nresults = train_test_ml_model(X_train,y_train,X_test,model,Model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recursive_elimination(model,X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = splitting(X,Y)\nX_new = selectbest_chi2(X,Y,30)\nX_new = scaling(X_new)\nX_train, X_test, y_train, y_test = splitting(X_new,Y)\nresults_XGBC = pd.DataFrame()\nresults_XGBC = train_test_ml_model(X_train,y_train,X_test,model,Model)\n\nresults = results.append(results_XGBC.iloc[0,:])\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shown below is an Interactive Plotly diagram of the various feature importances for the XGBoost."},{"metadata":{"trusted":true},"cell_type":"code","source":"title= 'XGBoost Feature Importance'\nRanking(model,title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's make some combinations!!"},{"metadata":{},"cell_type":"markdown","source":"**First Combination**: LogisticRegression, RandomForestClassifier and SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.data import iris_data\nfrom mlxtend.plotting import plot_decision_regions\n\n# Initializing Classifiers\nclf1 = LogisticRegression(C=0.35000000000000003, solver=\"newton-cg\", max_iter=200) #Create the First Model\nclf2 = RandomForestClassifier(n_estimators=100, bootstrap = True, max_features = 'sqrt') #Create the Second Model\nclf3 = svm.SVC(kernel='linear', C=1.0, probability=True)  # Create the Third Model\n\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[3, 1.5, 2], voting='soft')\n\n\nmodel = eclf\nModel = \"Combination1\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = splitting(X,Y)\nX_new = selectbest_chi2(X,Y,40)\nX_new = scaling(X_new)\nX_train, X_test, y_train, y_test = splitting(X_new,Y)\n\nresults_Combination1 = pd.DataFrame()\nresults_Combination1 = train_test_ml_model(X_train,y_train,X_test,model,Model)\nresults_Combination1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Second Combination**: LogisticRegression, XGBoost and SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier  #Import package related to Model\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom mlxtend.data import iris_data\nfrom mlxtend.plotting import plot_decision_regions\n\n# Initializing Classifiers\nclf1 = LogisticRegression(C=0.35000000000000003, solver=\"newton-cg\", max_iter=200) #Create the First Model\nclf2 = XGBClassifier()  #Create the Second Model\nclf3 = svm.SVC(kernel='linear', C=1.0, probability=True)  # Create the Third Model\n\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[3, 1.5, 2], voting='soft')\n\n\nmodel = eclf\nModel = \"LR+XGB+SVM\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = splitting(X,Y)\nX_new = selectbest_chi2(X,Y,40)\nX_new = scaling(X_new)\nX_train, X_test, y_train, y_test = splitting(X_new,Y)\n\nresults_Combination2 = pd.DataFrame()\nresults_Combination2 = train_test_ml_model(X_train,y_train,X_test,model,Model)\nresults_Combination2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Below, the table that resumes the results of all the models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Results = pd.DataFrame\nResults = results_logistic\nT = [results_linearSVM,results_forest,results_XGBC,results_Combination2]\nfor i in range(len (T)):\n    Results = Results.append(T[i])\n\nResults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*******************************"},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{},"cell_type":"markdown","source":"### AdaBoost"},{"metadata":{},"cell_type":"markdown","source":"# Remarques\n### * we can use the **ensemble learning** which means we combine different models and create one model which it result is the mean of the other results and this to get\n    \n    **1**. better accuracy (low error)\n    \n    **2**. Higher consistency (avoid the overfitting)\n    \n    **3**. Reduce bias and variance errors\n\nThe methods to apply ensemble learning are:\n### * Bagging: split the train set into subsets \n### * Boosting: Also split the train set into subsets but when finding some wrong predicted points in a subset (bag), we coppy that point in the next subset and run thee model \n### * GetDummies, Label Encoder and One Hot Encoder\n   Label Encoder replaces each category with a number (1,2,3,4,5...) ==> this might gives different and confusing weights to different categiries!! that's a problem\n       \n   To correct this problem we use the OneHot encoder which replace the numerical values with binary values\n       \n   ==> So, we might use simply GetDummies\n       \n   GetDummies = Label Encoder + OneHot Encoder\n           \n### * For other information on **mlxtend** check: http://joss.theoj.org/papers/10.21105/joss.00638"},{"metadata":{},"cell_type":"markdown","source":"*******************"},{"metadata":{},"cell_type":"markdown","source":"# Najed's Remarques\n\n* decision tree: features' importance do not match well the tree!! find out why?\n\nTry to plot the tree with 4 or 5 features only whch are the most important!\n\n* Add more comments! U have to explain more chi2!!\n\n* Absenteeism: try to classify people with the cause! See the main cause!\nAdd more comments! especially the correspondance between the number and the cause of absence!\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}