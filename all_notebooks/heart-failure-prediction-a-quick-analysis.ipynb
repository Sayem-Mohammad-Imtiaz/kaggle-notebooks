{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first public Notebook. I'm still learning all the in-and-outs of ML and creating/sharing notebooks.\nAll constructive feedback would be greatly appreciated.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data discovery","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's first load the data into a Pandas Dataframe","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nrandom_seed = 297\n\nimport os\nclinical_data_filepath = \"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\"\nclinical_data = pd.read_csv(clinical_data_filepath)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Pandas Profiling we analyze the data to check if all data is nicely behaving, and already check for possible correlations with the predictor","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas_profiling\npandas_profile = pandas_profiling.ProfileReport(clinical_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pandas_profile.to_widgets()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Based on the pandas profiling, we have several boolean columns which are not yet recognized as such (for example anaemia). We will convert these to booleans.\nIn addition, we can already see that \"Time\" seems to possive a significant negative correlation with \"Death_event\". We expect this will serve as a good predictor compared to the other features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data = clinical_data.astype({'anaemia': 'bool', 'diabetes': 'bool', 'high_blood_pressure':'bool', 'smoking':'bool'})\nclean_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we'll separate into a training/test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split into X and y dataset\ny = clean_data.DEATH_EVENT\nall_features = ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes','ejection_fraction', 'high_blood_pressure', 'platelets','serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']\nfeatures = all_features #For this initial version, we'll just take all features for input. Due to the low amount of samples compared to features, we do fear for overfitting\nX = clean_data[features]\n\n# Create train/test set\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=random_seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models\n\nWe'll use a RandomForestClassifier for the actual prediction model. We propose this model due to the low sample size and several boolean features (which mean \"easy\" yes/no decision leaves).\nFor showing the importance of each feature we'll also train a XGBoost model and plot the importance of each of the features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# RandomForestClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# Specify Model\nhearth_failure_RFCmodel = RandomForestClassifier(random_state=random_seed)\n# Fit Model\nhearth_failure_RFCmodel.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_roc_curve\nplot_roc_curve(hearth_failure_RFCmodel, val_X, val_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\n\n# Specify Model\nhearth_failure_XGBmodel = xgboost.XGBClassifier(random_state=random_seed)\n# Fit Model\nhearth_failure_XGBmodel.fit(train_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_roc_curve\nplot_roc_curve(hearth_failure_XGBmodel, val_X, val_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the XGBoost model performs slightly worse compared to the RandomForest (I assume due to overfitting considering the small amount of samples), let's use it to take a look at the importance of the different features.\nAlthough there are several downsides to this importance metric and it might be better to use SHAP values (see [TDS SHAP article](https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d)), it does give us an easy quick overview.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost.plot_importance(hearth_failure_XGBmodel)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, this is aligned with our initial thinking that \"Time\" has a big correlation with \"Death_event\" and is hence also used as the primary predictor in the model.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}