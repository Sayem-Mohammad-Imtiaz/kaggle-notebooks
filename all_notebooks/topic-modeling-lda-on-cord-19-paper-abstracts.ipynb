{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook we aim to get a high-level picture of CORD-19 by (1) training an unsupervised topic model (LDA) on paper abstracts and (2) printing questions asked by the authors in the abstracts of the papers. "},{"metadata":{},"cell_type":"markdown","source":"# Training LDA on paper abstracts"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport json\nimport time\nimport warnings \nwarnings.filterwarnings('ignore')\nimport datetime\n\ndf = pd.read_csv('/kaggle/input/CORD-19-research-challenge/2020-03-13/all_sources_metadata_2020-03-13.csv')\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col='abstract'\nkeep = df.dropna(subset=[col])\nprint(keep.shape)\ndocs = keep[col].tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenize the documents.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code adaptead from https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim.corpora import Dictionary\n\ntokenizer = RegexpTokenizer(r'\\w+')\nfor idx in range(len(docs)):\n    # Convert to lowercase.\n    docs[idx] = docs[idx].lower()  \n    # Split into words.\n    docs[idx] = tokenizer.tokenize(docs[idx])  \n\n# Remove numbers\ndocs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n\n# Remove one-character words\ndocs = [[token for token in doc if len(token) > 1] for doc in docs]\n\n# Remove stopwords \nstop_words = stopwords.words(\"english\")\ndocs = [[token for token in doc if token not in stop_words] for doc in docs]\n\n# Lemmatize\nlemmatizer = WordNetLemmatizer()\ndocs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n\n# Create a dictionary representation of the documents\ndictionary = Dictionary(docs)\n\n# Filter out words that occur less than 20 documents, or more than 50% of the documents\ndictionary.filter_extremes(no_below=20, no_above=0.5)\n\n# Create Bag-of-words representation of the documents\ncorpus = [dictionary.doc2bow(doc) for doc in docs]\n\nprint('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train LDA model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import LdaModel, LdaMulticore\n\n# Set training parameters.\nnum_topics = 10\n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\nmodel = LdaMulticore(\n    corpus=corpus,\n    id2word=id2word,\n    chunksize=2000,\n    eta='auto',\n    iterations=10,\n    num_topics=num_topics,\n    passes=10,\n    eval_every=None,\n    workers=4\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Print Topics"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_topics = model.top_topics(corpus) \nfor i, (topic, sc) in enumerate(top_topics): \n    print(\"\\nTopic {}: \".format(i) + \", \".join([w for score,w in topic]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Print Questions Asked in the Abstracts of Papers from 2020"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_to_sentences(text):\n    return re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"has_question = keep[keep.abstract.dropna().map(lambda x: '?' in x)]\nhas_question.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.publish_time.dropna().map(lambda x: x.split()[0].split('-')[0]).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, row in has_question.dropna(subset=['publish_time']).iterrows():\n    if not '2020' in row.publish_time:\n        continue\n    print(\"\\nTITLE: {}\".format(row.title))\n    print(\"\\tDATE:  {}\".format(row.publish_time))\n    for sent in split_to_sentences(row.abstract):\n        if sent[-1] == '?':\n            print(\"Q:\\t{}\".format(sent))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}