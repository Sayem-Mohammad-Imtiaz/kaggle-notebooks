{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"font-family: Helvetica, fantasy; line-height: 1.3; font-size: 26px; letter-spacing: 3px; text-align: center; color: #99e600\">Prediction of Cars prices using Linear Regressors & Ensemble methods</p>\n\n![](https://www.newneuromarketing.com/media/zoo/images/NNM-2015-019-Cost-consciousness-increase-product-sales-with-Price-Primacy_6a73d15598e2d828b0e141642ebb5de3.png)","metadata":{}},{"cell_type":"markdown","source":"# Beginning libraries üìö","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nimport numpy as np","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the data & gathering short information üìñ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/cars-dataset-audi-bmw-ford-hyundai-skoda-vw/cars_dataset.csv')\nprint(df.shape)\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categoricals = list(df.select_dtypes('object').columns)\nnumericals = [col for col in df.columns if col not in categoricals]\nprint(categoricals)\nprint(numericals)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turn 'object' into category for less memory usage.\ndf[categoricals] = df[categoricals].astype('category')\ndf[categoricals].dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA) üß≠","metadata":{}},{"cell_type":"code","source":"# Relationships between numerical features\nsns.pairplot(df, corner=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in df[categoricals]:\n    print(f'We have {len(df[col].unique())} unique values in --{col}-- column: {df[col].unique()}', '\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's induce some categorical variables on our relationships --- transmission\nsns.pairplot(df, hue='transmission', corner=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's induce some categorical variables on our relationships --- fuelType\nsns.pairplot(df, hue='fuelType', corner=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's induce some categorical variables on our relationships --- Make\nsns.pairplot(df, hue='Make', corner=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore categorical features üìä","metadata":{}},{"cell_type":"code","source":"# Count plots for 'transmission', 'fuelType', 'Make'.\nx=0\nfig=plt.figure(figsize=(20,10))\nplt.subplots_adjust(wspace = 0.5)\nplt.suptitle(\"Count of 'transmission', 'fuelType', and 'Make'\", x=0.4 ,y=0.95, family='Sherif', size=18, weight='bold')\nfor i in df[categoricals[1:]]:\n    ax = plt.subplot(241+x)\n    ax = sns.countplot(data=df, y=i, color='#a6ff4d')\n    plt.grid(axis='x')\n    x+=1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = ['#101907', '#314c17', '#63992e', '#95e545', '#aeff5e', '#c0ff82', '#dbffb7']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of 'transmission' by  'Make'\nfig=plt.figure(figsize=(15,8))\nplt.suptitle(\"Count of 'transmission' by 'Make'\", x=0.5 ,y=0.92, family='Sherif', size=18, weight='bold')\nsns.countplot(data=df, x='transmission', hue='Make', palette=colors)\nplt.grid(axis='y')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count of 'fuelType' by 'Make'\nfig=plt.figure(figsize=(15,8))\nplt.suptitle(\"Count of 'fuelType' by 'Make'\", x=0.5 ,y=0.92, family='Sherif', size=18, weight='bold')\nsns.countplot(data=df, x='fuelType', hue='Make', palette=colors)\nplt.grid(axis='y')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Explore numerical features üìà","metadata":{}},{"cell_type":"code","source":"# Variance of numerical features\ndf.var()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation matrix\ncorr = df.corr()\n\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)]=True\nwith sns.axes_style('white'):\n    fig, ax = plt.subplots(figsize=(18,10))\n    sns.heatmap(corr,  mask=mask, annot=True, cmap=colors, center=0, square=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - Moderate positive correlation between price & engineSize\n - Moderate negative correlation between year & mileage","metadata":{}},{"cell_type":"code","source":"# Show spines (black border of the plot)\nplt.rcParams['axes.spines.left'] = True\nplt.rcParams['axes.spines.right'] = True\nplt.rcParams['axes.spines.top'] = True\nplt.rcParams['axes.spines.bottom'] = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of numerical features\nx=0\nfig=plt.figure(figsize=(15,10),constrained_layout =True)\nplt.subplots_adjust(wspace = 0.5)\nplt.suptitle(\"Distribution of numerical variables\",y=0.95, family='Sherif', size=18, weight='bold')\nfor i in df[numericals]:\n    ax = plt.subplot(231+x)\n    ax = sns.histplot(data=df, x=i, bins=20, color='#a6ff4d')\n    x+=1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investigate both categorical & numerical features ü§ù\n","metadata":{}},{"cell_type":"markdown","source":"Answering some questions:\n\n - **What is the mean price of a car by its 'Make'?**\n\n - **What is the mean price of a car by its 'transmission'?**\n\n - **What is the mean price of a car by its 'fuelType'?**","metadata":{}},{"cell_type":"code","source":"# Hide spines (black border of the plot)\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mp_make = df.groupby('Make')['price'].mean().sort_values()\nmp_transmission = df.groupby('transmission')['price'].mean().sort_values()\nmp_fueltype = df.groupby('fuelType')['price'].mean().sort_values()\n\n\nfig, ax = plt.subplots(1,3, figsize=[15,7], constrained_layout=True)\nplt.suptitle(\"Mean price of a car by certain feature\",y=1.15, family='Sherif', size=18, weight='bold')\n\n# First plot\nvals_0 = [round(i) for i in mp_make]\nax[0].barh(mp_make.index, mp_make, color = '#95e545')\nax[0].set_title(\"Make\")\nax[0].set_xticks([])\nfor index, value in enumerate(vals_0):\n    ax[0].text(value, index, str(value))\n\n# Second plot\nvals_1 = [round(i) for i in mp_transmission]\nax[1].barh(mp_transmission.index, mp_transmission, 0.45, color = '#c0ff82')\nax[1].set_title(\"Transmission\")\nax[1].set_xticks([])\nfor index, value in enumerate(vals_1):\n    ax[1].text(value, index, str(value))\n\n# Third plot\nvals_2 = [round(i) for i in mp_fueltype]\nax[2].barh(mp_fueltype.index, mp_fueltype, 0.6, color = '#537f26')\nax[2].set_title(\"fuelType\")\nax[2].set_xticks([])\nfor index, value in enumerate(vals_2):\n    ax[2].text(value, index, str(value))\n\n# Layout spacing\nfig.set_constrained_layout_pads(w_pad=2 / 72, h_pad=2 / 72, hspace=0.2,\n                                wspace=0.2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above approach can be applied to further investigate with other aggregate functions or other numerical columns such as: 'mileage', 'tax', 'mpg' or 'engineSize'.","metadata":{}},{"cell_type":"markdown","source":"**Explore 'mileage' on 'year'**","metadata":{}},{"cell_type":"code","source":"# Show spines (black border of the plot)\nplt.rcParams['axes.spines.left'] = True\nplt.rcParams['axes.spines.right'] = True\nplt.rcParams['axes.spines.top'] = True\nplt.rcParams['axes.spines.bottom'] = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig, axes = plt.subplots(4, 1, figsize=(18, 12), constrained_layout =True)\n\n# First plot\nax = sns.lineplot(ax = axes[0], data=df, x=\"year\", y='mileage', ci=None)\nax.set_xticks(np.arange(1996, 2020, 1))\nax.set_title(\"Mean 'mileage' by 'year'\")\nax.set_xlim(1996,2020)\n\n# Second plot\nax1 = sns.lineplot(ax = axes[1], data=df, x='year', y='mileage', hue='transmission', ci=None)\nax1.set_xticks(np.arange(1996, 2020, 1))\nax1.set_title(\"Mean 'mileage' by 'year' for each 'transmission'\")\nax1.set_xlim(1996,2020)\n\n# Third plot\nax2 = sns.lineplot(ax = axes[2], data=df, x='year', y='mileage', hue='fuelType', ci=None)\nax2.set_xticks(np.arange(1996, 2020, 1))\nax2.set_title(\"Mean 'mileage' by 'year' for each 'fuelType'\")\nax2.set_xlim(1996,2020)\n\n# Third plot\nax3 = sns.lineplot(ax = axes[3], data=df, x='year', y='mileage', hue='Make', ci=None)\nax3.set_xticks(np.arange(1996, 2020, 1))\nax3.set_title(\"Mean 'mileage' by 'year' for each 'Make'\")\nax3.set_xlim(1996,2020)\nax3.legend(loc='upper right')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bins number preset\nb = 30\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 7), constrained_layout =True)\n\n# First plot\nax = sns.histplot(ax = axes[0,0], data=df, x='price', hue='transmission', element='poly', bins = b)\nax.set_title(\"'price' distribution by 'transmission'\")\n\n# Second plot\nax1 = sns.histplot(ax = axes[0,1], data=df, x='mileage', hue='transmission', element='poly', bins = b, legend=False)\nax1.set_title(\"'mileage' distribution by 'transmission'\")\n\n# Third plot\nax2 = sns.histplot(ax = axes[0,2], data=df, x='year', hue='transmission', element='poly', bins = b, legend=False)\nax2.set_title(\"'year' distribution by 'transmission'\")\n\n\n# Fourth plot\nax3 = sns.histplot(ax = axes[1,0], data=df, x='mpg', hue='transmission', element='poly', bins = b, legend=False)\nax3.set_title(\"'mpg' distribution by 'transmission'\")\n\n# Fifth plot\nax4 = sns.histplot(ax = axes[1,1], data=df, x='engineSize', hue='transmission', element='poly', bins = b, legend=False)\nax4.set_title(\"'engine' distribution by 'transmission'\")\n\n# Sixth plot\nax5 = sns.histplot(ax = axes[1,2], data=df, x='tax', hue='transmission', element='poly', bins = b, legend=False)\nax5.set_title(\"'tax' distribution by 'transmission'\")\n\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split data by 'make'\nbmw = df[df['Make'] == 'BMW']\nford = df[df['Make'] == 'Ford']\nhyundai = df[df['Make'] == 'Hyundai']\naudi = df[df['Make'] == 'audi']\nskoda = df[df['Make'] == 'skoda']\ntoyota = df[df['Make'] == 'toyota']\nvw = df[df['Make'] == 'vw']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bins number & color preset \nb = 40\nc='#a6ff4d'\n\n# Price distribution of each 'Make'\nfig, axes = plt.subplots(3, 3, figsize=(20, 7), constrained_layout =True)\naxes[-1, -1].axis('off') # hide axes\naxes[-1, -2].axis('off') # hide axes\nplt.suptitle(\"Price distribution for each 'Make'\",y=1.15, family='Sherif', size=18, weight='bold')\n\n# First plot\nax = sns.histplot(ax = axes[0,0], data=bmw, x='price', element='poly', bins = b, color = c)\nax.set_title(\"BMW\")\n\n# Second plot\nax = sns.histplot(ax = axes[0,1], data=ford, x='price', element='poly', bins = b, color = c)\nax.set_title(\"Ford\")\n\n# Third plot\nax = sns.histplot(ax = axes[0,2], data=hyundai, x='price', element='poly', bins = b, color = c)\nax.set_title(\"Hyundai\")\n\n# Fourth plot\nax = sns.histplot(ax = axes[1,0], data=audi, x='price', element='poly', bins = b, color = c)\nax.set_title(\"Audi\")\n\n# Fifth plot\nax = sns.histplot(ax = axes[1,1], data=skoda, x='price', element='poly', bins = b, color = c)\nax.set_title(\"Skoda\")\n\n# Sixth plot\nax = sns.histplot(ax = axes[1,2], data=toyota, x='price', element='poly', bins = b, color = c)\nax.set_title(\"Toyota\")\n\n# Seventh plot\nax = sns.histplot(ax = axes[2,0], data=vw, x='price', element='poly', bins = b, color = c)\nax.set_title(\"VW\")\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To investigate the distribution other numerical features for each 'Make', the above approach can be applied by changing the 'x' of each plot to a different numerical variable.","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning üßπ","metadata":{}},{"cell_type":"code","source":"transmission_other = df[df['transmission'] == 'Other']\nprint(transmission_other)\n\n# Replace 'other' with most frequent unique ('Manual') since  it only contains only 4 rows (I consider not enough information for the model)\ndf['transmission'] = df['transmission'].replace('Other', 'Manual')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"electric_fuel = df[df['fuelType'] == 'Electric']\nprint(electric_fuel)\n\nother_fuel = df[df['fuelType'] == 'Other']\nprint(other_fuel)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign 'Electric' fuelType  unique  to 'Other' since  it only contains only 5 rows (I consider not enough information for the model).\ndf['fuelType'] = df['fuelType'].replace('Electric', 'Other')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are several values of 'mpg' higher than 400, and seems to appear as outliers if we check the scatterplots.\n\nHowever, these values seems to be very explainable for the BMW 'Make', since the values are for these cars. Considering that, the values will be kept.","metadata":{}},{"cell_type":"markdown","source":"# Feature engineering ‚öôÔ∏è","metadata":{}},{"cell_type":"code","source":"# Turn categoricals into numeric\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfor c in categoricals:\n    df[c] = le.fit_transform(df[c])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert our categorical columns to dummies\nfor col in categoricals:\n    dumm = pd.get_dummies(df[col], prefix = str(col)+'_', dtype=int)\n    df = pd.concat([df,dumm], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the original categories since we one hot encoded them\ndf.drop(categoricals, axis=1, inplace=True)\ndf.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation üç≥","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Separate features & target\ny = df['price']\nX = df.drop('price', axis = 1)\nprint(X.shape, y.shape)\nprint('\\n')\n\n# Standardize features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n\n# Split into training (80%) and testing set (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling üèóÔ∏è","metadata":{}},{"cell_type":"markdown","source":"# Linear regressors","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, ElasticNet, Lars, Lasso, BayesianRidge, HuberRegressor\nfrom sklearn.metrics import mean_squared_error\n\nr_squared = []\nrmses = []\n \nlin_reg = [('LR', LinearRegression()), ('Ridge', Ridge()), ('SGDR', SGDRegressor()), \n            ('ElasticNet', ElasticNet()), ('Lars', Lars()), ('Lasso', Lasso()),\n            ('BayesianRidge', BayesianRidge()), ('HuberRegressor', HuberRegressor())] \n\n\nfor name, model in lin_reg:\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, preds))\n    rs = model.score(X_test, y_test)\n    r_squared.append(rs)\n    rmses.append(rmse)\n    print(f'The accuracy of {name} is {rmse:.3f}')\n    print(f'The R^2 of {name} is {rs:.3f}')\n    print('\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dataframe that contains relevant performances of the linear regressors\nmodels = [name for name, model in lin_reg]\n\n# Exclude worst models & their performances\nremove_indices = [0,2,4]\nmodels = [i for j, i in enumerate(models) if j not in remove_indices]\nr_squared = [i for j, i in enumerate(r_squared) if j not in remove_indices]\nrmses =  [i for j, i in enumerate(rmses) if j not in remove_indices]\n\nscores_lin_reg = pd.DataFrame({'Model': models, 'Test_R^2': r_squared, 'Test_RMSE': rmses})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hide spines (black border of the plot)\nplt.rcParams['axes.spines.left'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.bottom'] = False\n\n# Plot linear regressors performance\nfig, axes = plt.subplots(1, 2, figsize=(15, 5), constrained_layout =True)\nplt.suptitle(\"Linear Regressors performance\", x=0.5 ,y=1.15, family='Sherif', size=18, weight='bold')\n\nax = sns.barplot(ax = axes[0], data=scores_lin_reg.sort_values('Test_RMSE'), x='Model', y='Test_RMSE', palette=colors)\nax.set_title('Root Mean Squared Error')\nax.grid(axis='y')\n\nax1 = sns.barplot(ax = axes[1], data=scores_lin_reg.sort_values('Test_R^2', ascending = False), x='Model', y='Test_R^2', palette=colors)\nax1.set_title('R^2')\nax1.grid(axis='y')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can be observed there are 3 models with similar results.\n\nHowever, the one that slighty wins, is the BayesianRidge Regressor:\n - RMSE: 3104.238\n - R^2: 0.891\n\n Let's try other modeling approaches.","metadata":{}},{"cell_type":"markdown","source":"# Ensemble methods ","metadata":{}},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nSEED = 123\nrf = RandomForestRegressor(random_state=SEED)\nrf.fit(X_train, y_train)\npreds = rf.predict(X_test)\nrmse_rf = np.sqrt(mean_squared_error(y_test, preds))\nrs_rf = rf.score(X_test, y_test)\n\nprint(f'Random Forest RMSE is: {rmse_rf}')\nprint(f'Random Forest R^2 is: {rs_rf}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Boosting","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor(random_state=SEED)\ngb.fit(X_train, y_train)\npreds = gb.predict(X_test)\nrmse_gb = np.sqrt(mean_squared_error(y_test, preds))\nrs_gb = gb.score(X_test, y_test)\n\nprint(f'Gradient Boosting RMSE is: {rmse_gb}')\nprint(f'Gradient Boosting R^2 is: {rs_gb}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extreme Gradient Boosting (XGBoost)","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\nxgb_reg = xgb.XGBRegressor(seed=SEED)\nxgb_reg.fit(X_train, y_train)\npreds = xgb_reg.predict(X_test)\nrmse_xgb = np.sqrt(mean_squared_error(y_test, preds))\nrs_xgb = xgb_reg.score(X_test, y_test)\n\nprint(f'XGBost RMSE is: {rmse_xgb}')\nprint(f'XGBoost R^2 is: {rs_gb}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create dataframe with ensemble methods performances\nmodels_en = ['RandomForest', 'GradientBoosting', 'XGBoost']\nrmses_en = [rmse_rf, rmse_gb, rmse_xgb]\nr_squared_en = [rs_rf, rs_gb, rs_xgb]\n\nscores_en = pd.DataFrame({'Model': models_en, 'Test_R^2': r_squared_en, 'Test_RMSE': rmses_en})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot ensemble methods performance\nfig, axes = plt.subplots(1, 2, figsize=(15, 5), constrained_layout =True)\nplt.suptitle(\"Ensemble methods performance\", x=0.5 ,y=1.15, family='Sherif', size=18, weight='bold')\n\nax = sns.barplot(ax = axes[0], data=scores_en.sort_values('Test_RMSE'), x='Model', y='Test_RMSE', palette=colors)\nax.set_title('Root Mean Squared Error')\nax.grid(axis='y')\n\nax1 = sns.barplot(ax = axes[1], data=scores_en.sort_values('Test_R^2', ascending = False), x='Model', y='Test_R^2', palette=colors)\nax1.set_title('R^2r')\nax1.grid(axis='y')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be observed that the ensemble methods perform much better than the linear regressors.\n\nThe best performance is obtained by the RandoMForest:\n - RMSE: 1821.37\n - R^2:  0.96\n\n\n Let's see if these results can be improved.","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameters tuning for Random Forest ‚ú®","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparams_rf = {'n_estimators':[50, 100, 200],\n               'max_depth':[None, 1, 2],\n               'min_samples_leaf':[0.5,1,1.5,2]}\n\ngrid_rf = GridSearchCV(estimator=rf, \n                       param_grid=params_rf,\n                       cv=5, \n                       scoring = 'neg_mean_squared_error',\n                       )\n\ngrid_rf.fit(X_train, y_train)\n\nbest_hyperparams = grid_rf.best_params_\n\nprint(f'The best hyperparameters found for RF are: {best_hyperparams}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"best_rf = grid_rf.best_estimator_\n\npreds = best_rf.predict(X_test)\nrmse_best_rf = np.sqrt(mean_squared_error(y_test, preds))\nrs_best_rf = best_rf.score(X_test, y_test)\n\nprint(f'Best RF RMSE is: {rmse_best_rf}')\nprint(f'Best RF R^2 is: {rs_best_rf}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The performance of the RandomForest Regressor was slightly improved.\n\nThe RMSE was reduced to 1814.25","metadata":{}}]}