{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import re    # for regular expressions \nimport nltk  # for text manipulation \nimport string \nimport warnings \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt  \npd.set_option(\"display.max_colwidth\", 200) \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \ntrain  = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/train.csv') \ntest = pd.read_csv('../input/twitter-sentiment-analysis-hatred-speech/test.csv')\n\ntrain[train['label'] == 1].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"length_train = train['tweet'].str.len() \nlength_test = test['tweet'].str.len() \nplt.hist(length_train, bins=20, label=\"train_tweets\") \nplt.hist(length_test, bins=20, label=\"test_tweets\") \nplt.legend() \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combi = train.append(test, ignore_index=True) \ncombi.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n    return input_txt   \n\ncombi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\") \ncombi.head()\n\ncombi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \") \ncombi.head(10)\n\ncombi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ncombi.head(10)\n\ntokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \ntokenized_tweet.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.porter import * \nstemmer = PorterStemmer() \ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\ntokenized_tweet.head()\n\nfor i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])    \ncombi['tidy_tweet'] = tokenized_tweet\n\ncombi.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport gensim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Text features can be:\n1. Bag of words\n2. TF-IDF\n3. Word embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow = bow_vectorizer.fit_transform(combi['tidy_tweet'])\nbow.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \nmodel_w2v = gensim.models.Word2Vec(\n            tokenized_tweet,\n            size=200, # desired no. of features/independent variables\n            window=5, # context window size\n            min_count=2,\n            sg = 1, # 1 for skip-gram model\n            hs = 0,\n            negative = 10, # for negative sampling\n            workers= 2, # no.of cores\n            seed = 34) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_w2v.wv.most_similar(positive=\"dinner\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_w2v.wv.most_similar(positive=\"trump\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_w2v['india']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extracting train and test BoW features \ntrain_bow = bow[:31962,:] \ntest_bow = bow[31962:,:] \n# splitting data into training and validation set \nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'],random_state=42,                                                           test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lreg = LogisticRegression() \n# training the model \nlreg.fit(xtrain_bow, ytrain) \nprediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set \nprediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0 prediction_int = prediction_int.astype(np.int) \nf1_score(yvalid, prediction_int) # calculating f1 score for the validation set","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}