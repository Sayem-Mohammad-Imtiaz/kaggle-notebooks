{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the StarDataset\n\nIn this notebook, I will try to demonstrate how to preprocess this messy Star Dataset.\n\nhttps://www.kaggle.com/vinesmsuic/star-categorization-giants-and-dwarfs","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Here are some useful info you might want to look at before playing with this notebook:\n\n* [Data Cleaning course on Kaggle Learn](https://www.kaggle.com/alexisbcook/handling-missing-values)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Take a first look at the data\nThe first thing we'll need to do is\n* Import Libraries\n* Check the files we have\n* Load the raw dataset","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Use pd.read_csv to read file\npath = \"../input/star-categorization-giants-and-dwarfs/Star99999_raw.csv\"\nraw_data = pd.read_csv(path)\n\nraw_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"^We can see the dataset included duplicated Index column, so we need to remove it later.\n\nLets read some statistics of the dataset first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `Vmag` - Visual Apparent Magnitude of the Star \n* `Plx` - Distance Between the Star and the Earth \n* `e_Plx` - Standard Error of `Plx` (Drop the Row if you find the e_Plx is too high!)\n* `B-V` - B-V color index. (A hot star has a B-V color index close to 0 or negative, while a cool star has a B-V color index close to 2.0. Other stars are somewhere in between.)\n* `SpType` -  [Stellar classification.](https://en.wikipedia.org/wiki/Stellar_classification) (Roman Numerals &gt;IV are giants. Otherwise are dwarfs) \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# read some statistics of the dataset\nraw_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"^Why the describe function is not returning summary of all columns?\n\nIt is probably because this dataframe has mixed column types. The default behavior of pandas describe function is to only provide a summary for the numerical columns.\n\nhttps://stackoverflow.com/questions/24524104/pandas-describe-is-not-returning-summary-of-all-columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the DataType of our dataset\nraw_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"^As we can see, Both `Vmag`, `Plx`, `e_Plx`,`B-V` are marked as object, but they were supposed to be float value.\n\nThen we should convert our columns to numeric.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"If you run this code:\n```python\n# Convert Columns data type to float values\nraw_data[\"Vmag\"] = pd.to_numeric(raw_data[\"Vmag\"], downcast=\"float\")\nraw_data[\"Plx\"] = pd.to_numeric(raw_data[\"Plx\"], downcast=\"float\")\nraw_data[\"e_Plx\"] = pd.to_numeric(raw_data[\"e_Plx\"], downcast=\"float\")\nraw_data[\"B-V\"] = pd.to_numeric(raw_data[\"B-V\"], downcast=\"float\")\n```\nError would Occur : `ValueError: Unable to parse string \"     \" at position 25189`\n\nFrom Above Error, we can observe that some cells were filled with whitespaces, so they become unable to parse.\n\nWe can add a parameter `errors='coerce'` to force the function to convert bad non-numeric values to NaN.\n\nhttps://stackoverflow.com/questions/40790031/pandas-to-numeric-find-out-which-string-it-was-unable-to-parse","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert Columns data type to float values\nraw_data[\"Vmag\"] = pd.to_numeric(raw_data[\"Vmag\"], downcast=\"float\", errors='coerce')\nraw_data[\"Plx\"] = pd.to_numeric(raw_data[\"Plx\"], downcast=\"float\", errors='coerce')\nraw_data[\"e_Plx\"] = pd.to_numeric(raw_data[\"e_Plx\"], downcast=\"float\", errors='coerce')\nraw_data[\"B-V\"] = pd.to_numeric(raw_data[\"B-V\"], downcast=\"float\", errors='coerce')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's check the info again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the DataType of our dataset\nraw_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Actually , if you want to show all the columns you can add parameter `include='all'`.\nraw_data.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking Missing data\nIt is very common that a dataset have some missed values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the number of missing data points per column\nmissing_values_count = raw_data.isnull().sum()\n\nmissing_values_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many total missing values do we have?\ntotal_cells = np.product(raw_data.shape)\ntotal_missing = missing_values_count.sum()\n\n# percentage of data that is missing\npercent_missing = (total_missing/total_cells)\nprint(\"Percentage Missing:\", \"{:.2%}\".format(percent_missing))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"^From the percentage of missing data, since it is so small (only 0.7%), we can just drop the rows.\n\nLet's see what will happen if we remove all the rows that contain a missing value.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dropping Missing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove all the rows that contain a missing value\n# better to store it into a new variable to avoid confusion\nraw_data_na_dropped = raw_data.dropna() \n\nraw_data_na_dropped","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# just how much rows did we drop?\ndropped_rows_count = raw_data.shape[0]-raw_data_na_dropped.shape[0]\nprint(\"Rows we dropped from original dataset: %d \\n\" % dropped_rows_count)\n\n# Percentage we dropped\npercent_dropped = dropped_rows_count/raw_data.shape[0]\nprint(\"Percentage Loss:\", \"{:.2%}\".format(percent_dropped))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lastly, read the statistics and info again.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data_na_dropped.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"^Oh, almost forgot, we need to drop the first column of dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dropping Unwanted Column\n\nThis Stackoverflow link will give you idea of how to drop a column from pandas dataframe.\nhttps://stackoverflow.com/questions/13411544/delete-column-from-pandas-dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#The best way to do this in pandas is to use drop:\nraw_data_na_dropped = raw_data_na_dropped.drop('Unnamed: 0', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data_na_dropped.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data_na_dropped.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"^We noticed the Int64Index have 96742 entries, but only 0 to 99998.\n\nTherefore we need to reindex our dataframe.\n\nhttps://stackoverflow.com/questions/40755680/how-to-reset-index-pandas-dataframe-after-dropna-pandas-dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data_na_dropped_reindex = raw_data_na_dropped.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_data_na_dropped_reindex.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like we have finally cleaned out all the missing values!\n\nFinally, we can save our progress to a csv file.\n* Remember to use `index=False` if you don't want to create separate column of indexes again!\n\nhttps://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html\nhttps://stackoverflow.com/questions/20845213/how-to-avoid-python-pandas-creating-an-index-in-a-saved-csv","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Optional - Save our progress\n#raw_data_na_dropped_reindex.to_csv(\"Star99999_na_dropped.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating New Column for Amag","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Finding Absolute Magnitude\n\nThe absolute magnitude of the stars were generated via the equation:\n![](https://i.imgur.com/tt1h8bu.png)\nWhere $M$ represents the absolute magnitude `Amag`, \n$m$ represents the visual apparent magnitude `Vmag`\nand $p$ represents stellar parallax `Plx`.\n\n\nIn this session, we will create a new column `Amag` to store $M$.\n\nThings need to be aware:\n* Taking log of 0 would result in a infinity, which is what we dont want to see\n  * To fix this: Dropping rows with `Plx` = 0\n* Taking log of -ve numbers would result complex numbers, which is what we dont want to see too.\n  * To fix this: Taking Absolute value of `Plx`\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save a copy, so I can call it in a easier way\ndf = raw_data_na_dropped_reindex.copy()\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping rows that `Plx` = 0\ndf = df[df.Plx != 0]\n\n#Reindex the dataframe\ndf = df.reset_index(drop=True)\n\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like we successfully dropped all the rows that `Plx` = 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Implement the equation\ndf[\"Amag\"] = df[\"Vmag\"] + 5* (np.log10(abs(df[\"Plx\"]))+1)\n\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Column Mapping\n\nIn this session, we will create a new column `TargetClass` to store whether it is a Giant or Dwarf.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Convert SpType into Giants and Dwarf\n\n* Roman Numerals >IV are giants. Otherwise are dwarfs\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at our SpType column\ndf['SpType']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Copy the SpType column to a new column called TargetClass\ndf['TargetClass'] = df['SpType']\n\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the SpType contains the Roman Numerals we need.\n\n> Best Practice is to use Regex (Regular Expression) to extract the pattern.\n\nBut I will try to use an intuitive approach first.\n\nI just took a look from the dataset, the Roman Numeral contains: I, II, III, IV, V, VI, VII in the string.\nTherefore:\n\n* Dwarfs (I, II, III, VII)\n* Giants (IV, V, VI)\n* Other Special Stars (None)\n\n^Edit0821: Sorry I made a huge mistake here... Now fixed!\n\nFor the First Character of the String, we don't need to worry there will be I or V.\n![](https://cdn.britannica.com/17/143617-050-6042AB2A/diagram-Hertzsprung-Russell-Annie-Jump-Cannon-type-order.jpg)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#The intuitive approach (Could take a long time if you have a huge dataset)\nfor i in range(len(df['TargetClass'])):\n    if \"V\" in df.loc[i,'TargetClass']: \n        if \"VII\" in df.loc[i,'TargetClass']: \n            df.loc[i,'TargetClass'] = 0 # VII is Dwarf\n        else:\n            df.loc[i,'TargetClass'] = 1 # IV, V, VI are Giants\n    elif \"I\" in df.loc[i,'TargetClass']: \n        df.loc[i,'TargetClass'] = 0 # I, II, III are Dwarfs\n    else: \n        df.loc[i,'TargetClass'] = 9 # None\n        \ndf['TargetClass']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"^When we use the data to analysis, the label is better in numeric values otherwise we might need to map them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save our progress\n#df.to_csv(\"Star99999_preprocessed0821.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Balancing Data\nAlmost forgot, we need to balance the data.\n\nThis post will give you the idea of why do we need to balance the data.\n<br>\nhttps://elitedatascience.com/imbalanced-classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['TargetClass'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt # plot graphs\nimport seaborn as sns # plot graphs\n\nsns.countplot(df['TargetClass'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We only need the Dwarfs and Giants Record.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping rows that `TargetClass` = 9\ndf = df[df.TargetClass != 9]\n\n#Reindex the dataframe\ndf = df.reset_index(drop=True)\n\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have so many records, we will just downsample the majority class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate the labels\ndf_giants = df[df.TargetClass == 1]\ndf_dwarfs = df[df.TargetClass == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numbers of rows of Giants and Dwarfs\nnum_of_giant = df_giants.shape[0]\nnum_of_dwarf = df_dwarfs.shape[0]\nprint(\"Giants(1):\",num_of_giant)\nprint(\"Dwarfs(0):\",num_of_dwarf)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To downsample the class, we can just use a loop to loop through the records, but there is a way better approach.\n\nLet's import `resample` from `sklearn`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import resample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Downsample majority class\ndf_giants_downsampled = resample(df_giants, \n                                 replace=False,    # sample without replacement\n                                 n_samples=num_of_dwarf,     # to match minority class\n                                 random_state=1) # reproducible results\n \n# Combine minority class with downsampled majority class\ndf_downsampled = pd.concat([df_giants_downsampled, df_dwarfs])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_downsampled['TargetClass'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df_downsampled['TargetClass'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our Dataset is finally balanced!\n![](https://i.imgflip.com/303krn.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" Last but not Least, we need to check our dataset to see whether there are still some problem.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_downsampled.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_downsampled.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yeah, Reindex.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_balanced = df_downsampled.reset_index(drop=True)\n\ndf_balanced.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_balanced","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Did you notice? Since we concat the 2 Dataframes, we need to shuffle our data before feeding them to a model.\n\n> Pandas has a shuffle method called `sample`.\n> You can also use sklearn to shuffle if you want to.\n\nhttps://stackoverflow.com/questions/29576430/shuffle-dataframe-rows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_balanced = df_balanced.sample(frac=1).reset_index(drop=True)\n\ndf_balanced","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally done!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save our dataset, we can finally play with it!!!\ndf_balanced.to_csv(\"Star39552_balanced.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can use this link to check more notebooks about this dataset:\n\nhttps://www.kaggle.com/vinesmsuic/star-categorization-giants-and-dwarfs/notebooks\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}