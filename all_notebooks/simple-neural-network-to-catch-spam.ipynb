{"cells":[{"metadata":{"colab_type":"text","id":"rq40x61v485z","_uuid":"f17eca935ae49f84d84e9e63aade09bdc2fa3a99"},"cell_type":"markdown","source":"# First try to learn Logistic Regression with Neutral Network\n\n        This is my implement of logistic regression with a neutral network. \n\n### Math and materials \n\n   All materials come from Andrew Y. Ng course.\n   \n   [COURSE](www.coursera.org/specializations/deep-learning)\n    \n### Dataset\n\n   This is a dataset with sms spam collection from Kaggle competition \n   \n   [LINK](https://www.kaggle.com/uciml/sms-spam-collection-dataset/data)\n   The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\n   ","outputs":[],"execution_count":null},{"metadata":{"colab_type":"text","id":"mzcDjm7R4854","_uuid":"9b194f4ebd41f41d18f96df7274b02f7a6c692c8"},"cell_type":"markdown","source":"## 1. Overview ##\n\nFirst take a look on dataset, convert words to numerical expression\nand change classifier to 0 when SMS is spam, 1 when SMS is ham.\n","outputs":[],"execution_count":null},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","id":"eZDz3eFe4859","trusted":true,"_uuid":"ce7d95d7735b11742317541c59c1d92351ea4e8f"},"cell_type":"code","source":"# import useful package\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd \nimport matplotlib.pyplot as plt \nfrom keras.preprocessing import sequence\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":206,"output_extras":[{"item_id":1}]},"colab_type":"code","executionInfo":{"elapsed":774,"status":"ok","timestamp":1518826821766,"user":{"displayName":"Rafał B.","photoUrl":"//lh4.googleusercontent.com/-KErhzOo-dPk/AAAAAAAAAAI/AAAAAAAAAM4/esfarWimf4Q/s50-c-k-no/photo.jpg","userId":"104824888809991073855"},"user_tz":-60},"id":"Wp-XutSC486U","outputId":"0acbc89b-66bf-4497-cdc3-bac3fee1f929","trusted":true,"_uuid":"9a32a6f557b6307cea25dc1d768a55c3165dcc48"},"cell_type":"code","source":"df = pd.read_csv('../input/spam.csv', encoding= \"ISO-8859-1\")\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"BllEfOtQ486h","_uuid":"b7e8c3daa15074f46967b9c055c3cd2cf104f7a7"},"cell_type":"markdown","source":"## 2. Clean data ##\n\n1. Delete unnecessary columns\n2. Change ham/spam to binary class\n3. Convert chars into integers and select length of message","outputs":[],"execution_count":null},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"SEEZQi8s486k","trusted":true,"_uuid":"7ad70c93d97c50ad5ffc50a6831a0557af11b1b6"},"cell_type":"code","source":"# Delete columns \ndf = df.iloc[:,:2]","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"w_fPInF_486s","trusted":true,"_uuid":"70656477610f9b37396e105b84c4feafa7a0a77e"},"cell_type":"code","source":"# Change ham/spam to binary class\ndf.columns = [\"Class\",\"Text\"]\ndf['Class_bin'] = pd.factorize(df['Class'])[0]","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"MloxWbwi486y","trusted":true,"_uuid":"75f945b4bf4fd45a7ab0a89f9417164e620efa27"},"cell_type":"code","source":"# Convert chars into integers\n\ndef toInt(row):\n    xn = [ord(x) for x in row['Text']]\n    return xn\n\ndf['Text_int'] = df.apply(toInt, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"pVMZtodh4866","trusted":true,"_uuid":"837c4207ccd756f75c6ebbcea98ec09b1e115c42"},"cell_type":"code","source":"df['Amount_of_char'] = df['Text_int'].map(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":206,"output_extras":[{"item_id":1}]},"colab_type":"code","executionInfo":{"elapsed":638,"status":"ok","timestamp":1518826827522,"user":{"displayName":"Rafał B.","photoUrl":"//lh4.googleusercontent.com/-KErhzOo-dPk/AAAAAAAAAAI/AAAAAAAAAM4/esfarWimf4Q/s50-c-k-no/photo.jpg","userId":"104824888809991073855"},"user_tz":-60},"id":"Mxm_ckhN487F","outputId":"51cfc801-cf7f-450c-c3e1-119817cc7a36","trusted":true,"_uuid":"7d9e4e4ba4a43ca497de8101b07d89a2da72fd91"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":352,"output_extras":[{"item_id":1},{"item_id":2}]},"colab_type":"code","executionInfo":{"elapsed":636,"status":"ok","timestamp":1518826828568,"user":{"displayName":"Rafał B.","photoUrl":"//lh4.googleusercontent.com/-KErhzOo-dPk/AAAAAAAAAAI/AAAAAAAAAM4/esfarWimf4Q/s50-c-k-no/photo.jpg","userId":"104824888809991073855"},"user_tz":-60},"id":"2SwiQO69487T","outputId":"95458488-9ecb-43df-81e0-13f8dc206e49","trusted":true,"_uuid":"077e41e0df5a54b7d006d305b5afc0e6ba3a6ed3"},"cell_type":"code","source":"plt.hist(df['Amount_of_char'], log=True)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34,"output_extras":[{"item_id":1}]},"colab_type":"code","executionInfo":{"elapsed":674,"status":"ok","timestamp":1518826829790,"user":{"displayName":"Rafał B.","photoUrl":"//lh4.googleusercontent.com/-KErhzOo-dPk/AAAAAAAAAAI/AAAAAAAAAM4/esfarWimf4Q/s50-c-k-no/photo.jpg","userId":"104824888809991073855"},"user_tz":-60},"id":"ErNP2Qv7487j","outputId":"cc94262e-fe2d-4e18-cbbc-cabcd838eb19","trusted":true,"_uuid":"ddbc1e65cd098c9188e8310a89d500a10d9832db"},"cell_type":"code","source":"# Get max element from text\ndf['Max_of'] = df['Text_int'].map(lambda x: max(x))\n\nprint(\"Number of max value: \",df['Max_of'].max())","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"7BW4eI3W487y","trusted":true,"_uuid":"6d9a1f1b7c7fd7eda43a43a50bcdb8fc7514ac4c"},"cell_type":"code","source":"X = df['Text_int'].values\ny = df['Class_bin'].values\n\nfor i in range(len(df)):\n    X[i] = np.asarray(X[i])","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"CMY6axFa4876","trusted":true,"_uuid":"bdaec5973ccbe7a402f6bb313671749a86460702"},"cell_type":"code","source":"y = y.reshape(len(y),1)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34,"output_extras":[{"item_id":1}]},"colab_type":"code","executionInfo":{"elapsed":650,"status":"ok","timestamp":1518826833185,"user":{"displayName":"Rafał B.","photoUrl":"//lh4.googleusercontent.com/-KErhzOo-dPk/AAAAAAAAAAI/AAAAAAAAAM4/esfarWimf4Q/s50-c-k-no/photo.jpg","userId":"104824888809991073855"},"user_tz":-60},"id":"F7HBLeMo488G","outputId":"40dd39f0-698f-4c34-f1b3-29f5f76b746b","trusted":true,"_uuid":"a55934db62139bf3677838a9f4287373fb6c7ce0"},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":52,"output_extras":[{"item_id":1}]},"colab_type":"code","executionInfo":{"elapsed":551,"status":"ok","timestamp":1518826834214,"user":{"displayName":"Rafał B.","photoUrl":"//lh4.googleusercontent.com/-KErhzOo-dPk/AAAAAAAAAAI/AAAAAAAAAM4/esfarWimf4Q/s50-c-k-no/photo.jpg","userId":"104824888809991073855"},"user_tz":-60},"id":"it-Azron488P","outputId":"ac55d985-c63b-4437-b32f-5925d5734a78","trusted":true,"_uuid":"0d5a9538e91f4de84d3829dbf7c660921819a5a1"},"cell_type":"code","source":"#check our message\nprint(''.join([chr(x) for x in X[0]]))\nprint(\"This message is:\", y[0], \"it's\", df['Class'][0], \"message\")","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"oeSAQmVS488X","trusted":true,"_uuid":"1356f2865fc71fa12e824d06dbd8dcdf97be6a57"},"cell_type":"code","source":"# split the data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"hE0iKQHf489G","trusted":true,"_uuid":"48cdc95c0915f96030049d0d50a94d5d315b9373"},"cell_type":"code","source":"# Change length of message to 100 chars\nmax_review_length = 60\nX_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\nX_test = sequence.pad_sequences(X_test, maxlen=max_review_length)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"z4OMJffk489b","trusted":true,"_uuid":"ad4cb167b13980d83d8025f91769a4bad7a75727"},"cell_type":"code","source":"X_train = X_train / 247\nX_test = X_test / 247","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":86,"output_extras":[{"item_id":1}]},"colab_type":"code","executionInfo":{"elapsed":671,"status":"ok","timestamp":1518826838519,"user":{"displayName":"Rafał B.","photoUrl":"//lh4.googleusercontent.com/-KErhzOo-dPk/AAAAAAAAAAI/AAAAAAAAAM4/esfarWimf4Q/s50-c-k-no/photo.jpg","userId":"104824888809991073855"},"user_tz":-60},"id":"qrLseT2T489t","outputId":"4cd41d34-89ef-48ec-cd67-f02bd32830b1","trusted":true,"_uuid":"11ce13ebf715c850c9df7dc39fc973266a536e7d"},"cell_type":"code","source":"#reshape all array\nX_test = X_test.T\nX_train = X_train.T\ny_test = y_test.T\ny_train = y_train.T\n\n\nprint (\"X_train shape: \" + str(X_train.shape))\nprint (\"y_train shape: \" + str(y_train.shape))\nprint (\"X_test shape: \" + str(X_test.shape))\nprint (\"y_test  shape: \" + str(y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"B03TiewP4897","_uuid":"9e829b36de3248333918082072bf2fac62189794"},"cell_type":"markdown","source":"## 3 - General Architecture of the learning algorithm ##\n\n<img src=\"images/LogReg_mesage.jpg\" style=\"width:650px;height:400px;\">\n\n**Mathematical expression of the algorithm**:\n\nFor one example $x^{(i)}$:\n$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n\nThe cost is then computed by summing over all training examples:\n$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$\n\n**Key steps**:\nIn this exercise, I will carry out the following steps: \n    - Initialize the parameters of the model\n    - Learn the parameters for the model by minimizing the cost  \n    - Use the learned parameters to make predictions (on the test set)\n    - Analyse the results and conclude","outputs":[],"execution_count":null},{"metadata":{"colab_type":"text","id":"w9V7ga92489_","_uuid":"4df812e7b23f786b3a03b71347fef447aedf8f92"},"cell_type":"markdown","source":"## 4. Make helper function ##\n\nFirst implement `sigmoid()`. $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$","outputs":[],"execution_count":null},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"sZP0Y0b448-F","trusted":true,"_uuid":"bcd719c97efd36ac1e2e8782ad1ed81c2239f499"},"cell_type":"code","source":"def sigmoid(z):\n    \"\"\"\n    Compute the sigmoid of z\n\n    Arguments:\n    z -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(z)\n    \"\"\"\n    s = 1/(1+np.exp(-z))\n\n    return s","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"LqnvO-XK48-O","_uuid":"04f90c47fbd3edf798c1d5c0a71832a9f8026fef"},"cell_type":"markdown","source":"## Initializing parameters ##\n","outputs":[],"execution_count":null},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"rvbKz3aC48-S","trusted":true,"_uuid":"989e15f47c08b805a70b70878f4c4f4bb284bac6"},"cell_type":"code","source":"def initialize_with_zeros(dim):\n    \"\"\"\n    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n    \n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    \n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias)\n    \"\"\"\n    w = np.zeros((dim, 1))\n    b = float(0)\n\n    assert(w.shape == (dim, 1))\n    assert(isinstance(b, float) or isinstance(b, int))\n    \n    return w, b","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"vIUn6Ml248-r","_uuid":"c18f155135b6ccd86c446ade7ebd5ae5e33fdccc"},"cell_type":"markdown","source":"## Forward and backward propagation \n\nImplement a function `propagate()` that computes the cost function and its gradient.\n\n**Hints**:\n\nForward Propagation:\n- Get X\n- Compute $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n- Calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n\nHere are the two formulas I will be using: \n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$","outputs":[],"execution_count":null},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"zOJoA0Sl48-w","trusted":true,"_uuid":"8834ac3e33cfb50ab15ed979b518d1281887b6be"},"cell_type":"code","source":"def propagate(w, b, X, Y):\n    \"\"\"\n    Implement the cost function and its gradient for the propagation explained above\n\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b\n    \"\"\"\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    A = sigmoid(np.dot(w.T, X)+b)                                  # compute activation\n    cost = (-1/m) * (Y*np.log(A) + (1-Y)*np.log(1-A)).sum()            # compute cost\n    \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    dw = np.dot(X, (A-Y).T) / m\n    db = 1/m * (A-Y).sum()\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"tXR_3uLK48_S","_uuid":"6fe6f70f1bb42952675831e81ed28ddba4ce1b62"},"cell_type":"markdown","source":"## Optimization ##\n\n- Initialized Ours parameters.\n- Compute a cost function and its gradient.\n- Update the parameters using gradient descent.\n\nThe goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate.","outputs":[],"execution_count":null},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"GB7zXbnd48_U","trusted":true,"_uuid":"4bec14893cd4eab2862e63fa8ec8ab531395c350"},"cell_type":"code","source":"def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        # Cost and gradient calculation\n        grads, cost = propagate(w, b, X, Y)\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        ### END CODE HERE ###\n        \n        # Record the costs\n        if i % 100 == 0:\n            costs.append(cost)\n        \n        # Print the cost every 100 training examples\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"mk3mog6t48_a","_uuid":"64b1d58e7d5c131748a9cb4e93f4e837ed183c75"},"cell_type":"markdown","source":"## Predict ##\n\n Implement the `predict()` function. There is two steps to computing predictions:\n\n1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n\n2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. ","outputs":[],"execution_count":null},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"7qWO20rp48_c","trusted":true,"_uuid":"8c2d7e99e58e349bdfaa98cbe746665fcda1bc0f"},"cell_type":"code","source":"def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (num_px * num_px * 3, number of examples)\n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n    A = sigmoid(np.dot(w.T, X) + b)\n           \n    # Convert probabilities A[0,i] to actual predictions p[0,i]\n    Y_prediction = np.around(A, decimals=0)\n    \n    assert(Y_prediction.shape == (1, m))\n    \n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"SEn9ySWs48_k","_uuid":"b47d079105064234f2e9a325531ffa67179dbb0e"},"cell_type":"markdown","source":"# 5. Merge all function into a model ","outputs":[],"execution_count":null},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"--xsMahy48_n","trusted":true,"_uuid":"8d5eaebbf57f2a6de370055f19d5487101335683"},"cell_type":"code","source":"def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n    \"\"\"\n    Builds the logistic regression model by calling the function We've implemented previously\n    \n    Arguments:\n    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d -- dictionary containing information about the model.\n    \"\"\"\n\n    # initialize parameters with zeros\n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    # Gradient descent\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test/train set examples\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    # Print train/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations}\n    \n    return d","execution_count":null,"outputs":[]},{"metadata":{"colab_type":"text","id":"nDtr6uUw48_r","_uuid":"49bee21c0b0576836081509f62aa25c01be6adf5"},"cell_type":"markdown","source":"### Test our data ###","outputs":[],"execution_count":null},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":573,"output_extras":[{"item_id":6}]},"colab_type":"code","executionInfo":{"elapsed":1993,"status":"ok","timestamp":1518826849223,"user":{"displayName":"Rafał B.","photoUrl":"//lh4.googleusercontent.com/-KErhzOo-dPk/AAAAAAAAAAI/AAAAAAAAAM4/esfarWimf4Q/s50-c-k-no/photo.jpg","userId":"104824888809991073855"},"user_tz":-60},"id":"tVvuMear49AP","outputId":"de22570c-bca6-4143-d5f5-2fc9d682ff73","trusted":true,"_uuid":"f89f7e4ddfb31db076eab6cd8038e9257a49c764"},"cell_type":"code","source":"d = model(X_train, y_train, X_test, y_test, num_iterations = 3000, learning_rate = 0.005, print_cost = True)","execution_count":null,"outputs":[]},{"metadata":{"colab":{"autoexec":{"startup":false,"wait_interval":0}},"colab_type":"code","collapsed":true,"id":"VtUaa34c49AU","trusted":true,"_uuid":"a69446a057960c19b905c968849d5c09a6a9a39d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"default_view":{},"name":"logistic_regression.ipynb","provenance":[],"version":"0.3.2","views":{}},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}