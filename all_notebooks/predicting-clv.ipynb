{"cells":[{"metadata":{},"cell_type":"markdown","source":"![BAIME banner](https://user-images.githubusercontent.com/47600826/89530907-9b3f6480-d7ef-11ea-9849-27617f6025cf.png)"},{"metadata":{},"cell_type":"markdown","source":"# Customer Lifetime Value prediction\n\n![CLV](https://www.searchwarrant.ca/wp-content/uploads/sites/99/2020/04/LTV_ConversionRate_Part2800x420.png)"},{"metadata":{},"cell_type":"markdown","source":"# The problem\n\nIn this notebook we look at the data we got via this [Kaggle dataset](https://www.kaggle.com/saniyajaswani/credit-card-data). \nIt involves the car insurance customer lifetime value.\n\nCustomer Lifetime Value Prediction( CLV ) value refers to net profit attributed to the entire future relationship with a customer. \nA bank will use different predictive analytic approaches to predict the revenue that can be generated from any customer in the future. \nThis helps the banks in segmentating the customers in specific groups based on their CLV.\n\nIdentifying customers with high future values will enable the organization to keep maintaining good relationships with such customers. \nIt can be done by investing more time and resources on them such as better prices, offers, discounts, customer care services, etc.\n\nFinding and engaging reliable and profitable customers has always been a great challenge for banks.\nWith the increasing competition, the banks need to keep a check on each and every activity of their customers for utilizing their resources effectively. \n\nTo solve this problem, Data Science in banking is being used for extracting actionable insights concerning customer behaviors and expectations.\nUsing Data Science models for predicting the CLV of a customer will help a bank to take some suitable decisions for their growth and profit.\n"},{"metadata":{},"cell_type":"markdown","source":"![CLV](https://2112leafletdistribution.co.uk/wp-content/uploads/2018/03/CLV.png)"},{"metadata":{},"cell_type":"markdown","source":"# Import the important libraries / packages\nThese packages are needed to load and use the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd #we use this to load, read and transform the dataset\nimport numpy as np #we use this for statistical analysis\nimport matplotlib.pyplot as plt #we use this to visualize the dataset\nimport seaborn as sns #we use this to make countplots\nimport sklearn.metrics as sklm #This is to test the models","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load and explore the dataset\nThe data is all in one csv file. In this next step I will first load the data to see how this looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"#here we load the data\ndata = pd.read_csv('/kaggle/input/credit-card-data/Fn-UseC_-Marketing-Customer-Value-Analysis.csv')\n\n#and immediately I would like to see how this dataset looks like\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#now let's look closer at the dataset we got\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that we have a lot of text / category information (these are of the Dtype 'object') and a few numerical columns (Dtypes 'int64' and 'float64'). \n\nThe column 'Customer Lifetime Value' is the column we would like to predict. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset consists of 9134 rows and 24 columns. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that we have some strange outliers for the CLV and claim amounts. We will look and handle these later on. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe(include='O')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's see what the options are in the text columns with two or three options (the objects)\nprint('Response: '+ str(data['Response'].unique()))\nprint('Coverage: '+ str(data['Coverage'].unique()))\nprint('Education: '+ str(data['Education'].unique()))\nprint('Employment Status: '+ str(data['EmploymentStatus'].unique()))\nprint('Gender: ' + str(data['Gender'].unique()))\nprint('Location Code: ' + str(data['Location Code'].unique()))\nprint('Married: ' + str(data['Marital Status'].unique()))\nprint('Policy Type: ' + str(data['Policy Type'].unique()))\nprint('Vehicle Size: ' + str(data['Vehicle Size'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Customer Lifetime Value \n\nAs Customer Lifetime Value is the column we want to predict, let's explore this column in the training dataset.\n\nThe formula to calculate the CLV:\n\n![CLV formula](https://d35fo82fjcw0y8.cloudfront.net/2018/08/30131556/calculation-for-customer-lifetime-value.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#As this is a numeric, thus continous number, I will use a scatterplot to see if there is a pattern. \nplt.hist(data['Customer Lifetime Value'], bins = 10)\nplt.title(\"Customer Lifetime Value\") #Assign title \nplt.xlabel(\"Value\") #Assign x label \nplt.ylabel(\"Customers\") #Assign y label \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.boxplot(data['Customer Lifetime Value'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We see that there are some great outliers here. \n#let's look closer to these outliers over 50000\noutliers = data[data['Customer Lifetime Value'] > 50000]\noutliers.head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like there are only 20 rows of the 9134 rows that have a lifetime value of more than 50000. \nWe will leave this as is for now"},{"metadata":{},"cell_type":"markdown","source":"# Handling missing values\nLet's continue with handling the missing values in this dataset. \nLet's see where and how many missing values there are in this dataset.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's look in what columns there are missing values \ndata.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seem to be no missing values in this dataset. "},{"metadata":{},"cell_type":"markdown","source":"## Making the text columns Numeric\nWe first need to make all column input numeric to use them further on. \nThis is what I will do now. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#First we drop the customer column, as this is a unique identifier and will bias the model\ndata = data.drop(labels = ['Customer'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's load the required packages\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's transform the categorical variables to continous variables\ncolumn_names = ['Response', 'Coverage', 'Education', \n                     'Effective To Date', 'EmploymentStatus', \n                     'Gender', 'Location Code', 'Marital Status',\n                     'Policy Type', 'Policy', 'Renew Offer Type',\n                     'Sales Channel', 'Vehicle Class', 'Vehicle Size', 'State']\n\nfor col in column_names:\n    data[col] = le.fit_transform(data[col])\n    \ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As my model can not handle floats, we will change these to integers."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Customer Lifetime Value'] = data['Customer Lifetime Value'].astype(int)\ndata['Total Claim Amount'] = data['Total Claim Amount'].astype(int)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Most important features\nLet's continue by looking at the most important features according to two different tests. \nThan we will use the top ones to train and test our first model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#First we need to split the dataset in the y-column (the target) and the components (X), the independent columns. \n#This is needed as we need to use the X columns to predict the y in the model. \n\ny = data['Customer Lifetime Value'] #the column we want to predict \nX = data.drop(labels = ['Customer Lifetime Value'], axis = 1)  #independent columns \n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Name of the column','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get correlations of each features in dataset\ncorrmat = data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,10))\n\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What pop's out when looking at the correlations for the CLV is the column 'Monthly Premium Auto' and the 'Total Claim Amount'\nThese might be the best features to use."},{"metadata":{},"cell_type":"markdown","source":"Seems that the feature selection models differ a bit in which feature is the most important.\nFor the first test I will keep:\n- Total Claim Amount (high in all both tests)\n- Monthly Premium Auto (high in all both tests and the highest in the correlation)\n- Income (high in two tests)\n- Months Since Policy Inception (High in the best features test)\n- Coverage (High in the correlation)\n"},{"metadata":{},"cell_type":"markdown","source":"# Machine learning Model\nWe want to predict a continous number, therefore we need a linear regression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split the dataset in train and test\nBefore we are going to use the model choosen, we will first split the dataset in a train and test set.\nThis because we want to test the performance of the model on the training set and to be able to check it's accuracy. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#First try with the 5 most important features\nX_5 = data[['Total Claim Amount', 'Monthly Premium Auto', 'Income', 'Coverage', 'Months Since Policy Inception']] #independent columns chosen \ny = data['Customer Lifetime Value']    #target column \n\n#I want to withhold 30 % of the trainset to perform the tests\nX_train, X_test, y_train, y_test= train_test_split(X_5,y, test_size=0.3 , random_state = 25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of X_train is: ', X_train.shape)\nprint('Shape of X_test is: ', X_test.shape)\nprint('Shape of Y_train is: ', y_train.shape)\nprint('Shape of y_test is: ', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To check the model, I want to build a check:\nimport math\ndef print_metrics(y_true, y_predicted, n_parameters):\n    ## First compute R^2 and the adjusted R^2\n    r2 = sklm.r2_score(y_true, y_predicted)\n    r2_adj = r2 - (n_parameters - 1)/(y_true.shape[0] - n_parameters) * (1 - r2)\n    \n    ## Print the usual metrics and the R^2 values\n    print('Mean Square Error      = ' + str(sklm.mean_squared_error(y_true, y_predicted)))\n    print('Root Mean Square Error = ' + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted))))\n    print('Mean Absolute Error    = ' + str(sklm.mean_absolute_error(y_true, y_predicted)))\n    print('Median Absolute Error  = ' + str(sklm.median_absolute_error(y_true, y_predicted)))\n    print('R^2                    = ' + str(r2))\n    print('Adjusted R^2           = ' + str(r2_adj))\n   \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression on 5 features\nLet's try the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear regression model\nmodel_5 = LinearRegression() \nmodel_5.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Predictions = model_5.predict(X_test)\nprint_metrics(y_test, Predictions, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hmmm, that is not a good result, just over 14% reliable...\n\n## Linear Regression on all\nLet's try the model on all features to see if this improves"},{"metadata":{"trusted":true},"cell_type":"code","source":"#I want to withhold 30 % of the trainset to perform the tests\nX_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.3 , random_state = 25)\n\nprint('Shape of X_train is: ', X_train.shape)\nprint('Shape of X_test is: ', X_test.shape)\nprint('Shape of Y_train is: ', y_train.shape)\nprint('Shape of y_test is: ', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear regression model\nmodel = LinearRegression() \nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Predictions = model.predict(X_test)\nprint_metrics(y_test, Predictions, 22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is even worse. \n\n# Conclusion\n\nThis model does not perform well to predict the CLV, as the CLV data is highly skewed.\nTo improve the prediction, we could try to normalize the distribution of the CLV column. \nI will try this here below using Box Cox and Log (two different methods)\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#to see the CLV data as is (without having the extremes removed)\ndata.hist('Customer Lifetime Value', bins = 10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Chech the skewness, if p < 0.05 it is skewed\nclv = data['Customer Lifetime Value']\nfrom scipy.stats import shapiro\nshapiro(clv)[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#as this does not work, let's continue with the log function\nlog_clv = np.log(clv)\nimport seaborn as sns\nsns.distplot(log_clv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#it is slightly improved regarding the skewness. Let's try Box Cox now\nfrom scipy.stats import boxcox\nboxcox_clv = boxcox(clv)[0]\nsns.distplot(boxcox_clv)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BoxCox improved the normal distribution a bit better. Let's try our linear regression now. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#I want to withhold 30 % of the trainset to perform the tests\nX_train, X_test, y_train, y_test= train_test_split(X_5,boxcox_clv, test_size=0.3 , random_state = 25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_5.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Predictions_box = model_5.predict(X_test)\nprint_metrics(y_test, Predictions_box, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see a slight improvement to 18,5% now. But we need to do further feature improvement to better the result. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}