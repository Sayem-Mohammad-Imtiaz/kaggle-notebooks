{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset=pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\",encoding='latin-1')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset['Unnamed: 2'].fillna(\"\",inplace=True)\ndataset['Unnamed: 3'].fillna(\"\",inplace=True)\ndataset['Unnamed: 4'].fillna(\"\",inplace=True)\ndataset['new']=dataset['v2']+dataset['Unnamed: 2']+dataset['Unnamed: 3']+dataset['Unnamed: 4']\ndataset['v1']=(dataset['v1']==\"spam\").astype(int)\ndataset.drop(['v2','Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nimport collections\nros = RandomOverSampler(random_state=0)\nX, y = ros.fit_resample(dataset[dataset.columns.drop(['v1'])], dataset['v1'])\n\nprint(collections.Counter(dataset['v1']),collections.Counter(y))\nX=pd.DataFrame({'text': X.iloc[:, 0]})\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\ntraining_sentences = X['text'].values\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\nreverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\ndef sequence_to_text(list_of_indices):\n    # Looking up words in dictionary\n    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n    return(words)\n\n# Creating texts \nmy_texts = list(map(sequence_to_text, training_sequences))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"j=[]\nfor i in range(len(my_texts)):\n  j.append(str(' '.join(my_texts[i])))\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 5000)\nx = cv.fit_transform(j).toarray()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrainx,testx,trainy,testy = train_test_split(x,y,test_size=0.2,random_state=2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nrf_model = RandomForestClassifier(n_estimators=1000,max_depth=100,random_state=1,max_features=1)\nrf_model.fit(trainx, trainy)\nrf_test_predictions = rf_model.predict(testx)\nrf_test_mae = accuracy_score(rf_test_predictions, testy)\nprint(rf_test_mae)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import svm\nsvmclf_model = svm.SVC()\nsvmclf_model.fit(trainx, trainy)\nsvc = svmclf_model.predict(testx)\nsvm_mae = accuracy_score(svc, testy)\nprint(svm_mae)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state=0,max_iter=1000)\nlogreg.fit(trainx, trainy)\nlogregpred = logreg.predict(testx)\nlogreg_mae = accuracy_score(logregpred, testy)\nprint(logreg_mae)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nmy_model_1 = XGBClassifier(random_state=0)\n\nmy_model_1.fit(trainx, trainy)\nxgpredictions=my_model_1.predict(testx)\nprint(accuracy_score(xgpredictions, testy))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(testy, logregpred),display_labels=['Ham','Spam'])\ndisp.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(testy, logregpred)\nplt.plot(fpr,tpr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}