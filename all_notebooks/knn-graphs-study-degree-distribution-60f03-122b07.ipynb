{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is about \n\nStudy degree distribution of KNN graphs for model data clouds - Gauss, uniform, sphere.\nFor high dimensions it seems to be  power law like. We also see drastical difference between sphere-torus vs Gauss-Uniform.\nSee discussion in : \nhttps://cstheory.stackexchange.com/questions/47957/power-law-for-degree-distribution-of-random-knn-graphs\nsee also https://stats.stackexchange.com/questions/500250/recognize-distribution-on-integers-0-mean-5-median-2-percentiles-75-5\n\nV20 - same as 18,19 - but simulate Gauss, Uniform, Sphere, Torus \n\nV19 - same as 18, but trials increase to 100, and dim 1000 repeated twice with n_sample 1e4,1e5,\n\nV18 - high dims explore: dim 100,1000,10000, K =5 n_sample = 1e4, Gauss \n\nV17 uniform , dim = 50, K = 10 \n\nV16 same with , dim = 10, K = 20  strange thing  - mean sometimes not K for big sample size - is not a bug in cuml ? \nfor dim 10 we see mean equals to median , while for dim 50 we saw mean was = percentile 75 \n\nV15 same with , dim = 10, K = 10\n\nV14 same with , dim = 10, K = 5\n\nV13: K = 20\n\nV12: repeat V11 for K = 10\n\nV11 - study distribution in details for particular case - K=5, dim = 50 , change n_samples, look at percentiles, power law etc..  Findings summarized here: https://stats.stackexchange.com/questions/500250/recognize-distribution-on-integers-0-mean-5-median-2-percentiles-75-5\n\nV10 - same as V9 - increase samples to 10_000_000 - crashed by memory after processing first case \n\nV9 - same as V8 - increase samples to 1_000_000 - 150 seconds \n\nV8 - simulations for Sphere and Torus added. See drastical difference for sphere-torus VS Gauss-uniform. 100_000 samples - 7 seconds \n\n\nV7 - same as V6 , but n_sample = 1e6 - run out of time 9 hours\n\n\nV6 - linear interplotation of loglog plot - estimate exponent of power law of the distibution \n\nV5 - same as V4 but compare GPU and sklearn implementations - should coincide - indeed coincide \n\n\nV4 - pdf of degrees K=5, various dimensions , n_sample = 1e4\n\nV3: plot pdf for :\n\nGauss dim50 n_neighbors3 n_sample100000 2276.7 2276.7 seconds passed trial, total\n\nGauss dim50 n_neighbors10 n_sample100000 2294.3 4571.1 seconds passed trial, total\n\nV2: plot pdf for :\n\nGauss dim10 n_neighbors5 n_sample100000 73.2 73.2 seconds passed trial, total\n\nGauss dim50 n_neighbors5 n_sample100000 2313.5 2386.7 seconds passed trial, total\n\nV1 - fast calculation:\n\nGauss dim10 n_neighbors5 n_sample10000 1.5 1.5 seconds passed trial, total\n\nGauss dim50 n_neighbors5 n_sample10000 11.9 13.4 seconds passed trial, total\n","metadata":{}},{"cell_type":"markdown","source":"dim10 n_neighbors5 n_sample1000 0.1 0.1 seconds passed trial, total\n\ndim10 n_neighbors5 n_sample10000 1.5 1.6 seconds passed trial, total\n\ndim10 n_neighbors5 n_sample100000 66.1 67.7 seconds passed trial, total","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-02T14:38:01.36804Z","iopub.execute_input":"2021-07-02T14:38:01.368372Z","iopub.status.idle":"2021-07-02T14:38:01.378896Z","shell.execute_reply.started":"2021-07-02T14:38:01.368341Z","shell.execute_reply":"2021-07-02T14:38:01.377963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import kneighbors_graph\nimport time\nimport matplotlib.pyplot as plt\n\n_t00 = time.time()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-07-02T14:38:01.38159Z","iopub.execute_input":"2021-07-02T14:38:01.382053Z","iopub.status.idle":"2021-07-02T14:38:01.389545Z","shell.execute_reply.started":"2021-07-02T14:38:01.382015Z","shell.execute_reply":"2021-07-02T14:38:01.38686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    import cudf\n    from cuml.neighbors import NearestNeighbors\n    #from cuml.datasets import make_blobs    \nexcept:\n    print('GPU library cannot be imported. Turn ON GPU')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T14:38:01.39081Z","iopub.execute_input":"2021-07-02T14:38:01.391096Z","iopub.status.idle":"2021-07-02T14:38:01.399355Z","shell.execute_reply.started":"2021-07-02T14:38:01.39106Z","shell.execute_reply":"2021-07-02T14:38:01.398505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom scipy.stats import rv_continuous\nimport scipy.integrate as integrate\nimport scipy.special as special\nimport plotly.graph_objs as go\nfrom collections import Counter\n\nclass CustomDistribution(rv_continuous):\n    def init(self, momtype = 'pdf', a=0, b=1):\n        super().init(a=a, b=b)\n    \n    def _get_support(*args, **kwargs):\n        return 0, 1\n    \n    def _pdf(self, x):\n        if x < 0 or x > 1:\n             return 0\n            \n        integral = integrate.quad(lambda t: np.sinh(t), 0, 1)[0]\n        res = (np.sinh(x)) / integral\n        return res\n\ndef scalar(x, y):\n    return min(-1, (-x[0]*y[0] + x[1]*y[1] + x[2]*y[2]))\n\ndef metric(x, y):\n    return np.arccosh(-scalar(x, y))\n\ndef get_data(distribution_type, dim, n_sample, R):\n    if distribution_type == 'Gauss':\n        X = np.random.randn(n_sample,dim)\n    elif distribution_type == 'Sphere':\n        X = np.random.randn(n_sample,dim+1)\n        s = np.sqrt( (X*X).sum(axis=1))\n        X = X / s[:,np.newaxis]\n    elif distribution_type == 'Torus':\n        X = ( np.random.rand(n_sample,dim) )\n        X1 = np.sin(2*np.pi*X)\n        X2 = np.cos(2*np.pi*X)\n        X = np.concatenate((X1,X2),axis=1)\n    elif distribution_type == 'hyperbolic':\n        t0 = CustomDistribution().rvs(size=(n_sample, 1)) * R\n        t1 = (np.random.rand(n_sample) * 2 * np.pi)\n        t1 = t1.reshape(n_sample, 1)\n        X0 = np.cosh(t0)\n        X1 = np.sinh(t0) * np.cos(t1)\n        X2 = np.sinh(t0) * np.sin(t1)\n        X = np.concatenate((X0,X1,X2), axis=1)\n        print('hyperbolic data is ready')\n    else:\n        X = np.random.uniform(-1,1, size = (n_sample,dim) )\n    return X\n\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2021-07-02T14:38:01.401485Z","iopub.execute_input":"2021-07-02T14:38:01.402092Z","iopub.status.idle":"2021-07-02T14:38:01.422931Z","shell.execute_reply.started":"2021-07-02T14:38:01.402056Z","shell.execute_reply":"2021-07-02T14:38:01.422034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\nfrom scipy.stats import poisson\nfrom scipy.stats import geom\n\ndef repeat_exp(distances,indices,num_iter):\n    all_res = []\n    all_edg = []\n    for i in range(num_iter):\n        graph =  {x: set() for x in range(len(indices))}\n        inp_edg = {x: set() for x in range(len(indices))}\n        roots = []\n        for a, b in indices:\n            if a < b and indices[b][1] == a:\n                roots.append(a)\n        for a, b in indices:\n            graph[a].add(b)\n            graph[b].add(a)\n            inp_edg[b].add(a)\n\n        all_edg += list(map(len, inp_edg.values()))\n        res = bfs(graph, roots)\n        all_res += res\n\n#     connect(X, indices)\n    return all_res, all_edg\n\ndef draw_comp_length(distances, indices):\n    res = repeat_exp(distances, indices, 1)\n    result, edges = res\n    plot_hist(result, True)\n    \ndef bfs(graph, roots): \n    sizes = []\n    visited = set()\n    for root in roots:\n        if root not in visited:\n            queue = collections.deque([root])\n            visited.add(root)\n            num = 1 \n            while queue: \n                vertex = queue.popleft()\n                for neighbour in graph[vertex]: \n                    if neighbour not in visited:\n                        num += 1 \n                        visited.add(neighbour) \n                        queue.append(neighbour)\n            sizes.append(num)\n    return sizes\n\ndef plot_hist(res, d):\n    res = np.array(res)\n\n    theta_geom = 1 / np.mean(res - 1)\n    theta_poiss = np.mean(res - 2)\n    plt.figure(figsize= (10, 10))\n    rv = poisson(theta_poiss)\n    #plt.hist(res, density = True, alpha = 0.7, bins = [i for i in range(2, max(res) + 1)])\n    plot_list(res, d)\n    x = np.arange(2, np.max(res) + 1)\n    plt.plot(x, rv.pmf(x - 2), label='poisson', color = \"red\")\n    rv = geom(theta_geom)\n    plt.plot(x, rv.pmf(x - 1), label='geom', color = \"green\")\n    plt.legend(loc='best', frameon=False)\n    plt.show()\n    \ndef plot_list(ar, density=False):\n    dc = {}\n    if type(ar) is list:\n        st = set(ar)\n        dc = dict.fromkeys(st, 0)\n        for el in st:\n            dc[el] = ar.count(el)\n            if density:\n                dc[el] /= len(ar)\n    else:\n        unique, counts = np.unique(ar, return_counts=True)\n        if density:\n            dc = dict(zip(unique, counts / len(ar)))\n        else:\n            dc = dict(zip(unique, counts))\n#     plt.plot(list(dc.keys()), list(dc.values()), 'ob-', mfc='r')\n    plt.semilogy(list(dc.keys()), list(dc.values()), 'ob-', mfc='r')\n    s = sorted(dc.keys())\n    prop = []\n    for i in range(len(s) - 1):\n        prop.append(dc[s[i]] / dc[s[i+1]])\n    #plt.plot(prop)\n\ndef get_results():\n    result = dict()\n    edges = dict()\n    for dim in [2, 5,]: # {2 : {100: [], 1000: []}}\n        result[dim] = {100: [], 1000: []}\n        edges[dim] = {100: [], 1000: []}\n        for num_samples in result[dim]:\n            result[dim][num_samples], edges[dim][num_samples] = repeat_exp(dim, num_samples, 25)\n            print(f\"Done dim {dim}, num_samples {num_samples}\")\n    return result, edges\n\ndef draw_graph(X, distances, indices):\n    z, x, y = X.transpose()\n    trace = go.Scatter3d(\n       x = x, y = y, z = z,mode = 'markers', marker = dict(\n          size = 3,\n          color = z, # set color to an array/list of desired values\n          colorscale = 'Viridis'\n          )\n       )\n    \n    layout = go.Layout(title = '3D Scatter plot')\n    fig = go.Figure(data = [trace], layout = layout)\n    for i, j in indices:\n        z0, x0, y0 = X[i]\n        z1, x1, y1 = X[j]\n        x=np.linspace(x0, x1, 15)\n        y=np.linspace(y0, y1, 15)\n        z=np.linspace(z0, z1, 15)\n        edge_trace = go.Scatter3d(x=x, y=y, z=z, mode='lines')\n        fig.add_trace(edge_trace)\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T14:38:01.424032Z","iopub.execute_input":"2021-07-02T14:38:01.425813Z","iopub.status.idle":"2021-07-02T14:38:01.463899Z","shell.execute_reply.started":"2021-07-02T14:38:01.425777Z","shell.execute_reply":"2021-07-02T14:38:01.463213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\n\nplt.style.use('ggplot')\n\nfig = plt.figure(figsize=(20,12))\n\nt00=time.time()\ndf = pd.DataFrame({'mean' : [],'std': []}) # mean/std table for each parameters set\nlist_save_results = []\n\nn_sample0 = 100\n\nfor distribution_type in ['hyperbolic']:\n    fig = plt.figure(figsize=(20,8))\n\n    for R in [1, 5]:\n        for dim , n_neighbors, n_sample in [\n             (3, 1, n_sample0),\n#              (3, 3, n_sample0),\n#              (3, 5, n_sample0),\n\n        ]:\n\n            df_stat_over_trials = pd.DataFrame()\n            all_degs = []\n\n            for trial in range(2):\n                t0=time.time()\n                X = get_data(distribution_type, dim, int(n_sample), R)\n\n                model = NearestNeighbors(n_neighbors=n_neighbors+1, algorithm='ball_tree', metric=metric)\n                model.fit(X)\n\n                # get 3 nearest neighbors\n                m = model.kneighbors_graph(X)\n\n                vec_degs = np.sum(m,axis = 0 )\n                vec_degs = np.squeeze(np.array(vec_degs.ravel()))\n                vec_degs -= np.ones_like(vec_degs ) # no include_self - so that is a way round\n                all_degs.append(vec_degs)\n\n            # Build histogram\n            bins = np.arange(200)\n\n            degs = bins[:-1]\n            h = np.histogram(np.concatenate(all_degs), bins = bins, density=True)\n            probs = h[0]\n            m = h[0] > 1e-12\n\n            label = f\"{distribution_type} dim-{dim} n_neighbors-{n_neighbors} n_sample-{n_sample}\"\n            plt.plot( h[0][m],'*-', label = label, linewidth=4)\n            plt.title(f'distribution of degrees R-{R}')\n            plt.xlabel('degree')\n            plt.ylabel('probability')\n            plt.legend(fontsize = 12)\n            d = {'mean': [np.concatenate(all_degs).mean()], 'std': [np.concatenate(all_degs).std()]}\n            df1 = pd.DataFrame(data=d)\n            df1.index = [f'{distribution_type} dim-{dim} n_neighbors-{n_neighbors} n_sample-{n_sample}']\n            df = df.append(df1)\n\n            if n_neighbors == 1:\n                distances, indices = model.kneighbors(X)\n                draw_comp_length(distances, indices)\n                draw_graph(X, distances, indices)\n\n    plt.show()\n#     df.style.set_caption(f'R-{R}')\n    print(df)    \nseconds_passed_total = time.time() - t00\nprint( np.round(seconds_passed_total,1),  np.round(seconds_passed_total/60,1), np.round(seconds_passed_total/3600,1), 'seconds, minutes, hours passed')\n\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T14:38:01.46733Z","iopub.execute_input":"2021-07-02T14:38:01.467566Z","iopub.status.idle":"2021-07-02T14:38:07.184749Z","shell.execute_reply.started":"2021-07-02T14:38:01.467543Z","shell.execute_reply":"2021-07-02T14:38:07.184035Z"},"trusted":true},"execution_count":null,"outputs":[]}]}