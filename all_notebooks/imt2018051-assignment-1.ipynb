{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport sklearn as sklearn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Instructions\n1. We will be conducting the entire assignment through this notebook. You will be entering your code in the cells provided, and any explanation and details asked in markdown cells. \n2. You are free to add more code and markdown cells for describing your answer, but make sure they are below the question asked and not somewhere else. \n3. The notebook needs to be submitted on LMS. You can find the submission link [here](https://lms.iiitb.ac.in/moodle/mod/assign/view.php?id=13932). \n4. The deadline for submission is **5th October, 2020 11:59PM**."},{"metadata":{},"cell_type":"markdown","source":"# Data import\nThe data required for this assignment can be downloaded from the following [link](https://www.kaggle.com/dataset/e7cff1a2c6e29e18684fe6b077d3e4c42f9a7ae6199e01463378c60fe4b4c0cc), it's hosted on kaggle. Do check directory paths on your local system.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/alcoholism/student-mat.csv\")\nfifadata = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/fifa18/data.csv\")\naccidata1 = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/accidents/accidents_2005_to_2007.csv\")\naccidata2 = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/accidents/accidents_2009_to_2011.csv\")\naccidata3 = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/accidents/accidents_2012_to_2014.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part - 1\n## Alcohol Consumption Data\nThe following data was obtained in a survey of students' math course in secondary school. It contains a lot of interesting social, gender and study information about students. \n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Try to visualize correlations between various features and grades and see which features have a significant impact on grades. \nTry to engineer the three grade parameters (G1, G2 and G3) as one feature for such comparisons.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null entries in the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***The grades as a whole convey the same information as the three grades separately, hence G1, G2, G3 can be dropped and a new feature Average_G i.e. the mean of G1, G2, G3 is introduced***"},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata[\"Average_G\"] = alcdata[[\"G1\",\"G2\",\"G3\"]].mean(axis = 1)\nalcdata.drop([\"G1\",\"G2\",\"G3\"],axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cat_alcdata is alcdata without encoding categorical features\ncat_alcdata = alcdata.copy()\n\nalcdata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"female_alcdata = alcdata.groupby(\"sex\").get_group(\"F\")\nmale_alcdata = alcdata.groupby(\"sex\").get_group(\"M\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let us compare how different features have impact on the grades."},{"metadata":{},"cell_type":"markdown","source":"Histograms and boxplots are used for the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(female_alcdata.Average_G,bins = 20)\nsns.distplot(male_alcdata.Average_G,bins = 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The orange histogram corresponds to male, and blue correspondings to female. \nWe see that the two histograms almost overlap but there is slight variations.\n\n# Let's explore more using boxplots!\nWe are chosing boxplots because it's easier to analyse the range, compare the medians, min and max using them."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='sex',y='Average_G',data=alcdata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From this box plot, we can infer that,**\n* Mean grade for male students is slightly higher than female students. \n* The minimum and maximum grades obtained by male students are higher than those of female students."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='famsize',y='Average_G',data = alcdata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can infer that,\n* Grades of students with family size lesser than 3, have a shorter range of grades\n* Maximum grade is obtained by student whose family size id greater than 3.\n* Median grade of students with family size less than 3 is slightly greater than the other.\n\nTherefore family size might have a small contribution to grades."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='school',y='Average_G',data = alcdata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that,\n* Students of MS school have a better minimum grade. The worst grade of MS school is still better than some of the students from GP.\n* Student from GP has the best grade\n* While the range might be different, the median is almost the same for both the schools"},{"metadata":{},"cell_type":"markdown","source":"### 2. If there is a need for encoding some of the features,  how would you go  about it? \nWould you consider combining certain encodings together ?\n"},{"metadata":{},"cell_type":"markdown","source":"We look at the no. of unique values of the features.\nThe categorical variables will return the no. of unique values while the numerical features return NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata.describe(include='all').loc['unique', :]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that in most of the above categorical features, no. of unique values is 2. Hence we can use **One-hot encoding for these**"},{"metadata":{},"cell_type":"markdown","source":"**Mjob**, **Fjob, reason, guardian** are cat features that need to encoded.\nTwo options that we can consider:\n* Label Encoding : Since it is efficient\n* One-hot : When we do not want any order.\nSince by having an order, we do not want to give priority to a particular job, reason,\n\n# Let's go with one-hot encoding using get_dummies() function!"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_attributes = alcdata.select_dtypes(include = ['object'])\n\nfor col in cat_attributes:    \n    dumm = pd.get_dummies(alcdata[col])\n    alcdata = pd.concat([alcdata, dumm], axis = 1)\n    alcdata.drop(columns=[col], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### 3. Try to find out how family relation(famrel) and parents cohabitation(Pstatus) affect grades of students. \n"},{"metadata":{},"cell_type":"markdown","source":"### Scatter plots will hep us capture relationships between famrel and cohabitation status with grades"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(alcdata[\"famrel\"].unique())\nsns.catplot(x = 'famrel', y = 'Average_G', data = alcdata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distributions of grade corresponding to quality of family relationship 4 has the highest and least grade student.\nWhile the density of students having a better grade is high for a better famrel.\nAs the distributions are almost the same, it shows that grades of students aren't much dependent on Famrel."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = 'Pstatus', y = 'Average_G', data = cat_alcdata)\n#One point is way too below. Can be outlier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly note that there are a lot of people with good grades whose parents stay together.\nGrades of students whose parents are not together are focussed around 10 while the grades of students with Pstatus = T is focussed around 13"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = 'famrel', y = 'Average_G', hue = \"Pstatus\", kind = \"box\", data = cat_alcdata, aspect = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* From this we can see that a student with a very good family relationship has the highest median grade even though his/her parents don't stay together.\n* The highest of grades belongs to a student with famrel = 4 and and parents stay together.\n* Surprisingly, the least grade is also obtained by the student with famrel = 4 but parents stay away."},{"metadata":{},"cell_type":"markdown","source":"\n### 4. Figure out which features in the data are skewed, and propose a way to remove skew from all such columns. "},{"metadata":{},"cell_type":"markdown","source":"Only numerical continuous attributes can be used to check for skew. Hence we have to filter out the numerical attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_attributes = cat_alcdata.select_dtypes(include = ['int', 'float'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Age, Average_G, absences are the only continuous features. Out of which changing average_g will corrupt our data as it conveys information. "},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_attributes[[\"absences\", \"Average_G\"]].hist(figsize = (12,5), bins = 50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that absences is skewed. To remove skew we apply log transform. But before checking skew, let's check if there are any  outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x = 'absences', data = numerical_attributes[[\"absences\"]], aspect = 2, kind ='box')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As there are a lot of outliers, min-max norm won't apply, we must use z-transform only."},{"metadata":{"trusted":true},"cell_type":"code","source":"# numerical_attributes[[\"absences\"]] = numerical_attributes[[\"absences\"]].replace({0: None}).dropna()\n# # transformed_absences.replace(0, \"nan\").dropna(axis=1,how=\"all\")\n# print(numerical_attributes[[\"absences\"]])\n\ndef ztransform(x):\n    return (x - numerical_attributes.absences.mean())/numerical_attributes.absences.std()\n\nsns.distplot(numerical_attributes[[\"absences\"]].apply(lambda x: np.log(ztransform(x)+0.001)), bins = 30)\n\nprint(\"Skew before: \" + str(numerical_attributes[[\"absences\"]].apply(lambda x: ztransform(x).skew())))\nprint(\"Skew after: \" + str(numerical_attributes[[\"absences\"]].apply(lambda x: np.log(ztransform(x)+0.0001).skew())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We were successful at reducing the skew!"},{"metadata":{},"cell_type":"markdown","source":"# Part - 2\n## FIFA 2019  Data\n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Which clubs are the most economical? How did you decide that?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nfifadata.info()\nfifadata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wage Value and Release Clause are of money type and hence have to be converted into float.\n### The following function will preprocess the money type and will convert into float. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def PreProcess(i):\n    if(isinstance(i,str)):\n        if(i[-1]=='M'):\n            return(float(i.lstrip('€').rstrip('M'))*1000000)\n        elif(i[-1]=='K'):\n            return(float(i.lstrip('€').rstrip('K'))*1000)\n        else:\n            return(float(i.replace('€','')))\n        \nfor col in ['Wage', 'Value', 'Release Clause']:\n    fifadata[col] = fifadata[col].apply(lambda x: PreProcess(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Value is an asset, Wage is an expense. I have used (Value - Wage) to determine which club is the most economical! "},{"metadata":{"trusted":true},"cell_type":"code","source":"#We have summed up value and wage of all players of a particular club and sorted to get the most economical club\n\nclub_Wage = fifadata['Wage'].groupby(fifadata['Club']).apply(lambda x : x.sum())\nclub_Value = fifadata['Value'].groupby(fifadata['Club']).apply(lambda x : x.sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(club_Value - club_Wage).sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the above inference, we can conclude that REAL MADRID is the most economical. "},{"metadata":{},"cell_type":"markdown","source":"### 2. What is the relationship between age and individual potential of the player? How does age influence the players' value? At what age does the player exhibit peak pace ?"},{"metadata":{},"cell_type":"markdown","source":"We look at the null values first and fill them with the mean of the data column if required"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(fifadata[\"Potential\"].isnull().sum())\nprint(fifadata[\"Value\"].isnull().sum())\nprint(fifadata[\"SprintSpeed\"].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fifadata['SprintSpeed'] = fifadata['SprintSpeed'].fillna(fifadata['SprintSpeed'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig, axes = plt.subplots(figsize=(7,5))\n# axes.set_ylabel('Potential')\n# fifadata[[\"Age\", \"Potential\"]].plot(x = 'Age',ax = axes, y = 'Potential', kind = 'scatter')\n\nsns.lmplot(x = 'Age',y = 'Potential', order = 2, data = fifadata, aspect = 1.5)\nsns.lmplot(x = 'Age',y = 'Value', order = 2, data = fifadata, aspect = 1.5)\nsns.lmplot(x = 'Age',y = 'SprintSpeed', order = 2, data = fifadata, aspect = 1.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Potential and Age have an inverse exponential relationship. \nAs the age increases potential decreases.\n\n* Age and value is almost a uniform distribution\nThis can be used to infer that that the value a player brings to the team doesn't necessarily depend on the age, but it does depend on potential"},{"metadata":{},"cell_type":"markdown","source":"### 3. What skill sets are helpful in deciding a player's potential? How do the traits contribute to the players' potential? "},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in fifadata.iloc[:,54:87]:\n    if(np.abs(fifadata.corr()['Potential'][col]) > 0.4):\n        print( str(col) + \" is related to \" + 'Potential and hence might be helpful in deciding Potential' )\n    \n\n# plt.figure(figsize=(10,10))\n# sns.heatmap(fifadata[54:87].corr(), vmin=-1, cmap=\"coolwarm\", annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Which features directly contribute to the wages of the players?"},{"metadata":{},"cell_type":"markdown","source":"We analyse the features by a scatterplot of wages vs some features.\nAccording to my intuition, Sprint speed, overall and potential must affect the wage. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns.lmplot(x = 'Wage',y = 'SprintSpeed', order = 2, data = fifadata)\nfifadata.plot(kind = 'scatter',y='Wage',x='SprintSpeed',figsize=(10,10))\nfifadata.plot(kind = 'scatter',y='Wage',x='Overall',figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* SprintSpeed should technically affect the wage. But the distribution is almost uniform.\n* But the wage is exponentially increasing with The overall and potentail as expected"},{"metadata":{"trusted":true},"cell_type":"code","source":"fifadata.plot(kind = 'scatter',y='Wage',x='Potential',figsize=(10,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. What is the age distribution in different clubs? Which club has most players young?"},{"metadata":{},"cell_type":"markdown","source":"### Let's look at min max median and mean age of all the clubs to analyse age distribution.\nConsidering young players are of age <= 20, we can sort clubs based on no. of young players."},{"metadata":{"trusted":true},"cell_type":"code","source":"age_dist = fifadata[\"Age\"].groupby(fifadata[\"Club\"])\nlist_values = [\"min\", \"max\", \"median\", \"mean\"]\nprint(age_dist.agg(list_values))\n\nclub_age = fifadata['Age'].groupby(fifadata['Club']).apply(lambda x : (x<=20).sum())\nclub_age.sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## FC Nordsjælland has the most no. of young players!"},{"metadata":{},"cell_type":"markdown","source":"# Part - 3\n## UK Road Accidents Data\n\n\nThe UK government amassed traffic data from 2000 and 2016, recording over 1.6 million accidents in the process and making this one of the most comprehensive traffic data sets out there. It's a huge picture of a country undergoing change."},{"metadata":{},"cell_type":"markdown","source":"### 1. The very first step should be to merge all the 3 subsets of the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accidata1.shape)\nprint(accidata2.shape)\nprint(accidata3.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \naccidata = pd.concat([accidata1, accidata2, accidata3])\naccidata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. What are the number of casualties in each day of the week? Sort them in descending order. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer. \nfig, axes = plt.subplots(figsize=(10,5))\naxes.set_ylabel('Casualities')\n\ncasualties = accidata[[\"Number_of_Casualties\",\"Day_of_Week\"]].groupby([\"Day_of_Week\"], as_index = False).sum().sort_values(by = 'Number_of_Casualties',ascending = False)\ncasualties[\"Day_of_Week\"] = casualties[\"Day_of_Week\"].map({1:\"Monday\",2:\"Tuesday\",3:\"Wednesday\",4:\"Thursday\",5:\"Friday\",6:\"Saturday\",7:\"Sunday\"})\n\ncasualties.plot(x = 'Day_of_Week', y = 'Number_of_Casualties', ax = axes,kind='bar',color= 'red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saturday and Friday have the most no. of casualties. While Sunday doesn't have that many. This shows that the day being a weekend affects the no. of casualties. "},{"metadata":{},"cell_type":"markdown","source":"### 3. On each day of the week, what is the maximum and minimum speed limit on the roads the accidents happened?"},{"metadata":{"trusted":true},"cell_type":"code","source":"speed_limit_data = accidata[[\"Speed_limit\", \"Day_of_Week\"]].groupby(\"Day_of_Week\")\nlist_values = [\"min\", \"max\", \"median\", \"mean\"]\nspeed_limit_data.agg(list_values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Min and max speed limit almost remains the same i.e 10 and 70 respectively for all days except for Thursday when the min speed limit is 20\nThis shows that speed limit is not dependent on the week of the day.\n\nBoxplot can be used to get a better visualisation of the same!"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x =\"Day_of_Week\", y = 'Speed_limit', data = accidata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. What is the importance of Light and Weather conditions in predicting accident severity? What does your intuition say and what does the data portray?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.countplot(data=accidata[[\"Weather_Conditions\",\"Accident_Severity\"]], x= 'Weather_Conditions', hue = 'Accident_Severity')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In case of weather, we expect accidents to happen in bad weather like heavy wind, snowing and etc, but the data shows that the most number of accidents happen with the highest severity when the weather is fine without high winds."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(7,5))\naxes.set_ylabel('Accident_Severity')\nseverity_light = accidata[['Accident_Severity','Weather_Conditions']].groupby([\"Weather_Conditions\"], as_index = False).mean().sort_values(by = \"Accident_Severity\")\nprint(severity_light)\nseverity_light.plot(x = 'Weather_Conditions', y = 'Accident_Severity', ax = axes, kind='bar',color= 'deepskyblue',ylim = [1,3], yticks = np.arange(0, 3, step=0.2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nsns.countplot(data=accidata[[\"Light_Conditions\",\"Accident_Severity\"]], x= 'Light_Conditions', hue = 'Accident_Severity')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  According to our intuition, we expect more accidents to happen at night without light, but on the contrary most accidents happened during daylight with the most severity and the least when there's darkness without street lighting."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(7,5))\naxes.set_ylabel('Accident_Severity')\nseverity_light = accidata[['Accident_Severity','Light_Conditions']].groupby([\"Light_Conditions\"], as_index = False).mean().sort_values(by = \"Accident_Severity\")\nprint(severity_light)\nseverity_light.plot(x = 'Light_Conditions', y = 'Accident_Severity', ax = axes, kind='bar',color= 'deepskyblue',ylim = [1,3], yticks = np.arange(0, 3, step=0.2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. To predict the severity of the accidents which columns do you think are unnecessary and should be dropped before implementing a regression model. Support your statement using relevant plots and hypotheses derived from them."},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are a lot of missing values in the dataset.\n\n* Junction_Detail and Junction_Control and LSOA_of_Accident_Location have almost all_values missing. Hence, these two can be dropped.\n* Accident index just seems to be an index and will not contribute much."},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata_new = accidata.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata_new.drop(columns = ['Junction_Detail','Junction_Control', 'LSOA_of_Accident_Location', 'Accident_Index'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_date_get_month(d):\n    if(isinstance(d,str)):\n        return(int(d.split('/')[1]))\n    else:\n        return 1\n    \ndef split_date_get_day(d):\n    if(isinstance(d,str)):\n        return(int(d.split('/')[0]))\n    else:\n        return 1\n\naccidata_new['Day'] = accidata_new['Date'].apply(lambda x: split_date_get_day(x))\naccidata_new['Month'] = accidata_new['Date'].apply(lambda x: split_date_get_month(x))\naccidata_new[['Accident_Severity','Day','Month','Year']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if these values are actually relevant to Accident_Severity"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.heatmap(accidata_new[['Accident_Severity','Day','Month','Year']].corr(), vmin=-1, cmap=\"coolwarm\", annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata_new['Time'].head()\ndef split_time(t):\n    if(isinstance(t, str)):\n        return (int(t.split(':')[0]), int(t.split(':')[1]))\n    return (0,0)\n\naccidata_new['Hours'], accidata_new['Minutes'] = accidata_new['Time'].apply(lambda x: split_time(x)[0]), accidata_new['Time'].apply(lambda x: split_time(x)[1])\naccidata_new[['Accident_Severity', 'Hours', 'Minutes']].head()\nsns.heatmap(accidata_new[['Hours', 'Minutes', 'Accident_Severity']].corr(), vmin=-1, cmap=\"coolwarm\", annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the date doesn't influence severity. Hence all values of dat, time can be conveniently removed."},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata_new.drop(columns = ['Day','Month','Year', 'Time', 'Date', 'Hours', 'Minutes'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_attributes = accidata_new.select_dtypes(include = ['int', 'float'])\nplt.figure(figsize=(25,15))\nsns.heatmap(numerical_attributes.corr(), vmin=-1, cmap=\"coolwarm\", annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this, we can drop highly correlated columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata_new.drop(columns = ['Location_Easting_OSGR','Location_Northing_OSGR','Local_Authority_(District)'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata_new.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By intuition, we can drop Police Force and Local_Authority_(Highway) as they don't affect the severity of the accident. "},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata_new.drop(columns = ['Police_Force', 'Local_Authority_(Highway)', 'Did_Police_Officer_Attend_Scene_of_Accident'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = accidata_new.select_dtypes(include = ['object'])\nfor col in cat:\n    print(col)\n#     print(accidata_new[col].isnull().sum())\n    print(accidata_new[col].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some features are overly dominant. We can drop those!"},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata_new.drop(columns = ['Special_Conditions_at_Site', 'Pedestrian_Crossing-Human_Control', 'Carriageway_Hazards'], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata_new.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fill all the null values with the most frequent category."},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata_new = accidata_new.apply(lambda x:x.fillna(x.value_counts().index[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encode all the categorical variables using mean encoding because "},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = accidata_new.select_dtypes(include = ['object'])\nfor col in cat:\n    mean_encode = accidata_new.groupby(col)['Accident_Severity'].mean()\n    accidata_new.loc[:,col] = accidata_new[col].map(mean_encode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Implement a basic Logistic Regression Model using scikit learn with cross validation = 5, where you predict the severity of the accident (Accident_Severity). Note that here your goal is not to tune appropriate hyperparameters, but to figure out what features will be best to use."},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn.linear_model as linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegressionCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = accidata_new['Accident_Severity']\nx = accidata_new.drop(columns=['Accident_Severity'],inplace= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nx = scaler.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cf = LogisticRegressionCV(cv=5, multi_class=\"multinomial\", max_iter=1000,verbose=100).fit(x, y)\nprint(cf.score(x,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We can see that it is giving 85%"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}