{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import basic libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read the file\ndf = pd.read_csv(\"../input/south-german-credit-updated/german_credit.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check dataset's size\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"credit_risk\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset is imbalanced due to that it has more \"good\" values (for credit_risk column) than \"bad\" values. This imbalance may cause us to make weak predictions."},{"metadata":{},"cell_type":"markdown","source":"## Data Cleaning and Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Is there any duplicate value in the dataset?\ndf.duplicated().value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is not any duplicate value in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Is there any null value in the dataset?\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"There is not any null value in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#How many columns does this dataset have?\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Majority of this dataset consists of categorical values and I plan to use some classification models to predict the credit risk. Therefore, I will need to drop some of the variables that aren't categorical and then, I will convert them to numerical labels with **LabelEncoder()**: "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = df.drop([\"duration\",\"amount\",\"age\"],1)\ndataset.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nfor x in dataset.columns:\n    dataset[x] = LabelEncoder().fit_transform(dataset[x])\n\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification "},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_x = dataset[['status', 'credit_history', 'purpose', 'savings',\n       'employment_duration', 'installment_rate', 'personal_status_sex',\n       'other_debtors', 'present_residence', 'property',\n       'other_installment_plans', 'housing', 'number_credits', 'job', 'people_liable',\n       'telephone', 'foreign_worker']]\nfeature_y = dataset[[\"credit_risk\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\ndef best_features(X_train,y_train,X_test):\n    fs = SelectKBest(score_func = f_classif, k=\"all\")\n    fs.fit(X_train,y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs,X_test_fs,fs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(feature_x,feature_y,test_size=0.33,random_state=21)\nX_train_fs,X_test_fs,fs = best_features(X_train,np.ravel(y_train),X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# what are scores for the features\nfor i in range(len(fs.scores_)):\n\tprint('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification - Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = dataset[['status', 'credit_history', 'purpose', 'savings','personal_status_sex',\n              'property','other_installment_plans', 'housing','job','foreign_worker']]\ntrain_y = dataset[[\"credit_risk\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_lr,X_test_lr,y_train_lr,y_test_lr = train_test_split(train_x,train_y,test_size=0.33,random_state=21)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(solver='liblinear')\nfit = lr.fit(X_train_lr,np.ravel(y_train_lr))\nfit","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat = fit.predict(X_test_lr)\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test_lr, yhat)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test_lr, yhat))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we mentioned earlier, due to that the dataset is imbalanced, our prediction model may not perform well. In order to solve this, we will oversample our data with SMOTE method"},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression with SMOTE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_smote,y_train_smote = smote.fit_sample(train_x,train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before SMOTE: \", train_y[\"credit_risk\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"After SMOTE: \", y_train_smote[\"credit_risk\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train_smote,np.ravel(y_train_smote))\ny_pred2 = lr.predict(X_test_lr)\nprint(classification_report(y_test_lr, y_pred2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although I applied SMOTE() and reapplied the train sets, the regression model didn't perform well enough. Therefore, I will try another classification model"},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvclassifier = SVC(kernel=\"poly\",degree=10)\nsvclassifier.fit(X_train_smote,np.ravel(y_train_smote))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"yhat_svm = svclassifier.predict(X_test_lr)\nprint(classification_report(y_test_lr, yhat_svm))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}