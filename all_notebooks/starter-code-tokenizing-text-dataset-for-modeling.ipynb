{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# STARTER CODE: Tokenizing Text Dataset for Modeling","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Setting Up","metadata":{}},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2021-07-25T23:14:22.874187Z","iopub.execute_input":"2021-07-25T23:14:22.874648Z","iopub.status.idle":"2021-07-25T23:14:22.878355Z","shell.execute_reply.started":"2021-07-25T23:14:22.874613Z","shell.execute_reply":"2021-07-25T23:14:22.877635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Print Directory Items","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-25T23:05:07.415195Z","iopub.execute_input":"2021-07-25T23:05:07.415558Z","iopub.status.idle":"2021-07-25T23:05:07.44022Z","shell.execute_reply.started":"2021-07-25T23:05:07.415529Z","shell.execute_reply":"2021-07-25T23:05:07.438913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read in Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv('/kaggle/input/political-though-work-corpus/all-data.csv')\ndata = data[data['Text'].apply(lambda x:isinstance(x, str))==True]\ndata.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-07-25T23:05:08.290763Z","iopub.execute_input":"2021-07-25T23:05:08.291234Z","iopub.status.idle":"2021-07-25T23:05:10.395546Z","shell.execute_reply.started":"2021-07-25T23:05:08.291193Z","shell.execute_reply":"2021-07-25T23:05:10.394447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Vectorize Data","metadata":{}},{"cell_type":"markdown","source":"This script collects a list of texts and converts them to a padded, tokenized TensorFlow dataset. Because almost all the string-level operations are performed within `tf.strings`, the process takes very little time to process large quantities of text (about two-thirds of a minute).","metadata":{}},{"cell_type":"code","source":"import time\nstart = time.time()\n\n'''\n====================================================================================\nSTART OF RELEVANT TOKENIZATION SCRIPT\n====================================================================================\n'''\n\n# importing necessary function\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# collect training data\ntrain_data = data['Text'].tolist()\n\n# quickly count number of unique words\ncomplete_text = tf.strings.join([tf.constant(text) for text in data['Text']])\ny, idx, count = tf.unique_with_counts(tf.strings.split(complete_text))\n\n# set important parameters\nnum_words = y.shape[0]\noov_token = '<UNK>'\npad_type = 'post'\ntrunc_type = 'post'\n\n# define and fit tokenizer\ntokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\ntokenizer.fit_on_texts(train_data)\ntrain_sequences = tokenizer.texts_to_sequences(train_data)\n\n# pad sequences\nmaxlen = max([len(x) for x in train_sequences])\ntrain_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\ntrain_padded = tf.constant(train_padded)\n\n# create tensorflow dataset\ndata = tf.data.Dataset.from_tensor_slices(train_padded)\n\n'''\n====================================================================================\nEND OF RELEVANT TOKENIZATION SCRIPT\n====================================================================================\n'''\n\nend = time.time()\nprint(f'Took {round(end-start,3)} seconds.')","metadata":{"execution":{"iopub.status.busy":"2021-07-25T23:38:42.07785Z","iopub.execute_input":"2021-07-25T23:38:42.078246Z","iopub.status.idle":"2021-07-25T23:39:21.412151Z","shell.execute_reply.started":"2021-07-25T23:38:42.078211Z","shell.execute_reply":"2021-07-25T23:39:21.411334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can 'detokenize' a vectorization by passing it through `tokenizer.sequences_to_texts`.","metadata":{}},{"cell_type":"code","source":"decoded_string = tokenizer.sequences_to_texts(train_padded.numpy()[0:1])[0]\ndecoded_string[:1000]","metadata":{"execution":{"iopub.status.busy":"2021-07-25T23:48:20.936579Z","iopub.execute_input":"2021-07-25T23:48:20.937136Z","iopub.status.idle":"2021-07-25T23:48:23.265231Z","shell.execute_reply.started":"2021-07-25T23:48:20.937088Z","shell.execute_reply":"2021-07-25T23:48:23.264262Z"},"trusted":true},"execution_count":null,"outputs":[]}]}