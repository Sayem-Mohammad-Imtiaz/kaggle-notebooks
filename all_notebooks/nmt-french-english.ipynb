{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**<h1 align='ccenter'>Neural Machine translation</h1>** <hr/>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom IPython.display import Markdown","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-01T16:39:19.512809Z","iopub.execute_input":"2021-06-01T16:39:19.513222Z","iopub.status.idle":"2021-06-01T16:39:25.436553Z","shell.execute_reply.started":"2021-06-01T16:39:19.513139Z","shell.execute_reply":"2021-06-01T16:39:25.435514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.style.use(\"seaborn\")\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:39:25.438142Z","iopub.execute_input":"2021-06-01T16:39:25.438587Z","iopub.status.idle":"2021-06-01T16:39:25.44386Z","shell.execute_reply.started":"2021-06-01T16:39:25.438538Z","shell.execute_reply":"2021-06-01T16:39:25.443146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file='../input/englishfrench-fornmt/english-french.csv'\ndf = pd.read_csv(file)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:39:25.445153Z","iopub.execute_input":"2021-06-01T16:39:25.445551Z","iopub.status.idle":"2021-06-01T16:39:26.894815Z","shell.execute_reply.started":"2021-06-01T16:39:25.445513Z","shell.execute_reply":"2021-06-01T16:39:26.893601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Markdown(f'- **Total number of rows** : {df.shape[0]}'))\ndisplay(Markdown(f'- **Some random data**'))\ndisplay(df.sample(n=9))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:39:26.896499Z","iopub.execute_input":"2021-06-01T16:39:26.896849Z","iopub.status.idle":"2021-06-01T16:39:26.939136Z","shell.execute_reply.started":"2021-06-01T16:39:26.896815Z","shell.execute_reply":"2021-06-01T16:39:26.938051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<h2 align=\"center\">Data cleaning and Preprocessing</h2>**<hr>","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"### **Cleaning Data**","metadata":{}},{"cell_type":"code","source":"import re, string, json5","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:39:26.940644Z","iopub.execute_input":"2021-06-01T16:39:26.941033Z","iopub.status.idle":"2021-06-01T16:39:26.949036Z","shell.execute_reply.started":"2021-06-01T16:39:26.940982Z","shell.execute_reply":"2021-06-01T16:39:26.947984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = '../input/englishfrench-fornmt/eng-contraction.JSON'\nwith open(file,'r') as f:\n    contractions = dict(json5.load(f))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:39:26.950395Z","iopub.execute_input":"2021-06-01T16:39:26.95088Z","iopub.status.idle":"2021-06-01T16:39:27.094177Z","shell.execute_reply.started":"2021-06-01T16:39:26.950847Z","shell.execute_reply":"2021-06-01T16:39:27.093064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replace_contractions_by_words(text):\n  text = ' '.join(\n      [\n       contractions[word] if word in contractions\n       else word for word in text.split()\n    ]\n  )\n  return text\n\ndef remove_punctuations(text):\n  # Extracting text within bracket\n  text = re.sub('\\((\\w+)\\)', '\\g<1>', text)\n  text = re.sub('\\[(\\w+)\\]', '\\g<1>', text)\n  text = re.sub('\\{(\\w+)\\}', '\\g<1>', text)\n  \n  punct = list(string.punctuation)\n  # Actually not removing some punctions make the model more accurate\n  for symbol in \"'\":\n    punct.remove(symbol)\n\n  # Remove punctuations and some unwanted characters\n  punct = ''.join(punct)\n  for p in f\"{punct}»«…\":\n    text = text.replace(p, \" \")\n  return text\n\ndef cleaning(text, lang=None):\n  \"\"\"Clean the text\"\"\"\n  text = text.lower()\n  if lang =='en': \n    text = replace_contractions_by_words(text)\n  text = remove_punctuations(text)\n  # Remove whitespaces characters\n  text = re.sub('\\s+',' ', text)\n  return text.strip()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:39:27.0956Z","iopub.execute_input":"2021-06-01T16:39:27.095898Z","iopub.status.idle":"2021-06-01T16:39:27.104721Z","shell.execute_reply.started":"2021-06-01T16:39:27.095868Z","shell.execute_reply":"2021-06-01T16:39:27.103927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove non necessary characters\ndf.english = df.english.apply(lambda text:cleaning(text, lang='en'))\ndf.french = df.french.apply(cleaning)\ndisplay(df.sample(n=5))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:39:27.10679Z","iopub.execute_input":"2021-06-01T16:39:27.107216Z","iopub.status.idle":"2021-06-01T16:39:43.128185Z","shell.execute_reply.started":"2021-06-01T16:39:27.107178Z","shell.execute_reply":"2021-06-01T16:39:43.127198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Data preprocessing**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T16:39:43.130445Z","iopub.execute_input":"2021-06-01T16:39:43.130748Z","iopub.status.idle":"2021-06-01T16:39:43.797072Z","shell.execute_reply.started":"2021-06-01T16:39:43.130718Z","shell.execute_reply":"2021-06-01T16:39:43.796178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_data(dataframe:pd.DataFrame, sample_size=20_000, test_size=0.35):\n  dataframe = dataframe.sample(n=sample_size, random_state=np.random.randint(100))\n  train_set, test_set = train_test_split(\n    dataframe, test_size=test_size, random_state=np.random.randint(500),\n  )\n  train_set = train_set.reset_index(drop=True)\n  test_set = test_set.reset_index(drop=True)\n  return train_set, test_set\n\ndef tokenize(sentences):\n  tokenizer = Tokenizer(oov_token='UNK', filters='',)\n  tokenizer.fit_on_texts(sentences)\n  return tokenizer\n\ndef get_maxlen(sentences): \n  return max(len(s.split(' ')) for s in sentences.to_numpy())\n\ndef get_vocab_size(tokenizer):\n  return len(tokenizer.word_index)+1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:39:43.799085Z","iopub.execute_input":"2021-06-01T16:39:43.799392Z","iopub.status.idle":"2021-06-01T16:39:43.806762Z","shell.execute_reply.started":"2021-06-01T16:39:43.799362Z","shell.execute_reply":"2021-06-01T16:39:43.805908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the source language as French language and target the English\nsource, target = 'french', 'english'\n# Beginning and ending of every sentence in target language\nbegin, end = 'debut', 'fin' \n\n# Prepare data for use in teacher forcing learning\ndf = df.rename(columns={source:'source', target: 'target'})\ndf = df[['source', 'target']]\ndf.target = (df.target.apply(lambda x: f'{begin} {x} {end}'))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:39:43.808089Z","iopub.execute_input":"2021-06-01T16:39:43.808353Z","iopub.status.idle":"2021-06-01T16:39:44.047389Z","shell.execute_reply.started":"2021-06-01T16:39:43.808327Z","shell.execute_reply":"2021-06-01T16:39:44.046498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create word dictionnary and encode texts sentences to sequences of numbers\nsrc_tokenizer = tokenize(df.source)\ntar_tokenizer = tokenize(df.target)\n\n# Get the size of vocabularies\nsrc_vocabsize =  get_vocab_size(src_tokenizer) \ntar_vocabsize =  get_vocab_size(tar_tokenizer) \n\n# Get length of the longuest sentence in each language \nsrc_seqlen = get_maxlen(df.source)\ntar_seqlen = get_maxlen(df.target)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:39:44.048803Z","iopub.execute_input":"2021-06-01T16:39:44.049111Z","iopub.status.idle":"2021-06-01T16:39:52.933576Z","shell.execute_reply.started":"2021-06-01T16:39:44.04908Z","shell.execute_reply":"2021-06-01T16:39:52.932812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Dataset splitting**","metadata":{}},{"cell_type":"code","source":"# SPlit the dataset into three data \n# Train set, Validation set and Test set\ntrain_set, test_set = train_test_data(df, sample_size=df.shape[0], test_size=0.081)\ntrain_set, val_set = train_test_data(train_set, sample_size=train_set.shape[0], test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:39:52.934787Z","iopub.execute_input":"2021-06-01T16:39:52.935247Z","iopub.status.idle":"2021-06-01T16:39:53.338376Z","shell.execute_reply.started":"2021-06-01T16:39:52.935215Z","shell.execute_reply":"2021-06-01T16:39:53.337331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<h2 align=\"center\">Data visualisation</h2>**<hr>","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:24:05.862185Z","iopub.execute_input":"2021-06-01T11:24:05.862542Z","iopub.status.idle":"2021-06-01T11:24:05.868411Z","shell.execute_reply.started":"2021-06-01T11:24:05.862506Z","shell.execute_reply":"2021-06-01T11:24:05.867437Z"}}},{"cell_type":"code","source":"def word_count (txt):\n  return len(txt.split(' '))\n\ndef add_stats_data(dataframe):\n  dataframe[f'{target}_word_count'] = dataframe.target.apply(word_count)\n  dataframe[f'{source}_word_count'] = dataframe.source.apply(word_count)\n  return dataframe\n\ndef get_frequency(dataframe, lang, col):\n  frequency = [\n     dataframe\n      .loc[dataframe[col] == i, lang]\n      .count() for i in range(1, 32)\n  ]\n  return frequency\n\ndef plot_bar_line(rows, title, color=\"blue\"):\n  plt.bar(range(1, 32), rows, color=color)\n  plt.plot(range(1, 32), rows, color=color)\n  plt.xticks(range(1, 32), )\n\n  plt.xlabel(\"Number of words\", fontsize='12', fontweight='bold')\n  plt.ylabel(\"Number of word's frequencies\", fontsize='12', fontweight='bold')\n  plt.title(title, fontsize='14', fontweight='bold')\n\ndef visualize_frequency(dataframe, figsize=(18,5)):\n  x={ 'target':get_frequency(dataframe, 'target', f'{target}_word_count'),\n      'source': get_frequency(dataframe, 'source', f'{source}_word_count')}\n\n  fig = plt.figure(figsize=figsize)\n  plt.subplot(121)\n  plot_bar_line(x['source'], f\"{source} (source) words distribution\")\n  plt.subplot(122)\n  plot_bar_line(x['target'], f\"{target} (target) words distribution\")\n  fig.suptitle('Words count frequencies', fontsize=23, fontweight='bold')\n  plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:39:53.339651Z","iopub.execute_input":"2021-06-01T16:39:53.33994Z","iopub.status.idle":"2021-06-01T16:39:53.351214Z","shell.execute_reply.started":"2021-06-01T16:39:53.339911Z","shell.execute_reply":"2021-06-01T16:39:53.349956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Stats**","metadata":{}},{"cell_type":"code","source":"#@title **Data frame sizes**\ndisplay(pd.DataFrame(\n  {\n    \"Train_size\":[train_set.shape[0]],\n    \"Validation_size\":[val_set.shape[0]],\n    \"Test_size\":[test_set.shape[0]],\n  }\n  ,index=[f'{source}/{target}']\n))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:39:53.3528Z","iopub.execute_input":"2021-06-01T16:39:53.353232Z","iopub.status.idle":"2021-06-01T16:39:53.376426Z","shell.execute_reply.started":"2021-06-01T16:39:53.353191Z","shell.execute_reply":"2021-06-01T16:39:53.375181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title\ndisplay(pd.DataFrame(\n  {\n   \"Is_source_language?\":[source=='french', source=='english'],\n   \"Is_target_language?\":[target=='french', target=='english'],\n   \"Vocabulary_size\":[src_vocabsize, tar_vocabsize],\n   \"Sequence_length\":[src_seqlen, tar_seqlen],\n  },\n  index=[source, target]\n))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:39:53.3797Z","iopub.execute_input":"2021-06-01T16:39:53.380022Z","iopub.status.idle":"2021-06-01T16:39:53.394554Z","shell.execute_reply.started":"2021-06-01T16:39:53.379993Z","shell.execute_reply":"2021-06-01T16:39:53.393531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Plots**","metadata":{}},{"cell_type":"code","source":"#@title **Data frame stats**\nplt.style.use('ggplot')\nfig = plt.figure(figsize=(20,7))\nlabels = [\n  [f'Source ({source})', f'Target ({target})'],\n  [f'Source ({source})', f'Target ({target})'],\n  ['Train set', 'Validation set', 'Test set']\n]\ndata = [\n  [src_vocabsize, tar_vocabsize],\n  [src_seqlen, tar_seqlen],\n  [train_set.shape[0], val_set.shape[0], test_set.shape[0]]\n]\ncolors = [\n  ['darkblue', '#02455f'],\n  ['darkblue', '#02455f'],\n  ['darkblue', '#02455f', '#02655f']\n]\nsizes = [[100, 100], [100, 100], [100, 100, 100]]\noffset = [900, 1, 2000]\ntitles = ['Vocabulary size', 'Sequence lenght', 'Dataframe sizes']\n\nfor i in range(3):\n  ax = plt.subplot(1, 3, i+1)\n  ax.bar(labels[i], data[i], color=colors[i])\n  ax.scatter(labels[i], data[i], s=sizes[i], marker='^', c='black')\n  for j, txt in enumerate(data[i]):\n    ax.text(labels[i][j], data[i][j]+offset[i], txt, fontsize=12)\n  if i == 2: ax.yaxis.tick_right()\n  plt.title(titles[i], fontsize='14', fontweight='bold',x=0.5, y=-0.17)\n  plt.xticks(fontsize=12, fontweight=\"bold\")\n  plt.yticks(fontsize=13 )\n  plt.ylabel(\"Size\", fontsize=12, fontweight=\"bold\")\n\nfig.suptitle('Dataframe Stats', fontsize=23, fontweight='bold')\nplt.show()\n\ndf = add_stats_data(df)\nvisualize_frequency(df, figsize=(20,7))\ndf = df.loc[:, ['source', 'target']]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:39:53.396194Z","iopub.execute_input":"2021-06-01T16:39:53.396542Z","iopub.status.idle":"2021-06-01T16:39:55.58979Z","shell.execute_reply.started":"2021-06-01T16:39:53.396509Z","shell.execute_reply":"2021-06-01T16:39:55.588846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<h1 align=\"center\">Deep Learning Model</h1>**<hr>","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T11:24:07.76274Z","iopub.execute_input":"2021-06-01T11:24:07.763041Z","iopub.status.idle":"2021-06-01T11:24:07.769222Z","shell.execute_reply.started":"2021-06-01T11:24:07.763014Z","shell.execute_reply":"2021-06-01T11:24:07.768361Z"}}},{"cell_type":"markdown","source":"## **Batch of data Generator**","metadata":{}},{"cell_type":"markdown","source":"Since we need to fit all of the data (`+150k` rows and a big vocabulary size) in to the model, we do not process all data at once but we process. Instead data will be passed to the model in batch. For that we use [python generator](https://wiki.python.org/moin/Generators 'Go to the documention of python generator').","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\n# Create a generator to avoid machine to crash while training \ndef get_batch_of_data(encoder_in_data, decoder_in_data, batch_size): \n  \"\"\"Take one batch of `batch_size` in `encoder_in_data` and `decoder_in_data`, preprocess them and return (yield) these processed batch of data.\"\"\"  \n  data_size = encoder_in_data.shape[0]\n  while True: ## Needed for keras fit method (an infinite loop)\n    for i in range(0, data_size, batch_size):\n      # Tokenize the batch of encoder data to be fed in the model\n      enc_sentences = encoder_in_data[i:i+batch_size]\n      encoder_input = src_tokenizer.texts_to_sequences(enc_sentences)\n      encoder_input = pad_sequences(encoder_input, padding='post', maxlen=src_seqlen)\n      \n      # Tokenize the batch of decoder data to be fed in the model\n      dec_sentences = decoder_in_data[i:i+batch_size]\n      decoder_input = tar_tokenizer.texts_to_sequences(dec_sentences)\n      decoder_input = pad_sequences(decoder_input, padding='post', maxlen=tar_seqlen)\n      decoder_input_data = decoder_input[:, :-1] # Do not get the last word 'eos'|'fin'\n\n      # Tokenize the batch of decoder data to be fed in the model\n      # Since the decoder outputs will use a dense layer to make word classification\n      # We we need to make One hot-encoding for the decoder target output data \n      decoder_output = to_categorical(decoder_input, num_classes=tar_vocabsize)\n      decoder_output = decoder_output[:, 1:, :] # Do not get the first word 'sos'|'debut'\n\n      inputs = [encoder_input, decoder_input_data]\n      outputs = decoder_output\n      yield inputs, outputs","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-01T16:39:55.59093Z","iopub.execute_input":"2021-06-01T16:39:55.591189Z","iopub.status.idle":"2021-06-01T16:39:55.59916Z","shell.execute_reply.started":"2021-06-01T16:39:55.591164Z","shell.execute_reply":"2021-06-01T16:39:55.598029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **RNN Model using GRU, Embedding and Dense layer in both encoder and decoder**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Embedding, GRU\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:39:55.600444Z","iopub.execute_input":"2021-06-01T16:39:55.600713Z","iopub.status.idle":"2021-06-01T16:39:55.617097Z","shell.execute_reply.started":"2021-06-01T16:39:55.600687Z","shell.execute_reply":"2021-06-01T16:39:55.616177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Model defininition**","metadata":{}},{"cell_type":"code","source":"def create_train_model(\n  enc_seqlen, dec_seqlen, enc_vocabsize, dec_vocabsize, hsize=512, \n  embsize=512, encoder_dropout=0.0, decoder_dropout=0.0, \n):\n  ### Encoder\n  encoder_inputs = Input(shape=(enc_seqlen,), name=\"encoder_input\")\n  embedding = Embedding(\n      enc_vocabsize, embsize, name=\"encoder_embedding\"\n  )\n  encoder_gru = GRU(\n      hsize, return_state=True, name=\"encoder_gru\", dropout=encoder_dropout\n  )\n  encoder_emb = embedding(encoder_inputs)\n  encoder_outputs, encoder_state = encoder_gru(encoder_emb)\n\n  ### Decoder\n  decoder_inputs = Input(shape=(dec_seqlen - 1,), name=\"decoder_input\")\n  embedding = Embedding(\n      dec_vocabsize, embsize, input_length=dec_seqlen-1, name=\"decoder_embedding\"\n   )\n  decoder_gru = GRU(\n      hsize, return_state=True, return_sequences=True, name=\"decoder_gru\", \n      dropout=decoder_dropout\n  )\n\n  decoder_emb = embedding(decoder_inputs)\n  decoder_outputs, _ = decoder_gru(decoder_emb, initial_state=encoder_state)\n  dense_layer = Dense(dec_vocabsize, activation=\"softmax\", name=\"dense_layer\")\n  decoder_outputs = dense_layer(decoder_outputs)\n\n  # Define the Model which accepts encoder/decoder inputs and outputs predictions\n  model = Model(\n      inputs=[encoder_inputs, decoder_inputs],\n      outputs=decoder_outputs,\n      name=\"encoder_decoder_model\",\n  )\n  return model","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:39:55.618379Z","iopub.execute_input":"2021-06-01T16:39:55.618692Z","iopub.status.idle":"2021-06-01T16:39:55.630591Z","shell.execute_reply.started":"2021-06-01T16:39:55.618665Z","shell.execute_reply":"2021-06-01T16:39:55.629501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_train(model, train_data, val_data, batch_size, epochs, callbacks=[]):\n  train_generator = get_batch_of_data(\n    train_data.source, train_data.target, batch_size\n  )\n  val_generator = get_batch_of_data(\n    val_data.source, val_data.target, batch_size\n  )\n  number_of_steps = train_data.shape[0]//batch_size\n\n  history = model.fit(\n    x=train_generator,\n    validation_data=val_generator,\n    epochs=epochs,\n    steps_per_epoch=number_of_steps,\n    validation_steps = number_of_steps,\n    callbacks=callbacks,\n  )\n  return history","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:39:55.63183Z","iopub.execute_input":"2021-06-01T16:39:55.632102Z","iopub.status.idle":"2021-06-01T16:39:55.6486Z","shell.execute_reply.started":"2021-06-01T16:39:55.632076Z","shell.execute_reply":"2021-06-01T16:39:55.647412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Some paths**","metadata":{}},{"cell_type":"code","source":"import os\nmodel_path = 'model/nmt_model'\nmodel_weights_path = 'model/weights/weights.ckpt'\nmodel_history_path = 'model/history.csv'\nos.makedirs(name='model/weights/', exist_ok=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:39:55.650288Z","iopub.execute_input":"2021-06-01T16:39:55.650742Z","iopub.status.idle":"2021-06-01T16:39:55.663996Z","shell.execute_reply.started":"2021-06-01T16:39:55.650695Z","shell.execute_reply":"2021-06-01T16:39:55.66283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_model_data(model, history):\n  # Save the training history\n  pd.DataFrame(history.history).to_csv(model_history_path, index=False)\n  # Save the full model, weights, opt,...\n  model.save(model_path, overwrite=True)\n    \ndef load_saved_model_data():\n  # Load saved model and get only the best weights\n  model = tf.keras.models.load_model(model_path)\n  model.load_weights(model_weights_path)\n  # Get training history\n  history = tf.keras.callbacks.History()\n  history.history = pd.read_csv(model_history_path).to_dict('list')\n  return model, history","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:39:55.667857Z","iopub.execute_input":"2021-06-01T16:39:55.668275Z","iopub.status.idle":"2021-06-01T16:39:55.683363Z","shell.execute_reply.started":"2021-06-01T16:39:55.668239Z","shell.execute_reply":"2021-06-01T16:39:55.682593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<h2 align=\"center\">Train the Model with cross validation</h2>**<hr>","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"### **Model creation** ","metadata":{}},{"cell_type":"code","source":"gru_hidden_units = 1060 \nemmbedding_size  = 256  \nencoder_dropout  = 0.1  \ndecoder_dropout  = 0.25 \nmodel = create_train_model(\n  src_seqlen, tar_seqlen, src_vocabsize, tar_vocabsize, \n  gru_hidden_units, emmbedding_size, \n  encoder_dropout, decoder_dropout\n)\n\nmodel.compile(\n  optimizer='adam', loss='categorical_crossentropy', metrics=['acc']\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:39:55.685284Z","iopub.execute_input":"2021-06-01T16:39:55.685717Z","iopub.status.idle":"2021-06-01T16:39:59.174458Z","shell.execute_reply.started":"2021-06-01T16:39:55.685685Z","shell.execute_reply":"2021-06-01T16:39:59.173548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model architecture\ndisplay(plot_model(model, show_shapes=True, to_file='train_model.png'))\n# display(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:39:59.17575Z","iopub.execute_input":"2021-06-01T16:39:59.176026Z","iopub.status.idle":"2021-06-01T16:39:59.667859Z","shell.execute_reply.started":"2021-06-01T16:39:59.175997Z","shell.execute_reply":"2021-06-01T16:39:59.666739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Training**","metadata":{}},{"cell_type":"code","source":"save_best_weights = tf.keras.callbacks.ModelCheckpoint(\n  model_weights_path, monitor='val_loss', mode='min',\n  save_weights_only=True, save_best_only=True, verbose=1, \n)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n  monitor='val_loss', patience=2, verbose=1,\n  mode='min', restore_best_weights=True, \n)\n\nbatch_size = 88 \nepochs = 100     \n\nhistory = make_train(\n  model, train_set, val_set, \n  batch_size, epochs,[save_best_weights, early_stopping]\n)\nsave_model_data(model, history)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T11:24:13.062766Z","iopub.execute_input":"2021-06-01T11:24:13.063035Z","iopub.status.idle":"2021-06-01T15:58:53.532457Z","shell.execute_reply.started":"2021-06-01T11:24:13.063008Z","shell.execute_reply":"2021-06-01T15:58:53.53157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Load the model with the best weights**","metadata":{}},{"cell_type":"code","source":"a_model, history = load_saved_model_data()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**<h1 align=\"center\">Model Testing and Generation of translation</h1>**<hr>\n","metadata":{"_kg_hide-input":true}},{"cell_type":"markdown","source":"## **Model evaluation**","metadata":{}},{"cell_type":"code","source":"batch_size = 70\nnumber_of_steps = test_set.shape[0]//batch_size\ntest_generator = get_batch_of_data(\n  test_set.source, test_set.target, batch_size\n)\nevaluation = a_model.evaluate(\n  x=test_generator, steps=number_of_steps, verbose=1\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T15:58:59.802772Z","iopub.execute_input":"2021-06-01T15:58:59.803059Z","iopub.status.idle":"2021-06-01T16:00:58.968801Z","shell.execute_reply.started":"2021-06-01T15:58:59.803035Z","shell.execute_reply":"2021-06-01T16:00:58.967968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(\n    {\n        \"Loss\":[f\"{evaluation[0]*100:.3f}%\"],\n        \"Accuracy\": [f\"{evaluation[1]*100:.3f}%\"]\n    },\n    index=[\"Loss/Accuracy\"]\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:00:58.972683Z","iopub.execute_input":"2021-06-01T16:00:58.974681Z","iopub.status.idle":"2021-06-01T16:00:58.99153Z","shell.execute_reply.started":"2021-06-01T16:00:58.97464Z","shell.execute_reply":"2021-06-01T16:00:58.990537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Learning curves**","metadata":{}},{"cell_type":"code","source":"def plot_one_history(x, y1, y2, input='loss', y1_label='Loss', y2_label='Validation loss'):\n  title = \"Loss's history\"\n  if input == 'acc':\n    y1_label, y2_label = 'Accuracy', 'Validation Accuracy'\n    title = \"Accuracy's history\"\n  y_label = y1_label\n  labels=('Epochs', 'acc')\n  plt.plot(x, y1, label=y1_label)\n  plt.plot(x, y2, label=y2_label)\n  plt.xlabel(\"Epochs\", c='darkred')\n  plt.ylabel(y_label, c='darkblue')\n  plt.title(title, c='darkgreen')\n  plt.legend()\n\ndef plot_history(hist, figsize=(17, 5)):\n  plt.figure(figsize=figsize)\n  nb_epoch = range(1, len(hist.history['acc'])+1)\n  plt.subplot(121)\n  accuracies = hist.history['acc']\n  val_accuracies = hist.history['val_acc']\n  plot_one_history(x=nb_epoch, y1=accuracies, y2=val_accuracies, input='acc')\n\n  plt.subplot(122)\n  losses     = hist.history['loss']\n  val_losses = hist.history['val_loss']\n  plot_one_history(x=nb_epoch, y1=losses, y2=val_losses, input='loss')\n  plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:00:58.995159Z","iopub.execute_input":"2021-06-01T16:00:58.997163Z","iopub.status.idle":"2021-06-01T16:00:59.009973Z","shell.execute_reply.started":"2021-06-01T16:00:58.997123Z","shell.execute_reply":"2021-06-01T16:00:59.009226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)\n# A huge overfitting thought","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:00:59.014254Z","iopub.execute_input":"2021-06-01T16:00:59.016561Z","iopub.status.idle":"2021-06-01T16:00:59.461658Z","shell.execute_reply.started":"2021-06-01T16:00:59.016524Z","shell.execute_reply":"2021-06-01T16:00:59.460783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Inference Model (Model for Translation)**","metadata":{}},{"cell_type":"code","source":"display(Markdown(\"<h1 align='center'>Creation of the inference model using the weight of trained model</h1>\"))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:00:59.465912Z","iopub.execute_input":"2021-06-01T16:00:59.46803Z","iopub.status.idle":"2021-06-01T16:00:59.477926Z","shell.execute_reply.started":"2021-06-01T16:00:59.467984Z","shell.execute_reply":"2021-06-01T16:00:59.476931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_inference_encoder_from(input_layer, embedding_layer, gru_layer):\n  encoder_embedding = embedding_layer(input_layer)\n  encoder_outputs, encoder_state = gru_layer(encoder_embedding)\n  encoder = Model(input_layer, encoder_state)\n  return encoder\n\ndef create_inference_decoder_from(input_layer, embedding_layer, gru_layer, dense_layer):\n  # decoder_input = Input(shape=(1,))\n  decoder_embedding = embedding_layer(input_layer)\n  input_shape = dense_layer.input.shape[-1]\n  decoder_inputs_state = Input(shape=(input_shape,))\n\n  decoder_output, decoder_output_state = gru_layer(\n    decoder_embedding, initial_state=decoder_inputs_state\n  )\n\n  decoder_prediction = dense_layer(decoder_output)\n  decoder = Model(\n    inputs=[input_layer, decoder_inputs_state], \n    outputs=[decoder_prediction, decoder_output_state]\n  )\n  return decoder","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-01T16:00:59.482623Z","iopub.execute_input":"2021-06-01T16:00:59.484837Z","iopub.status.idle":"2021-06-01T16:00:59.494485Z","shell.execute_reply.started":"2021-06-01T16:00:59.484795Z","shell.execute_reply":"2021-06-01T16:00:59.49362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(Markdown(\"<h3 align='center'>Here the name of each layer in the train model (for encoder && decoder)</h3>\"))\nlayers_name = [layer.name for layer in a_model.layers]; print(layers_name)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:00:59.499141Z","iopub.execute_input":"2021-06-01T16:00:59.501563Z","iopub.status.idle":"2021-06-01T16:00:59.516642Z","shell.execute_reply.started":"2021-06-01T16:00:59.50152Z","shell.execute_reply":"2021-06-01T16:00:59.515839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder = create_inference_encoder_from(\n  a_model.get_layer('encoder_input').input,\n  a_model.get_layer('encoder_embedding'),\n  a_model.get_layer('encoder_gru')\n)\n\ndecoder = create_inference_decoder_from(\n  a_model.get_layer('decoder_input').input,\n  a_model.get_layer('decoder_embedding'),\n  a_model.get_layer('decoder_gru'),\n  a_model.get_layer('dense_layer'),\n)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:00:59.519509Z","iopub.execute_input":"2021-06-01T16:00:59.520839Z","iopub.status.idle":"2021-06-01T16:01:00.663981Z","shell.execute_reply.started":"2021-06-01T16:00:59.520799Z","shell.execute_reply":"2021-06-01T16:01:00.6631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Inference Encoder/decoder architecture**","metadata":{}},{"cell_type":"code","source":"img_encoder = plot_model(encoder, show_shapes=True, to_file='encoder.png')\nimg_decoder = plot_model(decoder, show_shapes=True, to_file='decoder.png')\n\ndisplay(img_encoder)\ndisplay(img_decoder)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:01:00.665319Z","iopub.execute_input":"2021-06-01T16:01:00.66564Z","iopub.status.idle":"2021-06-01T16:01:01.032452Z","shell.execute_reply.started":"2021-06-01T16:01:00.665603Z","shell.execute_reply":"2021-06-01T16:01:01.031588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Generating Translation** ","metadata":{}},{"cell_type":"code","source":"def decode_sentence(sentence):\n  sentence = cleaning(sentence)\n  sequence = src_tokenizer.texts_to_sequences([sentence])\n  sequence = pad_sequences(sequence, padding='post', maxlen=src_seqlen)\n\n  state = encoder.predict(sequence)\n  tar_seq = np.zeros((1, 1))\n  tar_seq[0,0] = tar_tokenizer.word_index[begin]\n  sequence_translated = ''\n\n  for i in range(tar_seqlen):\n    de_prob, state = decoder.predict([tar_seq, state])\n    idx_of_predicted_word = np.argmax(de_prob[0,-1,:])\n    predicted_word = tar_tokenizer.index_word[idx_of_predicted_word]\n    \n    if predicted_word == end: break\n    sequence_translated += f'{predicted_word} '\n\n    tar_seq = np.zeros((1, 1))\n    tar_seq[0,0] = idx_of_predicted_word\n  return sequence_translated ","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:38:06.712423Z","iopub.execute_input":"2021-06-01T16:38:06.712775Z","iopub.status.idle":"2021-06-01T16:38:06.720251Z","shell.execute_reply.started":"2021-06-01T16:38:06.712742Z","shell.execute_reply":"2021-06-01T16:38:06.719199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cleaning(\"I'm having dinner, so i'm busy\")\ndef get_random_sentence_on(dataframe):\n  index = np.random.randint(dataframe.shape[0])\n  src = dataframe.source.iloc[index]\n  tar = dataframe.target.iloc[index]\n  tar = re.sub(f'{begin} (.*) {end}', '\\g<1>', tar)\n  return src, tar, index","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:38:09.334674Z","iopub.execute_input":"2021-06-01T16:38:09.335002Z","iopub.status.idle":"2021-06-01T16:38:09.340232Z","shell.execute_reply.started":"2021-06-01T16:38:09.33497Z","shell.execute_reply":"2021-06-01T16:38:09.339104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set a input sentence and rerun the cell \nsentences = [\n  \"le professeur est bon\",\n  \"L'université Ibn Tofail\", # OOV => `Ibn Tofail`\n  \"Ne soyez pas en colère\",\n  \"Comment avez-vous réussi le projet?\",\n  \"Je suis ton père\",\n  \"En été, la vie est belle\",\n  \"L'Afrique est un continent\",\n  \"Le model n'a pas bien appris\",\n  \"Le Maroc est un pays maghrebin\" # Bias detected,\n] \n\nfor sentence in sentences:\n    translation = decode_sentence(sentence)\n    display(Markdown(\n      f'''> **{source} source** : {sentence}  \n          > **{target} translated** : {translation}'''))\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T16:38:10.125786Z","iopub.execute_input":"2021-06-01T16:38:10.126132Z","iopub.status.idle":"2021-06-01T16:38:12.20348Z","shell.execute_reply.started":"2021-06-01T16:38:10.126087Z","shell.execute_reply":"2021-06-01T16:38:12.202692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Random phrases in the dataset**","metadata":{}},{"cell_type":"code","source":"#@title **Random phrases in the dataset** { run: \"auto\" }\nsources = []\ntargets = []\ntranslated = []\nnumber_of_sentences_to_translate = 48 #@param {type:\"slider\", min:1, max:100, step:1}\nfor i in range(number_of_sentences_to_translate):\n  src, tar, _ = get_random_sentence_on(test_set)\n  translation = decode_sentence(src)\n  sources.append(src)\n  targets.append(tar)\n  translated.append(translation)\n\n(\n pd.DataFrame(\n  {source: sources,target: targets, \n   f'{target} translated': translated})\n  .style.set_properties(**{'text-align': 'left'})\n  .set_table_styles(\n  [\n    {'selector': 'tr:hover',\n      'props': [('background-color', '#089'), ('color', '#fff')]\n    },\n  ])\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-01T16:04:18.13142Z","iopub.execute_input":"2021-06-01T16:04:18.131761Z","iopub.status.idle":"2021-06-01T16:04:34.88222Z","shell.execute_reply.started":"2021-06-01T16:04:18.131728Z","shell.execute_reply":"2021-06-01T16:04:34.881389Z"},"trusted":true},"execution_count":null,"outputs":[]}]}