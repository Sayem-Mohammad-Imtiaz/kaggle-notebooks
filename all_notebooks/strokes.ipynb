{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install seaborn --upgrade","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set()\nsns.__version__","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_index('id', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# check out some data"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.columns[:-1]:\n    sns.displot(df, x=col, hue='stroke')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"nan check"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df.columns:\n    if df[col].isnull().sum() > 0:\n        print(col,':', df[col].isnull().sum(), 'nan values')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"look into bmi"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df[['age', 'avg_glucose_level']].columns:\n    print(f'bmi corr with {col}: ', df['bmi'].corr(df[col]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['bmi'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.displot(df[df['bmi'].isnull()], x='stroke')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p1 = df[df['bmi'].isnull()]['stroke'].sum() / len(df[df['bmi'].isnull()]) * 100\np2 = df['stroke'].sum() / len(df) * 100\nprint(f'{p1.round()}% of subjects with unreported bmi had a stroke')\nprint(f'{p2.round()}% of subjects in data set had a stroke')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"those that don't report bmi are more likely to have a stroke"},{"metadata":{},"cell_type":"markdown","source":"# data processing"},{"metadata":{},"cell_type":"markdown","source":"split data for processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df.drop('stroke', axis=1)\ny = df['stroke']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = X_train.join(y_train)\ntest = X_test.join(y_test)\ndfs = [train, test]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"further outside research found that these are all risk factors for stroke : \n\n* high blood pressure\n* obesity : bmi >= 30\n* diabetic\n* smoking history\n* heart disease\n* gender : female\n* age : over 65 (70% strokes over age 65)\n* prior stroke\n* stress\n* hyperglycemia : >= 108 mg/dL (observed in 2/3 of ischemic strokes)\n\n* unreported bmi? could just be sample specific "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['bmi_unreported'] = np.where(train['bmi'].isnull(), 1, 0)\ntest['bmi_unreported'] = np.where(test['bmi'].isnull(), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train bmi median : ', train['bmi'].median())\nprint('test bmi median : ', test['bmi'].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"by replacing the missing bmi with the median, we won't pumping up the obese numbers ( >= 30 )"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['bmi'].fillna(train['bmi'].median(), inplace=True)\ntest['bmi'].fillna(test['bmi'].median(), inplace=True)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"nan check 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in dfs:\n    for col in df.columns:\n        if df[col].isnull().sum() > 0:\n            print(col,':', df[col].isnull().sum(), 'nan values')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in dfs:\n    df['ever_married'].replace({'Yes': 1, 'No': 0}, inplace=True)\n    df['Residence_type'].replace({'Urban': 1, 'Rural': 0}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dummies(df):\n    df = pd.get_dummies(df, columns=['work_type'], prefix='work')\n    df = pd.get_dummies(df, columns=['smoking_status'], prefix='smoking')\n    df = pd.get_dummies(df, columns=['gender'], prefix='gender')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = dummies(train)\ntest = dummies(test)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns = train.columns.str.lower()\ntest.columns = test.columns.str.lower()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"only 1 sample of gender_other so let's delete it"},{"metadata":{"trusted":true},"cell_type":"code","source":"test[test['gender_other']==1]['gender_other'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test.drop('gender_other', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for df in dfs:\n    print(df['avg_glucose_level'].describe(), '\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"add risk factors according to research"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_features(df):\n    df['obese'] = np.where(df['bmi'] >= 30, 1, 0)\n    df['hyperglycemic'] = np.where(df['avg_glucose_level'] >=108, 1, 0)\n    df['over_65'] = np.where(df['age'] >=65, 1, 0)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_features(train)\nadd_features(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"add a total risk factors feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"risk_factors = ['obese', 'hyperglycemic', 'over_65', 'gender_female', 'heart_disease', \n                'hypertension', 'smoking_formerly smoked', 'smoking_smokes']\ntrain['risk_factors'] = train[risk_factors].sum(axis=1)\ntest['risk_factors'] = test[risk_factors].sum(axis=1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PCA analysis of some features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ndef pca(X):\n    Xp = (X - X.mean(axis=0)) / X.std(axis=0)\n    pca = PCA(random_state=0)\n    X_pca = pca.fit_transform(Xp)\n    comps = [f'PC{1+i}' for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=comps, index=X.index)\n    \n    loadings = pd.DataFrame(pca.components_.T, columns=comps, index=X.columns)\n\n    return pca, X_pca, loadings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_features = ['age', 'avg_glucose_level', 'bmi', 'risk_factors']\npca_train, X_pca_train, loadings_train = pca(train[pca_features])\npca_test, X_pca_test, loadings_test = pca(test[pca_features])\nloadings_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loadings_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"looks like age * risk_factors may be a promising feature "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['age*risk_factors'] = train['age'] * (train['risk_factors'])\ntest['age*risk_factors'] = test['age'] * (test['risk_factors'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"add pca features to the data sets"},{"metadata":{},"cell_type":"markdown","source":"multiply test pc2 and pc4 by -1 to get into same orientation as training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.join(X_pca_train)\ntest = test.join(X_pca_test)\ntest['PC2'] = test['PC2'] * (-1)\ntest['PC4'] = test['PC4'] * (-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"get original features just for reference"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = train.columns\nadded_features = ['obese', 'hyperglycemic', 'over_65', 'risk_factors', \n                  'bmi_unreported', 'PC1', 'PC2', 'PC3', 'PC4', 'age*risk_factors']\nog_features = [x for x in features if x not in added_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = train[og_features]\n# test = test[og_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train.drop('stroke', axis=1)\ny_train = train['stroke']\n\nX_test = test.drop('stroke', axis=1)\ny_test = test['stroke']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"make sure the columns match between train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"print([x for x in X_test.columns if x not in X_train.columns])\nprint([x for x in X_train.columns if x not in X_test.columns])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# classifier"},{"metadata":{},"cell_type":"markdown","source":"heavily skewed towards non stroke subjects"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('y_train mean : ', y_train.mean())\nprint('y_test mean : ', y_test.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"scale our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"use smote to oversample the stroke data since it is heavily skewed"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nX_train_smote, y_train_smote = SMOTE().fit_resample(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('y_train_smote mean : ', y_train_smote.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"time to train and analyze some models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, auc, RocCurveDisplay\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = dict()\nrs = 0\nmodels['xgb clf'] = XGBClassifier(n_estimators=300, learning_rate=0.05, random_state=rs)\nmodels['gbd tree'] = GradientBoostingClassifier(random_state=rs)\nmodels['random forests'] = RandomForestClassifier(random_state=rs)\nmodels['log reg'] = LogisticRegression(random_state=rs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in models:\n    models[model].fit(X_train_smote, y_train_smote)\n    print(f'{model} : ✔')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_test(model):\n    model = models[m]\n    pred = model.predict(X_test_scaled)\n    pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n    auc = roc_auc_score(y_test, pred_proba)\n    cf = confusion_matrix(y_test, pred)\n    cr = classification_report(y_test, pred)\n    \n    print('- - - - - -\\n', m, '\\n')\n    print('roc auc: ', auc)\n    print(cf)\n    print(cr, '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for m in models:\n   model_test(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nfor m in models:\n    model = models[m]\n    prediction = models[m].predict_proba(X_test_scaled)[:, 1]\n    fpr, tpr, thresholds = roc_curve(y_test, prediction)\n    plt.plot(fpr, tpr, label=m)\nplt.xlabel('false positive rate')\nplt.ylabel('true positive rate')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the recall score is important in this analysis since we want to catch all the possible strokes even if we misclassify some"},{"metadata":{},"cell_type":"markdown","source":"we'll continue with the logistic regressor since it has the highest recall score by far"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_params = [{'solver':['liblinear'], 'penalty':['l1', 'l2'], 'C':[1.0, 10.0, 100.0]},\n              {'solver':['lbfgs'], 'penalty':['none', 'l2'], 'C':[1.0, 10.0, 100.0]}]\nlog_grid = GridSearchCV(models['log reg'], param_grid=log_params, scoring='recall').fit(X_train_smote, y_train_smote)\nprint(log_grid.best_score_)\nprint(log_grid.best_params_)\nprint(log_grid.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_test(log_grid.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_predicted = log_grid.best_estimator_.predict(X_test_scaled)\nsub = pd.DataFrame(data={'id': X_test.index, 'stroke':final_predicted})\nsub.to_csv('stroke_sub.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thanks to samsatp's notebook for guidance on the modeling structure. Their notebook can be found here : [https://www.kaggle.com/sathianpong/stroke-eda-visualization-prediction](http://)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}