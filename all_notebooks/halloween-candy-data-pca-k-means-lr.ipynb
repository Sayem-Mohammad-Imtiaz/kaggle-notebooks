{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Preliminary Steps."},{"metadata":{},"cell_type":"markdown","source":"**Import Libraries.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom IPython.display import display","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some Notebook Settings.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"warnings.filterwarnings('ignore') # ignore warnings.\n%config IPCompleter.greedy = True # autocomplete feature.\npd.options.display.max_rows = None # set maximum rows that can be displayed in notebook.\npd.options.display.max_columns = None # set maximum columns that can be displayed in notebook.\npd.options.display.precision = 2 # set the precision of floating point numbers.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check Encoding of Data.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"# # Check the encoding of data. Use ctrl+/ to comment/un-comment.\n\n# import chardet\n\n# rawdata = open('candy-data.csv', 'rb').read()\n# result = chardet.detect(rawdata)\n# charenc = result['encoding']\n# print(charenc)\n# print(result) # It's utf-8 with 99% confidence.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Read Data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/candy-data.csv', encoding='utf-8')\ndf.drop_duplicates(inplace=True) # drop duplicates if any.\ndf.shape # num rows x num columns.","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"(85, 13)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation."},{"metadata":{},"cell_type":"markdown","source":"**Check for missing values.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"(df.isnull().sum()/len(df)*100).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing values."},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a total of 12 variables that describe a chocolate. 9 of them are categorical and the rest, i.e. 3 are numerical variables."},{"metadata":{},"cell_type":"markdown","source":"1. chocolate: Does it contain chocolate?\n2. fruity: Is it fruit flavored?\n3. caramel: Is there caramel in the candy?\n4. peanutalmondy: Does it contain peanuts, peanut butter or almonds?\n5. nougat: Does it contain nougat?\n6. crispedricewafer: Does it contain crisped rice, wafers, or a cookie component?\n7. hard: Is it a hard candy?\n8. bar: Is it a candy bar?\n9. pluribus: Is it one of many candies in a bag or box?\n10. sugarpercent: The percentile of sugar it falls under within the data set.\n11. pricepercent: The unit price percentile compared to the rest of the set.\n12. winpercent: The overall win percentage according to 269,000 matchups."},{"metadata":{"trusted":false},"cell_type":"code","source":"df['winpercent'] = df['winpercent']/100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Deriving new features.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"df['sugarbyprice'] = df['sugarpercent'].div(df['pricepercent']) # higher value means the candy is sweet as well as cheap.\ndf['winbyprice'] = df['winpercent'].div(df['pricepercent']) # higher value means the candy is more liked as well as cheap.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"categorival_vars = ['chocolate', 'fruity', 'caramel', 'peanutyalmondy', 'nougat', 'crispedricewafer', 'hard', 'bar',\n                    'pluribus']\nnumerical_vars = ['sugarpercent', 'pricepercent', 'winpercent', 'sugarbyprice', 'winbyprice']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## Data Understanding."},{"metadata":{},"cell_type":"markdown","source":"**Some Questions one might ask.**"},{"metadata":{},"cell_type":"markdown","source":"1. Top 10 winner candies."},{"metadata":{"trusted":false},"cell_type":"code","source":"df['competitorname'] = df['competitorname'].str.replace('Ã•', \"'\") # Special character was appearing in name of candy.\ndf.sort_values(by=['winpercent', 'sugarpercent'], ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reese's seem to be a favourite. Note that all the top competitors are chocolaty as well. Also, Reese's Miniatures is very cheap when compared to top competitors and overall as well."},{"metadata":{},"cell_type":"markdown","source":"2. Competitors which are not chocolaty but winners."},{"metadata":{"trusted":false},"cell_type":"code","source":"df[df['chocolate']==0].sort_values(by=['winpercent', 'sugarpercent'], ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sour Patch Kids has a high `winbyprice`. They are cheap as well as a favourite."},{"metadata":{},"cell_type":"markdown","source":"3. Top `winbyprice` competitors."},{"metadata":{"trusted":false},"cell_type":"code","source":"df.sort_values(by=['winbyprice', 'winpercent'], ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tootsie Roll Midgies seems to give a bang for buck."},{"metadata":{},"cell_type":"markdown","source":"4. Top 10 sugary candies."},{"metadata":{"trusted":false},"cell_type":"code","source":"df.sort_values(by=['sugarpercent', 'winpercent'], ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5. Which candies are both chocolaty as well as fruity?"},{"metadata":{"trusted":false},"cell_type":"code","source":"df[(df['chocolate']==1)&(df['fruity']==1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"**Correlation Heatmap.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (20,8))        \nsns.heatmap(df.corr(),annot=True, cmap = 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We do have some correlation between features. We can use PCA for treating correlation as well as dimensionality reduction."},{"metadata":{},"cell_type":"markdown","source":"Should we drop correlated variables before performing K-means? -> https://stats.stackexchange.com/questions/62253/do-i-need-to-drop-variables-that-are-correlated-collinear-before-running-kmeans"},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## Principal Components Analysis."},{"metadata":{},"cell_type":"markdown","source":"**Perform PCA.**"},{"metadata":{},"cell_type":"markdown","source":"scikit has 4 steps -> import, instantiate, fit, transform."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Improting the PCA module. \n\nfrom sklearn.decomposition import PCA # import.\npca = PCA(svd_solver='randomized', random_state=123) #instantiate.\npca.fit(df.drop('competitorname', axis=1)) # fit.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Scree Plot.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Making the screeplot - plotting the cumulative variance against the number of components\n\nfig = plt.figure(figsize = (20,5))\nax = plt.subplot(121)\nplt.plot(pca.explained_variance_ratio_)\nplt.xlabel('principal components')\nplt.ylabel('explained variance')\n\nax2 = plt.subplot(122)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First 2 or 3 components are suggested by using the elbow method."},{"metadata":{},"cell_type":"markdown","source":"**Percentage of Variance retained.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"# what percentage of variance in data can be explained by first 2,3 and 4 principal components respectively?\n(pca.explained_variance_ratio_[0:2].sum().round(3),\npca.explained_variance_ratio_[0:3].sum().round(3),\npca.explained_variance_ratio_[0:4].sum().round(3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize Principal Components Loadings.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"# we'll use first 2 principal components as it retains 95% of variance.\n\ndf_pca_2_comp = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':df.drop(\n                              'competitorname', axis=1).columns})\n# df_pca_2_comp","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# we can visualize what the principal components seem to capture.\n\nfig = plt.figure(figsize = (6,6))\nplt.scatter(df_pca_2_comp.PC1, df_pca_2_comp.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(df_pca_2_comp.Feature):\n    plt.annotate(txt, (df_pca_2_comp.PC1[i],df_pca_2_comp.PC2[i]))\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Except `sugarbyprice` and `winbyprice`, all the other features seem to be clustered."},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"**Transform Data.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_pca = pca.transform(df.drop('competitorname', axis=1)) # our data transformed with new features as principal components.\ndf_pca = df_pca[:, 0:2] # Since we require first two principal components only.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Scale Data.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nstandard_scaler = StandardScaler()\ndf_s = standard_scaler.fit_transform(df_pca) # s in df_s stands for scaled.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Visualize Principal Components.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot(pd.DataFrame(df_s)) # Try to get some intuiton of data.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One cluster is very clearly visible. Seems to me that the second cluster will contain the data points not in the first cluster. Two clusters might suffice."},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## Clustering of Data."},{"metadata":{},"cell_type":"markdown","source":"**Is the data clusterable?**"},{"metadata":{},"cell_type":"markdown","source":"Hopkin's Statstic will tell us if the data is clusterable or not. If it is less than 0.5, clusters are not statistically significant."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) / (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"hopkins(pd.DataFrame(df_s))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes, Hopkin's statistic claims that this data is indeed highly clusterable."},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"**Clustering.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.cluster import KMeans # import.\n\n# silhouette scores to choose number of clusters.\nfrom sklearn.metrics import silhouette_score\ndef sil_score(df):\n    sse_ = []\n    for k in range(2, 15):\n        kmeans = KMeans(n_clusters=k, random_state=123).fit(df_s) # fit.\n        sse_.append([k, silhouette_score(df, kmeans.labels_)])\n    plt.plot(pd.DataFrame(sse_)[0], pd.DataFrame(sse_)[1])\n\nsil_score(df_s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maximum silhouette score at k=2."},{"metadata":{"trusted":false},"cell_type":"code","source":"# sum of squared distances.\n\ndef plot_ssd(df):\n    ssd = []\n    for num_clusters in list(range(1,19)):\n        model_clus = KMeans(n_clusters = num_clusters, max_iter=50, random_state=123)\n        model_clus.fit(df)\n        ssd.append(model_clus.inertia_)\n    plt.plot(ssd)\n\nplot_ssd(df_s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Elbow seems to form at 2."},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"**K-Means with 2 clusters.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"# K-means with K=2.\nkm2c = KMeans(n_clusters=2, max_iter=50, random_state=93)\nkm2c.fit(df_s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# creation of data frame with original features for analysis of clusters formed.\n\ndf_dummy = pd.DataFrame.copy(df)\ndfkm2c = pd.concat([df_dummy, pd.Series(km2c.labels_)], axis=1)\ndfkm2c.rename(columns={0:'Cluster ID'}, inplace=True)\n# dfkm2c.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# creation of data frame with features as principal components for analysis of clusters formed.\n\ndf_dummy = pd.DataFrame.copy(pd.DataFrame(df_s))\ndfpcakm2c = pd.concat([df_dummy, pd.Series(km2c.labels_)], axis=1)\ndfpcakm2c.columns = ['PC1', 'PC2', 'Cluster ID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot(data=dfpcakm2c, vars=['PC1', 'PC2'], hue='Cluster ID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"**K-means with 5 clusters.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"# K-means with K=5.\nkm5c = KMeans(n_clusters=5, max_iter=50, random_state=123)\nkm5c.fit(df_s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# creation of data frame with original features for analysis of clusters formed.\n\ndf_dummy = pd.DataFrame.copy(df)\ndfkm5c = pd.concat([df_dummy, pd.Series(km5c.labels_)], axis=1) # df-dataframe, km-kmeans, 5c-5clusters.\ndfkm5c.rename(columns={0:'Cluster ID'}, inplace=True)\n# dfkm5c.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# creation of data frame with features as principal components for analysis of clusters formed.\n\ndf_dummy = pd.DataFrame.copy(pd.DataFrame(df_s))\ndfpcakm5c = pd.concat([df_dummy, pd.Series(km5c.labels_)], axis=1)\ndfpcakm5c.columns = ['PC1', 'PC2', 'Cluster ID']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot(data = dfpcakm5c, vars=['PC1', 'PC2'], hue='Cluster ID')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysis of Clusters."},{"metadata":{},"cell_type":"markdown","source":"Let's see how cluster 0 differs from the rest."},{"metadata":{"trusted":false},"cell_type":"code","source":"dfkm5c.groupby('Cluster ID').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dfkm5c[dfkm5c['Cluster ID']!=0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. It is to be noted that only Cluster ID 4 (Dum Dums) and 1 (Tootsie Roll Midgies) are far away from Cluster ID 0.<br>\n2. 'Dum Dums' and 'Tootsie Roll Midgies' are sort of opposite of each other. The first one is fruity and the second one chocolaty.<br>\n3. Cluster ID 0 contains competitors which are mostly chocolaty, sugary and more favourable. Cluster ID 1, although being chocolaty has a low sugar percentile.<br>\n4. All the chocolates which don't belong to Cluster ID 0 have made the top 10 list of `winbyprice`. They are all cheap."},{"metadata":{},"cell_type":"markdown","source":"Let's put clusters other than 0 into one cluster and then analyze again."},{"metadata":{"trusted":false},"cell_type":"code","source":"dfkm5c['Cluster ID'] = dfkm5c['Cluster ID'].map(lambda x: 1 if (x!=0) else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"dfkm5c.groupby('Cluster ID').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, Cluster ID 0 contains competitors which are more chocolaty and more pricey."},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"## Predicting the win percentage."},{"metadata":{},"cell_type":"markdown","source":"**Scaling.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"X = df.drop(['competitorname', 'winpercent', 'sugarpercent', 'pricepercent', 'sugarbyprice', 'winbyprice'], axis=1)\ny = df['winpercent']\n\nfrom sklearn.preprocessing import MinMaxScaler\nminmax_scaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Cross-Validation.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn import linear_model # import.\nlr_rdg = linear_model.Ridge(random_state=123) # instantiate.\n\n# Perform cross-validation.\nfrom sklearn.model_selection import GridSearchCV\nhyperparameters = {'alpha': [0.01, 0.1, 1, 10, 100, 1000]}\nmodel_cv = GridSearchCV(estimator = lr_rdg, param_grid = hyperparameters, cv=10, scoring= 'neg_mean_absolute_error')\n#lr_rdg.get_params().keys() # hyperparameters that we can set.\n\nmodel_cv.fit(X, y) # fit.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cv_results = pd.DataFrame(model_cv.cv_results_)\n# cv_results.head()\n\n# Plotting mean test and train scoes with alpha.\ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# Plotting.\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<hr>"},{"metadata":{},"cell_type":"markdown","source":"**Ridge Linear Regression.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"model_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"alpha = 1\nridge = linear_model.Ridge(alpha=alpha)\nridge.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Results.**"},{"metadata":{"trusted":false},"cell_type":"code","source":"ridge.intercept_ # constant term.","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for x,y in zip(X.columns, ridge.coef_): # coefficients of features.\n    print(x, y*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These coefficients sort of matches with the analysis done by FiveThirtyEight -> https://fivethirtyeight.com/features/the-ultimate-halloween-candy-power-ranking/"},{"metadata":{},"cell_type":"markdown","source":"<hr>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}