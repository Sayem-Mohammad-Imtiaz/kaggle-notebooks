{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers\n!pip install keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch \nimport torch.nn as nn \nimport torch.optim as optim \nimport torch.nn.functional as F \nfrom torch.utils.data import * \nfrom keras.preprocessing.sequence import pad_sequences \nfrom transformers import AutoModel,AutoTokenizer\nfrom keras.datasets import imdb \ntorch.__version__\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_WORDS = 10000  # imdb’s vocab_size 即词汇表大小\nMAX_LEN = 512      # max length\nBATCH_SIZE = 256\nEMB_SIZE = 128   # embedding size\nHID_SIZE = 128   # lstm hidden size\nDROPOUT = 0.2 \nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(DEVICE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def idx2word(idxs):\n    word_index = imdb.get_word_index()\n    idx_word = dict([(value,key) for (key,value) in word_index.items()])\n    sentences = []\n    for ids in idxs:\n        sentences.append(' '.join([idx_word.get(index-3,'') for index in ids]))\n    return sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process(sentences):\n    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n    input_ids = []\n    for sentence in sentences:\n        input_id = tokenizer.encode(sentence,add_special_tokens=True,max_length=MAX_LEN) #没有padding\n        input_ids.append(input_id)\n    return input_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def padding_mask(input_ids):\n    padding_ids = []\n    att_masks = []\n    for input_id in input_ids:\n        padding_ids.append(input_id + (MAX_LEN - len(input_id))*[0])\n        att_masks.append([1] * len(input_id) + [0] * (MAX_LEN - len(input_id)))\n    return padding_ids,att_masks","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(x_train,y_train),(x_val,y_val) = imdb.load_data(num_words=100000)\ntrain_sen = idx2word(x_train)\nval_sen = idx2word(x_val)\ntrain_ids_ = process(train_sen)\nval_ids_ = process(val_sen)\ntrain_ids,train_masks = padding_mask(train_ids_)\nval_ids,val_masks = padding_mask(val_ids_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 转化为TensorDataset\ntrain_data = TensorDataset(torch.LongTensor(train_ids),torch.LongTensor(train_masks) ,torch.LongTensor(y_train))\ntest_data = TensorDataset(torch.LongTensor(val_ids),torch.LongTensor(val_masks) ,torch.LongTensor(y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 转化为 DataLoader\ntrain_sampler = RandomSampler(train_data)\ntrain_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n\ntest_sampler = SequentialSampler(test_data)\ntest_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 定义lstm模型用于文本分类\nclass Model(nn.Module):\n    def __init__(self, hid_size, dropout):\n        super(Model, self).__init__()\n        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n        for p in self.bert.parameters():\n            p.requires_grad = False\n        self.hid_size = hid_size\n        self.dropout = dropout\n        self.emb_size = 768\n#         self.Embedding = nn.Embedding(self.max_words, self.emb_size)\n        self.LSTM = nn.LSTM(self.emb_size, self.hid_size, num_layers=2,\n                            batch_first=True, bidirectional=True)   # 2层双向LSTM\n        self.dp = nn.Dropout(self.dropout)\n        self.fc1 = nn.Linear(self.hid_size*2, self.hid_size)\n        self.fc2 = nn.Linear(self.hid_size, 2)\n    \n    def forward(self,ids,masks):\n        \"\"\"\n        input : [bs, maxlen]\n        output: [bs, 2] \n        \"\"\"\n#         x = self.Embedding(x)  # [bs, ml, emb_size]\n        with torch.no_grad():\n            x,_ = self.bert(ids,masks)\n#         print(len(x))\n#         x = self.dp(x)\n        x, _ = self.LSTM(x)  # [bs, ml, 2*hid_size]\n        x = self.dp(x)\n        x = F.relu(self.fc1(x))   # [bs, ml, hid_size]\n        x = F.avg_pool2d(x, (x.shape[1], 1)).squeeze()  # [bs, 1, hid_size] => [bs, hid_size]\n        out = self.fc2(x)    # [bs, 2]\n        return out  # [bs, 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, device, train_loader, optimizer, epoch):   # 训练模型\n    model.train()\n    criterion = nn.CrossEntropyLoss()\n    for batch_idx, (x, y,z) in enumerate(train_loader):\n        x, y ,z = x.to(DEVICE), y.to(DEVICE) , z.to(DEVICE)\n        optimizer.zero_grad()\n        y_ = model(x,y)\n        loss = criterion(y_, z)  # 得到loss\n        loss.backward()\n        optimizer.step()\n        if(batch_idx + 1) % 10 == 0:    # 打印loss\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(x), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model, device, test_loader):    # 测试模型\n    model.eval()\n    criterion = nn.CrossEntropyLoss(reduction='sum')  # 累加loss\n    test_loss = 0.0 \n    acc = 0 \n    for batch_idx, (x, y,z) in enumerate(test_loader):\n        x, y , z = x.to(DEVICE), y.to(DEVICE),z.to(DEVICE)\n        with torch.no_grad():\n            y_ = model(x,y)\n        test_loss += criterion(y_, z)\n        pred = y_.max(-1, keepdim=True)[1]   # .max() 2输出，分别为最大值和最大值的index\n        acc += pred.eq(z.view_as(pred)).sum().item()    # 记得加item()\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n        test_loss, acc, len(test_loader.dataset),\n        100. * acc / len(test_loader.dataset)))\n    return acc / len(test_loader.dataset) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Model( HID_SIZE, DROPOUT).to(DEVICE)\n# print(model)\noptimizer = optim.Adam(model.parameters())\n\nbest_acc = 0.0 \nPATH = '/kaggle/working/params.pkl'  # 定义模型保存路径\nif os.path.exists(PATH):\n    model.load_state_dict(torch.load(PATH))\nfor epoch in range(5):  # 10个epoch\n    train(model, DEVICE, train_loader, optimizer, epoch)\n    acc = test(model, DEVICE, test_loader)\n    if best_acc < acc: \n        best_acc = acc \n        torch.save(model.state_dict(), PATH)\n    print(\"acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc, best_acc)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 检验保存的模型\nbest_model = Model(HID_SIZE, DROPOUT).to(DEVICE)\nbest_model.load_state_dict(torch.load(PATH))\ntest(best_model, DEVICE, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only lstm : the acc is about 87%\n# bert lstm : about 90%","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}