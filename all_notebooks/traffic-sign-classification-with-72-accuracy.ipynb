{"cells":[{"metadata":{"_uuid":"bc742362fab9c78693908d4d45bb729257b657af"},"cell_type":"markdown","source":"# # Objective: Traffic Sign Classification\n\nImport data set from: https://btsd.ethz.ch/shareddata/\n\n\nDownlaod sample dataset on google drive: https://drive.google.com/drive/folders/1EVvSiFKJCt4RoXhnfPWirXuUjUfDBIO8?usp=sharing","execution_count":null},{"metadata":{"_uuid":"bd62123841ee36187fa386f476139ff8f4a757cb"},"cell_type":"markdown","source":"## Let's import the needed libraries.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport skimage.data\nimport skimage.transform\n\nimport tensorflow as tf\n\nimport random\nimport os\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adc761bafa1f2af5a674a3e955cf2d581a36be98"},"cell_type":"code","source":"print(os.listdir(\"../input/BelgiumTSC_Training/Training\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"717ff268dc7c08c497006de86a58a962d1e9454f"},"cell_type":"code","source":"print(os.listdir(\"../input/BelgiumTSC_Testing/Testing\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38cd9175201fcec03f689e3e2d80d037831892f9"},"cell_type":"markdown","source":"## Parse and Load the Training Data\n\nThe Training directory contains sub-directories with sequental numerical names from 00000 to 00061. The name of the directory represents the labels from 0 to 61, and the images in each directory represent the traffic signs that belong to that label.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"df12a565a094b71988c548e87aad6769c4105e5e"},"cell_type":"code","source":"def load_data(data_dir):\n    \"\"\"Loads a data set and returns two lists: \n    images: a list of Numpy arrays, each representing an image.\n    labels: a list of numbers that represent the images labels.\n    \"\"\"\n    # Get all subdirectories of data_dir. Each represents a label.\n    directories = [d for d in os.listdir(data_dir) \n                   if os.path.isdir(os.path.join(data_dir, d))]\n    # Loop through the label directories and collect the data in\n    # two lists, labels and images.\n    labels = []\n    images = []\n    for d in directories:\n        label_dir = os.path.join(data_dir, d)\n        file_names = [os.path.join(label_dir, f) \n                      for f in os.listdir(label_dir) if f.endswith(\".ppm\")]\n        # For each label, load it's images and add them to the images list.\n        # And add the label number (i.e. directory name) to the labels list.\n        for f in file_names:\n            images.append(skimage.data.imread(f))\n            labels.append(int(d))\n    return images, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4aed2a4f4660e3da9d1e555f2667c5c066165ef"},"cell_type":"code","source":"# Load training and testing datasets.\nROOT_PATH = \"../input/\"\ntrain_data_dir = os.path.join(ROOT_PATH, \"BelgiumTSC_Training/Training\")\ntest_data_dir = os.path.join(ROOT_PATH, \"BelgiumTSC_Testing/Testing\")\n\nimages, labels = load_data(train_data_dir)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b683dd99a3f0d61eb91d7f1fc76f9b777265e7b0"},"cell_type":"markdown","source":"\n\nHere we're loading two lists:\n\n* images a list of images, each image is represted by a numpy array.\n* labels a list of labels. Integers with values between 0 and 61.","execution_count":null},{"metadata":{"_uuid":"9cd7682d4bec1ba8434d76b02591cc0849cd4839"},"cell_type":"markdown","source":"## Explore the Dataset\n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"7d40f194efe928c09e9df89a83453741a2552cf3"},"cell_type":"code","source":"print(\"Unique Labels: {0}\\nTotal Images: {1}\".format(len(set(labels)), len(images)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b99131f6888b6f446b84e7a8580ea4855597c405"},"cell_type":"markdown","source":"\n\nDisplay the first image of each label.\n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"5444bcc685bee81a76f3c3e17c0e0d480bb8799f"},"cell_type":"code","source":"def display_images_and_labels(images, labels):\n    \"\"\"Display the first image of each label.\"\"\"\n    unique_labels = set(labels)\n    plt.figure(figsize=(15, 15))\n    i = 1\n    for label in unique_labels:\n        # Pick the first image for each label.\n        image = images[labels.index(label)]\n        plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n        plt.axis('off')\n        plt.title(\"Label {0} ({1})\".format(label, labels.count(label)))\n        i += 1\n        _ = plt.imshow(image)\n    plt.show()\n\ndisplay_images_and_labels(images, labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebbc760484b656f59c067891605017ea9adc0628"},"cell_type":"markdown","source":"\n\nThat looks great! The traffic signs occupy most of the area of each image, which is going to make our job easier: we don't have to look for the sign in the image. And we have a variety of angles and lighting conditions, which will help our model generalize.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"0cf83f317cc61a364cd733d7093fcecbc9bec3fe"},"cell_type":"code","source":"def display_label_images(images, label):\n    \"\"\"Display images of a specific label.\"\"\"\n    limit = 24  # show a max of 24 images\n    plt.figure(figsize=(15, 5))\n    i = 1\n\n    start = labels.index(label)\n    end = start + labels.count(label)\n    for image in images[start:end][:limit]:\n        plt.subplot(3, 8, i)  # 3 rows, 8 per row\n        plt.axis('off')\n        i += 1\n        plt.imshow(image)\n    plt.show()\n\ndisplay_label_images(images, 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba92cdee1576bd836109efd51a29a15ea304459b"},"cell_type":"code","source":"display_label_images(images, 43)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7463e54ba95a1957b70512e664c4fe9d5cb92883"},"cell_type":"code","source":"display_label_images(images, 27)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8fb0c40b69ec70cae0de47fcbcc181cdb6264cd"},"cell_type":"markdown","source":"Interesting! It looks like our dataset considers all speeding limit signs to be of the same class regardless of the numbers on them. That's fine, as long as we know about it beforehand and don't let it confuse us later when the output doesn't match our expectation.","execution_count":null},{"metadata":{"_uuid":"380b33e29e0cad073b22d94a731800e98a7934ab"},"cell_type":"markdown","source":"## Handling images of different sizes?\n\nMost neural networks expect a fixed-size input, and our network is no exception. But as we've seen above, our images are not all the same size. A common approach is to crop and pad the images to a selected apect ratio, but then we have to make sure that we don't cut-off parts of the traffic signs in the process. That seems like it might require manual work! Let's do a simpler solution instead : We'll just resize the images to a fixed size and ignore the distortions caused by the different aspect ratios. A person can easily recognize a traffic sign even if it's compressed or stretched a bit, so we hope that our model can as well.\n\n\n### What are the sizes of our image anyway?\n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"54fcdaae83594d77cd9820ecc836f77c864847c8"},"cell_type":"code","source":"for image in images[:5]:\n    print(\"shape: {0}, min: {1}, max: {2}\".format(image.shape, image.min(), image.max()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5a96b843675acf18d9fc5df4ae7a63c8fd04b45"},"cell_type":"markdown","source":"\n\nThe sizes seem to hover around 128x128. If we resize them to, say, 32x32, we'll have reduced the data and the model size by a factor of 16. And 32x32 is probably still big enough to recognize the signs, so let's go with that.","execution_count":null},{"metadata":{"trusted":true,"_uuid":"d0136caa1660e152bfa0163410360c9636e73e14"},"cell_type":"code","source":"# Resize images\nimages32 = [skimage.transform.resize(image, (32, 32), mode='constant')\n                for image in images]\ndisplay_images_and_labels(images32, labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4285f57b6eeec2fee7d40ea0b42b377330821a6"},"cell_type":"markdown","source":"The 32x32 images are not as sharp but still recognizable.  Let's print the sizes of a few images to verify that we got it right.\n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"5e8d95b3a2be762892c83771201c13fe4d8aba84"},"cell_type":"code","source":"for image in images32[:5]:\n    print(\"shape: {0}, min: {1}, max: {2}\".format(image.shape, image.min(), image.max()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9828b350691960d556d196cc25208e84920914a1"},"cell_type":"markdown","source":"\n\nThe sizes are correct. But check the min and max values! They now range from 0 to 1.0, which is different from the 0-255 range we saw above. The resizing function did that transformation for us. Normalizing values to the range 0.0-1.0 is very common so we'll keep it. But remember to multiply by 255 if you later want to convert the images back to the normal 0-255 range.\n","execution_count":null},{"metadata":{"_uuid":"eeb111425f3043e1037fa49b96a2f9ff98a00c9f"},"cell_type":"markdown","source":"# Simple Model","execution_count":null},{"metadata":{"trusted":true,"_uuid":"eec8cd6eb6a6d55716df43b285417172ea66b0df"},"cell_type":"code","source":"labels_a = np.array(labels)\nimages_a = np.array(images32)\nprint(\"labels: \", labels_a.shape, \"\\nimages: \", images_a.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"304174920638416e7dc99834e72d350b2d34de4f"},"cell_type":"code","source":"# Create a graph to hold the model.\ngraph = tf.Graph()\n\n# Create model in the graph.\nwith graph.as_default():\n    # Placeholders for inputs and labels.\n    images_ph = tf.placeholder(tf.float32, [None, 32, 32, 3])\n    labels_ph = tf.placeholder(tf.int32, [None])\n\n    # Flatten input from: [None, height, width, channels]\n    # To: [None, height * width * channels] == [None, 3072]\n    images_flat = tf.contrib.layers.flatten(images_ph)\n\n    # Fully connected layer. \n    # Generates logits of size [None, 62]\n    logits = tf.contrib.layers.fully_connected(images_flat, 62, tf.nn.relu)\n\n    # Convert logits to label indexes (int).\n    # Shape [None], which is a 1D vector of length == batch_size.\n    predicted_labels = tf.argmax(logits, 1)\n\n    # Define the loss function. \n    # Cross-entropy is a good choice for classification.\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels_ph))\n\n    # Create training op.\n    train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n\n    # And, finally, an initialization op to execute before training.\n    init = tf.global_variables_initializer()\n\nprint(\"images_flat: \", images_flat)\nprint(\"logits: \", logits)\nprint(\"loss: \", loss)\nprint(\"predicted_labels: \", predicted_labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b60dbec8eede34fe65fbfc279b1513e55e03c621"},"cell_type":"markdown","source":"# Training","execution_count":null},{"metadata":{"trusted":true,"_uuid":"847b8c41d966ae59684622da786b930322acdc45"},"cell_type":"code","source":"# Create a session to run the graph we created.\nsession = tf.Session(graph=graph)\n\n# First step is always to initialize all variables. \n# We don't care about the return value, though. It's None.\n_ = session.run([init])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"911bb20e134fcfba6acc8aa4024a0ab86d503f79"},"cell_type":"code","source":"for i in range(201):\n    _, loss_value = session.run([train, loss], \n                                feed_dict={images_ph: images_a, labels_ph: labels_a})\n    if i % 10 == 0:\n        print(\"Loss: \", loss_value)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85efa8a3f01635eca9e047571eebcb73ce0a505c"},"cell_type":"markdown","source":"## Using the Model\n\nThe session object contains the values of all the variables in our model (i.e. the weights).\n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"9ed9de50ef1845d555cd7259649975cdf08f51f3"},"cell_type":"code","source":"# Pick 10 random images\nsample_indexes = random.sample(range(len(images32)), 10)\nsample_images = [images32[i] for i in sample_indexes]\nsample_labels = [labels[i] for i in sample_indexes]\n\n# Run the \"predicted_labels\" op.\npredicted = session.run([predicted_labels], \n                        feed_dict={images_ph: sample_images})[0]\nprint(sample_labels)\nprint(predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"658e985200443b2ff05ea17fdd9c7d62e7083740"},"cell_type":"code","source":"# Display the predictions and the ground truth visually.\nfig = plt.figure(figsize=(10, 10))\nfor i in range(len(sample_images)):\n    truth = sample_labels[i]\n    prediction = predicted[i]\n    plt.subplot(5, 2,1+i)\n    plt.axis('off')\n    color='green' if truth == prediction else 'red'\n    plt.text(40, 10, \"Truth:        {0}\\nPrediction: {1}\".format(truth, prediction), \n             fontsize=12, color=color)\n    plt.imshow(sample_images[i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7a846a7729927570923544fa1f21ee9509b0b2f"},"cell_type":"markdown","source":"## Evaluation\n\nIt's fun to visualize the results, but we need a more precise way to measure the accuracy of our model. Also, it's important to test it on images that it hasn't seen. And that's where the validation data set comes into play.\n","execution_count":null},{"metadata":{"trusted":true,"_uuid":"1fb23aac41ba572dcf17d5879def4789484877fe"},"cell_type":"code","source":"# Load the test dataset.\ntest_images, test_labels = load_data(test_data_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ff585efdf0666b4b474810c3788811b523ce844"},"cell_type":"code","source":"# Transform the images, just like we did with the training set.\ntest_images32 = [skimage.transform.resize(image, (32, 32), mode='constant')\n                 for image in test_images]\ndisplay_images_and_labels(test_images32, test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f508a13810f21a24a95c7113dbd4171ca5e4ffe"},"cell_type":"code","source":"# Run predictions against the full test set.\npredicted = session.run([predicted_labels], \n                        feed_dict={images_ph: test_images32})[0]\n# Calculate how many matches we got.\nmatch_count = sum([int(y == y_) for y, y_ in zip(test_labels, predicted)])\naccuracy = match_count / len(test_labels)\nprint(\"Accuracy: {:.3f}\".format(accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f39214236997ff01fb2f9569e40f2e59a0bb9eb"},"cell_type":"code","source":"# Close the session. This will destroy the trained model.\nsession.close()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}