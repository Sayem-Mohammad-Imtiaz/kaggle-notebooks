{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spark = SparkSession.Builder().getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = spark.read.csv('/kaggle/input/ccdata/CC GENERAL.csv',header = True,inferSchema=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.limit(5).toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.na.drop(how='any')\ntrain.limit(5).toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.withColumn(\"label\", train.PURCHASES_FREQUENCY>=0.5)\ntrain = train.withColumn(\"label\", train[\"label\"].cast(\"string\"))\n\nfrom pyspark.ml.feature import StringIndexer\nindexer = StringIndexer(inputCol=\"label\", outputCol=\"target\")\ntraining = indexer.fit(train).transform(train)\n\ntraining.limit(5).toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lest's create our classification model"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [col for col in training.columns if col not in ['target','CUST_ID','label','PURCHASES_FREQUENCY']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler()\\\n.setInputCols(columns)\\\n.setOutputCol(\"features\")\ntrain_calss = assembler.transform(training)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_calss.select(\"features\",\"target\").show(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random forest classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\nrf = RandomForestClassifier(featuresCol = 'features',labelCol = \"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"paramGrid = ParamGridBuilder()\\\n   .addGrid(rf.numTrees, [100, 200, 300])\\\n   .addGrid(rf.maxDepth, [1, 2, 3, 4, 5, 6, 7, 8])\\\n   .addGrid(rf.maxBins, [25, 28, 31])\\\n   .addGrid(rf.impurity, [\"entropy\", \"gini\"])\\\n   .build()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator(labelCol = \"target\", rawPredictionCol = \"prediction\") \n\ncrossval = CrossValidator(estimator = rf,\n                          estimatorParamMaps = paramGrid,\n                          evaluator = evaluator,\n                          numFolds = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_rf, test_rf = train_calss.randomSplit([0.8, 0.2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cvModel = crossval.fit(train_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = cvModel.transform(test_rf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.select(\"features\",\"prediction\",\"target\").limit(5).toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator(labelCol = \"target\", rawPredictionCol = \"prediction\") \nevaluator.evaluate(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(featuresCol=\"features\",labelCol=\"target\",maxIter=10, regParam=0.3, elasticNetParam=0.8)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_lr=train_calss.select(\"features\",\"target\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training, testing = train_lr.randomSplit([0.8, 0.2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lr.fit(training)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.transform(testing)\npredictions.select(\"prediction\", \"target\", \"features\").show(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator(labelCol = \"target\", rawPredictionCol = \"prediction\")\nevaluator.evaluate(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering"},{"metadata":{},"cell_type":"markdown","source":"# K-Means"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\ncolumns = [col for col in training.columns if col not in ['target','CUST_ID','label']]\nfrom pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler()\\\n.setInputCols(columns)\\\n.setOutputCol(\"features_clustering\")\n\ntrain_clustering = assembler.transform(training)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\ncost = np.zeros(20)\nfor k in range(2,20):\n    kmeans = KMeans()\\\n            .setK(k)\\\n            .setSeed(1) \\\n            .setFeaturesCol(\"features_clustering\")\\\n            .setPredictionCol(\"cluster\")\n\n    model_k = kmeans.fit(train_clustering)\n    cost[k] = model_k.computeCost(train_clustering)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# how many K do I need"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.mlab as mlab\nimport matplotlib.pyplot as plt\nimport seaborn as sbs\nfrom matplotlib.ticker import MaxNLocator\n\nfig, ax = plt.subplots(1,1, figsize =(8,6))\nax.plot(range(2,20),cost[2:20])\nax.set_xlabel('k')\nax.set_ylabel('cost')\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans().setK(2).setSeed(1).setFeaturesCol(\"features_clustering\")\nmodel = kmeans.fit(train_clustering)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions\npredictions = model.transform(train_clustering)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate clustering by computing Silhouette score\nevaluator = ClusteringEvaluator()\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show up the centers.\ncenters = model.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.select(\"features\",\"prediction\").limit(5).toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BisectingKMeans"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.clustering import BisectingKMeans\nbkm = BisectingKMeans().setK(2).setSeed(1)\nmodel2= bkm.fit(train_clustering)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions2 = model2.transform(train_clustering)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator2= ClusteringEvaluator()\nsilhouette2 = evaluator.evaluate(predictions2)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}