{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<H1> GET RID OF PASS IN ANALASYS FUNCTIONS</H1>\n\n<b>Missions left:\nNormalization, PCA dim reductionResampaling, Clustering, modeling, model-assesments\n</b>\n\nEyal:\n1. We treated feature 10 on sum parts as categorical and on others as numeric, we need to check again what we say and do about this feature in each part of the notebook\n\nAlon:\n1. what to do with junk @: def fill_categorical_nulls?\n\nDaniel:\n1. Convert_to_numerical feature\n\nDaniel & Eyal:\n1. Take care of sampling (check what is the threshold)\n2. Feature Selection\n3. \n\nNotes:\n\n2. Feature 0 Discussion is required after all the plots\n\n5. test categorical remove outliers, use most_freq from train or test?","metadata":{"papermill":{"duration":0.040149,"end_time":"2021-05-14T12:39:35.200343","exception":false,"start_time":"2021-05-14T12:39:35.160194","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","metadata":{"papermill":{"duration":0.052362,"end_time":"2021-05-14T12:39:35.290362","exception":false,"start_time":"2021-05-14T12:39:35.238","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:04.807879Z","iopub.execute_input":"2021-06-12T21:05:04.808228Z","iopub.status.idle":"2021-06-12T21:05:04.812797Z","shell.execute_reply.started":"2021-06-12T21:05:04.808199Z","shell.execute_reply":"2021-06-12T21:05:04.811344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>","metadata":{"papermill":{"duration":0.037066,"end_time":"2021-05-14T12:39:35.367948","exception":false,"start_time":"2021-05-14T12:39:35.330882","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<center>\n    <h2> Introduction to Machine Learning course </h2>\n    <h1>Course Final Project</h1>\n    <h6>By Group 21: Eyal Apelboim (302820832), Alon Regensteiner (205419716), Daniel Levkovitz (203321740)</h6>\n</center>","metadata":{"papermill":{"duration":0.037039,"end_time":"2021-05-14T12:39:35.442372","exception":false,"start_time":"2021-05-14T12:39:35.405333","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>","metadata":{"papermill":{"duration":0.038671,"end_time":"2021-05-14T12:39:35.519389","exception":false,"start_time":"2021-05-14T12:39:35.480718","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h2>Imports</h2>","metadata":{"papermill":{"duration":0.038223,"end_time":"2021-05-14T12:39:35.594784","exception":false,"start_time":"2021-05-14T12:39:35.556561","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.options.display.max_columns = None  # visual dataframe prints configuration\n\nfrom statsmodels.graphics.gofplots import qqplot\n\nimport seaborn as sns  # for graph output\nfrom scipy import stats  # for z-score\nfrom scipy import interp  \n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.dpi'] = 120  # setting default dpi value to all plots\nimport matplotlib.gridspec as gridspec\nfrom mpl_toolkits.mplot3d import Axes3D \nplt.style.use('ggplot')\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.metrics import silhouette_score, pairwise_distances, roc_curve\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n# Display more rows in df head\npd.options.display.max_columns = None","metadata":{"papermill":{"duration":1.89351,"end_time":"2021-05-14T12:39:37.525218","exception":false,"start_time":"2021-05-14T12:39:35.631708","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:04.817301Z","iopub.execute_input":"2021-06-12T21:05:04.817779Z","iopub.status.idle":"2021-06-12T21:05:05.384301Z","shell.execute_reply.started":"2021-06-12T21:05:04.817731Z","shell.execute_reply":"2021-06-12T21:05:05.383163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>","metadata":{"papermill":{"duration":0.039334,"end_time":"2021-05-14T12:39:37.60202","exception":false,"start_time":"2021-05-14T12:39:37.562686","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h1>0. Analysis Functions</h1>\nThis section contains various function we've build to analyze the data later on in the next sections.","metadata":{"papermill":{"duration":0.038077,"end_time":"2021-05-14T12:39:37.680067","exception":false,"start_time":"2021-05-14T12:39:37.64199","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def analyze_label(label):\n    return\n    \"\"\"\n    performing analysis for the data label\n    \"\"\"\n    des = label.describe()\n    rows = des['count']\n    mean_val = des['mean']\n    print(f'Out of {int(rows)} samples, {int(mean_val * rows)} ({str(mean_val * 100)[:5]}%) are labeled 1.')\n    print(f'And the rest ({str((1 - mean_val) * 100)[:5]}%) are labled 0.')\n    \n    # Creating pie chart for label values\n    names = [\"Label=1\", \"Label=0\"]\n    values = [mean_val * rows, (1 - mean_val) * rows]\n    null_pr = pd.DataFrame({\"names\": names, \"values\": values}).groupby(\"names\")[\"values\"].sum()\n    pie, ax = plt.subplots(figsize=[3,3])\n    plt.pie(x=null_pr, autopct=\"%.2f%%\", explode=[0.05]*2, labels=null_pr.keys(), pctdistance=0.5, colors= [\"lightgrey\", \"dodgerblue\"])\n    plt.title(f\"Labels\")\n    plt.show()\n    plt.clf()\n    return mean_val","metadata":{"papermill":{"duration":0.053047,"end_time":"2021-05-14T12:39:37.771321","exception":false,"start_time":"2021-05-14T12:39:37.718274","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.385887Z","iopub.execute_input":"2021-06-12T21:05:05.386195Z","iopub.status.idle":"2021-06-12T21:05:05.395194Z","shell.execute_reply.started":"2021-06-12T21:05:05.386166Z","shell.execute_reply":"2021-06-12T21:05:05.393809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def analyze_numerical_feature(dataframe, feature, bins=50):\n    return\n    \"\"\"\n    general function which performs full analysis for numerical freature\n    \"\"\"\n    df = dataframe.copy()\n    f = df[feature].copy()\n    # getting general information\n    print('Feature ' + feature + ' analysis:')\n    print('-----------------------')\n    print(f.describe())\n    print('-----------------------')\n    print(\"\\nNulls count:\", f.isna().sum())\n    print('\\nNull cells precentage: ' + str(f.isna().sum() * 100 / df.shape[0])[:6], '%')\n    print('-----------------------')\n    # Creating pie chart for Nulls & Non-Nulls\n    names = [\"Nulls\", \"Non-Nulls\"]\n    values = [f.isna().sum(), df.shape[0] - f.isna().sum()]\n    null_pr = pd.DataFrame({\"names\": names, \"values\": values}).groupby(\"names\")[\"values\"].sum()\n    pie, ax = plt.subplots(figsize=[3,3])\n    plt.pie(x=null_pr, autopct=\"%.2f%%\", explode=[0.05]*2, labels=null_pr.keys(), pctdistance=0.5, colors= [\"lightgrey\", \"crimson\"])\n    plt.title(f\"Feature {feature} Nulls Precentage\");\n    plt.show()\n    plt.clf()\n    \n    # creating histogram plot\n    plt.figure(figsize=(8, 24 * 4))\n    gs = gridspec.GridSpec(28, 1)\n    ax = plt.subplot(gs[1])\n    sns.distplot(f, bins=bins, label='All')\n    ax.set_xlabel('Value')\n    ax.set_title(f'Feature {feature} Distribution Histogram')\n    ax.legend()\n    # ax.set_ylim(0, 1.0)\n    plt.show()  # plotting graph\n    plt.clf()  # releasing plt\n    \n    # creating histogram plot with separation for label 0 and label 1\n    plt.figure(figsize=(8, 24 * 4))\n    gs = gridspec.GridSpec(28, 1)\n    ax = plt.subplot(gs[1])\n    sns.distplot(f[df.label == 1], bins=bins, label='label=1', color='darkorange')\n    sns.distplot(f[df.label == 0], bins=bins, label='label=0', color='dodgerblue')\n    ax.set_title(f'Feature {feature} Label Distribution Histogram')\n    ax.set_xlabel('Value')\n    ax.legend()\n    # ax.set_ylim(0, 1.0)\n    plt.show()  # plotting graph\n    plt.clf()  # releasing plt\n    \n    # creating box plot  \n    fig = plt.figure(figsize=(8, 1))\n    ax = fig.add_subplot(1, 1, 1)\n    ax.boxplot(list(f.dropna()), vert=False)\n    ax.set_title(f'Feature {feature} Boxplot graph')\n    ax.set_xlabel('Value')\n    ax.set_ylabel(' ')\n    plt.show()  # plotting graph\n    plt.clf()  # releasing plt\n    \n    # creating plot - probability of label 1\n    edges = f.describe(percentiles = [.02, .98])  # getting feature's edge values\n    bins = np.linspace(edges['2%'], edges['98%'], (15))\n    bins = list(np.around(np.array(bins), 1))\n    labels = bins[:-1]\n    rpt = (df.groupby(pd.cut(f, bins, labels=labels)).label.mean()).to_frame('label')\n    rpt.plot.bar(figsize=(8, 3), ylim=(0,1), title=f'Feature {feature} Probability of label=1 for each value', xlabel='Value', ylabel='Probability')\n    # adding mean label value from all features as a benchmark:\n    plt.axhline(y=label_bm, label =r\"$\\mu_{all} = $\" + str(label_bm)[:5], color='dodgerblue', linewidth=2.0)\n    plt.legend(loc ='upper right')  # showing legend\n    plt.show()\n    plt.clf()\n    \n    # creating QQ plot to check normal distribution\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    ax.set_title(f'Feature {feature} QQplot')\n    qqplot(f.dropna(), line='s', ax=ax)  # plotting graph\n","metadata":{"papermill":{"duration":0.064257,"end_time":"2021-05-14T12:39:37.878669","exception":false,"start_time":"2021-05-14T12:39:37.814412","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.397115Z","iopub.execute_input":"2021-06-12T21:05:05.397415Z","iopub.status.idle":"2021-06-12T21:05:05.422447Z","shell.execute_reply.started":"2021-06-12T21:05:05.397388Z","shell.execute_reply":"2021-06-12T21:05:05.42165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def analyze_categorical_feature(dataframe, feature):\n    return\n    \"\"\"\n    general function which performs full analysis for categorical freature\n    \"\"\"\n    df = dataframe.copy()\n    f = df[feature].copy()\n    # getting general information\n    print('Feature ' + feature + ' analysis:')\n    print('-----------------------')\n    print(f.describe())\n    print('-----------------------')\n    print(\"\\nNulls count:\", f.isna().sum())\n    print('\\nNull cells precentage: ' + str(f.isna().sum() * 100 / df.shape[0])[:6], '%')\n    \n    # Creating pie chart for Nulls & Non-Nulls\n    names = [\"Nulls\", \"Non-Nulls\"]\n    values = [f.isna().sum(), df.shape[0] - f.isna().sum()]\n    null_pr = pd.DataFrame({\"names\": names, \"values\": values}).groupby(\"names\")[\"values\"].sum()\n    pie, ax = plt.subplots(figsize=[3,3])\n    plt.pie(x=null_pr, autopct=\"%.2f%%\", explode=[0.05]*2, labels=null_pr.keys(), pctdistance=0.5, colors= [\"lightgrey\", \"crimson\"])\n    plt.title(f\"Feature {feature} Nulls Precentage\");\n    plt.show()\n    plt.clf()\n    \n    # Plotting categorial feature:\n    count = f.count()\n    print('\\nCategories Count:')\n    print(f.value_counts())\n    vc = f.value_counts()\n    counts = vc.values.tolist()\n    names = vc.index.tolist()\n    plt.figure(figsize=(8, 3))\n    plt.title(f'Feature {feature} Distribution')\n    splot = sns.barplot(x=names, y=counts, data=vc.reset_index())\n    for p in splot.patches:\n        splot.annotate(format(p.get_height() * 100 / count, '.2f') + '%',\n                       (p.get_x() + p.get_width() / 2., p.get_height()),\n                       ha = 'center', va = 'center', \n                       xytext = (0, 9),\n                       textcoords = 'offset points')\n    plt.xlabel('Category')\n    plt.ylabel('Count')\n    plt.show()\n    plt.clf()\n    \n    # creating plot - probability of label 1\n    rpt = (df.groupby(f).label.mean()).dropna()\n    rpt.plot.bar(figsize=(8, 3), ylim=(0,1), title=f'Feature {feature} Probability for label=1 per category', xlabel='Category', ylabel='Probability')\n    # adding mean label value from all features as a benchmark:\n    plt.axhline(y=label_bm, label =r\"$\\mu_{all} = $\" + str(label_bm)[:5], color='dodgerblue', linewidth=2.0)\n    plt.legend(loc ='upper right')  # showing legend\n    plt.show()\n    plt.clf()","metadata":{"papermill":{"duration":0.060599,"end_time":"2021-05-14T12:39:37.979141","exception":false,"start_time":"2021-05-14T12:39:37.918542","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.423921Z","iopub.execute_input":"2021-06-12T21:05:05.424457Z","iopub.status.idle":"2021-06-12T21:05:05.446166Z","shell.execute_reply.started":"2021-06-12T21:05:05.424408Z","shell.execute_reply":"2021-06-12T21:05:05.44533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def correlation_heatmap(df):\n    return\n    \"\"\"\n    display correlation matrix graph between different columns (features and label) in df,\n    using only numeric type columns\n    \"\"\"\n    # Plot\n    plt.figure(figsize=(10,8), dpi=300)\n    sns.heatmap(df.corr(), xticklabels=df.corr().columns, yticklabels=df.corr().columns,\n                cmap='RdYlGn', center=0, annot=True, annot_kws={\"fontsize\":7}, fmt='.2f')\n\n    # Decorations\n    plt.title('Correlogram of mtcars', fontsize=12)\n    plt.xticks(fontsize=8)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:05.447435Z","iopub.execute_input":"2021-06-12T21:05:05.448063Z","iopub.status.idle":"2021-06-12T21:05:05.466779Z","shell.execute_reply.started":"2021-06-12T21:05:05.448005Z","shell.execute_reply":"2021-06-12T21:05:05.465747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>","metadata":{"papermill":{"duration":0.038509,"end_time":"2021-05-14T12:39:38.056011","exception":false,"start_time":"2021-05-14T12:39:38.017502","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h1>1. Data Exploration</h1>\nUsing our pre-built analysis functions (Section <i>0. Analysis Functions</i>) to perform exploration of the raw data's scale, types & more.","metadata":{"papermill":{"duration":0.037522,"end_time":"2021-05-14T12:39:38.131709","exception":false,"start_time":"2021-05-14T12:39:38.094187","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"First, we'll have a quick look at the raw data input:","metadata":{"papermill":{"duration":0.037382,"end_time":"2021-05-14T12:39:38.207266","exception":false,"start_time":"2021-05-14T12:39:38.169884","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# importing raw data to data frame:\n\n#df = pd.read_csv('train.csv')  # path to raw train data file\n#df_origin_test = pd.read_csv('test_without_target.csv')  # path to raw train data file\ndf = pd.read_csv('../input/mldata/train.csv')  # path to raw train data file\ndf_origin_test = pd.read_csv('../input/mldata/test_without_target.csv')  # path to raw train data file\n\ndf.head()  # showing data head:","metadata":{"papermill":{"duration":0.26644,"end_time":"2021-05-14T12:39:38.511438","exception":false,"start_time":"2021-05-14T12:39:38.244998","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.4683Z","iopub.execute_input":"2021-06-12T21:05:05.468778Z","iopub.status.idle":"2021-06-12T21:05:05.725777Z","shell.execute_reply.started":"2021-06-12T21:05:05.468732Z","shell.execute_reply":"2021-06-12T21:05:05.724822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Initial inspection of the data shows that we have 20 features of various types.","metadata":{"papermill":{"duration":0.038444,"end_time":"2021-05-14T12:39:38.588888","exception":false,"start_time":"2021-05-14T12:39:38.550444","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3>Features Overview</h3>","metadata":{"papermill":{"duration":0.038198,"end_time":"2021-05-14T12:39:38.665731","exception":false,"start_time":"2021-05-14T12:39:38.627533","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# reviewing features types:\nd_types = df.dtypes\nprint('Columns Types:')\nprint(d_types)\nprint('-----------------\\n')\nprint('Total:')\nprint(d_types[:21].value_counts())","metadata":{"papermill":{"duration":0.057177,"end_time":"2021-05-14T12:39:38.762039","exception":false,"start_time":"2021-05-14T12:39:38.704862","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.727123Z","iopub.execute_input":"2021-06-12T21:05:05.727397Z","iopub.status.idle":"2021-06-12T21:05:05.743316Z","shell.execute_reply.started":"2021-06-12T21:05:05.727364Z","shell.execute_reply":"2021-06-12T21:05:05.74205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that some of the features are categorical, mostly are numeric and we might have some \"mixed types\" that may require conversion to a better suited type later on.\nWe will conduct a full analysis to each feature.","metadata":{"papermill":{"duration":0.04148,"end_time":"2021-05-14T12:39:38.843056","exception":false,"start_time":"2021-05-14T12:39:38.801576","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3>Label</h3>\nWe'll take a look at the label's 1/0 distribution:","metadata":{"papermill":{"duration":0.039857,"end_time":"2021-05-14T12:39:38.922452","exception":false,"start_time":"2021-05-14T12:39:38.882595","status":"completed"},"tags":[]}},{"cell_type":"code","source":"label_bm = analyze_label(df['label'])  # performing analysis for the data label","metadata":{"papermill":{"duration":0.226221,"end_time":"2021-05-14T12:39:39.188448","exception":false,"start_time":"2021-05-14T12:39:38.962227","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.746721Z","iopub.execute_input":"2021-06-12T21:05:05.747209Z","iopub.status.idle":"2021-06-12T21:05:05.762174Z","shell.execute_reply.started":"2021-06-12T21:05:05.747158Z","shell.execute_reply":"2021-06-12T21:05:05.761081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Currently, before we performed any processing, it seems that we have a smaller amount of '1' labels in our input data, compared to the ideal ratio of 50:50.\nIt seems that there's a non-neglectible difference between labels, that perhaps we should attend later on.\n","metadata":{"papermill":{"duration":0.041608,"end_time":"2021-05-14T12:39:39.273247","exception":false,"start_time":"2021-05-14T12:39:39.231639","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>","metadata":{"papermill":{"duration":0.041941,"end_time":"2021-05-14T12:39:39.35598","exception":false,"start_time":"2021-05-14T12:39:39.314039","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3>Feature 0 Analysis</h3>\nAccording to the initial overview - the first feature is float type.\nWe'll use our pre-built numeric analysis function:","metadata":{"papermill":{"duration":0.040566,"end_time":"2021-05-14T12:39:39.437591","exception":false,"start_time":"2021-05-14T12:39:39.397025","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '0')  # using pre-built analysis function","metadata":{"papermill":{"duration":2.704272,"end_time":"2021-05-14T12:39:42.183035","exception":false,"start_time":"2021-05-14T12:39:39.478763","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.764345Z","iopub.execute_input":"2021-06-12T21:05:05.764674Z","iopub.status.idle":"2021-06-12T21:05:05.776382Z","shell.execute_reply.started":"2021-06-12T21:05:05.764644Z","shell.execute_reply":"2021-06-12T21:05:05.775556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 0 Discussion:</h4>\n\nFeature 0 presents many outliers, as viewed on most graphs above. The boxplot clearly shows it.\nQQPlot displays a line in which the edges are not normal distributed, but the middle might present a normal distribution.\n","metadata":{"papermill":{"duration":0.055355,"end_time":"2021-05-14T12:39:42.293562","exception":false,"start_time":"2021-05-14T12:39:42.238207","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>","metadata":{"papermill":{"duration":0.052745,"end_time":"2021-05-14T12:39:42.401914","exception":false,"start_time":"2021-05-14T12:39:42.349169","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3>Feature 1 Analysis</h3>\nAccording to the initial overview - the second feature is object (text) type.\nWe'll use our pre-built categorical analysis function:","metadata":{"papermill":{"duration":0.051221,"end_time":"2021-05-14T12:39:42.504197","exception":false,"start_time":"2021-05-14T12:39:42.452976","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_categorical_feature(df, '1')  # using pre-built analysis function","metadata":{"papermill":{"duration":0.672355,"end_time":"2021-05-14T12:39:43.233053","exception":false,"start_time":"2021-05-14T12:39:42.560698","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.777714Z","iopub.execute_input":"2021-06-12T21:05:05.778166Z","iopub.status.idle":"2021-06-12T21:05:05.790033Z","shell.execute_reply.started":"2021-06-12T21:05:05.778123Z","shell.execute_reply":"2021-06-12T21:05:05.788905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 1 Discussion:</h4>\n\nFeature 1 has a very small amount of nulls. We see that value 'a' holds 2/3 of the rows. \nIn this feature, we can see that the category 'unknown' takes a large portion of the data and also have some tendency for label value 0. We'll leave it as-is due to the lack of information of it's meaning.\n(for example, 'Unknown' caller id might be a strong classifier for a 'scam' call).","metadata":{"papermill":{"duration":0.05831,"end_time":"2021-05-14T12:39:43.348381","exception":false,"start_time":"2021-05-14T12:39:43.290071","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 2 Analysis</h3>\nWe first start with a categorical analysis, as the feature type is 'object'.","metadata":{"papermill":{"duration":0.056352,"end_time":"2021-05-14T12:39:43.462109","exception":false,"start_time":"2021-05-14T12:39:43.405757","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_categorical_feature(df, '2')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:05.79151Z","iopub.execute_input":"2021-06-12T21:05:05.791908Z","iopub.status.idle":"2021-06-12T21:05:05.804657Z","shell.execute_reply.started":"2021-06-12T21:05:05.791868Z","shell.execute_reply":"2021-06-12T21:05:05.803848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems this feature requires some editing in order to be able perform any analysis on it, as it's not truly categorical. It seems that all values have a redundant 'd' char next to them.\nWe'll create a copy of the dataframe to check if modifing the feature helps:","metadata":{}},{"cell_type":"code","source":"df_test_f2 = df.copy()\ndf_test_f2['2'] = df_test_f2['2'].dropna().apply(lambda x: float(str(x)[:-1]))\ndf_test_f2.head()","metadata":{"papermill":{"duration":0.119767,"end_time":"2021-05-14T12:39:43.638997","exception":false,"start_time":"2021-05-14T12:39:43.51923","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.80568Z","iopub.execute_input":"2021-06-12T21:05:05.806071Z","iopub.status.idle":"2021-06-12T21:05:05.872757Z","shell.execute_reply.started":"2021-06-12T21:05:05.806043Z","shell.execute_reply":"2021-06-12T21:05:05.871692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyze_numerical_feature(df_test_f2, '2')  # using pre-built analysis function","metadata":{"papermill":{"duration":2.589465,"end_time":"2021-05-14T12:39:46.285568","exception":false,"start_time":"2021-05-14T12:39:43.696103","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.873986Z","iopub.execute_input":"2021-06-12T21:05:05.874265Z","iopub.status.idle":"2021-06-12T21:05:05.880331Z","shell.execute_reply.started":"2021-06-12T21:05:05.874238Z","shell.execute_reply":"2021-06-12T21:05:05.879268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 2 Discussion:</h4>\nThe boxplot reavels that nearly every value besides '-1' is an outlier.\nLooking at the graphs, we can see that this feature does not supply us any valuable information.\nThe QQPlot displays a straight line, which means it's abnormal.","metadata":{"papermill":{"duration":0.07192,"end_time":"2021-05-14T12:39:46.431161","exception":false,"start_time":"2021-05-14T12:39:46.359241","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 3 Analysis</h3>\nThis features seems to have numeric values. We'll use the proper tool to analyze it:","metadata":{"papermill":{"duration":0.075173,"end_time":"2021-05-14T12:39:46.580355","exception":false,"start_time":"2021-05-14T12:39:46.505182","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '3')  # using pre-built analysis function","metadata":{"papermill":{"duration":2.378717,"end_time":"2021-05-14T12:39:49.032114","exception":false,"start_time":"2021-05-14T12:39:46.653397","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.881639Z","iopub.execute_input":"2021-06-12T21:05:05.881944Z","iopub.status.idle":"2021-06-12T21:05:05.892172Z","shell.execute_reply.started":"2021-06-12T21:05:05.881887Z","shell.execute_reply":"2021-06-12T21:05:05.891096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 3 Discussion:</h4>\n\nFeature 3 has a very small amount of nulls.\nThis feature has a normal distribution behaviour according to the QQPlot, and we can see there's no difference between the lable's values, according to the label distribution. \n","metadata":{"papermill":{"duration":0.080212,"end_time":"2021-05-14T12:39:49.193503","exception":false,"start_time":"2021-05-14T12:39:49.113291","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 4 Analysis</h3>\nSeems like another numerical feature. Performing initial analysis accordingly:","metadata":{"papermill":{"duration":0.080961,"end_time":"2021-05-14T12:39:49.357472","exception":false,"start_time":"2021-05-14T12:39:49.276511","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '4')  # using pre-built analysis function","metadata":{"papermill":{"duration":2.525488,"end_time":"2021-05-14T12:39:51.963599","exception":false,"start_time":"2021-05-14T12:39:49.438111","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.893454Z","iopub.execute_input":"2021-06-12T21:05:05.893904Z","iopub.status.idle":"2021-06-12T21:05:05.905744Z","shell.execute_reply.started":"2021-06-12T21:05:05.893839Z","shell.execute_reply":"2021-06-12T21:05:05.904986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 4 Discussion:</h4>\n\nThis feature has a negligible amount of nulls.\nThis feature has high denisity around the 470 value, and a large portion of outliers as seen in the boxplot graph.","metadata":{"papermill":{"duration":0.09648,"end_time":"2021-05-14T12:39:52.162255","exception":false,"start_time":"2021-05-14T12:39:52.065775","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 5 Analysis</h3>\nNumerical as well:","metadata":{"papermill":{"duration":0.094027,"end_time":"2021-05-14T12:39:52.349793","exception":false,"start_time":"2021-05-14T12:39:52.255766","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '5')  # using pre-built analysis function","metadata":{"papermill":{"duration":2.358461,"end_time":"2021-05-14T12:39:54.80224","exception":false,"start_time":"2021-05-14T12:39:52.443779","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.906707Z","iopub.execute_input":"2021-06-12T21:05:05.907094Z","iopub.status.idle":"2021-06-12T21:05:05.9193Z","shell.execute_reply.started":"2021-06-12T21:05:05.907067Z","shell.execute_reply":"2021-06-12T21:05:05.918271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 5 Discussion:</h4>\n\nThis feature has a negilible amount of nulls.\nThis feature's distribution behaves as a normal distribution, according to the qqplot, boxplot, and the distribution. In addition, we can see that the edges have a higher probability to recieve the label '1'.\n","metadata":{"papermill":{"duration":0.108494,"end_time":"2021-05-14T12:39:55.014633","exception":false,"start_time":"2021-05-14T12:39:54.906139","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 6 Analysis</h3>\nSeems to be categorical. Using the proper pre-made function we've built to perform analysis:","metadata":{"papermill":{"duration":0.110436,"end_time":"2021-05-14T12:39:55.240341","exception":false,"start_time":"2021-05-14T12:39:55.129905","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_categorical_feature(df, '6')  # using pre-built analysis function","metadata":{"papermill":{"duration":0.863525,"end_time":"2021-05-14T12:39:56.213757","exception":false,"start_time":"2021-05-14T12:39:55.350232","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.921175Z","iopub.execute_input":"2021-06-12T21:05:05.921655Z","iopub.status.idle":"2021-06-12T21:05:05.934707Z","shell.execute_reply.started":"2021-06-12T21:05:05.921608Z","shell.execute_reply":"2021-06-12T21:05:05.933569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 6 Discussion:</h4>\nThe feature has a negilible amount of nulls.\nThe values A D and B are most frequent, and together hold more than 50% of the rows.\nWe can see that different categories have different probabilites to recieve 1/0 labels.","metadata":{"papermill":{"duration":0.112686,"end_time":"2021-05-14T12:39:56.436403","exception":false,"start_time":"2021-05-14T12:39:56.323717","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 7 Analysis</h3>","metadata":{"papermill":{"duration":0.109399,"end_time":"2021-05-14T12:39:56.657187","exception":false,"start_time":"2021-05-14T12:39:56.547788","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '7')  # using pre-built analysis function","metadata":{"papermill":{"duration":2.604304,"end_time":"2021-05-14T12:39:59.37113","exception":false,"start_time":"2021-05-14T12:39:56.766826","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.936127Z","iopub.execute_input":"2021-06-12T21:05:05.936463Z","iopub.status.idle":"2021-06-12T21:05:05.948745Z","shell.execute_reply.started":"2021-06-12T21:05:05.936416Z","shell.execute_reply":"2021-06-12T21:05:05.947839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 7 Discussion:</h4>\n\nThe feature has a negilible amount of nulls.\nThis feature seems to contribe of multiple normal distributions.\nWe don't see a strong correlation to specific labels.","metadata":{"papermill":{"duration":0.147258,"end_time":"2021-05-14T12:39:59.644545","exception":false,"start_time":"2021-05-14T12:39:59.497287","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 8 Analysis</h3>","metadata":{"papermill":{"duration":0.150503,"end_time":"2021-05-14T12:39:59.931542","exception":false,"start_time":"2021-05-14T12:39:59.781039","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '8')  # using pre-built analysis function","metadata":{"papermill":{"duration":0.695161,"end_time":"2021-05-14T12:40:00.758842","exception":false,"start_time":"2021-05-14T12:40:00.063681","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.949939Z","iopub.execute_input":"2021-06-12T21:05:05.950373Z","iopub.status.idle":"2021-06-12T21:05:05.963144Z","shell.execute_reply.started":"2021-06-12T21:05:05.95033Z","shell.execute_reply":"2021-06-12T21:05:05.961919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 8 Discussion:</h4>\nWe first tried to analyze this feature as a numerical feature, but then realized it's actaully categorical.","metadata":{"papermill":{"duration":0.125426,"end_time":"2021-05-14T12:40:01.013854","exception":false,"start_time":"2021-05-14T12:40:00.888428","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Analyzing it, the features seems categorical. ","metadata":{}},{"cell_type":"code","source":"analyze_categorical_feature(df, '8') # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:05.964622Z","iopub.execute_input":"2021-06-12T21:05:05.965133Z","iopub.status.idle":"2021-06-12T21:05:05.975608Z","shell.execute_reply.started":"2021-06-12T21:05:05.965086Z","shell.execute_reply":"2021-06-12T21:05:05.974766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 8 Discussion: (Categorical)</h4>\nThis feature has no nulls. The distribution of values is roughly even (only 2 values). We don't see strong correlation with the label, as both values are close to the mean of the label. However, the '0' value has stronger correlation with label '1'. ","metadata":{}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 9 Analysis</h3>","metadata":{"papermill":{"duration":0.126589,"end_time":"2021-05-14T12:40:01.270663","exception":false,"start_time":"2021-05-14T12:40:01.144074","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '9')  # using pre-built analysis function","metadata":{"papermill":{"duration":2.415009,"end_time":"2021-05-14T12:40:03.815419","exception":false,"start_time":"2021-05-14T12:40:01.40041","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.976625Z","iopub.execute_input":"2021-06-12T21:05:05.977067Z","iopub.status.idle":"2021-06-12T21:05:05.988617Z","shell.execute_reply.started":"2021-06-12T21:05:05.977024Z","shell.execute_reply":"2021-06-12T21:05:05.987643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 9 Discussion:</h4>\nNegilible amount of nulls.\nAccording to the qqplot and dist we can see that this feature does not match any known distribution. However,there're some outliers according to the boxplot.\nMoreover, we can see a similar line for both labels in the label distribution graph, which might prevent us from learning any information from this feature.","metadata":{"papermill":{"duration":0.142786,"end_time":"2021-05-14T12:40:04.104538","exception":false,"start_time":"2021-05-14T12:40:03.961752","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 10 Analysis</h3>","metadata":{"papermill":{"duration":0.139148,"end_time":"2021-05-14T12:40:04.384383","exception":false,"start_time":"2021-05-14T12:40:04.245235","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_categorical_feature(df, '10')  # using pre-built analysis function","metadata":{"papermill":{"duration":1.65401,"end_time":"2021-05-14T12:40:06.179618","exception":false,"start_time":"2021-05-14T12:40:04.525608","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:05.990088Z","iopub.execute_input":"2021-06-12T21:05:05.990449Z","iopub.status.idle":"2021-06-12T21:05:06.00123Z","shell.execute_reply.started":"2021-06-12T21:05:05.990405Z","shell.execute_reply":"2021-06-12T21:05:06.000052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try to view this feature as a numerical feature:","metadata":{}},{"cell_type":"code","source":"analyze_numerical_feature(df, '10')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.006076Z","iopub.execute_input":"2021-06-12T21:05:06.00639Z","iopub.status.idle":"2021-06-12T21:05:06.015094Z","shell.execute_reply.started":"2021-06-12T21:05:06.006356Z","shell.execute_reply":"2021-06-12T21:05:06.013955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 10 Discussion:</h4>\nThis feature seems numeric, after both analyzes.\nThe values seem to have some mathematical meaning.\nMost of the rows have the value '0'.\nWe can observe that the probabilty to get a label '1' is larger as we get farther away from value 0.","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 11 Analysis</h3>\n","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '11')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.018074Z","iopub.execute_input":"2021-06-12T21:05:06.018374Z","iopub.status.idle":"2021-06-12T21:05:06.028877Z","shell.execute_reply.started":"2021-06-12T21:05:06.018346Z","shell.execute_reply":"2021-06-12T21:05:06.027803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 11 Discussion:</h4>\nNegilible amount of nulls.The distribution is somewhat similar to a normal dist.\nHowever, we can see that the outliers prevent this feature from being normal, accodring to qqplot. We don't see any trend for the labels, which means it doesn't provide us with a lot of information.","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 12 Analysis</h3>","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_categorical_feature(df, '12')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.030136Z","iopub.execute_input":"2021-06-12T21:05:06.030455Z","iopub.status.idle":"2021-06-12T21:05:06.041595Z","shell.execute_reply.started":"2021-06-12T21:05:06.030415Z","shell.execute_reply":"2021-06-12T21:05:06.040252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 12 Discussion:</h4>\nNeglible amount of nulls.\nWe can see we have a lot more 'N' (which probably represents 'No'), than 'Y' ('yes').\n'y' has a strong correcltion to label 0.","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 13 Analysis</h3>","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '13')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.04285Z","iopub.execute_input":"2021-06-12T21:05:06.043345Z","iopub.status.idle":"2021-06-12T21:05:06.057354Z","shell.execute_reply.started":"2021-06-12T21:05:06.043309Z","shell.execute_reply":"2021-06-12T21:05:06.056315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see by the qqplot and distributons that this is a categorical feature.","metadata":{}},{"cell_type":"code","source":"analyze_categorical_feature(df, '13')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.058909Z","iopub.execute_input":"2021-06-12T21:05:06.059378Z","iopub.status.idle":"2021-06-12T21:05:06.067761Z","shell.execute_reply.started":"2021-06-12T21:05:06.059332Z","shell.execute_reply":"2021-06-12T21:05:06.06686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 13 Discussion:</h4>\nWe can see that value 2 is most of the rows. We don't see any strong correlation in the probabilities graph, however, 0 and 3 are better correlated with 1's, and values 1, 2 are better correlated with 0.","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 14 Analysis</h3>","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '14')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.069292Z","iopub.execute_input":"2021-06-12T21:05:06.069721Z","iopub.status.idle":"2021-06-12T21:05:06.080725Z","shell.execute_reply.started":"2021-06-12T21:05:06.06968Z","shell.execute_reply":"2021-06-12T21:05:06.079807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 14 Discussion:</h4>\nThere's a non-neglible amount of nulls, that might effect the model behaviour.\nThe feature distribution looks like an F distribution, which around the mean values are correlated with the label '0'.\nIn the label distribution graph we see a difference between the values of 0 and 1.\nThe probabilty to get the label '1' is stronger as the value gets larger.","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 15 Analysis</h3>","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '15')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.082101Z","iopub.execute_input":"2021-06-12T21:05:06.082516Z","iopub.status.idle":"2021-06-12T21:05:06.092104Z","shell.execute_reply.started":"2021-06-12T21:05:06.082461Z","shell.execute_reply":"2021-06-12T21:05:06.091055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This feature behaves more like a categorical feature. To analyze it, we'll use the categorical analyzation:","metadata":{}},{"cell_type":"code","source":"analyze_categorical_feature(df, '15')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.093473Z","iopub.execute_input":"2021-06-12T21:05:06.093826Z","iopub.status.idle":"2021-06-12T21:05:06.103131Z","shell.execute_reply.started":"2021-06-12T21:05:06.093788Z","shell.execute_reply":"2021-06-12T21:05:06.102348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 15 Discussion:</h4>\nThis feature doesn't give us a lot of information, because the overwhelming majority of values are '0'. ","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 16 Analysis</h3>","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_categorical_feature(df, '16')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.104108Z","iopub.execute_input":"2021-06-12T21:05:06.104481Z","iopub.status.idle":"2021-06-12T21:05:06.115297Z","shell.execute_reply.started":"2021-06-12T21:05:06.104453Z","shell.execute_reply":"2021-06-12T21:05:06.1145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 16 Discussion:</h4>\nWe assume this feature might represent time values (seconds, minutes, days).\nWe can see that 'M' is more common than S,D. However, non of which tell us a lot of informationm, because they all have a similar label distribution.","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 17 Analysis</h3>","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '17')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.116647Z","iopub.execute_input":"2021-06-12T21:05:06.117086Z","iopub.status.idle":"2021-06-12T21:05:06.127086Z","shell.execute_reply.started":"2021-06-12T21:05:06.117055Z","shell.execute_reply":"2021-06-12T21:05:06.126183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 17 Discussion:</h4>\nThis feature has an tendency to normal dist, accodring to qqplot and the distrubtion. Moreover, we see many outliers according to boxplot.\nThe edges tendency is to the '1' values, and the middle values have more '0' labels.","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 18 Analysis</h3>","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_categorical_feature(df, '18')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.12821Z","iopub.execute_input":"2021-06-12T21:05:06.128726Z","iopub.status.idle":"2021-06-12T21:05:06.140193Z","shell.execute_reply.started":"2021-06-12T21:05:06.128686Z","shell.execute_reply":"2021-06-12T21:05:06.139179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 18 Discussion:</h4>\nThis feature has about a third of it's values with the value 'a5', which correlates with the label 0.\nWe can see from the graph that most of the categories have some correlation with the label value.","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 19 Analysis</h3>","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_categorical_feature(df, '19')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.141381Z","iopub.execute_input":"2021-06-12T21:05:06.141831Z","iopub.status.idle":"2021-06-12T21:05:06.152982Z","shell.execute_reply.started":"2021-06-12T21:05:06.141786Z","shell.execute_reply":"2021-06-12T21:05:06.15199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 19 Discussion:</h4>\nMost of the values are nulls, therefore we expect this feature will be removed later on, because it doesn't provide us with any <b> reliable</b> information.","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Feature 20 Analysis</h3>","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"code","source":"analyze_numerical_feature(df, '20')  # using pre-built analysis function","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.154245Z","iopub.execute_input":"2021-06-12T21:05:06.154553Z","iopub.status.idle":"2021-06-12T21:05:06.163698Z","shell.execute_reply.started":"2021-06-12T21:05:06.15451Z","shell.execute_reply":"2021-06-12T21:05:06.162608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Feature 20 Discussion:</h4>\nThis features doesn't have any nulls. It looks like a normal distribution, without any correlation to one of the labels.\n","metadata":{"papermill":{"duration":0.145139,"end_time":"2021-05-14T12:40:06.470107","exception":false,"start_time":"2021-05-14T12:40:06.324968","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<h3>Numeric Features Correlation Analysis</h3>\nNow that we've analyzed all features, we'd like to look for correlation with the labels.","metadata":{}},{"cell_type":"code","source":"df_coor_test = df.copy()  # creating a copy of the df for correlations testing\ndf_coor_test['2'] = df_coor_test['2'].dropna().apply(lambda x: float(str(x)[:-1]))  # updating df copy so feature 2 will be numeric\ncorrelation_heatmap(df_coor_test)  # using your pre-built function to create a correlations graph for analysis","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.166221Z","iopub.execute_input":"2021-06-12T21:05:06.166574Z","iopub.status.idle":"2021-06-12T21:05:06.20423Z","shell.execute_reply.started":"2021-06-12T21:05:06.166525Z","shell.execute_reply":"2021-06-12T21:05:06.203159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Numeric Features Correlation Discussion:</h4>\nWe can see the highest correlation with the label classification is 0.43, and is related to feature no. 14.\nWe can see that featurs 10,8,2,0 have some correlation with a label classification. The other features have neglectible correlation values.\nIn addition, we also notice that features pair (9, 7), as well as (17, 5) have a high correlation.\nFeature pairs (9,3), (7,3), (10,2), have a medium correlation. The other features have either a low or a neglectible correlation. ","metadata":{}},{"cell_type":"markdown","source":"<hr>","metadata":{"papermill":{"duration":0.145746,"end_time":"2021-05-14T12:40:06.760485","exception":false,"start_time":"2021-05-14T12:40:06.614739","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h1>2. Pre-Processing</h1>\nAfter we've observed and studied our input data, we'll now prepare it for processing.","metadata":{"papermill":{"duration":0.148567,"end_time":"2021-05-14T12:40:07.732036","exception":false,"start_time":"2021-05-14T12:40:07.583469","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# fresh start: reading both train and test raw files\n#df = pd.read_csv('train.csv')  # path to raw train file\n#df_origin_test = pd.read_csv('test_without_target.csv')  # path to raw test file\ndf = pd.read_csv('../input/mldata/train.csv')  # path to raw train data file\ndf_origin_test = pd.read_csv('../input/mldata/test_without_target.csv')  # path to raw train data file","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.205441Z","iopub.execute_input":"2021-06-12T21:05:06.205757Z","iopub.status.idle":"2021-06-12T21:05:06.324885Z","shell.execute_reply.started":"2021-06-12T21:05:06.205728Z","shell.execute_reply":"2021-06-12T21:05:06.323832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating copies of dfs:\ndf_train = df.copy()\ndf_test = df_origin_test.copy()\ndf_test = df_test.drop(df_test.columns[0],axis=1,inplace=False)  # removing raw input index column","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.326613Z","iopub.execute_input":"2021-06-12T21:05:06.32703Z","iopub.status.idle":"2021-06-12T21:05:06.337297Z","shell.execute_reply.started":"2021-06-12T21:05:06.326986Z","shell.execute_reply":"2021-06-12T21:05:06.336388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking that both train and test have the same amount of features (columns excluding label column)\nprint(len(df_train.columns.drop('label')))\nprint(len(df_test.columns))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.338865Z","iopub.execute_input":"2021-06-12T21:05:06.339289Z","iopub.status.idle":"2021-06-12T21:05:06.345254Z","shell.execute_reply.started":"2021-06-12T21:05:06.339239Z","shell.execute_reply":"2021-06-12T21:05:06.344336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# displaying both sets head to compare:\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.346406Z","iopub.execute_input":"2021-06-12T21:05:06.346701Z","iopub.status.idle":"2021-06-12T21:05:06.380365Z","shell.execute_reply.started":"2021-06-12T21:05:06.346673Z","shell.execute_reply":"2021-06-12T21:05:06.379268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:06.381935Z","iopub.execute_input":"2021-06-12T21:05:06.382421Z","iopub.status.idle":"2021-06-12T21:05:06.414012Z","shell.execute_reply.started":"2021-06-12T21:05:06.382371Z","shell.execute_reply":"2021-06-12T21:05:06.413083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we hold both data sets in the same condition, we'll start applying changes so they can fit to the processing stage:","metadata":{}},{"cell_type":"markdown","source":"<h3>Values Types Handling</h3>","metadata":{}},{"cell_type":"code","source":"def remove_char(dataframe, feature, c):\n    \"\"\"\n    handling feature's numeric values with combined string element c by removing c from value\n    \"\"\"\n    dataframe[feature] = dataframe[feature].map(lambda x: x.strip(c) if type(x) == type('str') else x)\n    return None\n\ndef convert_to_numerical(dataframe, feature):\n    \"\"\"\n    updating feature's values from objects to numrics\n    \"\"\"\n    dataframe[feature] = dataframe[feature].apply(pd.to_numeric)\n    print('dtypes    ',feature, dataframe['2'].dtypes)\n    return None\n\n\n# Remove redundant characters from feature 2\nremove_char(df_train, '2', 'd')\nremove_char(df_test, '2', 'd')\nconvert_to_numerical(df_train, '2')\nconvert_to_numerical(df_test, '2')","metadata":{"papermill":{"duration":0.202654,"end_time":"2021-05-14T12:40:08.089917","exception":false,"start_time":"2021-05-14T12:40:07.887263","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:06.415225Z","iopub.execute_input":"2021-06-12T21:05:06.415714Z","iopub.status.idle":"2021-06-12T21:05:07.200242Z","shell.execute_reply.started":"2021-06-12T21:05:06.415673Z","shell.execute_reply":"2021-06-12T21:05:07.199249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_categorical(dataframe, feature):\n    \"\"\"\n    changing categorial features from numeric\n    \"\"\"\n    dataframe[feature] = dataframe[feature].astype(\"object\")\n    return None\n\n# updating categorical features to correct type (were initially classified as numeric due to their values type):\nconvert_to_categorical(df_train, '8')\nconvert_to_categorical(df_train, '13')\nconvert_to_categorical(df_train, '15')\nconvert_to_categorical(df_test, '8')\nconvert_to_categorical(df_test, '13')\nconvert_to_categorical(df_test, '15')","metadata":{"papermill":{"duration":0.191693,"end_time":"2021-05-14T12:40:08.431009","exception":false,"start_time":"2021-05-14T12:40:08.239316","status":"completed"},"scrolled":true,"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:07.201505Z","iopub.execute_input":"2021-06-12T21:05:07.20187Z","iopub.status.idle":"2021-06-12T21:05:07.221146Z","shell.execute_reply.started":"2021-06-12T21:05:07.201836Z","shell.execute_reply":"2021-06-12T21:05:07.220191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking both data sets are still aligned:\nprint(len(df_train.columns.drop('label')))\nprint(len(df_test.columns))","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:07.222423Z","iopub.execute_input":"2021-06-12T21:05:07.222748Z","iopub.status.idle":"2021-06-12T21:05:07.229639Z","shell.execute_reply.started":"2021-06-12T21:05:07.222718Z","shell.execute_reply":"2021-06-12T21:05:07.228743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:07.230594Z","iopub.execute_input":"2021-06-12T21:05:07.230859Z","iopub.status.idle":"2021-06-12T21:05:07.267857Z","shell.execute_reply.started":"2021-06-12T21:05:07.230834Z","shell.execute_reply":"2021-06-12T21:05:07.266876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:07.269282Z","iopub.execute_input":"2021-06-12T21:05:07.269602Z","iopub.status.idle":"2021-06-12T21:05:07.303953Z","shell.execute_reply.started":"2021-06-12T21:05:07.269558Z","shell.execute_reply":"2021-06-12T21:05:07.3028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for same types\ndf_test.dtypes == df_train.dtypes.drop('label')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:07.305383Z","iopub.execute_input":"2021-06-12T21:05:07.30573Z","iopub.status.idle":"2021-06-12T21:05:07.321176Z","shell.execute_reply.started":"2021-06-12T21:05:07.305694Z","shell.execute_reply":"2021-06-12T21:05:07.320118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Missing Data & Outliers Handling</h3>","metadata":{}},{"cell_type":"code","source":"def relocate_value(dataframe, feature, val):\n    \"\"\"\n    relocate selected value to most frequent category\n    \"\"\"\n    most_freq = dataframe[feature].describe()['top']\n    if most_freq == val:\n        print('most freqent is the value itself')\n        return None\n    return dataframe.replace([val], most_freq)\n    \ndef drop_nulls_rich_features(dataframe, threshold=0.5):\n    \"\"\"\n    reviewing each feature for precentage of nulls.\n    if precentage is higher than threshold - dropping the feature from dataframe.\n    \"\"\"\n    # Drop columns based on threshold limit\n    updated_df = dataframe.dropna(axis=1, thresh=len(dataframe.index) * (1 - threshold))\n    return updated_df\n\ndef get_mean_wo_outliers(dataframe, feature):\n    \"\"\"\n    getting z-score based mean from train dataframe\n    \"\"\"\n    z = np.abs(stats.zscore(dataframe[dataframe[feature].notna()][feature]))\n    mean_val = dataframe[dataframe[feature].notna()][z < 3][feature].mean()\n    return mean_val\n\ndef remove_outliers(dataframe):\n    \"\"\"\n    remove outliers from dataframe (according to z score)\n    Using data deviation\n    \"\"\"\n    dataframe = dataframe[(np.abs(stats.zscore(dataframe[dataframe.describe().columns.values[:-1]])) < 3).all(axis=1)]\n    return dataframe \n\ndef fill_numerical_nulls(dataframe, mean, feature): # perhapds add isTrain\n    \"\"\"\n    updating selected feature nulls with values based on the rest of the data\n    \"\"\"\n    f_type = dataframe[feature].dtype  # checking what is the type of the data to handle accordingly\n    if f_type == float:\n        mean_val = mean  # getting mean value to replace nulls with          \n        dataframe[feature] = dataframe[feature].fillna(mean_val)\n    return dataframe\n\ndef get_most_freq_imputer(dataframe, feature):\n    \"\"\"\n    finds and returns an imputer based on most frequent value of given df\n    \"\"\"\n    #return dataframe[feature].describe()['top']\n    imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n    imputer.fit(dataframe[feature].values.reshape(-1, 1))\n    return imputer\n\n\ndef fill_categorical_nulls(dataframe, feature, imputer):\n    \"\"\"\n    filling (replacing) nulls with the most frequent value on a certain feature\n    \"\"\"\n    dataframe[feature] = imputer.transform(dataframe[feature].values.reshape(-1, 1))\n    return dataframe","metadata":{"papermill":{"duration":0.164695,"end_time":"2021-05-14T12:40:08.742851","exception":false,"start_time":"2021-05-14T12:40:08.578156","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:07.322948Z","iopub.execute_input":"2021-06-12T21:05:07.323286Z","iopub.status.idle":"2021-06-12T21:05:07.338222Z","shell.execute_reply.started":"2021-06-12T21:05:07.323256Z","shell.execute_reply":"2021-06-12T21:05:07.337352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop null rich features, default threshold is: >50% nulls\ndf_train = drop_nulls_rich_features(df_train)\n\n# dropping test colums based on the ones that were dropped from the train set:\ndf_test = df_test[df_train.columns.drop('label')]","metadata":{"papermill":{"duration":0.184049,"end_time":"2021-05-14T12:40:09.075471","exception":false,"start_time":"2021-05-14T12:40:08.891422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:07.339894Z","iopub.execute_input":"2021-06-12T21:05:07.34063Z","iopub.status.idle":"2021-06-12T21:05:07.408306Z","shell.execute_reply.started":"2021-06-12T21:05:07.340581Z","shell.execute_reply":"2021-06-12T21:05:07.407259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill nulls for numerical\nfor f in df_train.describe().columns[:-1]:\n    feature_mean = get_mean_wo_outliers(df_train, f)\n    df_train = fill_numerical_nulls(df_train, feature_mean , f) # fill nulls according to train mean\n    df_test = fill_numerical_nulls(df_test, feature_mean, f) # fill nulls according to train mean","metadata":{"papermill":{"duration":0.184049,"end_time":"2021-05-14T12:40:09.075471","exception":false,"start_time":"2021-05-14T12:40:08.891422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:07.40959Z","iopub.execute_input":"2021-06-12T21:05:07.409897Z","iopub.status.idle":"2021-06-12T21:05:07.57767Z","shell.execute_reply.started":"2021-06-12T21:05:07.409867Z","shell.execute_reply":"2021-06-12T21:05:07.576511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill nulls for categorical and one-hot encoding\nfor f in df_train.select_dtypes(['O']).columns:\n    train_imputer = get_most_freq_imputer(df_train, f)\n    df_train = fill_categorical_nulls(df_train, f, train_imputer)\n    df_test = fill_categorical_nulls(df_test, f, train_imputer)","metadata":{"papermill":{"duration":0.184049,"end_time":"2021-05-14T12:40:09.075471","exception":false,"start_time":"2021-05-14T12:40:08.891422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:07.579214Z","iopub.execute_input":"2021-06-12T21:05:07.579531Z","iopub.status.idle":"2021-06-12T21:05:07.631462Z","shell.execute_reply.started":"2021-06-12T21:05:07.579498Z","shell.execute_reply":"2021-06-12T21:05:07.630456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove outliers from df (using data deviation)\ndf_train = remove_outliers(df_train)","metadata":{"papermill":{"duration":0.184049,"end_time":"2021-05-14T12:40:09.075471","exception":false,"start_time":"2021-05-14T12:40:08.891422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:07.632758Z","iopub.execute_input":"2021-06-12T21:05:07.633063Z","iopub.status.idle":"2021-06-12T21:05:07.687048Z","shell.execute_reply.started":"2021-06-12T21:05:07.633033Z","shell.execute_reply":"2021-06-12T21:05:07.686179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:07.688097Z","iopub.execute_input":"2021-06-12T21:05:07.688549Z","iopub.status.idle":"2021-06-12T21:05:07.717297Z","shell.execute_reply.started":"2021-06-12T21:05:07.688505Z","shell.execute_reply":"2021-06-12T21:05:07.716026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:07.718765Z","iopub.execute_input":"2021-06-12T21:05:07.719124Z","iopub.status.idle":"2021-06-12T21:05:07.746861Z","shell.execute_reply.started":"2021-06-12T21:05:07.71908Z","shell.execute_reply":"2021-06-12T21:05:07.745618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:07.74827Z","iopub.execute_input":"2021-06-12T21:05:07.748845Z","iopub.status.idle":"2021-06-12T21:05:07.777783Z","shell.execute_reply.started":"2021-06-12T21:05:07.748811Z","shell.execute_reply":"2021-06-12T21:05:07.776724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:07.779238Z","iopub.execute_input":"2021-06-12T21:05:07.779546Z","iopub.status.idle":"2021-06-12T21:05:07.793811Z","shell.execute_reply.started":"2021-06-12T21:05:07.779505Z","shell.execute_reply":"2021-06-12T21:05:07.792997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Categorical Features Handling</h3>","metadata":{}},{"cell_type":"code","source":"def one_hot_encoding(dataframe, categorial_feature):\n    \"\"\"\n    transforming categorial value to 'one hot vector'\n    \"\"\"\n    df = dataframe\n    f = categorial_feature\n    # create a hot vector matrix, naming the column's\n    # prefix with the featrue's name\n    du = pd.get_dummies(df[f],prefix = f)\n    # concating new columns with exisitng dataframe\n    update_df = pd.concat([df, du], axis=1)\n    # deleting old column\n    update_df = update_df.drop(columns=[f])\n    return update_df","metadata":{"papermill":{"duration":0.164695,"end_time":"2021-05-14T12:40:08.742851","exception":false,"start_time":"2021-05-14T12:40:08.578156","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:07.795095Z","iopub.execute_input":"2021-06-12T21:05:07.795673Z","iopub.status.idle":"2021-06-12T21:05:07.807812Z","shell.execute_reply.started":"2021-06-12T21:05:07.795627Z","shell.execute_reply":"2021-06-12T21:05:07.807057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transforing all categorical features to one-hot vectors\nfor f in df_train.select_dtypes(['O']).columns:\n    df_train = one_hot_encoding(df_train, f)\n    df_test = one_hot_encoding(df_test, f)\n\n","metadata":{"papermill":{"duration":0.184049,"end_time":"2021-05-14T12:40:09.075471","exception":false,"start_time":"2021-05-14T12:40:08.891422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:07.809084Z","iopub.execute_input":"2021-06-12T21:05:07.809667Z","iopub.status.idle":"2021-06-12T21:05:07.971641Z","shell.execute_reply.started":"2021-06-12T21:05:07.809622Z","shell.execute_reply":"2021-06-12T21:05:07.970718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making sure train and test are still alignd, after hot-vector operations\nprint(df_test.columns == df_train.columns.drop('label'))","metadata":{"papermill":{"duration":0.184049,"end_time":"2021-05-14T12:40:09.075471","exception":false,"start_time":"2021-05-14T12:40:08.891422","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-06-12T21:05:07.973008Z","iopub.execute_input":"2021-06-12T21:05:07.973286Z","iopub.status.idle":"2021-06-12T21:05:07.98014Z","shell.execute_reply.started":"2021-06-12T21:05:07.97326Z","shell.execute_reply":"2021-06-12T21:05:07.978788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Data Scaling</h3>","metadata":{}},{"cell_type":"code","source":"def get_unscaled_features(dataframe):\n    '''get list of features that are not standartaized'''\n    not_std_features = []\n    # go over all numerical features and label\n    for feature in dataframe.describe().columns:\n        # Look for one hot vectors or already normalized features\n        if not ((max(dataframe[feature]) == 1.0) and (min(dataframe[feature]) == 0.0)):\n            not_std_features.append(feature)   \n    return not_std_features\n\ndef get_fit_scaler_std(dataframe, features):\n    '''get the standartization scaler'''\n    scaler = StandardScaler()\n    return scaler.fit(dataframe[features])\n\ndef get_fit_scaler_minmax(dataframe, features):\n    '''get the minmax scaler'''\n    scaler = MinMaxScaler()\n    return scaler.fit(dataframe[features])\n\n# standardize feature with scalar\ndef scale_features(dataframe, features, scaler):\n    '''transform dataframe with scaler'''\n    df_scaled = pd.DataFrame(scaler.transform(dataframe[features].values), columns=features, index=dataframe.index)\n    return df_scaled","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:07.981329Z","iopub.execute_input":"2021-06-12T21:05:07.981763Z","iopub.status.idle":"2021-06-12T21:05:07.993153Z","shell.execute_reply.started":"2021-06-12T21:05:07.981722Z","shell.execute_reply":"2021-06-12T21:05:07.992124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we'll scale only numeric features, and avoid categorical one-hot-vectors, and the label.","metadata":{}},{"cell_type":"code","source":"unscaled_features = get_unscaled_features(df_train)\n\n# create standardScaler\nstandardScaler_train = get_fit_scaler_std(df_train, unscaled_features)\n# create minmax scaler\nminmaxScaler_train = get_fit_scaler_minmax(df_train, unscaled_features)\n\n# create copies for both scaling methods\ndf_train_std = df_train.copy()\ndf_test_std = df_test.copy()\ndf_train_minmax = df_train.copy()\ndf_test_minmax = df_test.copy()\n# standartization scaling:\ndf_train_std[unscaled_features] = scale_features(df_train, unscaled_features, standardScaler_train) # train\ndf_test_std[unscaled_features] = scale_features(df_test, unscaled_features, standardScaler_train) # test using the same scaler\n# minmax scaling:\ndf_train_minmax[unscaled_features] = scale_features(df_train, unscaled_features, minmaxScaler_train) # train\ndf_test_minmax[unscaled_features] = scale_features(df_test, unscaled_features, minmaxScaler_train) # test using the same scaler","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:07.994467Z","iopub.execute_input":"2021-06-12T21:05:07.995095Z","iopub.status.idle":"2021-06-12T21:05:08.403779Z","shell.execute_reply.started":"2021-06-12T21:05:07.995051Z","shell.execute_reply":"2021-06-12T21:05:08.402885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_std.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:08.404997Z","iopub.execute_input":"2021-06-12T21:05:08.405626Z","iopub.status.idle":"2021-06-12T21:05:08.440577Z","shell.execute_reply.started":"2021-06-12T21:05:08.40558Z","shell.execute_reply":"2021-06-12T21:05:08.439578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_std.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:08.442038Z","iopub.execute_input":"2021-06-12T21:05:08.44263Z","iopub.status.idle":"2021-06-12T21:05:08.480359Z","shell.execute_reply.started":"2021-06-12T21:05:08.442577Z","shell.execute_reply":"2021-06-12T21:05:08.479323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_minmax.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:08.481794Z","iopub.execute_input":"2021-06-12T21:05:08.482101Z","iopub.status.idle":"2021-06-12T21:05:08.518979Z","shell.execute_reply.started":"2021-06-12T21:05:08.482072Z","shell.execute_reply":"2021-06-12T21:05:08.517755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_minmax.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:08.520294Z","iopub.execute_input":"2021-06-12T21:05:08.520645Z","iopub.status.idle":"2021-06-12T21:05:08.55729Z","shell.execute_reply.started":"2021-06-12T21:05:08.520613Z","shell.execute_reply":"2021-06-12T21:05:08.555573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Dimension Reduction</h3>\nWe will use the PCA method reduce the dimentions by creating new dataframe with less columns which hold 98% of the variance","metadata":{}},{"cell_type":"code","source":"def get_pca(dataframe):\n    pca = PCA(n_components = dataframe.shape[1])\n    pca.fit(dataframe)\n    dataframe_new = pca.transform(dataframe)\n    explaind_variance_ratio = pca.explained_variance_ratio_\n    return dataframe_new, explaind_variance_ratio\n\ndef get_number_of_features_for_variance(dataframe, exp_variance):\n    pca = PCA(n_components = exp_variance, svd_solver = 'full')\n    pca.fit(dataframe)\n    dataframe_new = pca.transform(dataframe)\n    explaind_variance_ratio = pca.explained_variance_ratio_\n    return dataframe_new, explaind_variance_ratio\n\ndef get_pca_fit(dataframe, exp_variance):\n    pca = PCA(n_components = exp_variance, svd_solver = 'full')\n    return pca.fit(dataframe)\n\ndef pca_transformation(dataframe, pca_fitted):\n    dataframe_new = pd.DataFrame(pca_fitted.transform(dataframe), index=dataframe.index)\n    return dataframe_new\n\n\ndef accumulative_pca_graph(dataframe1, dataframe2, explained_variance_ratio1, explained_variance_ratio2):\n    fig, ax = plt.subplots(figsize=(10,5))\n    plt.plot(explained_variance_ratio1.cumsum(),color='navy', label=\"std\")\n    plt.plot(explained_variance_ratio2.cumsum(),color='green',linestyle=\"--\", label=\"minmax\")\n    plt.title(\"Accumulative explanation by PCA\", fontsize=11)\n    plt.ylabel('Accumulative explained variance', fontsize=11)\n    plt.xlabel('No. Of features', fontsize=11)\n    plt.xticks(range(0,dataframe1.shape[1],1))   \n    plt.axhline(0.98, c='r')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:08.558416Z","iopub.execute_input":"2021-06-12T21:05:08.558802Z","iopub.status.idle":"2021-06-12T21:05:08.569981Z","shell.execute_reply.started":"2021-06-12T21:05:08.558766Z","shell.execute_reply":"2021-06-12T21:05:08.569148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete the 'label'\ndf_train_std_no_label = df_train_std.copy()\ndel df_train_std_no_label['label']\ndf_train_std_no_label","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:08.571046Z","iopub.execute_input":"2021-06-12T21:05:08.571529Z","iopub.status.idle":"2021-06-12T21:05:08.625663Z","shell.execute_reply.started":"2021-06-12T21:05:08.571494Z","shell.execute_reply":"2021-06-12T21:05:08.624912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pca_std, variance_std = get_pca(df_train_std[unscaled_features])\ndf_pca_minmax, variance_minmax = get_pca(df_train_minmax[unscaled_features])\naccumulative_pca_graph(df_pca_std,df_pca_minmax,variance_std, variance_minmax)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:08.631175Z","iopub.execute_input":"2021-06-12T21:05:08.631631Z","iopub.status.idle":"2021-06-12T21:05:09.175176Z","shell.execute_reply.started":"2021-06-12T21:05:08.631598Z","shell.execute_reply":"2021-06-12T21:05:09.174329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the minmax scaled dataset requires a similar amount of 9 dimentions to conserve the 98% of the variance","metadata":{}},{"cell_type":"code","source":"train_pca_fit = get_pca_fit(df_train_std[unscaled_features], 0.98)\n# Transform train according to TRAIN fit\ntrain_std_pca_reduced = pca_transformation(df_train_std[unscaled_features], train_pca_fit )\n# Transform test according to TRAIN fit (TRAIN, not TEST)\ntest_std_pca_reduced = pca_transformation(df_test_std[unscaled_features], train_pca_fit )\n\ntrain_std_pca_reduced = pd.concat([train_std_pca_reduced, df_train_std.drop(unscaled_features, axis=1)], axis=1)\ntest_std_pca_reduced = pd.concat([test_std_pca_reduced, df_test_std.drop(unscaled_features, axis=1)], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:09.176811Z","iopub.execute_input":"2021-06-12T21:05:09.177267Z","iopub.status.idle":"2021-06-12T21:05:09.219033Z","shell.execute_reply.started":"2021-06-12T21:05:09.177222Z","shell.execute_reply":"2021-06-12T21:05:09.217706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_std_pca_reduced","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:09.22447Z","iopub.execute_input":"2021-06-12T21:05:09.224979Z","iopub.status.idle":"2021-06-12T21:05:09.308552Z","shell.execute_reply.started":"2021-06-12T21:05:09.224934Z","shell.execute_reply":"2021-06-12T21:05:09.307505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_pca_fit = get_pca_fit(df_train_std[unscaled_features], 0.98)\n# Transform train according to TRAIN fit\ntrain_minmax_pca_reduced = pca_transformation(df_train_std[unscaled_features], train_pca_fit )\n# Transform test according to TRAIN fit (TRAIN, not TEST)\ntest_minmax_pca_reduced = pca_transformation(df_test_std[unscaled_features], train_pca_fit )\n\ntrain_minmax_pca_reduced = pd.concat([train_minmax_pca_reduced, df_train_minmax.drop(unscaled_features, axis=1)], axis=1)\ntest_minmax_pca_reduced = pd.concat([test_minmax_pca_reduced, df_test_minmax.drop(unscaled_features, axis=1)], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:09.313917Z","iopub.execute_input":"2021-06-12T21:05:09.31686Z","iopub.status.idle":"2021-06-12T21:05:09.365578Z","shell.execute_reply.started":"2021-06-12T21:05:09.316785Z","shell.execute_reply":"2021-06-12T21:05:09.364502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_minmax_pca_reduced","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:09.370686Z","iopub.execute_input":"2021-06-12T21:05:09.373348Z","iopub.status.idle":"2021-06-12T21:05:09.449646Z","shell.execute_reply.started":"2021-06-12T21:05:09.373277Z","shell.execute_reply":"2021-06-12T21:05:09.448602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_std_pca_reduced","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:09.454574Z","iopub.execute_input":"2021-06-12T21:05:09.45726Z","iopub.status.idle":"2021-06-12T21:05:09.507331Z","shell.execute_reply.started":"2021-06-12T21:05:09.457182Z","shell.execute_reply":"2021-06-12T21:05:09.506622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_minmax_pca_reduced","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:09.508273Z","iopub.execute_input":"2021-06-12T21:05:09.508661Z","iopub.status.idle":"2021-06-12T21:05:09.553273Z","shell.execute_reply.started":"2021-06-12T21:05:09.508633Z","shell.execute_reply":"2021-06-12T21:05:09.552516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr>","metadata":{}},{"cell_type":"markdown","source":"<h1>3. Model Training</h1>","metadata":{}},{"cell_type":"markdown","source":"Now that both train and test dataset are fully proccessed, we can proceed to the model training stage. We'll start by examining the results, without any further manipulation on the data, including not balancing the label.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import fbeta_score\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:09.554254Z","iopub.execute_input":"2021-06-12T21:05:09.554659Z","iopub.status.idle":"2021-06-12T21:05:09.644061Z","shell.execute_reply.started":"2021-06-12T21:05:09.55463Z","shell.execute_reply":"2021-06-12T21:05:09.643284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create Train, and Test Sets. The train will be used for training. The test will only be used only for assesing the model, not for training.","metadata":{}},{"cell_type":"code","source":"X = train_std_pca_reduced.drop('label', axis=1)\ny = train_std_pca_reduced['label']","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:09.645035Z","iopub.execute_input":"2021-06-12T21:05:09.645437Z","iopub.status.idle":"2021-06-12T21:05:09.652465Z","shell.execute_reply.started":"2021-06-12T21:05:09.645407Z","shell.execute_reply":"2021-06-12T21:05:09.651628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:09.653763Z","iopub.execute_input":"2021-06-12T21:05:09.654245Z","iopub.status.idle":"2021-06-12T21:05:09.675967Z","shell.execute_reply.started":"2021-06-12T21:05:09.654214Z","shell.execute_reply":"2021-06-12T21:05:09.674915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:09.677167Z","iopub.execute_input":"2021-06-12T21:05:09.678002Z","iopub.status.idle":"2021-06-12T21:05:09.724086Z","shell.execute_reply.started":"2021-06-12T21:05:09.677964Z","shell.execute_reply":"2021-06-12T21:05:09.722855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_gnb = GaussianNB()\nclf_gnb.fit(X_train, y_train)\nclf_gnb.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:09.725504Z","iopub.execute_input":"2021-06-12T21:05:09.725855Z","iopub.status.idle":"2021-06-12T21:05:09.766077Z","shell.execute_reply.started":"2021-06-12T21:05:09.725822Z","shell.execute_reply":"2021-06-12T21:05:09.765061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import label_binarize\n\nknnparameters =  {'n_neighbors':[3,5,11,30,50],\n                  'weights':['uniform'],\n                  'metric': ['euclidean', 'manhattan']}\n\nGS_knn = GridSearchCV(KNeighborsClassifier(), knnparameters, cv=3, scoring='roc_auc')\nGS_knn.fit(X_train, y_train)\n\nprint ('KNN chosen parameters (recieved best AUC): {}'.format(GS_knn.best_params_))\nprint (\"KNN AUC score with the chosen parameters: \", GS_knn.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:09.767201Z","iopub.execute_input":"2021-06-12T21:05:09.767492Z","iopub.status.idle":"2021-06-12T21:05:14.826775Z","shell.execute_reply.started":"2021-06-12T21:05:09.767458Z","shell.execute_reply":"2021-06-12T21:05:14.823449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_knn = KNeighborsClassifier(**GS_knn.best_params_)\nclf_knn.fit(X_train, y_train)\nprint('score ',clf_knn.score(X_test, y_test)) # Model Accuracy","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:14.828231Z","iopub.status.idle":"2021-06-12T21:05:14.828919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ANN","metadata":{}},{"cell_type":"code","source":"parametersOptions = {'activation' : [\"logistic\", \"relu\"], #\n                      'hidden_layer_sizes' : [(100,), # 1 large hidden layer\n                                             (50, 50), # 2 medium size layers\n                                             (20, 20, 10, 10, 10)], # multiple small sized layers\n                      'batch_size' : [10, 50],\n                      'learning_rate_init' : [0.1, 0.01], #In some of the runs we saw that the network got stuck on a local min, for this reason we enlearge the defualt momentum\n                      'max_iter' : [1500]} \n\nGS_ann = GridSearchCV(MLPClassifier(), parametersOptions, cv=3, scoring='roc_auc')\nGS_ann.fit(X_train, y_train)\n\nprint ('ANN chosen parameters (recieved best AUC): {}'.format(GS_ann.best_params_))\nprint (\"ANN AUC score with the chosen parameters: \", GS_ann.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:14.830363Z","iopub.status.idle":"2021-06-12T21:05:14.831032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann_clf = MLPClassifier(**GS_ann.best_params_, random_state=42)  # Using the best parameters\nann_clf.fit(X_train, y_train)\nprint('score ',ann_clf.score(X_test, y_test)) # Model Accuracy","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:14.832495Z","iopub.status.idle":"2021-06-12T21:05:14.833156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [100,300,500,800,1200] #[int(x) for x in np.linspace(start = 100, stop = 300, num = 50)]\n# Maximum number of levels in tree\nmax_depth = [5,10,20,40]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\nparametersOptions = {'n_estimators': n_estimators,\n                       'max_depth': max_depth,\n                       'min_samples_split': min_samples_split,\n                       }\n\nGS_randomForest = GridSearchCV(RandomForestClassifier(), parametersOptions, cv=3, scoring='roc_auc')\nGS_randomForest.fit(X_train, y_train)\n\nprint ('RandomForest chosen parameters (recieved best AUC): {}'.format(GS_randomForest.best_params_))\nprint (\"RandomForest AUC score with the chosen parameters: \", GS_randomForest.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:14.834618Z","iopub.status.idle":"2021-06-12T21:05:14.835244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"randomForest_clf = RandomForestClassifier(**GS_randomForest.best_params_, random_state=42)  # Using the best parameters\nrandomForest_clf.fit(X_train, y_train)\nprint('score ', randomForest_clf.score(X_test, y_test) ) # Model Accuracy","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:14.836477Z","iopub.status.idle":"2021-06-12T21:05:14.837125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>4. Models Evaluation</h1>\nWe'll first create an Confusion Matrix for one of our trained models:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix \n\ndef create_confusion_matrix(trained_model, X, y):\n    \"\"\"\n    given trained model and data set, applying the data set without label\n    on the trained model to produce predictions.\n    Then comparing these predictions with the true labels of the data set\n    to produce a confusion matrix for the model.\n    \"\"\"\n    prediction = trained_model.predict(X)\n    plot_confusion_matrix(trained_model, X, y, cmap=plt.cm.Blues)\n    plt.show()  ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:14.838511Z","iopub.status.idle":"2021-06-12T21:05:14.839164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def KfoldPlot(X, y, X_valid, y_valid, clf, k):\n    \n    #str(clf) returns the name of the classifier, we set this as the title.\n    clfname = str(clf)[:str(clf).find(\"(\")]\n    #We prepare a figure, which will be filled in with some graphs inside.\n    plt.figure()\n    #We initialize the KFOLD, which will be iterated later.\n    kf = KFold(n_splits=k, shuffle=False)\n    #Since we need to interpolate data, we set an mean tpr and fpr for later porpuses.\n    mean_tpr = 0.0\n    mean_tpr_train = 0.0\n    mean_fpr = np.linspace(0, 1, 100)\n    mean_fpr_train = np.linspace(0, 1, 100)\n    i=1\n    \n    #We iterate over the folds.\n    for train_index, test_index in kf.split(X):\n        #Just printing the current fold\n        print(\"Folding No. \", i)\n        #Splitting into train and validation, based on the current fold.\n        X_train, X_test = X[train_index[0]:train_index[-1]], X[test_index[0]:test_index[-1]]\n        y_train, y_test = y[train_index[0]:train_index[-1]], y[test_index[0]:test_index[-1]]\n        #We fit with X_train and y_train.\n        clf.fit(X_train, y_train)\n        \n        # predict on train\n        #y_train_p = clf.predict_proba(X_train)[:,1]\n        # predict on test\n        #y_test_p = clf.predict_proba(X_test)[:,1]\n        #fpr_tr,tpr_tr,thresholds_tr=roc_curve(y_train,y_prob_tr) #get the FPR TPR for both test and train\n        #fpr_te,tpr_te,thresholds_te=roc_curve(y_test,y_prob_te)\n       # plt.plot(fpr_te, tpr_te, color='steelblue',linestyle='--')\n        \n        # The prediction we want to test against are the probability of 1 of the X_test\n        prob_prediction = clf.predict_proba(X_test)[:, 1]\n        # for evaluation, we'll predict on the train itself\n        prob_prediction_train = clf.predict_proba(X_train)[:, 1]\n        \n        # Right now we can set our ROC curve on the specific fold.\n        fpr, tpr, thresholds = roc_curve(y_test, prob_prediction)\n        # Doing the same for the train\n        fpr_train, tpr_train, thresholds_train = roc_curve(y_train, prob_prediction_train)\n        \n        #We interpolate the mean_tpr so all of the classifiers will be working under the same thersholds.\n        mean_tpr += interp(mean_fpr, fpr, tpr)\n        #We interpolate the mean_tpr so all of the classifiers will be working under the same thersholds.\n        mean_tpr_train += interp(mean_fpr_train, fpr_train, tpr_train)\n        \n        mean_tpr[0] = 0.0\n        mean_tpr_train[0] = 0.0\n        #We plot the current fold with the color #D3D3D3\n        plt.plot(fpr, tpr, color='#D3D3D3')\n        i+=1\n    plt.plot([0], [0], color='#D3D3D3', linestyle='-', label='K-folds')    \n    \n    \n    #Right now the mean_tpr is a sum (we added each iteration, so we finally divide by the number of folds)\n    mean_tpr /= k\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    \n    # we do the same for the train set\n    mean_tpr_train /= k\n    mean_tpr_train[-1] = 1.0\n    mean_auc_train = auc(mean_fpr_train, mean_tpr_train)\n    diff = np.abs(mean_auc_train - mean_auc)\n    \n    if diff > 0.1:\n        print(\"The model displays overfitting properties. The test-based AUC is \",\n              mean_auc, \"and the train-based auc is: \", mean_auc_train, \". The difference is: \", diff)\n    else:\n        print(\"The model *does not* displays overfitting properties. The test-based AUC is \",\n              mean_auc, \"and the train-based auc is: \", mean_auc_train, \". The difference is: \", diff)\n    \n    #After all of the iterations, we plot the random-guess line\n    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n    #And we plot the mean roc, the red line should be in the middle of all gray lines.\n    plt.plot(mean_fpr, mean_tpr, color='red', linestyle='-', label='Mean ROC (AUC = %0.3f)' % mean_auc)\n    #Setting some boundaries and adding a legend... nothing special.\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC for ' + clfname)\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    return mean_auc\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:14.840518Z","iopub.status.idle":"2021-06-12T21:05:14.841169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_confusion_matrix(clf_gnb, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:14.84266Z","iopub.status.idle":"2021-06-12T21:05:14.843276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = None\nmodels  = [clf_gnb, clf_knn, ann_clf, randomForest_clf]\nmax_auc = 0\nfor model in models:\n    curr_auc = KfoldPlot(X_train, y_train, X_test, y_test , model , 5)\n    if curr_auc > max_auc:\n        max_auc = curr_auc\n        best_model = model","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:14.844513Z","iopub.status.idle":"2021-06-12T21:05:14.845159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We choose according to the AUC metric. The highest AUC metric belongs to","metadata":{}},{"cell_type":"code","source":"best_model.fit(X_train,y_train)\nprediction = best_model.predict(X_test)\ncreate_confusion_matrix(best_model, X_test, y_test)\n#create_confusion_matrix(best_model, X, y)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:14.846397Z","iopub.status.idle":"2021-06-12T21:05:14.846907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1>5. Prediction</h1>","metadata":{}},{"cell_type":"code","source":"best_model.fit(X , y)\ny_pred = best_model.predict_proba(test_std_pca_reduced)\nresults = pd.DataFrame(pd.DataFrame(y_pred, columns = ['pred_proba', 'to_drop'])['pred_proba'])\nresults.to_csv('results.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-12T21:05:14.847713Z","iopub.status.idle":"2021-06-12T21:05:14.848095Z"},"trusted":true},"execution_count":null,"outputs":[]}]}