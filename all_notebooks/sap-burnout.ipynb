{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/hackerearth-employee-burnout-challenge/train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = data.shape[0]\nfor col in data.columns:\n    print('Column {} with missing value = {}%'.format(col,100*data[col].isna().sum()/rows))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dropna(subset=['Burn Rate','Mental Fatigue Score'],axis=0,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = data.shape[0]\nfor col in data.columns:\n    print('Column {} with missing value = {}%'.format(col,100*data[col].isna().sum()/rows))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Date of Joining'] =pd.to_datetime(data['Date of Joining'])\nrecent_joining = data['Date of Joining'].max()\nprint(recent_joining)\ndata['Date of Joining'] = data['Date of Joining'].apply(lambda x : data['Date of Joining'].max()-x)\ndata['Date of Joining'] = data['Date of Joining'].astype('int')/86400000000000","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.corr()\n## Resource allocation is a big factor in burn rate and mental fatigue score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\ncorr_plot = sns.heatmap(data.corr())\nfigure = corr_plot.get_figure()    \nfigure.savefig('corr_plot.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Insights with this\n    1. Date of Joining is not relevent (atleast this dataset says so)\n    2. Resource allocation plays a big part in fatigue and burn rate scoring","metadata":{}},{"cell_type":"code","source":"data.drop(['Date of Joining','Employee ID'],axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_variables = data.iloc[:,-2:]\nfeatures         = data.iloc[:,:-2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features = pd.get_dummies(features)\ntrain_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We will be using LightGBM which can handle NA values so we don't need to worry about the NA features","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV\nburnrate_model = LGBMRegressor()\nparam_grid = {\n    'n_estimators': [30, 128,200],\n    'colsample_bytree': [0.3,0.7],\n    'max_depth': [15,\n                  25],\n    'num_leaves': [50, 100,120],\n    'reg_alpha': [1.1, 1.3],\n    'reg_lambda': [1.1, 1.3],\n    'min_split_gain': [ 0.4],\n    'subsample': [0.7, 0.9],\n    'subsample_freq': [20]\n}\n\nburnrate_gs = GridSearchCV(\n    estimator=burnrate_model,\n    param_grid=param_grid, \n    cv=5, \n    n_jobs=-1, \n    scoring='neg_mean_absolute_error',\n    verbose=True\n)\nburnrate_fit = burnrate_gs.fit(train_features, target_variables.iloc[:,-1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"burnrate_fit.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fatigue_model = LGBMRegressor()\nparam_grid = {\n    'n_estimators': [30, 128,200],\n    'colsample_bytree': [0.3,0.7],\n    'max_depth': [15,\n                  25],\n    'num_leaves': [50, 100,120],\n    'reg_alpha': [1.1, 1.3],\n    'reg_lambda': [1.1, 1.3],\n    'min_split_gain': [ 0.4],\n    'subsample': [0.7, 0.9],\n    'subsample_freq': [20]\n}\n\nfatigue_gs = GridSearchCV(\n    estimator=fatigue_model,\n    param_grid=param_grid, \n    cv=5, \n    n_jobs=-1, \n    scoring='neg_mean_absolute_error',\n    verbose=True\n)\nfatigue_fit = burnrate_gs.fit(train_features, target_variables.iloc[:,-2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fatigue_fit.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}