{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Predict TripAdvisor Rating**\n\n*Проект № 3. О вкусной и здоровой пище*"},{"metadata":{},"cell_type":"markdown","source":"# import"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\n\n# Загружаем специальный удобный инструмент для разделения датасета:\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\nRANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Подробнее по признакам:\n* City: Город \n* Cuisine Style: Кухня\n* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n* Price Range: Цены в ресторане в 3 категориях\n* Number of Reviews: Количество отзывов\n* Reviews: 2 последних отзыва и даты этих отзывов\n* URL_TA: страница ресторана на 'www.tripadvisor.com' \n* ID_TA: ID ресторана в TripAdvisor\n* Rating: Рейтинг ресторана"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Reviews[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Как видим, большинство признаков у нас требует очистки и предварительной обработки."},{"metadata":{},"cell_type":"markdown","source":"## 1. Обработка NAN \nУ наличия пропусков могут быть разные причины, но пропуски нужно либо заполнить, либо исключить из набора полностью. Но с пропусками нужно быть внимательным, **даже отсутствие информации может быть важным признаком!**   \nПо этому перед обработкой NAN лучше вынести информацию о наличии пропуска как отдельный признак "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Для примера я возьму столбец Number of Reviews\ndata['Number_of_Reviews_isNAN'] = pd.isna(data['Number of Reviews']).astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Number_of_Reviews_isNAN']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Далее заполняем пропуски 0, вы можете попробовать заполнением средним или средним по городу и тд...\ndata['Number of Reviews'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Обработка признаков\nДля начала посмотрим какие признаки у нас могут быть категориальными."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.nunique(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# изменим название одного из городов 'Oporto' на более используемое в справочниках 'Porto'\ndata.City.replace('Oporto', 'Porto', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# составим список из городов, упомянутых в датафрейме\nmy_cities = list(data.City.value_counts().index)\n# из внешних источников загрузим файл с данными о городах мира и составим словари cities_pop город: население и cities_cap город:признак города\n# где признак города равен 2 - если это столица, 1 - если это крупный город, 0 - в остальных случаях\nworldcities = pd.read_csv('/kaggle/input/world-cities-datasets/worldcities.csv')\nworldcities_short = worldcities[['city_ascii', 'capital', 'population']]\ncities_pop = {}\ncities_cap = {}\nfor city in my_cities:\n    city_list = worldcities_short[worldcities_short.city_ascii==city]\n    # в загруженном справочнике есть города с одинаковым названием, выбираем из них один город, где самое большое население\n    pop_max = city_list.population.max()\n    capital = city_list[city_list.population == pop_max].capital.values[0]\n    if capital =='primary':\n        cap = 2\n    elif capital == 'admin':\n        cap = 1\n    else:\n        cap = 0\n    cities_pop[city] = pop_max\n    cities_cap[city] = cap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# посмотрим все ли города попали в словарь\nprint(f'Количество городов в cities_popt - {len(cities_pop)}')\nprint(f'Количество городов в my_cities - {len(my_cities)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# создадим колонки с названиям городов\ndf_cities = pd.get_dummies(data['City'], prefix='City')\ndata = data.join(df_cities)\ndisplay(data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# в соответствии со словарем заполним столбец признака населения Population\n# параллельно заполним признак Capital - признак города\nfor row in range(data.shape[0]):\n    data.loc[row, 'Population'] = cities_pop[data.loc[row, 'City']]\n    data.loc[row, 'Capital'] = cities_cap[data.loc[row, 'City']]\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# создадим столбец Reviews day, показывающий количество дней межу последними 2 отзывами, если отзывов нет или был всего один, то значение равно 0\nimport datetime\nfrom datetime import datetime, timedelta\nimport re\n\n\npattern = r'\\d{2}/\\d{2}/\\d{4}'\n\ndatetime_list = []\ncomments_days = []\nfor ind in range(data.shape[0]):\n    dates = re.findall(pattern, str(data.loc[ind,'Reviews']))\n    comments_len = len(dates)\n    if  comments_len > 0:\n        for i in range(comments_len):\n            datetime_list.append(datetime.strptime(dates[i], '%m/%d/%Y'))\n        if comments_len > 1:\n            dist = np.abs(datetime_list[-1] - datetime_list[-2])\n            comments_days.append(dist.days)\n        else:\n            comments_days.append(0)\n    else:\n        comments_days.append(0)\n\ndata['Reviews_days'] = comments_days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# заменим пропуски в признаке 'Cuisine Style' значением No cuisine\ndata['Cuisine Style'].fillna(' No cuisine ', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# создадим словарь с перечнем кухонь и список с количеством кухонь в каждом ресторане, если данных не было, то количество принимаем за 1\ncuisine = {}\ncol_cuisine = []\nfor ind in range(len(data)):\n    s = 0\n    string = data.loc[ind, 'Cuisine Style'][1:-1].split(',')\n    for item in string:\n        item = item.replace(\"'\", '')\n        item.strip()\n        s += 1\n        if ord(item[0]) == 32:\n            item = item[1:]\n        if item in cuisine:\n            cuisine[item] += 1\n        else:\n            cuisine[item] = 1\n    col_cuisine.append(s)\n\n# отсортируем полученный словарь cuisine по убыванию значений\ncuisine = {k: v for k, v in sorted(cuisine.items(), key=lambda item: item[1], reverse=True)}\n\ncuisine","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# создадим вспомогательный датафрейм df_cuisine, в котором будут отмечены кухни 10 наиболее популярных кухонь,\n# остальные кухни объеденены в 'Others'\n# так как значение No cuisine попадает в топ-10 кухонь, то возьмем 11 значений\n\ncuisine_top = list(cuisine)[:11]\ndf_cuisine = pd.DataFrame(columns=cuisine_top, dtype=int)\nfor row in range(data.shape[0]):\n    counter = 0\n    cuisine_string = data.loc[row, 'Cuisine Style']\n    for name in cuisine_top:\n        if name in cuisine_string:\n            df_cuisine.loc[row, name] = 1\n            counter += 1\n        else:\n            df_cuisine.loc[row, name] = 0\n    if counter < col_cuisine[row]: \n        df_cuisine.loc[row, 'Others'] = 1\n        \n# отсавшиеся незаполненными значние в признаке Other заполняем 0\ndf_cuisine.Others.fillna(0, inplace=True)\ndf_cuisine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# добавим этот датафрейм в исходную базу\ndata = data.join(df_cuisine)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# добавим новый количественный признак Cuisines_sum -  количествj кухонь в ресторане\ndata['Cuisines_sum'] = col_cuisine","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# исследуем id ресторанов \ndata.Restaurant_id.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# судя по тому, что некоторые id повторяются, можно говорить о наличии сетевых ресторанов\n# создадим новый признак Chain \ndict_chain = data.Restaurant_id.value_counts()\nfor id in dict_chain.keys():\n    if dict_chain[id] > 1:\n        dict_chain[id] = 1\n    else:\n        dict_chain[id] = 0\nfor row in range(data.shape[0]):\n    data.loc[row, 'Chain'] = dict_chain[data.loc[row, 'Restaurant_id']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# напишем функцию по созданию словаря из слов из отобранных отзывов\ndef words_in_dict(full_string, pattern=r'\\d{2}/\\d{2}/\\d{4}'):\n    def_dict = {}\n    for row in range(len(full_string)):\n        string_rev = str(full_string[row]).split(',')\n        if len(string_rev) <= 2:\n            continue   \n        for phrase in string_rev:\n            dates = re.findall(pattern, phrase)\n            if len(dates)>0:\n                continue\n            else:\n                words = phrase.split(' ')\n                for word in words:\n                    word=\"\".join(c for c in word if c.isalpha())\n                    word=word.lower()\n                    if word in def_dict:\n                        def_dict[word] += 1\n                    else:\n                        def_dict[word] = 1\n    def_dict = {k: v for k, v in sorted(def_dict.items(), key=lambda item: item[1], reverse=True)}\n    # удаляем из словаря союзы, предлоги, местоимения, мало значащие, но популярные слова\n    [def_dict.pop(key) for key in ['', 'a', 'and', 'an', 'the', 'in', 'at', 'for', 'of', 'with', 'it', 'is', 'i', 'as', 'be', 'to', 'food', 'service']]\n    return def_dict\n\n\n# заполним словарь для тех данных, где рейтинг 4 и выше\nbest_reviews = data[data.Rating>3.5].Reviews\nbest_reviews.reset_index(drop=True, inplace=True)\ngood_words = words_in_dict(best_reviews)\nprint(good_words)\n\n# на основе этих данных создадим вручную список наиболее характерных отзывов для положительных отзывов\nyes_words = ['good', 'great', 'nice', 'very', 'excellent', 'lovel', 'amaz', 'delicious', 'atmosphere', 'fantastic', 'tasty']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# теперь составим аналогичный словарь для отзывов ресторанов с рейтингом от 2.5 и ниже\nworst_reviews = data[data.Rating<3].Reviews\nworst_reviews.reset_index(drop=True, inplace=True)\nbad_words = words_in_dict(worst_reviews)\nprint(bad_words)\n\n# на основе этих данных создадим вручную список наиболее характерных отзывов для отрицательных отзывов\nno_words = ['never', 'no', 'hell', 'bad', 'worst', 'awful', 'terribl', 'sad', 'disappoint', 'not good', 'dont', 'avoid', 'horribl']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# на основе полученных списков заполняем новые признаки Bad reviews и Good reviews\n# если \"плохое\" слово присутствует в отзыве, то ставим 1 у Bad reviews\n# причем, если в отзыве текущей строки есть слова из списка \"плохих\", то Good reviews = 0\ndata['Good reviews'] = 0\ndata['Bad reviews'] = 0\ndata.Reviews.fillna('Empty', inplace=True)\nfor row in range(data.shape[0]):\n    for word_n in no_words:\n        if word_n in data.loc[row, 'Reviews'].lower():\n            data.loc[row, 'Bad reviews'] = 1\n    if data.loc[row, 'Bad reviews'] == 0:\n        for word_y in yes_words:\n            if word_y in data.loc[row, 'Reviews'].lower():\n                data.loc[row, 'Good reviews'] = 1\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Возьмем следующий признак \"Price Range\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"# всего в признаке 'Price Range' выделяются 3 состояния\ndata['Price Range'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"По описанию 'Price Range' это - Цены в ресторане.  \nИх можно поставить по возрастанию (значит это не категориальный признак). А это значит, что их можно заменить последовательными числами, например 1,2,3  \n*Попробуйте сделать обработку этого признака уже самостоятельно!*"},{"metadata":{"trusted":true},"cell_type":"code","source":"# заменим в столбце 'Price Range' символьные значения на более понятные: $$$$ - 3, $$ - $$$ - 2, $ - 1, пропуски заполним отличным значением - 0\ndict_price = {'$$$$':3, '$$ - $$$':2,'$':1}\ndata.replace({'Price Range':dict_price}, inplace=True)\ndata['Price Range'].fillna(0, inplace=True)\ndata['Price Range']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение признака"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (10,7)\ndf_train['Ranking'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"У нас много ресторанов, которые не дотягивают и до 2500 места в своем городе, а что там по городам?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['City'].value_counts(ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"А кто-то говорил, что французы любят поесть=) Посмотрим, как изменится распределение в большом городе:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Ranking'][df_train['City'] =='London'].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# посмотрим на топ 10 городов\nfor x in (df_train['City'].value_counts())[0:10].index:\n    df_train['Ranking'][df_train['City'] == x].hist(bins=100)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Получается, что Ranking имеет нормальное распределение, просто в больших городах больше ресторанов, из-за мы этого имеем смещение.\n\n>Подумайте как из этого можно сделать признак для вашей модели. Я покажу вам пример, как визуализация помогает находить взаимосвязи. А далее действуйте без подсказок =) \n"},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение целевой переменной"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Rating'].value_counts(ascending=True).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Посмотрим распределение целевой переменной относительно признака"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Ranking'][df_train['Rating'] == 5].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Ranking'][df_train['Rating'] < 4].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### И один из моих любимых - [корреляция признаков](https://ru.wikipedia.org/wiki/Корреляция)\nНа этом графике уже сейчас вы сможете заметить, как признаки связаны между собой и с целевой переменной."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,10)\nsns.heatmap(data.drop(['sample'], axis=1).corr(),)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\nТеперь, для удобства и воспроизводимости кода, завернем всю обработку в одну большую функцию."},{"metadata":{"trusted":true},"cell_type":"code","source":"# на всякий случай, заново подгружаем данные\nDATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'/kaggle_task.csv')\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\nworldcities = pd.read_csv('/kaggle/input/world-cities-datasets/worldcities.csv')\nworldcities_short = worldcities[['city_ascii', 'capital', 'population']]\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preproc_data(df_input):\n        \n    df_output = df_input.copy()\n    \n    # ################### 1. Предобработка ############################################################## \n    # убираем не нужные для модели признаки\n    df_output.drop(['URL_TA','ID_TA',], axis = 1, inplace=True)\n    \n    \n    # ################### 2. NAN ############################################################## \n    # Далее заполняем пропуски, вы можете попробовать заполнением средним или средним по городу и тд...\n    df_output['NumberReviews_isNAN'] = pd.isna(df_output['Number of Reviews']).astype('uint8')\n    df_output['Number of Reviews'].fillna(0, inplace=True)\n    df_output['Price Range'].fillna(0, inplace=True)\n    df_output['Cuisine Style'].fillna(' No cuisine ', inplace=True)\n    df_output['Reviews'].fillna('Empty', inplace=True)\n    \n    \n    # ################### 3. Encoding ############################################################## \n    # заменим в столбце 'Price Range' символьные значения на более понятные: $$$$ - 3, $$ - $$$ - 2, $ - 1, пропуски заполним отличным значением - 0\n    dict_price = {'$$$$':3, '$$ - $$$':2,'$':1}\n    df_output.replace({'Price Range':dict_price}, inplace=True)\n    # изменим название одного из городов 'Oporto' на более используемое в справочниках 'Porto'\n    df_output.City.replace('Oporto', 'Porto', inplace=True)\n    # составим список из городов, упомянутых в датафрейме\n    my_cities = list(df_output.City.value_counts().index)\n    cities_pop = {}\n    cities_cap = {}\n    for city in my_cities:\n        city_list = worldcities_short[worldcities_short.city_ascii==city]\n        # в загруженном справочнике есть города с одинаковым названием, выбираем из них один город, где самое большое население\n        pop_max = city_list.population.max()\n        capital = city_list[city_list.population == pop_max].capital.values[0]\n        if capital =='primary':\n            cap = 2\n        elif capital == 'admin':\n            cap = 1\n        else:\n            cap = 0\n        cities_pop[city] = pop_max\n        cities_cap[city] = cap\n    \n    # в соответствии со словарем заполним столбец признака населения Population\n    # параллельно заполним признак Capital - признак города\n    for row in range(df_output.shape[0]):\n        df_output.loc[row, 'Population'] = cities_pop[df_output.loc[row, 'City']]\n        df_output.loc[row, 'Capital'] = cities_cap[df_output.loc[row, 'City']]\n    # создадим колонки с названиям городов\n    df_cities = pd.get_dummies(df_output['City'], prefix='City')\n    df_output = df_output.join(df_cities)\n    \n    # ################### 4. Feature Engineering ####################################################\n    # создадим столбец Reviews day, показывающий количество дней межу последними 2 отзывами, если отзывов нет или был всего один, то значение равно 0\n    pattern = r'\\d{2}/\\d{2}/\\d{4}'\n\n    datetime_list = []\n    comments_days = []\n    for ind in range(df_output.shape[0]):\n        dates = re.findall(pattern, str(df_output.loc[ind,'Reviews']))\n        comments_len = len(dates)\n        if  comments_len > 0:\n            for i in range(comments_len):\n                datetime_list.append(datetime.strptime(dates[i], '%m/%d/%Y'))\n            if comments_len > 1:\n                dist = np.abs(datetime_list[-1] - datetime_list[-2])\n                comments_days.append(dist.days)\n            else:\n                comments_days.append(0)\n        else:\n            comments_days.append(0)\n\n    df_output['Reviews_days'] = comments_days\n    \n    # создадим словарь с перечнем кухонь и список с количеством кухонь в каждом ресторане, если данных не было, то количество принимаем за 1\n    cuisine = {}\n    col_cuisine = []\n    for ind in range(len(df_output)):\n        s = 0\n        string = df_output.loc[ind, 'Cuisine Style'][1:-1].split(',')\n        for item in string:\n            item = item.replace(\"'\", '')\n            item.strip()\n            s += 1\n            if ord(item[0]) == 32:\n                item = item[1:]\n            if item in cuisine:\n                cuisine[item] += 1\n            else:\n                cuisine[item] = 1\n        col_cuisine.append(s)\n    # отсортируем полученный словарь cuisine по убыванию значений\n    cuisine = {k: v for k, v in sorted(cuisine.items(), key=lambda item: item[1], reverse=True)}\n    \n    # создадим вспомогательный датафрейм df_cuisine, в котором будут отмечены кухни 10 наиболее популярных кухонь,\n    # остальные кухни объеденены в 'Others'\n    # так как значение No cuisine попадает в топ-10 кухонь, то возьмем 11 значений\n    cuisine_top = list(cuisine)[:11]\n    df_cuisine = pd.DataFrame(columns=cuisine_top, dtype=int)\n    for row in range(df_output.shape[0]):\n        counter = 0\n        cuisine_string = df_output.loc[row, 'Cuisine Style']\n        for name in cuisine_top:\n            if name in cuisine_string:\n                df_cuisine.loc[row, name] = 1\n                counter += 1\n            else:\n                df_cuisine.loc[row, name] = 0\n        if counter < col_cuisine[row]: \n            df_cuisine.loc[row, 'Others'] = 1\n        \n    # отсавшиеся незаполненными значние в признаке Other заполняем 0\n    df_cuisine.Others.fillna(0, inplace=True)\n    # добавим этот датафрейм в исходную базу\n    df_output = df_output.join(df_cuisine)\n    \n    # добавим новый количественный признак Cuisines_sum - количество кухонь в ресторане \n    df_output['Cuisines_sum'] = col_cuisine\n    \n    # создадим новый признак Chain \n    dict_chain = df_output.Restaurant_id.value_counts()\n    for id in dict_chain.keys():\n        if dict_chain[id] > 1:\n            dict_chain[id] = 1\n        else:\n            dict_chain[id] = 0\n    for row in range(df_output.shape[0]):\n        df_output.loc[row, 'Chain'] = dict_chain[df_output.loc[row, 'Restaurant_id']]\n        \n    yes_words = ['good', 'great', 'nice', 'very', 'excellent', 'lovel', 'amaz', 'delicious', 'atmosphere', 'fantastic', 'tasty']\n    no_words = ['never', 'no', 'hell', 'bad', 'worst', 'awful', 'terribl', 'sad', 'disappoint', 'not good', 'dont', 'avoid', 'horribl']\n    # на основе полученных списков заполняем новые признаки Bad reviews и Good reviews\n    # если \"плохое\" слово присутствует в отзыве, то ставим 1 у Bad reviews\n    # причем, если в отзыве текущей строки есть слова из списка \"плохих\", то Good reviews = 0\n    df_output['Good reviews'] = 0\n    df_output['Bad reviews'] = 0\n    for row in range(df_output.shape[0]):\n        for word_n in no_words:\n            if word_n in df_output.loc[row, 'Reviews'].lower():\n                df_output.loc[row, 'Bad reviews'] = 1\n        if df_output.loc[row, 'Bad reviews'] == 0:\n            for word_y in yes_words:\n                if word_y in df_output.loc[row, 'Reviews'].lower():\n                    df_output.loc[row, 'Good reviews'] = 1\n    \n    \n    # ################### 5. Clean #################################################### \n    # убираем признаки которые еще не успели обработать, \n    # модель на признаках с dtypes \"object\" обучаться не будет, просто выберим их и удалим\n    object_columns = [s for s in df_output.columns if df_output[s].dtypes == 'object']\n    df_output.drop(object_columns, axis = 1, inplace=True)\n    \n    return df_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">По хорошему, можно было бы перевести эту большую функцию в класс и разбить на подфункции (согласно ООП). "},{"metadata":{},"cell_type":"markdown","source":"#### Запускаем и проверяем что получилось"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc = preproc_data(data)\ndf_preproc.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Rating.values            # наш таргет\nX = train_data.drop(['Rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Перед тем как отправлять наши данные на обучение, разделим данные на еще один тест и трейн, для валидации. \nЭто поможет нам проверить, как хорошо наша модель работает, до отправки submissiona на kaggle.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n# выделим 20% данных на валидацию (параметр test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# проверяем\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model \nСам ML"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Импортируем необходимые библиотеки:\nfrom sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\nfrom sklearn import metrics # инструменты для оценки точности модели","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Обучаем модель на тестовом наборе данных\nmodel.fit(X_train, y_train)\n\n# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n# Предсказанные значения записываем в переменную y_pred\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# округлим значения y_pred до 0.5\ndef round_to_05(x):\n    return np.round(x*2)/2\n\n\ny_pred = round_to_05(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\nЕсли все устраевает - готовим Submission на кагл"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.drop(['Rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission = model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission = round_to_05(predict_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['Rating'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Выводы: \n1. Исследуемый датасет слабо реагировал на добавление новых признаков изменением метрики MAE до того самого момента, пока не вставил код, округляющий значения y_predict до 0.5. Это сдвинуло значение метрики с 0,21 на 0,18-0,17.\n2. За прошедшее время с начала обучения я НЕ научился приемам \"убыстрения\" обработки базы. Полный прогон кода занимает порядка 20 минут, что, на мой взгляд, очень долго. (Хотелось бы, чтобы этим приемам именно УЧИЛИ на курсе, а не мы сами приходили к этому, шертся сотни страниц google). Пока в этом моменте курсы меня разочаровывают.\n3. В целом, было создано и оттестировано несколько новых количественных признаков, в том числе, с задействованием внешних источников информации.\n4. Были реализованы разные подходы к обработке пропусков - заполнение наиболее часто встречающимся значением, заполнение отличным от других значением (чтобы не создавать отдельный признак отсутствия информации), заполнение константой.\n5. Были задействованы знания по использованию библиотек re и datetime.\n6. Были использованы приемы преобразования категориальных признаков в количественные с помощью метода get_dummies.\n7. В этом задании очень много работал с методом .loc, но не могу дать оценку насколько правильно я его задействовал, так как в целом меня не удовлетворили результаты скорости обработки информации моим кодом (см. п. 2).\n8. Довольно часто создавал словари, мне нравится их создавать. Но вопрос траты времени на создание словарей по сравнению с другими подходами остается открытым.\n9. В целом оцениваю уровень своего креатива на 3 по 5-балльной системе. \n10. Задание далось неожиданно очень тяжело. Сильно его усложняла скорость обработки (см. п. 2), так как каждый переапуск кода занимал очень долгое время.\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}