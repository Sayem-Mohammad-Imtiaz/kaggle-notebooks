{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Very simple XGBoost regression\n\n[XGBoost](https://xgboost.readthedocs.io/en/latest/) was the first of ***The Big Three*** [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) frameworks, released in 2014. The other two are [LightGBM](https://www.microsoft.com/en-us/research/project/lightgbm/) by Microsoft and launched in 2016, and [CatBoost](https://catboost.ai/) by Yandex, launched in 2017. Each of these frameworks are magnificent tools to tackling tabular data problems, using either regression or classification.\n\n#### What is '*boosting*'?\nFirst there was a **tree**. The underlying element of these technique is the decision tree. Decision trees were one of the first algorithms, dating back to the 1980s with examples such as CART, and ID3, C4.5 and C5.0 by Quinlan. Trees are wonderfully intuitive leading to easily interpretable results. See for example the notebook [\"*Titanic: some sex, a bit of class, and a tree...*\"](https://www.kaggle.com/carlmcbrideellis/titanic-some-sex-a-bit-of-class-and-a-tree). In view of this the most important hyperparameter for the [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) is the `max_depth`, although being a tree perhaps this should have really been called the maximum height...\nHowever, despite the appealing aspects such as few hyperparameters and interpretability, the drawback of decision trees is their high variance; a slight change in the input data can lead to a radically different tree structure. A similar thing can happen if there are [collinear variables](https://en.wikipedia.org/wiki/Multicollinearity) present. Sometimes individual decision trees are know as weak predictors, or weak learners.\n\nThen we have a **forest**. As we all know, listening to many opinions and taking an average leads to a more balanced consensus. With this in mind, why not randomly plant a lot of trees and then ensemble them into one aggregate output. Each of the trees are slightly different in that they are grown from a subset of randomly selected features, which are taken from a \"bootstrapped\" copy of the dataset which is made up from samples taken from the original dataset.\nWe now have the random forest, which outperforms the individual decision tree. Random forests are great in reducing the [variance](https://en.wikipedia.org/wiki/Variance) with respect to a single decision tree, are particularly immune to overfitting, and are wonderful for obtaining a baseline score against which to compare more extravagant techniques. For more details see the [introduction by Breiman and Cutler](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm) who invented the Random Forest in the early 2000's. With the [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) the most important hyperparameters are now `max_depth` as before, as well as `n_estimators`; which is the number of trees in the forest.\n\n**Gradient boosting**. This time, instead of simultaneously planting a load of independent trees all at once at random (bootstrapping and aggregating aka. [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating)), each successive tree that we plant is weighted in such a way as to compensate for any weakness (residual errors) in the previous tree. This is known as [boosting](https://en.wikipedia.org/wiki/Gradient_boosting). We have the hyperparameters `max_depth`, `n_estimators` as before, and now we have a `learning_rate` hyperparameter which is between 0 and 1, and controls the amount of *shrinkage* when creating each successive new tree.\n\n\n### Sample script:\n\nThis here is a minimalist script which applies XGBoost regression to the [House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) data set. The purpose of this script is to serve as a basic starting framework which one easily can adapt. \n\nSome suggestions for ways to improve the score are:\n\n* Feature selection: for example see [recursive feature elimination script](https://www.kaggle.com/carlmcbrideellis/recursive-feature-elimination-hp-v1)\n* Feature engineering: creating new features out of the existing features\n* Outlier removal\n* Imputation of missing values: XGBoost is resilient to missing values, however one may like to try using the [missingpy](https://github.com/epsilon-machine/missingpy) library\n\n**The best hyperparameters**. XGBoost has a multitude of hyperparameters, but here we shall only be using three of them. The optimal choice of these parameters can lead to a significant improvement in ones final score, so choosing the best values for these hyperparameters is important. To do this we shall perform a [cross-validated](https://scikit-learn.org/stable/modules/cross_validation.html) grid-search using the scikit-learn [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) routine. Note that this can be quite time consuming, as it tries out each and every hyperparameter combination, exhaustively checking for the best result.","metadata":{}},{"cell_type":"code","source":"#===========================================================================\n# load up the libraries\n#===========================================================================\nimport pandas  as pd\nimport numpy   as np\nimport xgboost as xgb\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv',index_col=0)\ntest_data  = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv',index_col=0)\n\n#===========================================================================\n# here, for this simple demonstration we shall only use the numerical columns \n# and ingnore the categorical features\n#===========================================================================\nX_train = train_data.select_dtypes(include=['number']).copy()\nX_train = X_train.drop(['SalePrice'], axis=1)\ny_train = train_data[\"SalePrice\"]\nX_test  = test_data.select_dtypes(include=['number']).copy()\n\n#===========================================================================\n# XGBoost regression: \n# Parameters: \n# n_estimators  \"Number of gradient boosted trees. Equivalent to number \n#                of boosting rounds.\"\n# learning_rate \"Boosting learning rate (also known as “eta”)\"\n# max_depth     \"Maximum depth of a tree. Increasing this value will make \n#                the model more complex and more likely to overfit.\" \n#===========================================================================\nregressor=xgb.XGBRegressor()\n\n#===========================================================================\n# exhaustively search for the optimal hyperparameters\n#===========================================================================\nfrom sklearn.model_selection import GridSearchCV\n# set up our search grid\nparam_grid = {\"max_depth\": [2, 3, 4, 5],\n              \"n_estimators\": [400, 500, 600, 700],\n              \"learning_rate\": [0.015, 0.020, 0.025]}\n\n# try out every combination of the above values\nsearch = GridSearchCV(regressor, param_grid, cv=5).fit(X_train, y_train)\n\nprint(\"The best hyperparameters are \",search.best_params_)","metadata":{"_uuid":"365f3e96-c7a9-41cf-9c5f-76dbfd46168c","_cell_guid":"e7842527-0531-44fa-8ce0-ae4cc3cfd0d7","execution":{"iopub.status.busy":"2021-07-09T10:27:47.927821Z","iopub.execute_input":"2021-07-09T10:27:47.928118Z","iopub.status.idle":"2021-07-09T10:30:20.119519Z","shell.execute_reply.started":"2021-07-09T10:27:47.92809Z","shell.execute_reply":"2021-07-09T10:30:20.118768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we shall now use these values for our hyperparameters in our final calculation","metadata":{}},{"cell_type":"code","source":"regressor=xgb.XGBRegressor(learning_rate = search.best_params_[\"learning_rate\"],\n                           n_estimators  = search.best_params_[\"n_estimators\"],\n                           max_depth     = search.best_params_[\"max_depth\"])\n\nregressor.fit(X_train, y_train)\n\n#===========================================================================\n# To use early_stopping_rounds: \n# \"Validation metric needs to improve at least once in every \n# early_stopping_rounds round(s) to continue training.\"\n#===========================================================================\n# first perform a test/train split \n#from sklearn.model_selection import train_test_split\n\n#X_train,X_test,y_train,y_test = train_test_split(X_train,y_train, test_size = 0.2)\n#regressor.fit(X_train, y_train, early_stopping_rounds=6, eval_set=[(X_test, y_test)], verbose=False)\n\n#===========================================================================\n# use the model to predict the prices for the test data\n#===========================================================================\npredictions = regressor.predict(X_test)","metadata":{"_uuid":"365f3e96-c7a9-41cf-9c5f-76dbfd46168c","_cell_guid":"e7842527-0531-44fa-8ce0-ae4cc3cfd0d7","execution":{"iopub.status.busy":"2021-07-09T10:30:20.121351Z","iopub.execute_input":"2021-07-09T10:30:20.121854Z","iopub.status.idle":"2021-07-09T10:30:20.913637Z","shell.execute_reply.started":"2021-07-09T10:30:20.121818Z","shell.execute_reply":"2021-07-09T10:30:20.912896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let us now calculate our score (for more details see my notebook [\"*House Prices: How to work offline*\"](https://www.kaggle.com/carlmcbrideellis/house-prices-how-to-work-offline))","metadata":{}},{"cell_type":"code","source":"# read in the ground truth file\nsolution   = pd.read_csv('../input/house-prices-advanced-regression-solution-file/solution.csv')\ny_true     = solution[\"SalePrice\"]\n\nfrom sklearn.metrics import mean_squared_log_error\nRMSLE = np.sqrt( mean_squared_log_error(y_true, predictions) )\nprint(\"The score is %.5f\" % RMSLE )","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:30:20.915108Z","iopub.execute_input":"2021-07-09T10:30:20.915627Z","iopub.status.idle":"2021-07-09T10:30:20.929677Z","shell.execute_reply.started":"2021-07-09T10:30:20.915593Z","shell.execute_reply":"2021-07-09T10:30:20.928648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and write out a `submission.csv` for the competition","metadata":{}},{"cell_type":"code","source":"#===========================================================================\n# write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({\"Id\":test_data.index, \"SalePrice\":predictions})\noutput.to_csv('submission.csv', index=False)","metadata":{"_uuid":"365f3e96-c7a9-41cf-9c5f-76dbfd46168c","_cell_guid":"e7842527-0531-44fa-8ce0-ae4cc3cfd0d7","execution":{"iopub.status.busy":"2021-07-09T10:31:15.952925Z","iopub.execute_input":"2021-07-09T10:31:15.953247Z","iopub.status.idle":"2021-07-09T10:31:15.969915Z","shell.execute_reply.started":"2021-07-09T10:31:15.953217Z","shell.execute_reply":"2021-07-09T10:31:15.96923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature importance\nLet us also take a very quick look at the feature importance too:","metadata":{}},{"cell_type":"code","source":"from xgboost import plot_importance\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 16})\n\nfig, ax = plt.subplots(figsize=(12,6))\nplot_importance(regressor, max_num_features=8, ax=ax)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-07-09T10:31:50.824752Z","iopub.execute_input":"2021-07-09T10:31:50.825218Z","iopub.status.idle":"2021-07-09T10:31:51.075725Z","shell.execute_reply.started":"2021-07-09T10:31:50.825188Z","shell.execute_reply":"2021-07-09T10:31:51.075097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Where here the `F score` is a measure \"*...based on the number of times a variable is selected for splitting, weighted by the squared improvement to the model as a result of each split, and averaged over all trees*.\" [1]\n### Links:\n* XGBoost: [documentation](https://xgboost.readthedocs.io/en/latest/index.html), [GitHub](https://github.com/dmlc/xgboost).\n* LightGBM: [documentation](https://lightgbm.readthedocs.io/en/latest/index.html), [GitHub](https://github.com/microsoft/LightGBM).\n* CatBoost: [documentation](https://catboost.ai/docs/), [GitHub](http://https://github.com/catboost).\n\n*See also*:\n\n* [Automatic tuning of XGBoost with XGBTune](https://www.kaggle.com/carlmcbrideellis/automatic-tuning-of-xgboost-with-xgbtune)\n* [GPU accelerated SHAP values with XGBoost](https://www.kaggle.com/carlmcbrideellis/gpu-accelerated-shap-values-jane-street-example)\n\n[1] [J. Elith, J. R. Leathwick, and T. Hastie \"*A working guide to boosted regression trees*\", Journal of Animal Ecology **77** pp. 802-813 (2008)](https://doi.org/10.1111/j.1365-2656.2008.01390.x)","metadata":{}}]}