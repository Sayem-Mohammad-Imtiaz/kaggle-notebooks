{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom IPython.display import Image as img\n\nimg(filename = '../input/imaages/1.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom scipy.stats import kstest\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Для виконання завдань курсової роботи був обраний датасет, що містить дані про витрати на різні етапи розвитку стартапа, та його чистий прибуток. На основі цих даних планується прогнозувати прибуток, в залежності від деякого набору параметрів, тому задача зводиться до задачі **регресії**."},{"metadata":{},"cell_type":"markdown","source":"Датасет для подальшого опрацювання виглядає наступним чином:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/startup-logistic-regression/50_Startups.csv')\ndata.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Одразу можна помітити, що категорії State потребують переведення у числовий формат, оскільки використовувати текстові дані для навчання моделі неможливо. Кодування текстових категорій відбувається за допомогою **LabelEncoder()**.\n\n[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html](http://)"},{"metadata":{},"cell_type":"markdown","source":"Датасет після викорастання **LabelEncoder()**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['State'] = LabelEncoder().fit_transform(data['State'])\ndata.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1.\tПроаналізувати набір даних на наявність пропущених значень та NaN. У випадку наявності таких значень замінити їх за допомогою методу ковзного вікна."},{"metadata":{},"cell_type":"markdown","source":"Для виявлення пропущених значень та NaN використовується метод **isna()**, який повертає датафрейм тієї ж розмірності, але кожне значення замінюється на True, якщо на цьому місці стоїть NaN або значення відсутнє, та False в інших випадках.\n\n[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isna.html](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data.isna())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Як видно з графіка, всі значення дорівнюють 0(False), отже пропущених значень та NaN у датасеті немає, тому додаткова обробка на цьому етапі непотрібна."},{"metadata":{},"cell_type":"markdown","source":"# 2. Провести візуалізацію даних. Побудувати такі типи графіків: Line Plot, Bar Chart, Histogram, Heatmap. При цьому для візуалізації багатовимірних даних з кількістю ознак більше двох обов’язково необхідно за допомогою subplot збудувати 9 графіків за допомогою комбінування різних ознак."},{"metadata":{},"cell_type":"markdown","source":"Графіки **Line Plot** використовуються для первинного відстеження залежностей між всіма можливими парами (вхід, вихід):"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (20, 6))\n\nax_1 = fig.add_subplot(2, 2, 1)\nax_2 = fig.add_subplot(2, 2, 2)\nax_3 = fig.add_subplot(2, 2, 3)\nax_4 = fig.add_subplot(2, 2, 4)\n\nsns.lineplot(data = data, x = \"R&D Spend\", y = \"Profit\", ax = ax_1)\nsns.lineplot(data = data, x = \"Administration\", y = \"Profit\", ax = ax_2)\nsns.lineplot(data = data, x = \"Marketing Spend\", y = \"Profit\", ax = ax_3)\nsns.lineplot(data = data, x = \"State\", y = \"Profit\", ax = ax_4)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Аналізуючи перший графік можна помітити майже лінійну залежність між витратами на науково-дослідну роботу та прибутком, менше помітна, але все-таки залежність прослідковується між прибутком та витратами на маркетинг."},{"metadata":{},"cell_type":"markdown","source":"На графіках **Bar Chart** зручно аналізувати числові характеристики залежностей:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (20, 6))\n\nax_1 = fig.add_subplot(2, 2, 1)\nax_2 = fig.add_subplot(2, 2, 2)\nax_3 = fig.add_subplot(2, 2, 3)\nax_4 = fig.add_subplot(2, 2, 4)\n\nsns.barplot(data = data, y = \"Profit\", x = \"R&D Spend\", ax = ax_1)\nsns.barplot(data = data, y = \"Profit\", x = \"Administration\", ax = ax_2)\nsns.barplot(data = data, y = \"Profit\", x = \"Marketing Spend\", ax = ax_3)\nsns.barplot(data = data, y = \"Profit\", x = \"State\", ax = ax_4)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Використовуючи графік **pairplot** можна візуалізувати відношення між усіма можливими парами змінних:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data, hue = 'State')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"За допомогою метода **corr()** та графіка **heatmap** можна переконатися у правдивості знайдених раніше залежностей та оцінити кореляцію між всіма парами параметрів :"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data = data.corr(), linewidths = .5, annot = True, cmap = \"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Числові значення цієї матриці - коефіцієнти кореляції Пірсона.\n\nКоефіцієнт кореляції Пірсона - це коефіцієнт лінійної залежності між двома змінними, що набуває значення від -1 до 1. При цьому при від’ємному значенні коефіцієнту Пірсона ми говоримо про зворотну залежність, а при низьких по модулю значеннях ми говоримо про слабку або відсутню залежність.","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/2.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Як і очікувалось, R&D Spend та Marketing Spend мають високу кореляцію з Profit."},{"metadata":{},"cell_type":"markdown","source":"# 3.Вирахувати математичне сподівання  та дисперсію σ для набору даних. Стандартизувати дані."},{"metadata":{},"cell_type":"markdown","source":"Стандартизація даних[1] відбувається за наступною формулою:","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/3.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = ['R&D Spend', 'Administration', 'Marketing Spend', 'State', 'Profit']\n\nfor i in titles:\n    print(f'\\nматематичне сподівання {i} =', round(data[f'{i}'].mean(), 3))\n    print(f'дисперсія σ {i} =', round(data[f'{i}'].std(), 3))\n\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Програмно дані стандартизуються за допомогою метода **StandardScaler()**:\n\n[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.DataFrame(StandardScaler().fit_transform(data)).rename(columns = \n    {0: titles[0], 1: titles[1], 2: titles[2], 3: titles[3], 4: titles[4]})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Датасет після стандартизації:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Математичне сподівання та дисперсія σ даних для перевірки успішності процесу стандартизації:"},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = ['R&D Spend', 'Administration', 'Marketing Spend', 'State', 'Profit']\n\nfor i in titles:\n    print(f'\\nматематичне сподівання {i} =', abs(round(data[f'{i}'].mean(), 3)))\n    print(f'дисперсія σ {i} =', round(data[f'{i}'].std(), 3))\n\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.\tЗа допомогою Random Forest моделі визначити інформативність признаків."},{"metadata":{},"cell_type":"markdown","source":"Використовуючи модель Random Forest, можна виміряти інформативність ознаки[2]. Для цього необхідно навчити модель на вибірці і під час побудови моделі для кожного елемента навчальної вибірки порахувати out-of-bag-помилку за наступною формулою :","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/4.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Потім для кожного об'єкта така помилка усереднюється по всьому випадковому лісу. Щоб оцінити інформативність ознаки, її значення перемішуються для всіх об'єктів навчальної вибірки і out-of-bag-помилка рахується знову. Важливість ознаки оцінюється шляхом усереднення по всіх деревах різниці показників out-of-bag-помилок до і після перемішування значень.\n\nПрограмно інформативність ознак визначається за допомогою метода **feature_importances** класа **RandomForestRegressor**:\n\n[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop('Profit', axis = 1)\nY = data['Profit']\n\nforest = RandomForestRegressor().fit(X, Y)\nimportances = forest.feature_importances_\n\ntitles = ['R&D Spend', 'Administration', 'Marketing Spend', 'State']\nfor i in range(len(titles)):\n    print(round(importances[i], 5), f'- інформативність {titles[i]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Зменшити розмірність даних за допомогою методу Principal Components Analysis (PCA) до двох вимірів та провести візуалізацію, що вимагається в пункті 2."},{"metadata":{},"cell_type":"markdown","source":"**Аналіз головних компонент (pгincipal component analysis, РСА)**([1], [3]) - це метод лінійного перетворення, що належить до типу навчання без учителя, який широко використовується в самих різних областях, найчастіше для зниження розмірності. **РСА** допомагає ідентифікувати повторювані образи в даних, ґрунтуючись на кореляції між ознаками. Якщо коротко, то **РСА** знаходить напрямки з максимальною дисперсією в багатовимірних даних і проектує їх на новий підпростір з меншим числом розмірностей, ніж вихідне. Ортогональні осі нового підпростору можна інтерпретувати як напрямки максимальної дисперсії в умовах обмеження, що осі нових признаків ортогональні.\n\nСпочатку для стандартизованого d-вимірного набору даних вираховуєтсья коваріаційна матриця. Після цього відбувається розклад матриці на власні вектори та власні числа. Наступним кроком є вибір k власних векторів які відповідають k найбільшим власним значенням, де k - кількість вимірів у новому просторі даних. Оскільки власні числа відповідають довжині власних векторів, вибравши найбільші в якості осей для нового простору, ми залишаємо найбільшу частину дисперсії, що в свою чергу зменшує кількість втраченої інформації. Останнім кроком є створення проекційної матриці W, із k найбільших векторів та перепроектування d-вимірного набору даних у k-вимірний.\n\n[https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_PCA_pl = PCA(n_components = 2).fit_transform(X)\nPCA_data_pl = pd.DataFrame(data = X_PCA_pl, columns = ['PC1', 'PC2'])\nPCA_data_pl['Profit'] = data['Profit']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Датасет після зменешення розмірності даних:"},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_data_pl.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Візуалізація даних:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (20, 6))\n\nax_1 = fig.add_subplot(1, 2, 1)\nax_2 = fig.add_subplot(1, 2, 2)\n\nsns.lineplot(data = PCA_data_pl, x = \"PC1\", y = \"Profit\", ax = ax_1)\nsns.lineplot(data = PCA_data_pl, x = \"PC2\", y = \"Profit\", ax = ax_2)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (20, 6))\n\nax_1 = fig.add_subplot(1, 2, 1)\nax_2 = fig.add_subplot(1, 2, 2)\n\nsns.barplot(data = PCA_data_pl, y = \"Profit\", x = \"PC1\", ax = ax_1)\nsns.barplot(data = PCA_data_pl, y = \"Profit\", x = \"PC2\", ax = ax_2)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(PCA_data_pl)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data = PCA_data_pl.corr(), linewidths = .5, annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Вирахувати відсотки дисперсії для власних векторів та визначити скільки потрібно залишити власних векторів, щоб залишилося більше 90% дисперсії."},{"metadata":{},"cell_type":"markdown","source":"Для визначення відсотків дисперсії власних векторів, метод **PCA** використовується для датасету без вказання кількості компонент:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components = None)\n_ = pca.fit(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Власні числа коваріаційної матриці можна дізнатися за дпомогою методу **explained__variance__**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.explained_variance_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Відсотки дисперсії для власних векторів визначаються методом **explained__variance__ratio__**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Тому, для збереження > 90% дисперсії необхідно залишити 3 власних вектора."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_PCA = PCA(n_components = 3).fit_transform(X)\nPCA_data = pd.DataFrame(data = X_PCA, columns = ['PC1', 'PC2', 'PC3'])\nPCA_data['Profit'] = data['Profit']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"датасет після зменешення розмірності даних до 3:"},{"metadata":{"trusted":true},"cell_type":"code","source":"PCA_data.head(6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.\tОцінити залежність між набором даних та набором предикторів за допомогою коефіцієнту Пірсона."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(data = PCA_data.corr(), linewidths = .5, annot = True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Значення коефіцієнтів Пірсона зменшились в порівнянні з початковими даними, шо не дивно, адже початкова розмірність даних невелика, тому для подальшої роботи залишається стандартизований датасет без використання **РСА**."},{"metadata":{},"cell_type":"markdown","source":"# 7.\tВикористати правило 68-95-99.7 для набору даних, визначити чи розподіл даних є подібним до нормального розподілу. Для цього вирахувати:\n# a.\tPr( µ-1σ ≤ X ≤ µ+1σ)\n# b.\tPr( µ-2σ ≤ X ≤ µ+2σ)\n# c.\tPr( µ-3σ ≤ X ≤ µ+3σ)"},{"metadata":{},"cell_type":"markdown","source":"При вимірюванні випадкова величина X з математичним сподіванням µ та дисперсією σ приймає випадкові значення. Для випадкової величини з нормальним розподілом діє правило[4]:","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/5.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Отже за допомогою цього правила можна знаходити викиди, як всі елементи, що не належать інтервалу  µ-3σ; µ+3σ."},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = ['PC1', 'PC2', 'PC3', 'Profit']\n\nfor i in titles:\n    m = PCA_data[f'{i}'].mean()\n    s = PCA_data[f'{i}'].std()\n    val = [0, 0, 0]\n    for j in PCA_data[f'{i}'].values:\n        if j > m - s and j < m + s:\n            val[0] += 1\n        if j > m - 2 * s and j < m + 2 * s:\n            val[1] += 1\n        if j > m - 3 * s and j < m + 3 * s:    \n            val[2] += 1\n    print(f'\\nPr⁡(µ - 1σ ≤ {i} ≤ µ + 1σ) =', val[0] / 50)\n    print(f'Pr⁡(µ - 2σ ≤ {i} ≤ µ + 2σ) =', val[1] / 50)\n    print(f'Pr⁡(µ - 3σ ≤ {i} ≤ µ + 3σ) =', val[2] / 50)\n\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8.Видалити з набору даних всі дані, що не належать до інтервалу  µ-3σ; µ+3σ"},{"metadata":{},"cell_type":"markdown","source":"Для даного датасета, в кожному наборі всі дані знаходяться в межах інтервалу  µ-3σ; µ+3σ, тому додаткова обробка даних на цьому етапі непотрібна. "},{"metadata":{},"cell_type":"markdown","source":"# 9.\tПровести тест Колмогорова-Смірнова."},{"metadata":{},"cell_type":"markdown","source":"Критерій узгодженості Колмогорова-Смірнова призначений для перевірки гіпотези про приналежність вибірки деякому закону розподілу, тобто перевірки того, що емпіричний розподіл відповідає передбачуваної моделі. Тест Колмогорова-Смiрнова[5] показує наскiльки значущими є вiдмiнностi мiж двома розподiлами випадкових величин. В нашому випадку, розподiл ознаки порівнюється з нормальним розподiлом. Для цього вiдбувається пiдрахунок статистики Колмогорова-Смiрнова:","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/6.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Для достатньо великого датасету гiпотеза відкидається на рівні значущості α при умові:","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/7.png')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Тест Колмогорова-Смірнова проводиться за допомогою функції **kstest**:\n\n[https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_test_result(statistic, distribution = 'norm', alpha = 0.05):\n    critical_value = (1 / np.sqrt(50)) * np.sqrt(-np.log(alpha / 2))\n    if statistic < critical_value:\n        return f'гіпотеза про належність виборки до розподілу {distribution} приймається на рівні значущості α = {alpha}'\n    else:\n        return f'гіпотеза про належність виборки до розподілу {distribution} відкидається на рівні значущості α = {alpha}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = kstest(PCA_data['PC1'], 'norm').statistic\nprint('D =', res)\nprint(check_test_result(res))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = kstest(PCA_data['PC2'], 'norm').statistic\nprint('D =', res)\nprint(check_test_result(res))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = kstest(PCA_data['PC3'], 'norm').statistic\nprint('D =', res)\nprint(check_test_result(res))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = kstest(PCA_data['Profit'], 'norm').statistic\nprint('D =', res)\nprint(check_test_result(res))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Реалізувати та навчити різні моделі з різною архітектурою для розв’язання поставленої задачі регресії. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.5, random_state = 13)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Для розв’язання задачі використовуються всі моделі, які були розглянуті протягом семестру, при чому для наглядності та об'єктивності, їх параметри, за винятком можливих непередбачуваних ситуацій, встановлені за замовчуванням. \n\nДля оцінки моделей використовується коефіцієнт детермінації - статистичний показник, що використовується в статистичних моделях як міра залежності варіації залежної змінної від варіації незалежних змінних. Вказує наскільки отримані спостереження підтверджують модель. Коефіцієнт детермінації визначається наступним чином:","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/8.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_titles = ['Lasso', 'Ridge', 'Elnet', 'KNN', 'MLP', 'SVR', 'RanFor']\ntrain_error = []\ntest_error = []","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Метод регресії «Lasso» полягає у введенні додаткової складової регулювання в функціонал оптимізації моделі, що часто дозволяє отримувати більш стійке рішення. Умова мінімізації квадратів помилки при оцінці параметрів β виражається наступною формулою:\n\n[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html](http://)","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/9.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_regression = Lasso()\n\nlasso_regression.fit(X_train, Y_train)\nlasso_result = lasso_regression.predict(X_test)\n\ntrain_err = 1 - lasso_regression.score(X_train, Y_train)\ntest_err = 1 - lasso_regression.score(X_test, Y_test)\n\ntrain_error.append(train_err)\ntest_error.append(test_err)\n\nprint('train error =', train_err, '\\ntest error =', test_err)\nprint('RMSE error =', mean_squared_error(Y_test, lasso_regression.predict(X_test), squared = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Метод регресії «Ridge» відрізняється від моделі Lasso тільки експонентою при параметрі регуляризації. Умова мінімізації квадратів помилки при оцінці параметрів β виражається наступною формулою:\n\n[https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html](http://)","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/10.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_regression = Ridge()\n\nridge_regression.fit(X_train, Y_train)\nridge_result = ridge_regression.predict(X_test)\n\ntrain_err = 1 - ridge_regression.score(X_train, Y_train)\ntest_err = 1 - ridge_regression.score(X_test, Y_test)\n\ntrain_error.append(train_err)\ntest_error.append(test_err)\n\nprint('train error =', train_err, '\\ntest error =', test_err)\nprint('RMSE error =', mean_squared_error(Y_test, ridge_regression.predict(X_test), squared = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Метод регресії «Elastic Net» являється узагальненням регресії з регуляризацією, вона об'єднує Ridge регресію (при λ2 = 0) і регресію Lasso (при λ1 = 0). Умова мінімізації квадратів помилки при оцінці параметрів β виражається наступною формулою:\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/11.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elnet_regression = ElasticNet()\n\nelnet_regression.fit(X_train, Y_train)\nelnet_result = elnet_regression.predict(X_test)\n\ntrain_err = 1 - elnet_regression.score(X_train, Y_train)\ntest_err = 1 - elnet_regression.score(X_test, Y_test)\n\ntrain_error.append(train_err)\ntest_error.append(test_err)\n\nprint('train error =', train_err, '\\ntest error =', test_err)\nprint('RMSE error =', mean_squared_error(Y_test, elnet_regression.predict(X_test), squared = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ключова ідея методу, що лежить в основі «KNN», полягає в формулюванні моделі в термінах евклідових відстаней в вихідному багатовимірному просторі ознак. Завдання полягає в тому, щоб для кожної тестованої точки знайти такий δ-окіл, щоб в ній помістилося k точок з відомими значеннями y. Тоді прогноз можна отримати, усереднюючи значення всіх навчальних спостережень з δ.\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_regression = KNeighborsRegressor()\n\nknn_regression.fit(X_train, Y_train)\nknn_result = knn_regression.predict(X_test)\n\ntrain_err = 1 - knn_regression.score(X_train, Y_train)\ntest_err = 1 - knn_regression.score(X_test, Y_test)\n\ntrain_error.append(train_err)\ntest_error.append(test_err)\n\nprint('train error =', train_err, '\\ntest error =', test_err)\nprint('RMSE error =', mean_squared_error(Y_test, knn_regression.predict(X_test), squared = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Багатошаровий персептрон (MLP) - це алгоритм навчання з учителем, складається щонайменше з трьох шарів: вхідного шару, прихованого шару та вихідного шару. За винятком вхідних вузлів, кожен вузол є нейроном, який використовує нелінійну функцію активації. MLP використовує контрольовану техніку навчання, яка називається зворотним поширенням помилки для навчання. Його багатошаровість і нелінійна активація дозволяють розрізняти дані, які не є лінійно роздільними.\n\n[https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html](http://)","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/12.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mlp_regression = MLPRegressor()\n\nmlp_regression.fit(X_train, Y_train)\nmlp_result = mlp_regression.predict(X_test)\n\ntrain_err = 1 - mlp_regression.score(X_train, Y_train)\ntest_err = 1 - mlp_regression.score(X_test, Y_test)\n\ntrain_error.append(train_err)\ntest_error.append(test_err)\n\nprint('train error =', train_err, '\\ntest error =', test_err)\nprint('RMSE error =', mean_squared_error(Y_test, mlp_regression.predict(X_test), squared = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Метод «SVM» - алгоритм навчання з учителем. Основна ідея методу - переведення вихідних векторів у простір більш високої розмірності і пошук розділяючої гіперплощини з максимальним зазором у цьому просторі. Дві паралельні гіперплощини будуються по обі сторони від гіперплощини. Гіперплощина, що розділяє, максимізує відстань до двох паралельних гіперплощин. Чим більше розбіжність або відстань між цими паралельними площинами - тим менше буде середня помилка.\n\n[https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html](http://)","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/13.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_regression = SVR()\n\nsvm_regression.fit(X_train, Y_train)\nsvm_result = svm_regression.predict(X_test)\n\ntrain_err = 1 - svm_regression.score(X_train, Y_train)\ntest_err = 1 - svm_regression.score(X_test, Y_test)\n\ntrain_error.append(train_err)\ntest_error.append(test_err)\n\nprint('train error =', train_err, '\\ntest error =', test_err)\nprint('RMSE error =', mean_squared_error(Y_test, svm_regression.predict(X_test), squared = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random forest - алгоритм машинного навчання, основна ідея полягає в використанні великого ансамблю дерев рішень, кожне з яких саме по собі дає не дуже високу якість передбачення, але за рахунок їх великої кількості результат виходить хорошим. Дерево рішень будує регресійні моделі у вигляді деревної структури. Він розбиває набір даних на все менші та менші підмножини, в той час, як пов'язане дерево рішень поступово розробляється. Кінцевим результатом є дерево з вузлами прийняття рішень та вузлами листів. Вузол прийняття рішень має дві або більше гілок, кожна з яких представляє значення для перевіреного атрибута. Вузол листа представляє рішення щодо числової цілі. Вузол найвищого рішення у дереві, який відповідає найкращому предиктору, називається кореневим вузлом.\n\n[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html](http://)","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"img(filename = '../input/imaages/14.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest_regression = RandomForestRegressor()\n\nforest_regression.fit(X_train, Y_train)\nforest_result = forest_regression.predict(X_test)\n\ntrain_err = 1 - forest_regression.score(X_train, Y_train)\ntest_err = 1 - forest_regression.score(X_test, Y_test)\n\ntrain_error.append(train_err)\ntest_error.append(test_err)\n\nprint('train error =', train_err, '\\ntest error =', test_err)\nprint('RMSE error =', mean_squared_error(Y_test, forest_regression.predict(X_test), squared = False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data = pd.DataFrame(data = {'name': model_titles, 'train error': train_error, 'test error': test_error})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Моделі та їх результати представлені у наступному датасеті:"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data.head(7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# На основі аналізу їх результатів вибрати найкращі моделі та ансамблювати їх для отримання більш стійкого результату."},{"metadata":{},"cell_type":"markdown","source":"Для подальшого використання залишаються тільки моделі з помилкою на основі коефіцієнта детермінації на тестових даних < 0.1 :"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data.drop(model_data[model_data['test error'] >= 0.1].index, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Отже, найкращі моделі для розв'язання даної задачі - **Ridge**, **MLPRegressor** та **RandomForestRegressor**.\n\nАнсамблювання моделей використовуючи **VotingRegressor()**:\n\n[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = VotingRegressor([('Ridge', ridge_regression), ('MLP', mlp_regression), ('RanFor', forest_regression)])\n\nensemble.fit(X_train, Y_train)\nensemble_result = ensemble.predict(X_test)\n\ntrain_err = 1 - ensemble.score(X_train, Y_train)\ntest_err = 1 - ensemble.score(X_test, Y_test)\n\nprint('train error =', train_err, '\\ntest error =', test_err)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(ridge_result, 'gd', label = 'Ridge')\nplt.plot(mlp_result, 'b^', label = 'MLP')\nplt.plot(forest_result, 'ys', label = 'RanFor')\nplt.plot(ensemble_result, 'r*', ms = 10, label = 'VotingRegressor')\n\nplt.tick_params(axis = 'x', which = 'both', bottom = False, top = False, labelbottom = False)\nplt.ylabel('прогноз')\nplt.xlabel('тестова вибірка')\nplt.legend(loc = \"best\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Аналіз результатів"},{"metadata":{},"cell_type":"markdown","source":"# 1.\tВплив відновлення пропущених значень порівняно з видаленням їх з вибірки."},{"metadata":{},"cell_type":"markdown","source":"Вибраний датасет не містив пропущені значення або NaN, але під час виконання лабораторних робіт практично було встановлено що краще відновити пропущені значення, аніж видаляти їх."},{"metadata":{},"cell_type":"markdown","source":"# 2.\tАналіз побудованих графіків та гістограм для різних ознак."},{"metadata":{},"cell_type":"markdown","source":"З Linear- та Bar-plots видно, що R&D Spend та Marketing є більш ваговими ознаками ніж Administration та State, а на графіку pairplot можна помітити що Profit залежить від R&D Spend майже лінійно, що підтверджується кореляційною матрицею."},{"metadata":{},"cell_type":"markdown","source":"# 3.\tАналіз отриманих даних після застосування алгоритму зменшення розмірності."},{"metadata":{},"cell_type":"markdown","source":"Після зменшення розмірності загальні тенденції не змінилися, PC1 візуально складає з Profit таку ж майже лінійну залежність, як і R&D Spend до застосування алгоритма РСА."},{"metadata":{},"cell_type":"markdown","source":"# 4.\tАналіз впливу зменшення розмірності даних на точність моделі."},{"metadata":{},"cell_type":"markdown","source":"Зменшення розмірності негативно вплинуло на точність всіх моделей, похибка на навчальних та тестових вибірках зросла майже на 20%, тому моделі навчалися на повних, але стандартизованих даних."},{"metadata":{},"cell_type":"markdown","source":"# 5.\tАналіз залежності між предикторами та досліджуваною величиною."},{"metadata":{},"cell_type":"markdown","source":"Кореляційна матриця повністю підтверджує результати обробки даних, отриманих на етапі візуалізації: коефіцієнт Пірсона між Profit та R&D Spend і Marketing становить відповідно 0.97 та 0.75, що свідчить про високу кореляцію, коефіцієнт Пірсона між Profit та Administration і State становить відповідно 0.2 та 0.1, що свідчить про низьку кореляцію."},{"metadata":{},"cell_type":"markdown","source":"# 6.\tАналіз розподілу вхідних даних, аналіз викидів, після переведення його до нормального, за правилом 68-95-99.7"},{"metadata":{},"cell_type":"markdown","source":"Розподіл початкових вхідних даних не схожий на типові статистичні розподіли, після стандартизації даних, розподілом став стандартно-нормальний, після використання правила 3-сігма не було визначено та вилучено жодного викида, тому що значення всіх признаків знаходяться в межах µ - 3σ ; µ + 3σ."},{"metadata":{},"cell_type":"markdown","source":"# 7.\tАналіз результатів тесту Колмогорова-Смірнова."},{"metadata":{},"cell_type":"markdown","source":"Тест Колмогорова-Смірнова для кожного набору даних, як і очікувалось, підтвердив гіпотезу про приналежність даних до стандартно-нормального розподілу на рівні значущості 0.05."},{"metadata":{},"cell_type":"markdown","source":"# 8.\tЯк підрахунок різних точностей описує поведінку навчених моделей."},{"metadata":{},"cell_type":"markdown","source":"Для оцінювання точності моделей використовувались коефіцієнт детермінації та cередньоквадратична похибка, в процентному співвідношенні для кожної моделі відношення оцінок знаходяться на приблизно однакових рівнях,що дозволяє зробити висновок при прийнятність обох методів оцінки."},{"metadata":{},"cell_type":"markdown","source":"# 9.\tВибір оптимальних параметрів моделі, їх обґрунтування."},{"metadata":{},"cell_type":"markdown","source":"Для об'єктивності тестування моделей всі без винятку параметри були встановлені за замовчуванням, але для даної задачі ці параметри майже завжди і є оптимальними."},{"metadata":{},"cell_type":"markdown","source":"# 10.\tОцінка помилок на начальній та тестовій вибірках."},{"metadata":{},"cell_type":"markdown","source":"Похибка на навчальній та тестовій виборках виводиться під кожною нодою, що містить модель."},{"metadata":{},"cell_type":"markdown","source":"# 11.\tВибір найкращих моделей для ансамблювання на основі їх точностей та вибір всіх моделей для ансамблювання."},{"metadata":{},"cell_type":"markdown","source":"Після аналізу результатів в якості найкращих моделей були обрані Ridge, MLPRegressor та RandomForestRegressor. Їх ансамблювання дозволило отримати більш стійкий та зважений результат, який виявився кращим ніж точність всіх моделей поодинці. Ансамблювання всіх моделей, на нашу думку, не має сенсу, в умовах тих результатів, які ми отримали."},{"metadata":{},"cell_type":"markdown","source":"# 12.\tПорівняння точності ансамблю та найкращої моделі."},{"metadata":{},"cell_type":"markdown","source":"Точність найкращої моделі, Ridge:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(1 - model_data.iloc[0]['test error'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Точність ансамблю:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ensemble.score(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Отже, різниця в точності складає:"},{"metadata":{"trusted":true},"cell_type":"code","source":"diff = ensemble.score(X_test, Y_test) + model_data.iloc[0]['test error'] - 1\nprint(diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Різниця в точності у відсотках:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(round(100 * diff / model_data.iloc[0]['test error']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Висновки"},{"metadata":{},"cell_type":"markdown","source":"Отже, під час виконання курсової роботи, ми навчилися знаходити й позбуватися від пропущених значень в даних, візуалізувати та знаходити залежності даних. Також розібрались в одному з можливих способів визначання інформативності признаків, за допомогою моделі Random Forest, та у методі пониження розмірності даних - PCA. Навчились оцінювати залежності між набором даних та набором предикторів використовуючи коефіцієнт Пірсона, а також проводити тест Колмогорова-Смірнова. Пересвідчились в коректності й використали правило 68-95-99.7.\n\nМи провели значний аналіз всіх результатів, тим самим навчилися виявляти важливі фактори, які впливають на точність результату, та навпаки\nфактори які є негативними для нас. Ці, і не тільки ці навички, які ми здобули виконуючи курсовую роботу, допомогли нам глибше розібратися в актуальних методах машинного навчання, а також в ефективному аналізу даних, що є цінним досвідом для майбутніх проєктів."},{"metadata":{},"cell_type":"markdown","source":"# Література"},{"metadata":{},"cell_type":"markdown","source":"1. Себастьян Рашка - «Python и машинное обучение»\n1. Рысьмятова Анастасия - «Методы отбора признаков»\n1. Ian T. Jolliffe and Jorge Cadima - «Principal component analysis: a review and recent developments»\n1. Michael Galarnyk - «Explaining the 68-95-99.7 rule for a Normal Distribution»\n1. Queen Mary's School of Mathematical Sciences - «Kolmogorov-Smirnov Tests»"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}