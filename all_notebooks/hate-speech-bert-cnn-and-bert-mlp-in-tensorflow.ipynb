{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Implementation of text classification with BERT \n\nStill Working on it. Code needs to be refactored!\n\nThis notebook is based in this TensorFlow tutorial: [Classify text with BERT](https://www.tensorflow.org/tutorials/text/classify_text_with_bert)\n\nBERT [(article link)](https://arxiv.org/abs/1810.04805) and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.\n\n![](http://www.d2l.ai/_images/nlp-map-pretrain.svg)\n\nSource: http://www.d2l.ai/chapter_natural-language-processing-pretraining/index.html\n\nBERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n\nIn this notebook, I am going to use a pretreined BERT to compute vector-space representations of a hate speech dataset to feed two different downsteam Archtectures (CNN and MLP).\n\nSentiment Analysis\n\nThis notebook trains a sentiment analysis model to classify the [Hate Speech and Offensive Language Dataset]( https://www.kaggle.com/mrmorj/hate-speech-and-offensive-language-dataset) tweets in three classes:\n \n* 0 - hate speech \n* 1 - offensive language \n* 2 - neither as positive or negative","metadata":{}},{"cell_type":"markdown","source":"## Installing dependencies and importing packages","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install --upgrade pip\n#!python -m pip install --upgrade pip\n#!pip install example --use-feature=2020-resolver\n!/opt/conda/bin/python3.7 -m pip install --upgrade pip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A dependency of the preprocessing for BERT inputs\n!pip install -q tensorflow-text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q tf-models-official","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.core.display import HTML\nHTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport tensorflow.keras.backend as K\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nfrom transformers import *\n\nnp.set_printoptions(suppress=True)\nprint(tf.__version__)\n\nimport shutil\n\nfrom official.nlp import optimization  # to create AdamW optmizer\n\ntf.get_logger().setLevel('ERROR')\n\n#physical_devices = tf.config.list_physical_devices('GPU') \n#tf.config.experimental.set_memory_growth(physical_devices[0], True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading and preparing the dataset","metadata":{}},{"cell_type":"code","source":"PATH = '../input/hate-speech-and-offensive-language-dataset/'\ndf = pd.read_csv(PATH+'labeled_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nRowsRead = None # specify 'None' if want to read whole file\n# labeled_data.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndf0 = pd.read_csv('../input/hate-speech-and-offensive-language-dataset/labeled_data.csv', delimiter=',', nrows = nRowsRead)\ndf0.dataframeName = 'labeled_data.csv'\nnRow, nCol = df0.shape\nprint('There are {} rows and {} columns'.format(nRow, nCol))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df0.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Doing some adjustments\n\nc=df0['class']\ndf0.rename(columns={'tweet' : 'text',\n                   'class' : 'category'}, \n                    inplace=True)\na=df0['text']\nb=df0['category'].map({0: 'hate_speech', 1: 'offensive_language',2: 'neither'})\n\ndf= pd.concat([a,b,c], axis=1)\ndf.rename(columns={'class' : 'label'}, \n                    inplace=True)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grouping data by label\ndf.groupby('label').count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is an unbalanced dataset. ","metadata":{}},{"cell_type":"code","source":"hate, ofensive, neither = np.bincount(df['label'])\ntotal = hate + ofensive + neither\nprint('Examples:\\n    Total: {}\\n    hate: {} ({:.2f}% of total)\\n'.format(\n    total, hate, 100 * hate / total))\nprint('Examples:\\n    Total: {}\\n    Ofensive: {} ({:.2f}% of total)\\n'.format(\n    total, ofensive, 100 * ofensive / total))\nprint('Examples:\\n    Total: {}\\n    Neither: {} ({:.2f}% of total)\\n'.format(\n    total, neither, 100 * neither / total))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting the data between train, validation and test sets:","metadata":{}},{"cell_type":"code","source":"X_train_, X_test, y_train_, y_test = train_test_split(\n    df.index.values,\n    df.label.values,\n    test_size=0.10,\n    random_state=42,\n    stratify=df.label.values,    \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(\n    df.loc[X_train_].index.values,\n    df.loc[X_train_].label.values,\n    test_size=0.10,\n    random_state=42,\n    stratify=df.loc[X_train_].label.values,  \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['data_type'] = ['not_set']*df.shape[0]\ndf.loc[X_train, 'data_type'] = 'train'\ndf.loc[X_val, 'data_type'] = 'val'\ndf.loc[X_test, 'data_type'] = 'test'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(['category', 'label', 'data_type']).count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df.loc[df[\"data_type\"]==\"train\"]\ndf_train.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_val = df.loc[df[\"data_type\"]==\"val\"]\ndf_val.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df.loc[df[\"data_type\"]==\"test\"]\ndf_test.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Word Clouds for each class","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\nstopwords.add(\"RT\")\n\nprint(type(STOPWORDS))\n\nimport random\n\ndef random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n    h = 344\n    s = int(100.0 * 255.0 / 255.0)\n    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=60, \n                          random_state=42\n                         ).generate(str(df.loc[df[\"category\"]==\"offensive_language\"].text))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),\n           interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n    h = 20\n    s = int(100.0 * 255.0 / 255.0)\n    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=60, \n                          random_state=42\n                         ).generate(str((df.loc[df[\"category\"]==\"neither\"].text)))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),\n           interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords.add(\"Name\")\n\ndef random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n    h = 180\n    s = int(100.0 * 255.0 / 255.0)\n    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=60, \n                          random_state=42\n                         ).generate(str((df.loc[df[\"category\"]==\"hate_speech\"].text)))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),\n           interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build TensorFlow input \n[Reference](https://www.tensorflow.org/guide/data)","metadata":{}},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((df_train.text.values, df_train.label.values))\nval_ds = tf.data.Dataset.from_tensor_slices((df_val.text.values, df_val.label.values))\ntest_ds = tf.data.Dataset.from_tensor_slices((df_test.text.values, df_test.label.values))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While tf.data tries to propagate shape information, the default settings of Dataset.batch result in an unknown batch size because the last batch may not be full. Note the Nones in the shape:\n\nbatched_dataset\n```\n<BatchDataset shapes: ((None,), (None,)), types: (tf.int64, tf.int64)>\n```\nUse the drop_remainder argument to ignore that last batch, and get full shape propagation:","metadata":{}},{"cell_type":"code","source":"train_ds = train_ds.shuffle(len(df_train)).batch(32, drop_remainder=False)\ntrain_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_ds = val_ds.shuffle(len(df_val)).batch(32, drop_remainder=False)\nval_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = test_ds.shuffle(len(df_test)).batch(32, drop_remainder=False)\ntest_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Printing some Tweets","metadata":{}},{"cell_type":"code","source":"for feat, targ in train_ds.take(1):\n  print ('Features: {}, Target: {}'.format(feat, targ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading models from TensorFlow Hub","metadata":{}},{"cell_type":"code","source":"bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n#bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'\n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_base/2',\n    'electra_small':\n        'https://tfhub.dev/google/electra_small/2',\n    'electra_base':\n        'https://tfhub.dev/google/electra_base/2',\n    'experts_pubmed':\n        'https://tfhub.dev/google/experts/bert/pubmed/2',\n    'experts_wiki_books':\n        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/1',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/1',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/1',\n    'electra_small':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'electra_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'experts_pubmed':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'experts_wiki_books':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}')\nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I've chosen \"bert_en_uncased_L-12_H-768_A-12\"\n\nThis TF Hub model uses the implementation of BERT from the TensorFlow Models repository on GitHub at tensorflow/models/official/nlp/bert. It uses L=12 hidden layers (i.e., Transformer blocks), a hidden size of H=768, and A=12 attention heads.","metadata":{}},{"cell_type":"markdown","source":"### The preprocessing model\n\nText inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models, which implements this transformation using TF ops from the TF.text library. Hence, It is not necessary to run pure Python code outside the TensorFlow model to preprocess text.\n\nThe preprocessing model must be the one referenced by the documentation of the BERT model, which can be read at the URL printed above. For BERT models from the drop-down above, the preprocessing model is selected automatically.","metadata":{}},{"cell_type":"code","source":"bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try the preprocessing model on some text and see the output:","metadata":{}},{"cell_type":"code","source":"for text_batch, label_batch in train_ds.take(1):\n  for i in range(1):\n    tweet = text_batch.numpy()[i]\n    print(f'Tweet: {text_batch.numpy()[i]}')\n    label = label_batch.numpy()[i]\n    print(f'Label : {label}')\n\ntext_test = ['this is such an amazing movie!']\ntext_test = [tweet]\n\n\ntext_preprocessed = bert_preprocess_model(text_test)\n\nprint(f'Keys       : {list(text_preprocessed.keys())}')\nprint(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\nprint(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\nprint(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\nprint(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model = hub.KerasLayer(tfhub_handle_encoder)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_results = bert_model(text_preprocessed)\n\nprint(f'Loaded BERT: {tfhub_handle_encoder}')\nprint(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\nprint(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\nprint(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\nprint(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Techniques to deal with unbalanced data","metadata":{}},{"cell_type":"markdown","source":"### Calculate class weights\n\nOne of the goals is to identify hate speech, but we don't have very many of those samples to work with, so I would want to have the classifier heavily weight the few examples that are available. I am going to do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class.","metadata":{}},{"cell_type":"code","source":"weight_for_0 = (1 / hate)*(total)/3.0 \nweight_for_1 = (1 / ofensive)*(total)/3.0\nweight_for_2 = (1 / neither)*(total)/3.0\n\n\nclass_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))\nprint('Weight for class 2: {:.2f}'.format(weight_for_2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set the correct initial bias\n\nThese initial guesses (for the bias) are not great. The dataset is imbalanced. Set the output layer's bias to reflect that (See: [A Recipe for Training Neural Networks: \"init well\"](http://karpathy.github.io/2019/04/25/recipe/#2-set-up-the-end-to-end-trainingevaluation-skeleton--get-dumb-baselines)). This can help with initial convergence.\n\nWith the default bias initialization the loss should be about log(1/n_classes): math.log(3) = 1,098612","metadata":{}},{"cell_type":"code","source":"#initial_output_bias = np.array([3.938462, 6.535164, 5.])\ninitial_output_bias = np.array([3.938462, 15, 5.])\ninitial_output_bias ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT + MLP\n\nI am going to create a simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.","metadata":{}},{"cell_type":"code","source":"def build_classifier_model(output_bias=None):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n        #print(output_bias)\n        \n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dense(512, activation=\"relu\")(net)\n    net = tf.keras.layers.Dropout(0.2)(net)\n#   net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n    net = tf.keras.layers.Dense(3, activation=\"softmax\", name='classifier', bias_initializer=output_bias)(net)\n    \n    return tf.keras.Model(text_input, net)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_model = build_classifier_model(output_bias=initial_output_bias)\n\nbert_raw_result = classifier_model(tf.constant(text_test))\nprint(tf.sigmoid(bert_raw_result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_model.get_weights()[-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(classifier_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n#metrics = tf.metrics.Accuracy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 80\nsteps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  classifier_model.compile(optimizer=optimizer,\n#                           loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n#                           metrics=['accuracy'])\nclassifier_model.compile(optimizer=optimizer,\n                         loss=loss,\n                         metrics=metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Training model with {tfhub_handle_encoder}')\nhistory = classifier_model.fit(x=train_ds,\n                               validation_data=val_ds,\n                               epochs=epochs,\n                               # The class weights go here\n                               class_weight=class_weight\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = classifier_model.evaluate(test_ds)\n\nprint(f'Loss: {loss}')\nprint(f'Accuracy: {accuracy}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = history.history\nprint(history_dict.keys())\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n# acc = history_dict['binary_accuracy']\n# val_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(12, 10))\nfig.tight_layout()\n\nplt.subplot(2, 1, 1)\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'r', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\n# plt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Export for inference\n\nNow you just save your fine-tuned model for later use.","metadata":{}},{"cell_type":"code","source":"dataset_name = 'mpl_hate_speech'\nsaved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n\nclassifier_model.save(saved_model_path, include_optimizer=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results for MLP","metadata":{}},{"cell_type":"code","source":"result =  classifier_model.predict(test_ds)\nprint(result.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result[0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = np.argmax(result, axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Doing predictions and saving to np.array","metadata":{}},{"cell_type":"code","source":"tweet = []\ntest_labels = []\npredictions = []\nfor tweet, labels in test_ds.take(-1):\n  tweet = tweet.numpy()\n  test_labels.append(labels.numpy())\n  predictions.append(classifier_model.predict(tweet))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels[0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions[0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import chain\nflatten_list = list(chain.from_iterable(predictions))\ny_pred = np.argmax(flatten_list, axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = np.array(list(chain.from_iterable(test_labels)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix MLP","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test with Random Initialization of softmax bias.","metadata":{}},{"cell_type":"code","source":"# random_classifier_model = build_classifier_model()\n# bert_raw_result = random_classifier_model(tf.constant(text_test))\n# print(tf.sigmoid(bert_raw_result))\n\n# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n# metrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n# #metrics = tf.metrics.Accuracy()\n\n# epochs = 5\n# steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n# num_train_steps = steps_per_epoch * epochs\n# num_warmup_steps = int(0.1*num_train_steps)\n\n# init_lr = 3e-5\n# optimizer = optimization.create_optimizer(init_lr=init_lr,\n#                                           num_train_steps=num_train_steps,\n#                                           num_warmup_steps=num_warmup_steps,\n#                                           optimizer_type='adamw')\n\n# random_classifier_model.compile(optimizer=optimizer,\n#                          loss=loss,\n#                          metrics=metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(f'Training model with {tfhub_handle_encoder}')\n# random_history = random_classifier_model.fit(x=train_ds,\n#                                validation_data=val_ds,\n#                                epochs=epochs,\n#                                # The class weights go here\n#                                class_weight=class_weight\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history_dict = random_history.history\n# print(history_dict.keys())\n\n# acc = history_dict['accuracy']\n# val_acc = history_dict['val_accuracy']\n# # acc = history_dict['binary_accuracy']\n# # val_acc = history_dict['val_binary_accuracy']\n# loss = history_dict['loss']\n# val_loss = history_dict['val_loss']\n\n# epochs = range(1, len(acc) + 1)\n# fig = plt.figure(figsize=(10, 6))\n# fig.tight_layout()\n\n# plt.subplot(2, 1, 1)\n# # \"bo\" is for \"blue dot\"\n# plt.plot(epochs, loss, 'r', label='Training loss')\n# # b is for \"solid blue line\"\n# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n# plt.title('Training and validation loss')\n# # plt.xlabel('Epochs')\n# plt.ylabel('Loss')\n# plt.legend()\n\n# plt.subplot(2, 1, 2)\n# plt.plot(epochs, acc, 'r', label='Training acc')\n# plt.plot(epochs, val_acc, 'b', label='Validation acc')\n# plt.title('Training and validation accuracy')\n# plt.xlabel('Epochs')\n# plt.ylabel('Accuracy')\n# plt.legend(loc='lower right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT + CNN","metadata":{}},{"cell_type":"code","source":"def build_CNN_classifier_model():\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    #net = outputs['pooled_output'] # [batch_size, 768].\n    net = sequence_output = outputs[\"sequence_output\"] # [batch_size, seq_length, 768]\n      \n    \n    net = tf.keras.layers.Conv1D(32, (2), activation='relu')(net)\n    #net = tf.keras.layers.MaxPooling1D(2)(net)\n    \n    net = tf.keras.layers.Conv1D(64, (2), activation='relu')(net)\n    #net = tf.keras.layers.MaxPooling1D(2)(net)\n    net = tf.keras.layers.GlobalMaxPool1D()(net)\n    \n#    net = tf.keras.layers.Flatten()(net)\n    \n    net = tf.keras.layers.Dense(512, activation=\"relu\")(net)\n    \n    net = tf.keras.layers.Dropout(0.1)(net)\n#   net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n    net = tf.keras.layers.Dense(3, activation=\"softmax\", name='classifier')(net)\n    \n    return tf.keras.Model(text_input, net)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the implementation with CNN, I am using the sequence_output as input to the convolutional layer. It represents each input token in the context. The shape is [batch_size, seq_length, H]. You can think of this as a contextual embedding for every token in the tweet. I belive that this outputs saves positional information about the inputs, then it would male cense to feed a convolutional layer.","metadata":{}},{"cell_type":"code","source":"cnn_classifier_model = build_CNN_classifier_model()\nbert_raw_result = cnn_classifier_model(tf.constant(text_test))\nprint(tf.sigmoid(bert_raw_result))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnn_classifier_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(cnn_classifier_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n#metrics = tf.metrics.CategoricalCrossentropy()\n#metrics = tf.metrics.Accuracy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 80\nsteps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')\n\ncnn_classifier_model.compile(optimizer=optimizer,\n                          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                          metrics=tf.keras.metrics.SparseCategoricalAccuracy('accuracy'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Training model with {tfhub_handle_encoder}')\ncnn_history = cnn_classifier_model.fit(x=train_ds,\n                                       validation_data=val_ds,\n                                       epochs=epochs,\n                                       class_weight=class_weight\n                                      )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = cnn_classifier_model.evaluate(test_ds)\n\nprint(f'Loss: {loss}')\nprint(f'Accuracy: {accuracy}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dict = cnn_history.history\nprint(history_dict.keys())\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\n# acc = history_dict['binary_accuracy']\n# val_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(12, 10))\nfig.tight_layout()\n\nplt.subplot(2, 1, 1)\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'r', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\n# plt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Export for inference\n\nNow you just save your fine-tuned model for later use.","metadata":{}},{"cell_type":"code","source":"dataset_name = 'cnn_hate_speech'\nsaved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n\ncnn_classifier_model.save(saved_model_path, include_optimizer=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reloaded_model = tf.saved_model.load(saved_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results for CNN","metadata":{}},{"cell_type":"code","source":"result =  cnn_classifier_model.predict(test_ds)\nprint(result.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result[0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for tweet, classes in test_ds:\n#     for i in classes:\n#         print(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = np.argmax(result, axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Doing predictions and saving to np.array","metadata":{}},{"cell_type":"code","source":"tweet = []\ntest_labels = []\npredictions = []\nfor tweet, labels in test_ds.take(-1):\n  tweet = tweet.numpy()\n  test_labels.append(labels.numpy())\n  predictions.append(cnn_classifier_model.predict(tweet))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels[0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions[0:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flatten_list = list(chain.from_iterable(predictions))\ny_pred = np.argmax(flatten_list, axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = np.array(list(chain.from_iterable(test_labels)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Confusion Matrix CNN","metadata":{}},{"cell_type":"code","source":"confusion_matrix(y_test, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}