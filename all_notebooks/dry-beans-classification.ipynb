{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dry Beans Classification\nThe data comes from the analysis of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. A total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.\n\nThe original study is from KOKLU, M. and OZKAN, I.A., (2020), “Multiclass Classification of Dry Beans Using Computer Vision and Machine Learning Techniques.” Computers and Electronics in Agriculture, 174, 105507. DOI: https://doi.org/10.1016/j.compag.2020.105507. ","metadata":{}},{"cell_type":"markdown","source":"1. Main objective.\n2. Dataset description.\n3. Data exploration.\n4. Clustering models.\n5. Key findings.\n6. Future steps.","metadata":{"execution":{"iopub.status.busy":"2021-07-01T18:35:44.901367Z","iopub.execute_input":"2021-07-01T18:35:44.902019Z","iopub.status.idle":"2021-07-01T18:35:44.910569Z","shell.execute_reply.started":"2021-07-01T18:35:44.90192Z","shell.execute_reply":"2021-07-01T18:35:44.907586Z"}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 1. Main objective\n\nThe analysis intends to classify the beans in different clusters and see whether the clusters are similar to the original classes.","metadata":{}},{"cell_type":"markdown","source":"## 2. Dataset description\n\nThe dataset contains the following variables:\n\n1. Area (A): The area of a bean zone and the number of pixels within its boundaries.\n2. Perimeter (P): Bean circumference is defined as the length of its border.\n3. Major axis length (L): The distance between the ends of the longest line that can be drawn from a bean.\n4. Minor axis length (l): The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n5. Aspect ratio (K): Defines the relationship between L and l.\n6. Eccentricity (Ec): Eccentricity of the ellipse having the same moments as the region.\n7. Convex area (C): Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n8. Equivalent diameter (Ed): The diameter of a circle having the same area as a bean seed area.\n9. Extent (Ex): The ratio of the pixels in the bounding box to the bean area.\n10.Solidity (S): Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\n11.Roundness (R): Calculated with the following formula: (4piA)/(P^2)\n12.Compactness (CO): Measures the roundness of an object: Ed/L\n13.ShapeFactor1 (SF1)\n14.ShapeFactor2 (SF2)\n15.ShapeFactor3 (SF3)\n16.ShapeFactor4 (SF4)\n17.Class (Seker, Barbunya, Bombay, Cali, Dermosan, Horoz and Sira)","metadata":{}},{"cell_type":"markdown","source":"## 3. Data exploration","metadata":{}},{"cell_type":"markdown","source":"### 3.1. Importing data","metadata":{}},{"cell_type":"code","source":"# Import python packages to be used\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:70% !important; }</style>\"))\n\nimport os, seaborn as sns, pandas as pd, numpy as np, matplotlib.pyplot as plt\nfrom pprint import pprint","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:32.452535Z","iopub.execute_input":"2021-07-01T20:54:32.452922Z","iopub.status.idle":"2021-07-01T20:54:33.194965Z","shell.execute_reply.started":"2021-07-01T20:54:32.452867Z","shell.execute_reply":"2021-07-01T20:54:33.194051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the data\ndataset = pd.read_csv('../input/beans-classification/Dry_Bean_Dataset.csv')\ndataset","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:33.199324Z","iopub.execute_input":"2021-07-01T20:54:33.19957Z","iopub.status.idle":"2021-07-01T20:54:33.332117Z","shell.execute_reply.started":"2021-07-01T20:54:33.199544Z","shell.execute_reply":"2021-07-01T20:54:33.331203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2. Checking for ranges and invalid values","metadata":{}},{"cell_type":"code","source":"COLUMNS = dataset.columns.tolist()\nfor c in COLUMNS:\n    if dataset[c].isnull().values.any():\n        print('{0}: {1} invalid values found'.format(c, dataset[c].isnull().sum()))\n    else:\n        print('{0}: ok'.format(c))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:33.333452Z","iopub.execute_input":"2021-07-01T20:54:33.333701Z","iopub.status.idle":"2021-07-01T20:54:33.348242Z","shell.execute_reply.started":"2021-07-01T20:54:33.333676Z","shell.execute_reply":"2021-07-01T20:54:33.347152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:33.349569Z","iopub.execute_input":"2021-07-01T20:54:33.34983Z","iopub.status.idle":"2021-07-01T20:54:33.356858Z","shell.execute_reply.started":"2021-07-01T20:54:33.349803Z","shell.execute_reply":"2021-07-01T20:54:33.355964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:33.358166Z","iopub.execute_input":"2021-07-01T20:54:33.358476Z","iopub.status.idle":"2021-07-01T20:54:33.426764Z","shell.execute_reply.started":"2021-07-01T20:54:33.358438Z","shell.execute_reply":"2021-07-01T20:54:33.425768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No rows are incomplete. All columns have very different scales, so they will have to be normalized.","metadata":{}},{"cell_type":"markdown","source":"Let's see if the categories are balanced.","metadata":{}},{"cell_type":"code","source":"dataset.Class.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:33.427959Z","iopub.execute_input":"2021-07-01T20:54:33.428519Z","iopub.status.idle":"2021-07-01T20:54:33.437001Z","shell.execute_reply.started":"2021-07-01T20:54:33.428476Z","shell.execute_reply":"2021-07-01T20:54:33.436305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The categories are not very balanced. ","metadata":{}},{"cell_type":"code","source":"feature_cols = COLUMNS[:-1]\nsns.set(style='darkgrid')\nfig, ax_list = plt.subplots(nrows=4, ncols=4, sharey=False, figsize=(36,24))\nax_list = ax_list.flatten()\nfor name, ax in zip(feature_cols, ax_list):\n     g = sns.histplot(dataset, x=name, bins=10, ax=ax).set(title=name)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:33.43814Z","iopub.execute_input":"2021-07-01T20:54:33.438715Z","iopub.status.idle":"2021-07-01T20:54:36.778291Z","shell.execute_reply.started":"2021-07-01T20:54:33.43867Z","shell.execute_reply":"2021-07-01T20:54:36.775898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There appear to be a few individuals whose solidity is considerably larger than the rest.","metadata":{}},{"cell_type":"code","source":"dataset.sort_values(['Solidity'], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:36.780531Z","iopub.execute_input":"2021-07-01T20:54:36.780807Z","iopub.status.idle":"2021-07-01T20:54:36.810929Z","shell.execute_reply.started":"2021-07-01T20:54:36.78078Z","shell.execute_reply":"2021-07-01T20:54:36.810047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3. Encoding","metadata":{}},{"cell_type":"markdown","source":"Let's encode our class labels.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndataset['Class'] = le.fit_transform(dataset.Class)\ndataset['Class']","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:36.812389Z","iopub.execute_input":"2021-07-01T20:54:36.812634Z","iopub.status.idle":"2021-07-01T20:54:36.940007Z","shell.execute_reply.started":"2021-07-01T20:54:36.812609Z","shell.execute_reply":"2021-07-01T20:54:36.938966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us see if there is any correlation between variables.","metadata":{}},{"cell_type":"markdown","source":"### 3.4. Correlation","metadata":{}},{"cell_type":"code","source":"# Calculate the correlation values\ncorr_values = dataset[feature_cols].corr()\n\n# Simplify by emptying all the data below the diagonal\ntril_index = np.tril_indices_from(corr_values)\n\n# Make the unused values NaNs\nfor coord in zip(*tril_index):\n    corr_values.iloc[coord[0], coord[1]] = np.NaN\n    \n# Stack the data and convert to a data frame\ncorr_values = (corr_values\n               .stack()\n               .to_frame()\n               .reset_index()\n               .rename(columns={'level_0':'feature1',\n                                'level_1':'feature2',\n                                0:'correlation'}))\n\n# Get the absolute values for sorting\ncorr_values['abs_correlation'] = corr_values.correlation.abs()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:36.941158Z","iopub.execute_input":"2021-07-01T20:54:36.941464Z","iopub.status.idle":"2021-07-01T20:54:36.978202Z","shell.execute_reply.started":"2021-07-01T20:54:36.941435Z","shell.execute_reply":"2021-07-01T20:54:36.977286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_context('talk')\nsns.set_style('white')\n\nax = corr_values.abs_correlation.hist(bins=10, figsize=(12, 5))\nax.set(xlabel='Absolute Correlation', ylabel='Frequency');\n\n# The most highly correlated values\ncorr_values.sort_values('abs_correlation', ascending=False).query('abs_correlation>0.8')","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:36.979372Z","iopub.execute_input":"2021-07-01T20:54:36.979636Z","iopub.status.idle":"2021-07-01T20:54:37.189092Z","shell.execute_reply.started":"2021-07-01T20:54:36.979609Z","shell.execute_reply":"2021-07-01T20:54:37.188168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before continuing, we will scale our X columns to values between zero and one, using the Yeo-Johnson transformation and the standard scaler.","metadata":{}},{"cell_type":"code","source":"feature_columns = [x for x in dataset.columns if x not in ['Class']]\n\nskew_columns = (dataset[feature_columns].skew().sort_values(ascending=False))\nskew_columns = skew_columns.loc[skew_columns > 0.8]\nskew_columns","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:37.190284Z","iopub.execute_input":"2021-07-01T20:54:37.190564Z","iopub.status.idle":"2021-07-01T20:54:37.207059Z","shell.execute_reply.started":"2021-07-01T20:54:37.190534Z","shell.execute_reply":"2021-07-01T20:54:37.20623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import yeojohnson\n\n# Apply transformation to long-tailed columns\nyeoj = dict()\nyeoj_fields = ['ShapeFactor4', 'Solidity', 'Eccentricity', 'roundness']\nyeoj_fields = yeoj_fields\nfor f in yeoj_fields:\n    yeoj[f] = yeojohnson(dataset[f])\n    dataset[f] = yeoj[f][0]\n    print(\"{0} transformed with lambda {1}\".format(f, yeoj[f][1]))","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:37.208108Z","iopub.execute_input":"2021-07-01T20:54:37.208377Z","iopub.status.idle":"2021-07-01T20:54:37.35685Z","shell.execute_reply.started":"2021-07-01T20:54:37.20835Z","shell.execute_reply":"2021-07-01T20:54:37.35604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nfor col in skew_columns.index.tolist():\n    dataset[col] = np.log1p(dataset[col])\n\nsc = StandardScaler()\ndataset[feature_columns] = sc.fit_transform(dataset[feature_columns])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:37.358173Z","iopub.execute_input":"2021-07-01T20:54:37.358588Z","iopub.status.idle":"2021-07-01T20:54:37.408663Z","shell.execute_reply.started":"2021-07-01T20:54:37.358544Z","shell.execute_reply":"2021-07-01T20:54:37.40778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nplt.close()\nsns.set_style('whitegrid')\nsns.pairplot(dataset, hue='Class', height=3)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:54:37.411725Z","iopub.execute_input":"2021-07-01T20:54:37.411979Z","iopub.status.idle":"2021-07-01T20:59:13.545074Z","shell.execute_reply.started":"2021-07-01T20:54:37.411953Z","shell.execute_reply":"2021-07-01T20:59:13.543337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At first sight, it would seem that it will be possible to split our beans into different classes from their attributes alone. ","metadata":{}},{"cell_type":"code","source":"fig, ax_list = plt.subplots(nrows=4, ncols=4, sharey=False, figsize=(36,36))\nax_list = ax_list.flatten()\nfor name, ax in zip(feature_cols, ax_list):\n     g = sns.histplot(dataset, x=name, bins=10, ax=ax).set(title=name)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:59:13.546685Z","iopub.execute_input":"2021-07-01T20:59:13.547043Z","iopub.status.idle":"2021-07-01T20:59:17.035996Z","shell.execute_reply.started":"2021-07-01T20:59:13.547004Z","shell.execute_reply":"2021-07-01T20:59:17.035081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:59:17.037423Z","iopub.execute_input":"2021-07-01T20:59:17.037727Z","iopub.status.idle":"2021-07-01T20:59:17.064059Z","shell.execute_reply.started":"2021-07-01T20:59:17.037696Z","shell.execute_reply":"2021-07-01T20:59:17.062901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Clustering models","metadata":{}},{"cell_type":"markdown","source":"Definition of a function that will find the maximum values of the confusion matrices using the Hungarian algorithm, and also rearrange the confusion matrix to have the maxima in a neat diagonal.","metadata":{}},{"cell_type":"code","source":"!pip install munkres\n\nimport sys\nfrom munkres import Munkres, print_matrix\n\ndef rearrange_confusion_matrix(df):\n    m = Munkres()\n    matrix = (df.copy()).to_numpy().tolist()\n\n    cost_matrix = []\n    for row in matrix:\n        cost_row = []\n        for col in row:\n            cost_row += [sys.maxsize - col]\n        cost_matrix += [cost_row]\n\n    indexes = m.compute(cost_matrix)\n#     print(indexes)\n#     print_matrix('Highest profit through this matrix:\\n', matrix)\n    total = 0\n    for row, column in indexes:\n        value = matrix[row][column]\n        total += value\n    print('Correctly classified =', total)\n\n    matrix = pd.DataFrame(matrix, columns=list(range(len(matrix))))\n    rearranged = matrix.copy()\n#     print(rearranged.columns)\n    row_order = list(range(len(matrix)))\n    col_order = rearranged.columns.tolist()\n\n    for i, j in zip(indexes, row_order):\n        new_cols = rearranged.columns.tolist()\n        if j == new_cols[j] and i[1] == new_cols[i[1]]:\n            new_cols[new_cols[j]] = i[1]\n            new_cols[i[1]] = j\n            rearranged = rearranged.copy().reindex(columns=new_cols)\n        elif j != new_cols[j] and i[1] != new_cols[i[1]]:\n            m = new_cols[j]\n            n = new_cols.index(i[1])\n            new_cols[j], new_cols[n] = new_cols[n], new_cols[j]\n            rearranged = rearranged.copy().reindex(columns=new_cols)\n        else:\n            m = new_cols[j]\n            n = new_cols.index(i[1])\n            new_cols[j], new_cols[n] = new_cols[n], new_cols[j]\n            rearranged = rearranged.copy().reindex(columns=new_cols)\n\n    print('Accuracy: {0}'.format(total/rearranged.stack().sum()))\n    return(rearranged)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:59:17.06555Z","iopub.execute_input":"2021-07-01T20:59:17.065951Z","iopub.status.idle":"2021-07-01T20:59:25.675225Z","shell.execute_reply.started":"2021-07-01T20:59:17.065906Z","shell.execute_reply":"2021-07-01T20:59:25.673639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1. K-means clustering model\n- Fit a K-means clustering model with seven clusters, one per class.\n- Examine the clusters by counting each class in each cluster.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nmethod = 'kmeans'\n\nkm = KMeans(n_clusters=7, random_state=42)\nkm = km.fit(dataset[feature_columns])\ndataset[method] = km.predict(dataset[feature_columns])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:59:25.67686Z","iopub.execute_input":"2021-07-01T20:59:25.677241Z","iopub.status.idle":"2021-07-01T20:59:27.633455Z","shell.execute_reply.started":"2021-07-01T20:59:25.677196Z","shell.execute_reply":"2021-07-01T20:59:27.632602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame with labels and varieties as columns: df\nconfusion = pd.DataFrame({'Class': dataset['Class'], 'Clusters': dataset[method]})\n# Create crosstab: ct\nct_km = pd.crosstab(confusion['Clusters'], confusion['Class'])\nct_km = rearrange_confusion_matrix(ct_km)\n\n# Plot confusion matrix\n_, ax = plt.subplots(figsize=(5,5))\nax = sns.heatmap(ct_km, annot=True, fmt='d')  \nax.set_ylabel('Classes', fontsize=20);\nax.set_xlabel('Clusters', fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:59:27.634613Z","iopub.execute_input":"2021-07-01T20:59:27.635052Z","iopub.status.idle":"2021-07-01T20:59:28.085631Z","shell.execute_reply.started":"2021-07-01T20:59:27.635019Z","shell.execute_reply":"2021-07-01T20:59:28.084611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Classes 5 and 6 get partly confused.","metadata":{}},{"cell_type":"markdown","source":"### 4.2. Agglomerative clustering model (ward)\n- Fit an agglomerative clustering model with seven clusters, one per class.\n- Examine the clusters by counting each class in each cluster.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nmethod = 'agglom-ward'\n\nag = AgglomerativeClustering(n_clusters=7, linkage='ward', compute_full_tree=True)\nag = ag.fit(dataset[feature_columns])\ndataset[method] = ag.fit_predict(dataset[feature_columns])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:59:28.087226Z","iopub.execute_input":"2021-07-01T20:59:28.087647Z","iopub.status.idle":"2021-07-01T20:59:45.655914Z","shell.execute_reply.started":"2021-07-01T20:59:28.087604Z","shell.execute_reply":"2021-07-01T20:59:45.654944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame with labels and varieties as columns: df\nconfusion = pd.DataFrame({'Class': dataset['Class'], 'Clusters': dataset[method]})\n# Create crosstab: ct\nct_ag = pd.crosstab(confusion['Class'], confusion['Clusters'])\nct_ag = rearrange_confusion_matrix(ct_ag)\n\n# Plot confusion matrix\n_, ax = plt.subplots(figsize=(5,5))\nax = sns.heatmap(ct_ag, annot=True, fmt='d')  \nax.set_ylabel('Classes', fontsize=20);\nax.set_xlabel('Clusters', fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:59:45.657238Z","iopub.execute_input":"2021-07-01T20:59:45.65755Z","iopub.status.idle":"2021-07-01T20:59:46.144159Z","shell.execute_reply.started":"2021-07-01T20:59:45.65752Z","shell.execute_reply":"2021-07-01T20:59:46.143161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Classes 0 and 3, and 0 and 2 remain partly confused.","metadata":{}},{"cell_type":"markdown","source":"### 4.3. Agglomerative clustering model (complete)\n- Fit an agglomerative clustering model with seven clusters, one per class.\n- Examine the clusters by counting each class in each cluster.","metadata":{}},{"cell_type":"code","source":"method = 'agglom-complete'\n\nagc = AgglomerativeClustering(n_clusters=7, linkage='complete', compute_full_tree=True)\nagc = agc.fit(dataset[feature_columns])\ndataset[method] = agc.fit_predict(dataset[feature_columns])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T20:59:46.145243Z","iopub.execute_input":"2021-07-01T20:59:46.145501Z","iopub.status.idle":"2021-07-01T21:00:01.768738Z","shell.execute_reply.started":"2021-07-01T20:59:46.145475Z","shell.execute_reply":"2021-07-01T21:00:01.7679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame with labels and varieties as columns: df\nconfusion = pd.DataFrame({'Class': dataset['Class'], 'Clusters': dataset[method]})\n# Create crosstab: ct\nct_agc = pd.crosstab(confusion['Class'], confusion['Clusters'])\nct_agc = rearrange_confusion_matrix(ct_agc)\n\n# Plot confusion matrix\n_, ax = plt.subplots(figsize=(5,5))\nax = sns.heatmap(ct_agc, annot=True, fmt='d')  \nax.set_ylabel('Classes', fontsize=20);\nax.set_xlabel('Clusters', fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T21:00:01.770098Z","iopub.execute_input":"2021-07-01T21:00:01.770507Z","iopub.status.idle":"2021-07-01T21:00:03.103041Z","shell.execute_reply.started":"2021-07-01T21:00:01.770466Z","shell.execute_reply":"2021-07-01T21:00:03.102074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Classes 6 and 3 are almost merged in the same cluster, while cluster 2 has a mixture of classes 0, 2, and 6.","metadata":{}},{"cell_type":"markdown","source":"### 4.4. Agglomerative clustering model (single)\n- Fit an agglomerative clustering model with seven clusters, one per class.\n- Examine the clusters by counting each class in each cluster.","metadata":{}},{"cell_type":"code","source":"method = 'agglom-single'\n\nags = AgglomerativeClustering(n_clusters=7, linkage='single', compute_full_tree=True)\nags = ags.fit(dataset[feature_columns])\ndataset[method] = ags.fit_predict(dataset[feature_columns])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T21:00:03.104401Z","iopub.execute_input":"2021-07-01T21:00:03.104793Z","iopub.status.idle":"2021-07-01T21:00:08.033744Z","shell.execute_reply.started":"2021-07-01T21:00:03.104752Z","shell.execute_reply":"2021-07-01T21:00:08.032906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataFrame with labels and varieties as columns: df\nconfusion = pd.DataFrame({'Class': dataset['Class'], 'Clusters': dataset['agglom-single']})\n# Create crosstab: ct\nct_ags = pd.crosstab(confusion['Class'], confusion['Clusters'])\nct_ags = rearrange_confusion_matrix(ct_ags)\n\n# Plot confusion matrix\n_, ax = plt.subplots(figsize=(5,5))\nax = sns.heatmap(ct_ags, annot=True, fmt='d')  \nax.set_ylabel('Classes', fontsize=20);\nax.set_xlabel('Clusters', fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T21:00:08.037661Z","iopub.execute_input":"2021-07-01T21:00:08.03796Z","iopub.status.idle":"2021-07-01T21:00:08.515986Z","shell.execute_reply.started":"2021-07-01T21:00:08.03793Z","shell.execute_reply":"2021-07-01T21:00:08.514818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost all observations were merged in a single cluster, so this method clearly is not appropriate for this dataset.","metadata":{}},{"cell_type":"markdown","source":"## 5. Key findings\n\n1. The K-means method was the most accurate of those attempted.\n2. It was possible to build confusion matrices to see which clusters represented which classes, because each algorighm would build the clusters its own way. By rearranging each matrix and finding the diagonal with the highest values, it was possible to find the accuracy of our models.","metadata":{}},{"cell_type":"markdown","source":"## 6. Future steps\n\n1. The mean shift method was attempted, but the computation time was very long. A future step of adding this method would be more likely to succeed if PCA is applied first.\n\n2. A check for overfitting should be included, perhaps by including test/train splits, or some other method.\n","metadata":{}}]}