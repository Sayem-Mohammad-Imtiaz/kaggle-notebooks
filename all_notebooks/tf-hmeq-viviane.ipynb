{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Data Mining e Machine Learning II** \n* Professor: MARCOS GUIMARÃES\n* Aluna: Viviane Silvestre"},{"metadata":{},"cell_type":"markdown","source":"# **Análises Exploratórias**"},{"metadata":{},"cell_type":"markdown","source":"1. A análise exploratória serrá realizada para conhecer o conjunto de dados de modo a resumir as características do departamento de crédito de um banco. O objetivo é a automatizar o processo de tomada de decisão para aprovação das linhas de crédito do patrimonio líquido. Assim, seguindo as recomendações da Lei de Igualdade de Oportunidades de Crédito, será criado um modelo de pontuação derivado e estatisticamente sólido baseado nos dados coletados pelas conceções recentes de crédito através do processo atual. O modelo será construído a partir de ferramentas de modelagem preditiva. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importação do conjunto de dados \ndf= pd.read_csv('../input/hmeq-data/hmeq.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Verificando o Dataset\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando a imensão do Dataset\nprint(df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando os tipos de variáveis\ndf.dtypes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Analises de estatísticas descritivas \nprint(df.describe().T)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O conjunto de dados possuir em seu maior número 5960 registros, no entanto, algumas variáveis possuem valores nulos, a variável DEBTINC possui 4693, sendo necessário utilizar técnicas para padronizar o conjunto de dados. Que podem ser imputação ou exclusão dos valores missings. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# imprimindo os primeiros registros do Dataset\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Tratando Missing**"},{"metadata":{},"cell_type":"markdown","source":"O primeiro passo é verificar a quantidade de missings em cada tipo de variável e assim tomar a decisão de que tipo de técnica será utilizada para padronizar a base de dados. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# verificando os valores missing nas variáveis númericas e categóricas\nfeat_missing = []\n\nfor f in df.columns:\n    missings = df[f].isnull().sum()\n    if missings > 0:\n        feat_missing.append(f)\n        missings_perc = missings/df.shape[0]\n        \n        # printing summary of missing values\n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n\n# how many variables do present missing values?\nprint()\nprint('O total, de {} variáveis com valores missings'.format(len(feat_missing)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A analise demonstrou que a maior parte dos valores missings está na variável MOTDUE, que é a variável que possui o valor devido pelos clientes nas hipótecas. Como esses valores são muito diferentes, a decisão foi por excluir os valores missings, e assim analisar apenas os valores reais da base de dados. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping rows that have missing data\ndf.dropna(axis=0, how='any', inplace=True)\ndf.info()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A base de dados padronizada possui 3364 registros e 13 colunas, um conjunto de dados relativamente pequeno para aplicar modelos de predição. Assim vamos criar uma nova coluna que irá verificar se o valor atual da propriedade é maior ou menor que o valor devido da hipoteca. \nVALUE_MORTDUE = Valor da propriedade - valor da hipoteca"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['VALUE_MORTDUE'] = df['VALUE'] - df['MORTDUE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Análise da Variável - Target (Preditora)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizações \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nax = sns.countplot(y='BAD', data=df).set_title(\"Situação dos Clientes\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A variável BAD \"Target\" verifica se o cliente está inadimplente\na resposta 1 = Inadimplete e 0 = Pagamento em Dia\nO gráfico demonstrou que a maioria dos clientes estão em dia com o pagamento."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Analise do Dataset\nimport pandas\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualização em Gráficos das variáveis categórias\nsns.set( rc = {'figure.figsize': (4, 4)})\nfcat = ['REASON','JOB']\n\nfor col in fcat:\n    plt.figure()\n    sns.countplot(y=df[col], data=df, palette=\"RdPu\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Análises dos gráficos \nReason**\n1. DebtCon = debt consolidation = Consolidação da Dívida\n1. HomeImp = home improvement - Melhoria na casa\nA análise demonstrou que a maior parte dos clientes tiveram a dívida consolidade e que apenas 1000 clientes fizeram melhorias no imóvel.\n\n**Job\n1. Refere-se a Seix Categorias Ocupacionais** \nOther, Office, Mgr, ProfExe, Sales, Self\nA maioria dos clientes não se enquandraram nas categorias e classificaram a ocupação como outras, a maior parte são ProfExe, Office e Mgr. \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualizing numeric variables using seaborn\nf, axes = plt.subplots(3,3, figsize=(20,20))\nsns.distplot( df[\"LOAN\"] , color=\"skyblue\", bins=15, kde=False, ax=axes[0, 0])\nsns.distplot( df[\"DEBTINC\"] , color=\"olive\", bins=15, kde=False, ax=axes[0, 1])\nsns.distplot( df[\"MORTDUE\"] , color=\"orange\", bins=15, kde=False, ax=axes[0, 2])\nsns.distplot( df[\"YOJ\"] , color=\"yellow\", bins=15, kde=False, ax=axes[1, 0])\nsns.distplot( df[\"VALUE\"] , color=\"pink\", bins=15, kde=False, ax=axes[1, 1])\nsns.distplot( df[\"CLAGE\"] , color=\"gold\", bins=15, kde=False, ax=axes[1, 2])\nsns.distplot( df[\"CLNO\"] , color=\"teal\", bins=15, kde=False, ax=axes[2, 1])\nsns.distplot( df['DEROG'], color=\"blue\", bins=15, kde=False, ax=axes[2, 2])\nsns.distplot( df['DELINQ'], color=\"green\", bins=15, kde=False, ax=axes[2, 0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. **Análises das variáveis númericas - Gráficos de Histograma **\n* As variáveis quantitativas demosntram em todos os casos distruíção assimétrica a esquerda.\n* Variável LOAN - Os valores dos empréstimos estão concentrados até 20.000,00, com alguns valores extremos acima de 80.000,00\n* Variável DEBTINC - que é a razão entre a dívida e o rendimento ou seja a capacidade de pagamento, possui mais concentração entre 30 e 40.\n* Variável MORTDUE - que verifica o valor devido da hipóteca, mostra que a maioria dos clientes posseum dívidas que variam entre 40.000 e 80.000\n* Variável YOJ - Verifica os anos que trabalha no emprego atual. A maioria trabalha até 5 anos na empresa, alguns clientes trabalham a mais de 40 anos (número fora do padrão)\n* Variável VALUE - Que verifica os valores dos imóveis demonstra que a maioria dos imóveis estão avaliados entre 50.000 e 100.000, existindo casos de imóveis acima de 400.000\n* Variável CLAGE - verifica a idade em meses da mais da linha de crédito, a concentração está entre 100 e 300 meses (entre 8 e 25 anos) em que adquiriu a primeira linha de crédito. \n* Variável CLNO - verifica quantas linhas de crédito o cliente possui. A maioria possui entre 10 e 25 linhas de crédito. \n* Variável DEROG - quantidade relatórios depreciativos a concentração está entre 1 e 3.\n* Variável DELINQ - Quantidade de linhas de crédito inadimplentes concentram-se entre 0 e 3."},{"metadata":{},"cell_type":"markdown","source":"* Para as próximas análises será criado variáveis Dummies com objetivo de melhorar a base de dados nas predições que serão realizadas. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criar variáveis Dummies\nDum_df = pd.get_dummies(df)\nprint(Dum_df.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A variável **VALUE_MORTDUE** foi criada para verificar se existe saldo suficiente sobre o valor do imóvel (VALUE) caso seja necessário executar o valor da hipóteca (MORTDUE).\n\nNo gráfico de boxplot observou-se que a média do saldo do imóvel está em torno de 20.000. \nExistem valores extremos negativos, que significa que o valor do imóvel não é suficiente para pagar a hipoteca e valores extremos positivos mostram que os valores dos imóveis é superior ao valor da hipoteca."},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(x=\"VALUE_MORTDUE\", data=Dum_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nsns.scatterplot(data=Dum_df, x='VALUE', y='MORTDUE')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(data=Dum_df, x='VALUE', y='LOAN')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Gráficos de Correlação**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Base sem Dummies\ncorr = df.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(10,8))\n#Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No gráfico de correlação antes da criação das variáveis dummies mostra uma correlação forte entre as variáveis \"MORTDUE\" e \"VALUE\" em relação a variável target BAD."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Base com Dummies\ncorr = Dum_df.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(10,8))\n#Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Após a criação das variáveis dummies percebe-se uma melhora na correlação das variáveis LOAN YOJ, continuando mostrar correlação forte entre VALUE e LOAN."},{"metadata":{},"cell_type":"markdown","source":"# **Modelos estatísticos**"},{"metadata":{},"cell_type":"markdown","source":"Para a aplicação dos modelos estatísticos a base de dados será dividida entre teste e treino. \n1. Serão aplicados os modelos de RandomForestClassifier, cross_val_score, XGBoost e GradientBoostingClassifier. Com o objetivo de verificar se o modelo é capaz de auxiliar na tomada de decisão de conceder ou não o crédito."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividindo o DataFrame\nfrom sklearn.model_selection import train_test_split\n\n# Treino e teste\ntrain, test = train_test_split(Dum_df, test_size=0.15, random_state=42)\n\n# Veificando o tanho dos DataFrames\ntrain.shape, test.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Dum_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecionado as features\nfeats = [c for c in Dum_df.columns if c not in ['BAD']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Dum_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trabalhando com RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nrf = RandomForestClassifier(n_estimators=200, min_samples_split=5, max_depth=4, random_state=42)\nrf.fit(train[feats], train['BAD'])\naccuracy_score(test['BAD'], rf.predict(test[feats]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importance com RF\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Usar o cross validation\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(rf, train[feats], train['BAD'], n_jobs=-1, cv=5)\n\nscores, scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importance com RF\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trabalhando com XGBoost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(n_estimators=200, learning_rate=0.09, random_state=42)\nxgb.fit(train[feats], train['BAD'])\naccuracy_score(test['BAD'], xgb.predict(test[feats]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importance com RF\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trabalhando com GBM\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbm = GradientBoostingClassifier(n_estimators=200, learning_rate=1.0, max_depth=1, random_state=42)\ngbm.fit(train[feats], train['BAD'])\naccuracy_score(test['BAD'], gbm.predict(test[feats]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importance com RF\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"cross validation - \", scores, scores.mean())\nprint(\"RandomForest - \",accuracy_score(test['BAD'], rf.predict(test[feats])))\nprint(\"GBM - \",accuracy_score(test['BAD'], gbm.predict(test[feats])))\nprint(\"XGBoost - \",accuracy_score(test['BAD'], xgb.predict(test[feats])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nas análises de acima o algoritmo que melhor explica o modelo é XGBoost com 95% de acurácia."},{"metadata":{},"cell_type":"markdown","source":"Outra metodologia aplicada será a clusterização, o primeiro passo será selecionar as variáveis, para isso considerou o gráfico de correlação. As variáveis escolhidas foram 'MORTDUE', 'LOAN' e 'YOJ'. O objetivo é determinar a quantidade de clusters (grupos) necessários para explicar o modelo. "},{"metadata":{},"cell_type":"markdown","source":"# **Clusterização**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Determinando a quantidade de clusters\n\n# Importando o k-means\nfrom sklearn.cluster import KMeans\n\n# Selecionando as variaveis para utilizar no modelo.\nX= Dum_df[['MORTDUE','LOAN', 'YOJ']]\n\n# Cálculo do SSE - Sum of Squared Erros\nsse = []\n\nfor k in range (1, 12):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(X)\n    sse.append(kmeans.inertia_)\nprint(sse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Definindo a quantidade clusters utilizando o método Elbow \nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, 12), sse, 'bx-')\nplt.title('Elbow Method')\nplt.xlabel('Número de clusters')\nplt.ylabel('SSE')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O gráfico do modelo \"Gráfico de Cotovelo\" mostrou que o número bom de clusters é 4, uma vez que de 1 a 3 teríamos poucos agrupamentos, ou seja, a informação estaria muito concentrada e de 5 a 6 os agrupamentos estariam muitos dispersos. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Excecutando a clusterização com 4 clusters\nkmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\ncluster_id = kmeans.fit_predict(X)\ncluster_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Guardar os resultados no dataframe\nX['cluster_id'] = cluster_id\n\n#veficando o tamanho do DF\nX.sample(10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Verifica-se que o cluster 2 possui um maior numero de iformações."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotando os agrupamentos e os centroídes\nfig = plt.figure(figsize=(12,8))\n\nplt.scatter(X.values[:,0], X.values[:,1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:,0] ,kmeans.cluster_centers_[:,1], color='red', marker=\"x\", s=200)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Os grupos são agrupados por suas similaridades baseado no conceito de distância, a técnica consiste em atribuir um centróide a cada elemento, em seguida, utilizando a distância euclidiana, calcula-se as distâncias de cada elemento, e divide os centróides de acordo com o número de clustes, nesse caso 4.\nA partir de então,uUma vez que os pontos foram atribuídos aos clusters conforme sua distância, recalcula-se o valor dos centróide, então é calculada a média dos valores dos pontos de dados de cada cluster e o valor médio será o novo centróide. E a partir do centróide são calculadas as distâncias de cada elemento e agrupando aqueles com maior próximidade. \nAtavés dessa técnica verificamos que os grupos laranja e preto tiveram uma quantidade de elementos com menor distância, tornan-se grupos maiores e dois grupos com quantidade menor de elementos por esses elementos estarem mais distantes dos centróides. \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Dum_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Modelo de Regressão Logística**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, roc_curve, classification_report,\\\n                            accuracy_score, confusion_matrix, auc\n\n#Utilizando o statsmodels\n#Verificar as chances de um cliente ser inadimplente em função das variáveis LOAN + YOJ + MORTDUE (Valor do empréstimo, ocupação e o valor da hipoteca)\n#BAD = 1 (Cliente Inadimplente)\n#BAD = 2 (Cliente adimplente)\n\nmodelo = smf.glm(formula='BAD ~ LOAN + YOJ + MORTDUE', data=Dum_df,\n                family = sm.families.Binomial()).fit()\nprint(modelo.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gerar osdados em percentuais relativos de chances de inadimplencia\nprint(np.exp(modelo.params[1:]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(np.exp(modelo.params[1:]) -1) * 100\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Para a regressão logistica foi utilizada a base com a criação das variáveis dummies. \nTodos os coeficientes foram estatísticamente significativos para o modelo, pois o p-valor foi abaixo do nível de significância de 5%.\nA chance de um cliente ser inadimplente é de 99% se for considerado o montande do empréstimo LOAN, 96% está associado a ocupação YOJ e 99% ao valor devido da hipoteca MORTDUE. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Agora vamos fazer com sklearn para aproveitar as métricas\nmodel1 = LogisticRegression(penalty='none', solver='newton-cg')\nbaseline_df = Dum_df[['LOAN', 'YOJ', 'MORTDUE']].dropna()\ny = Dum_df.BAD\nX = pd.get_dummies(Dum_df[['LOAN', 'YOJ', 'MORTDUE']], drop_first=True)\nprint(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Aplicando a Regressão logística com sklearn\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='none',\n                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n                   warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model1.coef_) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predizendo as probabilidades\nyhat = model1.predict_proba(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# manter somente para a classe positiva\nyhat = yhat[:, 1] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# usando a função do sklearn\nconfusion_matrix(y, model1.predict(X)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imprimindo a matriz de confusão\npd.crosstab(y, model1.predict(X))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acuracia = accuracy_score(y, model1.predict(X))\nprint('O modelo obteve %0.4f de acurácia.' % acuracia)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}