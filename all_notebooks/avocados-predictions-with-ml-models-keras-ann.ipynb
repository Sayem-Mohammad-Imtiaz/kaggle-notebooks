{"cells":[{"metadata":{"_uuid":"efe0b12370ca94ef019529d8ac33245eb4e33268"},"cell_type":"markdown","source":"# Avocados - ML models, keras ANN, seaborn plots\n## Introduction\nI like avocados.\n\n## Import libraries and data"},{"metadata":{"trusted":true,"_uuid":"568dd5809e3ad6f9016c29990cee58e4746a9e9f"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a19da1288c986c271f070a71cab4ac649ff0635e"},"cell_type":"code","source":"df = pd.read_csv('../input/avocado.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1398fd0c6d135c7232d09da854764155dd99ac5"},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c886975f466e57e7cc75dd1e9bcf676344377ebc"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c62e2b6af1952e7eeaf03ad840cb9dfca4c2f24"},"cell_type":"code","source":"df = df.drop(['Unnamed: 0', 'Date'], axis = 1)\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33ea431e40204db7571425bdd40f355953af3fb0"},"cell_type":"markdown","source":"## Visualizations\nTotal volume is higly correlated with small bags and total bags which are correleted to each other too."},{"metadata":{"trusted":true,"_uuid":"a9d37f1b12393784273c51c507163775c6493347"},"cell_type":"code","source":"f, ax = plt.subplots(1, 1, figsize=(10,8))\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax)\nax.set_title(\"Correlation Matrix\", fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72e1e62e9af190d2adcfbfaed58577b98949b3ee"},"cell_type":"markdown","source":"Indeed, the smaller bags, the higher number of them  are taken"},{"metadata":{"trusted":true,"_uuid":"71e896657f3caf4fe140b171303792530711c83e"},"cell_type":"code","source":"sns.jointplot(x='Small Bags',y='Total Bags',data=df, color='red')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e74fedebb3093aefeb6dfe2cdc7d88f24382c955"},"cell_type":"markdown","source":"Surprisingly or not, price doesn't change among the years'"},{"metadata":{"trusted":true,"_uuid":"e55bd84df90a730f2cf1ee1441fdab2b22cc3fab"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10,6))\nsns.boxplot(x='year',y='AveragePrice',data=df,color='red')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b17189bf293a8315fd14166455b07a63ad6ad0ab"},"cell_type":"markdown","source":"Average Price distribution shows that for most cases price of avocado is between 1.1, 1.4."},{"metadata":{"trusted":true,"_uuid":"1631adc3d84d3ccc1ce190385b6d5626e56d10b8"},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(10,6))\nprice_val = df['AveragePrice'].values\nsns.distplot(price_val, color='r')\nax.set_title('Distribution of Average Price', fontsize=14)\nax.set_xlim([min(price_val), max(price_val)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f836a9b620a4311d15b95d742dc8621ada405bd"},"cell_type":"markdown","source":"##  Implementing machine learning models\n### Data prepearing and encoding categorical variables\n"},{"metadata":{"trusted":true,"_uuid":"7aedabb02b7bcc24625eec8a2ab4a6a3748cd065"},"cell_type":"code","source":"X = df.drop(['AveragePrice'], axis = 1).values\ny = df['AveragePrice'].values\n\n# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 8] = labelencoder_X_1.fit_transform(X[:, 9])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 9] = labelencoder_X_2.fit_transform(X[:, 10])\nlabelencoder_X_3 = LabelEncoder()\nX[:, 10] = labelencoder_X_3.fit_transform(X[:, 10])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b6e6afee016d6a02970288ed7d341118eb734a1"},"cell_type":"markdown","source":"### Standardize the variables"},{"metadata":{"trusted":true,"_uuid":"21a4c48ebaa1bcf235cefceb60b54db81a4d8c3b"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df.drop(['AveragePrice', 'type', 'year', 'region'],axis=1))\nscaled_features = scaler.transform(df.drop(['AveragePrice', 'type', 'year', 'region'],axis=1))\ndf_feat = pd.DataFrame(scaled_features,columns=df.columns[1:9])\ndf_feat.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f248c5c96b04d122fa5020a6286edc312f2c6c0"},"cell_type":"markdown","source":"### Train test split, label encoding"},{"metadata":{"trusted":true,"_uuid":"515ae3eda5487ef217fcec0871a897e13d2439e6"},"cell_type":"code","source":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1caf750ca19d0bc7f727783bce9cb36c64413bb2"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nfrom sklearn import preprocessing\nfrom sklearn import utils\nlab_enc = preprocessing.LabelEncoder()\ny_train = lab_enc.fit_transform(y_train)\ny_test = lab_enc.fit_transform(y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42f14fbac5a61dde12fba5a00692656254e91edf"},"cell_type":"markdown","source":"### ML models implementation"},{"metadata":{"trusted":false,"_uuid":"b3cca51198ad9fcf84c44df0393680d82c8a8b0b"},"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg = OneVsRestClassifier(logreg, n_jobs=1)\nlogreg.fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn = OneVsRestClassifier(knn, n_jobs=1)\nknn.fit(X_train,y_train)\npred_knn = knn.predict(X_test)\n\nfrom sklearn.svm import SVC\nsvc = SVC()\nsvc = OneVsRestClassifier(svc, n_jobs=1)\nsvc.fit(X_train, y_train)\npred_svc = svc.predict(X_test)\n\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier()\ndecision_tree = OneVsRestClassifier(decision_tree, n_jobs=1)\ndecision_tree.fit(X_train, y_train)\npred_tree = decision_tree.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2ba3b59d26f1808ede9ff071ec653ce42a788da"},"cell_type":"markdown","source":"### Confusion matrixes"},{"metadata":{"trusted":true,"_uuid":"7e98eb63eb0a9f3ae8113fbf78f78569b19d5f13"},"cell_type":"code","source":"sns.jointplot(x=y_test, y=pred_logreg, color= 'g')\nsns.jointplot(x=y_test, y=pred_knn, color= 'g')\nsns.jointplot(x=y_test, y=pred_svc, color= 'g')\nsns.jointplot(x=y_test, y=pred_tree, color= 'g')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da4eb664607366628af18c5aa8a322e14ebdeb80"},"cell_type":"markdown","source":"As we see, KNN method works the best here, logistic regression and decision tree completely do not work. We could tune k value.\n"},{"metadata":{"trusted":true,"_uuid":"fff2594547dcda7865bcb21b7d1bee36991e42c7"},"cell_type":"code","source":"error_rate = []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n    \nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51ea648c62e91cbf0006be2a69b8a217c322a7fb"},"cell_type":"markdown","source":"We choose k = 3."},{"metadata":{"trusted":true,"_uuid":"d195b65479b3a7214233a237cc12e301b27a2750"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn = OneVsRestClassifier(knn, n_jobs=1)\nknn.fit(X_train,y_train)\npred_knn = knn.predict(X_test)\nsns.jointplot(x=y_test, y=pred_knn, color= 'g')\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"337160d7370a00f83781c612c0be110d68fc3225"},"cell_type":"markdown","source":"## Keras neural network will appear soon. Stay tuned :)\nAnd it is. Here i present the best model for that moment. If you have any advices, please let me know :). For now it is not working the best, as SVC model. I tried a model with a few number of units, various batch size, drop out or without and chose the best one. Maybe different number of hidden layers would help."},{"metadata":{"trusted":true,"_uuid":"0ac541abe93be66c41de6173196b9a7e3fe2cc20"},"cell_type":"code","source":"print(X_train.shape[1])\nprint(X_train.shape[0])\nprint(len(np.unique(y_train)))\nprint((len(np.unique(y_train)) + X_train.shape[0]) /2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a9cdb6b568993bf73f7ccf15a286ca5e77e6a83"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.utils import np_utils\nfrom numpy import argmax\n\nBATCH_SIZE = 1000\nEPOCHS = 30\nVALIDATION_SPLIT = 0.1\nfile_path=\"weights_base3.best.hdf5\"\ncheckpoint = ModelCheckpoint(file_path, monitor='val_loss', save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\ncallbacks_list = [checkpoint, early]\n\ndef get_model():\n    model = Sequential()\n    model.add(Dense(7428, input_dim=X_train.shape[1]))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(7428))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(256, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', \n                  optimizer='adam', \n                  metrics=['accuracy']\n                 )\n    model.summary()\n    return model\nmodel_nn = get_model()\nmodel_nn.fit(X_train, np_utils.to_categorical(y_train),\n                  batch_size=BATCH_SIZE, \n                  epochs=EPOCHS,\n                  callbacks=callbacks_list,\n                  validation_split=VALIDATION_SPLIT\n             )\nmodel_nn.load_weights(file_path)\npred_ann = argmax(model_nn.predict(X_test), axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"625baae85721a26529eded0c15b8ee63ac66c0e9"},"cell_type":"code","source":"sns.jointplot(x=y_test, y=pred_ann, color= 'g')\nplt.plot()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}