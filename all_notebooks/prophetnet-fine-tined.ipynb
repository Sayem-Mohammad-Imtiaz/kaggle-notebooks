{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install rouge \n!git clone https://github.com/microsoft/ProphetNet\n!pip install fairseq==v0.9.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path\nimport os\nfrom rouge import Rouge \nimport string\nfrom IPython.display import display, Markdown\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH_TO_CRYPTO_NEWS = Path('../input/news-about-major-cryptocurrencies-20132018-40k/')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(PATH_TO_CRYPTO_NEWS / 'crypto_news_parsed_2013-2017_train.csv')\nvalid_df = pd.read_csv(PATH_TO_CRYPTO_NEWS / 'crypto_news_parsed_2018_validation.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"valid_df['text'].fillna(' ', inplace=True)\ntrain_df = train_df.dropna()\ntrain_df = train_df[train_df['title']!=' ']\n\ntitle_val = valid_df['title'] + '\\n'\ntext_val = valid_df['text'] + '\\n'\n\ntitle_tr = train_df['title'] + '\\n'\ntext_tr = train_df['text'] + '\\n'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"val_text.txt\", 'w') as f:\n    f.writelines(text_val.values.tolist())\nwith open(\"val_target.txt\", 'w') as f:\n    f.writelines(title_val.values.tolist())\n\nwith open(\"tr_target.txt\", 'w') as f:\n    f.writelines(title_tr.values.tolist())\nwith open(\"tr_text.txt\", 'w') as f:\n    f.writelines(text_tr.values.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the data\nFor the current dataset, I additionally truncate the source length at 400 tokens and the target at 15.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize.treebank import TreebankWordDetokenizer\nimport tqdm\nfrom transformers import BertTokenizer\n\ndef preprocess(fin, fout, keep_sep=False, max_len=512):\n    fin = open(fin, 'r', encoding='utf-8')\n    fout = open(fout, 'w', encoding='utf-8')\n    twd = TreebankWordDetokenizer()\n    bpe = BertTokenizer.from_pretrained('bert-base-uncased')\n    for line in tqdm.tqdm(fin.readlines()):\n        line = line.strip().replace('``', '\"').replace('\\'\\'', '\"').replace('`', '\\'')\n        s_list = [twd.detokenize(x.strip().split(\n            ' '), convert_parentheses=True) for x in line.split('<S_SEP>')]\n        tk_list = [bpe.tokenize(s) for s in s_list]\n        output_string_list = [\" \".join(s) for s in tk_list]\n        if keep_sep:\n            output_string = \" [X_SEP] \".join(output_string_list)\n        else:\n            output_string = \" \".join(output_string_list)\n        output_string = \" \".join(output_string.split(' ')[:max_len-1])\n        fout.write('{}\\n'.format(output_string))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir preprocessed_data","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"max_art_length = 400\nmax_title_length = 15\npreprocess('val_text.txt', 'preprocessed_data/valid.src', keep_sep=False, max_len=max_art_length)\npreprocess('val_target.txt', 'preprocessed_data/valid.tgt', keep_sep=True, max_len=max_title_length)\npreprocess('tr_text.txt', 'preprocessed_data/train.src', keep_sep=False, max_len=max_art_length)\npreprocess('tr_target.txt', 'preprocessed_data/train.tgt', keep_sep=True, max_len=max_title_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!fairseq-preprocess \\\n--user-dir ProphetNet/src/prophetnet/ \\\n--task translation_prophetnet \\\n--source-lang src --target-lang tgt \\\n--trainpref preprocessed_data/train \\\n--validpref preprocessed_data/valid \\\n--destdir dest_data/processed \\\n--srcdict ProphetNet/src/vocab.txt  \\\n--tgtdict ProphetNet/src/vocab.txt \\\n--bpe bert \\\n--workers 20","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"[ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/pdf/2001.04063.pdf)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I fine-tuned prophetnet_large 10 epoch with lr = 0.0001. \nBest model was on 5 epoch with validation perplexity 11.11. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"```\nDATA_DIR='dest_data/processed'\nUSER_DIR='ProphetNet/src/prophetnet/'\nARCH='ngram_transformer_prophet_large'\nCRITERION='ngram_language_loss'\nSAVE_DIR='finetune/cryptonews'\nPRETRAINED_MODEL='prophetnet_large_pretrained_160G_14epoch_model.pt'\n\n!fairseq-train \\\n--user-dir $USER_DIR --task translation_prophetnet --arch $ARCH \\\n--optimizer adam --adam-betas '(0.9, 0.999)' --clip-norm 1 \\\n--lr 0.0001 \\\n--lr-scheduler inverse_sqrt --warmup-init-lr 1e-07 --warmup-updates 1000 \\\n--dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \\\n--criterion $CRITERION  \\\n--update-freq 32  --max-sentences 10 \\\n--num-workers 4 \\\n--bpe bert \\\n--load-from-pretrained-model $PRETRAINED_MODEL \\\n--load-sep \\\n--ddp-backend=no_c10d --max-epoch 10 \\\n--max-source-positions 402 --max-target-positions 17 \\\n--seed 1 \\\n--save-dir $SAVE_DIR \\\n--keep-last-epochs 1 \\\n$DATA_DIR ```","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Generating title for validation set\n**\n\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"BEAM=5\nLENPEN=1.2\nCHECK_POINT='../input/prophetnetlarge-finetuned/checkpoint_best.pt'\nTEMP_FILE='predict_outputs.txt'\nOUTPUT_FILE='sorted_outputs.txt'\n\n!fairseq-generate dest_data/processed --path $CHECK_POINT --user-dir ProphetNet/src/prophetnet --task translation_prophetnet --batch-size 32 --gen-subset valid --beam $BEAM --max-len-a 0 --max-len-b 15 --min-len 6 --num-workers 4 --no-repeat-ngram-size 3 --lenpen $LENPEN 2>&1 > $TEMP_FILE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!grep ^H $TEMP_FILE | cut -c 3- | sort -n | cut -f3- | sed \"s/ ##//g\" > $OUTPUT_FILE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('sorted_outputs.txt', 'r') as f:\n    predicted = f.readlines()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punctuation = string.punctuation\ntrue_val_titles = valid_df['title'].tolist()\ntrue_titles = []\nfor tr in true_val_titles:\n    for p in punctuation:\n        tr = tr.replace(p, f' {p} ')\n    true_titles.append(tr.lower().replace('  ', ' '))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = [x.lower().replace('\\n', '') for x in predicted]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom rouge import Rouge\nrouge = Rouge()\nscores = rouge.get_scores(hyps=predicted, refs=true_titles, avg=True, ignore_empty=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_metric = (scores['rouge-1']['f'] + scores['rouge-2']['f'] + scores['rouge-l']['f']) / 3\nfinal_metric","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Eyeballing the results: good and bad cases","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_by_example = rouge.get_scores(hyps=predicted, refs=true_titles, avg=False, ignore_empty=True)\nscores_by_example = np.array([(x['rouge-1']['f'] + x['rouge-2']['f'] + x['rouge-l']['f']) / 3 for x in scores_by_example])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_result(index):\n    display(Markdown('> **Rouge:** ' + str(round(scores_by_example[index], 3))))\n    display(Markdown('> **Title:** ' + valid_df['title'].iloc[index]))\n    display(Markdown('> **Generated:** ' + predicted[index]))\n    display(Markdown('> **Text:** ' + valid_df['text'].iloc[index]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_best_10 = scores_by_example.argsort()[-10:]\ntop_worst_10 = scores_by_example.argsort()[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in top_best_10:\n    print_result(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in top_worst_10:\n    print_result(i)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}