{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NOTEBOOK DESCRIPTION\nIn this notebook, we are going to classify mushrooms between poisonous or not. We will create a neuronal network model using Tensorflow and we will optimize its structure defining a HyperModel using the Keras Tuner. Finally, we will compare the optimized structure with two others made by hand to check their performance.\n\nThe dataset that we are going to be using has been downloaded from Kaggle and can be found in the next link: https://www.kaggle.com/uciml/mushroom-classification. It is composed of 23 columns, one for the class (p = poisonous, e = edible) and 22 categorical features. For more information, check the previous link."},{"metadata":{},"cell_type":"markdown","source":"# IMPORTS"},{"metadata":{"trusted":true},"cell_type":"code","source":"import IPython\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n# Uncoment if you don't have kerastuner in your environment\n!pip install -q -U keras-tuner\nimport kerastuner as kt\n\n# Uncoment if you don't have tensorflow_docs in your environment\n!pip install -q git+https://github.com/tensorflow/docs\nimport tensorflow_docs as tfdocs\nimport tensorflow_docs.modeling\nimport tensorflow_docs.plots\n\n\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GET DATA"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/mushroom-classification/mushrooms.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All columns are categorical and there is no NaN values\nprint(data.shape)\ndata.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PREPROCESSING"},{"metadata":{},"cell_type":"markdown","source":"#### Note:\nThe maximum number of categories is useful to know to select what kind of categorical feature column use."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_categories_per_column = data.apply(lambda x : len(x.unique()), axis=1)\nprint('Max # categories = {}'.format(max(num_categories_per_column)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train - Test split"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(data, test_size=0.3, random_state=42)\ntrain, val = train_test_split(train, test_size=0.2, random_state=42)\n\nprint('# test = {}'.format(len(test)))\nprint('# train = {}'.format(len(train)))\nprint('# val = {}'.format(len(val)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_to_dataset(dataframe, shuffle=False, batch_size=32):\n    dataframe = dataframe.copy()\n    \n    # For a correct binary classification, we are going to \n    # replace string labels with 0 and 1.\n    labels = dataframe.pop('class').map({'e':1, 'p':0})\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    \n    if shuffle:\n        ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    \n    return ds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\ntrain_ds = df_to_dataset(train, shuffle=True, batch_size= BATCH_SIZE)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=BATCH_SIZE)\n\nN_TRAIN = len(train)\nSTEPS_PER_EPOCH = N_TRAIN // BATCH_SIZE\nFIT_MAX_EPOCHS = 200","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Select columns to train the model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This will be the columns used by the model\ncolumns_selected = ['cap-shape', 'cap-surface', 'cap-color',  \n                    'stalk-shape', 'stalk-surface-above-ring', 'stalk-surface-below-ring',\n                    'stalk-color-above-ring', 'stalk-color-below-ring']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BUILD HYPER MODEL "},{"metadata":{},"cell_type":"markdown","source":"### Optimizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"schedules = tf.keras.optimizers.schedules.InverseTimeDecay(\n                initial_learning_rate=0.001,\n                decay_steps=100*STEPS_PER_EPOCH,\n                decay_rate=1,\n                staircase=False\n            )\n\ndef get_optimizer():\n    return tf.keras.optimizers.Adam(schedules)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_callbacks():\n    return [\n        tfdocs.modeling.EpochDots(report_every=50),\n        tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=50)\n    ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_feature_layer(columns):\n    # columns = data.columns.drop('class') # Be careful with add 'class' column as it is the target.\n    feature_columns = []\n\n    for column in columns:\n        categorical_column = tf.feature_column.categorical_column_with_vocabulary_list(column, data[column].unique())\n        feature_columns.append(tf.feature_column.indicator_column(categorical_column))\n\n    feature_layer = layers.DenseFeatures(feature_columns)\n    \n    return feature_layer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### HyperModel"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MushroomHyperModel(kt.HyperModel):\n    \n    def __init__(self, columns, *args, **kwargs):\n        self.columns = columns\n    \n    def build(self, hp):\n        model = tf.keras.Sequential()\n        \n        # Feature layer\n        model.add(get_feature_layer(self.columns))\n\n        # Hidden layers (1 or 2 layers with 2, 6, 10 or 14 neurons)\n        for i in range(hp.Int('num_layers', 1, 2, step=1)):\n            hp_units = hp.Int('units_' + str(i), min_value=2, max_value=14, step=4)\n            model.add(layers.Dense(units=hp_units, activation='relu'))\n\n        # Output layer\n        model.add(layers.Dense(1, activation='sigmoid'))\n\n        # Compile model\n        model.compile(optimizer=get_optimizer(),\n                      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                      metrics=[\n                          tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                          'accuracy'\n                      ])\n\n        return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create the tuner"},{"metadata":{"trusted":true},"cell_type":"code","source":"hypermodel = MushroomHyperModel(columns_selected)\n\ntuner = kt.Hyperband(hypermodel,\n                     objective = 'val_binary_crossentropy', \n                     max_epochs = 200,\n                     project_name = 'hyper_cap_stalk')   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define a class to clear the output of the keras Tuner"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ClearTrainingOutput(tf.keras.callbacks.Callback):\n    def on_train_end(*args, **kwargs):\n        IPython.display.clear_output(wait = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Search the best structure with the tuner"},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks = [ClearTrainingOutput()]\ncallbacks.append(get_callbacks())\ntuner.search(train_ds, validation_data = val_ds, callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get optimal hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_num_layers = best_hps.get('num_layers')\nprint('# Layers = {}'.format(best_num_layers))\n\nfor i in range(best_num_layers):\n    print('# units = {}'.format(best_hps.get('units_' + str(i))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train a model with the optimal hyper parameters"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"history = {}\n\nhyper_model = tuner.hypermodel.build(best_hps)\nhistory['hyper'] = hyper_model.fit(train_ds, \n                                    validation_data=val_ds,\n                                    epochs=FIT_MAX_EPOCHS,\n                                    callbacks=get_callbacks(),\n                                    verbose=0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# REGULAR MODELS (not HyperModels)"},{"metadata":{},"cell_type":"markdown","source":"### Compile and fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"def compile_and_fit(model):\n    model.compile(optimizer=get_optimizer(),\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                  metrics=[\n                      tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                      'accuracy'\n                  ])\n\n    history = model.fit(train_ds, \n                        validation_data=val_ds,\n                        epochs=FIT_MAX_EPOCHS,\n                        callbacks=get_callbacks(),\n                        verbose=0)\n    \n    return history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Small model"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"small_model = tf.keras.models.Sequential([\n    get_feature_layer(columns_selected),\n    layers.Dense(6, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\nhistory['small'] = compile_and_fit(small_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"big_model = tf.keras.models.Sequential([\n    get_feature_layer(columns_selected),\n    layers.Dense(18, activation='relu'),\n    layers.Dense(18, activation='relu'),\n    layers.Dense(18, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\n\nhistory['big'] = compile_and_fit(small_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SHOW PERFORMANCE"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history(history):\n    plt.figure(figsize=(12,6))\n    plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n    plotter.plot(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CONCLUSION\nDepending on the execution, the HyperModel may have a slightly better performance and, although it is not so better than the small one, is still a good example of how to use Keras Tuner to find a good structure for our model. I hope you find this notebook useful! "},{"metadata":{},"cell_type":"markdown","source":"# THANKS FOR READING! :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}