{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Analysis of student's perfomance in math using Bayesian models","metadata":{}},{"cell_type":"markdown","source":"## Intro","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport matplotlib.pyplot as plt\n\nimport pymc3 as pm\nimport arviz as az\n\naz.style.use('arviz-darkgrid')\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import Ridge","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's just read data and have a first look on it","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/students-performance-in-exams/StudentsPerformance.csv')\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are three possible targets, but we'll be working with only one of them: math score.\n\nFirst of all we need to specify the task to be solved. Task specification for such kind of target is a well-known interview question, and the preffered answer is that this one is a *regression* problem. The reason is that classification algorithms cannot compare two mistakes, i.e. in case when correct answer is 65 the prediction of 64 is the same bad as of 25 from the classifications metrics point of view, but it's pretty obvious, that the prediction of 64 is not bad at all when the other one is just terrible.\n\nOkay, so we specify a target column and features columns. It should be noted, that all features are categorical.","metadata":{}},{"cell_type":"code","source":"df = df.drop(columns=['reading score', 'writing score'])\ny = df['math score']\nX = df[['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's OHE them. ``drop='first'`` argument means that the first category will be encoded as zeros, so that the number of columns for the feature with $n$ categories will be $n - 1$.","metadata":{}},{"cell_type":"code","source":"X_ohe = OneHotEncoder(drop='first', sparse=False).fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just common train-test split.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    X_ohe, y, test_size=0.1, random_state=123\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bayesian model","metadata":{}},{"cell_type":"markdown","source":"Okay, let's create Bayesian model:\n$$\nbias \\sim \\mathcal{N}(0, 20),\n$$\n$$\nw_i \\sim \\mathcal{N}(0, 5),\n$$\n$$\n\\sigma \\sim |\\mathcal{N}(0, 2)|,\n$$\n$$\n\\nu \\sim Exp(1),\n$$\n$$\ny \\sim t(\\nu = \\nu, \\mu = bias + w \\cdot X, sd = \\sigma).\n$$\n\n<p style=\"text-align: center;\"><img src=\"https://i.ibb.co/vvr7t5m/bayes-model-diag-page-0001.jpg\" alt=\"Model\" border=\"0\"></p>","metadata":{}},{"cell_type":"code","source":"DIM = 12\n\nwith pm.Model() as robust_linreg_model:\n    X_data = np.array([pm.Data(f\"X_data_{i+1}\", X_train[:, i]) for i in range(DIM)])\n    w0 = pm.Normal('w0', mu=0, sd=np.array(20))\n    w = np.array([pm.Normal(f'w{i+1}', mu=0, sd=np.array(10)) for i in range(DIM)])\n    sigma = pm.HalfNormal('sigma', sd=2)\n    nu = pm.Exponential('nu', 1)\n    outputs = pm.StudentT('y', mu=w0 + np.sum(w*X_data), sd=sigma, nu=nu, observed=y_train)\n    \npm.model_to_graphviz(robust_linreg_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with robust_linreg_model:\n    inf_data_robust = pm.sample(draws=2000, tune=2000, chains=2, cores=2, \n                         return_inferencedata=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"az.summary(inf_data_robust, round_to=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`r_hat` is perfect","metadata":{}},{"cell_type":"code","source":"az.plot_trace(inf_data_robust, compact=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"... and sampling is also well-looking.","metadata":{}},{"cell_type":"code","source":"az.plot_forest(inf_data_robust,\n               model_names = ['Robust Linreg'],\n               hdi_prob=0.95, figsize=(6, 4));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Confidence intervals are small enough","metadata":{}},{"cell_type":"markdown","source":"## Predictions","metadata":{}},{"cell_type":"code","source":"with robust_linreg_model:\n    pm.set_data({f'X_data_{i+1}': X_test[:, i] for i in range(12)})\n    samples_train = pm.sample_posterior_predictive(inf_data_robust)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's evaluate our prediction using mean squared error. But before doing this we need to round our predictions and somehow pick the final prediction. Let's say it would be the most common one.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import mode\nmse(y_test, mode(np.rint(samples_train['y']))[0][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing this to common Ridge Regression we see, that our model does it's job not much worse. That's cool :)","metadata":{}},{"cell_type":"code","source":"mse(y_test, Ridge().fit(X_train, y_train).predict(X_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}