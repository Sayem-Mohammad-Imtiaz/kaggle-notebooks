{"cells":[{"metadata":{"id":"CjfwXdxpde4A","outputId":"5dd41873-c590-4d62-e0d1-98f127e9694e","trusted":true},"cell_type":"code","source":"!wget https://machinehack-be.s3.amazonaws.com/product_sentiment_classification_weekend_hackathon_19/Participants_Data.zip","execution_count":null,"outputs":[]},{"metadata":{"id":"L0LNnwHRdy5B","outputId":"28784d1f-a8d0-4ffc-f027-107f8adbd58d","trusted":true},"cell_type":"code","source":"!unzip Participants_Data.zip","execution_count":null,"outputs":[]},{"metadata":{"id":"x4nDFTuktela","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"n79PrHkFti56","trusted":true},"cell_type":"code","source":"train = pd.read_csv('Participants_Data/Train.csv')\ntest = pd.read_csv('Participants_Data/Test.csv')\nsub = pd.read_csv('Participants_Data/Sample Submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"JjphIIX3e-TK","outputId":"214f85a1-8098-48e6-c3cb-38a1da5b3ecd","trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"xCXtCYNXfLzs","outputId":"c84f29d7-3b12-4aef-a453-3e7d692b8316","trusted":true},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"KafEM4fd0y82","outputId":"d7cf9f44-483a-4c6b-e231-10a576534f53","trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nwc = WordCloud(background_color='white',\n                    stopwords =  set(STOPWORDS),\n                    max_words = 50, \n                    random_state = 42,)\nwc.generate(' '.join(train['Product_Description']))\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ozidtg3ft-qi","outputId":"8d023496-01c8-4b7d-a73b-11a4cf97c8d1","trusted":true},"cell_type":"code","source":"train.isnull().sum(),test.isnull().sum(),train.shape,test.shape,train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"KorpVn_1tp5C","trusted":true},"cell_type":"code","source":"df=train.append(test,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"VXLBUhQzwRTD","outputId":"5ba36e29-e12c-48f0-84ad-467fe0f29fe3","trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\n#df['punctuation_count'] = df['Product_Description'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation)))\ndf['numerics'] = df['Product_Description'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ndf['upper'] = df['Product_Description'].apply(lambda x: len([x for x in x.split() if x.isupper()]))","execution_count":null,"outputs":[]},{"metadata":{"id":"lfr_3gbJ1lek","trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.tokenize import TreebankWordTokenizer\ncvec = TfidfVectorizer(max_features=10000, norm = 'l1', lowercase=True, smooth_idf=False, sublinear_tf=False, ngram_range=(1,4), tokenizer=TreebankWordTokenizer().tokenize)\ndf_info = pd.DataFrame(cvec.fit_transform(df['Product_Description']).todense())\ndf_info.columns = ['Product_Description_Top_' + str(c) for c in df_info.columns]\ndf = pd.concat([df, df_info], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"V2fccgYutsdr","outputId":"fae5c6fd-8886-4679-da7b-af5965669c43","trusted":true},"cell_type":"code","source":"import re\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'@[a-zA-Z0-9_]+', '', text)   \n    text = re.sub(r'https?://[A-Za-z0-9./]+', '', text)   \n    text = re.sub(r'www.[^ ]+', '', text)  \n    text = re.sub(r'[a-zA-Z0-9]*www[a-zA-Z0-9]*com[a-zA-Z0-9]*', '', text)  \n    text = re.sub(r'[^a-zA-Z]', ' ', text)   \n    text = [token for token in text.split() if len(token) > 2]\n    text = ' '.join(text)\n    return text\n\ndf['Product_Description'] = df['Product_Description'].apply(clean_text)\nfrom wordcloud import WordCloud, STOPWORDS\nwc = WordCloud(background_color='white',\n                    stopwords =  set(STOPWORDS),\n                    max_words = 50, \n                    random_state = 42,)\nwc.generate(' '.join(df['Product_Description']))\nplt.imshow(wc)","execution_count":null,"outputs":[]},{"metadata":{"id":"RjDa5toktyK2","trusted":true},"cell_type":"code","source":"import string\npunctuation=string.punctuation\ndf['word_count']=df['Product_Description'].apply(lambda x: len(str(x).split(\" \")))\ndf['char_count'] = df['Product_Description'].str.len()\ndef avg_word(sentence):\n    words = sentence.split()\n    return (sum(len(word) for word in words)/(len(words)+1))\n\ndf['avg_word'] = df['Product_Description'].apply(lambda x: avg_word(x))\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ndf['stopwords'] = df['Product_Description'].apply(lambda x: len([x for x in x.split() if x in stop]))\ndf['word_density'] = df['char_count'] / (df['word_count']+1)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"J3jHiW81ZbaT","trusted":true},"cell_type":"code","source":"j=[]\nfor i in df['Product_Description']:\n  j.append(len(i))\ndf['len']=j","execution_count":null,"outputs":[]},{"metadata":{"id":"x9AzpvUZyhuo","trusted":true},"cell_type":"code","source":"from textblob import TextBlob\ndf['polarity'] = df.apply(lambda x: TextBlob(x['Product_Description']).sentiment.polarity, axis=1)\ndf['subjectivity'] = df.apply(lambda x: TextBlob(x['Product_Description']).sentiment.subjectivity, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"OmL-qONPpQYo","trusted":true},"cell_type":"code","source":"df['ID_Type']=df['Product_Type']+df['Text_ID']","execution_count":null,"outputs":[]},{"metadata":{"id":"8EajvGnUHPoi","outputId":"9a668ec2-9b44-4e2a-e551-b157d28abfb9","trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"c4VAplFlt0Zl","trusted":true},"cell_type":"code","source":"del df['Product_Description']\ntrain = df[df['Sentiment'].isnull()==False]\ntest = df[df['Sentiment'].isnull()==True]\ndel test['Sentiment']","execution_count":null,"outputs":[]},{"metadata":{"id":"nKqBYqb5kAdK","outputId":"b5e43d68-102f-4023-891d-cb0879048c44","trusted":true},"cell_type":"code","source":"train['Sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"jw23O6u1YKIv","trusted":true},"cell_type":"code","source":"train_df=train\ntest_df=test","execution_count":null,"outputs":[]},{"metadata":{"id":"tEBtdP68YNzr","trusted":true},"cell_type":"code","source":"X = train_df.drop(labels=['Sentiment'], axis=1)\ny = train_df['Sentiment'].values","execution_count":null,"outputs":[]},{"metadata":{"id":"PXCdENQvYDc9","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.10, random_state=101, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"id":"rzaKa3bnZSmm","outputId":"2303d16d-3262-43d9-b0d4-6bb7ac0cd3e0","trusted":true},"cell_type":"code","source":"X_train.shape, y_train.shape, X_cv.shape, y_cv.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"KWM1s5V2ZWQJ","trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss","execution_count":null,"outputs":[]},{"metadata":{"id":"crMDQv3UZfin","outputId":"99695608-0c7f-44c8-c1e2-e8e52a299da0","trusted":true},"cell_type":"code","source":"\nimport lightgbm as lgb\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_cv, label=y_cv)\n\nparam = {'objective': 'multiclass',\n         'num_class': 4,\n         'boosting': 'gbdt',  \n         'metric': 'multi_logloss',\n         'learning_rate': 0.01, \n         'num_iterations': 1000,\n         'num_leaves': 31,\n         'max_depth': -1,\n         'min_data_in_leaf': 15,\n         'bagging_fraction':0.9,\n         'bagging_freq': 2,\n         'feature_fraction': 0.9,\n         'lambda_l2': 0.9,\n         'min_data_per_group': 75,\n         'max_bin': 255,\n         'is_unbalance':True\n         }\n\nclf = lgb.train(params=param, \n                early_stopping_rounds=200,\n                verbose_eval=100,\n                train_set=train_data,\n                valid_sets=[test_data])\n\ny_pred = clf.predict(X_cv)","execution_count":null,"outputs":[]},{"metadata":{"id":"Co0ydv6fZjcQ","outputId":"bf811802-919e-4dc9-f462-262a32004f59","trusted":true},"cell_type":"code","source":"log_loss(y_cv, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"SdDK3wn0Z8Ig","trusted":true},"cell_type":"code","source":"Xtest = test_df","execution_count":null,"outputs":[]},{"metadata":{"id":"VEF6nOsjZ_-L","outputId":"cd02ec9f-81b5-49a5-ee96-fe1fcb935c10","trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nerrlgb = []\ny_pred_totlgb = []\n\nfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=2**31)\n\nfor train_index, test_index in fold.split(X, y):\n    \n    X_train, X_test = X.loc[train_index], X.loc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    test_data = lgb.Dataset(X_test, label=y_test)\n    \n    clf = lgb.train(params=param, \n                     early_stopping_rounds=200,\n                     verbose_eval=100,\n                     train_set=train_data,\n                     valid_sets=[test_data])\n\n    y_pred = clf.predict(X_test)\n    print(\"Log Loss: \", (log_loss(y_test, y_pred)))\n    \n    errlgb.append(log_loss(y_test, y_pred))\n    p = clf.predict(Xtest)\n    y_pred_totlgb.append(p)","execution_count":null,"outputs":[]},{"metadata":{"id":"k92XdbGRaCsw","outputId":"faf8ceb2-d829-4df5-dd83-3b5df6613e6d","trusted":true},"cell_type":"code","source":"np.mean(errlgb,0)","execution_count":null,"outputs":[]},{"metadata":{"id":"mIDmFAHxxECl","outputId":"19da2c5e-9cae-4078-d372-15b85f4ad2fe","trusted":true},"cell_type":"code","source":"'''\nx=[]\nfor i in errlgb:\n  if i>0.43:\n    xx=errlgb.index(i)\n    x.append(xx)\nx=sorted(x, reverse=True)\nprint(x)\nfor i in x:\n  del y_pred_totlgb[i]\n  del errlgb[i]\n'''","execution_count":null,"outputs":[]},{"metadata":{"id":"7CJMbgevbQtl","trusted":true},"cell_type":"code","source":"y_pred = np.mean(y_pred_totlgb,0)","execution_count":null,"outputs":[]},{"metadata":{"id":"LWFDdtpybUqj","outputId":"86f36a28-60e8-40bf-9b98-8f4f9a3a9a69","trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(data=y_pred, columns=sub.columns)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"6PbcdiA9kDnS","trusted":true},"cell_type":"code","source":"submission.to_csv('Mh13.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}