{"cells":[{"metadata":{"id":"tZRSiDYNQLSr","trusted":false},"cell_type":"code","source":"# ","execution_count":null,"outputs":[]},{"metadata":{"id":"tvTemFCOPqr4"},"cell_type":"markdown","source":"applying deep learning on twitterâ€™s sentiment analysis\n\n*   Train Model - use keras to build and train a deep neural network model\n\n*   Evaluate Model - measure the accuracy of the predictive model, and suggest further improvements\n"},{"metadata":{"id":"83bZgjYjwUZk"},"cell_type":"markdown","source":"IMPORTING DATASET\n"},{"metadata":{"id":"87T5BetZQYQv","trusted":false},"cell_type":"code","source":"from time import time\nimport pandas as pd\nimport numpy as np\nimport re\nimport csv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport itertools\nimport datetime\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn import decomposition, ensemble\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport pandas, xgboost, numpy, textblob, string\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\n","execution_count":null,"outputs":[]},{"metadata":{"id":"vGgPw6lXwgIK","outputId":"801d8a5c-c22b-4bfa-fa86-560c88f98c6a","trusted":false},"cell_type":"code","source":"#being able to read csv stored in google drive \nfrom google.colab import drive\ndrive.mount(\"/content/drive\")","execution_count":null,"outputs":[]},{"metadata":{"id":"fMujS02_wcVJ","outputId":"bbc0ab0d-b386-44aa-f2a1-d7363cbb5e60","trusted":false},"cell_type":"code","source":"# Reading the dataset with no columns titles and with latin encoding \ndf = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NLP/tweetsClean.csv')\ndf.sample(3)\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"YOckP_sYQcda","outputId":"ec0f347e-7652-401f-8794-f130523b710b","trusted":false},"cell_type":"code","source":"# Checking if there is any missing value and datatype \ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"_F9FfyFSQl_5","outputId":"c00d08d4-2366-456b-b556-1bb8c520f859","trusted":false},"cell_type":"code","source":"\n# checking for null values, if any\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"xalsFDNyQp66","trusted":false},"cell_type":"code","source":"#ditching all row when text is null, as need text for analysis\ndf.dropna(how='any', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"XryjXIYhQ_gM","outputId":"78ee8686-ce37-485e-b323-876d864ef567","trusted":false},"cell_type":"code","source":"df.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"id":"jcwO8vMB4vNr"},"cell_type":"markdown","source":"testing some embedding for deep learning"},{"metadata":{"id":"wRa095DbE6J0","outputId":"b843e8fa-2448-4823-a109-733bac3ddc4c","trusted":false},"cell_type":"code","source":"#bag of words = OPTION A - need to limit to 1000, else too long and gets 951k results\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=500, stop_words='english',analyzer='word', token_pattern=r'\\w{1,}')\nX = bow_vectorizer.fit_transform(df['clean'])\nX.shape\n","execution_count":null,"outputs":[]},{"metadata":{"id":"4Zo-_Rl0gv3C","outputId":"e6350e58-a2db-40ba-9a0c-da85ffae38c2","trusted":false},"cell_type":"code","source":"#2- CREATING a FAKE Y\n#ate 11 dec 2017\n#ate 10 dec 2018\n#ate end\n\ndef senti(x):\n  if x < 2018:\n    return 'BULL'\n  elif x > 2018:\n    return 'BULL'\n  else:\n    return 'BEAR'\n\ndf['sent'] = df['year'].apply(lambda x: senti(x) )\ndf.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"id":"EnGdKtu3gz6C","trusted":false},"cell_type":"code","source":"#y = np.array(df['sent']).ravel()\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(df['sent'])","execution_count":null,"outputs":[]},{"metadata":{"id":"rVzb6Gt0gDFs","trusted":false},"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras import layers\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=500, output_dim=100))  # We have 2000 words in the vocabulary, \n                                                        # and each word is represnted by a vector of size 30\nmodel.add(layers.LSTM(50, activation='tanh'))\nmodel.add(layers.Dense(20, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"id":"EUrxvsONgPdu","outputId":"5037dfc5-7681-40a7-e6fa-369a3388e003","trusted":false},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nes = EarlyStopping(patience=20)\n\nmodel.fit(df['clean'],y,epochs=100, batch_size=16,validation_split=0.3,\n          callbacks=[es], verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"OYnw7FRjczR1","trusted":false},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"cKN6ATJgIVSq"},"cell_type":"markdown","source":"CREATE A NEURAL NETWORK TEST "},{"metadata":{"id":"hhEsotp6YmIt","outputId":"10157b88-9a0e-43e5-c412-2a5795fd12de","trusted":false},"cell_type":"code","source":"# ENCODING X with ngram level tf-idf \nvect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=500)\nngram = vect_ngram.fit_transform(df['tags'])\nngram.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"mS4OFQPjYoLD","trusted":false},"cell_type":"code","source":"#X and Y and split\nX_ngram_train, X_ngram_test, y_train, y_test = train_test_split(ngram, y, test_size=0.3, random_state=30)","execution_count":null,"outputs":[]},{"metadata":{"id":"MYEa8nx7IYrK","trusted":false},"cell_type":"code","source":"def create_model_architecture(input_size):\n    # create input layer \n    input_layer = layers.Input((input_size, ), sparse=True)\n    \n    # create hidden layer\n    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n    \n    # create output layer\n    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n\n    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    return classifier ","execution_count":null,"outputs":[]},{"metadata":{"id":"T2mO4_A0j4TW","trusted":false},"cell_type":"code","source":"def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=True):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n    \n    return metrics.accuracy_score(predictions, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"t_w1gZdJahkx","outputId":"b8bb02a7-8ab0-4cb5-86b6-ba07cdfa3077","trusted":false},"cell_type":"code","source":"classifier = create_model_architecture(X_ngram_train.shape[1])\naccuracy = train_model(classifier,X_ngram_train, y_train, X_ngram_test)\nprint(\"NN, Ngram Level TF IDF Vectors\",  accuracy)","execution_count":null,"outputs":[]},{"metadata":{"id":"5XuPXbosIkZY"},"cell_type":"markdown","source":"CREATE A CNN"},{"metadata":{"id":"PGqA85rYZAwo","trusted":false},"cell_type":"code","source":"# load the pre-trained word-embedding vectors \nembeddings_index = {}\nfor i, line in enumerate(open('data/wiki-news-300d-1M.vec')):\n    values = line.split()\n    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n\n# create a tokenizer \ntoken = text.Tokenizer()\ntoken.fit_on_texts(trainDF['text'])\nword_index = token.word_index\n\n# convert text to sequence of tokens and pad them to ensure equal length vectors \ntrain_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\nvalid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n\n# create token-embedding mapping\nembedding_matrix = numpy.zeros((len(word_index) + 1, 300))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"id":"hRNcUsoCZCBk","trusted":false},"cell_type":"code","source":"#X and Y and split","execution_count":null,"outputs":[]},{"metadata":{"id":"KrLK3CfAInOX","trusted":false},"cell_type":"code","source":"def create_cnn():\n    # Add an Input Layer\n    input_layer = layers.Input((70, ))\n\n    # Add the word embedding Layer\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n    # Add the convolutional Layer\n    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n\n    # Add the pooling Layer\n    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n\n    # Add the output Layers\n    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\n    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n    # Compile the model\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\n    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    \n    return model\n\nclassifier = create_cnn()\naccuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint \"CNN, Word Embeddings\",  accuracy","execution_count":null,"outputs":[]},{"metadata":{"id":"9_zObs-JIyTw"},"cell_type":"markdown","source":"CREATE A LTSM"},{"metadata":{"id":"593lgrQBI6pq","trusted":false},"cell_type":"code","source":"def create_rnn_lstm():\n    # Add an Input Layer\n    input_layer = layers.Input((70, ))\n\n    # Add the word embedding Layer\n    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n    # Add the LSTM Layer\n    lstm_layer = layers.LSTM(100)(embedding_layer)\n\n    # Add the output Layers\n    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n    output_layer1 = layers.Dropout(0.25)(output_layer1)\n    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n    # Compile the model\n    model = models.Model(inputs=input_layer, outputs=output_layer2)\n    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n    \n    return model\n\nclassifier = create_rnn_lstm()\naccuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint \"RNN-LSTM, Word Embeddings\",  accuracy\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}