{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport os\nimport gc\n\n# TODO:\n# 1. Use data augmentation\n# 2. Fine-tune best model on data augmentation\n# 3. Evaluate on test set","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_fer = pd.read_csv('../input/fer2013/fer2013.csv')\ndata_fer.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\nidx_to_emotion_fer = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\", 6:\"Neutral\"}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_fer_train, y_fer_train = np.rollaxis(data_fer[data_fer.Usage == \"Training\"][[\"pixels\", \"emotion\"]].values, -1)\nX_fer_train = np.array([np.fromstring(x, dtype=\"uint8\", sep=\" \") for x in X_fer_train]).reshape((-1, 48, 48))\ny_fer_train = y_fer_train.astype('int8')\n\nX_fer_test_public, y_fer_test_public = np.rollaxis(data_fer[data_fer.Usage == \"PublicTest\"][[\"pixels\", \"emotion\"]].values, -1)\nX_fer_test_public = np.array([np.fromstring(x, dtype=\"uint8\", sep=\" \") for x in X_fer_test_public]).reshape((-1, 48, 48))\ny_fer_test_public = y_fer_test_public.astype('int8')\n\nX_fer_test_private, y_fer_test_private = np.rollaxis(data_fer[data_fer.Usage == \"PrivateTest\"][[\"pixels\", \"emotion\"]].values, -1)\nX_fer_test_private = np.array([np.fromstring(x, dtype=\"uint8\", sep=\" \") for x in X_fer_test_private]).reshape((-1, 48, 48))\ny_fer_test_private = y_fer_test_private.astype('int8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"X_fer_train shape: {X_fer_train.shape}; y_fer_train shape: {y_fer_train.shape}\")\nprint(f\"X_fer_test_public shape: {X_fer_test_public.shape}; y_fer_test_public shape: {y_fer_test_public.shape}\")\nprint(f\"X_fer_test_private shape: {X_fer_test_private.shape}; y_fer_test_private shape: {y_fer_test_private.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_counts = np.bincount(y_fer_train)\nx_ticks = np.arange(len(class_counts))\n\nplt.bar(x_ticks, class_counts)\nplt.xticks(x_ticks, idx_to_emotion_fer.values())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(X_fer_train[10], interpolation='none', cmap='gray')\nplt.title(idx_to_emotion_fer[y_fer_train[10]])\nplt.show()\nplt.imshow(X_fer_test_public[10], interpolation='none', cmap='gray')\nplt.title(idx_to_emotion_fer[y_fer_test_public[10]])\nplt.show()\nplt.imshow(X_fer_test_private[10], interpolation='none', cmap='gray')\nplt.title(idx_to_emotion_fer[y_fer_test_private[10]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications import VGG16\nfrom keras.models import Model, Sequential\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot(y):\n    return to_categorical(y, 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_history(history, metrics):\n    fig, ax = plt.subplots(1, 1+len(metrics), figsize=(20, 5))\n    ax[0].plot(history.history['loss'], label='Train loss')\n    ax[0].plot(history.history['val_loss'], label='Validation loss')\n    ax[0].legend()\n        \n    for i, metric in enumerate(metrics):\n        ax[i+1].plot(history.history[metric], label='Train %s' % metric)\n        ax[i+1].plot(history.history['val_%s' % metric], label='Validation %s' % metric)\n        ax[i+1].legend()\n    \n    plt.show()\n\n# TODO: delete sample history\n# class History:\n#     def __init__(self):\n#         self.history = dict({\n#             'categorical_accuracy': list(range(10)),\n#             'val_categorical_accuracy': [-i*2 for i in range(10)],\n#             'loss': list(range(10)),\n#             'val_loss': [-i*2 for i in range(10)]\n#         })\n\n# plot_history(History(), metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/rcmalli/keras-vggface.git\nfrom keras_vggface.vggface import VGGFace\nfrom keras_vggface import utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VGGFace(include_top = False, input_shape = (48,48,3),pooling = 'avg').summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reshape and encode training data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train = X_fer_train.reshape((-1, 48, 48, 1)), one_hot(y_fer_train)\nX_val, y_val = X_fer_test_public.reshape((-1, 48, 48, 1)), one_hot(y_fer_test_public)\n\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Take a small part of data for debugging"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_small_train, _, y_small_train, _ = train_test_split(X_train, y_train, train_size=0.9)\n\ndef create_normalize(mean, std):\n    def normalize(X):\n        return (X - mean) / std\n    return normalize\n\n# X_mean = X_train.mean(axis=0)\n# X_std = X_train.std(axis=0)\n\n# X_small_val, _, y_small_val, _ = train_test_split(X_val, y_val, train_size=0.9)\n\n# normalize = create_normalize(X_mean, X_std)\n\n# X_train_norm = normalize(X_train)\n# X_val_norm = normalize(X_val)\n\n# print(X_train_norm.shape, X_val_norm.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.preprocessing.image import ImageDataGenerator\n\n# gen = ImageDataGenerator(featurewise_center=True,\n#                          samplewise_center=False,\n#                          featurewise_std_normalization=True,\n#                          samplewise_std_normalization=False,\n#                          zca_whitening=False,\n#                          brightness_range=(0.3, 0.8),\n#                          horizontal_flip=True,\n#                          vertical_flip=False,\n#                          validation_split=0.1)\n\n# gen.fit(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Mix train and validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test, y_test = X_fer_test_private.reshape((-1, 48, 48, 1)), one_hot(y_fer_test_private)\n\nX_train_all = np.concatenate((X_train, X_val), axis=0)\ny_train_all = np.concatenate((y_train, y_val), axis=0)\n\nX_train_mix, X_val_mix, y_train_mix, y_val_mix = \\\n    train_test_split(X_train_all, y_train_all, test_size=0.1)\n\nprint(X_train_mix.shape, y_train_mix.shape, X_val_mix.shape, y_val_mix.shape)\n\nX_mix_mean = X_train_mix.mean(axis=0)\nX_mix_std = X_train_mix.std(axis=0)\n\nnormalize_mix = create_normalize(X_mix_mean, X_mix_std)\nX_train_mix_norm = normalize_mix(X_train_mix)\nX_val_mix_norm = normalize_mix(X_val_mix)\n\nX_test_norm = normalize_mix(X_test)\n\nprint(X_train_mix_norm.shape, y_train_mix.shape, X_val_mix_norm.shape, y_val_mix.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model selection and training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.metrics import Precision, Recall, CategoricalAccuracy\nfrom keras.layers import Flatten, Dense, Input, Concatenate, Dropout, BatchNormalization, ReLU\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.regularizers import l2\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_small, X_rest, y_train_small, y_rest = train_test_split(X_train_mix_norm, y_train_mix, train_size=0.2)\n# X_val_small, _, y_val_small, _ = train_test_split(X_rest, y_rest, train_size=0.02)\n\n# print(X_train_small.shape, X_val_small.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compose_model(feature_extractor, reg=0.0):\n    conv_output = feature_extractor(img_conc)\n\n    dense_1   = Dense(1024, kernel_regularizer=l2(reg))(conv_output)\n    bn_1      = BatchNormalization()(dense_1)\n    relu_1    = ReLU()(bn_1)\n    dropout_1 = Dropout(0.5)(relu_1)\n    \n    dense_2   = Dense(1024, activation='relu', kernel_regularizer=l2(reg))(dropout_1)\n    out       = Dense(7, activation='softmax')(dense_2)\n\n    return Model(inputs=img_input, outputs=out)\n\ndef train(params):\n    print('training with {} params'.format(params))\n    \n    vgg_features = VGGFace(weights='vggface', include_top=False, input_shape=(48,48,3), pooling='max')\n    # for x in vggfeatures.layers[:-5]: # [:-5] [:-9]\n    #     x.trainable = False\n    model = compose_model(vgg_features, reg=params['reg'])\n\n    model.compile(loss='categorical_crossentropy', \n                  optimizer=Adam(lr=params['lr']), \n                  metrics=['categorical_accuracy'])\n\n    batch_size = params['batch_size']\n\n    cat_weights = class_weight.compute_class_weight(\n        'balanced', np.unique(y_fer_train), y_fer_train)\n\n    history = model.fit(\n        X_train_mix_norm,\n        y_train_mix,\n        batch_size=batch_size,\n        epochs=10,\n        validation_data=(X_val_mix_norm, y_val_mix),\n        class_weight=cat_weights)\n    \n    return model, history\n\nparams = {\n    'reg': 0.5,\n    'lr': 5e-5,\n    'batch_size': 64\n}\n\nmodel, history = train(params)\n\n# params = {\n#     'reg': [0.2, 0.4, 0.6, 0.8],\n#     'lr': [1e-3, 3e-4, 1e-4, 1e-5],\n#     'batch_size': [64, 128, 256]\n# }\n\n# scores = []\n\n# for reg in params['reg']:\n#     for lr in params['lr']:\n#         for batch_size in params['batch_size']:\n#             config = {\n#                 'reg': reg,\n#                 'lr': lr,\n#                 'batch_size': batch_size\n#             }\n\n#             score = train(config)\n#             scores.append([score, config])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# max_score = 0\n# best_config = None\n\n# for pair in scores:\n#     score, config = pair\n#     if score > max_score:\n#         max_score = score\n#         best_config = config\n        \n# print(max_score, best_config)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_history(history, metrics=['categorical_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: visualize weights","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport seaborn as sns\n\n# X_test, y_test = X_fer_test_private.reshape((-1, 48, 48, 1)), one_hot(y_fer_test_private)\n\n\ndef evaluate(model):\n    labels = idx_to_emotion_fer.values()\n    \n    y_pred = model.predict(X_test_norm)\n\n    y_true_cat = np.argmax(y_test, axis=1)\n    y_pred_cat = np.argmax(y_pred, axis=1)\n    \n    report = classification_report(y_true_cat, y_pred_cat)\n    print(report)\n\n    conf = confusion_matrix(y_true_cat, y_pred_cat)\n    conf = conf / np.max(conf)\n\n    _, ax = plt.subplots(figsize=(8, 6))\n    ax = sns.heatmap(conf, annot=True, cmap='YlGnBu', \n                     xticklabels=labels, \n                     yticklabels=labels)\n    plt.show()\n    \n    return report\n\n\nreport = evaluate(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Saving model structure and weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\nmodel_json = model.to_json()\nwith open('model.json', 'w') as f:\n    f.write(model_json)\n\nwith open('params.json', 'w') as f:\n    f.write(json.dumps({\n        'reg': 0.5,\n        'lr': 5e-5,\n        'batch_size': 64\n    }))\n    \nwith open('report.txt', 'w') as f:\n    f.write(report)\n    \nmodel.save_weights('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}