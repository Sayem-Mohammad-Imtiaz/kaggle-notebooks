{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# architecture reference: \n# https://github.com/znxlwm/pytorch-MNIST-CelebA-GAN-DCGAN\n# https://github.com/lyeoni/pytorch-mnist-GAN\n\n# implementing Vanilla GAN on MNIST\n\n# 1. libraries loading\n# 2. data prepare + batch_size + num of iteration + epochs\n# 3. defining Generator and Discriminator models \n# 4. instantiate Generator and Discriminator + loss function \n#    + learning_rate + optimizers\n# 5. training D and G\n# 6. visualize samples generated by G","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# 1. libraries loading\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nfrom torch.autograd import Variable\nfrom sklearn.model_selection import train_test_split\nfrom torchvision.utils import save_image\nfrom timeit import default_timer as timer\nimport time\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2. data loading + batch_size + epochs + batch_num\n\n# gpu or cpu\ndtype = torch.FloatTensor\ndtype = torch.cuda.FloatTensor # uncomment this to run on GPU\n\n# loading data + normalize pixels' values between -1 and +1\ndata = pd.read_csv('../input/mnist_train.csv')\n#labels = data.label.to_numpy()\nimg_digits = (data.loc[:, data.columns != 'label'].values/255 - 0.5)*2\n\n# batch_size + iteration num\nbatch_size = 128\nepochs_num = 200\n\n# change DataFrame to numpy\ndigits_Tensor = torch.Tensor(img_digits).type(dtype)\n\n# build Dataset\ndigits_DataSet = torch.utils.data.TensorDataset(digits_Tensor)\n\n# build DataLoader\ndigits_DataLoader = torch.utils.data.DataLoader(digits_DataSet, batch_size = batch_size)\n\nbatch_num = len(digits_DataLoader) # numbers of mini batches\n\n# visualize data\nfirst_four = np.append(np.append(img_digits[0].reshape(28,28), img_digits[1].reshape(28,28), axis=0), np.append(img_digits[2].reshape(28,28), img_digits[3].reshape(28,28), axis = 0), axis=1)\nplt.imshow(first_four)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define functions to return mini batch of real data, noise\n# sequentially return mini batch of real data from DataLoader after each call\ndef sample_real():\n    while True:\n        for i, [batch] in enumerate(digits_DataLoader):\n            yield batch\nsample_real = sample_real() # sample_real now is a generator\n\n# randomly return a batch of noise\ndef sample_noise(size=batch_size):\n    batch = torch.from_numpy(np.random.randn(size,100)).type(dtype)\n    return batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. defining Generator and Discriminator\n\n# generator: 100 -> 256 -> 512 -> 1024 -> 784\nclass Generator(nn.Module):\n    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):\n        super(Generator, self).__init__()\n        \n        # 1st layer\n        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n        self.ac1 = nn.LeakyReLU(negative_slope=0.2)\n        # 2nd layer\n        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n        self.ac2 = nn.LeakyReLU(negative_slope=0.2)\n        # 3rd layer\n        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n        self.ac3 = nn.LeakyReLU(negative_slope=0.2)\n        # 4th layer (readout)\n        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n        self.ac4 = nn.Tanh()\n        \n    def forward(self, x):\n        out1 = self.ac1(self.fc1(x))\n        out2 = self.ac2(self.fc2(out1))\n        out3 = self.ac3(self.fc3(out2))\n        out = self.ac4(self.fc4(out3))\n        \n        return out\n    \n# discriminator: 784 -> 1024 -> 512 -> 256 -> 1\nclass Discriminator(nn.Module):\n    def __init__(self,input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):\n        super(Discriminator, self).__init__()\n        \n        # 1st layer\n        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n        self.ac1 = nn.LeakyReLU(negative_slope=0.2)\n        self.dropout1 = nn.Dropout(p=0.3)\n        # 2nd layer\n        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n        self.ac2 = nn.LeakyReLU(negative_slope=0.2)\n        self.dropout2 = nn.Dropout(p=0.3)\n        # 3rd layer\n        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n        self.ac3 = nn.LeakyReLU(negative_slope=0.2)\n        self.dropout3 = nn.Dropout(p=0.3)\n        # 4th layer (readout)\n        self.fc4 = nn.Linear(hidden_dim3, output_dim)\n        self.ac4 = nn.Sigmoid()\n        \n    def forward(self, x):\n        out1 = self.dropout1(self.ac1(self.fc1(x)))\n        out2 = self.dropout2(self.ac2(self.fc2(out1)))\n        out3 = self.dropout3(self.ac3(self.fc3(out2)))\n        out = self.ac4(self.fc4(out3))\n        \n        return out\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. instantiate G and D; loss function; learning_rate; optimizers\n\n# generator + discriminator\nG = Generator(100, 256, 512, 1024, 784).type(dtype)\nD = Discriminator(784, 1024, 512, 256, 1).type(dtype)\n\n# loss function: Binary Cross Entropy Loss\nerror = nn.BCELoss()\n\n# learning rate\nlearning_rate = 0.0002\n\n# optimizers: \nG_optimizer = torch.optim.Adam(G.parameters(), lr = learning_rate)\nD_optimizer = torch.optim.Adam(D.parameters(), lr = learning_rate)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize image generated by generator before training\nbefore_noise = sample_noise(1)\nbefore_result = G(before_noise).cpu() # copy tensor to host memory\n\nbefore_img = before_result.detach().numpy().reshape(28,28)\nplt.imshow(before_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5. training D and G\n\n# list to store loss, iter\nG_loss_list = []\nD_real_loss_list = []\nD_fake_loss_list = []\niter_list = []\n\n# function to save digit images generated by G after each epoch\ndef G_generate(epoch, size=batch_size):\n    generated = G(sample_noise(size)).cpu() #copy tensor to host\n    save_image(generated.view(generated.size(0), 1, 28, 28), 'epoch' + str(epoch) + '.png')\n\nall_start = timer()\nfor epoch in range(epochs_num):\n    epoch_start = timer()\n    for it, [real_batch] in enumerate(digits_DataLoader):\n        \n        ##### TRAIN D  #####\n        # on real batch\n        # clear gradients\n        D_optimizer.zero_grad()\n        # forward propagation\n        real_out = D(real_batch)\n        # soft label for real batch: 1\n        real_label = torch.ones(real_out.shape[0], 1).type(dtype)\n        # loss\n        D_real_loss = error(real_out, real_label)\n        # back propagation\n        D_real_loss.backward()\n        # update parameters\n        D_optimizer.step()\n    \n        # on fake batch\n        # sample noise + generate fake batch\n        noise_batch = sample_noise()\n        fake_batch = G(noise_batch)\n        # clear gradients\n        D_optimizer.zero_grad()\n        # forward propagation\n        fake_out = D(fake_batch)\n        # soft label for fake batch: 0\n        fake_label = torch.zeros(fake_out.shape[0], 1).type(dtype)\n        # loss\n        D_fake_loss = error(fake_out, fake_label)\n        # back propagation\n        D_fake_loss.backward()\n        # update parameters\n        D_optimizer.step()        \n        \n        ##### TRAIN G #####\n        # sample noise + generate fake batch\n        noise_batch = sample_noise()\n        fake_batch = G(noise_batch)\n        # clear gradients\n        G_optimizer.zero_grad()\n        # forward propagation\n        fake_out = D(fake_batch)\n        # soft label for fake batch: 1\n        fake_label = torch.ones(fake_out.shape[0], 1).type(dtype)\n        # loss \n        G_loss = error(fake_out, fake_label)\n        # back propagation\n        G_loss.backward()\n        # update parameters\n        G_optimizer.step()\n        \n        ##### store loss, it #####\n        if it % 500 == 0:\n            iter_list.append(it+batch_num*epoch)\n            G_loss_list.append(G_loss.data)\n            D_fake_loss_list.append(D_fake_loss.data)\n            D_real_loss_list.append(D_real_loss.data)\n                    \n            #print('Epoch {} [{}/{}]:'.format(epoch, it, batch_num))\n\n    print('Epoch {} :'.format(epoch))  \n    print('    G_loss: {}'.format(G_loss.data))\n    print('    D_fake_loss: {}'.format(D_fake_loss.data))\n    print('    D_real_loss: {}'.format(D_real_loss.data))\n            \n    epoch_duration = timer() - epoch_start\n    G_generate(epoch)\n    print('##### TRAIN EPOCH {} IN {} s #####'.format(epoch, epoch_duration))\n    \nall_duration = timer() - all_start\nprint('ALL TRAINING TIME: {}'.format(all_duration))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot D and G loss\nplt.plot(iter_list, G_loss_list, 'r')\nplt.plot(iter_list, D_real_loss_list, 'b')\nplt.plot(iter_list, D_fake_loss_list, 'g')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6. visualize samples generated by G\ngenerated = G(sample_noise()).cpu()\nimg_test = generated[0].detach().numpy().reshape(28,28)\nplt.imshow(img_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}