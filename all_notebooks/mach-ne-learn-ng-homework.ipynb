{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/column_2C_weka.csv\",sep = \",\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"color_list = ['red' if i == \"Abnormal\" else 'green' for i in data.loc[: ,'class']]\npd.plotting.scatter_matrix(data.loc[:,data.columns != 'class'],\n                          c = color_list,\n                          figsize = [15,15],\n                          diagonal = 'hist',\n                          alpha = 0.5,\n                          s = 200,\n                          marker = '*',\n                          edgecolor = \"black\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = \"class\",data=data)\ndata.loc[:,'class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LINEAR REGRESSION**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx  = data.loc[:,data.columns != 'class']\ny = data.loc[:,\"class\"]\n\n# x = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values\n\n\nprint(x.shape)\nprint(y.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import  train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3 , random_state = 1)\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=3)\n\nknn.fit(x_train,y_train)\n\nprediction = knn.predict(x_test)\n\nprint(\"{} knn score : {}\".format(3,knn.score(x_test,y_test)))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(x_train),len(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model complexity\nneig = np.arange(1,25)\ntrain_accuracy = []\ntest_accuracy = []\n\nfor i ,k in enumerate(neig):\n    knn=KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    train_accuracy.append(knn.score(x_train,y_train))\n    test_accuracy.append(knn.score(x_test,y_test))\n    \n#plot\n\nplt.figure(figsize = [13,8])\nplt.plot(neig,test_accuracy,label='testing accuracy')\nplt.plot(neig,train_accuracy,label='train accuracy')\nplt.legend()\nplt.title('values vs accuracy')\nplt.xlabel('number of neighbours')\nplt.ylabel('accuracy')\nplt.xticks(neig)\nplt.show()\nprint('best accuracy is {} with k = {}'.format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.loc[:,\"class\"] = [1 if each == \"Abnormal\" else 0 for each in data.loc[:,\"class\"]]\ndata.loc[:,\"class\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.loc[:,\"class\"]\nx_data = data.drop([\"class\"],axis = 1)\n\nprint(x_data.shape)\nprint(y.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NORMALİZATİON**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = (x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data)).values\nx\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state = 42)\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T.values.reshape(248,1)\ny_test = y_test.T.values.reshape(62,1)\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#parameter initialize and sigmoid function\n#dimension = 30\n\ndef initialize_weights_and_bias(dimension):\n    w =np.full((dimension,1),0.01)\n    #np.full içine aldığı (x,y),z ile x e y boyutlu z lerden oluşan bir arrray yapar\n    b=0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z)) #aynı zamanda bu sigmoid fonksiyon \n    #np.exp e üzeri demek\n    return y_head\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    #forward propagation\n    z = np.dot(w.T,x_train)+b #?\n    #np.dot matrix çarpımında kullanılıyor\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1] #?\n    \n    #backward propagaiton\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\":derivative_bias}\n    return cost, gradients\n\n    \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#update\ndef update (w,b,x_train,y_train,learning_rate,number_of_iteration):\n    cost_list=[]\n    cost_list2=[]\n    index=[]\n    \n    #updating(learning) parameters is number_of_iteration time\n    for i in range(number_of_iteration):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n    #lets upgrade\n    \n    w=w-learning_rate*gradients[\"derivative_weight\"]\n    b=b-learning_rate*gradients[\"derivative_bias\"]\n    \n    if i%10 == 0:\n        cost_list2.append(cost)\n        index.append(i)\n        print(\"cost after iteration %i:%f\" %(i,cost))\n        \n    #we update(learn) parameters weight and bias\n\n    parameters = {\"weight\":w,\"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"number of iteration\")\n    plt.ylabel(\"cost\")\n    plt.show()\n    return parameters,gradients,cost_list\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction\ndef predict(w,b,x_test):\n    z=sigmoid(np.dot(w.T,x_test)+b)\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            y_predction[0,i]=0\n        else:\n            y_prediction[0,i]=1\n        return y_prediction\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n    dimension=x_train.shape[0]\n    w,b=initialize_weight_and_bias(dimension)\n    parameters,gradients,cost_list = update(w,b,x_train,y_train,learning_rate,num_iterations)\n    y_prediction_test=predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n   \n    dimension =  x_train.shape[0]  \n    w,b = initialize_weights_and_bias(dimension)\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    #print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    #print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate = 0.01, num_iterations = 150)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SUPPORT VECTOR MACHINE"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}