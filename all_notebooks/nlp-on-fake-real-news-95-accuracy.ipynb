{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"F = pd. read_csv('../input/fake-and-real-news-dataset/Fake.csv')\nT = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Texts length of the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"F['text'].apply(len).mean() \n# Fake News Text length mean.. how long is the text of the news","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"T['text'].apply(len).mean()\n# True News Text length mean.. how long is the text of the news","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* # Found difference of 200 words\n* # trying to see if this can be used as an indicator","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"F['text'].apply(len).std() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"T['text'].apply(len).mean() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparing Title lengths","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"F['title'].apply(len).mean()\n# Fake News Title length mean.. how long is the title of the news","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"T['title'].apply(len).mean()\n# True News Title length mean.. how long is the title of the news\n# it looks like the title of real news is short.. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* # We have found some difference in the title length. \n* # Lets further explore it ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"F['title'].apply(len).std()\n# trying to figure out how much do they deviate.. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"T['title'].apply(len).std()\n# There seems to be an overlap, so cannot move forward with using lengths as a measure of Fake News","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* # Grouping the news by subject to spot difference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"F.groupby('subject').count()\n#Trying to figure out if the subject matters to determine fake or real","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"T.groupby('subject').count()\n#we can observe a difference","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We notice that the title of news is different in length for T/F News\n* We can use the titles for our analysis purpose rather than text.\n* First I will reate 2 labels: 1 for True, 0 for False\n* Then I will concat the data from both files into one","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"T['labels'] = 1\nF['labels'] = 0\ndf = pd.concat([T,F])\n# We will add labels to both the news types to concatanate them and then train them ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.count() # looking at total rows and columns\n# we will be working with only the titles. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # *Natural Language Processing Modelling*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import re # for data cleaning\nimport nltk# general.. for stop words and stem porter\nnltk.download('stopwords')# stop words part 1\nfrom nltk.corpus import stopwords # stopwords part 2\nfrom nltk.stem.porter import PorterStemmer # love and loved will be characterized as one word.. \n#so we need this","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string # we will try to remove punctuation by this method\nnopunc = [c for c in df['title'] if c not in string.punctuation]\n# if we directly try to enter df['title'], it gives a value error. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = []\nfor i in range(0,44898):\n    title = re.sub('[^a-zA-Z]', ' ', nopunc[i]) # over here, cannot directly take df['title']\n    #this also will remove punctuations and brackets and everything else\n    title = title.lower()#lower the uppercase\n    title = title.split()#will split each word\n    ps = PorterStemmer()\n    title = [ps.stem(words) for words in title if not words in set(stopwords.words('english'))]\n    #stopwords will be removed after splitting\n    title = ' '.join(title)#we join the remaining words back\n    corpus.append(title)#whatever is left we put it in the corpus\n    \n#This happens for 44898 times for every news.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split# trainign and testing \nX_train, X_test, y_train, y_test = train_test_split(corpus, df['labels'], test_size = 0.20, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\npipeline = Pipeline([('bow', CountVectorizer()), ('classifier', MultinomialNB())])\npipeline.fit(X_train, y_train)\nprediction = pipeline.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, prediction))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Another prediction using GaussianNB from Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer() # max_features = 13000 ---> we can add this argument(optional)\nX=cv.fit_transform(corpus).toarray()\ny= df['labels'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\nprediction2 = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, prediction2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* # Clearly MultiNomial is much better\n* # Upvote if you find this helpfula and useful :)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}