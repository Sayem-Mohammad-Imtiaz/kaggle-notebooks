{"cells":[{"metadata":{},"cell_type":"markdown","source":"**SKLEARN LIBRARY CLASSIFICATION ALGORITHMS COMPARISON WITH STRATEGEION RESUME SKILLS**\n\nIn this study, I will compare the supervised machine learning algorithms by strategeion resume skills. The dataset is related to human resurces area and has binary features:\n* 218 skill features: whether or not the corresponding skill is on the applicant's resume.\n* 4 protected features: demographic information about the applicant.\n\nI compare these models in study:\n* Logistic Regression\n* KNN Classification\n* Support Vector Machine Classification\n* Naive Bayes Classification\n* Decision Tree Classification\n* Random Forest Classification"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data1=pd.read_csv(\"../input/resumes_development.csv\")\ndata2=pd.read_csv(\"../input/resumes_pilot.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"EXPLORATORY DATA ANALYSIS (EDA)\n\nThe data is split into two files. For using model selection, I concatenate two files. Then, I try to understand and prepare my dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.concat([data1,data2])\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.isnull().any(axis=1)].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there is no NaN value in the dataset. So, we can take a step for machine learning algorithms. I want to predict the **veteran status** of the applicants by other skills. But I want to analyze *only skill features*. So, I drop the **demographic informations **(Female, URM, Disability) and \"Unnamed: 0\" column."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop([\"Female\", \"URM\", \"Disability\", \"Unnamed: 0\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the details of **veteran** feature:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[5,5])\nsns.set(style='darkgrid')\nsns.countplot(x=\"Veteran\", data=data, palette='RdYlBu')\ndata.loc[:,'Veteran'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SPLITTING DATA FOR TRAINING AND TESTING\n\nI am going to split my data set into as train (x_train, y_train) and test (x_test, y_test) datas.\nThen I am going to teach my machine learning algorithms by using trainig data set.\nLater I will use my trained model to predict my test data (y_pred).\nFinally I will compare my predictions (y_pred) with my test data (y_test)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#train test split\ny=data.Veteran.values\nx=data.drop([\"Veteran\"], axis=1)\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LOGISTIC REGRESSION CLASSIFICATION MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='lbfgs')\nlr.fit(x_train, y_train)\nlr_prediction = lr.predict(x_test)\nlr_score = lr.score(x_test,y_test)\nprint(\"Logistic Regression Test Accuracy: {}%\".format(round(lr.score(x_test,y_test)*100,2)))\n\n#Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\nlr_cm = confusion_matrix(y_test, lr_prediction)\n\n#Mean Squared Error\n\nfrom sklearn.metrics import mean_squared_error\nlr_mse = mean_squared_error(y_test, lr_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"K-NEAREST NEIGHBOUR (KNN) CLASSIFICATION MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"#K_Nearest Neighbour (KNN) Classification\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=14)\nknn.fit(x_train, y_train)\nknn_prediction = knn.predict(x_test)\nknn_score = knn.score(x_test, y_test)\nprint(\"KNN Classification Test Accuracy: {}%\".format(round(knn.score(x_test,y_test)*100,2)))\n\n#Confusion Matrix\n\nknn_cm = confusion_matrix(y_test, knn_prediction)\n\n#Mean Squared Error\n\nknn_mse = mean_squared_error(y_test, knn_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find Best K Value\n\nscore_list = []\nfor each in range(1,30):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train, y_train)\n    score_list.append(knn2.score(x_test, y_test))\nplt.plot(range(1,30), score_list)\nplt.xlabel(\"K Values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SUPPORT VECTOR MACHINE (SVM) CLASSIFICATION MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machine (SVM)\n\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=1, gamma='auto')\nsvm.fit(x_train, y_train)\nsvm_prediction = svm.predict(x_test)\nsvm_score = svm.score(x_test, y_test)\nprint(\"SVM Classification Test Accuracy: {}%\".format(round(svm.score(x_test,y_test)*100,2)))\n\n#Confusion Matrix\n\nsvm_cm = confusion_matrix(y_test, svm_prediction)\n\n#Mean Squared Error\n\nsvm_mse = mean_squared_error(y_test, svm_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NAIVE BAYES CLASSIFICATION MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Naive Bayes Classification\n\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\nnb_prediction = nb.predict(x_test)\nnb_score = nb.score(x_test, y_test)\nprint(\"Naive Bayes Classification Test Accuracy: {}%\".format(round(nb.score(x_test,y_test)*100,2)))\n\n#Confusion Matrix\n\nnb_cm = confusion_matrix(y_test, nb_prediction)\n\n#Mean Squared Error\n\nnb_mse = mean_squared_error(y_test, nb_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DECISION TREE CLASSIFICATION MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Decision Tree Classification\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\ndt_prediction = dt.predict(x_test)\ndt_score = dt.score(x_test, y_test)\nprint(\"Decision Tree Classification Test Accuracy: {}%\".format(round(dt.score(x_test,y_test)*100,2)))\n\n#Confusion Matrix\n\ndt_cm = confusion_matrix(y_test, dt_prediction)\n\n#Mean Squared Error\n\ndt_mse = mean_squared_error(y_test, dt_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RANDOM FOREST CLASSIFICATION MODEL"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest Classification\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=1)\nrf.fit(x_train, y_train)\nrf_prediction = rf.predict(x_test)\nrf_score = rf.score(x_test, y_test)\nprint(\"Random Forest Classification Test Accuracy: {}%\".format(round(rf.score(x_test,y_test)*100,2)))\n\n#Confusion Matrix\n\nrf_cm = confusion_matrix(y_test, rf_prediction)\n\n#Mean Squared Error\n\nrf_mse = mean_squared_error(y_test, rf_prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CONFUSION MATRIX\n\nA confusion matrix is a summary of prediction results on a classification problem.\n\nPositive (P) : Observation is positive (for example: is a Veteran).\n\nNegative (N) : Observation is not positive (for example: is not a Veteran).\n\nTrue Positive (TP) : Observation is positive, and is predicted to be positive.\n\nFalse Negative (FN) : Observation is positive, but is predicted negative.\n\nTrue Negative (TN) : Observation is negative, and is predicted to be negative.\n\nFalse Positive (FP) : Observation is negative, but is predicted positive."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualization of Confusion Matrix\n\nplt.figure(figsize=(20,15))\n\nplt.suptitle(\"Confusion Matrixes\", fontsize=18)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(lr_cm, cbar=False, annot=True, cmap=\"PuBuGn\", fmt=\"d\")\n\nplt.subplot(2,3,2)\nplt.title(\"KNN Classification Confusion Matrix\")\nsns.heatmap(knn_cm, cbar=False, annot=True, cmap=\"PuBuGn\", fmt=\"d\")\n\nplt.subplot(2,3,3)\nplt.title(\"SVM Classification Confusion Matrix\")\nsns.heatmap(svm_cm, cbar=False, annot=True, cmap=\"PuBuGn\", fmt=\"d\")\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Classification Confusion Matrix\")\nsns.heatmap(nb_cm, cbar=False, annot=True, cmap=\"PuBuGn\", fmt=\"d\")\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classification Confusion Matrix\")\nsns.heatmap(dt_cm, cbar=False, annot=True, cmap=\"PuBuGn\", fmt=\"d\")\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest Classification Confusion Matrix\")\nsns.heatmap(rf_cm, cbar=False, annot=True, cmap=\"PuBuGn\", fmt=\"d\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For a better analysis, I summarize the confusion matrix values, accuracy and mean squared error scores of models:"},{"metadata":{"trusted":true},"cell_type":"code","source":"TN = [lr_cm[0,0], knn_cm[0,0], svm_cm[0,0], nb_cm[0,0], dt_cm[0,0], rf_cm[0,0]]\nFP = [lr_cm[0,1], knn_cm[0,1], svm_cm[0,1], nb_cm[0,1], dt_cm[0,1], rf_cm[0,1]]\nFN = [lr_cm[1,0], knn_cm[1,0], svm_cm[1,0], nb_cm[1,0], dt_cm[1,0], rf_cm[1,0]]\nTP = [lr_cm[1,1], knn_cm[1,1], svm_cm[1,1], nb_cm[1,1], dt_cm[1,1], rf_cm[1,1]]\nAccuracy = [lr_score, knn_score, svm_score, nb_score, dt_score, rf_score]\nMSE = [lr_mse, knn_mse, svm_mse, nb_mse, dt_mse, rf_mse]\nClassification = [\"Logistic Regression\", \"KNN Classification\", \"SVM Classification\", \"Naive Bayes Classification\", \n                  \"Decision Tree Classification\", \"Random Forest Classification\"]\nlist_matrix = [Classification, TN, FP, FN, TP, Accuracy, MSE]\nlist_headers = [\"Model\", \"TN\", \"FP\", \"FN\", \"TP\", \"Accuracy\", \"MSE\"]\nzipped = list(zip(list_headers, list_matrix))\ndata_dict = dict(zipped)\ndf=pd.DataFrame(data_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see TN, FP, FN, TP values of classification models on the stacked bar plot:"},{"metadata":{"trusted":true},"cell_type":"code","source":"trace1 = {\n    'x':df.Model,\n    'y':df.TN,\n    'name':'True Negative',\n    'type':'bar'}\n\ntrace2 = {\n    'x':df.Model,\n    'y':df.FP,\n    'name':'False Positive',\n    'type':'bar'}\n\ntrace3 = {\n    'x':df.Model,\n    'y':df.FN,\n    'name':'False Negative',\n    'type':'bar'}\n\ntrace4 = {\n    'x':df.Model,\n    'y':df.TP,\n    'name':'True Positive',\n    'type':'bar'}\n\ngraph = [trace1, trace2, trace3, trace4];\nlayout = {\n  'xaxis': {'title': 'Classification Models'},\n  'barmode': 'relative',\n  'title': 'Confusion Matrix Values of Classification Models'\n};\nfig = go.Figure(data = graph, layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"COMPARISON OF ACCURACY AND MEAN SQUARED ERROR SCORES"},{"metadata":{},"cell_type":"markdown","source":"**Accuracy:**\n\nIt is the ratio of number of correct predictions to the total number of input samples.\nAccuracy= Number of Correct Predictions/ Total Number of Predictions Made"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy\nplt.figure(figsize=(15,10))\nax= sns.barplot(x=df.Model, y=df.Accuracy, palette = sns.cubehelix_palette(len(df.Model)))\nax.set_xticklabels(ax.get_xticklabels(),rotation=30)\nplt.xlabel('Classification Models')\nplt.ylabel('Accuracy')\nplt.title('Accuracy Scores of Classification Models')\nfor i in ax.patches:\n    ax.text(i.get_x()+.19, i.get_height()-0.3, \\\n            str(round((i.get_height()), 4)), fontsize=15, color='white')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Mean Squared Error(MSE):**\n\nMean Squared Error(MSE) takes the average of the square of the difference between the original values and the predicted values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#MSE\nplt.figure(figsize=(15,10))\nax= sns.barplot(x=df.Model, y=df.MSE, palette = sns.cubehelix_palette(len(df.Model)))\nax.set_xticklabels(ax.get_xticklabels(),rotation=30)\nplt.xlabel('Classification Models')\nplt.ylabel('Mean Squared Error')\nplt.title('MSE Scores of Classification Models')\nfor i in ax.patches:\n    ax.text(i.get_x()+.19, i.get_height()-0.1, \\\n            str(round((i.get_height()), 5)), fontsize=15, color='white')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition, we can see y_test and prediction values on dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"d = {'y_test': y_test, 'Logistic_Regression_prediction': lr_prediction, 'KNN_prediction': knn_prediction, \n     'SVM_prediction': svm_prediction, 'Naive_Bayes_prediction': nb_prediction, 'Decision_Tree_prediction': dt_prediction, \n     'Random_Forest_prediction': rf_prediction}\ndata1=pd.DataFrame(data=d)\ndata1.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As as result **KNN Classification** and **SVM Classification** models have the best performance. In addition, **Decision Tree Classification** model has the worst performance for this study."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}