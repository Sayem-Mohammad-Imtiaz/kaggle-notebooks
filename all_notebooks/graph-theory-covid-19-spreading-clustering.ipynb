{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 Spreading\n## Using Graph Theory to Visualize the Spreading of the COVID-19 Virus\n\nThe purpose of this notebook is to represent the COVID-19 spreading around the World using graph / networks theory. Then, we will study the properties of such graph.\n\n![Virus Spreading](https://static.seattletimes.com/wp-content/uploads/2020/03/coronavirus-spread-W-780x226.jpg)\n\n** How would it be possible to transform a set of evolution lines / time series in a network graph? **\n> It`s not impossible at all! We will try to link the countries following a criteria: (1) each country will represent a node and (2) countries where the explosion of cases started nearly at the same time will tend to have a connection between them.\n\nOf course there are different ways to define what would be \"cases that started at the same time\". We will discuss it and, then, we will create a clustering algorithm to classify the countries in \"transmission groups\" (i.e: groups of countries that were infected by the virus at **nearly** the same time).\n\nHere is an example of what we are going to do! :)","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"img_dir = '/kaggle/input/clusterfigurescovid19/GRAPH_COVID1.png'\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimg = mpimg.imread(img_dir)\n\nfig = plt.figure(figsize = (15, 15))\nax = fig.add_subplot(111)\nax.axis('off')\nax.imshow(img, interpolation='none')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Exploring the Dataset (EDA)\n\nAfter loading our dataset, we will just play a little with what have:","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport plotly\nimport plotly.express as px\n\nimport networkx as nx\n\nimport scipy.spatial.distance as ssd\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom sklearn.cluster import AgglomerativeClustering\n\ndf_in = pd.read_csv('/kaggle/input/novel-corona-virus-2019-dataset/time_series_covid_19_deaths.csv')\ndf_in.iloc[:, 1:10].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also have informations about the number of confirmed cases and recovered people but these numbers will not be used here. There is a **good** reason for that:\n> We cannot trust the number of confirmed cases for every country. Many places are having huge troubles to test all the population. Also, we can't determine exactly the number of infected people since the COVID-19 is characterized by a great number of assymptomatic people. When someone dies, the tests tend to be executed with a higher probability than for those who are infected with no symptoms.\n\nOk. So, imagine we have an exponential function:\n\n$$ f(x) = K.e^x $$\n\nWe can easily calculate the average value of the function:\n\n$$ \\overline{f}(x) = \\int_{x = 0}^{x = T_{final}} f(x) $$\n\nThis will lead us to a value with the same units of \"f(x) = y\". Alright. Let's follow a similar approach to calculate the average value of \"x\" instead of \"y\":\n\n$$ \\overline{x} = \\frac{\\int_{x = 0}^{x = T_{final}} x.f(x)}{\\int_{x = 0}^{x = T_{final}} f(x)} $$\n\n> You can imagine that the average value of a function is a weighted average of the function along the time axis with all different times having the same weight. So, we can generalize that to take the average \"x\" but, this time, we will use the function values as weights. The final value is indeed something with the same units of \"x\" - in our case...the time! We are calculating the average date of the deaths.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dates_vec = list(df_in.columns)[3:]\naverage_time_vec = [None] * df_in.shape[0]\n\nfor i, row_index in enumerate(df_in.index):\n\n    weighted_sum, total_deaths = 0, 0\n    \n    for j, date in enumerate(dates_vec):\n        current_term = df_in.at[row_index, date]\n        weighted_sum += j * current_term\n        total_deaths += current_term\n    \n    average_time_vec[i] = weighted_sum / total_deaths\n    \ndf_in['avg_time'] = average_time_vec\n\nn_lines = int((df_in.shape[0] * (df_in.shape[0] - 1)) / 2)\nlist_country1, list_country2, list_w, list_d =\\\n    [None] * n_lines, [None] * n_lines, [None] * n_lines, [None] * n_lines\n\nline_index = 0\nepsilon = 0.001\nfor i in range(0, df_in.shape[0] - 1):\n    for j in range(i + 1, df_in.shape[0]):\n        index_i, index_j = df_in.index[i], df_in.index[j]\n        list_country1[line_index] = df_in.at[index_i, 'Country/Region']\n        list_country2[line_index] = df_in.at[index_j, 'Country/Region']\n        diff_time = df_in.at[index_i, 'avg_time'] - df_in.at[index_j,'avg_time']\n        list_w[line_index] = (1 / (abs(diff_time) + epsilon))\n        list_d[line_index] = abs(diff_time)\n        line_index += 1\n        \ndf_graph = pd.DataFrame(dict(\n    Country1 = list_country1,\n    Country2 = list_country2,\n    Weight = list_w,\n    Distance = list_d\n))\n\ndf_graph.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finaly, with this datatable, we can represent our graph and explore its structure. Notice the definition of the graph weight that we used:\n\n$$ W_{ij} = (\\lVert \\overline{x_i} - \\overline{x_j} \\rVert + \\epsilon) ^{-1} $$\n\nWe take the inverse of the difference between the mean times. The epsilon factor corrects the fact that the difference may be zero. In that case, the nodes will be strongly linked since the epsilon factor is a small number (0.001).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_graph.to_csv('df_graph.csv', index=False)\ndf_graph.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Plotting / Exploring the Graph\n\nThe graph was generated using the Gephi editor. It's true that we could create the graph using the NetworkX python library but it's easier to use the Gephi software and, also, it's more complete and with it's customization I was able to get good insights about the structure of the COVID-19 propagation network. You can check more informations about Gephi in the [Official Website](https://gephi.org/), it's open source and very interesting to deal with networks in general.\n\n![https://gephi.files.wordpress.com/2008/09/logo_about.png?w=584](https://gephi.files.wordpress.com/2008/09/logo_about.png?w=584)\n\n> **2A.** I used the size of the nodes, their colors and the colors of the edges directly related to the weighted average of each node, where the weight is bigger if the mean time of occurrences are similar. So, we have the following figure:\n\n![https://i.ibb.co/bg7PQ3c/GRAPH-COVID1.png](https://i.ibb.co/bg7PQ3c/GRAPH-COVID1.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Our first impressions are:\n\n* China takes a central role in the COVID-19 propagation, which makes sense **BUT**\n* It seems that something similar happens in France, Canada, United Kingdom and Denmark\n* Then, from Europe, a huge number of African / Asian countries were affected\n\nThe weighted degree of each node can be calculated by:\n\n$$ D_i = \\sum_{j \\in Neighbours} W_{ij} $$\n\nSo it's the average weight of the neighbours that are linked to the node in analysis. We can continue to get other visualizations. Now:\n\n> **2B.** Let's use a Gephi filter see just countries with a weighted degree bigger than a certain number. We will increase the threshold progressively:\n\n![https://i.ibb.co/YQs9gDV/GRAPH-COVID2.png](https://i.ibb.co/YQs9gDV/GRAPH-COVID2.png)\n\n> **2C.** Filtering more to get just the main points:\n\n![https://i.ibb.co/1RR7b5g/GRAPH-COVID3.png](https://i.ibb.co/1RR7b5g/GRAPH-COVID3.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can take some nice conclusions looking at the last network, where we look at the points with a weighted degree bigger or equal than the weighed degree of Denmark:\n\n* There is a strong link between Canada and France. They are connected, which makes sense when we think that there is a great cultural proximity between Quebec and France, which may interfer in the flights and in the personal contacts among different kinds of people.\n* The graph also shows us that it seems that we have a propagation following the path \"China - Europe - Asia / Africa - America\".\n\nFinally, we can study the graph metrics and try to take more conclusions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. Graph Metrics\n\nThis step will be really simple: we will just study the distribution of the weighted degree of the graph. Other metrics are possible, like the Page Rank or the Concentration. Some of them are used mainly in Social Network applications.\n\nSo, let's start by plotting the distribution of the edge weights:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(x=df_graph['Weight'])\nfig.update_layout(yaxis_type='log', title='Edges Weight Histogram', xaxis_title='Weight', yaxis_title='Count (Log)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The edge nodes degree can be computed by taking the sum of the weights of all edges that link a given node. We can calculate it programatically, but we can also use the NetworkX library tools:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"covid_graph = nx.from_pandas_edgelist(df_graph, 'Country1', 'Country2', 'Weight')\nw_degrees = covid_graph.degree(weight='Weight')\ndf_w_degrees = pd.DataFrame(w_degrees, columns=['Country', 'Degree'])\ndf_w_degrees.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(x=df_w_degrees['Degree'])\nfig.update_layout(yaxis_type='log', title='Weighted Degrees', xaxis_title='Weighted Deg.', yaxis_title='Count (Log)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the histogram taking the values with a weighted degree smaller than \"20.000\":","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(x=df_w_degrees.query('Degree < 20000')['Degree'])\nfig.update_layout(yaxis_type='log', title='Weighted Degrees', xaxis_title='Weighted Deg.', yaxis_title='Count (Log)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And if we list the countries with a degree bigger than 20.000 we will get the \"big nodes\" of our Gephi figure and, also, the medium sized nodes located at the center of the figure:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_w_degrees.query('Degree > 20000').sort_values('Degree', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, what can we do with all these informations? We can consider that the inverse of the edge weight is a kind of distance between the nodes and use it to find a good clusterization algorithm in order to classify the countries in \"transmission groups\"! Remember that the weight of the graph is the inverse of the absolute value of the difference between the mean times:\n\n$$ W_{ij}^{-1} = ((\\lVert \\overline{x_i} - \\overline{x_j} \\rVert + \\epsilon) ^{-1})^{-1} = \\lVert \\overline{x_i} - \\overline{x_j} \\rVert + \\epsilon$$","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. Finding Transmission Groups with Clustering\n\nWe can try to use ahierarchical clustering model to find transmission groups.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"graph_distance = nx.from_pandas_edgelist(df_graph, 'Country1', 'Country2', 'Distance')\nadj_matrix = nx.adjacency_matrix(graph_distance, weight='Distance')\nadj_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The matrix indexes are ordered with the same order as the dict keys:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"graph_distance._node.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the histogram of the distribution of the distances:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(x=df_graph['Distance'])\nfig.update_layout(yaxis_type='log', title='Distances Distribution', xaxis_title='Dist.', yaxis_title='Count (Log)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Noting that we have a bigger concentration of points at smaller values of X:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.histogram(x=df_graph.query('Distance < 20')['Distance'])\nfig.update_layout(yaxis_type='log', title='Distances Distribution', xaxis_title='Dist.', yaxis_title='Count (Log)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the dendrogram, since we will work with a hierarchical agglomerative clustering:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = [16, 6]\nplt.style.use('ggplot')\nD = dendrogram(linkage(adj_matrix.todense()), no_labels=True, truncate_mode='level')\nplt.ylim(0, 600)\nplt.yticks(fontsize=20)\nplt.title('Dendrogram - Hierarchical Clustering', fontsize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, if we take a threshold equal to 300, we will have 6 clusters, which is a reasonable number of clusters and, in this case, we will have 2 outliers (the figure helps us to find a good trade-off between the number of clusters and the number of outliers). Let's fit our model:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ac_model = AgglomerativeClustering(n_clusters=6, affinity='precomputed', linkage='complete')\nac_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ac_model.fit(adj_matrix.toarray())\nac_model.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_classification = pd.DataFrame(dict(Country=list(graph_distance._node.keys()), Label=ac_model.labels_.tolist()))\ndf_classification.to_csv('df_classification.csv', index=False)\ndf_classification.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The obtained dataframe can be used to color the different nodes of the graphs and to draw a new Gephi Networks, and they will be presented in the next sections as our final results.\n\n# 4. Final Visualizations / Results\n\nWe actually have 3 clusters (0, 1 and 2). The clusters 3, 4 and 5 are composed by single outliers as we can see in the table below:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_classification.query('Label > 2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can draw the clusters that we found and try to understand each one of them:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4.A. Cluster 0: The Majority\n\nThe first cluster takes a huge number of countries when compared to others. They are formed by a great group of african countries but it seems that Denmark has a strong influence in this cluster:\n\n![https://i.ibb.co/18LYmQy/Cluster0.png](https://i.ibb.co/18LYmQy/Cluster0.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4.B. Cluster 1: The Origins\n\nChina takes the main role in this cluster. We can also notice a strong presence of european countries like France and UK:\n\n![https://i.ibb.co/wwLhXQr/Cluster1.png](https://i.ibb.co/wwLhXQr/Cluster1.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4.C. Cluster 2: Australia + Other Countries\n\n![https://i.ibb.co/VJM4wBK/Cluster2.png](https://i.ibb.co/VJM4wBK/Cluster2.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 5. Conclusions\n\nWe can use graph / network theory to study the propagation of the virus. It may be useful to understand how the countries should act in case of a new pandemy.\n\nA given country should aways be aware about new diseases in its \"graph main neighbouts\". This is not a perfect analysis since some countries have not a good testing rate and are passing by subnotification problems but it's nice to see how we can transform transmission curves in instants of time.\n\nThe sky is the limit **but** it's crucial to have a massive testing policy everywhere in order to obtain better results. And,remember:\n\n![https://ca-indosuez.com/var/indosuez/storage/images/_aliases/full_news/6/2/3/3/33326-15-eng-GB/Visuel%20EN.jpg](https://ca-indosuez.com/var/indosuez/storage/images/_aliases/full_news/6/2/3/3/33326-15-eng-GB/Visuel%20EN.jpg)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}