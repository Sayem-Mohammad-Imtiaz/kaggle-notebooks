{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport time\ntime_start=time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport matplotlib.pyplot as plt\nfrom torch import optim\nfrom sklearn import linear_model \nfrom sklearn.preprocessing import MinMaxScaler \n\nimport datetime\nnowTime0 = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')  # 现在时间\n\ntorch.set_default_tensor_type(torch.DoubleTensor)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read data and spliting to generate training samples\n\ndef read_data(prob_No, window_size, stride, MaxRUL):\n    '''\n    :param prob_No: 1,2,3,4,  ----representing FD001 ~ FD004\n    :param window_size: slide window sizes\n    :param stride: slide step length\n    : return training & testing set data (dims: num x  lens/or window_size x channels) & labels (dims: num x lens/or window_size) or (dims: num x lens/1)\n    ''' \n    train_df = pd.read_csv('../input/cmapssdata/train.csv', usecols=[0,1,2,  3,4,5,   7,8,9,12,13,14,16,17,18,19,20,22,25,26])\n    test_df = pd.read_csv('../input/cmapssdata/test.csv', usecols=[0,1,2,  3,4,5,   7,8,9,12,13,14,16,17,18,19,20,22,25,26])\n    rul_df = pd.read_csv('../input/cmapssdata/RUL.csv')\n    \n    # 训练集\n    train_data = train_df[(train_df.dataset_id == 'FD00'+str(prob_No))]\n    train_data_total = []\n    train_label_total = []\n    cyclenum1 = train_data.unit_id.max()\n    LRmodel = linear_model.LinearRegression()   # 线性回归\n    train_cyclen_list = []\n    train_sample_num = 0\n    \n    for cycle in range(cyclenum1):\n        data = train_data[(train_data.unit_id == cycle+1)]\n        cyclens = data.cycle.max()\n        train_cyclen_list.append(cyclens)\n        \n        data = data.values[:, 6:].astype(float)\n        labels = [(cyclens - t if cyclens - t <= MaxRUL else MaxRUL) for t in range(1, cyclens + 1)]\n        \n        for i in range(0, cyclens, stride):\n            x = np.array(list(range(i+1-window_size, i+1, stride)))   # 对time step回归，x为time step list\n            if i >= window_size - 1:\n                scaled_data = MinMaxScaler(feature_range=(0, 1)).fit_transform(data[i+1-window_size:i+1])\n            else:\n                padding_data = np.zeros((window_size - i - 1, data[0:i+1].shape[-1]))    # 不足窗口的数据填为0  补充零待考证\n                y = np.vstack((padding_data, data[0:i+1]))\n                scaled_data = MinMaxScaler(feature_range=(0, 1)).fit_transform(y)\n            \n            max_data = np.max(scaled_data,axis=0)    # 最大值\n            mean_data = np.mean(scaled_data,axis=0)    # 均值\n#             std_data = np.std(scaled_data,axis=0,ddof=1)    # 标准差\n\n            coef = 0    # 回归系数\n            for sensor in range(scaled_data.shape[-1]):\n                LRmodel.fit(x.reshape(-1,1), scaled_data[:, sensor].reshape(-1,1))    # 利用sklearn中的线性回归模型\n                coef = np.hstack((coef,LRmodel.coef_[:,0]))   # 获得回归系数\n\n            sample_data = torch.tensor(np.vstack((scaled_data, max_data, mean_data, coef[1:]))) #含最大值，最小值，极差，均值，标准差，回归系数及归一化的时间窗数据\n            \n            train_data_total.append(sample_data.T)\n            train_label_total.append(torch.tensor(labels[i]))\n            \n            train_sample_num += 1\n    \n    # 测试集\n    test_data = test_df[(test_df.dataset_id == 'FD00'+str(prob_No))]\n    test_data_total = []\n    test_label_total = []\n    cyclenum2 = test_data.unit_id.max()\n    test_cyclen_list = []\n    test_sample_num = 0\n    \n    test_label = rul_df[(rul_df.dataset_id == 'FD00'+str(prob_No))]\n    test_label = test_label.values[:,2] # 1维\n    \n    for cycle in range(cyclenum2):\n        data = test_data[(test_data.unit_id == cycle+1)]\n        cyclens = data.cycle.max()\n        test_cyclen_list.append(cyclens)\n        \n        data = data.values[:, 6:].astype(float)\n        labels = [((test_label[cycle] + cyclens - 1 - t) \\\n                   if (test_label[cycle] + cyclens - 1 - t) <= MaxRUL else MaxRUL) for t in range(cyclens)]\n        \n        for i in range(0, cyclens, stride):\n            x = np.array(list(range(i+1-window_size, i+1, stride)))   # 对time step回归，x为time step list\n            if i >= window_size - 1:\n                scaled_data = MinMaxScaler(feature_range=(0, 1)).fit_transform(data[i+1-window_size:i+1])\n            else:\n                padding_data = np.zeros((window_size - i - 1, data[0:i+1].shape[-1]))    # 不足窗口的数据填为0  补充零待考证\n                y = np.vstack((padding_data, data[0:i+1]))\n                scaled_data = MinMaxScaler(feature_range=(0, 1)).fit_transform(y)\n            \n            max_data = np.max(scaled_data,axis=0)    # 最大值\n\n            mean_data = np.mean(scaled_data,axis=0)    # 均值\n#             std_data = np.std(scaled_data,axis=0,ddof=1)    # 标准差\n             \n            coef = 0    # 回归系数\n            for sensor in range(scaled_data.shape[-1]):\n                LRmodel.fit(x.reshape(-1,1), scaled_data[:, sensor].reshape(-1,1))    # 利用sklearn中的线性回归模型\n                coef = np.hstack((coef,LRmodel.coef_[:,0]))\n\n            sample_data = torch.tensor(np.vstack((scaled_data, max_data, mean_data, coef[1:])))    #含均值 回归系数的归一化的时间窗数据\n            \n            test_data_total.append(sample_data.T)\n            test_label_total.append(torch.tensor(labels[i]))\n            \n            test_sample_num += 1\n    \n    return train_data_total, train_label_total, train_sample_num, train_cyclen_list, test_data_total, test_label_total, test_sample_num, test_cyclen_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Main Function - 1\nwindow_size = 60    # FD001-0004的时间窗大小分别为50,50,50,50\n# read data and spliting to generate training samples\nnowTime = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')  # 现在时间\nprint(str(nowTime)+': 初始化完成，正在读取数据，请稍等......')\n\ntrain_data, train_label, train_sample_num, train_cyclen_list, test_data, test_label, test_sample_num, test_cyclen_list = read_data(3, window_size, 1, 125)    # 如果maxRUL无limited， 此处可设maxRUL为极大值如10000\n\nnowTime = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')  # 现在时间\nprint(str(nowTime)+': 读取数据完成')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport torch.nn as nn\n\n# SE Block\nclass SEblock(nn.Module):\n    def __init__(self, sensor_num,r):\n        super(SEblock, self).__init__()\n        ':r 降维率'\n        # squeeze\n        self.sq = torch.nn.AdaptiveAvgPool1d(1)\n        # excitation\n        self.fc1= nn.Linear(sensor_num, int(sensor_num/r))\n        self.relu = nn.ReLU()\n        self.fc2= nn.Linear(int(sensor_num/r), sensor_num)\n        self.sigmoid = nn.Sigmoid()\n        \n    def init_weights(self):\n        self.fc1.weight.data.normal_(0, 0.01)\n        self.fc2.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        torch.set_default_tensor_type(torch.FloatTensor)\n        z_c = self.sq(x).transpose(-1,-2)    # B x 1 x C\n        s = self.fc1(z_c)    # B x 1 x C/r\n        s = self.relu(s)     # B x 1 x C/r\n        s = self.fc2(s)      # B x 1 x C\n        s = self.sigmoid(s).transpose(-1,-2)  # B x C x 1\n        o = s * x   # B x C x L\n        return o","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.nn.utils import weight_norm\n\n\n# 卷积结束后会因为padding导致卷积之后的新数据的尺寸B>输入数据的尺寸A，所以只保留输出数据中前面A个数据\n# 修改数据尺寸\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x): \n        return x[:, :, :-self.chomp_size].contiguous()\n\n\n# Residual Block --- TemporalBlock\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size1,  kernel_size2, kernel_size3, stride, dilation, dropout):\n        super(TemporalBlock, self).__init__()\n        # 尺度1\n        padding1 = (kernel_size1-1) * dilation\n        self.conv11 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size1,\n                                           stride=stride, padding=padding1, dilation=dilation))\n        self.chomp11 = Chomp1d(padding1)\n#         self.relu11 = nn.ReLU()\n#         self.relu11 = nn.LeakyReLU()\n        self.relu11 = nn.PReLU(n_outputs)\n        self.dropout11 = nn.Dropout(dropout)\n        \n        self.conv21 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size1,\n                                           stride=stride, padding=padding1, dilation=dilation))\n        self.chomp21 = Chomp1d(padding1)\n#         self.relu21 = nn.ReLU()\n#         self.relu21 = nn.LeakyReLU()\n        self.relu21 = nn.PReLU(n_outputs)\n        self.dropout21 = nn.Dropout(dropout)\n        \n        # 结构： 膨胀因果卷积 → 修改尺寸 → ReLU → dropout → \n        #       膨胀因果卷积 → 修改尺寸 → ReLU → dropout \n        self.net1 = nn.Sequential(self.conv11, self.chomp11, self.relu11, self.dropout11,\n                                 self.conv21, self.chomp21, self.relu21, self.dropout21)\n        \n        # 尺度2\n        padding2 = (kernel_size2-1) * dilation\n        self.conv12 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size2,\n                                           stride=stride, padding=padding2, dilation=dilation))\n        self.chomp12 = Chomp1d(padding2)\n#         self.relu12 = nn.ReLU()\n#         self.relu12 = nn.LeakyReLU()\n        self.relu12 = nn.PReLU(n_outputs)\n        self.dropout12 = nn.Dropout(dropout)\n        \n        self.conv22 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size2,\n                                           stride=stride, padding=padding2, dilation=dilation))\n        self.chomp22 = Chomp1d(padding2)\n#         self.relu22 = nn.ReLU()\n#         self.relu22 = nn.LeakyReLU()\n        self.relu22 = nn.PReLU(n_outputs)\n        self.dropout22 = nn.Dropout(dropout)\n        \n        self.net2 = nn.Sequential(self.conv12, self.chomp12, self.relu12, self.dropout12,\n                                 self.conv22, self.chomp22, self.relu22, self.dropout22)\n        \n        # 尺度3\n        padding3 = (kernel_size3-1) * dilation\n        self.conv13 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size3,\n                                           stride=stride, padding=padding3, dilation=dilation))\n        self.chomp13 = Chomp1d(padding3)\n#         self.relu13 = nn.ReLU()\n#         self.relu13 = nn.LeakyReLU()\n        self.relu13 = nn.PReLU(n_outputs)\n        self.dropout13 = nn.Dropout(dropout)\n        \n        self.conv23 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size3,\n                                           stride=stride, padding=padding3, dilation=dilation))\n        self.chomp23 = Chomp1d(padding3)\n#         self.relu23 = nn.ReLU()\n#         self.relu23 = nn.LeakyReLU()\n        self.relu23 = nn.PReLU(n_outputs)\n        self.dropout23 = nn.Dropout(dropout)\n        \n        self.net3 = nn.Sequential(self.conv13, self.chomp13, self.relu13, self.dropout13,\n                                 self.conv23, self.chomp23, self.relu23, self.dropout23)\n\n        # 残差链接 跨层连接的1x1卷积\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n#         self.relu = nn.ReLU()\n#         self.relu = nn.LeakyReLU()\n        self.relu = nn.PReLU(n_outputs)\n        self.init_weights()\n\n    #初始权值\n    def init_weights(self):\n        self.conv11.weight.data.normal_(0, 0.01)\n        self.conv21.weight.data.normal_(0, 0.01)\n        self.conv12.weight.data.normal_(0, 0.01)\n        self.conv22.weight.data.normal_(0, 0.01)        \n        self.conv13.weight.data.normal_(0, 0.01)\n        self.conv23.weight.data.normal_(0, 0.01)\n        \n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out1 = self.net1(x)\n        out2 = self.net2(x)\n        out3 = self.net3(x)\n        out = out1 + out2 + out3\n        res = x if self.downsample is None else self.downsample(x)  # 残差连接\n        o = self.relu(out + res)\n        return o\n\n\n# 时间卷积网络 主结构 网络整体\nclass TemporalConvNet(nn.Module):\n    # num_inputs:输入数据通道数\n    # num_channels: 网络结构各隐藏层即输出层 输入输出通道数\n    def __init__(self, num_inputs, num_channels, kernel_size1,  kernel_size2, kernel_size3, dropout):\n        super(TemporalConvNet, self).__init__()\n        layers = []\n        num_levels = len(num_channels)  # 网络层数\n        for i in range(num_levels):     # 推导各层的结构\n            dilation_size = 2 ** i      # 膨胀卷积系数为2的扩张\n\n            # 第0层即输入层的输入通道数根据原始数据的通道数确定 否则其他层从设定的网络结构中读取--即需要设定隐藏、输出层的输入输出数\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n\n            # 各层就是一个TemporalBlock 膨胀系数不一样，卷积核一致\n            # padding=（kernel_size-1）*dilation\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size1,  kernel_size2, kernel_size3, stride=1, dilation=dilation_size, dropout=dropout)]\n\n        self.network = nn.Sequential(*layers)   #整体网络 由这些一系列layers连接成\n\n    def forward(self, x):\n        return self.network(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, sensor_num, window_size, r, tcn_inputs_size, num_channels, kernel_size1, kernel_size2, kernel_size3, dropout, fc_note1, fc_note2, output_size):\n        super(Model, self).__init__()\n        self.seblock1 = SEblock(sensor_num, r)\n        \n        self.tcn = TemporalConvNet(tcn_inputs_size, num_channels, kernel_size1,  kernel_size2, kernel_size3,  dropout=dropout)\n\n        self.seblock2 = SEblock(num_channels[-1],r)\n        \n#         self.relu2 = nn.ReLU()\n#         self.relu2 = nn.LeakyReLU()\n        self.relu2 = nn.PReLU(num_channels[-1])\n    \n        self.fc1 = nn.Linear(num_channels[-1]*(window_size+3), fc_note1)   # window_size+6这里最佳是写成TCN输出的数据的-1 dim，即是每sensor数据的长度，包括均值等\n#         self.relu3 = nn.ReLU()\n#         self.relu3 = nn.LeakyReLU()\n        self.relu3 = nn.PReLU(fc_note1)\n        \n        self.fc2 = nn.Linear(fc_note1, fc_note2)\n#         self.relu4 = nn.ReLU()\n#         self.relu4 = nn.LeakyReLU()        \n        self.relu4 = nn.PReLU(fc_note2)\n\n        \n        self.output = nn.Linear(fc_note2, output_size)\n#         self.relu5 = nn.ReLU()\n#         self.relu5 = nn.LeakyReLU()\n        self.relu5 = nn.PReLU(output_size)\n\n\n    def forward(self, x):\n\n        x = self.seblock1(x)    # B x C x L(windowsize+6)\n\n        out = self.tcn(x)\n        \n        o = self.seblock2(out)     # B x C x L(windowsize+6)\n\n        o = self.relu2(o)\n        o = torch.flatten(o, start_dim = -2, end_dim = -1)    # flatten to 1 dim\n\n        o = self.fc1(o)    #  B x L -> B x L'\n        o = self.relu3(o)\n\n        o = self.fc2(o)    # B x L' -> B x L\"\n        o = self.relu4(o)\n\n        o = self.output(o)   # B x L\" -> output 1\n        o = self.relu5(o)\n\n        return o","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset,DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\nclass RulDataSet(Dataset):\n    def __init__(self, sensor_data, Rul_label):\n        self.sensor_data = sensor_data\n        self.Rul_label = Rul_label\n\n    def __getitem__(self, item):\n        return self.sensor_data[item], self.Rul_label[item]\n\n    def __len__(self):\n        return len(self.sensor_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training function\ndef train(model, mse_loss, optimizer,lossdata, data_total, label_total):\n    # train()状态 ---dropout working\n    model.train()\n\n    dataset = RulDataSet(data_total, label_total)\n\n    # 将dataset封装为data_loader\n    data_loader = DataLoader(dataset, batch_size=256, shuffle=True)\n    loss_total = []\n\n    for i, data_label in enumerate(data_loader):\n        data, label = data_label\n        \n        data = data.to(device)\n        label = label.reshape(-1,1)\n        label = label.to(device)\n#         print('data: {}'.format(data))\n#         print('label: {}'.format(label))\n        \n        # 获得预测值\n        out = model(data)\n        \n        # 数据类型有出入 进行转化\n        out = out.to(torch.double)\n        label = label.to(torch.double)\n#         print('out: {}'.format(out))\n#         print('label: {}'.format(label))\n\n        # 代价函数\n        loss = torch.sqrt(mse_loss(out, label)) \n       \n        loss_total.append(loss.item())\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    lossdata.append(np.mean(loss_total))\n    print('训练集RMSE:{0}'.format(np.mean(loss_total)))\n\n    return lossdata\n\n\n# testing function\ndef test(model, mse_loss, lossdata, test_data_total, test_label_total):\n    # eval()--测试/评估状态 ---dropout不作用--即所有神经元工作\n    model.eval()\n    \n    dataset = RulDataSet(test_data_total, test_label_total)\n\n    # 将dataset封装为data_loader\n    data_loader = DataLoader(dataset, batch_size=256, shuffle=False)\n    lossi = []\n    for i, data_label in enumerate(data_loader):\n        # 获得一个批次的数据和标签\n        inputs, labels = data_label\n        labels = labels.reshape(-1,1)\n        labels = labels.to(torch.double)\n        \n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        # 获得预测值\n        out = model(inputs)\n\n        # 数据类型有出入 进行转化\n        out = out.to(torch.double)\n\n#         print(out)\n#         print(labels)\n        loss = torch.sqrt(mse_loss(out, labels))\n\n        lossi.append(loss.item())\n    \n    print(\"测试集RMSE:{0}\".format(np.mean(lossi)))\n    lossdata.append(np.mean(lossi))\n    \n    \n\n    return lossdata\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 导入RAdam\nimport math\n# import torch\nfrom torch.optim.optimizer import Optimizer\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        \n        self.degenerated_to_sgd = degenerated_to_sgd\n        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n            for param in params:\n                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n                    param['buffer'] = [[None, None, None] for _ in range(10)]\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = group['buffer'][int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    elif self.degenerated_to_sgd:\n                        step_size = 1.0 / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = -1\n                    buffered[2] = step_size\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:\n                    if group['weight_decay'] != 0:\n                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n                    p.data.copy_(p_data_fp32)\n                elif step_size > 0:\n                    if group['weight_decay'] != 0:\n                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n                    p.data.copy_(p_data_fp32)\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 测试评估训练好的模型\n\"\"\"Score 函数： h < 0 h >=0 S = Sigma(e^(-h/13)-1) or Sigma(e^(h/10)-1) \"\"\"\n\ndef eval_model(model, data_total, label_total, sample_num, cyclen_list, trainset=True, Draw=True):\n    # eval()状态\n    model.eval()\n\n    dataset = RulDataSet(data_total, label_total)\n    # 将dataset封装为data_loader\n    data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n    \n    pred_rul = []\n    true_rul = []\n    lossi = 0\n    Score = 0  # 计分\n    accy = 0    # 正确个数\n    \n    for i, data_label in enumerate(data_loader):\n        data, label = data_label\n        data = data.to(device)\n        label = label.reshape(-1,1)\n        label = label.to(device)\n        \n        # 获得预测值\n        out = model(data)\n\n        pred_rul.append(out.item())\n        true_rul.append(label.item())\n        \n        # 计算 score\n        h = out.item() - label.item()\n        lossi += h**2\n        si = (math.exp((-h/13.0)) - 1) if h < 0 else (math.exp((h/10.0)) - 1)\n        Score += si\n        # 准确 计数\n        if -13 <= h <= 10:\n            accy += 1 \n    print('训练集RMSE：{}'.format(np.sqrt(lossi/sample_num))) if trainset else print('测试集RMSE：{}'.format(np.sqrt(lossi/sample_num)))\n    print('训练集Score：{}'.format(Score)) if trainset else print('测试集Score：{}'.format(Score))\n    print('训练集正确率：{}'.format(accy/sample_num)) if trainset else print('测试集正确率：{}'.format(accy/sample_num))\n    \n    # 仅对sample末尾进行误差计算\n    Score = 0  # 计分\n    accy = 0    # 正确个数\n    loss2 = 0\n    result_true = []\n    result_pred = []\n    for No_sample in range(len(cyclen_list)):\n        y_hat = pred_rul[sum(cyclen_list[:No_sample+1]) - 1]\n        y_true = true_rul[sum(cyclen_list[:No_sample+1]) - 1]\n        \n        result_true.append(y_true)\n        result_pred.append(y_hat)\n        \n        h = y_hat - y_true\n        loss2 = loss2 + (h**2)\n        si = (math.e**(-h/13.0) - 1) if h < 0 else (math.e**(h/10.0) - 1)\n        Score += si\n        # 准确 计数\n        if -13 <= h <= 10:\n            accy += 1\n    print('仅对sample末尾的训练集RMSE：{}'.format(np.sqrt(loss2/len(cyclen_list))))  if trainset else print('仅对sample末尾的测试集RMSE：{}'.format(np.sqrt(loss2/len(cyclen_list))))\n    print('仅对sample末尾的训练集Score：{}'.format(Score)) if trainset else print('仅对sample末尾的测试集Score：{}'.format(Score))\n    print('仅对sample末尾的训练集正确率：{}'.format(accy/len(cyclen_list))) if trainset else print('仅对sample末尾的测试集正确率：{}'.format(accy/len(cyclen_list)))\n    \n    # 绘制预测结果图\n    if not trainset:\n        x_No = [No+1 for No in range(len(cyclen_list))]\n        plt.title('Results')\n        plt.plot(x_No, result_pred, 'blue')\n        plt.plot(x_No, result_true, 'r')\n        plt.legend([\"Predictied\",\"True\"])    # 图例\n        plt.plot(x_No, result_pred, '.b')\n        plt.plot(x_No, result_true, '.r')\n        plt.show()\n    ####仅对sample末尾进行误差计算End\n    \n    # 绘图 \n    if Draw:\n        for No_sample in range(len(cyclen_list)):\n            x = [time for time in range(cyclen_list[No_sample])]\n            y_hat = pred_rul[sum(cyclen_list[:No_sample+1]) - cyclen_list[No_sample] : sum(cyclen_list[:No_sample+1])]\n            y_true = true_rul[sum(cyclen_list[:No_sample+1]) - cyclen_list[No_sample] : sum(cyclen_list[:No_sample+1])]\n            plt.title('No.'+str(No_sample+1)+' sample of training set') if trainset else plt.title('No.'+str(No_sample+1)+' sample of test set')\n            plt.plot(x, y_hat, 'blue')      # 预测\n            plt.plot(x, y_true, 'r')       # 准确\n            plt.legend([\"Predictied\",\"True\"])    # 图例\n            plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Main Function - 2\n# model\n# 各层channel  -- 最后需要一致\nlevel_channels = [8, 16]\n# level_num = len(level_channels)    # level depth\nsk1 = 5    # kernel size\nsk2 = 3\nsk3 = 2\nmodel = Model(sensor_num=14, window_size=window_size, r=2, tcn_inputs_size=14, num_channels=level_channels,\n              kernel_size1=sk1, kernel_size2=sk2, kernel_size3=sk3, dropout=0.2, fc_note1=64, fc_note2=32, output_size=1)\nmodel.to(device)\n\n# lossfunction\nmse_loss = nn.MSELoss()\n\n# optimizer\noptimizer = RAdam(model.parameters(), lr=0.001)    \nscheduler = optim.lr_scheduler.StepLR(optimizer, 50, gamma=0.1, last_epoch=-1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Main Function - 3\nnowTime = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')  # 现在时间\nprint(str(nowTime)+': 开始训练模型')\n\nlossdata_train = []\nlossdata_test = []\niter_n = 100\n\n# Runinig\nfor epoch in range(iter_n):\n    print('epoch:', epoch)\n    lossdata_train = train(model, mse_loss, optimizer, lossdata_train, train_data, train_label)\n    # eval testing set data\n    lossdata_test = test(model, mse_loss, lossdata_test, test_data, test_label)\n    scheduler.step()\n        \nplt.title(\"loss with epoch\")\nplt.xlabel(\"epoch\")\nplt.xlim(0, int(iter_n))\nplt.ylabel(\"RMSEloss\")\n\nplt.plot([i for i in range(iter_n)], lossdata_train, 'r', label='training set')\nplt.plot([i for i in range(iter_n)], lossdata_test, 'b', label='testing set')\nplt.legend()\nplt.show()\n\n#save trianed model\ntorch.save(model.state_dict(), '/kaggle/working/SEblockTCN_FD001_trained_'+str(iter_n)+'iter_'+str(nowTime0)+'.pth')\n\nnowTime = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')  # 现在时间\nprint(str(nowTime)+': 训练完成，已保存模型 SongPaper_FD001_trained_'+str(iter_n)+'iter_'+str(nowTime0)+'.pth')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Main Function - 4\nnowTime = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')  # 现在时间\nprint(str(nowTime) + ': 评估最终模型')\n# training set\neval_model(model, train_data, train_label, train_sample_num, train_cyclen_list, trainset=True, Draw=False)\n# testing set\neval_model(model, test_data, test_label, test_sample_num, test_cyclen_list, trainset=False, Draw=False)\nnowTime = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')  # 现在时间\nprint(str(nowTime)+': 本次训练评估实验完成')\ntime_end=time.time()\nprint('time cost',time_end-time_start,'s')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}