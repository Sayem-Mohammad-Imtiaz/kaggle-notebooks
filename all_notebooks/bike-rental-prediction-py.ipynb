{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Index**\n-     <a href='#1'>1 Introduction</a>\n    - <a href='#1.1'>      1.1 Problem Statement</a>\n    - <a href='#1.2'>      1.2 Data </a>\n-     <a href='#2'>2 Methodology</a>\n    - <a href='#2.1'>  2.1 Exploratory Data Analysis </a>\n        - <a href='#2.1.1'>          2.1.1 Descriptive Analysis </a>\n           - <a href='#2.1.1.1'>            2.1.1.1 Features Analysis </a>\n           - <a href='#2.1.1.2'>            2.1.1.2 Missing Value Analysis </a>\n           - <a href='#2.1.1.3'>            2.1.1.3 Target Variable Analysis </a>\n        - <a href='#2.1.2'>          2.1.2 Visualization </a>\n           - <a href='#2.1.2.1'>            2.1.2.1 Attributes Distributions and trends </a>\n           - <a href='#2.1.2.2'>            2.1.2.2 Outlier Analysis </a>\n    - <a href='#2.2'>      2.2 Data Preprocessing and Analysis </a>\n        - <a href='#2.2.1'>          2.2.1 Outlier Handling </a>\n        - <a href='#2.2.2'>          2.2.2 Feature Selection </a>\n        - <a href='#2.2.3'>          2.2.3 Feature Engineering </a>\n    - <a href='#2.3'>      2.3 Modeling </a>\n        - <a href='#2.3.1'>          2.3.1 Random Sampling</a>\n        - <a href='#2.3.2'>          2.3.2 Multilinear & Regularization Regression </a>\n        - <a href='#2.3.3'>          2.3.3 Random Forest </a>\n        - <a href='#2.3.4'>          2.3.4 Gradient Boosting </a>\n- <a href='#3'>3 Final Model</a>"},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'>1. Introduction</a>\n\nThe usage of bicycles as a mode of transportation has gained traction in recent years due to with environmental and health issues. The cities across the world have successfully rolled out bike sharing programs to encourage usage of bikes. Under such programs, the riders can rent bicycles using manual or automated stalls spread across the city for defined periods. In most cases, riders can pick up bikes from one location and returned them any other designated place. \n \nThe bike sharing programs from across the world are hotspots of all sorts of data, ranging from travel time, start and end location, demographics of riders, and so on. This data along with alternate sources of information such as weather, traffic, terrain, season and so on. \n\n\n## <a id='1.1'>1.1 Problem Statement</a>\n\nThe objective of this Case is to Predication of bike rental count on daily based on the environmental and seasonal settings. The objective is to forecast bike rental demand of Bike sharing program in Washington, D.C based on historical usage patterns in relation with weather, environment and other data. We would be interested in predicting the rentals on various factors including season, temperature, weather and building a model that can successfully predict the number of rentals on relevant factors.\n\n## <a id='1.2'>1.2 Data</a>\n\nThis dataset contains the seasonal and weekly count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding temperature and humidity information. Bike sharing systems are a new way of traditional bike rentals. The wohle process from memberhsip to rental and retrun back has become automatic. The data was generated by 500 bike-sharing programs and was collected by the Laboratory of Artiﬁcial Intelligence and Decision Support (LIAAD), University of Porto. Given below is the description of the data which is a (731, 16) shaped data.\n\n### short description of features\n1.  instant: Record index\n1.  dteday: Date\n1.  season: Season (1:spring, 2:summer, 3:fall, 4:winter)\n1.  yr: Year (0: 2011, 1:2012)\n1.  mnth: Month (1 to 12)\n1.  holiday: weather day is holiday or not (extracted from Holiday Schedule)\n1.  weekday: Day of the week\n1.  workingday: If day is neither weekend nor holiday it's 1, otherwise is 0.\n1.  weathersit: (extracted from Freemeteo)\n>1.  Clear, Few clouds, Partly cloudy, Partly cloudy\n>2.  Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n>3.  Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n>4.  Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n1.  temp: Normalized temperature in Celsius. \n1.  atemp: Normalized feeling temperature in Celsius. \n1.  hum: Normalized humidity. The values are divided to 100 (max)\n1.  windspeed: Normalized wind speed. The values are divided to 67 (max)\n1.  casual: count of casual users\n1.  registered: count of registered users\n1.  cnt: count of total rental bikes including both casual and registered"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport os, sys\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # visualization\nimport matplotlib.pyplot as plt # visualization\nimport random \nimport pandas_profiling as pp\n\n# configure font_scale and linewidth for seaborn\nsns.set_context('paper', font_scale=1.3, rc={\"lines.linewidth\": 2})\n\n# preprocessing and metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, make_scorer\n\n# model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n# regresson model\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n# pipeline builder\nfrom sklearn.pipeline import Pipeline, make_pipeline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list files in dir\nprint(os.listdir('../input/bike-sharing-dataset/'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load data\nbike_day = pd.read_csv('../input/bike-sharing-dataset/day.csv')\nprint('Shape of the data:', bike_day.shape) # shape of data\nbike_day.head() # top 5 rows of data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data informations\nbike_day.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# renaming the columns\nbike_day.rename(columns={'dteday':'datetime','yr':'year','mnth':'month','weathersit':'weather_condition','hum':'humidity','cnt':'total_count'},inplace=True)\n\n#Type casting the datetime and numerical attributes to category\nbike_day['datetime']=pd.to_datetime(bike_day.datetime) # datetime conversion\nbike_day['season']=bike_day.season.astype('category') # categorical conversion\nbike_day['year']=bike_day.year.astype('category')\nbike_day['month']=bike_day.month.astype('category')\nbike_day['holiday']=bike_day.holiday.astype('category')\nbike_day['weekday']=bike_day.weekday.astype('category')\nbike_day['workingday']=bike_day.workingday.astype('category')\nbike_day['weather_condition']=bike_day.weather_condition.astype('category')\n\nbike_day.info() # data information after typecasting","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='2'>2. Methodology</a>\n\n# <a id='2.1'>2.1 Exploratory Data Analysis</a>\nIn this section, we'll explore the attributes and data values. Familiarity with data will provide more insight knowledge for data pre-processing, analysize how to use graphical and numerical techniques to begin uncovering the structure of our data.\n\nBy looking at data I came across that data is without any missing values however, casual user variable has outliers in it. Visualizations of the bike rental count base on the season, month, day of the week, the type of day, is it a weekday, is it a holiday, and the type of weather, then calculating the mean of temperature, humidity, wind speed and rental count. The purpose of this summarization is to ﬁnd a general relationship between variables regardless of which year the data is from.\n\n### <a id='2.1.1'>2.1.1 Descriptive Analysis</a>"},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.1.1.1'>2.1.1.1 Feature Analysis</a>\n\n#### Generating profile report using pandas_profiling\nFor each column the following statistics are presented in an interactive HTML page:\n* Essentials: type, unique values, missing values\n* Quantile statistics : minimum value, Q1, median, Q3, maximum, range, interquartile range\n* Descriptive statistics : mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n* Most frequent values\n* Histogram\n* Correlations highlighting of highly correlated variables, Spearman and Pearson matrixes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# profile report generated in the saved repository as a html file\nprofile = pp.ProfileReport(bike_day)\nprofile.to_file(\"profile.html\") # saving profile report as html doc\nprofile","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Click profile to open profile report in new tab.\n<button><a href='./profile.html' ><b>profile</b></a></button>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#diffrent value counts in each categorical features\ncategorical_col = ['season', 'year', 'month', 'holiday', 'weekday','workingday', 'weather_condition']\nend='\\n'+'*'*10+'\\n' # end line seperator\n\nfor col in categorical_col:\n    print(col,':\\n',bike_day[col].value_counts(),end=end) # listing frequency of each value for all of the categorical features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.1.1.2'>2.1.1.2 Missing Value Analysis</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing value checking\nprint('Number of missing values:\\n',bike_day.isnull().sum())\n\n#  description \nbike_day.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.1.1.3'>2.1.1.3 Sneakpeak for target variable</a>\n\nTarget variable (total_count) distribution\n> target variable is normally distributed, no skewness observed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of target variable\n_ , ax = plt.subplots(1,2, figsize=(15,5)) # 1 row 2 column subplot \n\nsns.distplot(bike_day.total_count, bins=50, ax=ax[0]) # dependent variable distribution plot with 50 bins\nax[0].set_title('Dist plot')\n\nax[1] = bike_day.total_count.plot(kind='kde') # dependent variable KDE plot \nax[1].set_title('KDE plot')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.1.2'>2.1.2 Visualization</a>\n\nIt is the process of projecting the data, or parts of it, into Cartesian space or into abstract images. With a little domain knowledge, data visualizations can be used to express and demonstrate key relationships in plots and charts that are more visceral to yourself and stakeholders than measures of association or significance. In the data mining process, data exploration is leveraged in many different steps including preprocessing, modeling, and interpretation of results.\nOne of our main goals for visualizing the data here, is to observe which features are most intiuitive in predicting target. The other, is to draw general trend, may aid us in model selection and hyper parameter selection.\n\n\n### <a id='2.1.2.1'>2.1.2.1 Attribute Distribution and Trends </a>\n\n####  Categorical features\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = categorical_col\nprint('Categorical features :',', '.join(categorical_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Holiday wise yearly count of bike rental\nbike_day[['season','year', 'total_count', 'holiday']].groupby([ 'year', 'holiday']).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# weather condition wise  count of bike rental\nbike_day[['weather_condition', 'total_count']].groupby(['weather_condition']).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# weather condition wise  avg_count of bike rental\nbike_day[['weather_condition', 'total_count']].groupby(['weather_condition']).mean().round().astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# weather condition wise  avg_count of bike rental\nax = sns.barplot(x='weather_condition',y='total_count',data=bike_day, ci=None)\nax.set_title(\"Weather_condition: avg_count of bike rental\") # set title\nax.set_xticklabels(['Clear', 'Mist', 'Snow']) # set x-tick labels\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2)  =  plt.subplots(nrows=1, ncols=2, figsize=(13, 6)) # 1 row 2 column subplot \n\n# Counts of Bike Rentals by season\nax1 = bike_day[['season','total_count']].groupby(['season']).sum().reset_index().plot(kind='bar',legend = False, title =\"Counts of Bike Rentals by season\", stacked=True, fontsize=12, ax=ax1)\nax1.set_xlabel(\"season\", fontsize=12)  # set x-axis labels\nax1.set_ylabel(\"Count\", fontsize=12)  # set y-axis labels\nax1.set_xticklabels(['spring','summer','fall','winter'])  # set x-tick labels\n\n# Counts of Bike Rentals by weather condition \nax2 = bike_day[['weather_condition','total_count']].groupby(['weather_condition']).sum().reset_index().plot(kind='bar', legend = False, stacked=True, title =\"Counts of Bike Rentals by weather condition\", fontsize=12, ax=ax2)\nax2.set_xlabel(\"weather_condition\", fontsize=12)  # set x-axis labels\nax2.set_ylabel(\"Count\", fontsize=12)  # set y-axis labels\nax2.set_xticklabels(['1: Clear','2: Mist','3: Light Snow'])  # set x-tick labels\n\nf.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count of bike on workingdays/holidays for each season\nbike_day[['season','year', 'total_count', 'holiday']].groupby(['season',  'holiday']).sum().plot(kind='bar') # plotting bar graph\nplt.title('Count of bike on workingdays/holidays for each season') # set title","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (axs1,axs2,axs3) = plt.subplots(ncols=3, figsize=(18,5))  # 1 row 3 column subplot\n\n# Total Count of bike for Each year\nsns.barplot(x='year', y='total_count', data=bike_day, hue='season', ax=axs1, ci=None) # barplot\naxs1.set_title('Total Count of bike for Each year') # set tilte\naxs1.legend(['spring','summer','fall','winter']) # set legend\naxs1.set_xticklabels(['2011', '2012']) # set x-tick label\n\n# Casual Count of bike for Each year\nsns.barplot(x='year', y='casual', data=bike_day, hue='season', ax=axs2, ci=None) # barplot\naxs2.set_title('Casual Count of bike for Each year') # set title\naxs2.legend(['spring','summer','fall','winter']) # set legend\naxs2.set_xticklabels(['2011', '2012']) # set x-tick label\n\n# Registered Count of bike for Each year\nsns.barplot(x='year', y='registered', data=bike_day, hue='season', ax=axs3, ci=None) # barplot\naxs3.set_title('Registered Count of bike for Each year') # set title\naxs3.legend(['spring','summer','fall','winter']) # set legend\naxs3.set_xticklabels(['2011', '2012']) # set x-tick label\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (axs1,axs2,axs3) = plt.subplots(ncols=3, figsize=(18,5)) # 1 row 3 column subplot\n\n# Total Count of bike, season wise\nsns.barplot(x='season', y='total_count', data=bike_day, hue='weather_condition', ax=axs1, ci=None) # barplot\naxs1.set_title('Total Count of bike, season wise') # set title\naxs1.set_xticklabels(['spring','summer','fall','winter']) # set x-tick label\naxs1.legend(labels=['Clean', 'Mist', 'Snow']) # set legend\n\n# Casual Count of bike, season wise\nsns.barplot(x='season', y='casual', data=bike_day, hue='weather_condition', ax=axs2, ci=None) # barplot\naxs2.set_title('Casual Count of bike, season wise') # set title\naxs2.set_xticklabels(['spring','summer','fall','winter']) # set x-tick label\naxs2.legend(labels=['Clean', 'Mist', 'Snow']) # set legend\n\n# Registered Count of bike, season wise\nsns.barplot(x='season', y='registered', data=bike_day, hue='weather_condition', ax=axs3, ci=None) # barplot\naxs3.set_title('Registered Count of bike, season wise') # set title\naxs3.set_xticklabels(['spring','summer','fall','winter']) # set x-tick label\naxs3.legend(labels=['Clean', 'Mist', 'Snow']) # set legend\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Season-wise weekday bike count\nplt.figure(figsize=(15,8)) # set figure size\n\nsns.barplot(x='season', y='total_count', data=bike_day, hue='weekday', ci=None) # barplot\nplt.legend(['Monday', 'Tuesday', 'Wednesday', 'Thrusday', 'Friday', 'Saturday', 'Sunday']) # set legend\nplt.title('# Season-wise weekday bike count') # set title\nplt.xticks(np.arange(4),['spring','summer','fall','winter']) # set x-tick label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Season-wise holiday bike count\nplt.figure(figsize=(10,8)) # set figure size\n\nsns.barplot(x='season', y='total_count', data=bike_day, hue='holiday', ci=None) # barplot\nplt.legend(title='Day',labels= ['Holiday', 'Workday']) # set legend\nplt.title('Season-wise holiday bike count') # set title\nplt.xticks(np.arange(4),['spring','summer','fall','winter']) # set x-tick label\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Consistency of Bike count on weekdays by monthly basis\nplt.figure(figsize=(18,8)) # set figure size\n\nsns.pointplot(x='month', y='total_count', data=bike_day, hue='weekday', ci=None) # pointplot\nplt.legend(['Monday', 'Tuesday', 'Wednesday', 'Thrusday', 'Friday', 'Saturday', 'Sunday']) # set legend\nplt.title('Bike count on weekdays by monthly basis') # set title","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, ax=plt.subplots(nrows=2, ncols=1, figsize=(15,10))\n\n# Total count: season wise of weekdays\nsns.pointplot(x='season', y='total_count', data=bike_day, hue='weekday', ci=None, ax=ax[0]) # pointplot\nax[0].set_title('Total count: season wise of weekdays') # set title\nax[0].legend(['Monday', 'Tuesday', 'Wednesday', 'Thrusday', 'Friday', 'Saturday', 'Sunday']) # set legend\nax[0].set_xticklabels(['spring','summer','fall','winter']) # set x-tick label\n\n# Total count: season wise of both year\nsns.pointplot(x='season', y='total_count', data=bike_day, hue='year', ci=None, ax=ax[1]) # pointplot\nax[1].set_title('Total count: season wise of both year') # set title\nax[1].legend(['2011', '2012']) # set legend\nax[1].set_xticklabels(['spring','summer','fall','winter']) # set x-tick label\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Extract weekend column as a feature by using datetime feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generally, 1-Monday and 0-Sunday in datetime , for weekend: Saturday-6 & Sunday-0\n# creating new feature 'isweekend' using datetime feature and computing is that date is fall over the weekend\nbike_day['isweekend']=bike_day['datetime'].apply( lambda x :1 if (x.weekday()==0) |(x.weekday()==6) else 0 ) # for weekday = 0 or 6, isweekend = 1 else 0 \nbike_day[bike_day['isweekend']==1]['weekday'].value_counts() # number of weekend days","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Days wise average causal and registered bike count\n\ncasual_avg = bike_day.groupby(['weekday'])['casual'].mean().round().astype(int) # average casual rental on weekday\nregistered_avg = bike_day.groupby(['weekday'])['registered'].mean().round().astype(int) # average casual rental on weekday\n\nprint('Total count: casual {}, registered {}'.format(casual_avg.sum(), registered_avg.sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avg Use of the bikes by casual users on weekdays\n_, ax=plt.subplots(nrows=2, ncols=1, figsize=(15,10)) # 2 row 1 col subplot\nsns.pointplot(x='weekday', y='casual', data=bike_day, ci=None, ax=ax[0]) # pointplot\nax[0].set(title=\"Avg Use of the bikes by casual users\") # set title\nax[0].set_xticklabels(['Monday', 'Tuesday', 'Wednesday', 'Thrusday', 'Friday', 'Saturday', 'Sunday']) # set x-tick label\n\nfor c in ax[0].collections:\n    for val,of in zip(casual_avg,c.get_offsets()):\n        ax[0].annotate(val, of)                         # set annotations for average of each weekday for  casual rentals\n        \n# Avg Use of the bikes by registered users on weekday\nsns.pointplot(x='weekday', y='registered', data=bike_day, ci=None, ax=ax[1]) # set pointplot\nax[1].set(title=\"Avg Use of the bikes by registered users\") # set title\nax[1].set_xticklabels(['Monday', 'Tuesday', 'Wednesday', 'Thrusday', 'Friday', 'Saturday', 'Sunday']) # set x-tick label\n\nfor c in ax[1].collections:\n    for val,of in zip(registered_avg,c.get_offsets()):\n        ax[1].annotate(val, of)                          # set annotations for average of each weekday for registered rentals\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation: all Categorical features\n- people like to rent bikes more when the sky is clear.\n- the count of number of rented bikes is maximum in fall (Autumn) season and least in spring season.\n- number of bikes rented per season over the years has increased for both casual and registered users.\n- registered users have rented more bikes than casual users overall.\n- casual users travel more over weekends as compared to registered users (Saturday / Sunday).\n- registered users rent more bikes during working days as expected for commute to work / office.\n- demand for bikes are more on working days as compared to holidays ( because majority of the bike users are registered )\n"},{"metadata":{},"cell_type":"markdown","source":"####   Continuous features\n\n<b> A. Pairplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns present in dataset after feature adding\nbike_day.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization of continuous variable varition and thier co-relation\nax = sns.pairplot(bike_day[['temp', 'atemp', 'windspeed', 'humidity']] ) # pairplot\nax.fig.suptitle('Continuous variable varition and thier co-relation',  y=1.0) # set title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> B. Regression plot\n    \n> <b> Regression plot of seaborn used to depict the relationship between continous features and target variable.</b>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regresson plots between temp, windspeed and humidity against total_count\n_ , ax = plt.subplots(1,3, figsize=(18,5)) # 1 row 3 column subplot\n\nsns.regplot(x = 'temp', y='total_count', data=bike_day, ax= ax[0]) # Regression plot\nax[0].set_title('+ve relation between temp and total_count') # set title\n\nsns.regplot(x = 'windspeed', y='total_count', data=bike_day, ax= ax[1]) # Regression plot\nax[1].set_title('-ve relation between windspeed and total_count') # set title\n\nsns.regplot(x = 'humidity', y='total_count', data=bike_day, ax= ax[2]) # Regression plot\nax[2].set_title('+ve relation between humidity and total_count') # set title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\nHere we considered \"count\" vs \"temp\", \"humidity\", \"windspeed\".\n- A +ve correlation with temperature was observed ( sky is clear with increase in temperature )\n- A -ve correlation with humidity and windspeed was observed as people avoid travelling when weather is very windy or humid."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regresson plots between temp, windspeed and humidity to show thier relation with each other\n_ , ax = plt.subplots(1,3, figsize=(18,5)) # 1 row 3 column subplots\n\nsns.regplot(x = 'temp', y='humidity', data=bike_day, ax= ax[0])# Regression plot\nax[0].set_title('+ve relation between temp and humidity') # set title\n\nsns.regplot(x = 'windspeed', y='humidity', data=bike_day, ax= ax[1])# Regression plot\nax[1].set_title('-ve relation between windspeed and humidity') # set title\n\nsns.regplot(x = 'temp', y='windspeed', data=bike_day, ax= ax[2])# Regression plot\nax[2].set_title('+ve relation between temp and windspeed') # set title","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Observation\n\n* A +ve correlation between humidity and temperature was observed (as temp increases the amount of water vapour present in the air also increases)\n* A -ve correlation between windspeed with humidity and temperature was observed (as wind increases, it draws heat from the body, thereby temperature and humidity decreases)"},{"metadata":{},"cell_type":"markdown","source":"<b> C. Correlation | Heatmap </b>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate Co-variance of new data\nbike_corr = bike_day.corr()\n\n# Create mask for upper triangle of co-var matrix \n\nmask1 = np.array(bike_corr)\nmask1[np.tril_indices_from(mask1)] = False # setting upper triangle show to False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# heatmap of continous variables\n_, ax = plt.subplots(1,1, figsize=(15,12)) # 2 row 1 column subplot\nsns.heatmap(bike_corr, mask=mask1, annot=True, square=False, ax=ax) # heatmap\nax.set_title('Corelation of Continuos variables') # set title\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation: continuous features¶\n* Temp, Atemp looks normally distributed.\n* A strong corelation can be seen for temp and atemp.\n* windspeed, humidity, temp and atemp are all normalised in the dataset already.\n* With increase in temperature, the count of bike rentals increases as shown in reg plot."},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.1.2.2'>2.1.2.2  Outlier Analysis: Box Plots, Pair Plots</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outlier Analysis\nfig, axes = plt.subplots(nrows=4,ncols=2) # 4 row 2 column subplots\nfig.set_size_inches(20, 16) # set figure size\nplt.subplots_adjust(hspace=0.3) # set hspace to avoid overlapping\n\n# boxplots for categorical and continuous features\nsns.boxplot(data=bike_day,y=\"total_count\", ax=axes[0][0])\nsns.boxplot(data=bike_day,y=\"total_count\",x=\"season\", ax=axes[0][1])\nsns.boxplot(data=bike_day,y=\"total_count\",x=\"year\", ax=axes[1][0])\nsns.boxplot(data=bike_day,y=\"total_count\",x=\"weather_condition\", ax=axes[1][1])\nsns.boxplot(data=bike_day,y=\"total_count\",x=\"month\", ax=axes[2][0])\nsns.boxplot(data=bike_day,y=\"total_count\",x=\"weekday\", ax=axes[2][1])\nsns.boxplot(data=bike_day,x=\"windspeed\", ax=axes[3][0])\nsns.boxplot(data=bike_day,x=\"humidity\", ax=axes[3][1])\n\n\naxes[0][0].set(ylabel='Count',title=\"Box Plot On Count\")\naxes[0][1].set(ylabel='Count',title=\"Box Plot On Count Across Season\")\naxes[0][1].set_xticklabels(['spring','summer','fall','winter'])\n\naxes[1][0].set(ylabel='Count',title=\"Box Plot On Count Across year\")\naxes[1][0].set_xticklabels(['2011','2012'])\n\naxes[1][1].set(ylabel='Count',title=\"Box Plot On Count Across weather\")\naxes[1][1].set_xticklabels(['Clean','Mist','Snow'])\n\naxes[2][0].set(ylabel='Count',title=\"Box Plot On Count Across month\")\naxes[2][1].set(ylabel='Count',title=\"Box Plot On Count Across weekday\")\naxes[2][1].set_xticklabels(['Monday', 'Tuesday', 'Wednesday', 'Thrusday', 'Friday', 'Saturday', 'Sunday'])\n\naxes[3][0].set(ylabel='windspeed',title=\"Box Plot On windspeed\")\naxes[3][1].set(ylabel='humidity',title=\"Box Plot On humidity\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pairplot for outlier analysis\nax = sns.pairplot(data=bike_day[['humidity','windspeed','temp','total_count']],palette='hls')\nax.fig.suptitle('Pairplot for outlier analysis', y=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observation: \n\n- temp has got positive correlation with count as people like to travel more when the sky is clear.\n- humidity is inversely related to count as expected as when weather is humid people will not like to travel on a bike.\n- windspeed is also having a negative correlation with \"count\".\n- \"atemp\" and \"temp\" variable has got strong correlation with each other. During model building any one of the variable has to be dropped since they will exhibit multicollinearity in the data.\n- \"weather_condition\" and count are inversely related. This is because for our data as weather increases from (1 to 4) implies that weather is getting more worse and so lesser people will rent bikes.\n- \"registered\" and count are highly related which indicates that most of the bikes that are rented are registered.\n- \"Casual\" and \"Registered\" are also not taken into account since they are leakage variables in nature and need to dropped during model building to avoid bias. (casual + registered = count)\n- \"instant\" variable can also be dropped during model building as it indicates index.\n"},{"metadata":{},"cell_type":"markdown","source":"# <a id='2.2'>2.2 Data preprocessing</a>\n\n### <a id='2.2.1'>2.2.1 Outlier handling</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_day.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding outliers \nwind_humidity = pd.DataFrame(bike_day, columns=['windspeed', 'humidity'])\n\n# get outliers for windspeed and humidity features\nfor i in ['windspeed', 'humidity']:\n    q75, q25 = np.percentile(wind_humidity.loc[:,i], [75,25]) # get q75 and q25\n    IQR = q75 - q25 # calculate  IQR for boxplot outlier method\n    max = q75+(IQR*1.5) # get max bound \n    min = q25-(IQR*1.5) # get min bound\n    wind_humidity.loc[wind_humidity.loc[:,i]<min,:i] = np.nan # replacing outliers with NAN\n    wind_humidity.loc[wind_humidity.loc[:,i]>max,:i] = np.nan # replacing outliers with NAN\n\nprint('Shape after dropping outlier (windspeed,humidity):',wind_humidity.dropna().shape)\nprint('Shape before dropping outlier (windspeed,humidity):',bike_day[['windspeed','humidity']].shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculating outlier indexs\nindex=[]\noutlier = pd.DataFrame()\nfor i in range(wind_humidity.shape[0]):\n    if wind_humidity.loc[i,].isna().any(): # if either of windspeed or humidity is NAN, for each column\n        outlier.loc[i,'outlier'] = 1 # store index as outlier 1\n        index.append(i) # store indices of outliers\n    else:\n        outlier.loc[i,'outlier'] = 0\n        \nwind_humidity['outlier'] = outlier['outlier'].astype(int) # convert outlier column as integer \nwind_humidity.loc[index,] # show outliers with thier respective indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_day['outlier'] = wind_humidity['outlier'] # add oulier feature in bike data\n\n#dropping all the outliers present in dataframe\nbike_day.drop(bike_day[(bike_day.outlier==1) ].index, inplace=True) # dropping all the outliers\nprint('Shape after dropping outlier:',bike_day.shape) # shape of the after removing outlier rows\nprint(bike_day.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualization after removing outliers\nax = sns.pairplot(data=bike_day[['humidity','windspeed','temp','total_count']],palette='hls') # pairplot of continuous features\nax.fig.suptitle('Pairplot after removing outliers', y=1.0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.2.2'>2.2.2 Feature Selection</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorising features\ncategorical_features = [\"season\",\"holiday\",\"weather_condition\",\"weekday\",\"month\",\"year\",'isweekend','workingday']\ncontinous_features = [\"temp\",\"humidity\",\"windspeed\"]\ndropFeatures = ['casual',\"datetime\",\"instant\",\"registered\",\"atemp\",\"outlier\"]\ntarget=['total_count']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop unwanted features\nbike_FE = bike_day.drop(dropFeatures, axis=1)\nbike_FE.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.2.3'>2.2.3 Feature Engineering</a>\nConverting categorical features to numercial features to feed our models using <b>\"pd.get_dummies()\"</b>. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# create dummy data \ndummy_data = bike_FE.copy()\n\n# fucntion for creating dummy features\ndef get_dummy(df, col):\n    df = pd.concat([df, pd.get_dummies(df[col], prefix=col, drop_first=True)], axis=1) # create dummy features and dropping first feature, since it's redundant\n    df = df.drop([col], axis = 1)                                                      # drop feature of which dummy is created\n    return df                                                                          # return dummy dataframe\n\n# features to create dummy\n# get_dummy_features = [\"season\",\"weather_condition\",\"weekday\",\"month\"]\nget_dummy_features = categorical_features\n\n# create dummy for features\nfor col in get_dummy_features:\n    dummy_data = get_dummy(dummy_data, col) # create dummy for all categorical features\n    \ndummy_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='2.3'>2.3 Modeling</a>\n\n### <a id='2.3.1'>2.3.1 Sampling</a>\n\n<b> Splitting data</b> in train and test in 75% and 25% of total data respectively"},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting data in test and train set\nX_train, X_test, y_train, y_test = train_test_split(dummy_data.drop(['total_count'], axis=1),dummy_data.total_count,test_size=0.25,random_state=14)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function: get metrics\ndef metrics(regressor_name, regressor, y_pred):\n    \"\"\"Print metrics: r2, adj r2, rmse, rmsle\n    parameters: \n        regressor_name: list, dataframe, matrix\n        regressor: fitted model object\n        y_pred: list, dataframe, matrix\n    \"\"\"\n    print(regressor_name)                          # print regressor name\n    print('R^2:',regressor.score(X_test, y_test))  # Returns the coefficient of determination R^2 of the prediction.\n    print('Adj R^2: ', 1 - (1-regressor.score(X_test, y_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)) # Returns the coefficient of determination Adj R^2 of the prediction.\n    print('RMSE {}: {}'.format(regressor_name,  np.sqrt(mean_squared_error(y_test, y_pred)))) # Retrun rmse score\n    print('RMSLE {}: {}'.format(regressor_name,  np.sqrt(mean_squared_log_error(y_test, y_pred)))) # Retrun rmsle score\n\n# function: plot scores\ndef plot_score(grid_cv, name):\n    score = pd.DataFrame(grid_cv.cv_results_) # create dataframe of the cv results\n    score['alpha'] = score['param_model__alpha'] # rename parameter columns\n    score['r2'] = score['mean_test_score']\n    sns.pointplot(data=score, x='alpha', y='r2', color=random.choice(['r','b','g','y']))\n    plt.title(name)\n    plt.grid(True)\n    plt.show()\n    \n# function: plot residuals graph\ndef plotResiduals(y_test, y_pred, name):\n    residuals = y_test - y_pred                    # get residuals\n    _, ax = plt.subplots()                         # create subplot\n    ax.scatter(y_test, residuals)                  # scatter plot for actuals and residuals\n    ax.axhline()                                   # create reg line\n    ax.set_xlabel('Observed')\n    ax.set_ylabel('Residuals')\n    ax.title.set_text(name[:15]+ ' Residual Plot | RSME: ' + str(np.sqrt(mean_squared_error(y_test, y_pred))))\n    plt.show()\n\n# function: plot distribution graph\ndef distPlot(y_pred, y_test, name):\n    plt.figure(figsize=(10,5))\n    sns.distplot(y_pred)\n    sns.distplot(y_test)\n    plt.legend(['y_pred','y_test'])\n    plt.title(name+'|comparison of prediction distribution')  \n    plt.show()\n\n# function: simple regressor\ndef simpleRegressor(model):\n    \"\"\"Simple Regressor function for each model given in model variable by creating pipeline with StandardScale() and fit data to predict.\n        model: dict()\n        \n    \"\"\"\n    for regressor_name, regressor in linear_models.items():\n        pipeline = Pipeline( [('scaler', StandardScaler()), ('model',regressor)] ) # create pipeline of each model with StandardScale\n        regressor = pipeline.fit(X_train, y_train)                                 # fit data to the pipeline \n        y_pred = regressor.predict(X_test)                                         # Predict using the pipeline model\n     \n        metrics(regressor_name, regressor, y_pred)                                 # get metrics : r2, adj r2, rmse, rmsle\n        print(\"\\n\")\n    \n# function: simple ensemble regressor\ndef ensembleRegressor(model):\n    \"\"\"Ensemble Regressor function for each model given in model variable by creating pipeline with StandardScale() and fit data to predict.\n        model: dict()\n        \n    \"\"\"\n    if type(model) == dict:\n        for regressor_name_, regressor_ in model.items():\n            pipeline = Pipeline( [('scaler', StandardScaler()),  ('model',regressor_)] ) # create pipeline of each model with StandardScale\n            regressor_ = pipeline.fit(X_train, y_train)                                  # fit data to the pipeline \n            y_pred = regressor_.predict(X_test)                                          # Predict using the pipeline model\n            metrics(regressor_name_,regressor_, y_pred )                                 # get metrics : r2, adj r2, rmse, rmsle\n            plotResiduals(y_test, y_pred, regressor_name_)                               # plot residual graphs\n\n            \n# function: Compare Algorithms\ndef comparePlot(results, names):\n    fig = plt.figure()\n    fig.suptitle( 'Algorithm Comparison' )\n    ax = fig.add_subplot(111)\n    plt.scatter(names, results)\n    ax.set_xticklabels(names)\n    plt.show()\n\n# function: ensemble regressor with hyper-parameter tunning\ndef gridSearchRegressor(model, param_grid, scoring = 'r2', plot_score_ = False, compare_score = False):\n    \"\"\"GridSearchRegressor function for each model given in model variable by creating pipeline with StandardScale() and fit data to predict.\n        model: dict()\n        scoring: r2 default\n        plot_score_ = optional False default, for plotting socre\n        compare_score = optional False default, for comparing the model scores\n    \"\"\"\n    best_score = []\n    name = []\n\n    for regressor_name, regressor in model.items():\n        pipeline = Pipeline( [('scaler', StandardScaler()),  ('model',regressor)] ) # create pipeline of each model with StandardScale\n        grid_cv = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring = scoring, cv = 5) # estimate best parameter using gridSearchCV()\n        grid_cv.fit(X_train, y_train) #  # fit data to the pipeline \n        \n        name.append(regressor_name)\n        best_score.append(grid_cv.best_score_) # append best score from gridSearch\n        \n        print(regressor_name)\n        print('Best score:',grid_cv.best_score_) # print best score from gridSearch\n        print('Best param:',grid_cv.best_params_) # print best parameter from gridSearch\n        \n        best_grid = grid_cv.best_estimator_ # get best score from gridSearch best estimator\n        y_pred=best_grid.predict(X_test) # predict using best estimator\n          \n        metrics(regressor_name,best_grid, y_pred )  # get metrics : r2, adj r2, rmse, rmsle\n        plotResiduals(y_test, y_pred, regressor_name)        \n        distPlot(y_pred, y_test, name=regressor_name) \n        if plot_score_:\n            plot_score(grid_cv, regressor_name) # plot score\n    if compare_score:    \n        comparePlot(best_score, name)    # plot comparison\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.3.2'>2.3.2 MultiLinear & Regularization model: LinearRegression| Ridge| Lasso| ElasticNet</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# liner models\nlinear_models = {'LinearRegression':LinearRegression(), 'Ridge':Ridge(), 'Lasso':Lasso(), 'ElasticNet':ElasticNet()}\n\n# Score without StandardScale\nprint('Score without StandardScale:\\n')\nfor regressor_name, regressor in linear_models.items():\n    regressor.fit(X_train, y_train) # fit data to models\n    y_pred = regressor.predict(X_test) # predict using models\n\n    metrics(regressor_name, regressor, y_pred)\n    print(\"\\n\")\n\n# Score without StandardScale:\n\n# LinearRegression\n# R^2: 0.8503150391570842\n# Adj R^2:  0.8213759467274537\n# RMSE LinearRegression: 720.8842925663198\n# RMSLE LinearRegression: 0.2465516805375247\n\n\n# Ridge\n# R^2: 0.8478419303061329\n# Adj R^2:  0.8184247034986518\n# RMSE Ridge: 726.8151540447384\n# RMSLE Ridge: 0.2381610980768202\n\n\n# Lasso\n# R^2: 0.8504890420887281\n# Adj R^2:  0.8215835902258822\n# RMSE Lasso: 720.4651707841829\n# RMSLE Lasso: 0.23878836957603555\n\n\n# ElasticNet\n# R^2: 0.379145172923085\n# Adj R^2:  0.25911323968821476\n# RMSE ElasticNet: 1468.1536519170734\n# RMSLE ElasticNet: 0.46300107772627347\n\n    \n    \n# Score with StandardScale\nprint('Score with StandardScale:\\n')\nsimpleRegressor(linear_models)\n\n\n\n# Score with StandardScale:\n\n# LinearRegression\n# R^2: 0.8503150391570844\n# Adj R^2:  0.8213759467274541\n# RMSE LinearRegression: 720.8842925663195\n# RMSLE LinearRegression: 0.24655168053752335\n\n\n# Ridge\n# R^2: 0.8502971862653109\n# Adj R^2:  0.8213546422766044\n# RMSE Ridge: 720.9272811386522\n# RMSLE Ridge: 0.24640128765416605\n\n\n# Lasso\n# R^2: 0.85043352042173\n# Adj R^2:  0.8215173343699311\n# RMSE Lasso: 720.5989325995707\n# RMSLE Lasso: 0.24569841589499636\n\n\n# ElasticNet\n# R^2: 0.7690031028056619\n# Adj R^2:  0.7243437026814232\n# RMSE ElasticNet: 895.528789006083\n# RMSLE ElasticNet: 0.2952233398791348","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regularizarion models with hyper_tunning parameter:  Ridge| Lasso| ElasticNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# linear regularization model with hypertuning parameters\nregularization_linear_models = { 'Ridge':Ridge(), 'Lasso':Lasso(), 'ElasticNet':ElasticNet()}\n\n# Regularization hyper-parameter tunning with GridSearchCV\nprint('Score with GridSearchCV:\\n')\nparam_grid1 = {'model__alpha' : [0.1, 1, 2, 3, 4, 5, 10, 30, 50, 80, 100],'model__max_iter':[3000] } # parameters for feeding in gridSearch\ngridSearchRegressor(regularization_linear_models, param_grid=param_grid1, plot_score_=True, compare_score=True )\n\n    \n# Score with GridSearchCV:\n\n# Ridge\n# Best score: 0.8223825219791182\n# Best param: {'model__alpha': 5, 'model__max_iter': 3000}\n# Ridge\n# R^2: 0.8501161318996594\n# Adj R^2:  0.8211385840669269\n# RMSE Ridge: 721.3631032400775\n# RMSLE Ridge: 0.24571028154769625\n\n# Lasso\n# Best score: 0.8220643248274393\n# Best param: {'model__alpha': 3, 'model__max_iter': 3000}\n# Lasso\n# R^2: 0.8505544480598202\n# Adj R^2:  0.8215178229750227\n# RMSE Lasso: 720.3075640121864\n# RMSLE Lasso: 0.24420533245983528\n# Best param: {'model__alpha': 3, 'model__max_iter': 3000}\n\n# ElasticNet\n# Best score: 0.8209891999677044\n# Best param: {'model__alpha': 0.1, 'model__max_iter': 3000}\n# ElasticNet\n# R^2: 0.8473885489959245\n# Adj R^2:  0.7243433797638379\n# RMSE ElasticNet: 727.8971844088097\n# RMSLE ElasticNet: 0.24186048423041664\n# Best param: {'model__alpha': 0.1, 'model__max_iter': 3000}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.3.3'>2.3.3 Ensemble model:  RandomForestRegressor</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple RandomForestRegressor\nensemble_model1 = {'RandomForestRegressor':RandomForestRegressor(random_state=867)}\nensembleRegressor(ensemble_model1)\n\n# RandomForestRegressor\n# R^2: 0.8649320139183166\n# Adj R^2:  0.8388188699425245\n# RMSE RandomForestRegressor: 684.7825594871541\n# RMSLE RandomForestRegressor: 0.2333816742631807","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper-parameter tunning: RandomForestRegressor\n\nparam_grid = {\n    'model__n_estimators' : [10,900],\n    'model__max_depth': [5,6,7,10],\n    'model__max_features' : ['log2','sqrt','auto'],\n    'model__random_state': [897]\n}\n\nensemble_model = {'RandomForestRegressor':RandomForestRegressor(random_state=867) }\ngridSearchRegressor(ensemble_model,param_grid=param_grid )\n\n# RandomForestRegressor\n# Best score: 0.8587517092660517\n# Best param: {'model__max_depth': 10, 'model__max_features': 'auto', 'model__n_estimators': 900, 'model__random_state': 897}\n# RandomForestRegressor\n# R^2: 0.8715849042781012\n# Adj R^2:  0.8467579857718674\n# RMSE RandomForestRegressor: 667.7048312259761\n# RMSLE RandomForestRegressor: 0.2349906934957484","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a id='2.3.4'>2.3.4 Boosting model: GradientBoostingRegressor</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple GradientBoostingRegressor\nensemble_model2 = {'GradientBoostingRegressor':GradientBoostingRegressor(random_state=867)}\nensembleRegressor(ensemble_model2)\n\n# GradientBoostingRegressor\n# R^2: 0.8862719523839226\n# Adj R^2:  0.8642845298448143\n# RMSE GradientBoostingRegressor: 628.3625167761045\n# RMSLE GradientBoostingRegressor: 0.19722626748641778","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyper-parameter tunning: GradientBoostingRegressor\n\nhyper_param = {\n                   'model__n_estimators' : [250,400,500,650,800], # The number of boosting stages to perform.\n                   'model__max_depth' : [5,6,7,8], # maximum depth of the individual regression estimators\n                   'model__max_features' : ['log2','sqrt','auto'], # The number of features to consider when looking for the best split:\n                   'model__subsample' : [0.7,0.85,0.9], # The fraction of samples to be used for fitting the individual base learners.  If smaller than 1.0 this results in Stochastic Gradient Boosting.\n                   'model__random_state': [17]\n\n                  }\n\ngbm_ensemble_model = {'GradientBoostingRegressor':GradientBoostingRegressor() }\ngridSearchRegressor(gbm_ensemble_model,hyper_param )\n\n# GradientBoostingRegressor\n# Best score: 0.8869327747143647\n# Best param: {'model__max_depth': 5, 'model__max_features': 'sqrt', 'model__n_estimators': 250, 'model__random_state': 17, 'model__subsample': 0.7}\n# GradientBoostingRegressor\n# R^2: 0.8870138124626565\n# Adj R^2:  0.8651698162054368\n# RMSE GradientBoostingRegressor: 626.3097260903672\n# RMSLE GradientBoostingRegressor: 0.2143924133958078","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='3'>3. Final Model</a>\n\n## Model: GradientBoostingRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final model: Tuned GradientBoostingRegressor\n\n# parameters\nparams = {'max_depth': 5, \n          'max_features': 'sqrt', \n          'n_estimators': 250, \n          'random_state': 147, \n          'subsample': 0.85\n         }\n# regressor\nregressor_name = 'GradientBoostingRegressor'\n\n# pipeline\npipeline = Pipeline( [('scaler', StandardScaler()),  ('model',GradientBoostingRegressor(**params))] ) # create pipeline\npipeline.fit(X_train, y_train)  # fit data to the pipeline\ngbm_y_pred = pipeline.predict(X_test) # make prediction using pipeline\n\n# metrics and plots\nprint(regressor_name)\nmetrics(regressor_name, pipeline, gbm_y_pred ) # \nplotResiduals(y_test, gbm_y_pred, regressor_name)        \ndistPlot(gbm_y_pred, y_test, name=regressor_name)\n\n\n\n# GradientBoostingRegressor\n# R^2: 0.8819155688462875\n# Adj R^2:  0.8590859121565698\n# RMSE GradientBoostingRegressor: 640.28422142774\n# RMSLE GradientBoostingRegressor: 0.21093760709288065","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ##  Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final submission\nbikeTestPred = pd.DataFrame()\nbikeTestPred['y_test'] = y_test\nbikeTestPred['gbm_y_pred'] = gbm_y_pred\nbikeTestPred['gbm_y_pred'] = bikeTestPred['gbm_y_pred'].astype(int)\n\nbikeTestPred.to_csv('Bike_Renting_Python.csv')\nbikeTestPred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Download CSV for Test result:\n<a  href=\"Bike_Renting_Python.csv\" target=\"_blank\">download Bike_Renting_Python</a>\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}