{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## Reading the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/predicting-churn-for-bank-customers/Churn_Modelling.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dividing the data into dependant and independant sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# RowNumber,CustomerId and Surname are not value adding attributes when determining if a particular customer is going to stay in the bank or going to leave the bank. We can remove this two columns from our dependant data set.\nX = data.iloc[:,3:-1].values\nY = data.iloc[:,-1].values\n# 2D arrays of both X & Y created","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Taking care of missing data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()\n# There are no missing values in the data set.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding the categorical variables in X data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding the Gender column using Label Encoder\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\n#Gender column is the second column in the data set.\n#X[:,2] means select all the rows and select second column the X data set.\nX[:,2] = label.fit_transform(X[:,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One Hot encoding the Geography column\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])],remainder='passthrough')\nX = np.array(ct.fit_transform(X))\n# The columns created via one hot encoding needs to be transformed in 2d array.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X)\n#Post One Hot encoding, the geography column has not moved to the first column of the data set.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the data set into testing and training data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape,X_test.shape,Y_train.shape,Y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We shall pe using Standardization for feature scaling\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n# We need to standardize the test data using the same scaler with which we standardized the training data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Artificial Neural Network"},{"metadata":{},"cell_type":"markdown","source":"## Building the ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_variable = tf.keras.models.Sequential()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding the input and hidden layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_variable.add(tf.keras.layers.Dense(units=8,activation='relu'))\n#units tell the number of neurons to be used in the hidden layer\n#relu determines the rectifier threshold activation function used in the ANN","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Addition of another hidden layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_variable.add(tf.keras.layers.Dense(units=8,activation='relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding the Output Layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_variable.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))\n#The output layer is going to contain 1 neuron in our current case. Hence units is 1\n#Acitvation function is sigmoid as the objective of the output layer is to have binary output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training the ANN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_variable.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n#optimizer updates the weights. adam represents the stochastic gradient method.\n#loss calculated the difference b/w actual and predictd values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ann_variable.fit(X_train,Y_train,batch_size=32,epochs=200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = ann_variable.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming the predicted data into binary representation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = (Y_pred > 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Determining the Accuracy of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(Y_pred,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(Y_pred,Y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accuracy score of 85.30% achieved using ANN model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}