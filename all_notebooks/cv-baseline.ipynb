{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Downloading Dependencies","metadata":{"id":"CtLPLbA0cxBE"}},{"cell_type":"code","source":"# First, we need to install pycocotools. This library will be used for computing the evaluation metrics following the COCO metric for intersection over union.\n# The version by default in Colab has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n!pip install cython\n!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'","metadata":{"id":"2h7Cxnb-5XHg","executionInfo":{"status":"ok","timestamp":1623563657036,"user_tz":-480,"elapsed":13988,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"outputId":"a81057b4-3d86-4ccf-8826-3888941dd927","execution":{"iopub.status.busy":"2021-06-13T10:33:03.706034Z","iopub.execute_input":"2021-06-13T10:33:03.706365Z","iopub.status.idle":"2021-06-13T10:33:17.373293Z","shell.execute_reply.started":"2021-06-13T10:33:03.706334Z","shell.execute_reply":"2021-06-13T10:33:17.372372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport math\nimport os\nimport random\nimport sys\nimport time\n\nimport numpy as np\nimport torch\nimport torch.utils.data\nimport torchvision\n\nfrom PIL import Image, ImageDraw\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom torchvision import transforms\nfrom torchvision.ops import batched_nms\nfrom torchvision.transforms import functional as F","metadata":{"id":"xealk6oLy1nH","executionInfo":{"status":"ok","timestamp":1623563659758,"user_tz":-480,"elapsed":2731,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:17.375197Z","iopub.execute_input":"2021-06-13T10:33:17.375542Z","iopub.status.idle":"2021-06-13T10:33:17.383799Z","shell.execute_reply.started":"2021-06-13T10:33:17.375504Z","shell.execute_reply":"2021-06-13T10:33:17.38297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import List, Tuple\n\nimport torchvision.models as models\n\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.ops import MultiScaleRoIAlign","metadata":{"id":"bOh0f5W0zGNR","executionInfo":{"status":"ok","timestamp":1623563659758,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:17.385773Z","iopub.execute_input":"2021-06-13T10:33:17.386098Z","iopub.status.idle":"2021-06-13T10:33:17.39367Z","shell.execute_reply.started":"2021-06-13T10:33:17.386065Z","shell.execute_reply":"2021-06-13T10:33:17.392894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In `references/detection/,` we have a number of helper functions to simplify training and evaluating detection models.\nHere, we will use `references/detection/utils.py` and `references/detection/coco_eval.py`.\n\nLet's copy those files (and their dependencies) in here so that they are available in the notebook","metadata":{"id":"_ssnCCqtzfkn"}},{"cell_type":"code","source":"\n# Download TorchVision repo to use some files from references/detection\n!git clone https://github.com/pytorch/vision.git\n\n# copying files \n!cp ./vision/references/detection/utils.py /kaggle/working\n!cp ./vision/references/detection/coco_eval.py /kaggle/working","metadata":{"id":"Qe2V9r4wzl_q","executionInfo":{"status":"ok","timestamp":1623563665352,"user_tz":-480,"elapsed":5599,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"outputId":"260a871e-8b4e-4761-cc27-b139f46de755","execution":{"iopub.status.busy":"2021-06-13T10:33:17.39531Z","iopub.execute_input":"2021-06-13T10:33:17.395661Z","iopub.status.idle":"2021-06-13T10:33:19.501119Z","shell.execute_reply.started":"2021-06-13T10:33:17.395626Z","shell.execute_reply":"2021-06-13T10:33:19.50001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import utils\nfrom coco_eval import CocoEvaluator","metadata":{"id":"4bpxOrWJzJmh","executionInfo":{"status":"ok","timestamp":1623563665352,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:19.502695Z","iopub.execute_input":"2021-06-13T10:33:19.503058Z","iopub.status.idle":"2021-06-13T10:33:19.510314Z","shell.execute_reply.started":"2021-06-13T10:33:19.503014Z","shell.execute_reply":"2021-06-13T10:33:19.509538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if cuda GPU is available, make sure you're using GPU runtime on Google Colab\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device) # you should output \"cuda\"","metadata":{"id":"uUiwwAIP6_o_","executionInfo":{"status":"ok","timestamp":1623563665353,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"outputId":"50dc028c-14fb-4f09-e2eb-1880ec3303aa","execution":{"iopub.status.busy":"2021-06-13T10:33:19.513216Z","iopub.execute_input":"2021-06-13T10:33:19.513466Z","iopub.status.idle":"2021-06-13T10:33:19.519475Z","shell.execute_reply.started":"2021-06-13T10:33:19.513442Z","shell.execute_reply":"2021-06-13T10:33:19.518572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Object Detection Dataset\nWe will be providing the base dataset that will be used for the first task of the Object Detection competition.","metadata":{"id":"t9z4qZ8A8HTn"}},{"cell_type":"code","source":"# Kaggle dataset\n\ntraining_path = \"../input/dsta-brainhack-2021/c1_release/c1_release\"","metadata":{"id":"eghj9hRaFWUz","executionInfo":{"status":"ok","timestamp":1623564832930,"user_tz":-480,"elapsed":302,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:19.520915Z","iopub.execute_input":"2021-06-13T10:33:19.521564Z","iopub.status.idle":"2021-06-13T10:33:19.527708Z","shell.execute_reply.started":"2021-06-13T10:33:19.521528Z","shell.execute_reply":"2021-06-13T10:33:19.526854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at the dataset and how it is layed down.\n\nThe data is structured as follows\n```\nc1_release/\n  images/\n    0001e6adc4fbab0c.jpg\n    00067fe83e3e21c8.jpg\n    0008ab3d8674f6ca.jpg\n    ...\n  labels.json\n  train.json\n  val.json\n```\n\n`labels.json` contains the labels for the whole dataset (`train.json` + `val.json`) if you need it.","metadata":{"id":"8KW4FNBBCt4E"}},{"cell_type":"markdown","source":"### Defining the Dataset\n\nThe [torchvision reference scripts for training object detection](https://github.com/pytorch/vision/tree/v0.3.0/references/detection) allows for easily supporting adding new custom datasets.\nThe dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n\nThe only specificity that we require is that the dataset `__getitem__` should return:\n\n* image: a PIL Image of size (H, W)\n* target: a dict containing the following fields\n    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n\nIf your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.\n\nAdditionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.\n\nLet's write a `torch.utils.data.Dataset` class for this dataset.","metadata":{"id":"lBZhr1inB_RU"}},{"cell_type":"code","source":"class TILDataset(torch.utils.data.Dataset):\n    def __init__(self, root, annotation, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        self.coco = COCO(annotation)\n        self.ids = list(sorted(self.coco.imgs.keys()))\n        cats = self.coco.loadCats(self.coco.getCatIds())\n        self.cat2name = {cat['id']:cat['name'] for cat in cats} # maps category id to category name (useful for visualization)\n\n    def __getitem__(self, index):\n        coco = self.coco\n        img_id = self.ids[index] # Image ID\n        ann_ids = coco.getAnnIds(imgIds=img_id) # get annotation id from coco\n        coco_annotation = coco.loadAnns(ann_ids) # target coco_annotation file for an image\n        path = coco.loadImgs(img_id)[0]['file_name'] # path for input image\n        img = Image.open(os.path.join(self.root, 'images', path)).convert('RGB') # open the input image\n\n        # number of objects in the image\n        num_objs = len(coco_annotation)\n\n        # Bounding boxes for objects\n        # In coco format, bbox = [xmin, ymin, width, height]\n        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n        boxes = []\n        for i in range(num_objs):\n            xmin = coco_annotation[i]['bbox'][0]\n            ymin = coco_annotation[i]['bbox'][1]\n            xmax = xmin + coco_annotation[i]['bbox'][2]\n            ymax = ymin + coco_annotation[i]['bbox'][3]\n            boxes.append([xmin, ymin, xmax, ymax])\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n\n        # Labels\n        labels = []\n        for i in range(num_objs):\n            labels.append(coco_annotation[i]['category_id'])\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        # Tensorise img_id\n        img_id = torch.tensor([img_id])\n\n        # Size of bbox (Rectangular)\n        areas = []\n        for i in range(num_objs):\n            areas.append(coco_annotation[i]['area'])\n        areas = torch.as_tensor(areas, dtype=torch.float32)\n\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        # Annotation is in dictionary format\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = img_id\n        target[\"area\"] = areas\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ids)","metadata":{"id":"j8Iv0DXlINyH","executionInfo":{"status":"ok","timestamp":1623565036403,"user_tz":-480,"elapsed":284,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:19.531078Z","iopub.execute_input":"2021-06-13T10:33:19.531384Z","iopub.status.idle":"2021-06-13T10:33:19.552436Z","shell.execute_reply.started":"2021-06-13T10:33:19.531359Z","shell.execute_reply":"2021-06-13T10:33:19.551641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:33:19.554573Z","iopub.execute_input":"2021-06-13T10:33:19.55522Z","iopub.status.idle":"2021-06-13T10:33:19.56952Z","shell.execute_reply.started":"2021-06-13T10:33:19.555181Z","shell.execute_reply":"2021-06-13T10:33:19.568444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's all for the dataset. Let's see how the outputs are structured for this dataset","metadata":{"id":"oWm6AvzjDdLs"}},{"cell_type":"code","source":"til_root = \"../input/dsta-brainhack-2021/c1_release/c1_release\" # extracted training dataset path\ntrain_annotation = os.path.join(til_root, \"train.json\")\nprint(train_annotation==\"../input/dsta-brainhack-2021/c1_release/c1_release/train.json\")\nval_annotation = os.path.join(til_root, \"val.json\")\n\ndataset = TILDataset(til_root, train_annotation)\ndataset[30]","metadata":{"id":"19AIZfTM4R4a","executionInfo":{"status":"ok","timestamp":1623565041099,"user_tz":-480,"elapsed":790,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"outputId":"4cba6eba-cedc-4ca0-f11a-d1b013d54c68","execution":{"iopub.status.busy":"2021-06-13T10:33:19.572203Z","iopub.execute_input":"2021-06-13T10:33:19.573884Z","iopub.status.idle":"2021-06-13T10:33:19.656286Z","shell.execute_reply.started":"2021-06-13T10:33:19.573846Z","shell.execute_reply":"2021-06-13T10:33:19.655554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we can see that by default, the dataset returns a `PIL.Image` and a dictionary containing several fields, including `boxes` and `labels`.\n\nLet's next look at one example from the training set.","metadata":{"id":"jNUhbVfc4xVz"}},{"cell_type":"code","source":"source_img, img_annots = dataset[30]\ndraw = ImageDraw.Draw(source_img)\nfor i in range(len(img_annots[\"boxes\"])):\n    x1, y1, x2, y2 = img_annots[\"boxes\"][i]\n    label = int(img_annots[\"labels\"][i])\n\n    draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n    text = f'{dataset.cat2name[label]}'\n    draw.text((x1+5, y1+5), text)\ndisplay(source_img)","metadata":{"id":"vgvaYqPf6uyo","execution":{"iopub.status.busy":"2021-06-13T10:33:19.657483Z","iopub.execute_input":"2021-06-13T10:33:19.657846Z","iopub.status.idle":"2021-06-13T10:33:20.050888Z","shell.execute_reply.started":"2021-06-13T10:33:19.65781Z","shell.execute_reply":"2021-06-13T10:33:20.050123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting up the Model\n\nIn this object detection example, we will make use of Faster R-CNN model with a ResNet50-FPN backbone. To understand the underlying code structure, you can read this [article](https://zhuanlan.zhihu.com/p/145842317) (right click and translate to English).\n\nFeel free to explore with different hyper-parameters to see what works best!","metadata":{"id":"3OWoxpWcM3D4"}},{"cell_type":"code","source":"# hyper-parameters\n# change here for model\nparams = {'BATCH_SIZE': 16, # affect the most\n          'LR': 0.05, # affect\n          'CLASSES': 5,\n          'MAXEPOCHS': 20,\n          'BACKBONE': 'resnet101',\n          'FPN': True,\n          'ANCHOR_SIZE': ((32,), (64,), (128,), (256,), (512,)),\n          'ASPECT_RATIOS': ((0.5, 1.0, 2.0),),\n          'MIN_SIZE': 512,\n          'MAX_SIZE': 512,\n          'IMG_MEAN': [0.485, 0.456, 0.406],\n          'IMG_STD': [0.229, 0.224, 0.225],\n          'IOU_THRESHOLD': 0.5\n          }","metadata":{"id":"c96yh1yaykGE","executionInfo":{"status":"ok","timestamp":1623565048398,"user_tz":-480,"elapsed":12,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:20.052146Z","iopub.execute_input":"2021-06-13T10:33:20.052638Z","iopub.status.idle":"2021-06-13T10:33:20.059366Z","shell.execute_reply.started":"2021-06-13T10:33:20.052601Z","shell.execute_reply":"2021-06-13T10:33:20.058493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_resnet_backbone(backbone_name: str):\n    \"\"\"\n    Returns a resnet backbone pretrained on ImageNet.\n    Removes the average-pooling layer and the linear layer at the end.\n    \"\"\"\n    if backbone_name == 'resnet18':\n        pretrained_model = models.resnet18(pretrained=True, progress=False)\n        out_channels = 512\n    elif backbone_name == 'resnet34':\n        pretrained_model = models.resnet34(pretrained=True, progress=False)\n        out_channels = 512\n    elif backbone_name == 'resnet50':\n        pretrained_model = models.resnet50(pretrained=True, progress=False)\n        out_channels = 2048\n    elif backbone_name == 'resnet101':\n        pretrained_model = models.resnet101(pretrained=True, progress=False)\n        out_channels = 2048\n    elif backbone_name == 'resnet152':\n        pretrained_model = models.resnet152(pretrained=True, progress=False)\n        out_channels = 2048\n\n    backbone = torch.nn.Sequential(*list(pretrained_model.children())[:-2])\n    backbone.out_channels = out_channels\n\n    return backbone","metadata":{"id":"a0GWSdRqy9pA","executionInfo":{"status":"ok","timestamp":1623565048399,"user_tz":-480,"elapsed":13,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:20.060875Z","iopub.execute_input":"2021-06-13T10:33:20.061424Z","iopub.status.idle":"2021-06-13T10:33:20.070727Z","shell.execute_reply.started":"2021-06-13T10:33:20.061383Z","shell.execute_reply":"2021-06-13T10:33:20.069865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_resnet_fpn_backbone(backbone_name: str):\n    \"\"\"\n    Returns a specified ResNet backbone with FPN pretrained on ImageNet.\n    \"\"\"\n    return resnet_fpn_backbone(backbone_name, pretrained=True, trainable_layers=3)","metadata":{"id":"Un4UIW_d8PEe","executionInfo":{"status":"ok","timestamp":1623565048400,"user_tz":-480,"elapsed":13,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:20.072173Z","iopub.execute_input":"2021-06-13T10:33:20.0727Z","iopub.status.idle":"2021-06-13T10:33:20.079468Z","shell.execute_reply.started":"2021-06-13T10:33:20.072657Z","shell.execute_reply":"2021-06-13T10:33:20.078715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_anchor_generator(anchor_size: Tuple[tuple] = None, aspect_ratios: Tuple[tuple] = None):\n    \"\"\"Returns the anchor generator.\"\"\"\n    if anchor_size is None:\n        anchor_size = ((16,), (32,), (64,), (128,))\n    if aspect_ratios is None:\n        aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_size)\n\n    anchor_generator = AnchorGenerator(sizes=anchor_size,\n                                       aspect_ratios=aspect_ratios)\n    return anchor_generator","metadata":{"id":"W5oH8VNty-kX","executionInfo":{"status":"ok","timestamp":1623565048400,"user_tz":-480,"elapsed":13,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:20.080592Z","iopub.execute_input":"2021-06-13T10:33:20.081146Z","iopub.status.idle":"2021-06-13T10:33:20.093654Z","shell.execute_reply.started":"2021-06-13T10:33:20.081113Z","shell.execute_reply":"2021-06-13T10:33:20.092837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_roi_pool(featmap_names: List[str] = None, output_size: int = 7, sampling_ratio: int = 2):\n    \"\"\"Returns the ROI Pooling\"\"\"\n    if featmap_names is None:\n        # default for resnet with FPN\n        featmap_names = ['0', '1', '2', '3']\n\n    roi_pooler = MultiScaleRoIAlign(featmap_names=featmap_names,\n                                    output_size=output_size,\n                                    sampling_ratio=sampling_ratio)\n\n    return roi_pooler","metadata":{"id":"Uj92yxjOzNft","executionInfo":{"status":"ok","timestamp":1623565048401,"user_tz":-480,"elapsed":13,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:20.095117Z","iopub.execute_input":"2021-06-13T10:33:20.095704Z","iopub.status.idle":"2021-06-13T10:33:20.102971Z","shell.execute_reply.started":"2021-06-13T10:33:20.095657Z","shell.execute_reply":"2021-06-13T10:33:20.102111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_fasterRCNN(backbone: torch.nn.Module,\n                   anchor_generator: AnchorGenerator,\n                   roi_pooler: MultiScaleRoIAlign,\n                   num_classes: int,\n                   image_mean: List[float] = [0.485, 0.456, 0.406],\n                   image_std: List[float] = [0.229, 0.224, 0.225],\n                   min_size: int = 512,\n                   max_size: int = 1024,\n                   **kwargs\n                   ):\n    \"\"\"Returns the Faster-RCNN model. Default normalization: ImageNet\"\"\"\n    model = FasterRCNN(backbone=backbone,\n                       rpn_anchor_generator=anchor_generator,\n                       box_roi_pool=roi_pooler,\n                       num_classes=num_classes,\n                       image_mean=image_mean,  # ImageNet\n                       image_std=image_std,  # ImageNet\n                       min_size=min_size,\n                       max_size=max_size,\n                       **kwargs\n                       )\n    model.num_classes = num_classes\n    model.image_mean = image_mean\n    model.image_std = image_std\n    model.min_size = min_size\n    model.max_size = max_size\n\n    return model","metadata":{"id":"OQNv6A4zzRqc","executionInfo":{"status":"ok","timestamp":1623565048401,"user_tz":-480,"elapsed":13,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:20.104378Z","iopub.execute_input":"2021-06-13T10:33:20.104958Z","iopub.status.idle":"2021-06-13T10:33:20.113738Z","shell.execute_reply.started":"2021-06-13T10:33:20.104921Z","shell.execute_reply":"2021-06-13T10:33:20.11288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_fasterRCNN_resnet(num_classes: int,\n                          backbone_name: str,\n                          anchor_size: List[float],\n                          aspect_ratios: List[float],\n                          fpn: bool = True,\n                          min_size: int = 512,\n                          max_size: int = 1024,\n                          **kwargs\n                          ):\n    \"\"\"Returns the Faster-RCNN model with resnet backbone with and without fpn.\"\"\"\n\n    # Backbone\n    if fpn:\n        backbone = get_resnet_fpn_backbone(backbone_name=backbone_name)\n    else:\n        backbone = get_resnet_backbone(backbone_name=backbone_name)\n\n    # Anchors\n    anchor_size = anchor_size\n    aspect_ratios = aspect_ratios * len(anchor_size)\n    anchor_generator = get_anchor_generator(anchor_size=anchor_size, aspect_ratios=aspect_ratios)\n\n    # ROI Pool\n    with torch.no_grad():\n        backbone.eval()\n        random_input = torch.rand(size=(1, 3, 512, 512))\n        features = backbone(random_input)\n\n    if isinstance(features, torch.Tensor):\n        from collections import OrderedDict\n\n        features = OrderedDict([('0', features)])\n\n    featmap_names = [key for key in features.keys() if key.isnumeric()]\n\n    roi_pool = get_roi_pool(featmap_names=featmap_names)\n\n    # Model\n    return get_fasterRCNN(backbone=backbone,\n                          anchor_generator=anchor_generator,\n                          roi_pooler=roi_pool,\n                          num_classes=num_classes,\n                          min_size=min_size,\n                          max_size=max_size,\n                          **kwargs)\n","metadata":{"id":"wfn_4EbLzfHf","executionInfo":{"status":"ok","timestamp":1623565048401,"user_tz":-480,"elapsed":12,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:20.11503Z","iopub.execute_input":"2021-06-13T10:33:20.115636Z","iopub.status.idle":"2021-06-13T10:33:20.126447Z","shell.execute_reply.started":"2021-06-13T10:33:20.1156Z","shell.execute_reply":"2021-06-13T10:33:20.125553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_fasterRCNN_resnet(num_classes=params['CLASSES'],\n                              backbone_name=params['BACKBONE'],\n                              anchor_size=params['ANCHOR_SIZE'],\n                              aspect_ratios=params['ASPECT_RATIOS'],\n                              fpn=params['FPN'],\n                              min_size=params['MIN_SIZE'],\n                              max_size=params['MAX_SIZE'],\n                              image_mean=params['IMG_MEAN'],\n                              image_std=params['IMG_STD'])","metadata":{"id":"hljmqSJOzibW","executionInfo":{"status":"ok","timestamp":1623565050539,"user_tz":-480,"elapsed":2150,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"outputId":"3ad91040-547e-456f-e2ec-b40bfcb51b39","execution":{"iopub.status.busy":"2021-06-13T10:33:20.127744Z","iopub.execute_input":"2021-06-13T10:33:20.128126Z","iopub.status.idle":"2021-06-13T10:33:21.772474Z","shell.execute_reply.started":"2021-06-13T10:33:20.128089Z","shell.execute_reply":"2021-06-13T10:33:21.771575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load pretrained weights for FasterRCNN ResNet50 FPN\npretrained_dict = torch.hub.load_state_dict_from_url('https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth', progress=True)\nmodel_dict = model.state_dict()\n\n# filter out roi_heads.box_predictor weights\npretrained_dict = {k: v for k, v in pretrained_dict.items() if not k.startswith('roi_heads.box_predictor')}\n# overwrite entries in the existing state dict\nmodel_dict.update(pretrained_dict)\n# load the new state dict\nmodel.load_state_dict(model_dict)","metadata":{"id":"FpG3AhNjOJ6-","executionInfo":{"status":"ok","timestamp":1623565051937,"user_tz":-480,"elapsed":1404,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"outputId":"fb3d13c4-8ccc-40ee-f9f7-485c4f74909f","execution":{"iopub.status.busy":"2021-06-13T10:33:21.774579Z","iopub.execute_input":"2021-06-13T10:33:21.774852Z","iopub.status.idle":"2021-06-13T10:33:21.893769Z","shell.execute_reply.started":"2021-06-13T10:33:21.774825Z","shell.execute_reply":"2021-06-13T10:33:21.89281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# move model to the right device\nmodel.to(device)","metadata":{"id":"dacuDIn3z69Y","executionInfo":{"status":"ok","timestamp":1623565051937,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"outputId":"b5165dbe-4b4a-4da2-9597-aeac8226230b","execution":{"iopub.status.busy":"2021-06-13T10:40:00.032062Z","iopub.execute_input":"2021-06-13T10:40:00.032389Z","iopub.status.idle":"2021-06-13T10:40:00.062238Z","shell.execute_reply.started":"2021-06-13T10:40:00.03236Z","shell.execute_reply":"2021-06-13T10:40:00.060234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# construct an optimizer\nmodel_params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(model_params, \n                            lr=params['LR'],\n                            momentum=0.9, \n                            weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{"id":"5iPlWCjV28P5","executionInfo":{"status":"ok","timestamp":1623565051938,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:21.931075Z","iopub.status.idle":"2021-06-13T10:33:21.931473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation\n\nLet's write some helper functions for data augmentation / transformation.\n\nDo not just stop here, add in your own data augmentations! Remember to also augment the bounding boxes accordingly.","metadata":{"id":"E3_rz0d4Khpn"}},{"cell_type":"code","source":"class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target","metadata":{"id":"Bmr8kmOZhJ91","executionInfo":{"status":"ok","timestamp":1623565051938,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:21.932574Z","iopub.status.idle":"2021-06-13T10:33:21.933137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converts the image, a PIL image, into a PyTorch Tensor\nclass ToTensor(object):\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target","metadata":{"id":"iNeXIW8shR0_","executionInfo":{"status":"ok","timestamp":1623565051938,"user_tz":-480,"elapsed":5,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:21.934384Z","iopub.status.idle":"2021-06-13T10:33:21.934956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# randomly horizontal flip the images and ground-truth labels\nclass RandomFlip(object):\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n        elif random.random() > self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip()\n            bbox = target[\"boxes\"]\n            bbox[[0,2], :] = height - bbox[[2,0], :]\n            target[\"boxes\"] = bbox\n        return image, target","metadata":{"id":"jwFX52gbhUQX","executionInfo":{"status":"ok","timestamp":1623565051939,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:21.936187Z","iopub.status.idle":"2021-06-13T10:33:21.936763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mirror images and ground-truth labels\nclass RandomMirror(object):\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.mirror()\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n        return image, target","metadata":{"execution":{"iopub.status.busy":"2021-06-13T10:33:21.938132Z","iopub.status.idle":"2021-06-13T10:33:21.938813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transform(train):\n    if train:\n        transforms = Compose([\n            ToTensor(), \n            RandomHorizontalFlip(0.5)\n        ])\n    else: # during evaluation, no augmentations will be done\n        transforms = Compose([\n            ToTensor()\n        ])\n    \n    return transforms","metadata":{"id":"omm4z5GsJ-wS","executionInfo":{"status":"ok","timestamp":1623565051939,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:21.940193Z","iopub.status.idle":"2021-06-13T10:33:21.940808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled by the R-CNN model.","metadata":{"id":"aqp_tYw1IXvI"}},{"cell_type":"markdown","source":"## Data Loaders\n\nLet's now set up our data loaders so that we can streamline the batch loading of data for our model training later on.\n\nWe now have the dataset class, the models and the data transforms. Let's instantiate them","metadata":{"id":"Q9hhlhDDMEos"}},{"cell_type":"code","source":"NUM_WORKERS = 4\n\n# use our dataset and defined transformations\ntrain_dataset = TILDataset(til_root, train_annotation, get_transform(train=True))\nval_dataset = TILDataset(til_root, val_annotation, get_transform(train=False))\n\n# define training and validation data loaders\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=params['BATCH_SIZE'], shuffle=True, num_workers=NUM_WORKERS,\n    collate_fn=utils.collate_fn)\n\nval_loader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=1, shuffle=False, num_workers=NUM_WORKERS,\n    collate_fn=utils.collate_fn)","metadata":{"id":"0DBaaGV7JP1W","executionInfo":{"status":"ok","timestamp":1623565051939,"user_tz":-480,"elapsed":5,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"outputId":"0c88db21-0806-41b4-b6d6-c0dc8697742e","execution":{"iopub.status.busy":"2021-06-13T10:33:21.942525Z","iopub.status.idle":"2021-06-13T10:33:21.943219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training\n\nAnd now let's train the model, evaluating at the end of every epoch.","metadata":{"id":"0amZn1wMHvkP"}},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1. / 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            print(loss_dict)\n            sys.exit(1)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        metric_logger.update(loss=losses, **loss_dict)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])","metadata":{"id":"QnNPceNj5vTc","executionInfo":{"status":"ok","timestamp":1623565052454,"user_tz":-480,"elapsed":519,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:21.944543Z","iopub.status.idle":"2021-06-13T10:33:21.945222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, data_loader, device):\n    n_threads = torch.get_num_threads()\n    torch.set_num_threads(1)\n    model.eval()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Test:'\n\n    coco = data_loader.dataset.coco\n    iou_types = [\"bbox\"]\n    coco_evaluator = CocoEvaluator(coco, iou_types)\n\n    for image, targets in metric_logger.log_every(data_loader, 100, header):\n        image = list(img.to(device) for img in image)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        outputs = model(image)\n\n        outputs = [{k: v for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n        evaluator_time = time.time()\n        coco_evaluator.update(res)\n        evaluator_time = time.time() - evaluator_time\n        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n\n    # gather the stats from all processes\n    metric_logger.synchronize_between_processes()\n    print(\"Averaged stats:\", metric_logger)\n    coco_evaluator.synchronize_between_processes()\n\n    # accumulate predictions from all images\n    coco_evaluator.accumulate()\n    coco_evaluator.summarize()\n    torch.set_num_threads(n_threads)\n    return coco_evaluator\n","metadata":{"id":"K8uKl51Z506h","executionInfo":{"status":"ok","timestamp":1623565052454,"user_tz":-480,"elapsed":5,"user":{"displayName":"Yong Kang Chia","photoUrl":"","userId":"00941704455460551754"}},"execution":{"iopub.status.busy":"2021-06-13T10:33:21.946561Z","iopub.status.idle":"2021-06-13T10:33:21.947291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(params['MAXEPOCHS']):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n\n    # update the learning rate\n    lr_scheduler.step()\n\n    # evaluate on the test dataset\n    evaluate(model, val_loader, device=device)","metadata":{"id":"trjdd2wgXqfv","outputId":"116cddf6-af64-4968-9ce4-f85106e07c6c","execution":{"iopub.status.busy":"2021-06-13T10:33:21.948716Z","iopub.status.idle":"2021-06-13T10:33:21.949474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization of results\n\nNow that training has finished, let's have a look at what it actually predicts.","metadata":{"id":"ACQjk3ekP7cc"}},{"cell_type":"code","source":"# pick one image from the validation set\nimg, _ = val_dataset[391]\n\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])\n\nprediction","metadata":{"id":"YHwIdxH76uPj","execution":{"iopub.status.busy":"2021-06-13T10:33:21.950838Z","iopub.status.idle":"2021-06-13T10:33:21.951449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\nThe dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, and `scores` as fields.\n\nLet's inspect the image and the predicted boxes.\nFor that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format.","metadata":{"id":"omW9nKNYPOmr"}},{"cell_type":"code","source":"# convert the image, which has been rescaled to 0-1 and had the channels flipped\npred_img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\ndraw = ImageDraw.Draw(pred_img)\n\nimg_preds = prediction[0]\nfor i in range(len(img_preds[\"boxes\"])):\n    x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n    label = int(img_preds[\"labels\"][i])\n    score = float(img_preds[\"scores\"][i])\n\n    draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n    text = f'{dataset.cat2name[label]}: {score}'\n    draw.text((x1+5, y1+5), text)\n\ndisplay(pred_img)","metadata":{"id":"7yQom5VmPg0G","execution":{"iopub.status.busy":"2021-06-13T10:33:21.952541Z","iopub.status.idle":"2021-06-13T10:33:21.953351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post-processing\n\nWe might notice that there are duplicate detections in the image. Let's post-process the detections with non-maximum suppression.\n\n** Update: FasterRCNN already has NMS built into it, so you actually do not need to do NMS again.","metadata":{"id":"NyVWaxdXQEMI"}},{"cell_type":"code","source":"# img_preds = prediction[0]\n# keep_idx = batched_nms(boxes=img_preds[\"boxes\"], scores=img_preds[\"scores\"], idxs=img_preds[\"labels\"], iou_threshold=params['IOU_THRESHOLD'])","metadata":{"id":"qmui7x3zQOvY","execution":{"iopub.status.busy":"2021-06-13T10:33:21.954563Z","iopub.status.idle":"2021-06-13T10:33:21.955181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the predictions again after applying nms.\n\n** Update: You should not see any difference unless you have specified a lower IoU threshold than the default of 0.5.","metadata":{"id":"-kctZV8vQbLD"}},{"cell_type":"code","source":"# # convert the image, which has been rescaled to 0-1 and had the channels flipped\n# pred_img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n# draw = ImageDraw.Draw(pred_img)\n\n# for i in range(len(img_preds[\"boxes\"])):\n#     if i in keep_idx:\n#         x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n#         label = int(img_preds[\"labels\"][i])\n#         score = float(img_preds[\"scores\"][i])\n\n#         draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n#         text = f'{dataset.cat2name[label]}: {score}'\n#         draw.text((x1+5, y1+5), text)\n\n# display(pred_img)","metadata":{"id":"cEYw-SWxQclI","execution":{"iopub.status.busy":"2021-06-13T10:33:21.956522Z","iopub.status.idle":"2021-06-13T10:33:21.957226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let's further filter out the non-confident detections.","metadata":{"id":"n1vTbbN2Q1Ys"}},{"cell_type":"code","source":"# det_threshold = 0.5\n\n# # convert the image, which has been rescaled to 0-1 and had the channels flipped\n# pred_img = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n# draw = ImageDraw.Draw(pred_img)\n\n# for i in range(len(img_preds[\"boxes\"])):\n#     if i in keep_idx:\n#         x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n#         label = int(img_preds[\"labels\"][i])\n#         score = float(img_preds[\"scores\"][i])\n\n#         # filter out non-confident detections\n#         if score > det_threshold:\n#             draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n#             text = f'{dataset.cat2name[label]}: {score}'\n#             draw.text((x1+5, y1+5), text)\n\n# display(pred_img)","metadata":{"id":"HpfDgkUzRCSy","execution":{"iopub.status.busy":"2021-06-13T10:33:21.958542Z","iopub.status.idle":"2021-06-13T10:33:21.959265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Much better! Once you are satisfied with the results, save your model weights.","metadata":{"id":"Zjsr7fSkRKbB"}},{"cell_type":"code","source":"# # save model weights\n# base_folder = \"./\"\n# save_path = os.path.join(base_folder, \"c1_weights.pth\")\n# torch.save(model.state_dict(), save_path)","metadata":{"id":"lycezchIRPK7","execution":{"iopub.status.busy":"2021-06-13T10:33:21.960441Z","iopub.status.idle":"2021-06-13T10:33:21.96107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation on Validation Set\n\nAs a sanity check, let's evaluate the model performance on the validation set","metadata":{"id":"ivhZ0aMlg_nF"}},{"cell_type":"code","source":"with open(val_annotation) as json_file:\n    val_data = json.load(json_file)\n\nmodel.eval()\ndetections = []\nwith torch.no_grad():\n    for image in val_data[\"images\"]:\n        img_name = image[\"file_name\"]\n        img_id = image[\"id\"]\n\n        img = Image.open(os.path.join(til_root, 'images', img_name)).convert('RGB')\n        img_tensor = transforms.ToTensor()(img)\n\n        preds = model([img_tensor.to(device)])[0]\n\n        for i in range(len(preds[\"boxes\"])):\n            x1, y1, x2, y2 = preds[\"boxes\"][i]\n            label = int(preds[\"labels\"][i])\n            score = float(preds[\"scores\"][i])\n\n            left = int(x1)\n            top = int(y1)\n            width = int(x2 - x1)\n            height = int(y2 - y1)\n\n            detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})","metadata":{"id":"uCbo_MyRhEVK","execution":{"iopub.status.busy":"2021-06-13T10:33:21.962216Z","iopub.status.idle":"2021-06-13T10:33:21.962841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_json = os.path.join(base_folder, \"validation_preds.json\")\nwith open(validation_json, 'w') as f:\n    json.dump(detections, f)","metadata":{"id":"aysXD7DwjFSs","execution":{"iopub.status.busy":"2021-06-13T10:33:21.964032Z","iopub.status.idle":"2021-06-13T10:33:21.964671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get evaluation score against validation set to make sure your prediction json file is in the correct format\ncoco_gt = COCO(val_annotation)\ncoco_dt = coco_gt.loadRes(validation_json)\ncocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\ncocoEval.evaluate()\ncocoEval.accumulate()\ncocoEval.summarize()","metadata":{"id":"yymAERJJhcc_","execution":{"iopub.status.busy":"2021-06-13T10:33:21.965821Z","iopub.status.idle":"2021-06-13T10:33:21.966407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate Predictions on Test Images","metadata":{"id":"rUGlO1lFRQQd"}},{"cell_type":"code","source":"til_test_root = \"../input/dsta-brainhack-2021/c1_test_release/c1_test_release\" # extracted testing images path\ntest_img_root = os.path.join(til_test_root, \"images\")\nimg_dir = os.scandir(test_img_root)","metadata":{"id":"VImfhYxKuvKz","execution":{"iopub.status.busy":"2021-06-13T10:33:21.967655Z","iopub.status.idle":"2021-06-13T10:33:21.968302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load model weights (if not using the current trained model)\nmodel.load_state_dict(torch.load(save_path, map_location=device))\nmodel.to(device)\nmodel.eval()","metadata":{"id":"W7QnHBguS4qZ","execution":{"iopub.status.busy":"2021-06-13T10:33:21.96962Z","iopub.status.idle":"2021-06-13T10:33:21.970249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize some predictions on the test images. Run this a few times to visualize different images.","metadata":{"id":"X1dODyYiTXSF"}},{"cell_type":"code","source":"img = Image.open(next(img_dir).path).convert('RGB')\ndraw = ImageDraw.Draw(img)\ndet_threshold = 0.5\n\n# do the prediction\nwith torch.no_grad():\n    img_tensor = transforms.ToTensor()(img)\n    img_preds = model([img_tensor.to(device)])[0]\n\nfor i in range(len(img_preds[\"boxes\"])):\n    x1, y1, x2, y2 = img_preds[\"boxes\"][i]\n    label = int(img_preds[\"labels\"][i])\n    score = float(img_preds[\"scores\"][i])\n\n    # filter out non-confident detections\n    if score > det_threshold:\n        draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\")\n        text = f'{dataset.cat2name[label]}: {score}'\n        draw.text((x1+5, y1+5), text)\n\ndisplay(img)","metadata":{"id":"VBwlAvrCTZgU","execution":{"iopub.status.busy":"2021-06-13T10:33:21.971396Z","iopub.status.idle":"2021-06-13T10:33:21.972016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission of Results\n\nSubmission json file should be in [COCO format](https://cocodataset.org/#format-results).\n\n```\n[{\n    \"image_id\": int, \n    \"category_id\": int, \n    \"bbox\": [x,y,width,height], \n    \"score\": float,\n}]\n```\n\nRefer to **sample_submission_cv.json** for an example.\n\nFor this competition, the metric for evaluation will be mAP @ 0.50:0.95","metadata":{"id":"qeGHrPzIa9_7"}},{"cell_type":"code","source":"# generate detections on the folder of test images (this will be used for submission)\ndetections = []\nwith torch.no_grad():\n    for image in img_dir:\n        img_id = int(image.name.split('.')[0])\n\n        img = Image.open(image.path).convert('RGB')\n        img_tensor = transforms.ToTensor()(img)\n\n        preds = model([img_tensor.to(device)])[0]\n\n        for i in range(len(preds[\"boxes\"])):\n            x1, y1, x2, y2 = preds[\"boxes\"][i]\n            label = int(preds[\"labels\"][i])\n            score = float(preds[\"scores\"][i])\n\n            left = int(x1)\n            top = int(y1)\n            width = int(x2 - x1)\n            height = int(y2 - y1)\n\n            detections.append({'image_id':img_id, 'category_id':label, 'bbox':[left, top, width, height], 'score':score})","metadata":{"id":"aJqFPKWITzgo","execution":{"iopub.status.busy":"2021-06-13T10:33:21.973088Z","iopub.status.idle":"2021-06-13T10:33:21.973806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred_json = os.path.join(base_folder, \"test_preds.json\")\nwith open(test_pred_json, 'w') as f:\n    json.dump(detections, f)","metadata":{"id":"hGPpF_F7QX98","execution":{"iopub.status.busy":"2021-06-13T10:33:21.975162Z","iopub.status.idle":"2021-06-13T10:33:21.975794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check Your Submission JSON Format (RUN THIS BEFORE SUBMISSION)\n\nRun this function first with the given sample json (change this path for the different challenges) to make sure everything works before you submit. Just need to ensure that there are no errors when you run this. **If you get any errors, check your generated json file.**","metadata":{"id":"HkJTEJ6mlxgz"}},{"cell_type":"code","source":"sample_json_path = os.path.join(til_test_root, \"c1_test_sample.json\")\n\ncoco_gt = COCO(sample_json_path)\ncoco_dt = coco_gt.loadRes(test_pred_json)\ncocoEval = COCOeval(cocoGt=coco_gt, cocoDt=coco_dt, iouType='bbox')\ncocoEval.evaluate()\ncocoEval.accumulate()\ncocoEval.summarize()","metadata":{"id":"7ugbAD9RXW2E","execution":{"iopub.status.busy":"2021-06-13T10:33:21.976915Z","iopub.status.idle":"2021-06-13T10:33:21.977556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you get an `AssertionError: Results do not correspond to current coco set`, it most likely means that some of the \"image_id\" are out of range (either 0 or higher than number of test images).","metadata":{"id":"iZ1KgDOGlw8n"}},{"cell_type":"code","source":"","metadata":{"id":"Q4MLUw05l4VR"},"execution_count":null,"outputs":[]}]}