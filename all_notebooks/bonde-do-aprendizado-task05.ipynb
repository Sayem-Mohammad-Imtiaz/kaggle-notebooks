{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n    <h2> Aprendizado de Máquina </h2>\n    <hr >\n    <h1>Task 05 - Viral Tweets Prediction Challenge</h1>\n    <hr >\n    <h4>Diego J Talarico Ferreira- 3166561</h4>\n    <h4>Gabriel Castro Gulin Rosa - 11218181</h4>\n    <h4>Júlio Trevisan Centanin - 11218240</h4>\n    <h4>Matheus Victal Cerqueira - 10276661</h4>\n    <h4>Murilo Henrique Soave - 10688813</h4>\n    <hr >\n    <img src= 'https://img.olhardigital.com.br/wp-content/uploads/2021/05/shutterstock_1963706737-1000x450.jpg' width=\"700\" >\n    <hr >\n</center>","metadata":{"id":"OHXRZnL0ehTn"}},{"cell_type":"markdown","source":"# Predição de Tweets Virais","metadata":{"id":"siOY6tFCedaI"}},{"cell_type":"markdown","source":"___\n\n\n## Análise Descritiva\n\nUma análise exploratória descritiva bem feita é extremamente importante na identificação de tendências e comportamentos dos dados de interesse, além de localizar possíveis problemas como valores nulos, os quais podem ser tratados em uma etapa de pré-processamento. Assim, podemos ter uma visão holística dos dados, o que permite que o ajuste de modelos de aprendizado de máquina seja mais simples. Como aqui temos uma base de dados considerávelmente grande, devemos analisar cuidadosamente a estrutura e comportamento das observações contidas em tal conjunto, a fim de facilitar a modelagem posteriormente.","metadata":{"id":"OkezI_DuedaR"}},{"cell_type":"code","source":"# Bibliotecas básicas e para análise descritiva\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Análise de correlações e redução de dimensionalidade\nfrom sklearn.decomposition import PCA\n\n# Balanceamento de dados\nfrom imblearn.over_sampling import SMOTE\n\n# Modelagem\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score","metadata":{"id":"8-fpp7NTedaR","outputId":"b7e2bc4c-1b34-439f-ee32-44f55e1bbf4a","execution":{"iopub.status.busy":"2021-07-14T02:46:17.051833Z","iopub.execute_input":"2021-07-14T02:46:17.052415Z","iopub.status.idle":"2021-07-14T02:46:18.707634Z","shell.execute_reply.started":"2021-07-14T02:46:17.05233Z","shell.execute_reply":"2021-07-14T02:46:18.706491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Uma breve observação da estrutura dos dados","metadata":{"id":"h5o872zDedaS"}},{"cell_type":"code","source":"# Caminho base\nt_path = '../input/viral-tweets-prediction-dataset/Dataset/Tweets/'\nu_path = '../input/viral-tweets-prediction-dataset/Dataset/Users/'\n\n# Dados de treino\ntrain_tweets = pd.read_csv(t_path + 'train_tweets.csv')\ntrain_tweets_vectorized_media = pd.read_csv(t_path + 'train_tweets_vectorized_media.csv')\ntrain_tweets_vectorized_text = pd.read_csv(t_path + 'train_tweets_vectorized_text.csv')\n\n# Dados de usuário\nusers = pd.read_csv(u_path + 'users.csv')\nuser_vectorized_descriptions = pd.read_csv(u_path + 'user_vectorized_descriptions.csv')\nuser_vectorized_profile_images = pd.read_csv(u_path + 'user_vectorized_profile_images.csv')","metadata":{"id":"LYeE9Hm0edaT","execution":{"iopub.status.busy":"2021-07-14T02:47:04.631271Z","iopub.execute_input":"2021-07-14T02:47:04.631657Z","iopub.status.idle":"2021-07-14T02:47:44.67728Z","shell.execute_reply.started":"2021-07-14T02:47:04.631624Z","shell.execute_reply":"2021-07-14T02:47:44.676371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Os dados que aqui estamos estudando estão distribuídos em seis conjuntos que estão interrelacionados por dois atributos de identificação: o identificador de usuário (user_id) e o identificador de tweet (tweet_id). Tais conjuntos podem ser agrupados da seguinte forma:\n\n___ \nTweets:                                   \n\n * test_tweets_vectorized_media.csv         \n * test_tweets_vectorized_text.csv          \n * test_tweets.csv                          \n * train_tweets_vectorized_media.csv \n * train_tweets_vectorized_text.csv\n * train_tweets.csv\n \nUsers:\n\n* user_vectorized_descriptions.csv\n* user_vectorized_profile_images.csv\n* users.csv\n___\n\nVejamos como esses conjuntos de dados estão estruturados a partir de uma amostra de cada um deles (para os conjuntos que possuam um par _train-test_, iremos apenas observar o componente referente ao treino por agora).","metadata":{"id":"t-g9UVSfedaT"}},{"cell_type":"code","source":"train_tweets_vectorized_media.sample(3)","metadata":{"id":"FXKN-tOuedaU","outputId":"80f1d5a3-26ea-44b6-fefd-38005ef9e80e","execution":{"iopub.status.busy":"2021-07-14T02:47:53.967104Z","iopub.execute_input":"2021-07-14T02:47:53.96763Z","iopub.status.idle":"2021-07-14T02:47:54.013129Z","shell.execute_reply.started":"2021-07-14T02:47:53.967599Z","shell.execute_reply":"2021-07-14T02:47:54.012274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tweets_vectorized_text.sample(3)","metadata":{"id":"WdGVucppedaU","outputId":"479fbfa6-4fe1-420e-e52e-d14b9b3a6aa9","execution":{"iopub.status.busy":"2021-07-14T02:47:57.561349Z","iopub.execute_input":"2021-07-14T02:47:57.561721Z","iopub.status.idle":"2021-07-14T02:47:57.593668Z","shell.execute_reply.started":"2021-07-14T02:47:57.561689Z","shell.execute_reply":"2021-07-14T02:47:57.592511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tweets.sample(3)","metadata":{"id":"7YPNA0TcedaV","outputId":"78775cd6-f43d-4374-806e-5e7f0b070f30","execution":{"iopub.status.busy":"2021-07-14T02:48:01.742768Z","iopub.execute_input":"2021-07-14T02:48:01.743178Z","iopub.status.idle":"2021-07-14T02:48:01.770249Z","shell.execute_reply.started":"2021-07-14T02:48:01.743132Z","shell.execute_reply":"2021-07-14T02:48:01.769195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"users.sample(3)","metadata":{"id":"89ZBxZIledaV","outputId":"0ae12be2-0b0f-46cf-942a-712cb01d3e9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_vectorized_descriptions.sample(3)","metadata":{"id":"B3xbuNXAedaW","outputId":"e12422a8-027f-4ec5-86b8-3ffc9f560e91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_vectorized_profile_images.sample(3)","metadata":{"id":"vIbj4schedaX","outputId":"87954e7f-cdb8-4d7f-8caa-c9034c74c378"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Os conjuntos de dados referentes à informações vetorizadas oferecem poucas conclusões passíveis de interpretação qualitativa, assim sendo, iremos focar nossa atenção, momentanemente, aos conjuntos _users.csv_ e _train_tweets.csv_, os quais possuem features interpretáveis e que podem gerar conclusões interessnates antes de ajustarmos algum modelo propriamente dito.","metadata":{"id":"elJn5LfeedaX"}},{"cell_type":"markdown","source":"### Investigação qualitativa e quantitativa dos dados dos conjuntos de dados não vetorizados\n\nOs conjuntos _users.csv_ e _train_tweets.csv_, como já comentado, possuem atributos que permitem uma interpretação qualitativa em relação ao comportamento dos dados. Primeiramente, vejamos algumas características básicas:","metadata":{"id":"LCZJ1gi8edaX"}},{"cell_type":"code","source":"train_tweets.info()","metadata":{"id":"E1dyvPX0edaX","outputId":"6f3ff995-8df0-4c54-ddb5-248282cd15c4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tweets.shape","metadata":{"id":"6oK--HWzedaX","outputId":"2ede1b58-0e5a-4095-fdc4-b93f3d59bfc5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"É notório que a única coluna que apresenta dados faltantes é a coluna _tweet_topic_ids_. Observando-se nas amostras coletadas anteriormente deste conjunto, podemos perceber que as observações contidas nessa coluna possuem uma estrutura de lista de identificadores. Assim sendo, seu tratamento será mais complicado e deve ser investigado se sua utilização é realmente necessária. \n\nAgora, estudemos o comportamento desses _features_ a partir de alguns gráficos. Primeiramente, iremos analisar como as proporções entre as categorias de viralidade (de 1 a 5) se comportam de acordo com as covariáveis temporais dos dados.","metadata":{"id":"8vDe2tJEedaX"}},{"cell_type":"code","source":"plt.style.use('ggplot')","metadata":{"id":"XiQ2zDKDedaY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 10))\nsns.countplot(x = 'virality', data = train_tweets, palette=\"viridis\")","metadata":{"id":"2ZVRfAYgedaY","outputId":"bd2d94a6-0db9-41a3-a1ba-a24717d21dc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(18, 8))\n\nsns.histplot(x=\"tweet_created_at_year\", hue = 'virality', multiple = 'stack', discrete = True,\n             ax = axs[0,0], data=train_tweets, palette = \"viridis\")\nsns.histplot(x=\"tweet_created_at_month\", hue = 'virality', multiple = 'stack', discrete = True,\n             ax = axs[0,1], legend = False, data=train_tweets, palette = \"viridis\")\nsns.histplot(x=\"tweet_created_at_day\", hue = 'virality', multiple = 'stack', discrete = True,\n             ax = axs[1,0], legend = False, data=train_tweets, palette = \"viridis\")\nsns.histplot(x=\"tweet_created_at_hour\", hue = 'virality', multiple = 'stack', discrete = True,\n             ax = axs[1,1], legend = False, data=train_tweets, palette = \"viridis\")\nplt.show()","metadata":{"id":"UHDAhXSWedaY","outputId":"0d80642f-d2b3-459b-e1d3-4417cf189726"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observando-se os histogramas acima, os quais estão divididos pelo nível de viralidade, é possível notar que, por mais que o número de observações seja diferente para cada ano e para cada hora, as proporções entre as categorias de viralidade não parecem variar de forma sifnificativa a ponto de podermos atrelar tais atributos diretamente ao grau de viralidade. Um fenômeno semelhante ocorre para o caso das covariáveis de dia e mês, porém, nesse caso o comportamento parece se repetir para quase todo o espaço amostral, principalemente para o caso do dia do mês. Como tal variabilidade é pequena, é possível que tais covariáveis não estejam fortemente relacionadas com o grau de viralidade, pelo menos, não de uma forma trivial.\n\nAgora, vamos investigar alguns tipos de correlações entre variáveis. A ideia é tentar identificar algum padrão entre os níveis de viralidade e os atributos do conjunto de dados. Assim, investiguemos a correlação de [pearson](https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson), a correlação de [spearman](https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_postos_de_Spearman) e a correlação de [kendall](https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_tau_de_Kendall) entre a variável de viralidade e os demais fatores.","metadata":{"id":"43EW-uPredaa"}},{"cell_type":"code","source":"single1 = train_tweets.corr(method = \"pearson\")[['virality']].sort_values(by='virality', ascending=False)\nsingle2 = train_tweets.corr(method = \"spearman\")[['virality']].sort_values(by='virality', ascending=False)\nsingle3 = train_tweets.corr(method = \"kendall\")[['virality']].sort_values(by='virality', ascending=False)","metadata":{"id":"Kmb-wJDvedaa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 12))\nfig, axs = plt.subplots(1, 3, figsize=(18, 8))\n\nsns.heatmap(single1, vmin=-1, vmax=1, annot=True, ax = axs[0], cbar = False)\nsns.heatmap(single2, vmin=-1, vmax=1, annot=True, ax = axs[1], cbar = False, yticklabels=False)\nsns.heatmap(single3, vmin=-1, vmax=1, annot=True, ax = axs[2], yticklabels=False)\n\naxs[0].set_title(\"Pearson\")\naxs[1].set_title(\"Spearman\")\naxs[2].set_title(\"Kendall\")\n\nplt.show(True)","metadata":{"id":"lbwGVF-_edaa","outputId":"fd66fedb-b31c-41c2-ae3a-035a803a9175"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observando-se os mapas de correlação acima, podemos perceber que, para nenhum dos coeficientes de correlação acima, há retultados que indiquem forte relação entre a viralidade e os demais _features_. Assim sendo, se existir uma relação, ela provavelmente não é trivial e um modelo mais robusto talvez seja necessário para identificar como estas variáveis se relacionam.\n\nAnalisemos agora as colunas _tweet_has_attachment_ e _tweet_attachment_class_ em mais detalhe","metadata":{"id":"UtVfbMuKedaa"}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n\np1 = sns.histplot(x=\"tweet_has_attachment\", hue = 'virality', multiple = 'stack', discrete = True,\n                  ax = axs[0], data=train_tweets, palette = 'viridis')\np1.set(xticks = [0,1])\n\nsns.histplot(x=\"tweet_attachment_class\", hue = 'virality', multiple = 'stack', discrete = True,\n             ax = axs[1], legend = False, data=train_tweets, palette = 'viridis')\n\nplt.show(True)","metadata":{"id":"ZbKF-L8nedaa","outputId":"c5068177-668c-40ef-c96c-1455bb0addd5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como esperado, não há um padrão trivial significativo que permita visualizarmos uma relação direta entre a viralidade e tais colunas. Além disso, quando uma linha tem a coluna 'tweet_has_attachment' False, temos que a coluna 'tweet_attachment_class' é C para essa linha. Logo, essas duas colunas tem uma alta correlação. Quase não há linhas com 'tweet_attachment_class' com valor 'B'. \n\nPor fim, utilizemos de PCA para verificar se a correlação entre as features é alta e se é possível reduzir a dimensionalidade do conjunto com pouca perda de informação.","metadata":{"id":"zT7IZYmYedaa"}},{"cell_type":"code","source":"train_tweets.head(3)","metadata":{"id":"r5daVYUYedab","outputId":"797751dc-6e7e-4af0-f86a-b2805845d715"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.matrix(train_tweets.drop(['tweet_attachment_class', 'tweet_has_attachment'], axis = 1).iloc[:,0:10]) # aqui pegamos todas \n# as colunas menos a de viralidade e os tweet_topic_ids para aplicar o PCA. Além disso removem-se tweet_attachment_class e \n# tweet_has_attachment os quais não são numéricos.","metadata":{"id":"El3oIf2Uedac","outputId":"bf13d594-c3d2-4f3e-ccc3-038837fc100d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nPCA_copy = train_tweets.copy()\n\npca = PCA(n_components=2)\n\nX = np.matrix(PCA_copy.drop(['tweet_attachment_class', 'tweet_has_attachment'], axis = 1).iloc[:,0:10]) \npca.fit(X)\n\nprint(np.round(pca.explained_variance_ratio_,6))\n\nPCA1 = pca.transform(X)[:,0]\nPCA2 = pca.transform(X)[:,1]\nPCA_copy[\"Component 1\"] = PCA1\nPCA_copy[\"Component 2\"] = PCA2","metadata":{"id":"QaI8Rxg-edac","outputId":"4b717735-1469-4e1d-dd7b-b787b0d57be8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PCA_copy.head(3)","metadata":{"id":"dfseFGrzedac","outputId":"3c8ec73d-4e35-4b4c-d609-6b606e505b25"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Considerando-se as observações de acordo com a viralidade observada.","metadata":{"id":"FHau5XcAedad"}},{"cell_type":"code","source":"plt.figure(figsize=(15, 7))\n\nfig, axs = plt.subplots(1, 5, figsize=(18, 8))\nsns.scatterplot(data=PCA_copy[PCA_copy['virality']==1], x=\"Component 1\", y=\"Component 2\", palette = 'viridis', ax = axs[0])\nsns.scatterplot(data=PCA_copy[PCA_copy['virality']==2], x=\"Component 1\", y=\"Component 2\", palette = 'viridis', ax = axs[1])\nsns.scatterplot(data=PCA_copy[PCA_copy['virality']==3], x=\"Component 1\", y=\"Component 2\", palette = 'viridis', ax = axs[2])\nsns.scatterplot(data=PCA_copy[PCA_copy['virality']==4], x=\"Component 1\", y=\"Component 2\", palette = 'viridis', ax = axs[3])\nsns.scatterplot(data=PCA_copy[PCA_copy['virality']==5], x=\"Component 1\", y=\"Component 2\", palette = 'viridis', ax = axs[4])\n\naxs[0].set_title(\"Virality = 1\")\naxs[1].set_title(\"Virality = 2\")\naxs[2].set_title(\"Virality = 3\")\naxs[3].set_title(\"Virality = 4\")\naxs[4].set_title(\"Virality = 5\")","metadata":{"id":"HT9y0wcMedad","outputId":"a997b3fa-120f-4dbb-fc02-b0667745e9a9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Considerando-se todos os tipos de viralidade.","metadata":{"id":"74fCLsRredad"}},{"cell_type":"code","source":"plt.figure(figsize=(15, 7))\nsns.scatterplot(data=PCA_copy, x=\"Component 1\", y=\"Component 2\", palette = 'viridis', hue = \"virality\")","metadata":{"id":"IrigxgZ2edae","outputId":"a6f0157d-0eab-4da6-9974-21874c1797dd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aplicando-se PCA, pode-se perceber pela alta variância explicada pelas duas componentes principais mais explicativas que existe uma correlação considerável entre os features que passaram pelo processo. Porém, não é possível identificar um padrão de viralidade trivial utilizando-se apenas das duas componentes principais, como mostram on gráficos de dispersão acima.","metadata":{"id":"3R_PqRdXedae"}},{"cell_type":"markdown","source":"<br>\n<br>\n\nAgora, investiguemos o conjunto de dados _users_.","metadata":{"id":"ZnMjkFbzedae"}},{"cell_type":"code","source":"users.info()","metadata":{"id":"bR0STlBAedae","outputId":"dc9e9941-99fe-421e-d046-405662687737"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"users.shape","metadata":{"id":"NdjciLnAedae","outputId":"47888545-04ec-49e0-a403-c6c7f1455b0f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Aqui não há a ocorrência de dados faltantes, o que irá facilitar, de certa forma, o pré-processamento. Façamos alguns histogramas para as colunas relacionadas à relevância do usuário na rede (_user_like_count_, _user_followers_count_, _user_following_count_, _user_listed_on_count_).","metadata":{"id":"UBSDUP6Yedaf"}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 2, figsize=(15, 8))\n\nsns.histplot(users, x = 'user_like_count', ax = axs[0,0], color = 'darkviolet')\nsns.histplot(users, x = 'user_followers_count', ax = axs[0,1], color = 'darkviolet')\nsns.histplot(users, x = 'user_following_count', ax = axs[1,0], color = 'darkviolet')\nsns.histplot(users, x = 'user_listed_on_count', ax = axs[1,1], color = 'darkviolet')","metadata":{"id":"3kxLoNChedaf","outputId":"a893ebb1-2556-41ca-f6e5-ee848f070fe5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"É interessante perceber que tais features obedecem distribuições livres de escala, o que é comum em sistemas estruturados em rede tais como redes sociais. Isso quer dizer que muitos usuários possuem indicadores baixos de conectividade enquanto uma gama baixa de poucos usuários posssuem indicadores de conectividade altos, configurando-se como _hubs_ da rede. Tais _hubs_ estão relacionados à todo tipo de processo dinâmico que ocorre na rede e sua investigação pode levar à uma melhor compreensão da topologia do grafo em questão. É esperado que exista uma correlação considerável entre os features aqui tratados. \n\nEm análise de redes sociais, tais _hubs_ são extremamente importantes, pois podem corresponder à super-propagadores de opinião, influenciadores de tendências e coesão de comunidades. Porém, é preciso mais do que conhecer os hubs de uma rede para prever a viralidade de um tweet, já que o sistema envolvido é extremamente complexo e os processos dinâmicos nele envolvidos são não triviais.\n\nMuitos estudos estão voltados para a análise de redes sociais por meio de estruturas de redes complexas. Entre eles, temos o [estudo](https://osome.iu.edu/demos/echo/) realizado pelo observatório em mídias sociais [OSoMe](https://osome.iu.edu/tools) da Universidade de Indiana, onde é feita uma simulação de câmaras de eco, as quais são extremamente interessantes na área de comunicação e sociologia, sendo muitas vezes formadores de opinião pública e responsáveis pela emergência de comportamentos coletivos diversos. ","metadata":{"id":"18hhyA6Xedaf"}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nPCA_copy = users.copy()\n\npca = PCA(n_components=2)\n\nX = np.matrix(PCA_copy.iloc[:,1:11]) # análise das colunas, menos a coluna de identificação\npca.fit(X)\n\nprint(np.round(pca.explained_variance_ratio_,6))","metadata":{"id":"PQFsPpUAedag","outputId":"0674ab90-23f4-449e-fd14-e7af9dff58ca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nsns.heatmap(users.corr() , annot = True)\nplt.show()","metadata":{"id":"Kb6mKmfuedag","outputId":"7b74bffc-3d78-40c7-8b90-f0e61148f71d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como esperado, existe correlação moderada entre as colunas do conjunto de dados em questão, o que leva aos valores de variâcia explicada pelas duas componentes principais mais explicativas do PCA.","metadata":{"id":"7SzkugdBedah"}},{"cell_type":"markdown","source":"### Investigação quantitativa dos dados dos conjuntos de dados vetorizados\n\nComo já discutido, uma análise qualitativa dos conjuntos de dados vetorizados não é algo que iremos fazer devido ao  significado não muito intuitivo do que as colunas representam. Assim sendo, iremos apenas investigar a correlação entre os _features_ de cada conjunto.","metadata":{"id":"X_xLo0B3edah"}},{"cell_type":"markdown","source":"### _train_tweets_vectorized_media_","metadata":{"id":"OYdKBAlUedah"}},{"cell_type":"code","source":"train_tweets_vectorized_media.info()","metadata":{"id":"TmQZBkUkedah","outputId":"d0afd013-b895-42b1-b60e-592c9404e001"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tweets_vectorized_media.shape","metadata":{"id":"OwAAztD7edah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nPCA_copy = train_tweets_vectorized_media.copy()\n\npca = PCA(n_components=2)\n\nX = np.matrix(PCA_copy.iloc[:,0:2050]) # análise das colunas, menos a coluna de identificação\npca.fit(X)\n\nprint(np.round(pca.explained_variance_ratio_,6))","metadata":{"id":"3U-Gvevnedai"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(train_tweets_vectorized_media.corr() , annot = False)\nplt.title(\"Mapa de calor de correlação de Pearson (train_tweets_vectorized_media)\")\nplt.show()","metadata":{"id":"XwmrF9HUedai"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### _train_tweets_vectorized_text_","metadata":{"id":"33_FucJBedai"}},{"cell_type":"code","source":"train_tweets_vectorized_text.info()","metadata":{"id":"tAhtMc3-edaj","outputId":"b6b16c28-f4ee-4901-8ef6-b3def511a395"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tweets_vectorized_text.shape","metadata":{"id":"TieCwTgSedaj","outputId":"43389a0b-3fa7-42fc-a3c7-11fcb3326026"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nPCA_copy = train_tweets_vectorized_text.copy()\n\npca = PCA(n_components=2)\n\nX = np.matrix(PCA_copy.iloc[:,0:769]) # análise das colunas, menos a coluna de identificação\npca.fit(X)\n\nprint(np.round(pca.explained_variance_ratio_,6))","metadata":{"id":"C-OkrRROedaj","outputId":"ad26721b-1ce2-4f14-b332-a227250f0a1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(train_tweets_vectorized_text.corr() , annot = False)\nplt.title(\"Mapa de calor de correlação de Pearson (train_tweets_vectorized_text)\")\nplt.show()","metadata":{"id":"9ORblS4Zedaj","outputId":"3f7ac55e-d1c0-45b6-b839-4ccf9d0ce9bb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### _user_vectorized_descriptions_","metadata":{"id":"ftMO12sBedak"}},{"cell_type":"code","source":"user_vectorized_descriptions.info()","metadata":{"id":"kzBfHWW2edak","outputId":"d425aba9-a977-4bff-d236-948ec8c3e223"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_vectorized_descriptions.shape","metadata":{"id":"AppycolSedak","outputId":"4f347da2-62b6-4a21-dbaa-db6ee54b4cf7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nPCA_copy = user_vectorized_descriptions.copy()\n\npca = PCA(n_components=2)\n\nX = np.matrix(PCA_copy.iloc[:,0:769]) # análise das colunas, menos a coluna de identificação\npca.fit(X)\n\nprint(np.round(pca.explained_variance_ratio_,6))","metadata":{"id":"KwexZ4QKedak","outputId":"7285dbeb-0784-4c4f-c4f5-e081c014a4c7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(user_vectorized_descriptions.corr() , annot = False)\nplt.title(\"Mapa de calor de correlação de Pearson (user_vectorized_descriptions)\")\nplt.show()","metadata":{"id":"etss5aEEedal","outputId":"5b30c348-e7e3-4283-e3ae-e8a8008e3d5e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### _user_vectorized_profile_images_","metadata":{"id":"g8AUUqGzedal"}},{"cell_type":"code","source":"user_vectorized_profile_images.info()","metadata":{"id":"iiOeILJgedal","outputId":"f9c3b4b6-e7fc-464e-b3d6-69282addcf2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_vectorized_profile_images.shape","metadata":{"id":"HHUqntG8edal","outputId":"d3afae13-8580-41c7-f1ac-724c98cad8a7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\nPCA_copy = user_vectorized_profile_images.copy()\n\npca = PCA(n_components=2)\n\nX = np.matrix(PCA_copy.iloc[:,0:2049]) # análise das colunas, menos a coluna de identificação\npca.fit(X)\n\nprint(np.round(pca.explained_variance_ratio_,6))","metadata":{"id":"Vb2PP3kAedam","outputId":"98b7f095-caf1-4eac-ea46-3b12cac03eae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(user_vectorized_profile_images.corr(), annot = False)\nplt.title(\"Mapa de calor de correlação de Pearson (user_vectorized_profile_images)\")\nplt.show()","metadata":{"id":"rLJpjLKcedam","outputId":"36085e1f-9132-4365-b9b1-00b7c42eb335"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As duas componentes principais mais explicativas para os quatro casos explicam uma enorme parte da variabilidade (mais de 85% para todos os casos, com três deles acima de 99%) de cada conjunto, o que indica que há correlações consideráveis entre os features. Tal conclusão é reafirmada pelos mapas de calor de correlação de Pearson.","metadata":{"id":"YnUi26eVedam"}},{"cell_type":"markdown","source":"### A feature \"influência\"","metadata":{"id":"709aAdIYedam"}},{"cell_type":"code","source":"users[\"influencia\"] = users[\"user_followers_count\"]/users[\"user_following_count\"]\nusers[\"log10_influencia\"] = np.log10(users[\"user_followers_count\"]/users[\"user_following_count\"])","metadata":{"id":"EPS6xZ2kedam","outputId":"5dd0cbff-800f-423b-e2ad-c0fa18a586e4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"users.replace([np.inf, -np.inf], np.nan, inplace=True)\nusers.dropna()","metadata":{"id":"GOXLvF5Cedan"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 8))\nsns.histplot(users, x = 'log10_influencia', color = \"darkviolet\")","metadata":{"id":"Nw2dz6Mxedan"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Primeira solução","metadata":{"id":"AvaGCfY3mhDn"}},{"cell_type":"markdown","source":"A ideia é utilizar todas as informações disponíveis. Vamos relizar um PCA e juntar todos os dataframes em um só.","metadata":{"id":"kR448NyAvWcx"}},{"cell_type":"markdown","source":"### Imports","metadata":{"id":"FQBLVS4uvTo4"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport imblearn\nimport warnings\nimport time\nwarnings.filterwarnings('ignore')","metadata":{"id":"m4VVLYbPmpiL","outputId":"647cc5bf-8997-4efe-ae21-07c0498bccd0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lendo os dados\n\nVamos tratar cada banco de dados individualmente e depois juntá-los","metadata":{"id":"TpO8ezUBnIDz"}},{"cell_type":"code","source":"train_tweets = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Tweets/train_tweets.csv')\ntrain_tweets_vectorized_media = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Tweets/train_tweets_vectorized_media.csv')\ntrain_tweets_vectorized_text = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Tweets/train_tweets_vectorized_text.csv')\nusers = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Users/users.csv')\nuser_vectorized_profile_images = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Users/user_vectorized_profile_images.csv')\nuser_vectorized_descriptions = pd.read_csv('../input/viral-tweets-prediction-dataset/Dataset/Users/user_vectorized_descriptions.csv')","metadata":{"id":"lnhjx_Vem8MY","execution":{"iopub.status.busy":"2021-07-14T02:50:59.019977Z","iopub.execute_input":"2021-07-14T02:50:59.020424Z","iopub.status.idle":"2021-07-14T02:51:17.241941Z","shell.execute_reply.started":"2021-07-14T02:50:59.020387Z","shell.execute_reply":"2021-07-14T02:51:17.240655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pré-processamento de dados\n\nVamos primeiro utilizar o PCA para eliminar qualquer correlação entre as colunas dos dados que estão vetorizados. Depois, vamos juntar todos os dataframes em um só, para montar o modelo.","metadata":{"id":"CHK6PeN7nfBT"}},{"cell_type":"markdown","source":"**Arquivo tweets_vectorized_media.csv**","metadata":{"id":"2miBmHt5nl_C"}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfeatures = [coluna for coluna in train_tweets_vectorized_media.columns if coluna not in ['media_id', 'tweet_id']]\n\n#Selecionando as colunas\nX_vectorized_media = train_tweets_vectorized_media[features].to_numpy()\n\n#Padronizando para o PCA\nX_vectorized_media = StandardScaler().fit_transform(X_vectorized_media)","metadata":{"id":"GNA3_ZWvnoJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n#Fazendo o PCA\npca = PCA(0.95)\ncomponents = pca.fit_transform(X_vectorized_media)","metadata":{"id":"J-DSYZlWnqhg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Excluindo as features antigas, para adicionar as componentes principais\ntrain_tweets_vectorized_media = train_tweets_vectorized_media.drop(features, axis = 1)","metadata":{"id":"NlsaA1QVnsvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adicionando as componentes principais ao dataframe 'train_tweets_vectorized_media'\ncolumns_components = ['img_pca_%i' % i for i in range(len(pca.explained_variance_ratio_))]\ndf_components = pd.DataFrame(data = components, columns = columns_components)","metadata":{"id":"NV92ef5MnucV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Juntando os dataframes train_tweets_vectorized_media com os valores das componentes do pca\ntrain_tweets_vectorized_media = pd.merge(train_tweets_vectorized_media, df_components, left_index = True, right_index = True)","metadata":{"id":"Cx0Euw7onwX9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tweets_vectorized_media.head(2)","metadata":{"id":"34kG9e4onx-P","outputId":"909acd85-13f2-42aa-dc91-db0d31b63c76"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Há tweets, com mais de uma mídia, vamos arrumar isso.","metadata":{"id":"Bp2d-5cDn5tS"}},{"cell_type":"code","source":"#Numero de linhas com tweet_id unico\nprint('Tweets com uma midia:', train_tweets_vectorized_media['tweet_id'].nunique())\nprint('Tweets com mais de uma midia:', train_tweets_vectorized_media['tweet_id'].duplicated().sum())","metadata":{"id":"Qkso1rdrn6aH","outputId":"da87abae-65c9-4cc9-a8a8-a1e6c6db0aa0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fix_duplicates(df):\n    ''' Funcao que substitui tweets com mais de uma imagem e pega a media delas, de cada feature '''\n    df_new = df.groupby('tweet_id').mean()\n    return df_new\n\nmedias_without_duplicates = fix_duplicates(train_tweets_vectorized_media)","metadata":{"id":"IPYl5lv4n78N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Tweets com mais de uma midia:', medias_without_duplicates.index.duplicated().sum())","metadata":{"id":"ju3i-Alqn-rg","outputId":"9fce63a5-aa37-4257-9a30-826dd9ebe0b2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fix_missing_rows(df,df_missing):\n    ''' Funcao que adiciona os tweets sem midia, com todas as features = 0, no dataset das midias '''\n    missing = []\n    for i in df['tweet_id']:\n        if i not in df_missing.index:\n            missing.append(i)\n    \n    #Dataframe com os tweets_id sem midias, com NaNs\n    df_null = pd.DataFrame(index=missing)\n    \n    df_missing = pd.concat([df_missing, df_null])\n    df_missing = df_missing.fillna(0)\n    \n    return df_missing\n\nmedias_without_duplicates = fix_missing_rows(train_tweets, medias_without_duplicates)","metadata":{"id":"S5H9b6egoAei"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Arquivo tweets_vectorized_text.csv**","metadata":{"id":"O5ZjWX0ooFMp"}},{"cell_type":"code","source":"features = [coluna for coluna in train_tweets_vectorized_text.columns if coluna not in ['tweet_id']]\n\n#Selecionando as colunas\nX_vectorized_text = train_tweets_vectorized_text[features].to_numpy()\n\n#Padronizando para o PCA\nX_vectorized_text = StandardScaler().fit_transform(X_vectorized_text)","metadata":{"id":"GhmvpPLdoCkR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fazendo o PCA\npca = PCA(0.95)\ncomponents = pca.fit_transform(X_vectorized_text)","metadata":{"id":"W8TFo7s9oJO5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Excluindo as features antigas, para adicionar as componentes principais\ntrain_tweets_vectorized_text = train_tweets_vectorized_text.drop(features, axis = 1)","metadata":{"id":"qCtibz-doM7n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adicionando as componentes principais ao dataframe 'train_tweets_vectorized_text'\ncolumns_components = ['txt_pca_%i' % i for i in range(len(pca.explained_variance_ratio_))]\ndf_components = pd.DataFrame(data = components, columns = columns_components)","metadata":{"id":"pA4Qj2s4oOfk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Juntando os dataframes train_tweets_vectorized_text com os valores das componentes do pca\ntrain_tweets_vectorized_text = pd.merge(train_tweets_vectorized_text, df_components, left_index = True, right_index = True)","metadata":{"id":"LhKHH7rjoRP5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tweets_vectorized_text.head(2)","metadata":{"id":"w22u-UHCoS38","outputId":"2d10d117-cc88-4b61-e2c0-4ba1bca2a888"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Arquivo user_vectorized_profile_images.csv**","metadata":{"id":"_4qFSvwHoUlQ"}},{"cell_type":"code","source":"features = [coluna for coluna in user_vectorized_profile_images.columns if coluna not in ['user_id']]\n\n#Selecionando as colunas\nX_vectorized_user_img = user_vectorized_profile_images[features].to_numpy()\n\n#Padronizando para o PCA\nX_vectorized_user_img = StandardScaler().fit_transform(X_vectorized_user_img)","metadata":{"id":"ekUUgDKVoXtG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fazendo o PCA\npca = PCA(0.95)\ncomponents = pca.fit_transform(X_vectorized_user_img)","metadata":{"id":"im3SU6y5oZ0s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Excluindo as features antigas, para adicionar as componentes principais\nuser_vectorized_profile_images = user_vectorized_profile_images.drop(features, axis = 1)","metadata":{"id":"8XoPsM3iobw3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adicionando as componentes principais ao dataframe 'user_vectorized_profile_images'\ncolumns_components = ['user_img_pca_%i' % i for i in range(len(pca.explained_variance_ratio_))]\ndf_components = pd.DataFrame(data = components, columns = columns_components)","metadata":{"id":"c6njxficod-R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Juntando os dataframes user_vectorized_profile_images com os valores das componentes do pca\nuser_vectorized_profile_images = pd.merge(user_vectorized_profile_images, df_components, left_index = True, right_index = True)","metadata":{"id":"EodKa-g6ofrt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Arquivo user_vectorized_descriptions.csv**","metadata":{"id":"xMwDieVPohfn"}},{"cell_type":"code","source":"features = [coluna for coluna in user_vectorized_descriptions.columns if coluna not in ['user_id']]\n\n#Selecionando as colunas\nX_vectorized_user_desc = user_vectorized_descriptions[features].to_numpy()\n\n#Padronizando para o PCA\nX_vectorized_user_desc = StandardScaler().fit_transform(X_vectorized_user_desc)","metadata":{"id":"am0a804toj9e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Fazendo o PCA\npca = PCA(0.95)\ncomponents = pca.fit_transform(X_vectorized_user_desc)","metadata":{"id":"ux6BdUhLolLj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Excluindo as features antigas, para adicionar as componentes principais\nuser_vectorized_descriptions = user_vectorized_descriptions.drop(features, axis = 1)","metadata":{"id":"RGfmFA40omV6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Adicionando as componentes principais ao dataframe 'user_vectorized_descriptions'\ncolumns_components = ['user_desc_pca_%i' % i for i in range(len(pca.explained_variance_ratio_))]\ndf_components = pd.DataFrame(data = components, columns = columns_components)","metadata":{"id":"3jU7DKzyooVO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Juntando os dataframes user_vectorized_descriptions com os valores das componentes do pca\nuser_vectorized_descriptions = pd.merge(user_vectorized_descriptions, df_components, left_index = True, right_index = True)","metadata":{"id":"hXeWr0jFopsQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Juntando todos os dataframes","metadata":{"id":"WiHOjG4HouzY"}},{"cell_type":"code","source":"#train_tweets com train_tweets_vectorized_media\ntrain_tweets_medias = pd.merge(train_tweets, medias_without_duplicates,\n                               left_on = \"tweet_id\", right_index = True)","metadata":{"id":"28-WC4IZowqB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_tweets_medias com train_tweets_vectorized_text\ntrain_tweets_medias_text = pd.merge(train_tweets_medias, train_tweets_vectorized_text,\n                                    left_on='tweet_id', right_on='tweet_id', how = 'left')","metadata":{"id":"U5uPhjvFoyFL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_tweets_medias_text com users\ntrain_tmt_user = pd.merge(train_tweets_medias_text, users,\n                          left_on='tweet_user_id', right_on='user_id')\ntrain_tmt_user = train_tmt_user.drop(['user_id'], axis = 1)","metadata":{"id":"qoFKRwpMo0Gd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_tmt_user com user_vectorized_profile_images\ntrain_tmt_user_images = pd.merge(train_tmt_user, user_vectorized_profile_images,\n                                 left_on='tweet_user_id', right_on='user_id')\ntrain_tmt_user_images = train_tmt_user_images.drop(['user_id'], axis = 1)","metadata":{"id":"hVakjcpxo11x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Merge com o user_vectorized_descriptions.csv\ntrain_full = pd.merge(train_tmt_user_images, user_vectorized_descriptions,\n                                 left_on='tweet_user_id', right_on='user_id')\ntrain_full = train_full.drop(['user_id'], axis = 1)","metadata":{"id":"t9HeJa-Lo4xE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_full.head(2)","metadata":{"id":"fRef6KkRo5Ww","outputId":"69e68ca8-c572-4a49-b666-4b7d49be294e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_full.info()","metadata":{"id":"x-zKRjGBo7hL","outputId":"06138881-7ca6-45d0-8574-0fc355c952b4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Precisamos ainda, mudar algumas colunas do _train_tweets_. As colunas _tweet_hashtag_count_, _tweet_url_count_ e _tweet_mention_count_ são do tipo inteiro. Além disso, a coluna _tweet_has_attachment_ é do tipo booleana,  _tweet_attachment_class_ é uma variável dummy e a coluna _tweet_topic_ids_ é uma coluna de listas, com alguns valores nulos. Não vamos usar essa última coluna para treinar o modelo.  ","metadata":{"id":"mpsXJ5K-o-Pl"}},{"cell_type":"code","source":"def fix_tweets(df):\n    ''' Arruma as colunas tweet_hashtag_count, tweet_url_count, tweet_mention_count\n        e tweet_attachment_class '''\n    \n    #Transformando o tipo das colunas\n    cols = ['tweet_hashtag_count', 'tweet_url_count', 'tweet_mention_count']\n    df[cols] = df[cols].applymap(np.int64)\n\n    #Transformando a coluna 'tweet_attachment_class' em dummy\n    dummy = pd.get_dummies(df['tweet_attachment_class'])\n    df = pd.concat([df, dummy], axis = 1)\n    df.drop('tweet_attachment_class', inplace=True, axis = 1)\n    \n    return df\n\ntrain_full = fix_tweets(train_full)","metadata":{"id":"b1jLNY9PpAZr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ajustando o modelo","metadata":{"id":"QXWhYTqkpDzT"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split,cross_val_score\n\n#Selecionando as colunas\ny = train_full.loc[:,'virality'].to_numpy()\n\n#Variaveis que vamos usar para 'X'\ncolunas = [coluna for coluna in train_full.columns if coluna not in ['tweet_id', 'tweet_user_id', 'tweet_created_at_year',\n                                                                     'tweet_created_at_month', 'tweet_created_at_day',\n                                                                     'tweet_topic_ids', 'virality']]\nX = train_full[colunas].to_numpy()\n\n#Para calcularmos o score\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size= 0.8, random_state=7)","metadata":{"id":"zN-YIyZtpCJp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\n\nrf_pipeline = Pipeline(steps = [('scale',StandardScaler()),('RF',RandomForestClassifier(random_state=7))])\n\n#Ajustando o modelo\nrf_pipeline.fit(X_train, y_train)\ny_pred_rf = rf_pipeline.predict(X_test)\nprint('Acuracia Random Forest:',accuracy_score(y_pred_rf, y_test))","metadata":{"id":"V2DcXInppH8d","outputId":"3b410469-52dc-408f-fc5f-d12b896463f9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgboost_pipeline = Pipeline(steps = [('scale',StandardScaler()), ('XGB',XGBClassifier(random_state=7, eval_metric='mlogloss'))])\n\n#Ajustando o modelo\nxgboost_pipeline.fit(X_train, y_train)\ny_pred_xgb = xgboost_pipeline.predict(X_test)\nprint('Acuracia XGBoost:',accuracy_score(y_pred_xgb, y_test))","metadata":{"id":"S2M_FSSFpKtY","outputId":"4e2e4f87-4c2d-48fe-8edb-a1d26efb31fd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como o modelo de Random Forest obteve, no geral, uma acurácia maior, vamos fazer uma feature importance deste modelo.","metadata":{"id":"azTKXpbVpP0y"}},{"cell_type":"code","source":"#Fazendo o feature importance\nfeature_importance=pd.DataFrame({'features':train_full[colunas].columns,\n                                 'feature_importance':rf_pipeline.steps[1][1].feature_importances_})\nfeature_importance.sort_values('feature_importance',ascending=False)","metadata":{"id":"_vKKIUuRpRJ-","outputId":"4784f529-e6db-416d-c8bb-c9d44f16671c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"O modelo possui 1558 features. Isso significa que, em média, cada feature contribui com 0.06% de importância. Há 277 features com importância acima da média. Essas features, sozinhas, possuem 66,64% da importância das features para o modelo.","metadata":{"id":"bWNL0iAzpWdB"}},{"cell_type":"code","source":"#Quao importante são as features acima da media\nprint(sum(feature_importance[feature_importance['feature_importance']>0.0006]['feature_importance']))\n\n#Numero de features acima da media\nfeature_importance[feature_importance['feature_importance']>0.0006]","metadata":{"id":"Mp05VogXpS6J","outputId":"0ebc1f8c-828a-41e8-e9e3-ee4d4060395b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Segunda solução\n","metadata":{"id":"CwuXrv_VezK2"}},{"cell_type":"markdown","source":"### Imports","metadata":{"id":"jgNjmIW3fYNq"}},{"cell_type":"code","source":"#Imports.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom pandas_profiling import ProfileReport\nfrom itertools import combinations_with_replacement\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn import metrics\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import NuSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.decomposition import PCA\nimport warnings\nfrom scipy.stats.stats import pearsonr  ","metadata":{"ExecuteTime":{"end_time":"2021-07-08T22:30:24.278249Z","start_time":"2021-07-08T22:30:22.215774Z"},"id":"dQglNi_afYNs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### Carregando os dados","metadata":{"id":"VGefpqU9fYNt"}},{"cell_type":"code","source":"#Tabelas de Treino.\n\n#Tabela 1 Tweets.\ndir_train_tweets = \"../input/viral-tweets-prediction-dataset/Dataset/Tweets/train_tweets.csv\"\ndata_train_tweets = pd.read_csv(dir_train_tweets,header = (0))\ndata_train_tweets.head(5)","metadata":{"ExecuteTime":{"end_time":"2021-07-08T22:40:18.092453Z","start_time":"2021-07-08T22:40:18.017993Z"},"id":"kt8jYkVafYNt","outputId":"d49dafee-9b2f-4bcc-a839-f4e2df8ee7a5","execution":{"iopub.status.busy":"2021-07-14T02:53:18.931964Z","iopub.execute_input":"2021-07-14T02:53:18.932369Z","iopub.status.idle":"2021-07-14T02:53:19.007891Z","shell.execute_reply.started":"2021-07-14T02:53:18.932338Z","shell.execute_reply":"2021-07-14T02:53:19.006659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tabela 2 Users.\ndir_users = \"../input/viral-tweets-prediction-dataset/Dataset/Users/users.csv\"\ndata_users = pd.read_csv(dir_users,header = (0))\ndata_users.head(5)","metadata":{"ExecuteTime":{"end_time":"2021-07-08T22:40:18.76345Z","start_time":"2021-07-08T22:40:18.747469Z"},"id":"T3pTHiV8fYNw","outputId":"c1678b6a-dfb9-4ddd-9795-d9ed580af798","execution":{"iopub.status.busy":"2021-07-14T02:53:21.703147Z","iopub.execute_input":"2021-07-14T02:53:21.70358Z","iopub.status.idle":"2021-07-14T02:53:21.724046Z","shell.execute_reply.started":"2021-07-14T02:53:21.703539Z","shell.execute_reply":"2021-07-14T02:53:21.722973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tabela 3 Tweets Medias.\ndir_tweets_medias = \"../input/viral-tweets-prediction-dataset/Dataset/Tweets/train_tweets_vectorized_media.csv\"\ndata_tweets_medias = pd.read_csv(dir_tweets_medias,header = (0))\ndata_tweets_medias.head(5)","metadata":{"ExecuteTime":{"end_time":"2021-07-08T22:40:28.976817Z","start_time":"2021-07-08T22:40:19.348691Z"},"id":"Kqnt_LHPfYNx","outputId":"884dae83-6c8a-4033-f6fa-4817143fe211","execution":{"iopub.status.busy":"2021-07-14T02:53:23.733319Z","iopub.execute_input":"2021-07-14T02:53:23.733713Z","iopub.status.idle":"2021-07-14T02:53:35.83754Z","shell.execute_reply.started":"2021-07-14T02:53:23.733679Z","shell.execute_reply":"2021-07-14T02:53:35.836492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center>\n    <hr >\n    <h2> Tratamento das colunas </h2>\n    <hr >\n    <img src= 'https://miro.medium.com/max/2000/1*ke5xITbKv-QmjwYTjlpCgg.png' width=\"1000\" >\n    <hr >\n</center>","metadata":{"id":"XAaScmpffYNy"}},{"cell_type":"markdown","source":"Neste caso vamos usar apenas 3 das 6 tabelas, sendo elas:\n\n    - Tweets\n    - Users\n    - tweet_vectorized_media\nNo entanto, vamos filtar algumas colunas de cada tabela e unir as tabelas gerando apenas 1 tabela para treinamento.","metadata":{"id":"TrLkPqemfYNy"}},{"cell_type":"code","source":"#Tabela Tweets, vamos remover as colunas: \ncolumns_to_drop_tweets = [ \"tweet_topic_ids\",\n                           \"tweet_created_at_year\",\n                           \"tweet_created_at_month\",\n                           \"tweet_created_at_day\",\n                           \"tweet_attachment_class\",\n                           \"tweet_language_id\",\n                           \"tweet_created_at_hour\"]\n#Remoção das colunas.\ndata_train_tweets = data_train_tweets.drop(columns = columns_to_drop_tweets)\n#Renomeando a coluna tweet_user_id\ndata_train_tweets['user_id'] = data_train_tweets['tweet_user_id']\ndata_train_tweets = data_train_tweets.drop(columns = ['tweet_user_id'])\n#Apresentação da tabela.\ndata_train_tweets.head(5)","metadata":{"ExecuteTime":{"end_time":"2021-07-08T22:40:29.183473Z","start_time":"2021-07-08T22:40:29.160885Z"},"id":"0ne3Aax0fYNz","outputId":"59f3c3c2-8cb0-4a13-d75f-b0b29daabe4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tabela User, vamos remover as colunas:\ncolumns_to_drop_tweets = [ \"user_created_at_year\",\n                           \"user_created_at_month\"]\n#Remoção das colunas.\ndata_users = data_users.drop(columns = columns_to_drop_tweets)\n#Apresentação da tabela.\ndata_users.head(5)","metadata":{"ExecuteTime":{"end_time":"2021-07-08T22:40:29.331826Z","start_time":"2021-07-08T22:40:29.319088Z"},"id":"xMRX8y7rfYN0","outputId":"86daaae5-78f1-4337-fe36-bbb81d220276"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tabela tweet_vectorized_media, vamos manter as colunas:\ncoluns_to_keep_tweet_vectorized_media = [\"tweet_id\",\n                                         \"media_id\"]\n#Selecionando apenas 2 colunas.\ndata_tweets_medias = data_tweets_medias[data_tweets_medias.columns[data_tweets_medias.columns.isin(coluns_to_keep_tweet_vectorized_media)]]\n#Apresentação da tabela.\ndata_tweets_medias.head(5)","metadata":{"ExecuteTime":{"end_time":"2021-07-08T22:40:29.493074Z","start_time":"2021-07-08T22:40:29.473913Z"},"id":"iWMfk-eMfYN1","outputId":"1de3368d-16ad-4ac6-fb5e-0956cc95cabe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Agora vamos criar a feature na Tabela Users:\n\n    - user_followers/following_count = user_followers_count/user_following_count","metadata":{"id":"zLIEa2U6fYN2"}},{"cell_type":"code","source":"#Criação da feature.\ndata_users[\"user_followers/following_count\"] = data_users[\"user_followers_count\"]/data_users[\"user_following_count\"]\n#Apresentação da tabela.\ndata_users.head(5)","metadata":{"ExecuteTime":{"end_time":"2021-07-08T22:40:29.595701Z","start_time":"2021-07-08T22:40:29.577248Z"},"id":"zXo07_0gfYN2","outputId":"56825ff4-ea9e-4dc2-ff7d-0fd87d42aa0f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Criando a contagem de imagens","metadata":{"ExecuteTime":{"end_time":"2021-07-06T00:10:27.856587Z","start_time":"2021-07-06T00:10:27.845537Z"},"id":"8La1N-wUfYN3"}},{"cell_type":"code","source":"#Lista de contagem de imagens.\nn_imagens = data_tweets_medias[\"tweet_id\"].value_counts()\ntweet_id = list(n_imagens.to_frame().index)\ncount_images = list(n_imagens.to_frame().tweet_id)\n#Adicionando ao data frame.\ndata_tweets_medias = pd.DataFrame({'tweet_id':tweet_id,'count_images':count_images})\n#Apresentação da tabela.\ndata_tweets_medias.head(5)","metadata":{"ExecuteTime":{"end_time":"2021-07-08T22:40:35.824777Z","start_time":"2021-07-08T22:40:35.801247Z"},"id":"I_JV0VWbfYN3","outputId":"a0138ca0-15b8-4504-edbf-d13bead27d74"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Unindo as 3 tabelas","metadata":{"id":"aztGsTcBfYN3"}},{"cell_type":"code","source":"#Unindo Tweets e Users.\ndata = pd.merge(data_train_tweets,data_users, on ='user_id', how = 'right')\n#Unindo a anterior e Medias.\ndata = pd.merge(data,data_tweets_medias, on ='tweet_id', how = 'right')\n#Apresentação da tabela.\ndata.head(5)","metadata":{"ExecuteTime":{"end_time":"2021-07-08T22:41:33.900824Z","start_time":"2021-07-08T22:41:33.870572Z"},"id":"xqng9k8vfYN4","outputId":"417c9bb5-3fbd-4fb1-92e4-a7b2c1310cc0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.replace(-np.inf, np.nan)\ndata = data.replace(np.inf, np.nan)\n\ndata = data.dropna()\n\ndata.head(5)","metadata":{"ExecuteTime":{"end_time":"2021-07-08T16:23:37.348471Z","start_time":"2021-07-08T16:23:37.315931Z"},"id":"OO0gACbQfYN4","outputId":"56bbf3e6-31b7-45ce-eabe-7558bc577f9e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Treinamento","metadata":{"id":"Xn656zb4fYN5"}},{"cell_type":"code","source":"X = data.drop(columns = \"virality\")\ny = data[\"virality\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","metadata":{"ExecuteTime":{"end_time":"2021-07-08T16:23:38.766187Z","start_time":"2021-07-08T16:23:38.746519Z"},"id":"JeLdPP-RfYN5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nrfc = RandomForestClassifier()\n\nrfc.fit(X_train, y_train)\ny_pred= rfc.predict(X_test)\n\nscore_cross = (cross_val_score(rfc, X_train, y_train, cv=10))\nprint(round(np.mean(score_cross),2),\"%\")\n\nprint(\"Accuracy on Traing set: \",rfc.score(X_train,y_train))\nprint(\"Accuracy on Testing set: \",rfc.score(X_test,y_test))","metadata":{"ExecuteTime":{"end_time":"2021-07-08T16:23:52.441729Z","start_time":"2021-07-08T16:23:41.340625Z"},"id":"nTjbtWxHfYN5","outputId":"7445cdcf-d3dc-4556-ee86-76b9c1276a94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nknn = KNeighborsClassifier()\n\nknn.fit(X_train, y_train)\ny_pred= knn.predict(X_test)\n\nscore_cross = (cross_val_score(knn, X_train, y_train, cv=10))\nprint(round(np.mean(score_cross),2),\"%\")\n\nprint(\"Accuracy on Traing set: \",knn.score(X_train,y_train))\nprint(\"Accuracy on Testing set: \",knn.score(X_test,y_test))","metadata":{"ExecuteTime":{"end_time":"2021-07-08T16:23:40.683407Z","start_time":"2021-07-08T16:23:39.323578Z"},"id":"11xyoSGnfYN6","outputId":"3fc1c055-860a-419b-a261-61c448432c87"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nwarnings.filterwarnings('ignore')\n\nxgb = XGBClassifier(eval_metric = 'mlogloss')\n\nxgb.fit(X_train, y_train)\ny_pred= xgb.predict(X_test)\n\nscore_cross = (cross_val_score(xgb, X_train, y_train, cv=10))\nprint(round(np.mean(score_cross),2),\"%\")\n\nprint(\"Accuracy on Traing set: \",xgb.score(X_train,y_train))\nprint(\"Accuracy on Testing set: \",xgb.score(X_test,y_test))","metadata":{"ExecuteTime":{"end_time":"2021-07-08T16:24:21.878763Z","start_time":"2021-07-08T16:23:52.568394Z"},"id":"TCpw7s3-fYN6","outputId":"2bd55081-a4f0-4924-82af-47d13d961f53"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Utilizando o PCA\n","metadata":{"id":"8QwRiJKCfYN7"}},{"cell_type":"code","source":"X = data.drop(columns = \"virality\")\ny = data[\"virality\"]\n\n#PCA\npca = PCA(n_components = 1)\npca.fit(X)\n\nX = pca.transform(X)\n\nprint(np.round(pca.explained_variance_ratio_))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","metadata":{"id":"cROBxZunfYN8","outputId":"796bd390-cf4c-4dde-bcf1-1e5c8e58400c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfc = KNeighborsClassifier()\n\nrfc.fit(X_train, y_train)\ny_pred= rfc.predict(X_test)\n\nscore_cross = (cross_val_score(rfc, X_train, y_train, cv=10))\nprint(round(np.mean(score_cross),2),\"%\")\n\nprint(\"Accuracy on Traing set: \",rfc.score(X_train,y_train))\nprint(\"Accuracy on Testing set: \",rfc.score(X_test,y_test))","metadata":{"id":"3J7E64N8fYN8","outputId":"1b02b971-f1ab-447c-9571-69383c8fa7ea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nwarnings.filterwarnings('ignore')\n\nxgb = XGBClassifier(eval_metric = 'mlogloss')\n\nxgb.fit(X_train, y_train.to_numpy().ravel())\ny_pred= xgb.predict(X_test)\n\nscore_cross = (cross_val_score(xgb, X_train, y_train, cv=10))\nprint(round(np.mean(score_cross),2),\"%\")\n\nprint(\"Accuracy on Traing set: \",xgb.score(X_train,y_train))\nprint(\"Accuracy on Testing set: \",xgb.score(X_test,y_test))","metadata":{"id":"xpO0lutAfYN8","outputId":"4c795960-5212-4471-a10b-c5022c79776f"},"execution_count":null,"outputs":[]}]}