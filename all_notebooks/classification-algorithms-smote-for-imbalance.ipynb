{"cells":[{"metadata":{"_uuid":"50cfc94db3afa3c21bc053dd2bd6efae0f1f98ac"},"cell_type":"markdown","source":"# Predicting How Someone Will Vote on Basic Income\n\n### The dataset\nThis study on basic income across Europe was conducted by Dalia Research in April 2016 across 28 EU member states. The sample of n = 9.649 was drawn from all 28 states, taking into account current population distributions with regards to age (14-65 years), gender, and region/country. The dataset is available on kaggle: https://www.kaggle.com/daliaresearch/basic-income-survey-european-dataset/home\n\nThe dataset contains **9649 records and 15 columns**. These include demographics such as age, gender, education etc. It also includes opinions on the effects of a basic income on someone's work choices, their familiarity with the idea of a basic income, convincing arguments for and against a basic income - and of course, whether they ultimately approve or reject the idea.\n\nOur **goal** in this notebook is **to predict how people are likely to vote**. The target variable originally consisted of multiple classes, however, it was converted to a binary outcome. We thus have a typical classification task to solve. Several different classification models such as Logistic Regression, Random Forest, XGBoost, and Support Vector Machine (SVM) are built, optimized, evaluated, and compared. Additionally, balancing the data using the SMOTE algorithm is applied to remove a bias in prediction.\n\n### The OSEMiN-approach\n\nThe OSEMiN Process is an acronym that rhymes with “awesome” and stands for **Obtain, Scrub, Explore, Model, and iNterpret**. It can be used as a blueprint for working on data problems using machine learning tools. Preprocessing involves scrubbing (also called cleaning) and exploring the data. Building the model, evaluating, and optimizing it make up the process of machine learning."},{"metadata":{"_uuid":"e9002b23bfc7dbb9a3d4954982c7543d668559d6"},"cell_type":"markdown","source":"# Table of contents\n<a id='Table of contents'></a>\n\n### <a href='#1. Obtaining and Viewing the Data'>1. Obtaining and Viewing the Data</a>\n\n### <a href='#2. Preprocessing the Data'>2. Preprocessing the Data</a>\n\n* <a href='#2.1. Renaming Columns'>2.1. Renaming Columns</a>\n* <a href='#2.2. Excluding Unrelated Data'>2.2. Excluding Unrelated Data</a>\n* <a href='#2.3. Dealing with Misleading Data'>2.3. Dealing with Misleading Data</a>\n* <a href='#2.4. Dealing with Missing Data'>2.4. Dealing with Missing Data</a>\n* <a href='#2.5. Dealing with Duplicate Data'>2.5. Dealing with Duplicate Data</a>\n* <a href='#2.6. Basic Feature Extraction and Engineering'>2.6. Basic Feature Extraction and Engineering</a>\n\n### <a href='#3. Data Visualization'>3. Data Visualization</a>\n* <a href='#3.1. Mosaic Plots'>3.1. Mosaic Plots</a>\n* <a href='#3.2. Bar Charts'>3.2. Bar Charts</a>\n\n### <a href='#4. Machine Learning'>4. Machine Learning</a>\n\n* <a href='#4.1. Recoding Categorical Features'>4.1. Recoding Categorical Features</a>\n* <a href='#4.2. Training a Logistic Regression'>4.2. Training a Logistic Regression</a>\n* <a href='#4.3. Training a Random Forest Classifier'>4.3. Training a Random Forest Classifier</a>\n* <a href='#4.4. Training an XGBoost Classifier'>4.4. Training an XGBoost Classifier</a>\n* <a href='#4.5. Training a Support Vector Machine'>4.5. Training a Support Vector Machine</a>\n* <a href='#4.6. Model Comparison'>4.6. Model Comparison</a>\n* <a href='#4.7. Balancing the Data'>4.7. Balancing the Data</a>\n* <a href='#4.8. Model Comparison II'>4.8. Model Comparison II</a>\n\n### <a href='#5. Conclusions'>5. Conclusions</a>\n* <a href='#5.1. Feature Importance'>5.1. Feature Importance</a>\n* <a href='#5.2. Recommendation'>5.2. Recommendation</a>"},{"metadata":{"_uuid":"682b865258ddc5f883baeba1bd9d50d2ac568815"},"cell_type":"markdown","source":"### 1. Obtaining and Viewing the Data\n<a id='1. Obtaining and Viewing the Data'></a>"},{"metadata":{"_uuid":"35df443cb56802474214e28f87a054f5f0fd1d14"},"cell_type":"markdown","source":"Let's start by obtaining and investigating the pandas DataFrame:"},{"metadata":{"trusted":true,"_uuid":"062e78a4909a8bcd32a566d886fe2153ea503966","_kg_hide-output":false},"cell_type":"code","source":"import xgboost as xgb\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nimport seaborn as sns\nfrom pprint import pprint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5eaf680c6d3163108443f1a16a7d509fa9cdfb69"},"cell_type":"code","source":"# reading in dataset and viewing it\ndf = pd.read_csv('../input/basic_income_dataset_dalia.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b8f4a84df4782c6e2df2c35aa1ce281708ef131"},"cell_type":"code","source":"# get the number of rows and columns\nprint(df.shape)\n\n# get datetype info\nprint()\nprint(df.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea978e65d39bb3e1cd4ab4add7061436a2ddcfba"},"cell_type":"code","source":"# get an overview of the numeric agecolumn (.T = transposing the dataframe's order)\ndf.describe().T","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"0da81c504cde4683539f816eb2b825b077d694fa"},"cell_type":"code","source":"# get an overview of all 14 object columns/features\ndf.describe(include='object').T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d92dd10683148de432f42fd3d45b2c003b46361d"},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n### 2. Preprocessing the Data\n<a id='2. Preprocessing the Data'></a>"},{"metadata":{"_uuid":"a823f72951da99c841ab910d0a0446467e1ba940"},"cell_type":"markdown","source":"#### 2.1. Renaming Columns \n<a id='2.1. Renaming Columns'></a>"},{"metadata":{"_uuid":"6f3e75c0f8532ec8be8da9fdef12bff8033d4efd"},"cell_type":"markdown","source":"The column labels are quite wordy. Let's change that:"},{"metadata":{"trusted":true,"_uuid":"786faf40a869e4ae7960f0107b89b9c2e33139c6"},"cell_type":"code","source":"df.rename(columns = {'rural':'city_or_rural',\n                     'dem_education_level':'education',\n                     'dem_full_time_job':'full_time_job',\n                     'dem_has_children':'has_children',\n                     'question_bbi_2016wave4_basicincome_awareness':'awareness',\n                     'question_bbi_2016wave4_basicincome_vote':'vote',\n                     'question_bbi_2016wave4_basicincome_effect':'effect',\n                     'question_bbi_2016wave4_basicincome_argumentsfor':'arg_for',\n                     'question_bbi_2016wave4_basicincome_argumentsagainst':'arg_against'},\n          inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97b8b60a3b9dda5cb6635f4cbcefd51c28172744"},"cell_type":"markdown","source":"#### 2.2. Excluding Unrelated Data\n<a id='2.2. Excluding Unrelated Data'></a>"},{"metadata":{"_uuid":"1d7a065fe23953e7e5c0506192aeae78edc5f55d"},"cell_type":"markdown","source":"Again, our target is to predict how people are likely to vote. Hence, features should be included only if they're suspected to be related to the target variable. As the goal of supervised classification is to predict the target, features that obviously have nothing to do with the target should be excluded.\n\nBoth variables, the **uuid** and the **weight** (given to obtain census-representative results), are irrelevant for our classification task here. As we want to construct our own age groups later, we will also drop the predefined **age group**:"},{"metadata":{"trusted":true,"_uuid":"2b0e454fb56322d1fe02c00faade0be2ce01ddbe"},"cell_type":"code","source":"df.drop(['uuid', 'weight', 'age_group'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"905d701db649b3eff1d3468403f0375eccf66f20"},"cell_type":"code","source":"# new number of rows and columns\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bfe1020c427429e8bbae7f6817438ef8916e5c4"},"cell_type":"markdown","source":"#### 2.3. Dealing with Misleading Data\n<a id='2.3. Dealing with Misleading Data'></a>"},{"metadata":{"_uuid":"2fa9477cbf38c8ad31da6f2a816e486808932aee"},"cell_type":"markdown","source":"All the data makes perfect sense; there is nothing to correct."},{"metadata":{"_uuid":"6f4ffd1fe682bc843a68e6c765be309940a5c14b"},"cell_type":"markdown","source":"#### 2.4. Dealing with Missing Data\n<a id='2.4. Dealing with Missing Data'></a>"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"74197d13488a95fb9dae97e836446a2f25178e4d"},"cell_type":"code","source":"# checking how much total missing data we have\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39e41e59ffdcff56883a55c13005f460b3900783"},"cell_type":"code","source":"# in percentage: 7%\nround(df['education'].isna().sum() / len(df), 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ece54d50a545549b6fe2d5de010f421a4e83bdf0"},"cell_type":"code","source":"df.education.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f449aa8081433a221834e3ca00f2b46c9e9d302"},"cell_type":"code","source":"df.education.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1648eed091def4c16213b124a5de191a392457f1"},"cell_type":"markdown","source":"663 missing values - that is of no small concern! These records likely represent people with no formal education who may have been averse to disclosing that information, or who thought that giving no answer would be the right answer. So I decide to fill the NaN's with **no** formal education:"},{"metadata":{"trusted":true,"_uuid":"bd998cbfa030a2998b4ceca285f76ee478ee1a51"},"cell_type":"code","source":"df['education'].fillna('no', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"472b2f1ef7488156e5247c68bd1f096cb000e2f1"},"cell_type":"code","source":"df.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc83a2ae27c5a2256a486cdf0fac4793aa10eb0a"},"cell_type":"code","source":"df.education.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec509f852ae6b8de965e223706a6c28cb43345bb"},"cell_type":"code","source":"# new number of rows and columns\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73ef219486c8191018d8ba9e361a62fb2fe18aa5"},"cell_type":"markdown","source":"#### 2.5. Dealing with Duplicate Data\n<a id='2.5. Dealing with Duplicate Data'></a>"},{"metadata":{"trusted":true,"_uuid":"41cdfa25abeb16fce72263838e18744e76070ed7"},"cell_type":"code","source":"# check if there are any duplicates\ndf.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"335abd82112ed6a6b3bef5177bcc1e962783d847"},"cell_type":"markdown","source":"Indeed, we have some duplicates, so let's drop them:"},{"metadata":{"trusted":true,"_uuid":"7177af5014a137c68ba80db5048619a46d821e0c"},"cell_type":"code","source":"df.drop_duplicates(keep='first', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"052ccdcef933234ff0379f0030e37bd83aab3329"},"cell_type":"code","source":"# final number of rows and columns\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b76e5924c8605cc4864c00fae10a14f0fe5235b3"},"cell_type":"markdown","source":"#### 2.6. Basic Feature Extraction and Engineering\n<a id='2.6. Basic Feature Extraction and Engineering'></a>"},{"metadata":{"_uuid":"2e3a4d2b7fa67243e78d669a7a88bc308ecac0c5"},"cell_type":"markdown","source":"* Someone who \"probably votes for\" basic income, will vote the same way as someone who \"votes for it\" - namely with \"yes\". The same holds for rejection. Our target is to first predict whether someone is for or against basic income. We're not particularly interested in someone who has no opinion and/or won't vote, so let's **simplify our target** and recode the answers to drop all records that won't take any clear action:"},{"metadata":{"trusted":true,"_uuid":"533b007683f7a38cf2c86cf955b47df49821fb87"},"cell_type":"code","source":"# recode voting\ndef vote_coding(row):\n    if row == 'I would vote for it' : return('for')\n    elif row == 'I would probably vote for it': return('for')\n    elif row == 'I would vote against it': return('against')\n    elif row == 'I would probably vote against it': return('against')\n    elif row == 'I would not vote': return('no_action')\n\n# apply function\ndf['vote'] = df['vote'].apply(vote_coding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a33c4c6efa1baf2feb0995d4de6a8dc331169be3"},"cell_type":"code","source":"# drop all records who are not \"for\" or \"against\"\ndf = df.query(\"vote != 'no_action'\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d242fccda793760bae1eba9b6b86b900714b73c"},"cell_type":"code","source":"df.vote.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1eee31f85652045903d787024da9fbb566190602"},"cell_type":"markdown","source":"* Another two columns, **\"awareness\" and \"effect\"**, contain whole sentences that need to be shortened to one word to then be ready to be processed later:"},{"metadata":{"trusted":true,"_uuid":"89ebd987eb991c38b61a5501cf2268797cfe7b8c"},"cell_type":"code","source":"def awareness_coding(row):\n    if row == 'I understand it fully': return('fully')\n    elif row == 'I know something about it': return('something')\n    elif row == 'I have heard just a little about it': return('little')\n    elif row == 'I have never heard of it': return('nothing')\n\ndf['awareness'] = df['awareness'].apply(awareness_coding)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d42def705dc69dac652ad6e64d2ce657f847263e"},"cell_type":"code","source":"def effect_coding(row):\n    if row == '‰Û_ stop working': return('stop_working')\n    elif row == '‰Û_ work less': return('work_less')\n    elif row == '‰Û_ do more volunteering work': return('volunteering_work')\n    elif row == '‰Û_ spend more time with my family': return('more_family_time')\n    elif row == '‰Û_ look for a different job': return('different_job')\n    elif row == '‰Û_ work as a freelancer': return('freelancer')\n    elif row == '‰Û_ gain additional skills': return('additional_skills')\n    elif row == 'A basic income would not affect my work choices': return('no_effect')\n    else: return('none_of_the_above')\n    \ndf['effect'] = df['effect'].apply(effect_coding).astype(str)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc0bdd1e77e15f51fd3061db27eb22585407e90d"},"cell_type":"markdown","source":"* Next, let's build **new age groups** according to the 0.2 percentiles, and then drop the numeric \"age\" column:"},{"metadata":{"trusted":true,"_uuid":"7db1d2e0157129afcbdee093965d1da89389561d"},"cell_type":"code","source":"df.age.describe(percentiles=[.2, .4, .6, .8])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12be52c6467ba86712eb88375710cb4a0e10c417"},"cell_type":"code","source":"def age_groups(row):\n    if row <= 26: return('14_26')\n    elif row <= 35: return('27_35')\n    elif row <= 42: return('36_42')\n    elif row <= 49: return('43_49')\n    else: return('above_50')\n    \ndf['age_group'] = df['age'].apply(age_groups)\ndf.drop(['age'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d77db02bb37ac1ad97f205da838cb1c02a016d06"},"cell_type":"code","source":"df['age_group'].value_counts(normalize=True).plot(kind='barh', figsize=(8,4));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2824a1721a016a699ecb6a088b9cffc5508ebfd0"},"cell_type":"markdown","source":"* Lastly, let's extract the **2 or 3 most mentioned arguments PRO** and the **2 or 3 most mentioned arguments CONTRA** a basic income, and build new columns with boolean values:"},{"metadata":{"trusted":true,"_uuid":"da01f75197faf7195d65d18f8f7debd9c89bcb31"},"cell_type":"code","source":"arg_for = ['It reduces anxiety about financing basic needs',\n           'It creates more equality of opportunity',\n           'It encourages financial independence and self-responsibility',\n           'It increases solidarity, because it is funded by everyone',\n           'It reduces bureaucracy and administrative expenses',\n           'It increases appreciation for household work and volunteering',\n           'None of the above']\n\n# count all arguments\ncounter = [0,0,0,0,0,0,0]\n\nfor row in df.iterrows():\n    for i in range(0, len(arg_for)):\n        if arg_for[i] in row[1]['arg_for'].split('|'):\n            counter[i] = counter[i] + 1\n\n# create a new dictionary \ndict_keys = ['less anxiety', 'more equality', 'financial independance', \n             'more solidarity', 'less bureaucracy', 'appreciates volunteering', 'none']\n\narg_dict = {}\n\nfor i in range(0, len(arg_for)):\n    arg_dict[dict_keys[i]] = counter[i]\n\n# sub-df for counted arguments\nsub_df = pd.DataFrame(list(arg_dict.items()), columns=['Arguments PRO basic income', 'count'])\n\n# plot\nsub_df.sort_values(by=['count'], ascending=True).plot(kind='barh', x='Arguments PRO basic income', y='count',  \n                                                      figsize=(10,6), legend=False, color='darkgrey',\n                                                      title='Arguments PRO basic income')\nplt.xlabel('Count'); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f9e1a41b1e6d8d99efcbd6984606ce0bfbff656"},"cell_type":"code","source":"df['less_anxiety'] = df['arg_for'].str.contains('anxiety')\ndf['more_equality'] = df['arg_for'].str.contains('equality')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"540545e219d6064608853dbb1f849551e833b91c"},"cell_type":"code","source":"arg_against = ['It is impossible to finance', 'It might encourage people to stop working',\n               'Foreigners might come to my country and take advantage of the benefit',\n               'It is against the principle of linking merit and reward', \n               'Only the people who need it most should get something from the state',\n               'It increases dependence on the state', 'None of the above']\n\n# count all arguments\ncounter = [0,0,0,0,0,0,0]\n\nfor row in df.iterrows():\n    for i in range(0, len(arg_against)):\n        if arg_against[i] in row[1]['arg_against'].split('|'):\n            counter[i] = counter[i] + 1\n\n# create a new dictionary \ndict_keys = ['impossible to finance', 'people stop working', 'foreigners take advantage', \n             'against meritocracy', 'only for people in need', 'more dependence on state', 'none']\n\narg_dict = {}\n\nfor i in range(0, len(arg_against)):\n    arg_dict[dict_keys[i]] = counter[i]\n\n# sub-df for counted arguments\nsub_df = pd.DataFrame(list(arg_dict.items()), columns=['Arguments AGAINST basic income', 'count'])\n\n# plot\nsub_df.sort_values(by=['count'], ascending=True).plot(kind='barh', x='Arguments AGAINST basic income', y='count',  \n                                                      figsize=(10,6), legend=False, color='darkgrey',\n                                                      title='Arguments AGAINST basic income')\nplt.xlabel('Count'); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5e89b6b50834ae94d6ea960bffe001d5bf21657"},"cell_type":"code","source":"df['in_need'] = df['arg_against'].str.contains('need')\ndf['stop_working'] = df['arg_against'].str.contains('stop working')\ndf['too_costly'] = df['arg_against'].str.contains('impossible')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a97bf58706cc6a3f5ff3793a020140ea5043f62"},"cell_type":"code","source":"df.drop(['arg_for', 'arg_against'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"022b6dc4a9c32f8a09bf56245297a8cf9007cd81"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"528245ae3aa5646bb79e1e4dfeb23ca47e86fbec"},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7439b020d64fb088344e2daa97c0a7bf0dca8161"},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n### 3. Data Visualization\n<a id='3. Data Visualization'></a>"},{"metadata":{"_uuid":"2bcc8d4ad01ed87970385a6c1fd85983d29d2791"},"cell_type":"markdown","source":"Data visualization is an important step that lies between preprocessing and model buildung. It serves as a sanity check for the features and target, and may help explore the relationship between both. This will guide us in model building, and assist us in our understanding of the model and predictions. The target is what we are asked to predict: a \"yes\" to basic income, or a \"no\". \n\n**We should first examine the number of votes that fall into each category.**"},{"metadata":{"trusted":true,"_uuid":"36297e523cb5d461b078bdcd4a9c13527f038338"},"cell_type":"code","source":"df['vote'].value_counts(normalize=True).plot(kind='barh', figsize=(8,4), \n                                             color=['maroon','midnightblue']);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"218baf3896b5b9cf487ed6a0efae7bcdbd74b171"},"cell_type":"markdown","source":"By looking at the number of records we have in each class, we see that roughly 70% of the votes are for a basic income, as opposed to 30% against."},{"metadata":{"_uuid":"09170470dc18ad434af9f91a1c0847c90a141541"},"cell_type":"markdown","source":"#### 3.1. Mosaic Plots\n<a id='3.1. Mosaic Plots'></a>"},{"metadata":{"trusted":true,"_uuid":"e28461c9d78ca35067434d1c750a0483528baada"},"cell_type":"code","source":"from statsmodels.graphics.mosaicplot import mosaic\n\nmosaic(df, ['gender', 'vote'], gap=0.015, title='Vote vs. Gender - Mosaic Chart');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"682dbddea1cc28d2a532795c5a00378d089132ed"},"cell_type":"code","source":"mosaic(df, ['city_or_rural', 'vote'], gap=0.015, title='Vote vs. Area - Mosaic Chart');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58176a0825b9059f52b37773917bdb38dfd06f67"},"cell_type":"code","source":"mosaic(df, ['full_time_job', 'vote'], gap=0.015, \n       title='Vote vs. Having a Full Time Job or not - Mosaic Chart');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"995bcd9e9f5bb6b03345e137870a4d0ade529c6b"},"cell_type":"code","source":"mosaic(df, ['has_children', 'vote'], gap=0.015, \n       title='Vote vs. Having children or not - Mosaic Chart');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"045cbc680139cd3070f2f6650955f5b9bdbf12a9"},"cell_type":"markdown","source":"#### 3.2. Bar Charts\n<a id='3.2. Bar Charts'></a>"},{"metadata":{"_uuid":"121d965b2a998ac9261bfbcc246d1cda1bc17998"},"cell_type":"markdown","source":"##### Vote vs. Full Time Job"},{"metadata":{"trusted":true,"_uuid":"3d6c9f343ec7ab59f0ab377d9fadf3355d6f3cee"},"cell_type":"code","source":"# Votes depending on having a full-time-job\n\nsub_df = df.groupby('full_time_job')['vote'].value_counts(normalize=True).unstack()\nsub_df.plot(kind='bar', color=['midnightblue', 'maroon'], figsize=(7,4))\nplt.xlabel(\"Full Time Job\")\nplt.xticks(rotation=0)\nplt.ylabel(\"Percentage of Voters\\n\")\nplt.title('\\nVote depending on having a full-time-job\\n', fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.2, 1.0), title='Vote');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18999e8e6093433fb2d1f18741d17e61f01685d1"},"cell_type":"code","source":"# Votes in GERMANY and GREECE - depending on having a full-time-job\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,5))\n\n# create sub-df for Germany\nsub_df_1 = df[df['country_code']=='DE'].groupby('full_time_job')['vote'].value_counts(normalize=True).unstack()\nsub_df_1.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax1, legend=False)\nax1.set_title('\\nVotes in GERMANY depending on having a full-time-job\\n', fontsize=14, fontweight='bold')\nax1.set_xlabel(\"Full Time Job\")\nax1.set_xticklabels(labels=['No', 'Yes'], rotation=0)\nax1.set_ylabel(\"Percentage of Voters\\n\")\n\n# create sub-df for Greece\nsub_df_2 = df[df['country_code']=='GR'].groupby('full_time_job')['vote'].value_counts(normalize=True).unstack()\nsub_df_2.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax2, legend=False)\nax2.set_title('\\nVotes in GREECE depending on having a full-time-job\\n', fontsize=14, fontweight='bold')\nax2.set_xlabel(\"Full Time Job\")\nax2.set_xticklabels(labels=['No', 'Yes'], rotation=0)\nax2.set_ylabel(\"Percentage of Voters\\n\")\n\n# create one legend\nhandles, labels = ax2.get_legend_handles_labels()\nfig.legend(handles, labels, bbox_to_anchor=(0.84, 0.85))\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a1fe25b9512ae948b876f0427b08f1c7446ce47"},"cell_type":"markdown","source":"##### Vote vs. Education Level"},{"metadata":{"trusted":true,"_uuid":"3c9c7a3e463b4ffbb6689802904dd560d006c231"},"cell_type":"code","source":"# Votes depending on education level\n\nsub_df = df.groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df.plot(kind='bar', color = ['midnightblue','maroon'], figsize=(12,5))\nplt.xlabel(\"Education Level\")\nplt.xticks(rotation=0)\nplt.ylabel(\"Percentage of Voters\\n\")\nplt.title('\\nVote depending on education level\\n', fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.15, 1), title='Vote');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a2f2fedef46c543a4b4ae88b17a58db8214c23e"},"cell_type":"code","source":"# Votes in GERMANY and GREECE - depending on education level\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n\n# create sub-df for Germany\nsub_df_1 = df[df['country_code']=='DE'].groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df_1.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax1, legend=False)\nax1.set_title('\\nVotes in GERMANY depending on education level\\n', fontsize=14, fontweight='bold')\nax1.set_xlabel(\"Education Level\")\nax1.set_xticklabels(labels=['High', 'Low', 'Medium', 'No'], rotation=0)\nax1.set_ylabel(\"Percentage of Voters\\n\")\n\n# create df for Greece\nsub_df_2 = df[df['country_code']=='GR'].groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df_2.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax2, legend=False)\nax2.set_title('\\nVotes in GREECE depending on education level\\n', fontsize=14, fontweight='bold')\nax2.set_xlabel(\"Education Level\")\nax2.set_xticklabels(labels=['High', 'Low', 'Medium', 'No'], rotation=0)\n\n# create one legend\nhandles, labels = ax2.get_legend_handles_labels()\nfig.legend(handles, labels, bbox_to_anchor=(0.83, 0.85))\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"12f22b884e3938a033d4c826e7b0357d2249cc3f"},"cell_type":"code","source":"# Votes in 4 countries - depending on education level\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14,10))\n\n# create sub-df for Germany\nsub_df_1 = df[df['country_code']=='DE'].groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df_1.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax1, legend=False)\nax1.set_title('\\nVotes in GERMANY depending on education level\\n', fontsize=14, fontweight='bold')\nax1.set_xlabel(\"Education Level\")\nax1.set_xticklabels(labels=['High', 'Low', 'Medium', 'No'], rotation=0)\nax1.set_ylabel(\"Percentage of Voters\\n\")\n\n# create sub-df for France\nsub_df_2 = df[df['country_code']=='FR'].groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df_2.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax2, legend=False)\nax2.set_title('\\nVotes in France depending on education level\\n', fontsize=14, fontweight='bold')\nax2.set_xlabel(\"Education Level\")\nax2.set_xticklabels(labels=['High', 'Low', 'Medium', 'No'], rotation=0)\nax2.set_ylabel(\"Percentage of Voters\\n\")\n\n# create sub-df for Italy\nsub_df_3 = df[df['country_code']=='IT'].groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df_3.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax3, legend=False)\nax3.set_title('\\nVotes in Italy depending on education level\\n', fontsize=14, fontweight='bold')\nax3.set_xlabel(\"Education Level\")\nax3.set_xticklabels(labels=['High', 'Low', 'Medium', 'No'], rotation=0)\n\n# create sub-df for Slovakia\nsub_df_4 = df[df['country_code']=='SK'].groupby('education')['vote'].value_counts(normalize=True).unstack()\nsub_df_4.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax4, legend=False)\nax4.set_title('\\nVotes in Slovakia depending on education level\\n', fontsize=14, fontweight='bold')\nax4.set_xlabel(\"Education Level\")\nax4.set_xticklabels(labels=['High', 'Low', 'Medium', 'No'], rotation=0)\n\n# create only one legend\nhandles, labels = ax2.get_legend_handles_labels()\nfig.legend(handles, labels, bbox_to_anchor=(1.0, 0.95))\nplt.tight_layout()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f055aa4ad3bd1813469d8ed2cf9c636cbae38462"},"cell_type":"markdown","source":"##### Vote vs. Awareness"},{"metadata":{"trusted":true,"_uuid":"4b8551f235fe81421dc71f1bd84221b711962b01"},"cell_type":"code","source":"# Votes depending on awareness\n\nsub_df = df.groupby('awareness')['vote'].value_counts(normalize=True).unstack()\nsub_df.plot(kind='bar', color=['midnightblue', 'maroon'], figsize=(7,4))\nplt.xlabel(\"Awareness\")\nplt.xticks(rotation=0)\nplt.ylabel(\"Percentage of Voters\\n\")\nplt.title('\\nVote depending on awareness\\n', fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.2, 1.0), title='Vote');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f66accff57ecfbb32c5f9b479d2e57c8683efec7"},"cell_type":"markdown","source":"##### Vote vs. age group"},{"metadata":{"trusted":true,"_uuid":"6c8797d24bc59e9725b2369591a4997d272feff8"},"cell_type":"code","source":"# Votes depending on age\n\nsub_df = df.groupby('age_group')['vote'].value_counts(normalize=True).unstack()\nsub_df.plot(kind='bar', color=['midnightblue', 'maroon'], figsize=(9,5))\nplt.xlabel(\"Age Group\")\nplt.xticks(rotation=0)\nplt.ylabel(\"Percentage of Voters\\n\")\nplt.title('\\nVote depending on age\\n', fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.2, 1.0), title='Vote');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43da30e28a595bf8f33f5b26918c3b244426514d"},"cell_type":"markdown","source":"##### Vote vs. expected effects"},{"metadata":{"trusted":true,"_uuid":"0deec0684ff7f3450221a2ea12fc37ebeba19d14"},"cell_type":"code","source":"# Votes depending on effect\n\nsub_df = df.groupby('effect')['vote'].value_counts(normalize=True).unstack()\nsub_df.plot(kind='bar', color=['midnightblue', 'maroon'], figsize=(14,5))\nplt.xlabel(\"\\nEffect of Basic Income\")\nplt.xticks(rotation=0)\nplt.ylabel(\"Percentage of Voters\\n\")\nplt.title('\\nVote depending on effect\\n', fontsize=14, fontweight='bold')\nplt.legend(bbox_to_anchor=(1.1, 1.0), title='Vote');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13649ec9004d47d13fc2844a42e0987fb2a001d1"},"cell_type":"markdown","source":"##### Vote vs. arguments"},{"metadata":{"trusted":true,"_uuid":"98233439728eba65e85fb40aed457f2ca5c750cb"},"cell_type":"code","source":"# plot votes in 4 countries - depending on education level\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16,12))\n\n# create sub-df for those who agree/disagree with the argument:\n# \"It reduces anxiety about financing basic needs\"\nsub_df_1 = df.groupby('less_anxiety')['vote'].value_counts(normalize=True).unstack()\nsub_df_1.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax1, legend=False)\nax1.set_title('\\nVotes depending on attitude towards reducing_anxiety\\n', fontsize=14, fontweight='bold')\nax1.set_xlabel('\"It reduces anxiety about financing basic needs\"')\nax1.set_xticklabels(labels=['False', 'True'], rotation=0)\nax1.set_ylabel(\"Percentage of Voters\\n\")\n\n# create sub-df for those who agree/disagree with the argument:\n# \"It creates more equality of opportunity\"\nsub_df_2 = df.groupby('more_equality')['vote'].value_counts(normalize=True).unstack()\nsub_df_2.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax2, legend=False)\nax2.set_title('\\nVotes depending on attitude towards more_equality\\n', fontsize=14, fontweight='bold')\nax2.set_xlabel('\"It creates more equality of opportunity\"')\nax2.set_xticklabels(labels=['False', 'True'], rotation=0)\n\n# create sub-df for those who agree/disagree with the argument:\n# \"It might encourage people to stop working\"\nsub_df_3 = df.groupby('stop_working')['vote'].value_counts(normalize=True).unstack()\nsub_df_3.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax3, legend=False)\nax3.set_title('\\nVotes depending on attitude towards people_stop_working\\n', fontsize=14, fontweight='bold')\nax3.set_xlabel('\"It might encourage people to stop working\"')\nax3.set_xticklabels(labels=['False', 'True'], rotation=0)\nax3.set_ylabel(\"Percentage of Voters\\n\")\n\n# create sub-df for those who agree/disagree with the argument:\n# \"Only the people who need it most should get something from the state\"\nsub_df_4 = df.groupby('in_need')['vote'].value_counts(normalize=True).unstack()\nsub_df_4.plot(kind='bar', color = ['midnightblue', 'maroon'], ax=ax4, legend=False)\nax4.set_title('\\nVotes depending on attitude towards only_for_people_in_need\\n', fontsize=14, fontweight='bold')\nax4.set_xlabel('\"Only the people who need it most should get something from the state\"')\nax4.set_xticklabels(labels=['False', 'True'], rotation=0)\n\n# create only one legend\nhandles, labels = ax2.get_legend_handles_labels()\nfig.legend(handles, labels, bbox_to_anchor=(1.0, 0.95))\nplt.tight_layout()\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddf6655b1e261e0f31f48624e042dc9365d4c906"},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n### 4. Machine Learning\n<a id='4. Machine Learning'></a>"},{"metadata":{"_uuid":"e19e3ff01f0e5ca56274d051d102ad4f85e5e462"},"cell_type":"markdown","source":"#### 4.1. Recoding Categorical Features\n<a id='4.1. Recoding Categorical Features'></a>"},{"metadata":{"_uuid":"631bf808b2a6d588187dd82ae4b060f68cc3e324"},"cell_type":"markdown","source":"Machine learning algorithms generally need all data - including categorical data - in numeric form. To satisfy these algorithms, categorical features are converted into separate binary features called dummy variables.\nTherefore, we have to find a way to represent these variables as numbers before handing them off to the model. One usual way is **one-hot encoding**, which creates a new column for each unique category in a categorical variable. Each observation receives a 1 in the column for its corresponding category and a 0 in all other new columns. To conduct one-hot encoding, we use the **pandas get_dummies function.**"},{"metadata":{"trusted":true,"_uuid":"a8a512ef8e00b70f90c14cd62e725b954377ee3c"},"cell_type":"code","source":"# define our features \nfeatures = df.drop([\"vote\"], axis=1)\n\n# define our target\ntarget = df[[\"vote\"]]\n\n# create dummy variables\nfeatures = pd.get_dummies(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2e351b04e38409fb2c7412314a614ce2552b3eb"},"cell_type":"code","source":"print(features.shape)\nfeatures.tail(2)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"8a5dcc86a0c137e5cdc7dd493ea0272b2289edf9"},"cell_type":"code","source":"print(target.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e7d3dfe741f2296b68c4d8c17f79f9d768d052c"},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n#### 4.2. Training a Logistic Regression\n<a id='4.2. Training a Logistic Regression'></a>"},{"metadata":{"_uuid":"914d44735b969f2b8a0a88f33cf7a13b675842a8"},"cell_type":"markdown","source":"When approaching a supervised learning problem like ours, we should always use multiple algorithms and compare the performances of the various models. Sometimes simplest is best, and so we will start by applying logistic regression. Logistic regression makes use of what's called the logistic function to calculate the odds that a given data point belongs to a given class. Once we have more models, we can compare them based on a few performance metrics.\n\nBefore we start, let's prepare our work and import all the libraries we need for classifying our data:"},{"metadata":{"trusted":true,"_uuid":"51cd4e46b49733cb4852628ad029ee8d9c85029b"},"cell_type":"code","source":"# import train_test_split function\nfrom sklearn.model_selection import train_test_split\n\n# import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# import metrics\nfrom sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n\n# suppress all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1cd71d30bc2151c1fda76a89e30e743ee6a30ff"},"cell_type":"code","source":"# split our data\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"094a57edd3d7277e7a8800abed5e033d3ee433c3"},"cell_type":"code","source":"# instantiate the logistic regression\nlogreg = LogisticRegression()\n\n# train\nlogreg.fit(X_train, y_train)\n\n# predict\ntrain_preds = logreg.predict(X_train)\ntest_preds = logreg.predict(X_test)\n\n# evaluate\ntrain_accuracy_logreg = accuracy_score(y_train, train_preds)\ntest_accuracy_logreg = accuracy_score(y_test, test_preds)\nreport_logreg = classification_report(y_test, test_preds)\n\nprint(\"Logistic Regression\")\nprint(\"------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_logreg * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_logreg * 100):.4}%\")\n\n# store accuracy in a new dataframe\nscore_logreg = ['Logistic Regression', train_accuracy_logreg, test_accuracy_logreg]\nmodels = pd.DataFrame([score_logreg])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44336a28cb31470914014e553f250f48600fbf89"},"cell_type":"markdown","source":"#### 4.3. Training a Random Forest Classifier\n<a id='4.3. Training a Random Forest Classifier'></a>"},{"metadata":{"_uuid":"49511e0a161af35820e69b271938e46641abdae7"},"cell_type":"markdown","source":"Next, let's run a Random Forest Classifier with predefined specifications or \"hyperparameters\". Some of the important ones to tune for a Random Forest are:\n\n* n_estimators = number of trees\n* criterion = splitting criterion (for maximizing the information gain from each split)\n* max_features = max number of features considered for splitting a node\n* max_depth = max number of levels in each decision tree\n* min_samples_split = min samples needed to make a split"},{"metadata":{"trusted":true,"_uuid":"2e58936750a6ad620ebffe058644c420d7b8bb47"},"cell_type":"code","source":"# import random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21230b3febd19a3c79c22c15993566cc36e39972"},"cell_type":"code","source":"# create a baseline\nforest = RandomForestClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78a47e98ef8b3a997fcc15f3dc8ebc489358be1e"},"cell_type":"code","source":"# create Grid              \nparam_grid = {'n_estimators': [80, 100, 120],\n              'criterion': ['gini', 'entropy'],\n              'max_features': [5, 7, 9],         \n              'max_depth': [5, 8, 10], \n              'min_samples_split': [2, 3, 4]}\n\n# instantiate the tuned random forest\nforest_grid_search = GridSearchCV(forest, param_grid, cv=3, n_jobs=-1)\n\n# train the tuned random forest\nforest_grid_search.fit(X_train, y_train)\n\n# print best estimator parameters found during the grid search\nprint(forest_grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"039bce2676e3cd35e7c02e4871802f11d30d3b73"},"cell_type":"code","source":"# instantiate the tuned random forest with the best found parameters\n# here I use the parameters originally got back from GridSearch in the first round\nforest = RandomForestClassifier(n_estimators=120, criterion='gini', max_features=9, \n                                max_depth=10, min_samples_split=4, random_state=4)\n\n# train the random forest\nforest.fit(X_train, y_train)\n\n# predict\ntrain_preds = forest.predict(X_train)\ntest_preds = forest.predict(X_test)\n\n# evaluate\ntrain_accuracy_forest = accuracy_score(y_train, train_preds)\ntest_accuracy_forest = accuracy_score(y_test, test_preds)\nreport_forest = classification_report(y_test, test_preds)\n\nprint(\"Random Forest\")\nprint(\"-------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_forest * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_forest * 100):.4}%\")\n\n# append accuracy score to our dataframe\nscore_forest = ['Random Forest', train_accuracy_forest, test_accuracy_forest]\nmodels = models.append([score_forest])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04b9c36f87a81e8b1d7fe7cb413a37904248b46f"},"cell_type":"markdown","source":"#### 4.4. Training an XGBoost Classifier\n<a id='4.4. Training an XGBoost Classifier'></a>"},{"metadata":{"_uuid":"7f9e5a5e06ea056556312ebc80a25a7d10aa31c2"},"cell_type":"markdown","source":"Gradient Boosting is one of the most powerful concepts in machine learning right now. The term Gradient Boosting refers to a class of algorithms, rather than any single one. The version with the highest performance right now is XGBoost, which is short for eXtreme Gradient Boosting. XGBoost is a great choice for classification tasks. It provides best-in-class performance compared to other classification algorithms (with the exception of Deep Learning)."},{"metadata":{"_uuid":"cbaa931b17f16df05c1c59e135068a9114bc55ed"},"cell_type":"markdown","source":"Some of the important hyperparameters to tune for an XGBoost are:\n\n* n_estimators = number of trees\n* learning_rate = rate at which our model learns patterns in data (After every round, it shrinks the feature weights to reach the best optimum:)\n* max_depth = max number of levels in each decision tree\n* colsample_bytree = similar to max_features (max number of features considered for splitting a node)\n* gamma = specifies the minimum loss reduction required to make a split"},{"metadata":{"trusted":true,"_uuid":"240fbffd34a1c1b0aa8cde265c0947ca5abdf0cb"},"cell_type":"code","source":"# create a baseline\nbooster = xgb.XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"2e1a9ac92ea4f02e8d61b2bfcbfb369405489b2e"},"cell_type":"code","source":"# create Grid\nparam_grid = {'n_estimators': [100],\n              'learning_rate': [0.05, 0.1], \n              'max_depth': [3, 5, 10],\n              'colsample_bytree': [0.7, 1],\n              'gamma': [0.0, 0.1, 0.2]}\n\n# instantiate the tuned random forest\nbooster_grid_search = GridSearchCV(booster, param_grid, scoring='accuracy', cv=3, n_jobs=-1)\n\n# train the tuned random forest\nbooster_grid_search.fit(X_train, y_train)\n\n# print best estimator parameters found during the grid search\nprint(booster_grid_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bea84e0cf0d824a68b935cfc1b57d76ee13c96c4"},"cell_type":"code","source":"# instantiate tuned xgboost\nbooster = xgb.XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=100,\n                            colsample_bytree=0.7, gamma=0.1, random_state=4)\n\n# train\nbooster.fit(X_train, y_train)\n\n# predict\ntrain_preds = booster.predict(X_train)\ntest_preds = booster.predict(X_test)\n\n# evaluate\ntrain_accuracy_booster = accuracy_score(y_train, train_preds)\ntest_accuracy_booster = accuracy_score(y_test, test_preds)\nreport_booster = classification_report(y_test, test_preds)\n\nprint(\"XGBoost\")\nprint(\"-------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_booster * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_booster * 100):.4}%\")\n\n# append accuracy score to our dataframe\nscore_booster = ['XGBoost', train_accuracy_booster, test_accuracy_booster]\nmodels = models.append([score_booster])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95b8870a7275d94db1ee10cfda8922756129bcf2"},"cell_type":"markdown","source":"#### 4.5. Training a Support Vector Machine\n<a id='4.5. Training a Support Vector Machine'></a>"},{"metadata":{"_uuid":"0b69b3aab5460a2aecfe6f5fcf622d4ec5a9136b"},"cell_type":"markdown","source":"Another fast and popular classification technique is: Support Vector Machines (also referred to as SVMs). The idea behind SVMs is that we perform classification by finding the seperation line, or \"hyperplane\", that best differentiates between two classes."},{"metadata":{"trusted":true,"_uuid":"4c8feff949391f65e8b763524d2877857f524127"},"cell_type":"code","source":"from sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"029b2ba05fb6d3d8c0656535ede112af558da987"},"cell_type":"code","source":"# instantiate Support Vector Classification\nsvm = svm.SVC(kernel='rbf', random_state=4)\n\n# train\nsvm.fit(X_train, y_train)\n\n# predict\ntrain_preds = svm.predict(X_train)\ntest_preds = svm.predict(X_test)\n\n# evaluate\ntrain_accuracy_svm = accuracy_score(y_train, train_preds)\ntest_accuracy_svm = accuracy_score(y_test, test_preds)\nreport_svm = classification_report(y_test, test_preds)\n\nprint(\"Support Vector Machine\")\nprint(\"-------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_svm * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_svm * 100):.4}%\")\n\n# append accuracy score to our dataframe\nscore_svm = ['Support Vector Machine', train_accuracy_svm, test_accuracy_svm]\nmodels = models.append([score_svm])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17972c51478f65a3b6ad877ae21d35a88bb6e738"},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n#### 4.6. Model Comparison\n<a id='4.6. Model Comparison'></a>"},{"metadata":{"_uuid":"6d94732abd42c648f28d51a05a665b365f4b062c"},"cell_type":"markdown","source":"Now that we have run several models, let's check the testing accuracy we stored in a dataframe on the side as well as the classification reports:"},{"metadata":{"trusted":true,"_uuid":"8078e508c46035a95d8ef98d480fbbe702aa2512"},"cell_type":"code","source":"models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19b8b7f86e638880c2412b24af8a17e52577a852"},"cell_type":"code","source":"models.columns = ['Classifier', 'Training Accuracy', \"Testing Accuracy\"]\nmodels.set_index(['Classifier'], inplace=True)\n# sort by testing accuracy\nmodels.sort_values(['Testing Accuracy'], ascending=[False])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75d377820c905a63ddb669efb67c5b27808f4883"},"cell_type":"code","source":"print('Classification Report XGBoost: \\n', report_booster)\nprint('------------------------------------------------------')\nprint('Classification Report Logistic Regression: \\n', report_logreg)\nprint('------------------------------------------------------')\nprint('Classification Report SVM: \\n', report_svm)\nprint('------------------------------------------------------')\nprint('Classification Report Random Forest: \\n', report_forest)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6aabe39ba06d0db4abcc6945a8178682180aa5dd"},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n#### 4.7. Balancing the Data\n<a id='4.7. Balancing the Data'></a>"},{"metadata":{"_uuid":"acd8372aa61eedd3e8bd94a2fe76d1bece2e0daa"},"cell_type":"markdown","source":"All our models have done similarly well, boasting a **weighted average** F1 score between 72% to 76%. However, looking at our classification report, we can see that the *\"for\"* votes are fairly well classified, but *\"against\"* votes are disproportionately misclassified.\n\nWhy might this be the case? Well, just by looking at the number of data points we have for each class, we see that we have far more data points for the *\"for\"* votes than for *\"against\"* votes, potentially skewing our model's ability to distinguish between classes. This also tells us that most of our model's accuracy is only driven by its ability to classify the *\"for\"* votes, which is less than ideal.\n\nTo account for our imbalanced dataset, we can use an oversampling algorithm called SMOTE (Synthetic Minority Oversampling Technique). SMOTE uses the nearest neighbors of observations to create synthetic data. It's important to know is that we **only oversample the training data** - that way, none of the information in the validation data is used to create synthetic observations."},{"metadata":{"trusted":true,"_uuid":"dfa973cd69c3b5759a1e495df68cb813b8186b21"},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f67318f28d6592b5dfdfd2c5cb95d7df69a0ecc"},"cell_type":"code","source":"# view previous class distribution\nprint(target['vote'].value_counts()) \n\n# resample data ONLY using training data\nX_resampled, y_resampled = SMOTE().fit_sample(X_train, y_train) \n\n# view synthetic sample class distribution\nprint(pd.Series(y_resampled).value_counts()) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"407e41c5632372fadfbd9a2121cf39a6c24c39dc"},"cell_type":"code","source":"# then perform ususal train-test-split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f58b1ae0e11b4dcc248416d0b73e60b6b04c609"},"cell_type":"markdown","source":"**Logistic Regression:**"},{"metadata":{"trusted":true,"_uuid":"a19d4faf27239c9d48769c6682ef31f1d1189bce"},"cell_type":"code","source":"# instantiate the logistic regression\nlogreg2 = LogisticRegression()\n\n# train\nlogreg2.fit(X_train, y_train)\n\n# predict\ntrain_preds = logreg2.predict(X_train)\ntest_preds = logreg2.predict(X_test)\n\n# evaluate\ntrain_accuracy_logreg2 = accuracy_score(y_train, train_preds)\ntest_accuracy_logreg2 = accuracy_score(y_test, test_preds)\nreport_logreg2 = classification_report(y_test, test_preds)\n\nprint(\"Logistic Regression with balanced classes\")\nprint(\"------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_logreg2 * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_logreg2 * 100):.4}%\")\n\n# store accuracy in a new dataframe\nscore_logreg2 = ['Logistic Regression balanced', train_accuracy_logreg2, test_accuracy_logreg2]\nmodels2 = pd.DataFrame([score_logreg2])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d4397392130baf6ffd07f775add71e964ab0831"},"cell_type":"markdown","source":"**Random Forest:**"},{"metadata":{"trusted":true,"_uuid":"474149c2234c215ac9388dace587465de4b55d8e"},"cell_type":"code","source":"# instantiate the random forest with the best found parameters\nforest2 = RandomForestClassifier(n_estimators=120, criterion='gini', max_features=9, \n                                 max_depth=10, min_samples_split=4, random_state=4)\n\n# train the random forest\nforest2.fit(X_train, y_train)\n\n# predict\ntrain_preds = forest2.predict(X_train)\ntest_preds = forest2.predict(X_test)\n\n# evaluate\ntrain_accuracy_forest2 = accuracy_score(y_train, train_preds)\ntest_accuracy_forest2 = accuracy_score(y_test, test_preds)\nreport_forest2 = classification_report(y_test, test_preds)\n\nprint(\"Random Forest with balanced classes\")\nprint(\"-------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_forest2 * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_forest2 * 100):.4}%\")\n\n# append accuracy score to our dataframe\nscore_forest2 = ['Random Forest balanced', train_accuracy_forest2, test_accuracy_forest2]\nmodels2 = models2.append([score_forest2])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6e3784b2f356a029f603cadbaa4e7ccf641cffc"},"cell_type":"markdown","source":"**XGBoost:**"},{"metadata":{"trusted":true,"_uuid":"030a150378f887e1d33f8c9c0de264430a5b6b78"},"cell_type":"code","source":"# instantiate tuned xgboost\nbooster2 = xgb.XGBClassifier(learning_rate=0.1, max_depth=5, n_estimators=100,\n                            colsample_bytree=0.7, gamma=0.1, random_state=4)\n\n# train\nbooster2.fit(X_train, y_train)\n\n# predict\ntrain_preds = booster2.predict(X_train)\ntest_preds = booster2.predict(X_test)\n\n# evaluate\ntrain_accuracy_booster2 = accuracy_score(y_train, train_preds)\ntest_accuracy_booster2 = accuracy_score(y_test, test_preds)\nreport_booster2 = classification_report(y_test, test_preds)\n\nprint(\"XGBoost with balanced classes\")\nprint(\"-------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_booster2 * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_booster2 * 100):.4}%\")\n\n# append accuracy score to our dataframe\nscore_booster2 = ['XGBoost balanced', train_accuracy_booster2, test_accuracy_booster2]\nmodels2 = models2.append([score_booster2])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d821d9abc395b375002b6ddd88a23d04f8d993a"},"cell_type":"markdown","source":"**SVM:**"},{"metadata":{"trusted":true,"_uuid":"6612b50988f7938f6cc238b95b41d923571a362b"},"cell_type":"code","source":"from sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2e5660d553002e186e2f74b939e4bfbb2e83e9e"},"cell_type":"code","source":"# instantiate Support Vector Classification\nsvm2 = svm.SVC(kernel='rbf')\n\n# train\nsvm2.fit(X_train, y_train)\n\n# predict\ntrain_preds = svm2.predict(X_train)\ntest_preds = svm2.predict(X_test)\n\n# evaluate\ntrain_accuracy_svm2 = accuracy_score(y_train, train_preds)\ntest_accuracy_svm2 = accuracy_score(y_test, test_preds)\nreport_svm2 = classification_report(y_test, test_preds)\n\nprint(\"Support Vector Machine with balanced classes\")\nprint(\"-------------------------\")\nprint(f\"Training Accuracy: {(train_accuracy_svm2 * 100):.4}%\")\nprint(f\"Test Accuracy:     {(test_accuracy_svm2 * 100):.4}%\")\n\n# append accuracy score to our dataframe\nscore_svm2 = ['SVM balanced', train_accuracy_svm2, test_accuracy_svm2]\nmodels2 = models2.append([score_svm2])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df3633a2527809e783c2754423863c5dd9300cb0"},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n#### 4.8. Model Comparison II\n<a id='4.8. Model Comparison II'></a>"},{"metadata":{"_uuid":"abe3747213e2927484648fe245ba767e4ba9f114"},"cell_type":"markdown","source":"We've now balanced our dataset. Did balancing our dataset improved the models' bias?"},{"metadata":{"trusted":true,"_uuid":"488352d1400218259a7a4d33afa700f5af64c3e5"},"cell_type":"code","source":"models2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20e537462e09fac40c3f7b6435c23110fbd0bad6"},"cell_type":"code","source":"# Accuracy for balanced data\nmodels2.columns = ['Classifier balanced', 'Training Accuracy', \"Testing Accuracy\"]\nmodels2.set_index(['Classifier balanced'], inplace=True)\nmodels2.sort_values(['Testing Accuracy'], ascending=[False])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b87642a3a1409e98ce167bb1b2e05523c35205c0"},"cell_type":"code","source":"# Accuracy for imbalanced data\nmodels.sort_values(['Testing Accuracy'], ascending=[False])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adf53c6249a35fadbc679ebfe2ca071e92c0642b"},"cell_type":"code","source":"print('Classification Report XGBoost: \\n', report_booster2)\nprint('------------------------------------------------------')\nprint('Classification Report Logistic Regression: \\n', report_logreg2)\nprint('------------------------------------------------------')\nprint('Classification Report SVM: \\n', report_svm2)\nprint('------------------------------------------------------')\nprint('Classification Report Random Forest: \\n', report_forest2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79daec5fd979fe54d1a94a5210cb17eaf30002e0"},"cell_type":"markdown","source":"**Success! Balancing our data has removed the bias towards the more prevalent class.**"},{"metadata":{"_uuid":"1503af7cba3f8364853c45eb156e58873924d64a"},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*\n### 5. Conclusions\n<a id='5. Conclusions'></a>"},{"metadata":{"_uuid":"86f23890db0c8c6c70915df673b9e476fba04ce6"},"cell_type":"markdown","source":"#### 5.1. Feature Importance \n<a id='5.1. Feature Importance'></a>"},{"metadata":{"_uuid":"5ea61e13b95ccbd95584cab9f685bee9abefeedf"},"cell_type":"markdown","source":"As we come to an end, let's take a look at the 10 most important features for predicting someone's vote. For the sake of simplicity we use the **XGBoost** classifier and the **Random Forest** classifier:"},{"metadata":{"trusted":true,"_uuid":"9b4d5e29ba51efceee88596e868941b2616d2f0e"},"cell_type":"code","source":"# plot the important features - based on XGBoost\nfeat_importances = pd.Series(booster.feature_importances_, index=features.columns)\nfeat_importances.nlargest(10).sort_values().plot(kind='barh', color='darkgrey', figsize=(10,5))\nplt.xlabel('Relative Feature Importance with XGBoost');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbb5a895575b631bb4740d72da41ffd84fcf2984"},"cell_type":"code","source":"# plot the important features - based on Random Forest\nfeat_importances = pd.Series(forest.feature_importances_, index=features.columns)\nfeat_importances.nlargest(10).sort_values().plot(kind='barh', color='darkgrey', figsize=(10,5))\nplt.xlabel('Relative Feature Importance with Random Forest');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4c56b74ccb1036370ea0f198d882c3214437271"},"cell_type":"markdown","source":"This last plot, produced by our tuned Random Forest, draws the most distinct picture. We see that for the Random Forest, the attitude towards the argument **\"It creates more equality of opportunity\"** is the most important feature to split people."},{"metadata":{"_uuid":"57bbfe790c63c51977fd80fcef5715947daf49f1"},"cell_type":"markdown","source":"#### 5.2. Recommendation \n<a id='5.2. Recommendation'></a>"},{"metadata":{"_uuid":"abde52b3e70dfb84e9fcf451341355bc078c7251"},"cell_type":"markdown","source":"Going back to our intial question: **\"Can we predict how people are likely to vote?\"**, we can now conclude that the answer is **\"Yes\"**.\n\nWith an XGBoost and the SMOTE algorithm to account for imbalanced classes in our target variable, we were able to both improve our accuracy to nearly 84% - in addition to removing any bias towards the yes-voters. So as fas as classification algorithms go, let's use the **XGBoost**."},{"metadata":{"_uuid":"7b4eb5032a32bc5203c0eb059f1547dce83dd913"},"cell_type":"markdown","source":"*Back to: <a href='#Table of contents'> Table of contents</a>*"},{"metadata":{"trusted":true,"_uuid":"e5656343b9f0817222878ee4c1a3af49515e4d37"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}