{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_t = pd.read_csv('/kaggle/input/language-translation-englishfrench/eng_-french.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_t.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train_t[0:100000]\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"english_text = df['English words/sentences']\nfrench_text = df['French words/sentences']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nenglish = []\nfrench = []\nfor i in range(len(english_text)):\n    text = english_text[i].lower()\n    text = re.sub('[^a-zA-Z]',' ',text)\n    english.append(text)\n    \n\nfor i in range(len(french_text)):\n    ftext = french_text[i].lower()\n    ftext = (re.sub(\"[^a-zA-Z' àâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ]\",' ',ftext))\n    french.append(\"START_ \" + ftext + \" _END\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"french","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Vocabulary of English\nall_eng_words = set()\nfor i in english:\n    for j in i.split():\n        all_eng_words.add(j)\n\n#vocabulary of french\nall_fre_words = set()\nfor i in french:\n    for j in i.split():\n        all_fre_words.add(j)\n\n#maxlen of the source sequence\nmax_length_src = 0\nfor i in english:\n    a = len(i.split())\n    if a>max_length_src:\n        max_length_src = a\n        \n#maxlen of the target sequence\nmax_length_tar = 0\nfor j in french:\n    b = len(j.split())\n    if b>max_length_tar:\n        max_length_tar = b\n        \n\ninput_words = sorted(list(all_eng_words))\ntarget_words = sorted(list(all_fre_words))\n\n# Calculate Vocab size for both source and targe\nnum_encoder_tokens = len(all_eng_words)\nnum_decoder_tokens = len(all_fre_words)\n\n\n#indexs for input and target sequences\ninput_index = dict([(words,i) for i,words in enumerate(input_words)])\ntarget_index = dict([(word, i) for i, word in enumerate(target_words)])\n\nreverse_input_index = dict((i, word) for word, i in input_index.items())\nreverse_target_index = dict((i, word) for word, i in target_index.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(max_length_src)\nprint(max_length_tar)\nprint(num_encoder_tokens)\nprint(num_decoder_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_input_data = np.zeros((100000, max_length_src, num_encoder_tokens),dtype='float32')\ndecoder_input_data = np.zeros((100000, max_length_tar, num_decoder_tokens),dtype='float32')\ndecoder_target_data = np.zeros((100000, max_length_tar, num_decoder_tokens),dtype='float32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(100000):\n    for i,text in enumerate(english[j].split()):\n        encoder_input_data[j,i,input_index[text]] = 1.\n\nfor j in range(100000):\n    for i,text in enumerate(french[j].split()):\n        decoder_input_data[j,i,target_index[text]] = 1.\n        if i>0:\n            decoder_target_data[j,i-1,target_index[text]] = 1.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras, tensorflow\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, Dense, Bidirectional","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\nepochs = 100\nlatent_dim = 256 #size of the lstms hidden state","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://miro.medium.com/max/3240/1*1I2tTjCkMHlQ-r73eRn4ZQ.png"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Time to bulid the model\n\n#inputs for the encoder\nencoder_inputs = Input(shape=(None,num_encoder_tokens))\n#encoder lstm\nencod_lstm = (LSTM(latent_dim,return_state = True))\nencoder_output,state_h,state_c = encod_lstm(encoder_inputs)\n\n#hidden from encoder to pass to the decoder as initial hidden state\nencoder_states = [state_h,state_c]\n\n#inputs for the decoder\ndecoder_inputs = Input(shape=(None,num_decoder_tokens))\n#decoder lstm \ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_output,_,_= decoder_lstm(decoder_inputs,initial_state = encoder_states)\n#The decoder output is passed through the softmax layer that will learn to classify the correct french character\n#Activation functions are used to transform vectors before computing the loss in the training phase\n#for more on softmax https://gombru.github.io/2018/05/23/cross_entropy_loss/\ndense_layer = Dense(num_decoder_tokens, activation='softmax')\ndecoder_output = dense_layer(decoder_output)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_output)\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit([encoder_input_data,decoder_input_data],decoder_target_data,batch_size= 64,epochs= 50,validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model = Model(encoder_inputs,encoder_states)\n\ndecoder_state_h = Input(shape=(latent_dim,))\ndecoder_state_c = Input(shape=(latent_dim,))\ndecode_state = [decoder_state_h,decoder_state_c]\n\ndecoder_outputs,state_h,state_c = decoder_lstm(decoder_inputs,initial_state = decode_state)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = dense_layer(decoder_outputs)\n\ndecoder_model = Model([decoder_inputs] + decode_state,[decoder_outputs] + decoder_states)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(decoder_model, to_file='model.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sequence(input_seq):\n    # encode the input sequence to get the internal state vectors.\n    states_value = encoder_model.predict(input_seq)\n  \n    # generate empty target sequence of length 1 with only the start character\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    target_seq[0, 0, target_index['START_']] = 1.\n  \n    # output sequence loop\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n    \n        # sample a token and add the corresponding character to the \n        # decoded sequence\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_index[sampled_token_index]\n        \n        if (sampled_char == \"_END\" or len(decoded_sentence) > max_length_tar):\n            stop_condition = True\n            break\n            \n        decoded_sentence += sampled_char\n        decoded_sentence +=' '\n      \n        # update the target sequence (length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n    \n        # update states\n        states_value = [h, c]\n    \n    return decoded_sentence\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"toks = ['i love you','run fast','she is the client','my name is tom']\nfor t in toks:\n    input_sentence = t\n    test_sentence_tokenized = np.zeros((1, max_length_src, num_encoder_tokens), dtype='float32')\n    for t, char in enumerate(input_sentence.split()):\n        test_sentence_tokenized[0, t, input_index[char]] = 1.\n    print(input_sentence)\n    print(decode_sequence(test_sentence_tokenized))\n    print(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#result je vous aime is i love you in english\n#result un fait vite is move fast in english\n#result elle est dans le tu is she’s in the you \n#result mon nom est tom is my name is tom","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}