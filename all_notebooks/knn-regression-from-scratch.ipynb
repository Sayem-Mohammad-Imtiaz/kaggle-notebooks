{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ntrain = pd.read_csv('../input/bigmart-sales-data/Train.csv')  #Both files have been added directly from 'BigMart Sales Data' dataset\ntest = pd.read_csv('../input/bigmart-sales-data/Test.csv')\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(train.info())    #Checking datatypes and missing values \nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.isna().sum())    #Checking  number of missing values for each column\nprint(test.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns = train.columns.str.lower()    #Changing column names to lowercase so I don't have to worry about case\ntest.columns = test.columns.str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Imputing missing values using the fillna method\ntrain.item_weight.fillna(train.item_weight.mean(), inplace = True)   #Imputing missing item_weight by mean item_weight (numerical data)   \ntrain.outlet_size.fillna(train.outlet_size.mode()[0], inplace=True)  #Imputing missing outlet_size by its mode (categorical data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.item_weight.fillna(test.item_weight.mean(), inplace = True)\ntest.outlet_size.fillna(test.outlet_size.mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.isna().sum())    #Checking  number of missing values for each column\nprint(test.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(train[['item_identifier','outlet_identifier','outlet_establishment_year']], inplace=True,  axis=1)\ntest.drop(test[['item_identifier','outlet_identifier','outlet_establishment_year']], inplace=True,  axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.columns)\nprint(test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies(train)\n#item_fat_content column has duplicate values - LF, Low Fat and low fat; Regular and reg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.item_fat_content.replace(['LF','low fat','reg'],['Low Fat','Low Fat','Regular'], inplace=True)   #Relpacing duplicate va;ues\ntest.item_fat_content.replace(['LF','low fat','reg'],['Low Fat','Low Fat','Regular'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.item_type.unique())                #Checking other duplicate values\nprint(train.outlet_size.unique()) \nprint(train.outlet_location_type.unique())\nprint(train.outlet_type.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.get_dummies(train)      #Dummifying categorical variables\ntest = pd.get_dummies(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = train.drop(train[['item_outlet_sales']], axis=1)   #Separating dependent and independent variables of the train dataset\ntrain_y = train['item_outlet_sales']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scaler(data):      #Creating a MinMax Scaler function\n    for col in data:\n        data[col] = (data[col] - min(data[col])) / (max(data[col]) - min(data[col]))\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = scaler(train_x)      #Normalizing the train_x dataframe\ntest = scaler(test)       #Normalizing the test dataframe\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_array = np.array(train_x)\ntest_array = np.array(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating distance between test and train row using (train - test)^2 = train^2 + test^2 - 2*train*test\ndef euclidian_distance(trainset, testset):      \n    m = trainset.shape[0]      #Assigning variables for number of rows of train_x and test_x datasets\n    n = testset.shape[0]\n    \n    trainset_dots = (trainset * trainset).sum(axis=1).reshape(m,1) * np.ones(shape=(1,n))    \n    \n    testset_dots = (testset * testset).sum(axis=1) * np.ones(shape=(m,1))\n    \n    test_train_dots = -2 * trainset.dot(testset.T)\n    \n    distance = np.sqrt(trainset_dots + testset_dots + test_train_dots)\n    \n    return distance","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distance_array = euclidian_distance(train_array, test_array)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distance_array.shape          #Shape should be equal to (number of rows of train, number of rows of test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Knn_regression(distance_data, k):\n    nn_indices = np.argsort(distance_data, axis=0)[:k:]    #Getting the corresponding indices of the elements of sorted distance matrix in original distance matrix  \n    nn_indices = pd.DataFrame(nn_indices)\n    \n    pred = pd.DataFrame(np.zeros((len(test),1)))                        #Creating a zero matrix of the same size as test_y\n    \n    for i, j in nn_indices.iteritems():             #Calculating sum of k-nearest neighbors and storing it in pred dataframe\n        sum = 0.0\n        for x in j:\n            sum = sum + train_y.iloc[x]\n        a = np.array(sum/k)\n        pred.iloc[i,0] = a\n    \n    return pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = Knn_regression(distance_array, 9)\npredicted.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = pd.DataFrame(predicted)\npredicted","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}