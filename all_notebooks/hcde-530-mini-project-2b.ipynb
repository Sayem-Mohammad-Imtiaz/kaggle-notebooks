{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Author: Kenny Nguyen\n# HCDE 530\n# Mini Project 2b\n\nThis notebook describes webscraping from the [Dear Abby letter archives](https://www.uexpress.com/life/dearabby/archives) and subsequent data cleaning after its conversion to a DataFrame. The CSV file made from this DataFrame was then imported into Clarifai for text classification using machine learning.","metadata":{}},{"cell_type":"markdown","source":"## Load Libraries\nLoading relevant libraries for scraping and data cleaning.","metadata":{}},{"cell_type":"code","source":"# Before running this cell, run the following line of code:\n# pip install beautifulsoup4\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport requests # Sends request to get website HTML\nfrom bs4 import BeautifulSoup # Parses HTML\n\n\nimport datetime as dt # Library for reading or converting date formats\nfrom collections import namedtuple # Cleaner way to access a tuple by field name instead of position index\nfrom calendar import monthrange # Indicates days in each month for specified year\n\nfrom tqdm import tqdm # Creates loading bar that is useful for displaying iterator progress\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-10T03:51:26.500059Z","iopub.execute_input":"2021-06-10T03:51:26.500369Z","iopub.status.idle":"2021-06-10T03:51:26.652675Z","shell.execute_reply.started":"2021-06-10T03:51:26.50034Z","shell.execute_reply":"2021-06-10T03:51:26.652031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gather List of Webpages\nAs of June 2021, each \"Dear Abby\" letter is displayed on its own webpage. The URLs for each webpage correspond to the year, month, and date of the letter.","metadata":{}},{"cell_type":"code","source":"# Create a named tuple whose fields can be accessed by name rather than index.\nDate = namedtuple(\"Date\", [\"year\", \"month\", \"day\"])\n\n# Create a function that returns all dates in a given year.\n# (Works for leap years!)\ndef all_dates_in_year(year):\n    for month in range(1, 13):\n        for day in range(1, monthrange(year, month)[1] + 1):\n            yield Date(year, month, day)\n\n# Initialize an empty list that will store the URLs to be scraped.\nurl_list = []\n\n# Use a for loop to add the URLs for each \"Dear Abby\" letter into the url_list.\nfor calendar_year in range(2010, 2021):\n    for date in all_dates_in_year(calendar_year):\n        base = \"https://www.uexpress.com/life/dearabby\"\n        year = date[0]\n\n        # Prepend 0 in front of single-digit months.\n        if date[1] < 10:\n            month = \"0\" + str(date[1])\n        else:\n            month = date[1]\n\n        # Prepend 0 in front of single-digit days.\n        if date[2] < 10:\n            day = \"0\" + str(date[2])\n        else:\n            day = str(date[2])\n\n        # Build full URL using base, year, month, and day then append to url_list.\n        full_link = base + \"/\" + str(year) + \"/\" + str(month) + \"/\" + day\n        url_list.append(full_link)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scrape Webpages\n\nScraping each webpage in the list of URLs for the date, title, and letter text. The top-most iterator is wrapped in tqdm() to display a loading bar for scraping progress. There is over 4000 URLs and the initial scraping took over 1.5 hours, so run this code block only once!","metadata":{}},{"cell_type":"code","source":"list_header = [\"year\", \"month\", \"day\", \"url\", \"title\", \"text\"] # Becomes header for DataFrame\ndata = [] # Stores data from webscraping\n\n# Iterate through each URL in the list to scrape its contents.\nfor url in tqdm(url_list):\n    # Send a GET request for the HTML of the webpage then parse it for readability.\n    req = requests.get(url)\n    soup = BeautifulSoup(req.content,'html.parser')\n    \n    # Target part of the webpage containing the letter.\n    block = soup.find(class_=\"ContentSidebar_content__main__3P2AH\")\n    if block is not None:\n        \n        # Some pages contain a series of letters. Target the webpage contents even\n        # further to return every letter.\n        block = block.find_all(class_=\"Article_article__section__2lhpN\")\n        \n        # Use a for loop to scan each letter on a webpage for contents.\n        for letter in block:\n            sub_data = []\n\n            # Convert datetime string on page into datetime object.\n            date_time_str = letter.find(\"time\").get(\"datetime\")\n            date_time_obj = dt.datetime.strptime(date_time_str, '%Y-%m-%d')\n\n            title = letter.find(\"h1\").get_text()\n            \n            # The text of an individual may be split into multiple paragraphs.\n            # To avoid a fencepost error, record the first paragraph before using a for-loop\n            # to add in every paragraph after it.\n            text = letter.find(\"p\").get_text()\n            if text is not None:\n                for paragraph in letter.find_all(\"p\")[1:]:\n                    # End text scraping when it reaches the author's reply to the reader's problem.\n                    if paragraph.get_text().startswith(\"DEAR\"):\n                        break\n                    else:\n                        text = text + \" \" + paragraph.get_text()\n\n                sub_data.append(date_time_obj.year)\n                sub_data.append(date_time_obj.month)\n                sub_data.append(date_time_obj.day)\n                sub_data.append(url)\n                sub_data.append(title)\n                sub_data.append(text)\n                data.append(sub_data)","metadata":{"execution":{"iopub.status.busy":"2021-06-08T23:36:19.510153Z","iopub.execute_input":"2021-06-08T23:36:19.510532Z","iopub.status.idle":"2021-06-09T00:54:25.312601Z","shell.execute_reply.started":"2021-06-08T23:36:19.510503Z","shell.execute_reply":"2021-06-09T00:54:25.311752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create and Clean DataFrame\nAfter scraping the webpages, they are converted into a DataFrame and saved as a CSV file. This will prevent us from having to scrape the pages again, which is a long process. Instead, we can directly read in the CSV file when we want to manipulate the data.","metadata":{}},{"cell_type":"code","source":"# Only run this line when you first scrape the code. Subsequent manipulation should be done by\n# reading in the CSV file that is created.\n# dataFrame = pd.DataFrame(data, columns = list_header)\n\n# Convert the DataFrame into a CSV file. This can be downloaded from the output folder.\n# dataFrame.to_csv(\"dearabby.csv\")\n\n# Run this line to read the CSV after it is uploaded. This should be the beginning step after the DataFrame\n# was created and converted into a CSV.\ndataFrame = pd.read_csv(\"../input/dear-abby-20102020/dearabby.csv\")\n\n# Filter for letters that are addressed to Abby. No letters starting with \"DEAR READERS\" i.e.\n# letters from the authors to the readers of the column.\ndataFrame = dataFrame[dataFrame[\"text\"].str.startswith(\"DEAR ABBY\")]\n\n# Remove the addressee text from the letters.\ndataFrame[\"text\"] = dataFrame[\"text\"].str.replace(\"DEAR ABBY: \", \"\")\n\n# Remove any double-quotes from the letters.\ndataFrame[\"text\"] = dataFrame[\"text\"].str.replace(\"\\\"\", \"\")\n\n# Wrap commas in double-quotes, which is a requirement for inputting text data into Clarifai.\ndataFrame[\"text\"] = dataFrame[\"text\"].str.replace(\",\", \"\\\",\\\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}