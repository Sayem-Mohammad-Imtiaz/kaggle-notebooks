{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Linear Regression with Assumptions**\n\n1. Regression analysis is one of the most widely used methods for prediction.","metadata":{}},{"cell_type":"markdown","source":"* Here I am explaining the linear regression algorithm with assumption.  ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Linear regression is a **linear model**, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y).\n\n2. More specifically, that y can be calculated from a linear combination of the input variables (x).\n\n3. When there is a **single input variable (x)**, the method is referred to as simple linear regression and When more than **one input variable** multiple input variables.\n\n4. Different techniques can be used to prepare or train the linear regression equation from data, the most common of which is called **Ordinary Least Squares**. It is common to therefore refer to a model prepared this way as Ordinary Least Squares Linear Regression or just Least Squares Regression.","metadata":{}},{"cell_type":"markdown","source":"**Linear Regression Equation:-**\n\n>                                 y = B0 + B1*x","metadata":{}},{"cell_type":"markdown","source":"1. The linear equation assigns one scale factor to each input value or column, called a coefficient and represented by the capital Greek letter Beta (B). \n\n2. One additional coefficient is also added, giving the line an additional degree of freedom (e.g. moving up and down on a two-dimensional plot) and is often called the intercept or the bias coefficient.\n\n3. In higher dimensions when we have more than one input (x), the line is called a plane or a hyper-plane.","metadata":{}},{"cell_type":"markdown","source":"**Types**\n\n*    There are many more techniques because the model is so well studied. \n\n**1. Simple Linear Regression**\n\nWith simple linear regression when we have a single input, we can use statistics to estimate the coefficients.\n\nThis requires that you calculate statistical properties from the data such as means, standard deviations, correlations and covariance. All of the data must be available to traverse and calculate statistics.\n\n\n**2. Ordinary Least Squares**\n\nWhen we have more than one input we can use Ordinary Least Squares to estimate the values of the coefficients.\n\nThe Ordinary Least Squares procedure seeks to minimize the sum of the squared residuals. This means that given a regression line through the data we calculate the distance from each data point to the regression line, square it, and sum all of the squared errors together.\n\nThis approach treats the data as a matrix and uses linear algebra operations to estimate the optimal values for the coefficients. It means that all of the data must be available and you must have enough memory to fit the data and perform matrix operations.\n\n**3. Gradient Descent**\n\nWhen there are one or more inputs you can use a process of optimizing the values of the coefficients by iteratively minimizing the error of the model on your training data.\n\nThis operation is called Gradient Descent and works by starting with random values for each coefficient. The sum of the squared errors are calculated for each pair of input and output values. A learning rate is used as a scale factor and the coefficients are updated in the direction towards minimizing the error. The process is repeated until a minimum sum squared error is achieved or no further improvement is possible.\n\nWhen using this method, you must select a learning rate (alpha) parameter that determines the size of the improvement step to take on each iteration of the procedure\n\n**4. Regularization**\n\nThere are extensions of the training of the linear model called regularization methods. \n\nThese seek to both minimize the sum of the squared error of the model on the training data (using ordinary least squares) but also to reduce the complexity of the model (like the number or absolute size of the sum of all coefficients in the model).\n\n*  Two popular examples of regularization procedures for linear regression are:\n\n**Lasso Regression:**\n        where Ordinary Least Squares is modified to also minimize the absolute sum of the coefficients (called L1 regularization).\n        \n        \n**Ridge Regression:**\n        where Ordinary Least Squares is modified to also minimize the squared absolute sum of the coefficients (called L2 regularization).\nThese methods are effective to use when there is collinearity in your input values and ordinary least squares would overfit the training data.","metadata":{}},{"cell_type":"markdown","source":"1. Indenpendent variable(X=YearsExperience) and Dependent variable(y=Salary)","metadata":{}},{"cell_type":"code","source":"#import required library \n\nimport numpy as np\nimport pandas as pd\nfrom pandas import Series,DataFrame\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read the dataset \n\ndf = pd.read_csv(\"../input/salary-data-dataset-for-linear-regression/Salary_Data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**EDA**","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check number of rows and number of columns\ndf.shape     #30 rows and 2 columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check dataset information \n\ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Here total 30 examples with no any missing values. \n\n* one columns is float and another is intger.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check any missing values\n\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#any outliers\n\ndf.skew()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.kurt()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* All values are approx. same","metadata":{}},{"cell_type":"code","source":"#check any correlation and covariance\n\ndf[['YearsExperience','Salary']].cov()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualization**","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df.corr(),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. * Here, clearly shows that linearly relationship between dependent and independent variable. ","metadata":{}},{"cell_type":"markdown","source":"**Independent and Dependent Variables**","metadata":{}},{"cell_type":"code","source":"X=df.drop('Salary',axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=df.Salary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Splitting the data**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0,test_size=0.30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Fitting**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR=LinearRegression()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR.intercept_   # beta 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR.coef_        # beta 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Prediction**","metadata":{}},{"cell_type":"code","source":"y_pred=LR.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluation**","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"R2=metrics.r2_score(y_test,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"R2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(metrics.mean_absolute_error(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(metrics.mean_squared_error(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.sqrt(metrics.mean_squared_error(y_test,y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(LR.predict([[5]])) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Assumptions**","metadata":{}},{"cell_type":"code","source":"error= y_test-y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**No Autocorrelation**","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as smt\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acf=plot_acf(error)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normality**","metadata":{}},{"cell_type":"code","source":"sns.distplot(error)\nplt.xlabel('residual')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Linearity**","metadata":{}},{"cell_type":"code","source":"sns.regplot(X,y)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Homoscedasticity**","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x=y_pred,y=error)\nplt.xlabel('predicted values')\nplt.ylabel('residuals')\nplt.xlim([0,150000])\nplt.ylim([-8000,8000])\nsns.lineplot([0,150000],[0,0],color='blue')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Multicollinearity**","metadata":{}},{"cell_type":"code","source":"sns.heatmap(df.corr(),annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}