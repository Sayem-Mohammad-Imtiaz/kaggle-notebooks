{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nplt.style.use('fivethirtyeight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Importing and viewing the data\ntest_data = pd.read_csv('/kaggle/input/predict-test-scores-of-students/test_scores.csv')\ntest_data.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean the data by removing unnecessary columns\ntest_data.drop(['classroom', 'student_id'], axis = 1, inplace = True)\ntest_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Conduct checks on the data\nlen(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for missing data\ntest_data.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run checks on the set of values in the categorical variables\n# The number of different schools in the data\nlen(set(test_data['school']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the options for the categorical variables\ndef categorical_options(categories):\n    cat_options = {}\n    for cat in categories:\n        options = list(set(test_data[cat]))\n        cat_options[cat] = options\n        \n    return cat_options\n        \ncategories = ['school_setting', 'school_type', 'teaching_method', 'gender', 'lunch']\ncategorical_options(categories)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## An exploratory data analysis on the data","metadata":{}},{"cell_type":"code","source":"# Extract pretest and posttest scores into numpy arrays\npretest = test_data['pretest'].values\nposttest = test_data['posttest'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average pretest and posttest scores\navg_pre = np.mean(pretest)\navg_post = np.mean(posttest)\navg_pre, avg_post","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of students who performed better than average in both tests\nnp.sum(pretest > avg_pre), np.sum(posttest > avg_post)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Highest pretest score\nnp.max(pretest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# how many students obtained the highest pretest scores?\nnp.sum(pretest == 93)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Which student obtained the highest pretest? What are his/her characteristics?\ntest_data.iloc[np.argmax(pretest), :]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Highest score in the posttest\nnp.max(posttest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many students obtained the highest posttest score?\nnp.sum(posttest == 100.0)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Looking into students who obtained the highest post test scores\ntest_data[test_data.posttest == 100]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Intersttingly, they share the same features except for gender (2 females, 6 males) and their pretest scores.","metadata":{}},{"cell_type":"code","source":"# Are there students who did not improve upon their pretest scores?\nnp.any(pretest >= posttest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many of such students are there?\nnp.sum(pretest >= posttest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualising the categorical variables","metadata":{}},{"cell_type":"code","source":"# Bar chart on school setting\nsns.countplot(x= test_data.school_setting);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bar chart on school type\nsns.countplot(x = test_data.school_type);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot of teaching method\nsns.countplot(x = test_data.teaching_method);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bar chart on gender\nsns.countplot(x = test_data.gender);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bar chart on lunch\nsns.countplot(x = test_data.lunch);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysing the continous variables (Pretest and Posttest)","metadata":{}},{"cell_type":"code","source":"# Run descriptive statistics on the continous variables\ntest_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visulaise the pretest scores with the box plot and histogram\nplt.style.use('seaborn-darkgrid')\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 6))\n\nsns.boxplot(x = test_data.pretest, ax = ax1, linewidth=1.5)\nsns.histplot(x = test_data['pretest'], ax = ax2, kde = True)\n\nax1.tick_params(labelsize = 14)\nax2.tick_params(labelsize = 14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visulaise the pretest scores with the box plot and histogram\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 6))\n\nsns.boxplot(y = test_data.posttest, x = test_data.gender, ax = ax1, linewidth=1.5)\nsns.histplot(x = test_data['posttest'], ax = ax2, kde = True)\nax1.tick_params(labelsize = 14)\nax2.tick_params(labelsize = 14)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A quick plot of both pretext and post test side by side\ntest_data[['pretest', 'posttest']].plot(figsize = (16, 9), linewidth = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding correlation between pre-test and post-test scores","metadata":{}},{"cell_type":"code","source":"# Is there a correlation between pretest and posttest scores?\n# Let's visualise by scatter - plotting the pretest against the post test\n\nplt.style.use('fivethirtyeight')\n\nfig, ax = plt.subplots(figsize = (12, 7))\n\nax.plot('pretest', 'posttest', 'o', data = test_data, color = 'maroon', markersize = 5)\n# ax.plot(x, y, ls = '--', lw = 1.5, color = 'black')\n\n# Add title to the plot\nfig.suptitle('A plot of pretest against posttest scores', fontweight = 'bold', fontsize = 18, color = 'maroon')\n\n# Customise then x and y labels\nax.set_xlabel('Pretest scores', fontsize = 16, fontweight = 'bold', color = 'firebrick')\nax.set_ylabel('Posttest scores', fontsize = 16, fontweight = 'bold', color = 'firebrick')\n\n# Add vertical and horizontal average lines average lines\n# ax.axhline(test_data['posttest'].mean(), ls = '--', linewidth = 1.5)\n# ax.axvline(test_data['pretest'].mean(), ls = '--', linewidth = 1.5, color = 'green')\n\nax.tick_params(labelsize = 14, labelcolor = 'orangered')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph, it can clearly be seen that there exist a strong positive correlation between the pretest and posttest score","metadata":{}},{"cell_type":"code","source":"# import scipy as sp\nimport scipy as sp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating for spearman's rank correlation among the scores\nprint(sp.stats.spearmanr(pretest, posttest))\n\n# Calculating for Pearson's correlation among the scores\npearsonr = sp.stats.pearsonr(pretest, posttest)\nprint('Pearsonr correlaton =', pearsonr[0], 'P-value =', pearsonr[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** Both correlations show that there exist a statistically significant (p < 0.05) **strong positive correlation** among the two-scores. \n\nThe Spearman's correlation specifically implies that there is a strong positive correlation when the students are ranked according to their performances in both tests. Thus students who ranked high in the pretest most likely ranked high in the post-test and vice-versa.","metadata":{}},{"cell_type":"markdown","source":"### Inferential statistics on the continous variables\n#### Perform paired sample t-test","metadata":{}},{"cell_type":"markdown","source":"**Test for normality in the scores**","metadata":{}},{"cell_type":"code","source":"# H0: The scores are normally distributed\n# H1: The scores are not normally distributed\n\n# check for normality on pretest scores\nsp.stats.shapiro(test_data.pretest)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# H0: The scores are normally distributed\n# H1: The scores are not normally distributed\n\n# Test for normality from the posttest scores\nsp.stats.shapiro(test_data.posttest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test for homogeneity of variances**\n\nNot necessary for paired samples because we are dealing with the same group.","metadata":{}},{"cell_type":"code","source":"# H0: The scores have equal variances\n# H1: The scores do not have equal variances\n\nsp.stats.levene(pretest, posttest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** The null hypothesis is upheld because p-value > 0.05 (Not statistically significant)","metadata":{}},{"cell_type":"markdown","source":"The test for normality for both scores suggest that they both fail the normaility test profoundly. Thus, the null hypothesis is rejected for both scores because they have significant p-values (p-value < 0.05).\n\n*In view of this, t-test is not advisable but we will try it anyway.*","metadata":{}},{"cell_type":"markdown","source":"**Performing the paired sample T-test**","metadata":{}},{"cell_type":"code","source":"# H0: There is no difference between the means of pre- and post-test scores\n# H1: There is a difference between the means of pre- and post-test scores\n\n# Performing the paired sample t-test\nsp.stats.ttest_rel(pretest, posttest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The null hypothesis is therefore rejected because p-value = 0.0[<0.05] (statistically significant).**\n\nHence, we accept the alternative hypothesis that there is a difference between the means of the pretest and posttest scores.","metadata":{}},{"cell_type":"code","source":"# A more appropriate statistic Wilconson test should be used\n\n# H0: There is no difference between the means of pre- and post-test scores\n# H1: There is a difference between the means of pre- and post-test scores\n\nsp.stats.wilcoxon(test_data.pretest, test_data.posttest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Like the T-test, the null hypothesis is therefore because p-value = 0.0[<0.05] (statistically significant)**","metadata":{}},{"cell_type":"markdown","source":"#### Performing independent sample T-test\n* For gender on pretest and posttest scores","metadata":{}},{"cell_type":"code","source":"# Examine the average performance on post_test and pre-test based on gender\ntest_data.groupby(['gender'])[['pretest', 'posttest']].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Independent Sample T-test between male and female on pre-test scores\nmale_pretest = test_data[test_data.gender == 'Male']['pretest']\nfem_pretest = test_data[test_data.gender == 'Female']['pretest']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test for normality in the male and female pretest scores\n\n# H0: The scores are normally distributed\n# H1: The scores are not normally distributed\n\nprint('Male_pretest:',sp.stats.shapiro(male_pretest))\nprint('Female_pretest:',sp.stats.shapiro(fem_pretest))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interpretation:** Both males and female pretest scores grossly failed the normality test. They are not normally distributed. This is evidenced by their p-values being <0.05, implying statistical significance. As a result the null hypothesis rejected in favour of the alternate hypothesis.","metadata":{}},{"cell_type":"code","source":"# Test for homegeneity of variance of pretest scores between males and females\n# H0: The scores have equal variances\n# H1: The scores do not have equal variances\nsp.stats.levene(male_pretest, fem_pretest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Implication:** The null hypothesis is rejected because p-value < 0.05 (statistically significant). Hence, the H1 is rather accepted.","metadata":{}},{"cell_type":"markdown","source":"With the data failing the normality and homogeneity tests, Independent Sample T test is not appropriate to us. However, we'll use it anyway for the sake of practice","metadata":{}},{"cell_type":"code","source":"# Independent Sample T-test between male and female on pre-test scores\n# H0: There is no difference between the mean pret-test scores for both gender\n# H1: There is a difference between the mean pre-test scores for both gender\n\nsp.stats.ttest_ind(male_pretest, fem_pretest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Implication:** With p-value of the statistic >0.05 (0.7801)[not statistically significant], the null hypothesis is upheld.\n\n      Thus there is no difference between the mean pre-test scores for males and females","metadata":{}},{"cell_type":"code","source":"# Using the ManWhotney U Test is more appropriate because of the absence of normality and homogeneity among both gender\nsp.stats.mannwhitneyu(male_pretest, fem_pretest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Similar to the independent sample t-test performed above, the null hypothesis is upheld because the p-value > 0.05 (0.399). Thus not statistically significant.**","metadata":{}},{"cell_type":"markdown","source":"**Conducting Independent Sample T test on gender for post-test scores**","metadata":{}},{"cell_type":"code","source":"male_posttest = test_data['posttest'][test_data.gender == 'Male']\nfemale_posttest = test_data['posttest'][test_data.gender == 'Female']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test of normality in both gender\nprint(sp.stats.shapiro(male_posttest))\nprint(sp.stats.shapiro(female_posttest))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">The posttest scores for both gender are not normally distributed. The null hypothesis is rejected for the alternate. Because their p-values are <0.05 (statistically significant).","metadata":{}},{"cell_type":"code","source":"# Test of homogeneity of variance among the 2 genders\nsp.stats.levene(male_posttest, female_posttest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The null hypothesis, that the post test scores for both gender have the same variance, is upheld. This is because p-value >0.05(0.19)","metadata":{}},{"cell_type":"code","source":"# Performing the MannWhitneyU Test\nsp.stats.mannwhitneyu(male_posttest, female_posttest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The null hypothesis is upheld [p-value = 0.3559] (p-value > 0.05)\nThis implies that the means of the postest scores for males and females are identical or the same.\nOr, there is no difference between the means of the postest scores for males and females.","metadata":{}},{"cell_type":"markdown","source":"## A machine learning regression model to predict test scores","metadata":{}},{"cell_type":"code","source":"# Visualise the data again\ntest_data.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into X and y\nX = test_data.drop('posttest', axis = 1)\ny = test_data['posttest']\n\n# Convert categorical values to numbers\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategories = ['school', 'school_setting', 'school_type', 'teaching_method', 'gender', 'lunch']\n\none_hot = OneHotEncoder()\ntransformer = ColumnTransformer([('one_hot', one_hot, categories)],\n                                remainder = 'passthrough')\n\nX_transformed = transformer.fit_transform(X)\n\n# Split the transformed data to training and test sets\nnp.random.seed(42)\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size = 0.2)\n\n# Import the Random Forest regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\n# Score the model\nmodel.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# y_preds = model.predict(X_test)\n\ncv_score = cross_val_score(model, X_transformed, y)\nnp.mean(cv_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross-validated mean-absolute-error\ncv_mae = cross_val_score(model, X_transformed, y, scoring = 'neg_mean_absolute_error')\ncv_mae.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross-validated mean-squared-error\ncv_mse = cross_val_score(model, X_transformed, y, scoring = 'neg_mean_squared_error')\ncv_mse.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\ndef evaluation_metrics(y_true, y_preds):\n    '''\n    A function to compute and return the evaluation metrics in the form of a dictionary.\n    '''\n    r2 = r2_score(y_true, y_preds)\n    mae = mean_absolute_error(y_true, y_preds)\n    mse = mean_squared_error(y_true, y_preds)\n    \n    metrics = {'r2_score' : r2,\n               'mean absolute error' : round(mae, 2),\n               'mean_squared error' : round(mse, 2)}\n    \n    print(f'R2_score: {r2 * 100:.2f}%')\n    print(f'MAE: {mae:.2f}')\n    print(f'MSE: {mse:.2f}')\n    \n    return metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into X and y\nX = test_data.drop('posttest', axis = 1)\ny = test_data['posttest']\n\n# Convert categorical values to numbers\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategories = ['school', 'school_setting', 'school_type', 'teaching_method', 'gender', 'lunch']\n\none_hot = OneHotEncoder()\ntransformer = ColumnTransformer([('one_hot', one_hot, categories)],\n                                remainder = 'passthrough')\nX_transformed = transformer.fit_transform(X)\n\n# Split the transformed data to training, validation and test sets\nnp.random.seed(94)\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val_test, y_train, y_val_test = train_test_split(X_transformed, y, test_size = 0.3)\n# Split the X_val_test and y_val_test equally into validation and test samples\nX_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size = 0.5)\n\n# Import the Random Forest regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\n# Make predictions on the validation set\ny_preds = model.predict(X_val)\n# Score the model\nbaseline = evaluation_metrics(y_val, y_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tune with RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Split the data into X and y\nX = test_data.drop('posttest', axis = 1)\ny = test_data['posttest']\n\n# Convert categorical values to numbers\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategories = ['school', 'school_setting', 'school_type', 'teaching_method', 'gender', 'lunch']\n\none_hot = OneHotEncoder()\ntransformer = ColumnTransformer([('one_hot', one_hot, categories)],\n                                remainder = 'passthrough')\nX_transformed = transformer.fit_transform(X)\n\n# Split the transformed data to training, validation and test sets\nnp.random.seed(94)\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val_test, y_train, y_val_test = train_test_split(X_transformed, y, test_size = 0.3)\nX_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size = 0.5)\n\n# Import the Random Forest regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_jobs = -1)\n\ngrid = {'n_estimators' : [10, 100, 200, 500, 1000, 1200],\n        'max_depth' : [None, 5, 10, 20, 30],\n        'max_features' : ['auto', 'sqrt'],\n        'min_samples_split' : [2, 4, 6 ],\n        'min_samples_leaf' : [1, 2, 4]}\n\nrs_model = RandomizedSearchCV(estimator = model, \n                              param_distributions = grid,\n                              n_iter = 10,\n                              cv = 5,\n                              verbose = 2)\nrs_model.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the best paramters\nrs_model.best_params_","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make predictions with the rs_model on the validation set\nrs_y_preds = rs_model.predict(X_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the RandomizedSearchCV\nrs_metrics = evaluation_metrics(y_val, rs_y_preds)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the model on the test sets\nrs_test_preds = rs_model.predict(X_test)\n\n# Evaluate the model\nrs_test_metrics = evaluation_metrics(y_test, rs_test_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}