{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","version":"3.6.3","nbconvert_exporter":"python","name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3"}},"cells":[{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"9f4d6d6cc9ef4dc96b3db691f5f4914df5f75ec6","_cell_guid":"b6e8d42d-1b09-41d7-bc48-eec2cabf2e49"},"source":"## 1.0 Call libraries\n\nimport time                   # To time processes\nimport warnings               # To suppress warnings\nimport plotly.graph_objs as go\nimport os                     # For os related operations\nimport sys                    # For data size\nimport numpy as np            # Data manipulation\nimport pandas as pd           # Dataframe manipulatio \nimport matplotlib.pyplot as plt                   # For graphics\n\n\nfrom sklearn import cluster, mixture              # For clustering\nfrom sklearn.preprocessing import StandardScaler  # For scaling dataset                  \nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n%matplotlib inline\n","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"_uuid":"0033957d76fb24439f6d6cc46423c55af2601ba0","collapsed":true,"_cell_guid":"a153371e-b7dd-4709-b79f-b8ccae291ec5"},"source":"# 2. Read data\nX= pd.read_csv(\"../input/2017.csv\", header=0)\nCountry_df=pd.read_csv(\"../input/2017.csv\", header=0)\n#X= pd.read_csv(\"iris.csv\", header = 0)\nCountry_df=Country_df['Country' ]","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{},"source":"# 3. Explore and scale\nX.columns.values\nX.shape                 # 155 X 12\nX = X.iloc[:, 2: ]      # Ignore Country and Happiness_Rank columns\nX.head(2)\nX.dtypes\nX.info","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{},"source":"\n# 3.1 Normalize dataset for easier parameter selection\n# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n#    Standardize features by removing the mean\n#      and scaling to unit variance\n# 3.1.2 Instantiate scaler object\nss = StandardScaler()\n# 3.1.3 Use ot now to 'fit' &  'transform'\nss.fit_transform(X)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"source":"\n#### 5. Begin Clustering   \n                                  \n# 5.1 How many clusters\n#     NOT all algorithms require this parameter\nn_clusters = 2 ","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"source":"##  KMeans\n# Ref: http://scikit-learn.org/stable/modules/clustering.html#k-means                                  \n# KMeans algorithm clusters data by trying to separate samples in n groups\n#  of equal variance, minimizing a criterion known as the within-cluster\n#   sum-of-squares.  \n\n\n##  Mean Shift\n# http://scikit-learn.org/stable/modules/clustering.html#mean-shift\n# This clustering aims to discover blobs in a smooth density of samples.\n#   It is a centroid based algorithm, which works by updating candidates\n#    for centroids to be the mean of the points within a given region.\n#     These candidates are then filtered in a post-processing stage to\n#      eliminate near-duplicates to form the final set of centroids.\n# Parameter: bandwidth dictates size of the region to search through. \n\n##  Mini Batch K-Means\n#  Ref: \n#     http://scikit-learn.org/stable/modules/clustering.html#mini-batch-k-means\n#  Similar to kmeans but clustering is done in batches to reduce computation time\n\n\n##  Spectral clustering\n# http://scikit-learn.org/stable/modules/clustering.html#spectral-clustering    \n# SpectralClustering does a low-dimension embedding of the affinity matrix\n#  between samples, followed by a KMeans in the low dimensional space. It\n#   is especially efficient if the affinity matrix is sparse.\n#   SpectralClustering requires the number of clusters to be specified.\n#     It works well for a small number of clusters but is not advised when \n#      using many clusters.\n\n## DBSCAN\n# http://scikit-learn.org/stable/modules/clustering.html#dbscan\n#   The DBSCAN algorithm views clusters as areas of high density separated\n#    by areas of low density. Due to this rather generic view, clusters found\n#     by DBSCAN can be any shape, as opposed to k-means which assumes that\n#      clusters are convex shaped.    \n#    Parameter eps decides the incremental search area within which density\n#     should be same\n\n# Affinity Propagation\n# Ref: http://scikit-learn.org/stable/modules/clustering.html#affinity-propagation    \n# Creates clusters by sending messages between pairs of samples until convergence.\n#  A dataset is then described using a small number of exemplars, which are\n#   identified as those most representative of other samples. The messages sent\n#    between pairs represent the suitability for one sample to be the exemplar\n#     of the other, which is updated in response to the values from other pairs. \n#       Two important parameters are the preference, which controls how many\n#       exemplars are used, and the damping factor which damps the responsibility\n#        and availability messages to avoid numerical oscillations when updating\n#         these messages.\n\n##  Birch\n# http://scikit-learn.org/stable/modules/clustering.html#birch    \n# The Birch builds a tree called the Characteristic Feature Tree (CFT) for the\n#   given data and clustering is performed as per the nodes of the tree\n\n# Gaussian Mixture modeling\n#  http://203.122.28.230/moodle/course/view.php?id=6&sectionid=11#section-3\n#  It treats each dense region as if produced by a gaussian process and then\n#  goes about to find the parameters of the process","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"source":"# Define different Clustering Techniques\n\ndef Kmeans():\n    # 5.1 Instantiate object\n    km = cluster.KMeans(n_clusters =n_clusters )\n    # 5.2.1 Fit the object to perform clustering\n    km_result = km.fit_predict(X)\n    return km_result\n\ndef MeanShift():\n    bandwidth = 0.1 \n    # 6.2 No of clusters are NOT predecided\n    ms = cluster.MeanShift(bandwidth=bandwidth)\n    # 6.3\n    ms_result = ms.fit_predict(X)\n    return ms_result\n\ndef MiniBatch():\n    two_means = cluster.MiniBatchKMeans(n_clusters=n_clusters)\n    two_means_result = two_means.fit_predict(X)\n    return two_means_result\n\ndef Spectral():\n    spectral = cluster.SpectralClustering(n_clusters=n_clusters)\n    sp_result= spectral.fit_predict(X)\n    return sp_result\n\ndef DBScan():\n    eps = 0.3\n    dbscan = cluster.DBSCAN(eps=eps)\n    db_result= dbscan.fit_predict(X)\n    return db_result\n\ndef Affinity():\n    damping = 0.9\n    preference = -200\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=damping, preference=preference)\n    affinity_propagation.fit(X)\n    ap_result = affinity_propagation .predict(X)\n    return ap_result\n\ndef Birch():\n    birch = cluster.Birch(n_clusters=n_clusters)\n    birch_result = birch.fit_predict(X)\n    return birch_result\n\ndef Gaussian():\n    gmm = mixture.GaussianMixture( n_components=n_clusters, covariance_type='full')\n    gmm.fit(X)\n    gmm_result = gmm.predict(X)\n    return gmm_result\n    \n","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{},"source":"# Create dataframe for result\nResult_DataFrame = pd.DataFrame(columns=['Country','KMeans','MeanShift','Minibatch','Spectral','DBSCAN','Affinity','Birch','Gaussian'])\nResult_DataFrame.loc[:,\"Country\"]=Country_df\n","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true},"source":"# Switcher similar to switch case statement\nswitcher = {\n        0: Kmeans,\n        1: MeanShift,\n        2: MiniBatch,\n        3: Spectral,\n        4: DBScan,\n        5: Affinity,\n        6: Birch,\n        7: Gaussian\n    }\n","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{},"source":"Cluster_Array=[0,1,2,3,4,5,6,7]\nCluster_list=['KMeans','MeanShift','Minibatch','Spectral','DBSCAN','Affinity','Birch','Gaussian']\ndef Cluster_tech(argument):\n    # Get the function from switcher dictionary\n    func = switcher.get(argument, \"nothing\")\n    # Execute the function\n    return func()\n\nfor i in Cluster_Array:\n    Result_DataFrame.iloc[:,i+1]=Cluster_tech(i)\n    plt.subplot(4, 2, i+1)\n    plt.title(Cluster_list[i])\n    plt.scatter(X.iloc[:, 4], X.iloc[:, 5],  c=Cluster_tech(i))\n    \nplt.subplots_adjust(bottom=-0.5, top=2.0)\nResult_DataFrame\n","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{},"source":"km = cluster.KMeans(n_clusters =n_clusters )\nkm_result = km.fit_predict(X)\ndata = dict(type = 'choropleth', \n           locations = Country_df,\n           locationmode = 'country names',\n           z = km_result, \n           text = Country_df,\n           colorbar = {'title':'Cluster Group'})\nlayout = dict(title = 'K-Means Clustering', \n             geo = dict(showframe = False, \n                       projection = {'type': 'Mercator'}))\nchoromap1 = go.Figure(data = [data], layout=layout)\niplot(choromap1)","outputs":[]}],"nbformat_minor":1,"nbformat":4}