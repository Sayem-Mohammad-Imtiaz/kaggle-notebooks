{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Multivariate Linear Regression ","metadata":{}},{"cell_type":"markdown","source":"Hi, I'm a total noob and have attempted to fit the life expctancy data on a linear regression model without using sklearn. My attempts at predicting for x_train is confusing(bottom) , therefore if anyone passing by knows a bit about how to train the x_test to my model please do shine light.:p thank you in advance. (my prediction for y just changes in range with changing theta and I dont quite understand how to get more accurate predictions.did my due diligence but hitting a wall.","metadata":{}},{"cell_type":"markdown","source":"### Data load and EDA","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint('Modules are imported.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/life-expectancy-who/Life Expectancy Data.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#make country column as the Index of the df\ndf.set_index(\"Country\",inplace= True)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Year'].unique","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#drop columns \nuseless_col = ['percentage expenditure','Hepatitis B','Status',\n       'Measles ', ' BMI ', 'under-five deaths ', 'Polio', 'Total expenditure',\n       'Diphtheria ', ' HIV/AIDS', 'Population',\n       ' thinness  1-19 years', ' thinness 5-9 years',\n       'Income composition of resources']\ndf.drop(useless_col,axis=1,inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find all the null values in df\ndf.isnull().sum() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Alcohol.value_counts() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#replace alcohol null values with the most recurrent number\ndf.Alcohol.fillna(0.01,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#replace GDP null values with the median\nb=df.GDP.median()\ndf.GDP.fillna(b,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the rest drop the null values\ndf.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation scores\ndf.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation depicted in heatmap\nsns.heatmap(df.corr())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#a scatterplot overview of the correlations between features\nsns.pairplot(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear Regression \n\n\nLinear regression predicts the dependent value (y) according to the independent variable (x). The output here is the dependent value, and the input is the independent value.\n\n$\n\\begin{equation}\ny = b + w_1.x_1 + w_2.x_2\n\\end{equation}\n$\n\nIn a vector form:\n\n$\n\\begin{equation}\ny = b + (w_1 w_2).\\binom{x_1}{x_2}\n\\end{equation}\n$\n\nWhere\n$\n\\begin{equation}\nW = (w_1 w_2)\n\\end{equation}\n$\nand\n$\n\\begin{equation}\nX = \\binom{x_1}{x_2}\n\\end{equation}\n$","metadata":{}},{"cell_type":"markdown","source":"### Cost Function\n\n\nThe loss over **m** examples:\nMean Squared Error:\n\n\n$\n\\begin{equation}\nJ = \\frac{1}{2m} \\sum_{i=1}^{m} (y - \\hat{y})^2\n\\end{equation}\n$\n\nThe objective of the gradient descent algorithm is to minimize this error  value.\n\nGradient Descent Objective is to \n$\n\\begin{equation}\nmin(J)\n\\end{equation}\n$\n\n\n\n\n\n### The gradient descent algorithm:\n\n\ndo it for all the theta values(no of theta values = no of features + 1).\n\n![Loss vs Param](GD.png)\n\n\n","metadata":{}},{"cell_type":"code","source":"X = df.values[:, 2:7]  #  input values from last 5 columns\ny = df.values[:, 1]  # get output values 2nd coulmn(life_exp)\nm = len(y) # Number of training examples\n\nprint('Total no of training examples (m) = %s \\n' %(m))\n\n# Show only first 5 records\nfor i in range(5):\n    print('x =', X[i, ], ', y =', y[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Then we Normalize the features because they all are not in the same range of measurement:\n\nFeature Scaling: In feature scaling we divide the input value by range(max - min) of input variable. By this technique we get new range of just 1.\n\nMean Normalization: In mean normalization we subtract the average value from the input variable and then divide it by range(max - min) or by standard deviation of input variable.","metadata":{}},{"cell_type":"code","source":"def feature_normalize(X):\n  \"\"\"\n    Normalizes the features(input variables) in X.\n\n    Parameters\n    ----------\n    X : n dimensional array (matrix), shape (n_samples, n_features)\n        Features(input varibale) to be normalized.\n\n    Returns\n    -------\n    X_norm : n dimensional array (matrix), shape (n_samples, n_features)\n        A normalized version of X.\n    mu : n dimensional array (matrix), shape (n_features,)\n        The mean value.\n    sigma : n dimensional array (matrix), shape (n_features,)\n        The standard deviation.\n  \"\"\"\n  # mean of column, hence axis = 0\n  mu = np.mean(X, axis = 0)  \n  # Notice the parameter ddof (Delta Degrees of Freedom)  value is 1\n  sigma = np.std(X, axis= 0, ddof = 1)  # Standard deviation (can also use range)\n  X_norm = (X - mu)/sigma\n  return X_norm, mu, sigma","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, mu, sigma = feature_normalize(X)\n\nprint('mu= ', mu)\nprint('sigma= ', sigma)\nprint('X_norm= ', X[:5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#New mean or avearage value of normalized X feature is 0\nmu_testing = np.mean(X, axis = 0) # mean\nmu_testing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#New range or standard deviation of normalized X feature is 1\nsigma_testing = np.std(X, axis = 0, ddof = 1) # mean\nsigma_testing","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets use hstack() function from numpy to add column of ones to X feature \n# This will be our final X matrix (feature matrix)\nX = np.hstack((np.ones((m,1)), X))\nX[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX, x_test, y, y_test = train_test_split(X,y,test_size =0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_test[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_cost(X, y, theta):\n  \"\"\"\n  Compute the cost of a particular choice of theta for linear regression.\n\n  Input Parameters\n  ----------------\n  X : 2D array where each row represent the training example and each column represent the feature ndarray. Dimension(m x n)\n      m= number of training examples\n      n= number of features (including X_0 column of ones)\n  y : 1D array of labels/target value for each traing example. dimension(1 x m)\n\n  theta : 1D array of fitting parameters or weights. Dimension (1 x n)\n\n  Output Parameters\n  -----------------\n  J : Scalar value.\n  \"\"\"\n  predictions = X.dot(theta)\n  #print('predictions= ', predictions[:5])\n  errors = np.subtract(predictions, y)\n  #print('errors= ', errors[:5]) \n  sqrErrors = np.square(errors)\n  #print('sqrErrors= ', sqrErrors[:5]) \n  #J = 1 / (2 * m) * np.sum(sqrErrors)\n  # OR\n  # We can merge 'square' and 'sum' into one by taking the transpose of matrix 'errors' and taking dot product with itself\n  # If your confuse about this try to do this with few values for better understanding  \n  J = 1/(2 * m) * errors.T.dot(errors)\n\n  return J","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradient_descent(X, y, theta, alpha, iterations):\n  \"\"\"\n  Compute cost for linear regression.\n\n  Input Parameters\n  ----------------\n  X : 2D array where each row represent the training example and each column represent the feature ndarray. Dimension(m x n)\n      m= number of training examples\n      n= number of features (including X_0 column of ones)\n  y : 1D array of labels/target value for each traing example. dimension(m x 1)\n  theta : 1D array of fitting parameters or weights. Dimension (1 x n)\n  alpha : Learning rate. Scalar value\n  iterations: No of iterations. Scalar value. \n\n  Output Parameters\n  -----------------\n  theta : Final Value. 1D array of fitting parameters or weights. Dimension (1 x n)\n  cost_history: Conatins value of cost for each iteration. 1D array. Dimansion(m x 1)\n  \"\"\"\n  cost_history = np.zeros(iterations)\n\n  for i in range(iterations):\n    predictions = X.dot(theta)\n    #print('predictions= ', predictions[:5])\n    errors = np.subtract(predictions, y)\n    #print('errors= ', errors[:5])\n    sum_delta = (alpha / m) * X.transpose().dot(errors);\n    #print('sum_delta= ', sum_delta[:5])\n    theta = theta - sum_delta;\n\n    cost_history[i] = compute_cost(X, y, theta)  \n\n  return theta, cost_history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need theta parameter for every input variable. since we have 6 input variable including X_0 (column of ones)\ntheta = np.zeros(6)\niterations = 400;\nalpha = 0.15;","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"theta, cost_history = gradient_descent(X, y, theta, alpha, iterations)\nprint('Final value of theta =', theta)\nprint('First 5 values from cost_history =', cost_history[:5])\nprint('Last 5 values from cost_history =', cost_history[-5 :])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(range(1, iterations +1), cost_history, color ='blue')\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.grid()\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"cost (J)\")\nplt.title(\"Convergence of gradient descent\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Effect Of Learning Rate On Convergence","metadata":{}},{"cell_type":"code","source":"iterations = 400;\ntheta = np.zeros(6)\n\nalpha = 0.005;\ntheta_1, cost_history_1 = gradient_descent(X, y, theta, alpha, iterations)\n\nalpha = 0.01;\ntheta_2, cost_history_2 = gradient_descent(X, y, theta, alpha, iterations)\n\nalpha = 0.02;\ntheta_3, cost_history_3 = gradient_descent(X, y, theta, alpha, iterations)\n\nalpha = 0.03;\ntheta_4, cost_history_4 = gradient_descent(X, y, theta, alpha, iterations)\n\nalpha = 0.15;\ntheta_5, cost_history_5 = gradient_descent(X, y, theta, alpha, iterations)\n\nplt.plot(range(1, iterations +1), cost_history_1, color ='black', label = 'alpha = 0.005')\nplt.plot(range(1, iterations +1), cost_history_2, color ='blue', label = 'alpha = 0.01')\nplt.plot(range(1, iterations +1), cost_history_3, color ='green', label = 'alpha = 0.02')\nplt.plot(range(1, iterations +1), cost_history_4, color ='brown', label = 'alpha = 0.03')\nplt.plot(range(1, iterations +1), cost_history_5, color ='blue', label = 'alpha = 0.15')\n\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.grid()\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"cost (J)\")\nplt.title(\"Effect of Learning Rate On Convergence of Gradient Descent\")\nplt.legend()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iterations = 100;\ntheta = np.zeros(6)\n\nalpha = 1.32;\ntheta_6, cost_history_6 = gradient_descent(X, y, theta, alpha, iterations)\n\nplt.plot(range(1, iterations +1), cost_history_6, color ='brown')\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.grid()\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"cost (J)\")\nplt.title(\"Effect of Large Learning Rate On Convergence of Gradient Descent\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfor i in range(50):\n    hu = (np.array(x_test[i]))\n    ki=hu[1:6]\n    normalize_test_data = ((ki - mu) / sigma)\n    normalize_test_data = np.hstack((np.ones(1), normalize_test_data))\n    y_pred = normalize_test_data.dot(theta_3)\n    print('Predicted age : {},real y value: {}'.format(y_pred, y_test[i]) )\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor i in range(50):\n    hu = (np.array(x_test[i]))\n    ki=hu[1:6]\n    normalize_test_data = ((ki - mu) / sigma)\n    normalize_test_data = np.hstack((np.ones(1), normalize_test_data))\n    y_pred = normalize_test_data.dot(theta_1)\n    print('Predicted age : {},real y value: {}'.format(y_pred, y_test[i]) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfor i in range(50):\n    hu = (np.array(x_test[i]))\n    ki=hu[1:6]\n    normalize_test_data = ((ki - mu) / sigma)\n    normalize_test_data = np.hstack((np.ones(1), normalize_test_data))\n    y_pred = normalize_test_data.dot(theta_2)\n    print('Predicted age : {},real y value: {}'.format(y_pred, y_test[i]) )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}