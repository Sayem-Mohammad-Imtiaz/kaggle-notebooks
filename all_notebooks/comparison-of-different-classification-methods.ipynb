{"cells":[{"metadata":{},"cell_type":"markdown","source":"**EXPLANATION**\n\nIn this kernel, I compare 6 different classification methods in machine learning.\n\n**CONTENTS**\n\n**1. Data Cleaning and Regulation**\n   \n   I am looking data in general and change number from text for example, Yes-->1 and No-->0. In addtion to this I drop columns which is consist of texts.\n\n**2. Normalization**\n\n**3. Train Test Splite**\n    \nI separate data to 2 different part which is train and test. They are dataset.\n\n**4. Classification Methods **\n\nLogistic Regression, Decision Tree, Random Forest, Navie Byes, SVM, KNN\n\n**5. Visualization**\n\n**6. Conclusion**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Data Cleaning and Regulation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/bank-full.csv\",sep=\",\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we delete columns whose data type is text\ndata1= data.drop([\"job\",\"education\",\"contact\",\"month\",\"poutcome\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1[\"default\"] = [0 if each== \"no\" else 1 for each in data1.default]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1[\"loan\"] = [0 if each== \"no\" else 1 for each in data1.loan]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1[\"y\"] = [0 if each== \"no\" else 1 for each in data1.y]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1[\"marital\"] = [1 if each == \"married\" else 0 if each == \"single\" else 0.5 for each in data1.marital]\ndata1[\"housing\"] = [1 if each == \"yes\" else 0  for each in data1.housing]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Normalization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining to y and x_data values for train data\ny = data1.y.values\nx_data = data1.drop([\"y\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalization\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Train Test Splite**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test =  train_test_split(x,y,test_size=0.15,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Classifications Methods**\n1. Logistic Regression\n2. Decision Tree\n3. Random Forest\n4. Naive Byes\n5. SVM Model\n6. KNN Model\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we define 2 list that one of them save results of models other list save name of model\nlabelList = []\nresultList = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logictic Regression with sklearn\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"test accuracy {}\".format(lr.score(x_test,y_test)))\n\n# adding result and label to lists\nlabelList.append(\"Log_Rec\")\nresultList.append(lr.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nprint(\"decison tree score : \",dt.score(x_test,y_test))\n\n# adding result and label to lists\nlabelList.append(\"Dec_Tree\")\nresultList.append(dt.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100,random_state = 1)\nrf.fit(x_train, y_train)\nprint(\"Random forest algor. result: \",rf.score(x_test,y_test))\n\n# adding result and label to lists\nlabelList.append(\"Rand_For\")\nresultList.append(rf.score(x_test,y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Naive Byes \nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"print accuracy of naive bayes algo: \",nb.score(x_test,y_test))\n\n# adding result and label to lists\nlabelList.append(\"Naive_Byes\")\nresultList.append(nb.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM model\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=3)\nsvm.fit(x_train,y_train)\nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))\n\n# adding result and label to lists\nlabelList.append(\"SVM\")\nresultList.append(svm.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KNN model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3) #n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n\n# score\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding optimum k value between 1 and 15\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each) # create a new knn model\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n\nplt.plot(range(1,15),score_list) # x axis is in interval of 1 and 15\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding max value in a list and it's index.\na = max(score_list) # finding max value in list\nb = score_list.index(a)+1 # index of max value.\n\nprint(\"k = \",b,\" and maximum value is \", a)\n\n# adding result and label to lists\nlabelList.append(\"KNN\")\nresultList.append(a)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5. Visualization**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(labelList,resultList)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above chart give information to us about results of classification algorithms but it is not sorted and clear chart. We can improve this graph to read it easily."},{"metadata":{"trusted":true},"cell_type":"code","source":"# First of all we combine 2 lists (labelList and resultList) by using zip method\nzipped = zip(labelList, resultList)\nzipped = list(zipped)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame(zipped, columns=['label','result'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_index = (df['result'].sort_values(ascending=False)).index.values \nsorted_data = df.reindex(new_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(sorted_data.loc[:,\"label\"],sorted_data.loc[:,\"result\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6. Conclusion**\n\nAccording to out results **Random Forest** has the biggest result value and others respectively KNN and Logistic Regression."},{"metadata":{},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}