{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-30T22:32:53.064024Z","iopub.execute_input":"2021-06-30T22:32:53.064395Z","iopub.status.idle":"2021-06-30T22:32:53.082147Z","shell.execute_reply.started":"2021-06-30T22:32:53.064319Z","shell.execute_reply":"2021-06-30T22:32:53.081036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Dataset","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:32:53.083919Z","iopub.execute_input":"2021-06-30T22:32:53.084414Z","iopub.status.idle":"2021-06-30T22:32:53.132042Z","shell.execute_reply.started":"2021-06-30T22:32:53.08438Z","shell.execute_reply":"2021-06-30T22:32:53.131227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:32:53.134691Z","iopub.execute_input":"2021-06-30T22:32:53.134965Z","iopub.status.idle":"2021-06-30T22:32:53.143486Z","shell.execute_reply.started":"2021-06-30T22:32:53.134942Z","shell.execute_reply":"2021-06-30T22:32:53.142535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:32:53.146103Z","iopub.execute_input":"2021-06-30T22:32:53.146334Z","iopub.status.idle":"2021-06-30T22:32:54.495211Z","shell.execute_reply.started":"2021-06-30T22:32:53.146312Z","shell.execute_reply":"2021-06-30T22:32:54.494455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning the Text","metadata":{}},{"cell_type":"code","source":"stemmer=PorterStemmer()\nlemmatizer=WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:32:54.49726Z","iopub.execute_input":"2021-06-30T22:32:54.497515Z","iopub.status.idle":"2021-06-30T22:32:54.50582Z","shell.execute_reply.started":"2021-06-30T22:32:54.49749Z","shell.execute_reply":"2021-06-30T22:32:54.505026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus=[]\nfor i in range(len(df)): \n    #Replacing all values other than alphabets in the df with a space\n    review=re.sub(\"[^a-zA-Z]\",\" \",df[\"Message\"][i])\n    #Converting the text into lowercase \n    review=review.lower()\n    #Converting review into a list \n    review=review.split()\n    #Lemmatizing the word in the review other than stopwords\n    review=[lemmatizer.lemmatize(word) for word in review if not word in set(stopwords.words(\"english\"))]\n    #Joining the list values to get review back\n    review=\" \".join(review)\n    #Appending in corpus list\n    corpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:32:54.507502Z","iopub.execute_input":"2021-06-30T22:32:54.508119Z","iopub.status.idle":"2021-06-30T22:33:06.674501Z","shell.execute_reply.started":"2021-06-30T22:32:54.508081Z","shell.execute_reply":"2021-06-30T22:33:06.67375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:33:06.676153Z","iopub.execute_input":"2021-06-30T22:33:06.676846Z","iopub.status.idle":"2021-06-30T22:33:06.704106Z","shell.execute_reply.started":"2021-06-30T22:33:06.676772Z","shell.execute_reply":"2021-06-30T22:33:06.703362Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the Bag of Words Model","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer()\nX=cv.fit_transform(corpus).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:40:48.990962Z","iopub.execute_input":"2021-06-30T22:40:48.99143Z","iopub.status.idle":"2021-06-30T22:40:49.213322Z","shell.execute_reply.started":"2021-06-30T22:40:48.991386Z","shell.execute_reply":"2021-06-30T22:40:49.212159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:40:50.71914Z","iopub.execute_input":"2021-06-30T22:40:50.719459Z","iopub.status.idle":"2021-06-30T22:40:50.727059Z","shell.execute_reply.started":"2021-06-30T22:40:50.71943Z","shell.execute_reply":"2021-06-30T22:40:50.726058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding the Dependent Variable","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ndf[\"Category\"]=le.fit_transform(df[\"Category\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:40:51.848686Z","iopub.execute_input":"2021-06-30T22:40:51.849033Z","iopub.status.idle":"2021-06-30T22:40:51.853839Z","shell.execute_reply.started":"2021-06-30T22:40:51.849004Z","shell.execute_reply":"2021-06-30T22:40:51.852699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:40:52.799295Z","iopub.execute_input":"2021-06-30T22:40:52.799605Z","iopub.status.idle":"2021-06-30T22:40:52.80777Z","shell.execute_reply.started":"2021-06-30T22:40:52.799576Z","shell.execute_reply":"2021-06-30T22:40:52.806967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=df[\"Category\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:40:53.869069Z","iopub.execute_input":"2021-06-30T22:40:53.86938Z","iopub.status.idle":"2021-06-30T22:40:53.873319Z","shell.execute_reply.started":"2021-06-30T22:40:53.869353Z","shell.execute_reply":"2021-06-30T22:40:53.872242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the Dataset into Train and Test set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:40:55.048565Z","iopub.execute_input":"2021-06-30T22:40:55.048908Z","iopub.status.idle":"2021-06-30T22:40:55.192438Z","shell.execute_reply.started":"2021-06-30T22:40:55.048878Z","shell.execute_reply":"2021-06-30T22:40:55.191419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the Model using Naive Bayes Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nspam_detect_model = MultinomialNB().fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:40:56.369218Z","iopub.execute_input":"2021-06-30T22:40:56.369525Z","iopub.status.idle":"2021-06-30T22:40:56.64888Z","shell.execute_reply.started":"2021-06-30T22:40:56.369497Z","shell.execute_reply":"2021-06-30T22:40:56.647799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting the values\ny_pred=spam_detect_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:41:22.910576Z","iopub.execute_input":"2021-06-30T22:41:22.911081Z","iopub.status.idle":"2021-06-30T22:41:22.949674Z","shell.execute_reply.started":"2021-06-30T22:41:22.911045Z","shell.execute_reply":"2021-06-30T22:41:22.948562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking Confusion Marix and Accuracy score","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score\ncm=confusion_matrix(y_test,y_pred)\ncm","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:43:10.785193Z","iopub.execute_input":"2021-06-30T22:43:10.785504Z","iopub.status.idle":"2021-06-30T22:43:10.79537Z","shell.execute_reply.started":"2021-06-30T22:43:10.785476Z","shell.execute_reply":"2021-06-30T22:43:10.794205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T22:43:51.445346Z","iopub.execute_input":"2021-06-30T22:43:51.445662Z","iopub.status.idle":"2021-06-30T22:43:51.452579Z","shell.execute_reply.started":"2021-06-30T22:43:51.445634Z","shell.execute_reply":"2021-06-30T22:43:51.451767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We got an accuracy of 97%, which is pretty impressive","metadata":{}}]}