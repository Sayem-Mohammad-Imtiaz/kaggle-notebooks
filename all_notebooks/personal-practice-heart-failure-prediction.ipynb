{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n \n Hello! Recently I started to study Machine Learning. I always found it quite interesting, but only now I actually started to study the methods, models, and the general ideia. It was quite interesting to see that it is basically linear regressions and matrix multiplication hahaha\n Anyway, I've bee following both [Andrew Ng's Machine Learning course](https://www.coursera.org/learn/machine-learning) and this [Udemy Machine Learning course](https://www.udemy.com/course/machinelearning/), made by different people. But are quite great, and even if I didn't finished them yet, I decided to start to practice what they teach.\n \n This notebook isn't the best to read and learn new things, but feedback is quite welcome!"},{"metadata":{},"cell_type":"markdown","source":"# Importing basic libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # Linear algebra\nimport pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os # Will help us to open the dataset\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt # Plotting\nimport seaborn as sns\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Import dataset\ndata = pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the dataset size\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test #1: Let's use all the features"},{"metadata":{},"cell_type":"markdown","source":"This is evidently a classification problem, where we want to find the risk of death by heart failure. Also, all the columns are great candidates for our features... at first, let's use them all, with Support Vector Classification. This will quite likely lead to overfitting, but let's see what we end up with anyway."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting the dataset between the features and the predicted variable, and also a train and a test set\nX=data[list(data.columns.drop([\"DEATH_EVENT\"]))]\ny=data[\"DEATH_EVENT\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n#Training our first model\nfrom sklearn.svm import LinearSVC\nsvcModel = LinearSVC()\nsvcModel.fit(X_train, y_train)\n#Creating some predictions, so that we can check our accuracy\ny_predict=svcModel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_train, svcModel.predict(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, y_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For now, we can see that this model did quite bad in both the training sample and the testing sample, so we must have something with high bias right now (that is, underfitted).\n\nThis makes sense, because of the *ConvergenceWarning* we got while fitting."},{"metadata":{},"cell_type":"markdown","source":"## Reducing the number of features"},{"metadata":{},"cell_type":"markdown","source":"We might be able to do something better by choosing fewer features. Good ways to find the best features are by viewing the correlations between the variables, and also by using an ExtraTreeClassifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Heatmap\nplt.figure(figsize=(10,10))\nsns.heatmap(data.corr(), vmin=-1, cmap='coolwarm', annot=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Extratrees\nfrom sklearn.ensemble import ExtraTreesClassifier\nplt.rcParams['figure.figsize']=16,9\nsns.set_style(\"darkgrid\")\n\nXExtraTrees = data[list(data.columns.drop([\"DEATH_EVENT\"]))]\nyExtraTrees = data[\"DEATH_EVENT\"]\n\ntreeModel = ExtraTreesClassifier()\ntreeModel.fit(XExtraTrees,yExtraTrees)\nfeat_importances = pd.Series(treeModel.feature_importances_, index=XExtraTrees.columns)\nfeat_importances.nlargest(12).plot(kind='barh')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" As we can see, time, ejection_fraction, and serum_creatinine are our best bet towards a better model. So, let's do everything the same, but with less features.\n \n The info that is [here](https://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15) might be interesting as well. I should use it in the future."},{"metadata":{},"cell_type":"markdown","source":"## Test #2: Let's use some features only"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=data[['time','ejection_fraction','serum_creatinine']]\ny=data[\"DEATH_EVENT\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\nsvcModelRed = LinearSVC()\nsvcModelRed.fit(X_train, y_train)\ny_predict=svcModelRed.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_train, svcModelRed.predict(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Strangely, this model did only slightly better than the other one.\n#It also looks that we still are underfitting.\na=accuracy_score(y_test,svcModelRed.predict(X_test))\nb=accuracy_score(y_train,svcModelRed.predict(X_train))\nabs(a-b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the difference between both is quite small (3%), we actually have a high bias problem, that is, we are underfitting our data. Now, we could tinker more with which features to use, or we can change our model. I'm going with the later, since sklearn accused LinearSVC of not converging, and also, Sklearn gives us other model to use in case of failure.\n\nLet's try nearest neighbors then!"},{"metadata":{},"cell_type":"markdown","source":"## Test #3: Testing another model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier()\nneigh.fit(X_train, y_train)\ny_predict=neigh.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_train,neigh.predict(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=accuracy_score(y_test,neigh.predict(X_test))\nb=accuracy_score(y_train,neigh.predict(X_train))\nabs(a-b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model did quite better now! The accuracy on both the training and testing set seem great, so we might be nicely fitting our model now. Maybe we could again try to use every feature possible and see what happens, or even try another model?"},{"metadata":{},"cell_type":"markdown","source":"## Test #4: Testing yet another model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nbayesModel = GaussianNB()\nbayesModel.fit(X_train, y_train)\ny_predict=bayesModel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_train,bayesModel.predict(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=accuracy_score(y_test,neigh.predict(X_test))\nb=accuracy_score(y_train,neigh.predict(X_train))\nabs(a-b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test #5: Let's use all the features again"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=data[list(data.columns.drop([\"DEATH_EVENT\"]))]\ny=data[\"DEATH_EVENT\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\nneigh = KNeighborsClassifier()\nneigh.fit(X_train, y_train)\ny_predict=neigh.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test, y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_train,neigh.predict(X_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test,neigh.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=accuracy_score(y_test,neigh.predict(X_test))\nb=accuracy_score(y_train,neigh.predict(X_train))\nabs(a-b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizations"},{"metadata":{},"cell_type":"markdown","source":"Some visualizations might be nice. After all, some features might be better described by polynomials... I'm not sure if anything interesting will come up, but doing some plots should be nice practice.\n\nAlso, in the future I should put them before fitting the models... oh well."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data,hue='DEATH_EVENT')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Last comments\n\n  I think it is about time I wrapped up this notebook, and go work in something else. In the end:\n \n - KNN did the best, followed by Naive Bayes;\n - Using less features was better than using all of them;\n - The features chosen as the most important kind of indicate different behavior on the diagonal plots.\n \n  So, nothing new I think haha. I should implement a cross validation test next time I think, and also make things more organized."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}