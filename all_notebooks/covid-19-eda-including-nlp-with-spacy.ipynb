{"cells":[{"metadata":{},"cell_type":"markdown","source":"# COVID-19 EDA\nMy exploration so far through the CORD-19 research challenge data set.  \n  \n  \nThis is my first kernel on the site and I am relatively new in this world (coming from software engineering). For that reason, any feedback of any kind is welcome.\n  \nIf you are curious, the repository for this EDA is on my github at: https://github.com/jbofill10/COVID-19-EDA  \n  \nThanks for taking the time to check my notebook out!  "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\n\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"file_paths = list()\nfor path, dirs, files in os.walk('/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv'):\n    for file in files:\n            file_paths.append(os.path.join(path, file))\nfor path, dirs, files in os.walk('/kaggle/input/CORD-19-research-challenge/comm_use_subset/comm_use_subset'):\n        for file in files:\n            file_paths.append(os.path.join(path, file))\nfor path, dirs, files in os.walk('/kaggle/input/CORD-19-research-challenge/custom_license/custom_license'):\n        for file in files:\n            file_paths.append(os.path.join(path, file))\nfor path, dirs, files in os.walk('/kaggle/input/CORD-19-research-challenge/noncomm_use_subset/noncomm_use_subset'):\n        for file in files:\n            file_paths.append(os.path.join(path, file))\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\nfor path in tqdm(file_paths):    \n    full_abstract = ''\n    full_body = ''\n    with open(path.encode('utf-8'), 'r') as file:\n        data = json.load(file)\n        paper_id=data['paper_id']\n        title = data['metadata']['title']\n        for x in data['abstract']:\n            abstract = x['text']\n            full_abstract += abstract\n\n        for x in data['body_text']:\n            body = x['text']\n            full_body += body\n\n    temp = pd.DataFrame([[paper_id, title, full_abstract, full_body]], columns=['paper_id', 'title', 'abstract', 'body'])\n    df = df.append(temp)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What do we know about Incubation?\nFrom the data I extrapolated, I arrived at an average of 12 days for how long COVID-19 should take to incubate. In hindsight, that is right around with what is common knowledge now (7-14 days). \n\n### What I did:\n* I searched each sentence of each body text of the articles for the word incubation \n* Then with regex searched for digits followed by the string \"day\" or \"days\". \n* I stripped all non-numerical characters in order to preserve pharses such as 3-7. There were still some serious outliers in the 600 range, but I ended up trimming the graph before the outliers begun.  "},{"metadata":{},"cell_type":"markdown","source":"Import re to find strings that follow this pattern: \"(random words) (numbers) day/days\". Not the best way, but provides some accuracy at least."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"style.use('seaborn-poster')\nstyle.use('ggplot')\n# I'm thinking of looking for strings with incubation to start\nincubation_occurrences = list()\npaper_titles = set()\n\nbody_text = df['body'].values\nindex = 0\nfor text in tqdm(body_text):\n    for sent in text.split('. '):\n        if 'incubation' in sent:\n            temp = re.findall('[1-9]{1,2}\\S*\\sday|s$', sent)\n            if len(temp) > 0:\n                for find in temp:\n                    find = re.sub('\\D', ' ', find)\n                    house = find.strip().split(' ')\n                    for nums in house:\n                        try:\n                            incubation_occurrences.append(int(nums))\n                            paper_titles.add((df.loc[index]['paper_id'].strip()))\n                        except:\n                            continue\n    index+=1\n    \nincubation_occurrences\nincubation_df = pd.DataFrame({'days': incubation_occurrences}, index=range(0, len(incubation_occurrences)))\nincubation_df = incubation_df.sort_index()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.hist(incubation_df['days'].value_counts(), bins=75, color='purple')\nplt.xlabel(\"Incubation Time in Days\", fontsize=20, color='black')\nplt.ylabel('Frequency Found in Literature', fontsize=20, color='black')\nplt.title('Distribution of Incubation Periods', fontsize=23, color='black')\nplt.xlim(1, 62)\nplt.rcParams['axes.grid'] = True\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n"},{"metadata":{},"cell_type":"markdown","source":"# What do We Know about Transmission?\n\n## What I did\n* I used Spacy to search for papers with had words with the lemmatization of \"transmit\" and \"transmission\"\n    * I could've improved this search, but it was for the sake of my PC\n    * Any ideas for improvement of the search would be much appreciated here\n* I stored those papers into a dataframe to do some visualization with them along with for retrieving any papers on COVID-19 relating to transmission"},{"metadata":{},"cell_type":"markdown","source":"Import spacy related libraries"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import spacy\nfrom spacy.matcher import Matcher","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"'''\nCode for building the dataframe of papers containing words with the lemmatization of transmit and transmission\n\nAs much as I'd like it to be live, I also don't want you waiting 45+ minutes for my notebook to load :)\n\ntransmit_keywords = ['transmit', 'transmission']\n        findings = dict()\n        paper_id_tracker = set()\n        temp_papers = list()\n\n        transmission_papers = pd.DataFrame(columns=['paper_id', 'title', 'abstract', 'body'])\n        body_text = df['body'].values\n\n        nlp = spacy.load(\"en_core_web_sm\", disable=['ner', 'parser'])\n        nlp.add_pipe(nlp.create_pipe('sentencizer'))\n        matcher = Matcher(nlp.vocab)\n\n        for i in transmit_keywords:\n            matcher.add(i, build_transmission_pattern(i))\n\n        index = 0\n        for i in tqdm(body_text):\n            \n            spacy.prefer_gpu()\n            \n            for docs in nlp.pipe(texts, disable=['parser', 'ner', 'entity_linker']):\n                \n                for match_id, start, end in matcher(docs):\n                    if df.loc[index]['paper_id'] not in paper_id_tracker:\n                        \n                        paper_id_tracker.add(df.loc[index]['paper_id'])    \n                        temp_papers.append([df.loc[index]['paper_id'], df.loc[index]['title'], df.loc[index]['abstract'], df.loc[index]['body']])\n                    \n                    if str(docs[start:end]) in findings:\n                    \n                        findings[str(docs[start:end]).lower()] += 1\n                    else:\n                    \n                        findings[str(docs[start:end]).lower()] = 1\n            index += 1\n\n        for i in temp_papers:\n            transmission_papers = pd.concat([transmission_papers, pd.DataFrame([[i[0], i[1], i[2], i[3]]],\n                                                                               columns=['paper_id', 'title', 'abstract',\n                                                                                        'body'])])\n                                                                                        \ndef build_transmission_pattern(keyword):\n    return [[{'LEMMA': keyword.lower()}]]\n'''\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How did the Spacy pipeline Perform?\n![image](https://github.com/jbofill10/COVID-19-EDA/blob/master/Charts/TransmissionFreq.png?raw=true)  \n  \nThis graph shows the different variations of the words \"transmit\" and \"transmission\" in the papers and the count. I thought this was cool.\n\n## Taking it a step further\nI took the dataframe that contained papers relating transmission and then took a subset of those papers that contained common words used to describe the process of transmission.  \nThese words were: 'Contact, Aerosol, Surface, and Breathing  \n![image](https://github.com/jbofill10/COVID-19-EDA/blob/master/Charts/TransmissionMediums.png?raw=true)  \n  \nEven though I know that a lot of the matches found in these papers don't match the context of actual transmission, but I still think it is somewhat accurate."},{"metadata":{},"cell_type":"markdown","source":"# What do we know about COVID-19 itself?\n*This section is still a work in progress*\n\nI searched for common flu-like symptoms and did some basic string matching  \nI chose not to go with spacy for this because it would just take too long :(  \n\nI still think I got some pretty neat results in the end.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_symptoms = ['fever', 'chills', 'cough', 'sore throat',\n                           'runny nose', 'headache', 'fatigue', 'vomiting',\n                           'shortness of breath', 'dizziness']\ntext = df['body'].values\n\nsymptom_freq = list()\n\nfor x in tqdm(text):\n    for sent in x.split('. '):\n        for symp in common_symptoms:\n            if symp in sent:\n                symptom_freq.append(symp)\nsymptoms_df = pd.DataFrame(symptom_freq, columns=['symptom'])\nsymp_vals = symptoms_df['symptom'].value_counts().sort_values()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(21, 10))\nsns.set(style=\"darkgrid\")\n    \nsns.barplot(x=symp_vals.values, y=list(range(0, len(symp_vals.index))), orient='h', palette='Spectral')\nplt.yticks(list(range(0, len(symp_vals.index))), [i.capitalize() for i in symp_vals.index], fontsize=15)\n    \nplt.xlabel('Times Mentioned in Literature', fontsize=20)\nplt.xticks(fontsize=15)\nplt.ylabel('Symptoms', fontsize=20)\n    \nplt.title('Common Flu Symptoms found in COVID-19 Literature', fontsize=22)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think most of the data aligns well with what we know about COVID-19... except I was surprised that I found a lot of vomitting related matches in comparison to something like sore throat.  \n  \nI was thinking maybe if I added better searches for cough/throat related symptoms that it would improve a bit."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}