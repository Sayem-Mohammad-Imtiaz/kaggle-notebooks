{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Index  \n[Goal of the notebook](#Goal-of-the-notebook)  \n[Data Visulaization](#Data-Visulaization)  \n[Prepare data for ML Classifier](#Prepare-data-for-ML-Classifier)  \n[ML Pipeline](#ML-Pipeline)  \n[Evaluation](#Evaluation)  \n[Conclusion](#Conclusion)  \n\nWARNING: due to the nature of the data there will be some swear words in the Data Visulaization section if you do not wish to view such words please skip the mentioned section.  \n\n[GitHub](https://github.com/FancyWhale69/toxic_tweets_classifier)  \n[DashBoard](https://toxic-tweets.herokuapp.com/)","metadata":{}},{"cell_type":"markdown","source":"# Goal of the notebook  \n\nThe goal of this notebook is to understand some characteristics of toxic tweets and to build a ML pipeline to classify tweets into toxic or non toxic","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport string\nfrom nltk.stem import SnowballStemmer\nimport re\nimport plotly.graph_objects as go\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import classification_report, confusion_matrix\nstemmer= SnowballStemmer('english')","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:35:35.011775Z","iopub.execute_input":"2021-06-15T19:35:35.012509Z","iopub.status.idle":"2021-06-15T19:35:36.793314Z","shell.execute_reply.started":"2021-06-15T19:35:35.012469Z","shell.execute_reply":"2021-06-15T19:35:36.792181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= pd.read_csv('../input/toxic-tweets-dataset/FinalBalancedDataset.csv')\ndf.drop('Unnamed: 0', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:35:39.885432Z","iopub.execute_input":"2021-06-15T19:35:39.885907Z","iopub.status.idle":"2021-06-15T19:35:40.151474Z","shell.execute_reply.started":"2021-06-15T19:35:39.88585Z","shell.execute_reply":"2021-06-15T19:35:40.150255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f= open('../input/stop-words/stop_words_english.txt', 'r', encoding='utf-8')\nstop_wordsV2= f.readlines()\nf.close()\n\nfor i in range(len(stop_wordsV2)):\n    stop_wordsV2[i]= stop_wordsV2[i].replace('\\n', '')\n    \nfor i in range(len(stop_wordsV2)):\n    if \"'\" in stop_wordsV2[i]:\n        stop_wordsV2.append(stop_wordsV2[i].replace(\"'\", ''))\n\nstop_wordsV2.append('i')","metadata":{"execution":{"iopub.status.busy":"2021-06-15T19:37:16.683713Z","iopub.execute_input":"2021-06-15T19:37:16.684382Z","iopub.status.idle":"2021-06-15T19:37:16.698805Z","shell.execute_reply.started":"2021-06-15T19:37:16.684325Z","shell.execute_reply":"2021-06-15T19:37:16.697636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Visulaization","metadata":{}},{"cell_type":"code","source":"data=df.groupby('Toxicity').count()['tweet']\nfig = go.Figure([go.Bar(x=data.index.get_level_values(0), y=data.values, text=data.values)])\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide', title='Dist. of data classes',\n                 xaxis_title='Classes (0=Non-Toxic, 1=Toxic)', yaxis_title='Count')\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwords= df[df['Toxicity']==1]['tweet'].apply(lambda x : [w for w in re.sub(' +', \" \", re.sub(\"@[0-9a-zA-Z]+|#|https?://[0-9a-zA-Z\\./\\-_\\?]+|â¦|(amp)|[0-9]+\", \"\", x)).translate(str.maketrans('', '', string.punctuation)).strip().split() if not w.lower() in stop_wordsV2])\nword={'words':[]}\nfor group in words:\n    for d in group:\n        word['words'].append(stemmer.stem(d))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.DataFrame(word).value_counts()[:10]\nfig = go.Figure([go.Bar(x=data.index.get_level_values(0), y=data.values, text=data.values)])\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide', title='Top 10 words in toxic tweets',\n                 xaxis_title='Words', yaxis_title='Count')\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwords= df[df['Toxicity']==0]['tweet'].apply(lambda x : [w for w in re.sub(' +', \" \", re.sub(\"@[0-9a-zA-Z]+|#|https?://[0-9a-zA-Z\\./\\-_\\?]+|â¦|(amp)|[0-9]+\", \"\", x)).translate(str.maketrans('', '', string.punctuation)).strip().split() if not w.lower() in stop_wordsV2])\nword={'words':[]}\nfor group in words:\n    for d in group:\n        word['words'].append(stemmer.stem(d))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.DataFrame(word).value_counts()[:10]\nfig = go.Figure([go.Bar(x=data.index.get_level_values(0), y=data.values, text=data.values)])\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide', title='Top 10 words in non-toxic tweets',\n                 xaxis_title='Words', yaxis_title='Count')\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words= df[df['Toxicity']==1]['tweet'].apply(lambda x : re.findall('#[a-zA-Z_0-9]+', x))\nword={'words':[]}\nfor group in words:\n    for d in group:\n        if len(re.findall('#[0-9]+',d)) == 0:#filter hashtags which are all numbers (e.g. #198473)\n            word['words'].append(d)\ndata=pd.DataFrame(word).value_counts()[:10]\nfig = go.Figure([go.Bar(x=data.index.get_level_values(0), y=data.values, text=data.values)])\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide', title='Top 10 toxic hashtags',\n                 xaxis_title='Hashtag', yaxis_title='Number of apperence')\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words= df[df['Toxicity']==0]['tweet'].apply(lambda x : re.findall('#[a-zA-Z_0-9]+', x))\nword={'words':[]}\nfor group in words:\n    for d in group:\n        if len(re.findall('#[0-9]+',d)) == 0:#filter hashtags which are all numbers (e.g. #198473)\n            word['words'].append(d)\ndata=pd.DataFrame(word).value_counts()[:10]\nfig = go.Figure([go.Bar(x=data.index.get_level_values(0), y=data.values, text=data.values)])\nfig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide', title='Top 10 non-toxic hashtags',\n                 xaxis_title='Hashtag', yaxis_title='Number of apperence')\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data for ML Classifier","metadata":{}},{"cell_type":"code","source":"#Clean text of meaningless words\ndef clean_words(x):\n    '''\n    Function to remove stop words, Hashtags, numbers, etc...\n    \n    Input- String\n    Output- String cleaned of meaningless words\n    '''\n    a= [w for w in re.sub(' +', \" \", re.sub(r\"#[0-9]+|@[0-9a-zA-Z]+|#|https?://[0-9a-zA-Z\\./\\-_\\?]+|â¦|(amp)|[^\\x20-\\x7e]|â|¥|ð|»|¼|ï|¸|¦|±|¯|[0-9]+\", \"\", x)).translate(str.maketrans('', '', string.punctuation)).strip().split() if not w.lower() in stop_wordsV2]\n    return \" \".join([stemmer.stem(i) for i in a])\n    \ndf['Cleaned_tweets']= df['tweet'].apply(lambda x : clean_words(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split data\nfrom sklearn.model_selection import train_test_split\nx=df['Cleaned_tweets']\ny=df['Toxicity']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ML Pipeline","metadata":{}},{"cell_type":"code","source":"pipeline= Pipeline([\n    ('count', CountVectorizer()),\n    ('tf', TfidfTransformer()),\n    ('clf', LinearSVC())\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipeline.fit(x2_train, y2_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"pred= pipeline.predict(x2_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y2_test, pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y2_test, pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"From tha data visulazation phase it can observed that toxic tweets contains a lot of swear words and usally found in ploitcal hastags. on the other hand non-toxic tweets contains positive words and mostly found in positive hashtags.\n\nML model achived an accuracy of 94%, while the LSTM network i built achived from 91% to 93% while being slow in the training phase unlike the ML model.","metadata":{}}]}