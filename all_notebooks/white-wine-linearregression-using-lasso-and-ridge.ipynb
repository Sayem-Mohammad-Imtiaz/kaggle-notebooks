{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing\n#### 1 - Reading Input data for White Wine","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/white-wine-quality/winequality-white.csv\", sep=\";\")\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Our white wine dataset has: {0} rows and {1} columns\".format(df.shape[0], df.shape[1]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2 - Identify NULL Values (if any)","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3 - There are no columns which are unique identifiers. Hence, no column will be removed","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4 - Measure skeweness in our dataset","metadata":{}},{"cell_type":"code","source":"colors = ['#78C850', '#F08030', '#6890F0','#F8D030', '#F85888', '#705898', '#98D8D8']\n\ndf.skew()\n\n\nl = df.columns.values\nnumber_of_columns=df.shape[1]/6\nnumber_of_rows = len(l)-1/number_of_columns\nplt.figure(figsize=(10*number_of_columns,5*number_of_rows))\nfor i in range(0,12):\n    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n    sns.set_style('whitegrid')\n    sns.boxplot(df[l[i]],color=colors[np.random.randint(6)], orient='v')\n    plt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5 - Understanding correlation between variables\n#### <u>Observations</u>:\n##### 1) Acidity in wine decreases, as we increase the pH value\n##### 2) Wine density/thickness increases rapidly as the content of residual sugar increases\n##### 3) Alcohol content is inversely proportional to residual sugar. Hence, if sugar content is increased, alcohol qualtiy automatically decreases\n##### 4) Wine quality increases, if Alcohol content increases\n\nHence, the right amount of acids, residual sugar and alcohol, is what will improve our wine quality","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.title(\"Correlation between Variables\")\nsns.heatmap(df.corr(), linewidths=0.5, cmap=\"coolwarm\", annot=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Understanding correlation between alcohol and wine quality","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(df.quality, df.alcohol)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6 - Model Development","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n####################################\n# Step 4: Data Sampling\n\n# Take all columns in X except our target variable\ndf_x = df.iloc[:, [0,1,2,3,4,5,6,7,8,9,10]]\n\n# Take target variable in Y\ndf_y = df.iloc[:, 11]\n\n####################################\n# Step 5: Train-Test split\n\n# Split 20% of data in test data and rest 80% in train. i.e. test_size = .2\ndf_x_train, df_x_test, df_y_train, df_y_test = train_test_split(df_x, df_y, test_size = .2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_x.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_y.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear Regression, since this is a regression problem\n#### We will identify MSE value, and use the model, whichever gives MSE value closer to 0. That will help us ensure that prediction error rate of our model is least","metadata":{}},{"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.model_selection import cross_val_score\n\n# Create an object using sklearn's LinearRegression() method\nmodel = linear_model.LinearRegression()\n\n# Use Cross-Validation technique for 5-folds to identify best MSE rate\nmse = cross_val_score(model, df_x, df_y, scoring=\"neg_mean_squared_error\", cv=5)\nmean_mse = np.mean(mse)\nprint(mean_mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This MSE Rate is very near to 0. However, let us identify if we can use LASSO/RIDGE to reduce it furthermore","metadata":{}},{"cell_type":"markdown","source":"#### 1 - Ridge","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\nridge = Ridge()\nparameters={\"alpha\": [-1, 0, 0.5, 1]}\nridge_regressor = GridSearchCV(ridge, parameters, scoring=\"neg_mean_squared_error\", cv=5)\nridge_regressor.fit(df_x, df_y)\n\nprint(ridge_regressor.best_params_)\nprint(ridge_regressor.best_score_)  # This is MSE for ridge","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2- Lasso","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n\nlasso = Lasso()\nparameters={\"alpha\": [-1, 0, 0.5, 1]}\nlasso_regressor = GridSearchCV(lasso, parameters, scoring=\"neg_mean_squared_error\", cv=5)\nlasso_regressor.fit(df_x, df_y)\n\nprint(lasso_regressor.best_params_)\nprint(lasso_regressor.best_score_)  # This is MSE for ridge","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MSE rate is least observed using Ridge Regression. So we will use Ridge in our Linear Regression model","metadata":{}},{"cell_type":"code","source":"ridge_regressor.fit(df_x_train, df_y_train)\n\n# Predict our test data\npred_test = ridge_regressor.predict(df_x_test)\n\n# Predict our train data\npred_train = ridge_regressor.predict(df_x_train)\n\n\n####################################\n# Step 5: Accuracy and Evaluation Metrics\n\n# Evaluation Metrics\nRsquare = ridge_regressor.score(df_x_train, df_y_train)\nprint(\"RSquare: \" + str(-Rsquare))\n\n\nK = df_x_train.shape[1]  # Total no. of Columns in Train data\nN = df_x_train.shape[0]  # Total no. of Rows/Values/Observations\n\nAdj_Rsquare = 1 - (1 - (-Rsquare)) * (N - 1)/ (N - K - 1)\nprint(\"Adjusted RSquare: \" + str(Adj_Rsquare))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7 - Estimate prediction error rate for our choosen Model\n#### <i>It must be between -3 to +3</i>","metadata":{}},{"cell_type":"code","source":"# Prediction error = Actual - Predicted values\n# Find error in train data prediction\nerror_pred = df_y_train - pred_train\n\n# Plot but with a line and see if it falls within range (-3, 3)\nsns.distplot(error_pred)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot model prediction accuracy\nplt.figure(figsize=(8,8))\nplt.plot(error_pred, \"*\")\n\nplt.axhline(y = np.mean(error_pred), color=\"r\")\n\n# Mark 3 and -3 limits in diff colors\nplt.axhline(y = 20, color = \"g\")\nplt.axhline(y = -20, color = \"orange\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot Actual vs Predicted values to observe the difference","metadata":{}},{"cell_type":"code","source":"temp = df_x_test\n\ntemp[\"actual\"] = df_y_test\ntemp[\"prediction\"] = pred_test\n\ntemp.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}