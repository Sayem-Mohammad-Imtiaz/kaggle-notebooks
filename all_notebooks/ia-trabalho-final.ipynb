{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Trabalho Final de Estágio Docente\n## Estagiário: Douglas Macedo Sgrott\n## Aluno: \n## Data de entrega: 21/06/2021 (segunda-feira)\n## O trabalho está organizado em partes:\n - ### **Dataset: Onde você irá limpar e pre processar o dataset. Atribua a versão final do dataset em um dataframe chamado df.**\n - Separação dos dados: Aqui os dados são normalizados e divididos em Treino/Validação. Não precisa modificar o código.\n - ### **Arquitetura da Rede Neural: Onde você vai definir a arquitetura da rede neural.**\n - ### **Parâmetros de otimização da Rede Neural: Onde você vai definir outros parâmetros da rede neural.**\n - Visualização dos resultados: Onde os resultados são obtidos\n - Exemplos: Servir como exemplo de análise, data cleaning e pré-processamento.\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# importing stuff\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# a lot of stuff\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Z-score / outliers stuff\nfrom scipy import stats\n\n# Rede Neural stuff\nfrom tensorflow.keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.utils import plot_model\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-16T20:46:38.613178Z","iopub.execute_input":"2021-06-16T20:46:38.613797Z","iopub.status.idle":"2021-06-16T20:46:45.270553Z","shell.execute_reply.started":"2021-06-16T20:46:38.613661Z","shell.execute_reply":"2021-06-16T20:46:45.269474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset\n#### Coloque aqui seu data cleaning e seu pre-processamento e atribua o dataset para um dataframe chamado **df**","metadata":{}},{"cell_type":"code","source":"\ndf = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\n\n# Processa df_original 1\n# Processa df_original 2\n# Processa df_original 3\n# Processa df_original ...\n# Processa df_original n\n\n# Versão bem simples de dataset para servir como exemplo e para conseguir rodar o notebook inteiro\ndf.drop(columns='city', inplace=True)\ndf.drop(columns='floor', inplace=True)\ndf.drop(columns='animal', inplace=True)\ndf.drop(columns='furniture', inplace=True)\ndf.dropna(inplace=True)\n\n# df = aqui vem o dataset processado, SEM ESCALONAMENTO/NORMALIZADOR\n\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:45.27172Z","iopub.execute_input":"2021-06-16T20:46:45.272025Z","iopub.status.idle":"2021-06-16T20:46:45.413622Z","shell.execute_reply.started":"2021-06-16T20:46:45.271997Z","shell.execute_reply":"2021-06-16T20:46:45.412542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Separação dos dados","metadata":{}},{"cell_type":"code","source":"# Normalizamos os dados de df em uma escala de [0, 1]\n# Estou fazendo isto aqui pois temos que \"desnormalizar\" na hora de gerar os gráficos de R²\ncolumn_names = df.columns\nscaler = MinMaxScaler()\nscaler.fit(df)\ndf = scaler.transform(df)\ndf = pd.DataFrame(df)\ndf.columns = column_names\n\n# Pegamos o dataset df e separamos em x (entrada) e y (saida), numa separação 70% treino e 30% validação\ninput_dim = df.shape[1] - 1\nx = df.drop(columns='total (R$)')\ny = df['total (R$)']\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.30, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:45.415416Z","iopub.execute_input":"2021-06-16T20:46:45.415728Z","iopub.status.idle":"2021-06-16T20:46:45.431158Z","shell.execute_reply.started":"2021-06-16T20:46:45.415699Z","shell.execute_reply":"2021-06-16T20:46:45.429935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Arquitetura da Rede Neural\n#### Criei um código bem simples pra permitir criar diferentes redes neurais modificando apenas algumas variáveis (EM CAPSLOCK),\n#### Mas se quiser criar sua própria arquitetura mais customizada, fique a vontade","metadata":{}},{"cell_type":"code","source":"NEURONIOS_CAMADA_INICIAL = 10\n\n# Número de camadas intermediárias e neurônios. Tamanho do array são os números de camadas, elementos do array são números de neurônios.\n# Ex: [30, 15] = 2 camadas intermediárias com 30 neurônios na primeira e 15 neurônios na segunda\n# Ex: [] = Nenhuma camada intermediária\n# Ex: [10, 10, 10, 10, 50] = 5 camadas intermediárias, com 10 neurônios nas 4 primeiras e 50 neurônios na última\nNEURONIOS_CAMADAS_INTERMEDIARIAS = [20, 30]\n\n# Usar dropout: True para usar, False para não usar\nUSAR_DROPOUT = False\n\n# Porcentagem de Dropout: valor entre 0 e 1\nDROPOUT_VALUE = 0.2\n\n# Regularizador: None = Não usar regularizador, 'l1' = Reg L1, 'l2' = Reg L2\nTIPO_REGULARIZADOR = None\n\n# Função de ativação: 'relu', 'tanh', 'sigmoid', 'softmax', 'softplus', 'elu'\nFN_ATIVACAO = 'relu'\n\n# #####################################################################################\n# Definição da ARQUITETURA da Rede Neural\nmodel = Sequential()\n\n# Primeira camada da RNA (input_dim entradas)\nmodel.add(Dense(units=NEURONIOS_CAMADA_INICIAL, input_dim=input_dim, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n# Camadas intermediárias\nfor UNITS in NEURONIOS_CAMADAS_INTERMEDIARIAS:\n    model.add(Dense(units=UNITS, activation=FN_ATIVACAO, kernel_regularizer=TIPO_REGULARIZADOR))\n    if USAR_DROPOUT:\n        model.add(Dropout(DROPOUT_VALUE, input_shape=(120,)))\n# Última camada da RNA (1 saída)\nmodel.add(Dense(units=1, activation=FN_ATIVACAO))\n\n\n# \"Doug, mas que código tosco!\" Também acho... Caso você queira criar sua própria arquitetura\n# sem usar os parâmetros acima, é bem simples. Segue abaixo um exemplo meio doideira:\n# model = Sequential()\n# model.add(Dense(units=30, input_dim=input_dim, activation='relu', kernel_regularizer='l1'))\n# model.add(Dropout(0.4, input_shape=(30,)))\n# model.add(Dense(units=20, activation='tanh', kernel_regularizer='l2'))\n# model.add(Dense(units=20, activation='relu', kernel_regularizer=None))\n# model.add(Dense(units=1, activation='relu'))\n\nplot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:45.434799Z","iopub.execute_input":"2021-06-16T20:46:45.435262Z","iopub.status.idle":"2021-06-16T20:46:45.974906Z","shell.execute_reply.started":"2021-06-16T20:46:45.435231Z","shell.execute_reply":"2021-06-16T20:46:45.973712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parâmetros de otimização da Rede Neural\n#### Pode alterar os valores das variáveis que estão EM CAPSLOCK","metadata":{}},{"cell_type":"code","source":"CALLBACKS = [] # Definição dos callbacks a serem utilizados. Isso aqui é opcional, mas pode ajudar: https://keras.io/api/callbacks/early_stopping/\nLOSS = 'mean_absolute_error' # 'mean_absolute_error', 'mean_squared_error'\nBATCH_SIZE = 128\nEPOCHS = 50\nOPTIMIZER = 'adam' # 'adam' é o mais utilizado. Caso prefira outro, como 'sgd', boa sorte!\n\n# Compilação do modelo + Definição da Função de Loss e do Otimizador\nmodel.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=LOSS)\n\nhistory = model.fit(\n    x=x_train,\n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=CALLBACKS,\n    validation_data=(x_valid, y_valid),\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:45.97644Z","iopub.execute_input":"2021-06-16T20:46:45.976745Z","iopub.status.idle":"2021-06-16T20:46:52.190809Z","shell.execute_reply.started":"2021-06-16T20:46:45.976714Z","shell.execute_reply":"2021-06-16T20:46:52.189566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualização dos resultados","metadata":{}},{"cell_type":"code","source":"\ndf_valid_scaled = np.column_stack((x_valid, y_valid))\ndf_valid = scaler.inverse_transform(df_valid_scaled)\ny_true = y_valid\n\n\ny_pred = model.predict(x_valid)\npred_df = pd.concat([pd.DataFrame(x_valid).reset_index(drop=True), pd.DataFrame(y_pred)], axis=1)\npred_df = scaler.inverse_transform(pred_df)\npred_df = pd.DataFrame(pred_df)\npred_df.columns = df.columns\npred_df\n\nr2 = r2_score(y_true, y_pred)\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\naxes[0].plot(history.history['loss'])\naxes[0].plot(history.history['val_loss'])\naxes[0].set_title('model loss | \"Quantidade de dados e colunas usadas: {}'.format(df.shape))\naxes[0].set_ylabel('loss')\naxes[0].set_xlabel('epoch')\naxes[0].legend(['train', 'val'], loc='upper left')\n\naxes[1].scatter(x=df_valid[:, -1], y=pred_df['total (R$)'])\n# axes[0].plot(history.history['val_loss'])\naxes[1].set_title('R² = {}'.format(r2))\naxes[1].set_ylabel('y_pred')\naxes[1].set_xlabel('y_true')\n\nprint(\"Quantidade de dados e colunas usadas: {}\".format(df.shape))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:52.192856Z","iopub.execute_input":"2021-06-16T20:46:52.193282Z","iopub.status.idle":"2021-06-16T20:46:52.760249Z","shell.execute_reply.started":"2021-06-16T20:46:52.193237Z","shell.execute_reply":"2021-06-16T20:46:52.75931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lembrando:\n\n## **Data de entrega:**\n#### - 21/06/2021 (segunda-feira)\n\n## **O que será avaliado de forma objetiva:**\n#### - Se o modelo está ajustado (sem overfit/underfit). Ou seja, se as curvas de treino (azul) e validação (laranja) estão suficientemente próximos.\n#### - A quantidade de dados (linhas de df) e features (colunas de df) foram utilizados.\n#### - O valor de R²\n\n## **O que será avaliado de forma subjetiva:**\n#### - Esforço. Ou seja, quando mais explorar as combinações de pre-processamento + parâmetros de rede, melhor. Para isso, crie novas versões do notebook.\n\n## Observações\n#### - O valor de R será analisado juntamente com o número de features e dados usados. Ou seja, um R alto obtido usando poucos dados e features não vale muita coisa. Mas é melhor que nada ;)\n#### - Se deixou de utilizar colunas do df = menos pontos. Se fez engenharia de features = mais pontos (pouco). Isto serve para incentivar a utilizar os dados categóricos.\n\n## Dicas:\n#### - Quando alcançar um resultado que queira salvar, clique em Save Version -> Quick Save. Isto fará com que o seus resultados atuais sejam mostrados na versão html. Se clicar em Save Version -> Save & Run All (commit), o notebook será rodado novamente, e isso pode modificar os resultados obtidos.\n#### - No começo, KISS (Keep it simple, silly). Comece de forma simples. Comece com menos features e com pouco pre processamento. Recomendo começar usando variáveis numéricas e filtro de outliers. Isso já deve ser suficiente para obter um R alto. MAS ISSO NÃO É SUFICIENTE! Depois que conseguir um modelo com alto R usando um modelo e pre processamento simples, salve a versão do notebook e continue explorando! A meta é conseguir utilizar TODAS as features e o maior número de dados possível. O objetivo é explorar a combinação de pré-processamento de dados + parâmetros da rede neural.\n#### - Eu vou analisar as diferentes versões do notebook (se houver). Ou seja, se na primeira versão obteve um modelo bom usando poucas features, ok... Se na segunda versão obteve um modelo um pouco pior ou melhor usando mais features, melhor ainda. Explore as combinações! You're a wizard Harry!","metadata":{}},{"cell_type":"markdown","source":"# FIM DO TRABALHO /\\\n# **INICIO DOS EXEMPLOS V**\n","metadata":{}},{"cell_type":"markdown","source":"# Dataset Original","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:52.761412Z","iopub.execute_input":"2021-06-16T20:46:52.761668Z","iopub.status.idle":"2021-06-16T20:46:52.825939Z","shell.execute_reply.started":"2021-06-16T20:46:52.761641Z","shell.execute_reply":"2021-06-16T20:46:52.824897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Análise de correlação entre as variáveis\n#### Pode ser interessante na hora de imputação","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\n\nsns.heatmap(df.corr())","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:52.828881Z","iopub.execute_input":"2021-06-16T20:46:52.829158Z","iopub.status.idle":"2021-06-16T20:46:53.232676Z","shell.execute_reply.started":"2021-06-16T20:46:52.829132Z","shell.execute_reply":"2021-06-16T20:46:53.231619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Análise de caractéres especiais","metadata":{}},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:53.234942Z","iopub.execute_input":"2021-06-16T20:46:53.235366Z","iopub.status.idle":"2021-06-16T20:46:53.243022Z","shell.execute_reply.started":"2021-06-16T20:46:53.235321Z","shell.execute_reply":"2021-06-16T20:46:53.242118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### É esperado que 'city', 'animal' e 'furniture'sejam considerados como tipo 'object', pois nestas colunas existem valores do tipo string. No entanto, 'floor' deveria ser considerado float64. Por que 'floor' está sendo considerado como object?","metadata":{}},{"cell_type":"code","source":"df['floor'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:53.244404Z","iopub.execute_input":"2021-06-16T20:46:53.244682Z","iopub.status.idle":"2021-06-16T20:46:53.261973Z","shell.execute_reply.started":"2021-06-16T20:46:53.244656Z","shell.execute_reply":"2021-06-16T20:46:53.260839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Olhando os valores presentes em 'floor', percebemos que existe o caractér '-' presente. Além disso, não existem valores iguais a 0. Temos algumas possibilidades aqui:\n - Considerar '-' como 0 e fazer esta substituição (onde '-' significaria andar 0 ou térreo)\n - Deletar todas as linhas com floor igual a '-'","metadata":{}},{"cell_type":"markdown","source":"#### Vamos aproveitar também e verificar os valores de 'city', 'animal' e 'furniture'","metadata":{}},{"cell_type":"code","source":"columns = ['city', 'animal', 'furniture']\n\nfor col in columns:\n    print(df[col].unique())","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:53.263262Z","iopub.execute_input":"2021-06-16T20:46:53.263567Z","iopub.status.idle":"2021-06-16T20:46:53.278914Z","shell.execute_reply.started":"2021-06-16T20:46:53.263539Z","shell.execute_reply":"2021-06-16T20:46:53.277733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Análise de Outliers","metadata":{}},{"cell_type":"markdown","source":"### Vamos usar o PairPlot do Seaborn para traçar o conjunto de histogramas e scatterplots das variáveis contínuas:\n#### 'hoa', 'rent amount', 'property tax', 'fire insurance', 'total'","metadata":{}},{"cell_type":"code","source":"# Comentei isso aqui pq demora batante pra gerar a imagem. Se quiser ver, pode descomentar.\n# sns.pairplot(df[['hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)', 'total (R$)']])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:53.280305Z","iopub.execute_input":"2021-06-16T20:46:53.280719Z","iopub.status.idle":"2021-06-16T20:46:53.290517Z","shell.execute_reply.started":"2021-06-16T20:46:53.280671Z","shell.execute_reply":"2021-06-16T20:46:53.289374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### É perceptível que existem outliers. Vamos traçar um Scatter Plot de 2 destas variáveis usando Plotly para criar um gráfico interativo para analisar um pouco melhor alguns desses outliers. Vamos também aplicar o Z-Score com z=3 para ter uma noção da faixa dos valores dos outliers.","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\n\ndata = df[['property tax (R$)', 'rent amount (R$)']].dropna()\nx = data['property tax (R$)'].values\ny = data['rent amount (R$)'].values\n\nmask1 = np.abs(stats.zscore(x)) < 3\nmask2 = np.abs(stats.zscore(y)) < 3\nmask = np.logical_and(mask1, mask2)\n\nfig = px.scatter(df, x=\"property tax (R$)\", y=\"rent amount (R$)\")\n\nfig.add_shape(type=\"rect\",\n    x0=min(x[mask]), y0=min(y[mask]), x1=max(x[mask]), y1=max(y[mask]),\n    line=dict(color=\"Green\", width=2,),\n    opacity=0.2,\n    fillcolor=\"Green\",\n)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:53.29191Z","iopub.execute_input":"2021-06-16T20:46:53.292378Z","iopub.status.idle":"2021-06-16T20:46:55.98099Z","shell.execute_reply.started":"2021-06-16T20:46:53.292339Z","shell.execute_reply":"2021-06-16T20:46:55.97992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Vamos plotar os Histogramas dessas variáveis contínuas sem filtros e com filtros de outliers (Z-Score com z=3)","metadata":{}},{"cell_type":"code","source":"columns = ['area', 'hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)', 'total (R$)']\n\nfig, axes = plt.subplots(nrows=len(columns), ncols=2, figsize=(15,30))\n\nfor i, col in enumerate(columns):\n    # Plotamos o histograma na esquerda\n    axes[i][0].hist(df[col], bins=50)\n    \n    # Obtemos a mascara booleana de possíveis outliers (de acordo com o valor de Z)\n    mask = np.abs(stats.zscore(df[col].dropna())) < 3\n    dado_filtrado = df[col].dropna()[mask]\n    \n    # Plotamops o histograma na direita, já filtrado\n    axes[i][1].hist(dado_filtrado)\n    \n    # Aproveitamos para plotar no esquerda, onde nós \"cortamos\" o histograma, para gerar o hist da direita\n    ymax = axes[i][0].get_yticks()[-1]\n    axes[i][0].vlines(x=max(dado_filtrado), ymin=0, ymax=ymax, color=\"red\")\n    \n    # Danos nomes aos gráficos\n    axes[i][0].set_title(\"{} - sem filtro de outliers\".format(col))\n    axes[i][1].set_title(\"{} - com filtro de outliers\".format(col))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:55.982678Z","iopub.execute_input":"2021-06-16T20:46:55.983125Z","iopub.status.idle":"2021-06-16T20:46:58.569579Z","shell.execute_reply.started":"2021-06-16T20:46:55.983077Z","shell.execute_reply":"2021-06-16T20:46:58.568553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### É possível remover estas variáveis outliers conforme indica o gráfico. Além disso, o valor de Z é ajustável: consideramos z=3, mas caso queira mais rigor, pode diminuir o valor de z ou aumentá-lo, caso queira um maior relaxamento na remoção de outliers.","metadata":{}},{"cell_type":"markdown","source":"# Distribuição das classes e dos números discretos","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\n\ncolumns = ['city', 'animal', 'furniture']\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\n\ndf['city'].value_counts().plot.barh(ax=axes[0])\ndf['animal'].value_counts().plot.barh(ax=axes[1])\ndf['furniture'].value_counts().plot.barh(ax=axes[2])\n\naxes[0].set_title(\"city\")\naxes[1].set_title(\"animal\")\naxes[2].set_title(\"furniture\")\nplt.tight_layout()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:58.571094Z","iopub.execute_input":"2021-06-16T20:46:58.571662Z","iopub.status.idle":"2021-06-16T20:46:59.127462Z","shell.execute_reply.started":"2021-06-16T20:46:58.571616Z","shell.execute_reply":"2021-06-16T20:46:59.126692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Para os numeros discretos:\ncolumns = ['rooms', 'bathroom', 'parking spaces', 'floor']\n\ndf_aux = df.copy()\ndf_aux['floor'].replace(to_replace='-', value=0, inplace=True)\ndf_aux['floor'] = pd.to_numeric(df_aux['floor'].values)\n\nfig, axes = plt.subplots(nrows=1, ncols=4, figsize=(18, 5))\n\nfor i, col in enumerate(columns):\n    axes[i].hist(df_aux[col], bins=df_aux[col].nunique())\n    axes[i].set_title(col)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:59.128555Z","iopub.execute_input":"2021-06-16T20:46:59.129013Z","iopub.status.idle":"2021-06-16T20:46:59.808796Z","shell.execute_reply.started":"2021-06-16T20:46:59.128969Z","shell.execute_reply":"2021-06-16T20:46:59.807831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Como estamos falando de número de vagas de estacionamentos, quartos e banheiros, o esperado é que a maior parte dos dados esteja nos valores pequenos, formando uma distribuição assimétrica. Ainda assim, existem alguns casos... atípicos, como por exemplo um imóvel com 12 quartos. Isso é um outlier? Depende! Se for um hotel, 12 quartos pode ser considerado pouco.","metadata":{}},{"cell_type":"markdown","source":"# Variáveis ausentes\n\n#### Vamos plotar o número de dados ausentes por coluna em um Gráfico de Barras","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\n\ndf.isna().sum().plot.barh()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:46:59.810034Z","iopub.execute_input":"2021-06-16T20:46:59.810453Z","iopub.status.idle":"2021-06-16T20:47:00.046296Z","shell.execute_reply.started":"2021-06-16T20:46:59.810422Z","shell.execute_reply":"2021-06-16T20:47:00.045503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Cerca de 10% dos dados de cada COLUNA estão ausentes.\n#### Vamos verificar a ausência de dados por LINHA:","metadata":{}},{"cell_type":"code","source":"df.isna().sum(axis=1).value_counts().plot.barh()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:47:00.047393Z","iopub.execute_input":"2021-06-16T20:47:00.047866Z","iopub.status.idle":"2021-06-16T20:47:00.217735Z","shell.execute_reply.started":"2021-06-16T20:47:00.047832Z","shell.execute_reply":"2021-06-16T20:47:00.216931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### A maioria das LINHAS possuem 1 dado ausente. Boa parte (cerca de 2900, segundo o gráfico) não contém NENHUM dado ausente. Outra boa parte dos dados possuem 2 dados ausentes.\n\n#### Com base nisso, existem algumas perguntas:\n - Quais variáveis valem a pena imputar os dados ausentes?\n - Quais variáveis vale a pena deletar os dados ausentes?\n - Se formos deletar linhas com variáveis ausentes, qual seria nossa estratégia? Deletamos linhas com mais de 3 var ausentes ou todas?\n \n#### Existem várias respostas, e todas elas tem prós e contras e dependendem do dataset e do contexto de modelagem.","metadata":{}},{"cell_type":"markdown","source":"# Exemplo de imputação dos valores ausentes das variáveis contínuas:\n#### 'hoa', 'rent amount', 'property tax', 'fire insurance', 'total'","metadata":{}},{"cell_type":"markdown","source":"#### Primeiro vamos tratar os dados para criar um regressor com dados \"limpos\". Com base nesse regressor \"limpo\", vamos fazer uma imputação dos dados ausentes do dataframe. Vamos também comparar o resultado obtido com o resultado da imputação usando um regressor \"sujo\", gerado a partir de dados não tratados.\n\n### Imputação com dados tratados:","metadata":{}},{"cell_type":"code","source":"df_original = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\ndf = df_original.copy()\n# Faremos agora algumas imputações de valores ausentes usando regressão\n# Para CRIAR o IMPUTADOR, vamos utilizar dados LIMPOS. Ou seja, sem dados ausentes e sem outliers.\ncolumns = ['rent amount (R$)', 'property tax (R$)', 'area', 'total (R$)']\ndf_regress = df_original[columns].copy()\n\nfor col in columns:\n    value = df_original[col].median()\n    df_regress[col].fillna(value=value, inplace=True)\n\nmask = (np.abs(stats.zscore(df_regress)) < 3).all(axis=1)\ndf_regress = df_regress[mask]\n\ndf_regress.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:47:00.218834Z","iopub.execute_input":"2021-06-16T20:47:00.219227Z","iopub.status.idle":"2021-06-16T20:47:00.281058Z","shell.execute_reply.started":"2021-06-16T20:47:00.219198Z","shell.execute_reply":"2021-06-16T20:47:00.280253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# Agora criaremos o regressor com os dados limpos\n# Criamos um objeto que fará a Imputação por Regressão\nimp_mean = IterativeImputer(random_state=0)\n# Treinamos a regressão com os dados disponiveis\nimp_mean.fit(df_regress.values)\n\n# Agora, iremos pegar o dataset \"sujo\" e imputar valores nulos com o regressor \"limpo\"\nX = df_original[['rent amount (R$)', 'property tax (R$)', 'area', 'total (R$)']].values\nregr_output = imp_mean.transform(X)\n\nfoo = pd.DataFrame(regr_output)\nfoo.columns = df_regress.columns\n\nfor col in foo.columns:\n    valores_menor_0 = (foo[col]<0).sum()\n    print(\"Existem {} dados com valor < 0 em '{}'\".format(valores_menor_0, col))\n\ndf[columns] = foo[mask]\n    \nfoo[mask].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:47:00.282146Z","iopub.execute_input":"2021-06-16T20:47:00.28256Z","iopub.status.idle":"2021-06-16T20:47:00.627964Z","shell.execute_reply.started":"2021-06-16T20:47:00.282529Z","shell.execute_reply":"2021-06-16T20:47:00.626687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imputação com dados não tratados:","metadata":{}},{"cell_type":"code","source":"df_original = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\ndf = df_original.copy()\n# Faremos agora algumas imputações de valores ausentes usando regressão\n\ncolumns = ['rent amount (R$)', 'property tax (R$)', 'area', 'total (R$)']\ndf_regress = df_original[columns].copy()\ndf_regress.dropna(inplace=True)\n\ndf_regress.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:47:00.633418Z","iopub.execute_input":"2021-06-16T20:47:00.634184Z","iopub.status.idle":"2021-06-16T20:47:00.699109Z","shell.execute_reply.started":"2021-06-16T20:47:00.634129Z","shell.execute_reply":"2021-06-16T20:47:00.697893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# Agora criaremos o regressor com os dados limpos\n# Criamos um objeto que fará a Imputação por Regressão\nimp_mean = IterativeImputer(random_state=0)\n# Treinamos a regressão com os dados disponiveis\nimp_mean.fit(df_regress.values)\n\n# Agora, iremos pegar o dataset \"sujo\" e imputar valores nulos com o regressor \"limpo\"\nX = df_original[['rent amount (R$)', 'property tax (R$)', 'area', 'total (R$)']].values\nregr_output = imp_mean.transform(X)\n\nfoo = pd.DataFrame(regr_output)\nfoo.columns = df_regress.columns\n\nfor col in foo.columns:\n    valores_menor_0 = (foo[col]<0).sum()\n    print(\"Existem {} dados com valor < 0 em '{}'\".format(valores_menor_0, col))\n\ndf[columns] = foo[mask]\n    \nfoo[mask].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:47:00.700686Z","iopub.execute_input":"2021-06-16T20:47:00.701141Z","iopub.status.idle":"2021-06-16T20:47:00.791303Z","shell.execute_reply.started":"2021-06-16T20:47:00.701104Z","shell.execute_reply":"2021-06-16T20:47:00.790073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exemplo de imputação dos valores ausentes das variáveis discretas:\n#### 'parking spaces', 'rooms', 'bathroom'","metadata":{}},{"cell_type":"markdown","source":"#### Várias estratégias podem ser usadas. Aqui, usaremos uma regressão, mas iremos arredondar os números gerados para continuarem discretos.\n\n#### Para esta imputação, usaremos 'rooms', 'bathroom' e 'parking spaces' que são valores discretos mas usaremos também 'rent amount' pois todas essas variáveis tem UM ALTO VALOR DE CORRELAÇÃO, e isso é importante na construção do regressor.","metadata":{}},{"cell_type":"code","source":"df_original = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\ndf = df_original.copy()\n\n# Faremos agora algumas imputações de valores ausentes usando regressão\ncolumns = ['rooms', 'bathroom', 'parking spaces', 'rent amount (R$)']\ndf_regress = df_original[columns].copy()\n\nfor col in columns:\n    value = df_original[col].median()\n    df_regress[col].fillna(value=value, inplace=True)\n\nmask = (np.abs(stats.zscore(df_regress)) < 3).all(axis=1)\ndf_regress = df_regress[mask]\n\n# Criamos um objeto que fará a Imputação por Regressão\nimp_mean = IterativeImputer(random_state=0)\n# Treinamos a regressão com os dados disponiveis\nimp_mean.fit(df_regress.values)\n\n# Agora, iremos pegar o dataset \"sujo\" e imputar valores nulos com o regressor\nX = df_original[columns].values\nregr_output = np.round(imp_mean.transform(X))\n\nfoo = pd.DataFrame(regr_output)\nfoo.columns = df_regress.columns\n\nfor col in ['rooms', 'bathroom', 'parking spaces']:\n    print(\"{}: {}\".format(col, foo[col].unique()))\n    \n\ndf[columns] = foo[mask]\n\nfoo[mask].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:47:00.796293Z","iopub.execute_input":"2021-06-16T20:47:00.797175Z","iopub.status.idle":"2021-06-16T20:47:00.946211Z","shell.execute_reply.started":"2021-06-16T20:47:00.797106Z","shell.execute_reply":"2021-06-16T20:47:00.945055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exemplo de imputação dos valores ausentes de variáveis categóricas:\n#### 'city', 'animal', 'furniture'","metadata":{}},{"cell_type":"markdown","source":"- Podemos substituir as variáveis ausentes pela moda dos valores (o número que mais se repete)\n- Podemos substituir o dado ausente pelo dado anterior/seguinte\n- Podemos criar uma nova categoria \"Indefinido\".","metadata":{}},{"cell_type":"code","source":"df_antes = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\ndf_depois = df_antes.copy()\n\n# Substituindo dados ausentes pela categoria com maior frequência\ndf_depois['animal'].fillna(value=df_depois['animal'].mode()[0], inplace=True)\n\n# Substituindo dados ausentes por \"Indefinido\"\ndf_depois['furniture'].fillna(value=\"Indefinido\", inplace=True)\n\n# Substituindo dados ausentes pelo próximo dado presente\ndf_depois.fillna(method='ffill', inplace=True)\n\nfig, axes = plt.subplots(3, 1, figsize=(15, 10))\n\nfor i, col in enumerate(['city', 'animal', 'furniture']):\n    axes[i].barh(width=df_depois[col].value_counts(), y=df_depois[col].unique(), label='depois')\n    axes[i].barh(width=df_antes[col].value_counts(), y=df_antes[col].dropna().unique(), label='antes')\n    axes[i].legend()\n    \naxes[0].set_title(\"Imputação copiando o dado anterior. Resultado proporcional\", fontsize=18)\naxes[1].set_title(\"Imputação usando a moda. Aumenta a desproporção\", fontsize=18)\naxes[2].set_title(\"Criação de uma nova classe\", fontsize=18)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:47:00.948423Z","iopub.execute_input":"2021-06-16T20:47:00.949226Z","iopub.status.idle":"2021-06-16T20:47:01.646845Z","shell.execute_reply.started":"2021-06-16T20:47:00.949172Z","shell.execute_reply":"2021-06-16T20:47:01.645818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exemplo Feature Encoding","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\n\ndf['animal'].replace(to_replace='acept', value=1, inplace=True)\ndf['animal'].replace(to_replace='not acept', value=0, inplace=True)\n\ndf['furniture'].replace(to_replace='furnished', value=1, inplace=True)\ndf['furniture'].replace(to_replace='not furnished', value=0, inplace=True)\n\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:47:01.64827Z","iopub.execute_input":"2021-06-16T20:47:01.648672Z","iopub.status.idle":"2021-06-16T20:47:01.724455Z","shell.execute_reply.started":"2021-06-16T20:47:01.648639Z","shell.execute_reply":"2021-06-16T20:47:01.723327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\n\n# OneHot Encoding\ndf = df[df['city'].notna()]\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(df[['city']])\nenc_df = pd.DataFrame(enc.transform(df[['city']]).toarray(), columns=enc.get_feature_names(['city']))\ndf.reset_index(drop=True, inplace=True)\nenc_df.reset_index(drop=True, inplace=True)\ndf = df.join(enc_df)\n\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:47:01.725989Z","iopub.execute_input":"2021-06-16T20:47:01.726323Z","iopub.status.idle":"2021-06-16T20:47:01.811939Z","shell.execute_reply.started":"2021-06-16T20:47:01.72629Z","shell.execute_reply":"2021-06-16T20:47:01.810849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exemplo de Feature Engineering (bem simples)\n#### Criar novas features a partir de features já existentes\n#### Obs: Não é obrigatório criar novas Features neste trabalho","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\n\ndf['Feature Nova 1'] = df['area'] ** 2\ndf['Feature Nova 2'] = df['fire insurance (R$)']/df['rent amount (R$)']\ndf['Feature Nova 3'] = df['rooms'] * df['bathroom']\n\ndf","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:47:01.813634Z","iopub.execute_input":"2021-06-16T20:47:01.814114Z","iopub.status.idle":"2021-06-16T20:47:01.889848Z","shell.execute_reply.started":"2021-06-16T20:47:01.814067Z","shell.execute_reply":"2021-06-16T20:47:01.888847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection\n#### Útil para decidir qual feature seria mais interessante de descartar. Estou inserindo apenas para fins de completude.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/aula-2-ia-dataset/CasasParaAlugar.csv', index_col=0)\n\ndf.dropna(inplace=True)\ndf['floor'].replace(to_replace='-', value=0, inplace=True)\ndf['floor'] = pd.to_numeric(df['floor'].values)\n\ncolumns = ['area', 'rooms', 'bathroom', 'parking spaces', 'floor', 'hoa (R$)', 'rent amount (R$)', 'property tax (R$)', 'fire insurance (R$)']\nx = df[columns]\ny = df['total (R$)']\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_regression\n\n# k é o numero de features que NÃO serão jogadas foras. Vamos primeiro ver os resultados, depois eliminar alguma feature.\nk = x.shape[1]\n# Utilizamos um método do sklearn para isso, usando a estratégia Chi Squared.\nselector = SelectKBest(f_regression, k=k)\nx_new = selector.fit_transform(x, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:47:01.891184Z","iopub.execute_input":"2021-06-16T20:47:01.891483Z","iopub.status.idle":"2021-06-16T20:47:01.957428Z","shell.execute_reply.started":"2021-06-16T20:47:01.891455Z","shell.execute_reply":"2021-06-16T20:47:01.955911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utilizo o log10 pois os valores são ou muito grandes, ou muito pequenos\nscores = -np.log10(selector.pvalues_)\n\nx_plot = list(range(len(scores)))\n\nfig, ax = plt.subplots(figsize=(8, 4))\nplt.bar(x_plot, scores)\nax.set_title(\"Score do método f-regression para Feature Selection\")\nax.set_xticks(x_plot)\nax.set_xticklabels(columns, rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T20:47:01.959424Z","iopub.execute_input":"2021-06-16T20:47:01.959964Z","iopub.status.idle":"2021-06-16T20:47:02.203733Z","shell.execute_reply.started":"2021-06-16T20:47:01.959914Z","shell.execute_reply":"2021-06-16T20:47:02.202695Z"},"trusted":true},"execution_count":null,"outputs":[]}]}