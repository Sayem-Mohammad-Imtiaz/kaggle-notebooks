{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport os\n#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport joblib\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, save_model, load_model\nfrom tensorflow.keras.layers import LSTM, Dropout, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom os import path, getcwd\nimport zipfile\n\n\n#tf.config.threading.set_inter_op_parallelism_threads(2)\n#tf.config.threading.set_intra_op_parallelism_threads(2)\n# detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nepochs = 50\nbatch_size = 128 # 16 * tpu_strategy.num_replicas_in_sync\nwindow_size = int(batch_size * 2)  # must be a multiple of batch_size\nvalidation_size = 8192 * batch_size  # must be a multiple of batch_size\ntest_size = 8192 * batch_size  # must be a multiple of batch_size\nma_periods = 14  # Simple Moving Average periods length\nticker = 'btcusd'  # Your data file name without extention\nstart_date = '2014-09-17'  \nseed = 42  # An arbitrary value to make sure your seed is the same\nmodel_path = f\"/kaggle/working/models/{ticker}-{batch_size}-{window_size}-{ma_periods}\"\nmodel_lite_path = f\"/kaggle/working/models/{ticker}-{batch_size}-{window_size}-{ma_periods}/model.tflite\"\nscaler_path = f'/kaggle/working/scalers/{ticker}-{batch_size}-{window_size}-{ma_periods}.bin'\nfull_time_series_path = \"/kaggle/input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv\"\ntrain_time_series_path = f'/kaggle/input/btcusdt-datasets/{ticker}-train.csv'\nvalidate_time_series_path = f'/kaggle/input/btcusdt-datasets/{ticker}-validate.csv'\ntest_time_series_path = f'/kaggle/input/btcusdt-datasets/{ticker}-test.csv'\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\ndef get_train(values, window_size):\n    X, y = [], []\n    len_values = len(values)\n    for i in range(window_size, len_values):\n        X.append(values[i-window_size:i])\n        y.append(values[i])\n    X, y = np.asarray(X), np.asarray(y)\n    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n    print(f\"X {X.shape}, y {y.shape}\")\n    return X, y\n\ndef get_val(values, window_size):\n    X = []\n    len_values = len(values)\n    for i in range(window_size, len_values):\n        X.append(values[i-window_size:i])\n    X = np.asarray(X)\n    X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n    y = values[-X.shape[0]:]\n    print(f\"X {X.shape}, y {y.shape}\")\n    return X, y\n\ndef zipdir(path, ziph):\n    # ziph is zipfile handle\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            if not file.endswith(\"zip\"):\n                ziph.write(os.path.join(root, file))\n\n\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\nos.makedirs(\"/kaggle/working/scalers\", exist_ok=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Begin training"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnp.random.seed(seed)\ntf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(train_time_series_path, dayfirst=True, \n    index_col=['Timestamp'], parse_dates=['Timestamp'])\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(24, 18))\nax1, ax2, ax3 = fig.subplots(3)\nax1.set_title('HLAvg')\nax1.set(xlabel='Timestamp', ylabel='High-Low Average')\nax1.plot(df['HLAvg'])\nax2.set_title('MA')\nax2.set(xlabel='Timestamp', ylabel='MA')\nax2.plot(df['MA'])\nax3.set_title('Returns')\nax3.set(xlabel='Timestamp', ylabel='Returns')\nax3.plot(df['Returns'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\ntrain_values = scaler.fit_transform(df[['Returns']].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(24, 8))\nax1 = fig.subplots(1)\nax1.set_title('Returns MinMax Scaled')\nax1.set(xlabel='Sample', ylabel='Scaled Returns')\nax1.plot(train_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = get_train(train_values, window_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_val = pd.read_csv(validate_time_series_path, dayfirst = True, usecols=['Timestamp','Returns'],\n    index_col=['Timestamp'], parse_dates=['Timestamp'])\ndf_val['Scaled'] = scaler.transform(df_val[['Returns']].values)\nX_val, y_val = get_val(df_val['Scaled'].values, window_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiating the model in the strategy scope creates the model on the TPU\n# with tpu_strategy.scope():\nif path.exists(model_path):\n    print(\"using existing model\")\n    model = load_model(model_path)\nelse:\n    print(\"using a new model\")\n    model = Sequential()\n\nmodel.add(LSTM(76, input_shape=(X.shape[1], 1), return_sequences = False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\noptimizer = tf.keras.optimizers.Adam()\nmodel.compile(loss=\"mse\", optimizer=optimizer)\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(X, y, validation_data=(X_val, y_val), epochs = epochs, batch_size = batch_size, shuffle=False, verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run_model = tf.function(lambda x: model(x))\nconcrete_func = run_model.get_concrete_function(\n    tf.TensorSpec([batch_size, 100, 1], model.inputs[0].dtype))\nmodel.save(model_path, save_format=\"tf\", signatures=concrete_func)\njoblib.dump(scaler, scaler_path) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12, 8))\nax1 = fig.subplots(1)\nax1.set_title('Model Loss')\nax1.set(xlabel='Epoch', ylabel='Loss')\nax1.plot(history.history['loss'][7:], label='Train Loss')\nax1.plot(history.history['val_loss'][7:], label='Val Loss')\nax1.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir('/kaggle/working')\nconverter = tf.lite.TFLiteConverter.from_saved_model(model_path)\ntfmodel = converter.convert()\nwith open('model.tflite', 'wb') as f:\n    f.write(tfmodel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zip_path = path.join(getcwd(), 'results.zip')\nif path.exists(zip_path):\n    os.remove(zip_path)\n    \nzipf = zipfile.ZipFile('results.zip', 'w', zipfile.ZIP_DEFLATED)\nzipdir('/kaggle/working', zipf)\nzipf.close()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}