{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"name":"python","version":"3.6.1","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"execution_count":null,"source":"## Introduction ##\n\nThis was an assignment for my Machine Learning class, thought I should share it here. \n\nThe purpose of the assignment was to implement a classifier to differentiate between the two classes diabetes vs no_diabetes in the *Pima* dataset. Using 5-crossfold validation technique, we should train the best classifier once using all 8 features of the data and once after reducing the dimensionality of the data using principal component analysis (PCA). \n\nFinally, we should compare the best accuracy with/without using PCA. \n\nYou will notice that I had to write my own PCA implementation as that was required from us, you can ignore that part. \n\nI was able to achieve **80.13%** average cross validation accuracy using the following fully connected model","metadata":{"_execution_state":"idle","collapsed":false,"_cell_guid":"4f296c2c-cd31-4e2d-b456-efed8fe64335","_uuid":"4c4da158375825082a7fce1c6d1baa18f60460f6"},"cell_type":"markdown","outputs":[]},{"execution_count":null,"source":"# coding: utf-8\nfrom __future__ import print_function\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd \n\nfrom sklearn import preprocessing, decomposition\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.core import Dense, Activation, Dropout, Flatten\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils\n\nnp.random.seed(1337)  # for reproducibility","metadata":{"_execution_state":"idle","trusted":false,"_cell_guid":"2c30c6ab-9a9e-4fa3-9cd8-b5bda94acc7e","_uuid":"596e5b1f2edc924a7b84873497f8d70bd250d5aa"},"cell_type":"code","outputs":[]},{"execution_count":null,"source":"# Some global variables\nnum_classes = 1\nnum_features = 8\nnum_reduce = 7\nepochs = 200\neig_vec = []","metadata":{"_execution_state":"idle","trusted":false,"collapsed":false,"_cell_guid":"052030fd-1603-4958-a3d4-1af07b47c77b","_uuid":"809be79c1f0fe50a5d2a50c08123cdc871ce97de"},"cell_type":"code","outputs":[]},{"execution_count":null,"source":"# You can ignore this, as this was part of my assignment\nclass PCA(object):\n    def  __init__(self, k):\n        self.U = None \n        self.mean = None\n        self.std = None\n        self.k = k\n\n    def process(self, X_t):\n        X = X_t.copy()\n        pca_var = None\n        if self.mean is None:\n            self.mean = np.mean(X, axis=0)\n            self.std = np.std(X, axis=0)\n\n        X -= self.mean\n        X /= self.std\n        \n        if self.U is None:\n            cov = X.T.dot(X) / X.shape[0]\n            self.U, S, V = np.linalg.svd(cov)\n            pca_var = np.sum(S[:self.k]) / np.sum(S)\n            \n        return X.dot(self.U[:, :self.k]), pca_var       \n        ","metadata":{"_execution_state":"idle","trusted":false,"collapsed":false,"_cell_guid":"7a9a0d12-d321-419f-a687-a2f431f3f3dd","_uuid":"22709edf09b601e23d635bf96b24a35b38214061"},"cell_type":"code","outputs":[]},{"execution_count":null,"source":"def read_data():\n    df = pd.read_csv(\"../input/diabetes.csv\")\n    data = df.as_matrix()\n    y = data[:,  -1]\n    X = data[:, :-1]\n    return X, y","metadata":{"_execution_state":"idle","trusted":false,"collapsed":false,"_cell_guid":"371e0bed-2025-46a0-b57c-e495c64fa0cc","_uuid":"ede1db0c5e5537ad3a1eeddde7f487c71ad3d989"},"cell_type":"code","outputs":[]},{"execution_count":null,"source":"def get_model():\n    model = Sequential()\n    model.add(Dense(4,activation='elu',input_dim=(num_reduce)))\n    model.add(Dense(6,activation='elu'))\n    model.add(Dense(7,activation='elu'))\n    model.add(Dense(8,activation='elu'))\n    model.add(Dense(num_classes,activation='sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy',  metrics=['accuracy'])\n    return model","metadata":{"_execution_state":"idle","trusted":false,"collapsed":false,"_cell_guid":"adb67c13-8ec7-4f76-af5f-9780c34b47a3","_uuid":"e231ab0eb16a6cb31a20a02b6123a752700a3423"},"cell_type":"code","outputs":[]},{"execution_count":null,"source":"def preprocess(X_train, X_val):\n    X_t = X_train.copy()\n    X_v = X_val.copy()\n    mean = np.mean(X_t, axis=0)\n    std = np.std(X_t, axis=0)\n    X_t -= mean\n    X_t /= std \n    X_v -= mean\n    X_v /= std\n    return X_t, X_v","metadata":{"_execution_state":"idle","trusted":false,"collapsed":false,"_cell_guid":"36d0b0cf-f716-4da3-a416-9975726aafec","_uuid":"8073a071e76a458a06d96b4da2af681d9e36d5b3"},"cell_type":"code","outputs":[]},{"execution_count":null,"source":"def cross_val(X, y, k_fold = 5):\n    step = X.shape[0] // k_fold \n    accuracies = []\n    pca_kept_var = []\n\n    for k in range(k_fold):\n        # Divide dataset to training and validation sets\n        X_val = X[k*step:((k+1)*step)]\n        y_val = y[k*step:((k+1)*step)]\n        X_train = np.delete(X,np.arange(k*step,((k+1)*step)), axis = 0)\n        y_train = np.delete(y,np.arange(k*step,((k+1)*step)))\n\n        if(num_features != num_reduce):\n            pca = PCA(num_reduce)\n            X_train, pca_var = pca.process(X_train)\n            X_val, _ = pca.process(X_val)\n            pca_kept_var.append(pca_var)\n        else:\n            X_train, X_val = preprocess(X_train, X_val)\n\n        model = get_model()\n        #model.summary() if k == 0 else None\n        history = model.fit(X_train, y_train, epochs=epochs, batch_size=50, validation_data=(X_val,y_val), verbose = 0)\n        accuracies.append(np.max(history.history[\"val_acc\"]))\n        print(\"accuracy #\",k,\": \",accuracies[k])\n    return np.mean(accuracies), np.mean(pca_kept_var) if len(pca_kept_var) > 0 else None","metadata":{"_execution_state":"idle","trusted":false,"collapsed":false,"_cell_guid":"4effd86d-78a7-47da-b960-b2394cdbc5af","_uuid":"256a6e97af4148a767cdd54b2ec2746df9c79223"},"cell_type":"code","outputs":[]},{"execution_count":null,"source":"X, y = read_data()\nprint(\"X, Y shape\", X.shape, y.shape)\nacc, pca_var = cross_val(X, y)\nprint(\"ACCR: \", acc)\nprint(\"PCA Kept Variance: \", pca_var)","metadata":{"_execution_state":"idle","trusted":false,"collapsed":false,"_cell_guid":"9a8b5c2c-5152-4a4a-a666-f3eb7df3ce7f","_uuid":"06421e20b55bda8abac0e06b1c92a2834895e6da"},"cell_type":"code","outputs":[]},{"execution_count":null,"source":"## Cross Validation Accuracy without PCA ##\n\nI preprocessed the data in the beginning by zero-centering the data then normalizing it\nusing the training set. This improved the accuracy significantly\n\n\n![enter image description here][1]\n\n\n  [1]: https://image.ibb.co/cfAyea/ACCR.png","metadata":{"_execution_state":"idle","collapsed":false,"_cell_guid":"ae17e064-10e3-4b8f-a4f3-6f448bc171e0","_uuid":"db2818bdaca3156ce6915294920970a72f4d1d2e"},"cell_type":"markdown","outputs":[]},{"execution_count":null,"source":"## Average Cross Validation Accuracy with PCA ##\n\n\n![enter image description here][1]\n\n\n  [1]: https://image.ibb.co/fQTUkF/ACCR_PCA.png","metadata":{"_execution_state":"idle","collapsed":false,"_cell_guid":"ffc2e717-d030-476f-ab83-a1cab187318b","_uuid":"b1b227121d448517482e05fddb2ecbfb2533f1fc"},"cell_type":"markdown","outputs":[]},{"execution_count":null,"source":"## PCA Kept-Variance vs Eigen Vectors ##\n\n![enter image description here][1]\n\n\n  [1]: https://image.ibb.co/cVKDCv/PCA_VAR.png","metadata":{"_execution_state":"idle","collapsed":false,"_cell_guid":"63f41631-3174-41f6-a2bf-8ca0f1dd5a4d","_uuid":"bdce6413e2793ed2c2809c67d9cca61721b61429"},"cell_type":"markdown","outputs":[]},{"execution_count":null,"source":"## Conclusion ##\nThe PCA didn’t enhance my classification accuracy, although reducing the data into 6 or 7 features got me a more or less similar accuracy, 78.6% compared to 80.13% with 8 features using the same network.\n\nIt’s important to note also that the pre-processing step I did using the full features: subtracting the mean and dividing by the standard deviation improved the accuracy significantly, without this step I got an ACCR of 71.76% using the full features, which is a much less accuracy than the ones I got using PCA.","metadata":{"_execution_state":"idle","collapsed":false,"_cell_guid":"8ecbf823-76b9-47ef-953e-5c8e3b0c495e","_uuid":"a64a82e14d64c42f2e11d3b59aa500fb59927e28"},"cell_type":"markdown","outputs":[]}]}