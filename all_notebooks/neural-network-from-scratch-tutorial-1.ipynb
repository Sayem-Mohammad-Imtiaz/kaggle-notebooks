{"cells":[{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/neuralnetwork/Screenshot from 2020-06-21 16-18-13.png\", width = '600px')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# What is Deep Learning?\n## [Deep learning](https://en.wikipedia.org/wiki/Deep_learning) is a subset of machine learning where artificial neural networks, algorithms inspired by the human brain, learn from large amounts of data. ... Deep learning allows machines to solve complex problems even when using a data set that is very diverse, unstructured and inter-connected.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# What is Neural Network?\n## [Artificial neural networks](https://en.wikipedia.org/wiki/Neural_network) or connectionist systems are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with task-specific rules. \n","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"/kaggle/input/neuralnetwork/3.png\", width = '800px')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Input Layer, Hiden Layer, Output Layer can be customizable. \n## The Hidden layer can be a multilayer. When the layer is more then two Layer is called Deep Neural Network","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport matplotlib.pylab as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Relu Function\n## The ReLU is the most used activation function in the world right now. Since, it is used in almost all the convolutional neural networks or deep learning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def relu_(x):\n    return np.maximum(0, x)\n\nfig = plt.figure(figsize=(7,7))\nax1 = plt.subplot2grid((1,1), (0,0))\nplt.style.use('ggplot') \n\nx = np.arange(-10.0,10.0, 0.1)\ny = relu_(x)\n\nplt.plot(x, y, color='green', linestyle='dashed', linewidth = 3, \n          markerfacecolor='blue', label='ReLU Curve')\nplt.legend()\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class relu: # Reference Link: https://nobulingual.com/?p=913\n    \n    def __init__(self, value):\n        self.value = value\n    \n    def ReLU(self):\n        if float(self.value) < 0 or float(self.value) == 0:\n            self.value = 0\n        else:\n            self.value = self.value\n        return self.value\n\nInput_Layer = np.random.randn()\nOutput_Value = relu(Input_Layer)\nprint(\"The Random number is : \" + str(Input_Layer))\nprint(\"The ReLU value is : {}\".format(Output_Value.ReLU()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sigmoid Function\n## With the help of Sigmoid activation function, we are able to reduce the loss during the time of training because it eliminates the gradient problem in machine learning model while training.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Mathematical Formula","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"/kaggle/input/neuralnetwork/5.png\", width = '400px')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nfig = plt.figure(figsize=(7,7))\nax1 = plt.subplot2grid((1,1), (0,0))\nplt.style.use('ggplot')\n\nx = np.arange(-10.0,10.0, 0.1)\ny = sigmoid(x)\n\nplt.plot(x, y, color='orange', linestyle='dashed',\n         linewidth = 3, markerfacecolor='blue', label='ReLU Curve')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class sigmoid:\n    \n    def __init__(self, value):\n        self.value = value\n\n    def Sigmoid(self):\n        sIgmoid = 1/(1+np.exp(self.value))\n        return sIgmoid\n    \nInput_Layer = np.random.randn()\nOutput_Value = sigmoid(Input_Layer)\nprint(\"The Random number is : \" + str(Input_Layer))\nprint(\"The Sigmoid value is : {}\".format(Output_Value.Sigmoid()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One Neuron \n## This is an example of one neuron present in a hidden layer. ","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/neuralnetwork/Screenshot from 2020-06-21 21-25-55.png\", width = '600px')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## After finishing the sigmoid function, We have to apply the activation function","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/neuralnetwork/6.png\", width = '600px')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Input_1 = np.random.randn();  Weight_1 = np.random.randn()\nInput_2 = np.random.randn();  Weight_2 = np.random.randn()\nInput_3 = np.random.randn();  Weight_3 = np.random.randn()\n\nInput_Value = [Input_1, Input_2, Input_3]\nWeight_value = [Weight_1, Weight_2, Weight_3]\nBias_Value = np.random.randn()\n\nprint(\"Input Values: \"+ str(Input_Value))\nprint(\"--------------------------------------------------------\\n\")\nprint(\"Weight value: \"+ str(Weight_value))\nprint(\"--------------------------------------------------------\\n\")\nprint(\"Bias Values: \"+ str(Bias_Value))\nprint(\"--------------------------------------------------------\\n\")\n# Multiplying and Adding Input, Weight and Bias Value \n\nresult = Input_Value[0]*Weight_value[0] + Input_Value[1]*Weight_value[1] + Input_Value[2]*Weight_value[2] + Bias_Value \n    \nprint(\"The Multiplying and Adding Result is : \"+ str(result))\nprint(\"--------------------------------------------------------\\n\\n\")\nprint(\"------------- Applying Sigmoid Function ----------------\\n\")\n\n\nsigmoid_output = sigmoid(result)\nprint(\"The Sigmoid value is : \"+ str(sigmoid_output))\nprint(\"--------------------------------------------------------\\n\\n\")\n\n\nprint(\"-------------- Applying ReLU Function ------------------\\n\")\nrelu_output = relu_(sigmoid_output)\nprint(\"The ReLU value is : \"+str(relu_output))\nprint(\"--------------------------------------------------------\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Multiply Neuron\n## Now, We are going to do the same process for multiply neuron present in the hidden layer","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"/kaggle/input/neuralnetwork/2.png\", width = '600px')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Input_1 = np.random.randn();  Weight_1 = np.random.randn()\nInput_2 = np.random.randn();  Weight_2 = np.random.randn()\nInput_3 = np.random.randn();  Weight_3 = np.random.randn()\n\nWeight_4 = np.random.randn();  Weight_7 = np.random.randn()\nWeight_5 = np.random.randn();  Weight_8 = np.random.randn()\nWeight_6 = np.random.randn();  Weight_9 = np.random.randn()\n\nBias_Value1 = np.random.randn()\nBias_Value2 = np.random.randn()\nBias_Value3 = np.random.randn()\n\n# we are creating the random input value \nInput_Value   = [Input_1, Input_2, Input_3]          \n\n# we are creating the random weight value for the first neuron\nWeight_value = [[Weight_1, Weight_2, Weight_3],  \n# we are creating the random weight value for the second neuron\n               [Weight_4, Weight_5, Weight_6],   \n# we are creating the random weight value for the third neuron\n               [Weight_7, Weight_8, Weight_9]]   \n\n# we are creating the random Bias value for the each neuron \nBias_Value    = [Bias_Value1, Bias_Value2, Bias_Value3]\n\n\nprint(\"Input Values: \"+ str(Input_Value))\nprint(\"--------------------------------------------------------\\n\")\nprint(\"Weight value 1: \"+ str(Weight_value1) )\nprint(\"Weight value 2: \"+ str(Weight_value2) )\nprint(\"Weight value 3: \"+ str(Weight_value3) )\nprint(\"--------------------------------------------------------\\n\")\nprint(\"Bias Values: \"+ str(Bias_Value))\nprint(\"--------------------------------------------------------\\n\\n\")\n# Multiplying and Adding Input, Weight and Bias Value \nprint(\"-- Multiplying and Adding Input, Weight and Bias Value --\\n\")\n\nlayer_output = []\nfor neuron_weights, neuron_bias in zip(Weight_value, Bias_Value):\n    neuron_output = 0\n    for n_input, weight in zip(Input_Value,neuron_weights):\n        neuron_output = neuron_output + n_input*weight\n    neuron_output = neuron_output +neuron_bias\n    layer_output.append(neuron_output)\n#                 (or)\n\n# result = [Input_Value[0]*Weight_value[0][0] + Input_Value[1]*Weight_value[0][1] + Input_Value[2]*Weight_value[0][2] + Bias_Value[0], \n#           Input_Value[0]*Weight_value[1][0] + Input_Value[1]*Weight_value[1][1] + Input_Value[2]*Weight_value[1][2] + Bias_Value[1], \n#           Input_Value[0]*Weight_value[2][0] + Input_Value[1]*Weight_value[2][1] + Input_Value[2]*Weight_value[2][2] + Bias_Value[2]] \n\nprint(\"Multiplying and Adding result : \"+ str(layer_output))\nprint(\"--------------------------------------------------------\\n\\n\")\n\n\nprint(\"------------- Applying Sigmoid Function ----------------\\n\")\nsigmoid_layer = []\nfor i in layer_output:\n    sigmoid_output = sigmoid(i)\n    sigmoid_layer.append(sigmoid_output)\nprint(\"Sigmoid value : \"+ str(sigmoid_layer))\nprint(\"--------------------------------------------------------\\n\\n\")\n\n\n\nprint(\"-------------- Applying ReLU Function ------------------\\n\")\nrelu_layer = []\nfor i in sigmoid_layer:\n    relu_output = relu_(i)\n    relu_layer.append(relu_output)\nprint(\"ReLU value : \"+str(relu_layer))\nprint(\"--------------------------------------------------------\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}