{"nbformat_minor":1,"cells":[{"cell_type":"code","metadata":{"_uuid":"cfaa847a7bc9d1c22f057ee7811f3411a00e2921","_cell_guid":"31814574-d5a0-424d-b349-b12d0ec63163","collapsed":true},"execution_count":null,"source":"from os import path                      # os level path manipulation\nimport numpy as np                       # array goodnes\nfrom pandas import DataFrame, read_csv   # excel for python\nfrom matplotlib import pyplot as plt     # plotting library\nfrom pandas import DataFrame, read_csv   # excel for python\nfrom tqdm import tqdm, trange            # progress bars\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (12, 8) # set plot size","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"f855a93c608572acc892234d56a67fe3fd926a51","_cell_guid":"46beddf7-5263-45b1-b2de-43e2061356db","collapsed":true},"source":"# Load Data"},{"cell_type":"code","metadata":{"_uuid":"978301d4e49cb20a948d09a5f93071a6a7c16d83","_cell_guid":"3ba3bf0e-054c-4486-a7d9-e5b728b995ee"},"execution_count":null,"source":"from glob import glob                            # Unix style pathname pattern expansion\nfrom PIL import Image                            # image loading / saving\nfrom skimage.exposure import equalize_adapthist  # CLAHE\nfrom sklearn.model_selection import train_test_split\n\n\ntarget_shape = (96, 96)\nbasepath = '../input/train/'\n\n\ndef preprocess_image(img, target_shape, clahe=False, onehot=False, cval=0):\n    img.thumbnail(target_shape, Image.NEAREST) # resize and keep aspect ratio\n    im = np.array(img)                         # convert to numpy array\n    if clahe:\n        im = equalize_adapthist(im)\n\n    if len(im.shape) == 3:\n        im = np.argmax(im, axis=-1)\n    \n    # padding to rarget_shape\n    padding = np.abs(np.array(im.shape) - target_shape)\n    lpad = padding // 2\n    rpad = padding - lpad\n    im = np.pad(im, [(lpad[0], rpad[0]), (lpad[1], rpad[1])],\n                'constant', constant_values=cval)\n\n    if onehot:\n        a = []\n        for idx in onehot:\n            a.append(im == idx)\n        im = np.stack(a, axis=2)\n    else:\n        im = im.reshape(target_shape+(1,))\n        \n    return im.astype(np.float)\n\n\ndef load_data(basepath, target_shape=(256, 256), max_samples=-1):\n    X, Y = [], []\n    for fn in tqdm(glob(path.join(basepath, 'BBBC010_v1_images', '*.tif')), desc='reading files'):\n        cn = path.split(fn)[-1]\n        wellcolumn = cn.split('_')[6]\n        itype = cn.split('_')[7]\n        if itype != 'w2': continue\n        x_img = Image.open(fn)\n        y_img = Image.open(path.join(basepath, 'BBBC010_v1_foreground', '%s_binary.png' % wellcolumn))\n        x = preprocess_image(x_img, target_shape, clahe=True)\n        y = preprocess_image(y_img, target_shape, clahe=False, onehot=[3, 0], cval=3)\n        X.append(x), Y.append(y)\n    return np.array(X), np.array(Y)\n\n\ndef data_augmentation(x, y):\n    _x, _y = [], []\n    for i in trange(len(x), desc='data augmentation'):\n        _x.append(x[i])\n        _y.append(y[i])\n        _x.append(x[i,::-1])\n        _y.append(y[i,::-1])\n        _x.append(x[i,::-1,::-1])\n        _y.append(y[i,::-1,::-1])\n        _x.append(x[i,:,::-1])\n        _y.append(y[i,:,::-1])\n    return np.array(_x), np.array(_y)\n\nX, Y = load_data('../input', target_shape=target_shape)\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=42)\nX_train, Y_train = data_augmentation(X_train, Y_train)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"f2da5ea40c8e9c9c48d93e30396fa694def61dcf","_cell_guid":"09babe0e-18f3-44bb-b074-16133185bbb5"},"source":"# Display Training / Validation Images"},{"cell_type":"code","metadata":{"_uuid":"4e6c370a6bfcf03aafafb989a38b7ddecfb87e9d","_cell_guid":"8f63bffe-8320-4e63-967b-05097809fa73"},"execution_count":null,"source":"from ipywidgets import interactive, fixed\n\ndef _plot(idx, x_y_p):\n    X, Y, P = x_y_p\n    idx = int(idx)\n    if type(P) != np.ndarray:\n        fig, axs = plt.subplots(1,3)\n    else:\n        fig, axs = plt.subplots(1,4)\n    \n    x = X[idx][...,0]\n    y = Y[idx][...,1]\n    \n    axs[0].set_title('image')\n    axs[0].imshow(x, cmap='plasma', vmin=0, vmax=1)\n    \n    axs[1].set_title('ground truth')\n    axs[1].imshow(y)\n    \n    axs[2].set_title('image & ground truth')\n    y_masked = np.copy(y)\n    y_masked[y_masked==0] = np.NaN\n    axs[2].imshow(x, cmap='Greys')\n    axs[2].imshow(y_masked, cmap='Reds', vmin=0, vmax=1, alpha=0.4)\n    \n    if type(P) == np.ndarray:\n        axs[3].set_title('prediction')\n        axs[3].imshow(P[idx])\n    \n    plt.show()\n\ndef interactive_plot(x, y, p=None):\n    return interactive(_plot, idx=range(len(x)), x_y_p=fixed((x,y,p)))\n\ninteractive_plot(X_train, Y_train)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"9b11c305b47a68afbbd93206186006bf4ad48d99","_cell_guid":"4035a954-3fb2-4722-95b5-f78b56819ef0"},"execution_count":null,"source":"interactive_plot(X_val, Y_val)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"4147073f949b88ed8bdc40d888a64c54b664171d","_cell_guid":"4aa10fda-a9a9-411f-ab4c-a0926b0a791a"},"source":"# Define Network Topology"},{"cell_type":"code","metadata":{"_uuid":"bbb1e8d557ef7153d0d2e1109d837db15e7ca443","_cell_guid":"62032183-f942-4582-9f78-1133fd9793a3"},"execution_count":null,"source":"import keras\nfrom keras.layers import Input, Conv2D, concatenate, AveragePooling2D\nfrom keras.layers import UpSampling2D, BatchNormalization\n\nchannels_per_level = [32, 64, 128, 256]\nbridge_channels = channels_per_level.pop()\nidentities = []\ninput_tensor = Input(shape=target_shape + (1,))\nnet = input_tensor\n\n# encoder\nfor channels in channels_per_level:\n    net = BatchNormalization(momentum=0.9)(net)\n    net = Conv2D(channels, 3, padding='same', activation='relu')(net)\n    identities.append(net)\n    net = AveragePooling2D(padding='same')(net)\n\n# bridge\nnet = BatchNormalization(momentum=0.9)(net)\nnet = Conv2D(bridge_channels, 3, padding='same', activation='relu')(net)\n\n# decoder\nfor channels in channels_per_level[::-1]:\n    net = UpSampling2D()(net)\n    net = concatenate([net, identities.pop()])\n    net = BatchNormalization(momentum=0.9)(net)\n    net = Conv2D(channels, 3, padding='same', activation='relu')(net)\n\n# classification\nn_classes = Y_train[0].shape[-1]\nnet = Conv2D(n_classes, 1, padding='same', activation='sigmoid')(net)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"15ebc3b74013b5d75af90a182831ea96081073c2","_cell_guid":"5299c9dc-8c6d-495f-88c5-4680c1366b44"},"source":"![](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"},{"cell_type":"markdown","metadata":{"_uuid":"1b316bf859393807e5b81046d14565dbb96c7966","_cell_guid":"3b670945-ed98-4ffc-be20-26f20de6c406"},"source":"# Creating the Model"},{"cell_type":"code","metadata":{"_uuid":"9e37d1abc5a5bd0d7dd87b1b06989f78a0d66f1d","_cell_guid":"a2a2b866-8611-4000-bb8b-0ff9843814f8","collapsed":true},"execution_count":null,"source":"import keras.backend as K\nfrom keras.models import Model\n\n#https://github.com/jocicmarko/ultrasound-nerve-segmentation/blob/master/train.py\ndef dice_coef(y_true, y_pred, smooth=1.0):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\n\nlearning_rate = 1e-3\nmodel = Model(input_tensor, net)\nmodel.compile(loss=dice_coef_loss,\n              metrics=[dice_coef],\n              optimizer=keras.optimizers.Adam(lr=learning_rate))","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"270d66d49955432ba8f300677a04c98e368fdbcf","_cell_guid":"47fdbaed-c877-443c-b845-a5c6fa770978"},"source":"# Train the Model"},{"cell_type":"code","metadata":{"_uuid":"465ef0966f171029bbc600e9c9aaf38cf003b1a8","_cell_guid":"480eab45-6b6c-414b-9064-b1985dc48ca2"},"execution_count":null,"source":"model.fit(X_train, Y_train, \n          validation_data=[X_val, Y_val],\n          epochs=1, batch_size=1)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"fc37486cf97891bf452eed0a4e073a6f00fa17b3","_cell_guid":"b25f787e-54d8-49c7-bbcd-392fdd574e2a","collapsed":true},"source":"# Predicting Segmentations"},{"cell_type":"code","metadata":{"_uuid":"015c9d4d442b0154417b1e74519a0bd23b5e7ccc","_cell_guid":"80df22fe-29c8-4f9d-8766-4cbd0c8049d0","collapsed":true},"execution_count":null,"source":"predictions = []\nfor i in trange(len(X_val)):\n    im = X_val[i]\n    im = im.reshape((1,)+im.shape)\n    prediction = model.predict(im)[0]\n    prediction = np.argmax(prediction, axis=-1)\n    predictions.append(prediction)\npredictions = np.array(predictions)","outputs":[]},{"cell_type":"code","metadata":{"_uuid":"3673908b6659295985c58f2410b82b99259c24d2","_cell_guid":"9d79be22-c26b-449f-9a96-c5d5afdc3708","collapsed":true},"execution_count":null,"source":"interactive_plot(X_val, Y_val, predictions)","outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"dcfc06cac8f7786809125563195840fe96210af3","_cell_guid":"5549133e-44d3-4be3-b426-4b4b193ec677","collapsed":true},"source":"# HANDS ON: Tune the Hyperparameters\n\nFirst:  \nExamine the reproducibility of the results. Does the neural network always deliver the same performance?\n\nSecond:  \nTry to tune different hyperparameters: \n  - How does the input shape of the images impact training performance?\n  - Does a deeper architecture yield better results?\n  - How about a wider architecture?\n  - Do more conv layers per down-/upsample step change the result?"},{"cell_type":"code","metadata":{"_uuid":"a5088be21341ef95ba47e42176c2cf9855fdbb5b","_cell_guid":"735c58b4-64c8-4c7d-9ea4-f1e27741ea6b","collapsed":true},"execution_count":null,"source":"","outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"file_extension":".py","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","nbconvert_exporter":"python","version":"3.6.4","name":"python","pygments_lexer":"ipython3"}},"nbformat":4}