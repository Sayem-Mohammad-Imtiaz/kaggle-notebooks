{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Searching relevant paragraphs and question answering\n\n## How it works:\n- We used an LDA topic embedding to extract articles from the corpus of papers which are potentially relevant to answer a given search query. \n- We then use:\n    1. The LDA model to highlight the most related paragraphs to the query by comparing topic distributions\n    2. A BERT model for highlighting which paragraphs are most relevant to the given query (using next sentence prediction)\n    3. A SciBERT model (trained on semantic scholar papers, finetuned on SQuAD) for directly extracting the answer from the paragraph\n    \nThe LDA Logic comes from [Daniel Wolffram's](https://www.kaggle.com/danielwolffram) notebook [\"Topic Modeling: Finding Related Articles\"](https://www.kaggle.com/danielwolffram/topic-modeling-finding-related-articles).<br />\n<br />\n"},{"metadata":{},"cell_type":"markdown","source":"## Install/Load Packages"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from IPython.utils import io\n!pip install scispacy\n!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport scispacy\nimport spacy\nimport en_core_sci_lg\nimport tensorflow as tf\nimport torch\nfrom transformers import *\n\nfrom scipy.spatial.distance import jensenshannon\n\nimport joblib\n\nfrom IPython.display import HTML, display\n\nfrom ipywidgets import interact, Layout, HBox, VBox, Box\nimport ipywidgets as widgets\nfrom IPython.display import clear_output\n\nfrom tqdm import tqdm\nfrom os.path import isfile\n\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and Prepare Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/cord-19-create-dataframe/cord19_df.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_texts = df.body_text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Latend Dirichlet Allocation"},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath = '../input/topic-modeling-finding-related-articles/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = en_core_sci_lg.load(disable=[\"tagger\", \"parser\", \"ner\"])\nnlp.max_length = 2000000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def spacy_tokenizer(sentence):\n    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load vectorizer\nvectorizer = joblib.load(filepath + 'vectorizer.csv')\ndata_vectorized = joblib.load(filepath + 'data_vectorized.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Load LDA Model\nlda = joblib.load(filepath + 'lda.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# Load previously computed topic distribution\ndoc_topic_dist = pd.read_csv(filepath + 'doc_topic_dist.csv')  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get Nearest Papers in Topic Space"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"is_covid19_article = df.body_text.str.contains('COVID-19|SARS-CoV-2|2019-nCov|SARS Coronavirus 2|2019 Novel Coronavirus')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def get_k_nearest_docs(doc_dist, k=5, lower=1950, upper=2020, only_covid19=False, get_dist=False):\n    '''\n    doc_dist: topic distribution (sums to 1) of one article\n    \n    Returns the index of the k nearest articles (as by Jensen–Shannon divergence in topic space). \n    '''\n    \n    relevant_time = df.publish_year.between(lower, upper)\n    \n    if only_covid19:\n        temp = doc_topic_dist[relevant_time & is_covid19_article]\n        \n    else:\n        temp = doc_topic_dist[relevant_time]\n         \n    distances = temp.apply(lambda x: jensenshannon(x, doc_dist), axis=1)\n    k_nearest = distances[distances != 0].nsmallest(n=k).index\n    \n    if get_dist:\n        k_distances = distances[distances != 0].nsmallest(n=k)\n        return k_nearest, k_distances\n    else:\n        return k_nearest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"task1 = [\"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\",\n\"Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\",\n\"Seasonality of transmission.\",\n\"Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\",\n\"Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\",\n\"Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\",\n\"Natural history of the virus and shedding of it from an infected person\",\n\"Implementation of diagnostics and products to improve clinical processes\",\n\"Disease models, including animal models for infection, disease and transmission\",\n\"Tools and studies to monitor phenotypic change and potential adaptation of the virus\",\n\"Immune response and immunity\",\n\"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\",\n \"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\",\n\"Role of the environment in transmission\"]\n\ntask2 = ['Data on potential risks factors',\n'Smoking, pre-existing pulmonary disease',\n'Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities',\n'Neonates and pregnant women',\n'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.',\n'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors', \n'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups',\n'Susceptibility of populations',\n'Public health mitigation measures that could be effective for control']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LDA Paragraph highlighting\nTo find how well each paragraph matches the given query, we compare the topic distribution of each individual paragraph with the one of the query. We do so by applying the LDA model trained on the corpus of papers."},{"metadata":{"trusted":true},"cell_type":"code","source":"def scoresLDA(paragraphs, query):\n    query_vectorized = vectorizer.transform([query])\n    query_topic_dist = lda.transform(query_vectorized)[0]\n\n    paragraphs_vectorized = vectorizer.transform(paragraphs)\n    paragraphs_topic_dist = lda.transform(paragraphs_vectorized)\n    \n    dists = [jensenshannon(paragraph_topic_dist, query_topic_dist) for paragraph_topic_dist in paragraphs_topic_dist]\n    min_dist, max_dist = min(dists), max(dists)\n    \n    return [((dist-min_dist) / (max_dist - min_dist))**8 for dist in dists]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def printMatch(query, score_fn):\n    query_vectorized = vectorizer.transform([query])\n    query_topic_dist = lda.transform(query_vectorized)[0]\n\n    recommended = get_k_nearest_docs(query_topic_dist, 1, 0, 20000, True)\n    article = all_texts[recommended[0]]\n    recommended = df.iloc[recommended[0]]\n\n    paragraphs = article.split(\"\\n\")\n\n    html = '<b>Query:</b><br />'\n    html += query\n    html += '<br /><br />'\n\n    html += '<b>Best match:</b><br />'\n    html += '<a href=\"' + recommended['url'] + '\" target=\"_blank\">'+ recommended['title'] + '</a>'\n    html += '<br/><br/>'\n\n    html += '<b>Article (important content is highlighted in red):</b>'\n    \n    scores = score_fn(paragraphs, query)\n    for paragraph, score in zip(paragraphs, scores):\n        color = 'rgb({},0,{})'.format(int(255 * score), int(255 * (1-score)))\n        html += '<p style=\"color:'+color+';\">'+paragraph+'</p>'\n\n    display(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = task1[0]\nprintMatch(query, scoresLDA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = task2[1]\nprintMatch(query, scoresLDA)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How the BERT model works\nWe use a pretrained BERT-base model. We frame the importance prediction as a next sentence prediction task, which is something that BERT was pretrained on during Google's training. The model outputs the probability that the given paragraph follows the given question and we use that as the importance measure."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertForNextSentencePrediction.from_pretrained('bert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BERT is not erudite (Ilsebill is split into subword tokens → BERT didn't read Günther Grass)\ntokenizer.tokenize(\"Ilsebill salzte nach.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_task(pair):\n    assert len(pair) == 2\n    # automatically takes care of adding [CLS] at the start and [SEP] in between and at the end\n    # and of building the attention mask (highlighting which tokens should be masked out during the self attention)\n    # and of building the token_type_id mask (highlighting which tokens belong to the question and to the paragraph)\n    encoding = tokenizer.encode_plus(pair[0], pair[1], add_special_tokens=True)\n    to_tensor = lambda x: tf.constant(x)[None, :]\n    return {\n        \"inputs\": to_tensor(encoding['input_ids']),\n        # useless for this exercise, but BERT still wants them. Normally tells BERT which tokens to mask out during the attention\n        \"attention_mask\": to_tensor(encoding['attention_mask']),\n        # tells BERT which tokens belong to the first and which to the second sequence\n        \"token_type_ids\": to_tensor(encoding['token_type_ids'])\n    }\n\n# 101 is the start ([CLS]) token, 102 is the separator\n# the token mask is 0 for the first sequence and 1 for the second\nencode_task([\"this is one sentence\", \"This is another sentence\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_sim_score(encoding):\n    output = model(**encoding)\n    # the model outputs the logits signifying the probability of the two tasks being sequiturs\n    return tf.math.softmax(output[0]).numpy()[0,0]\n\n# some simple tests\nlettuce = \"Lettuce (Lactuca sativa) is an annual plant of the daisy family, Asteraceae. It is most often grown as a leaf vegetable, but sometimes for its stem and seeds. Lettuce is most often used for salads, although it is also seen in other kinds of food, such as soups, sandwiches and wraps; it can also be grilled.\"\nmore_lettuce= \"One variety, the woju (t:萵苣/s:莴苣), or asparagus lettuce (Celtuce), is grown for its stems, which are eaten either raw or cooked. In addition to its main use as a leafy green, it has also gathered religious and medicinal significance over centuries of human consumption. Europe and North America originally dominated the market for lettuce, but by the late 20th century the consumption of lettuce had spread throughout the world. World production of lettuce and chicory for calendar year 2017 was 27 million tonnes, 56% of which came from China.\"\nlasso = \"In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces\"\n\nprint('lettuce and lettuce, should be true:', bert_sim_score(encode_task([lettuce, more_lettuce])))\nprint('lettuce and LASSO, doesnt mix well:', bert_sim_score(encode_task([lettuce, lasso])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scoresBERT(paragraphs, query):\n    score = lambda q, p: bert_sim_score(encode_task([q, p]))\n    return [score(query, para) for para in paragraphs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = task1[0]\nprintMatch(query, scoresBERT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query = task2[1]\nprintMatch(query, scoresBERT)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Question answering using SciBERT pretrained on SQuAD V2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we switch to pytorch since pretrained SciBERT is not available in TensorFlow\n# this model was also finetuned on the SQuAD dataset\n# as SQuAD V2 was used (which introduced unanswerable questions) the model should be able to not give back and answer if there is none\ntokenizer = AutoTokenizer.from_pretrained(\"ktrapeznikov/scibert_scivocab_uncased_squad_v2\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"ktrapeznikov/scibert_scivocab_uncased_squad_v2\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_qa(question, text):\n    encoding = tokenizer.encode_plus(question, text_pair = text, max_length=512)\n    # let BERT know when the second sequence starts by building the token_type embedding\n    return encoding\n\n# 101 is the start ([CLS]) token, 102 is the separator\n# the token mask is 0 for the first sequence and 1 for the second\nencode_qa(\"Why did the chicken cross the road\", \"To get to the other side\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def BERT_qa_answer(encoding):\n    input_ids = encoding['input_ids']\n    to_tensor = lambda x: torch.tensor([x])\n    start_scores, end_scores = model(to_tensor(input_ids), \n                                     token_type_ids=to_tensor(encoding['token_type_ids']),\n                                     attention_mask=to_tensor(encoding['attention_mask']))\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores) + 1])\n    # get back the words from subword tokens by merging them\n    return answer.replace(' ##', '')\n\nquestion = \"What increased the odds of in-hospital death?\"\ntext = \"191 patients (135 from Jinyintan Hospital and 56 from Wuhan Pulmonary Hospital) were included in this study, of whom 137 were discharged and 54 died in hospital. 91 (48%) patients had a comorbidity, with hypertension being the most common (58 [30%] patients), followed by diabetes (36 [19%] patients) and coronary heart disease (15 [8%] patients). Multivariable regression showed increasing odds of in-hospital death associated with older age (odds ratio 1·10, 95% CI 1·03–1·17, per year increase; p=0·0043), higher Sequential Organ Failure Assessment (SOFA) score (5·65, 2·61–12·23; p<0·0001), and d-dimer greater than 1 μg/mL (18·42, 2·64–128·55; p=0·0033) on admission. Median duration of viral shedding was 20·0 days (IQR 17·0–24·0) in survivors, but SARS-CoV-2 was detectable until death in non-survivors. The longest observed duration of viral shedding in survivors was 37 days.\"\nprint(question, 'answer:', BERT_qa_answer(encode_qa(question, text)))\nquestion = \"Why did the chicken cross the road?\"\ntext = '\"Why did the chicken cross the road?\" is a common riddle joke, with the answer being \"To get to the other side\". It is an example of anti-humor, in that the curious setup of the joke leads the listener to expect a traditional punchline, but they are instead given a simple statement of fact. \"Why did the chicken cross the road?\" has become iconic as an exemplary generic joke to which most people know the answer, and has been repeated and changed numerous times over the course of history.'\n# BERT was trained to put both start and end on [CLS] if it didn't find an answer\n# Regular BERT might have been able to answer this question since we took it from Wikipedia\n# But since SciBERT was trained on semanticscholar, it might not have the appropriate knowledge\nprint(question, 'answer:', BERT_qa_answer(encode_qa(question, text)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def printAnswers(question):\n    query_vectorized = vectorizer.transform([question])\n    query_topic_dist = lda.transform(query_vectorized)[0]\n\n    recommended = get_k_nearest_docs(query_topic_dist, 5, 0, 20000, True)\n    articles = [all_texts[i] for i in recommended]\n    recommendeds = [df.iloc[i] for i in recommended]\n    \n    \n    html = '<b>Question:</b><br />'\n    html += question\n    html += '<br /><br />'\n    \n    for article, recommended in zip(articles, recommendeds):\n        answered = False\n        paragraphs = article.split(\"\\n\")\n\n        answers = [\n            BERT_qa_answer(encode_qa(question, paragraph)) for paragraph in paragraphs\n        ]\n        answers = [answer for answer in answers \n                   if answer and \"[CLS]\" not in answer and \"[SEP]\" not in answer]\n        if answers:\n            html += '<b>Matched document:</b><br />'\n            html += '<a href=\"' + recommended['url'] + '\" target=\"_blank\">'+ recommended['title'] + '</a>'\n            html += '<br/><br/>'\n\n            html += '<b>Extracted answers:</b><br\\>'\n            for answer in answers:\n                html += '<div>' + answer + '</div>'\n            html += '<br/><br/>'\n    display(HTML(html))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printAnswers(\"Which risk factors exist?\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printAnswers(\"What is the average incubation period of the disease?\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"printAnswers(\"What increased the odds of in-hospital death?\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}