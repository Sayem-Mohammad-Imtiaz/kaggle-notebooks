{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-05-24T10:30:21.516504Z","iopub.status.busy":"2021-05-24T10:30:21.516187Z","iopub.status.idle":"2021-05-24T10:30:21.525739Z","shell.execute_reply":"2021-05-24T10:30:21.524643Z","shell.execute_reply.started":"2021-05-24T10:30:21.516476Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Perform necessary imports","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:30:23.082466Z","iopub.status.busy":"2021-05-24T10:30:23.08216Z","iopub.status.idle":"2021-05-24T10:30:23.085466Z","shell.execute_reply":"2021-05-24T10:30:23.084832Z","shell.execute_reply.started":"2021-05-24T10:30:23.082438Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:30:25.27986Z","iopub.status.busy":"2021-05-24T10:30:25.279336Z","iopub.status.idle":"2021-05-24T10:30:25.905268Z","shell.execute_reply":"2021-05-24T10:30:25.90446Z","shell.execute_reply.started":"2021-05-24T10:30:25.279816Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:30:30.382437Z","iopub.status.busy":"2021-05-24T10:30:30.382113Z","iopub.status.idle":"2021-05-24T10:30:30.407304Z","shell.execute_reply":"2021-05-24T10:30:30.406381Z","shell.execute_reply.started":"2021-05-24T10:30:30.382404Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check for null values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:30:32.247466Z","iopub.status.busy":"2021-05-24T10:30:32.247131Z","iopub.status.idle":"2021-05-24T10:30:32.263515Z","shell.execute_reply":"2021-05-24T10:30:32.26279Z","shell.execute_reply.started":"2021-05-24T10:30:32.247436Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**No null values are present. Hence we are good to go.**","metadata":{}},{"cell_type":"markdown","source":"# Check the distribution of positive and negative reviews","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=df['sentiment'])","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:30:33.772149Z","iopub.status.busy":"2021-05-24T10:30:33.771697Z","iopub.status.idle":"2021-05-24T10:30:33.929772Z","shell.execute_reply":"2021-05-24T10:30:33.928838Z","shell.execute_reply.started":"2021-05-24T10:30:33.77212Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:30:34.810568Z","iopub.status.busy":"2021-05-24T10:30:34.810228Z","iopub.status.idle":"2021-05-24T10:30:34.824059Z","shell.execute_reply":"2021-05-24T10:30:34.823256Z","shell.execute_reply.started":"2021-05-24T10:30:34.810532Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We have equal number of positive and negative reviews**","metadata":{}},{"cell_type":"markdown","source":"# Count the length of reviews","metadata":{}},{"cell_type":"code","source":"review_length = [len(review.split()) for review in df['review']]","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:30:37.557412Z","iopub.status.busy":"2021-05-24T10:30:37.557082Z","iopub.status.idle":"2021-05-24T10:30:38.155781Z","shell.execute_reply":"2021-05-24T10:30:38.155Z","shell.execute_reply.started":"2021-05-24T10:30:37.557384Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"review_len\"] = review_length","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:30:38.884399Z","iopub.status.busy":"2021-05-24T10:30:38.884076Z","iopub.status.idle":"2021-05-24T10:30:38.90148Z","shell.execute_reply":"2021-05-24T10:30:38.900562Z","shell.execute_reply.started":"2021-05-24T10:30:38.884373Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:30:39.582943Z","iopub.status.busy":"2021-05-24T10:30:39.582622Z","iopub.status.idle":"2021-05-24T10:30:39.593059Z","shell.execute_reply":"2021-05-24T10:30:39.592141Z","shell.execute_reply.started":"2021-05-24T10:30:39.582915Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))\nax1 = fig.add_subplot(122)\nsns.histplot(data=df[df[\"sentiment\"] == \"positive\"], x = \"review_len\", ax = ax1, kde=True, hue=\"sentiment\", bins=50)\ndescribe = df.review_len[df[\"sentiment\"] == \"positive\"].describe().to_frame().round(2)\n#print(describe)\n\nax2 = fig.add_subplot(121)\nax2.axis(\"off\")\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText=describe.values, rowLabels=describe.index,bbox=bbox, colLabels=describe.columns)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,10))\nax1 = fig.add_subplot(122)\nsns.histplot(data=df[df[\"sentiment\"] == \"negative\"], x = \"review_len\", ax = ax1, kde=True, hue=\"sentiment\", bins=50)\ndescribe = df.review_len[df[\"sentiment\"] == \"negative\"].describe().to_frame().round(2)\n#print(describe)\n\nax2 = fig.add_subplot(121)\nax2.axis(\"off\")\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText=describe.values, rowLabels=describe.index,bbox=bbox, colLabels=describe.columns)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wordcloud","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wordcloud for positive sentiments","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'positive'].review))                                                                       \nplt.imshow(wc, interpolation='bilinear')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Wordcloud for negative sentiments","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,20))\nwc = WordCloud(max_words = 200 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 'negative'].review))                                                                       \nplt.imshow(wc, interpolation='bilinear')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see words like \"br\" appearing in the word cloud. This shows that our dataset needs preprocessing.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Data cleaning","metadata":{}},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:30:52.196239Z","iopub.status.busy":"2021-05-24T10:30:52.195782Z","iopub.status.idle":"2021-05-24T10:30:53.024646Z","shell.execute_reply":"2021-05-24T10:30:53.023874Z","shell.execute_reply.started":"2021-05-24T10:30:52.196209Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Remove characters other than alphabets\n* Lemmatize the words so that all the words get reduced to their root words\n* Change the words to lowercase so that 'Girl' and 'girl' are not considered as two unique words.","metadata":{}},{"cell_type":"code","source":"# lemmatizer = WordNetLemmatizer()\n\n# doc = []\n\n# for i in range(len(df)):\n#     clean_text = re.sub('[^a-zA-Z]', ' ', df['review'][i] )\n#     clean_text = clean_text.lower()\n#     clean_text = clean_text.split()\n#     clean_text = [lemmatizer.lemmatize(word) for word in clean_text if word not in set(stopwords.words('english'))]\n#     clean_text = ' '.join(clean_text)\n#     doc.append(clean_text)","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.execute_input":"2021-05-24T10:30:53.908098Z","iopub.status.busy":"2021-05-24T10:30:53.90764Z","iopub.status.idle":"2021-05-24T10:30:53.911268Z","shell.execute_reply":"2021-05-24T10:30:53.910584Z","shell.execute_reply.started":"2021-05-24T10:30:53.908068Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentiment Mapping\nComputers understand only binary, so we convert 'positive' and 'negative' sentiments into binary form.\npositive: 0\nnegative: 1","metadata":{}},{"cell_type":"code","source":"label_sentiment = {'positive': 0, 'negative': 1}\n\ny = df.sentiment.map(label_sentiment)","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:30:55.767333Z","iopub.status.busy":"2021-05-24T10:30:55.766867Z","iopub.status.idle":"2021-05-24T10:30:55.775772Z","shell.execute_reply":"2021-05-24T10:30:55.774778Z","shell.execute_reply.started":"2021-05-24T10:30:55.767304Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We define 3 functions for carrying out three different tasks:\n1. Remove HTML tags from the reviews\n2. Remove special characters other than alphabets from the review\n3. Remove stopwords from the review","metadata":{}},{"cell_type":"code","source":"def clean_html(text):\n    clean_text = re.sub('<.*>', '', text)\n    return clean_text\n    \ndef clean_spcl_chars(text):\n    clean_text = re.sub('[^a-zA-Z]', ' ', text) \n    return clean_text\n\ndef remove_stopwords(text):\n    clean_text = []\n    text = text.split()\n    for word in text:\n        if word not in stopwords.words('english'):\n            clean_text.append(word)\n    return ' '.join(clean_text)\n    ","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:30:57.834937Z","iopub.status.busy":"2021-05-24T10:30:57.834366Z","iopub.status.idle":"2021-05-24T10:30:57.83988Z","shell.execute_reply":"2021-05-24T10:30:57.839047Z","shell.execute_reply.started":"2021-05-24T10:30:57.8349Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove the HTML tags","metadata":{}},{"cell_type":"code","source":"df[\"clean_review\"] = df.review.apply(clean_html)","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:31:04.191253Z","iopub.status.busy":"2021-05-24T10:31:04.190938Z","iopub.status.idle":"2021-05-24T10:31:04.310465Z","shell.execute_reply":"2021-05-24T10:31:04.30945Z","shell.execute_reply.started":"2021-05-24T10:31:04.191226Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:31:04.869472Z","iopub.status.busy":"2021-05-24T10:31:04.869136Z","iopub.status.idle":"2021-05-24T10:31:04.882199Z","shell.execute_reply":"2021-05-24T10:31:04.881325Z","shell.execute_reply.started":"2021-05-24T10:31:04.869437Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove special characters","metadata":{}},{"cell_type":"code","source":"df.clean_review = df.clean_review.apply(clean_spcl_chars)\ndf.head()","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:31:06.475483Z","iopub.status.busy":"2021-05-24T10:31:06.474853Z","iopub.status.idle":"2021-05-24T10:31:08.391258Z","shell.execute_reply":"2021-05-24T10:31:08.390612Z","shell.execute_reply.started":"2021-05-24T10:31:06.475448Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove stopwords","metadata":{}},{"cell_type":"code","source":"df.clean_review = df.clean_review.apply(remove_stopwords)\ndf.head()","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:31:10.545135Z","iopub.status.busy":"2021-05-24T10:31:10.544509Z","iopub.status.idle":"2021-05-24T10:42:41.908748Z","shell.execute_reply":"2021-05-24T10:42:41.90792Z","shell.execute_reply.started":"2021-05-24T10:31:10.545099Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lemmatize the words","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    text = text.split()\n    text = [lemmatizer.lemmatize(word) for word in text]\n    return ' '.join(text)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['lemmatized_review'] = df['clean_review'].apply(lemmatize_text)\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save the cleaned review dataframe as pickle object, so that I don't have to run the cleaning process everytime I open this notebook","metadata":{}},{"cell_type":"code","source":"df.to_pickle('cleaned_df.pkl')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vectorize review\nWe will use Term frequency - inverse document frequency (Tf-idf) as for vectorizing the reviews","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.execute_input":"2021-05-24T07:36:28.909753Z","iopub.status.busy":"2021-05-24T07:36:28.90931Z","iopub.status.idle":"2021-05-24T07:36:28.914321Z","shell.execute_reply":"2021-05-24T07:36:28.913305Z","shell.execute_reply.started":"2021-05-24T07:36:28.909713Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df['lemmatized_review']\n#X = TfidfVectorizer().fit_transform(X)","metadata":{"execution":{"iopub.execute_input":"2021-05-24T08:29:53.356653Z","iopub.status.busy":"2021-05-24T08:29:53.356234Z","iopub.status.idle":"2021-05-24T08:29:53.362424Z","shell.execute_reply":"2021-05-24T08:29:53.360728Z","shell.execute_reply.started":"2021-05-24T08:29:53.356592Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.execute_input":"2021-05-24T08:29:58.715108Z","iopub.status.busy":"2021-05-24T08:29:58.714574Z","iopub.status.idle":"2021-05-24T08:29:58.745326Z","shell.execute_reply":"2021-05-24T08:29:58.744151Z","shell.execute_reply.started":"2021-05-24T08:29:58.715064Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.execute_input":"2021-05-24T08:29:59.838705Z","iopub.status.busy":"2021-05-24T08:29:59.837809Z","iopub.status.idle":"2021-05-24T08:29:59.847983Z","shell.execute_reply":"2021-05-24T08:29:59.846188Z","shell.execute_reply.started":"2021-05-24T08:29:59.838646Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Pipeline\nWe will create a pipeline for diffrent classifiers","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, log_loss","metadata":{"execution":{"iopub.execute_input":"2021-05-24T08:30:01.688125Z","iopub.status.busy":"2021-05-24T08:30:01.687698Z","iopub.status.idle":"2021-05-24T08:30:01.694452Z","shell.execute_reply":"2021-05-24T08:30:01.693302Z","shell.execute_reply.started":"2021-05-24T08:30:01.688088Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe1 = Pipeline([('tfidf', TfidfVectorizer()), ('clf', MultinomialNB() )])\npipe2 = Pipeline([('tfidf', TfidfVectorizer()), ('clf', LinearSVC() )])\npipe3 = Pipeline([('tfidf', TfidfVectorizer()), ('clf', RandomForestClassifier() )])","metadata":{"execution":{"iopub.execute_input":"2021-05-24T08:33:44.136089Z","iopub.status.busy":"2021-05-24T08:33:44.135543Z","iopub.status.idle":"2021-05-24T08:33:44.144274Z","shell.execute_reply":"2021-05-24T08:33:44.142644Z","shell.execute_reply.started":"2021-05-24T08:33:44.136048Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipes = [pipe1, pipe2, pipe3]\nlog = []\nfor i,pipe in enumerate(pipes):\n    name = f'pipe_{i}'\n    pipe.fit(X_train, y = y_train)\n    y_pred = pipe.predict(X_test)\n    print('*****RESULTS*****')\n    acc = accuracy_score(y_test, y_pred).round(2)\n    log.append((name, acc))\n    print(classification_report(y_test, y_pred))\n    print(pipe.score(X_test, y_test))","metadata":{"execution":{"iopub.execute_input":"2021-05-24T09:01:04.300073Z","iopub.status.busy":"2021-05-24T09:01:04.299564Z","iopub.status.idle":"2021-05-24T09:03:34.490887Z","shell.execute_reply":"2021-05-24T09:03:34.489165Z","shell.execute_reply.started":"2021-05-24T09:01:04.300028Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log","metadata":{"execution":{"iopub.execute_input":"2021-05-24T10:08:25.223852Z","iopub.status.busy":"2021-05-24T10:08:25.223405Z","iopub.status.idle":"2021-05-24T10:08:25.281892Z","shell.execute_reply":"2021-05-24T10:08:25.28056Z","shell.execute_reply.started":"2021-05-24T10:08:25.223741Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_list = ['MultinomialNB', 'LinearSVC', 'RandomForset']","metadata":{"execution":{"iopub.execute_input":"2021-05-24T08:43:49.398435Z","iopub.status.busy":"2021-05-24T08:43:49.39793Z","iopub.status.idle":"2021-05-24T08:43:52.519867Z","shell.execute_reply":"2021-05-24T08:43:52.518494Z","shell.execute_reply.started":"2021-05-24T08:43:49.398388Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = [log[i][1].round(2) for i in range(len(log))]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(x=clf_list, y = accuracy)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LinearSVC has the highest accuracy of 86%","metadata":{}},{"cell_type":"markdown","source":"**If you have any doubt or suggestion, please feel free to comment**","metadata":{}},{"cell_type":"markdown","source":"**If you found my notebook, please do upvote. Thank you :)**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}