{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction:\n<br>\nThis is my final project on Supervised Learning. My Model building was okay, unfortunately I got mistaken the balance status of my data hence my evaluation metric was in the wrong direction. Next step, I will take time to improve this after end of my course in end December.","metadata":{}},{"cell_type":"markdown","source":"## ML Final Project","metadata":{}},{"cell_type":"markdown","source":"**Scenario:** You work at a multinational bank that is aiming to increase it's market share in \nEurope. Recently, it has been noticed that the number of customers using the banking \nservices has declined, and the bank is worried that existing customers have stopped \nusing them as their main bank. <br> \n\nAs a data scientist, you are tasked with finding out the \nreasons behind customer churn (when a customer stops using them as the main bank) and to predict customer churn. <br> \n\nThe marketing team, \nin particular, is interested in your findings and want to better understand existing \ncustomer behavior and possibly predict customer churn. Your results will help the \nmarketing team to use their budget wisely to target potential churners. To achieve \nthis objective, in this exercise, you will import the banking data (Churn_Modelling.csv) \nprovided by the bank and do some machine learning to solve their problem.","metadata":{}},{"cell_type":"markdown","source":"Data dictionary\n\n- CustomerID: Unique ID of each customer\n- CredRate: Credit Score of the customer \n- Geography: Country customer is from \n- Gender\n- Age\n- Tenure: How long customer has been with bank \n- balance: The amount of money customer has/had with bank\n- Prod Number: Number of products customer has with bank \n- HasCrCard: Does customer have credit card\n- ActMem: Is customer active member / whether customer was actively engaged with bank activities.\n- Estimated salary: Annual estimated salary of customer \n- Exited: Whether customer has churned (1 is yes)","metadata":{}},{"cell_type":"markdown","source":"# 1) Introduction","metadata":{}},{"cell_type":"markdown","source":"The aim for this study outcome is to explore and identify what could be the potential factors that influence/impact behind customer churn. Then build a prediction model to help predict and classify the potential customer that may be churn. Based on the performance, analysis and provide insights to the business on what kind of actions can be taken to minimize the chances of customer churn in order to help increase it's market share in Europe going forward.","metadata":{}},{"cell_type":"markdown","source":"# 2) Data understanding/ Preprocessing","metadata":{}},{"cell_type":"code","source":"#import libraries\n\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import Normalizer, StandardScaler\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Ridge, Lasso\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, r2_score, mean_absolute_error, mean_squared_error, roc_auc_score, roc_curve\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/customer-churn/Churn_Modelling.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Understanding Data","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[data.isna().any(axis=1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(data.corr(),cmap='coolwarm', annot=True, ax=ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above heatmap, we could get a sense the correlation of the data features. Surprisingly these features their correlation aren't that high. Only \"Age\" seems has a higher number of correlation of \"Exited\" feature.<br>\n<br>","metadata":{}},{"cell_type":"code","source":"#Checking how many countries inclusive\ndata.Geography.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the numberic of Tenure\ndata.Tenure.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the numberic of Prod Number\ndata[\"Prod Number\"].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking the indication for Has and do not has Credit Card, just 1 & 0\ndata.HasCrCard.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Don't see how CustomerId is helpful on the ML model, decided to drop it\ndata = data.drop([\"CustomerId\"], axis = 1)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handling Missing Value and check their number of values","metadata":{}},{"cell_type":"markdown","source":"Since the number of missung Value is that small amount, decided to drop all the rows with Missing Value.","metadata":{}},{"cell_type":"code","source":"data = data.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.Gender.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Gender'].value_counts()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Geography'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Exited'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['HasCrCard'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['Prod Number'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['ActMem'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Inspect again if any na value\n\ndata.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = 'Exited', 'Not Exited'\nsizes = data['Exited'].value_counts()\nexplode = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, explode = explode,autopct='%1.1f%%',\n        shadow=True, startangle=60)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objs as go\n\nvalue = data['Exited'].value_counts()\n\nfig = go.Figure(data=[go.Pie(labels=['Exited','Not Exited'],\n                            values=data['Exited'].value_counts(),\n                            textinfo = 'value + percent + label')])\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the Pie Charts above, it gave us a quick overview of the ratio of customer churn and indeed the percentage of \"Exited\" group is high and almost close to 80% of the total number.","metadata":{}},{"cell_type":"code","source":"# to get offline plotly/HTML\nimport plotly.offline as offline\noffline.init_notebook_mode(connected=True)\n \noffline.plot(fig)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x = 'Exited', hue = 'Gender', data = data)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tried to explore what kind of plot will able to give me the overview of each of the features relation to the churn. I found countplot is the best to do comparison so I take it to the next step and compare a few more features which created the plots below.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2,3, figsize = (15,10))\n\nsns.countplot(x='Geography', hue = 'Exited',data = data, ax = axes[0,0])\nsns.countplot(x='Gender', hue = 'Exited',data = data, ax = axes[0,1])\nsns.countplot(x='Tenure', hue = 'Exited',data = data, ax = axes[0,2])\nsns.countplot(x='Prod Number', hue = 'Exited',data = data, ax = axes[1,0])\nsns.countplot(x='HasCrCard', hue = 'Exited',data = data, ax = axes[1,1])\nsns.countplot(x='ActMem', hue = 'Exited',data = data, ax = axes[1,2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From these 6 Features countplots, here are my hypothesis on their correlation that in relates to the customer churn. <br>\n\n- <b>Geography</b>\n    - We can see that the ratio for Germany is smaller ratio of the differences compare to France\n- <b>Gender</b>\n    - Male has the higher not exited rate and lower exited rate compared to Female\n- <b>Tenure</b>\n    - The comparison on this feature rather balances, this could suggest that tenure may not be the factors of customer churn. Will find out more along the process.\n- <b>Prod Number</b>\n    - At a glance, it looks like while the Prod Number 1 & 2 of not exited are way higher, at the same time these numbers may also not be necessary the cause of customer churn.\n- <b>HasCrCard</b>\n    - Despite \"has credit card members\" has higher numbers of not exited customer but at the same time the exited customer also pretty high.\n- <b>ActMem</b>\n    - Surprising the non-Active Member exited is much higher than the Active Member, it may be worth a in depth study by the business.","metadata":{}},{"cell_type":"markdown","source":"As Countplot hard to present the correlation of Age and the likelihood of customer churn so decided to use the regplot to study the data.","metadata":{}},{"cell_type":"code","source":"AgePlt = sns.regplot(x=\"Age\", y = \"Exited\", data = data, logistic=True)\n\n# plotting the horiozontal & Vertical line to give a sense on if at threshold of 0.5, customer with age above 62 are likelihood to churn.\nAgePlt.axhline(0.5, ls='--', linewidth=1, color='red')\nAgePlt.axvline(62, ls='--', linewidth=1, color='red')\n","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see from the plot, the higher of the age the likelihood to churn/exit from the bank. The prediction models should able to find out if this feature is the biggest factor for customer churn.","metadata":{}},{"cell_type":"markdown","source":"### Data preprocessing: Encoding","metadata":{}},{"cell_type":"markdown","source":"Breaking down the factors to understand what kinds of data are they:\n- CredRate: Credit Score of the customer <b> - Integer</b> \n- Geography: Country customer is from <b>- Category</b>\n  - 1 = France\n  - 2 = Germany\n  - 2 = Spain\n- Gender  <b>- Category</b>\n  - 1 = male\n  - 2 = female\n- Age<b> - Integer</b> \n- Tenure: How long customer has been with bank <b> - Integer</b> \n- balance: The amount of money customer has/had with bank<b> - Integer</b> \n- Prod Number: Number of products customer has with bank <b> - Integer/Category</b> \n- HasCrCard: Does customer have credit card <b>- Category</b>\n  - 0 = No\n  - 1 = Yes\n- ActMem: Is customer active member / whether customer was actively engaged with bank activities. <b>- Category</b>\n  - 0 = No\n  - 1 = Yes\n- Estimated salary: Annual estimated salary of customer <b> - Integer</b> \n- Exited: Whether customer has churned (1 is yes) <b>- Category</b>\n  - 0 = Not Exited\n  - 1 = Exited","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data = pd.get_dummies(data, columns=[\"Geography\",\n                                           \"Gender\",\n                                           \"Prod Number\",\n                                          \"HasCrCard\",\n                                          \"ActMem\"])\nfinal_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data.rename({\"Geography_France\": \"France\", \n                   \"Geography_Germany\": \"Germany\",\n                   \"Geography_Spain\": \"Spain\",\n                   \"Gender_Female\":\"Female\",\n                   \"Gender_Male\":\"Male\",\n                   \"Prod Number_1\":\"ProdNum01\",\n                   \"Prod Number_2\":\"ProdNum02\",\n                   \"Prod Number_3\":\"ProdNum03\",\n                   \"Prod Number_4\":\"ProdNum04\",\n                   \"HasCrCard_0\":\"NoCrCard\",\n                   \"HasCrCard_1\":\"HasCrCard\",\n                   \"ActMem_0\": \"NotActMem\",\n                   \"ActMem_1\": \"ActMem\",\n                  }, axis='columns', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the kinds of data, decided to encode all the categorical factors but since \"HasCrCard\",\"ActMem\" and\"Exited\" columns already in binary number so I leave it as it is. <br>\n<br>\n\"Prod Number\" is kind of integar & categorical in my opinion, nevertheless decided to encode it to increase the accuracy.\n","metadata":{}},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"## Scaling/Feature Engineering","metadata":{}},{"cell_type":"code","source":"X = final_data.drop(columns=[\"Exited\"])\ny = final_data[\"Exited\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting Dataset to train set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state = 123)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss = StandardScaler()\n\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3 & 4) Machine learning model training & Evaluation/Results","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"Choosing Logistic Regression simply it is commonly use to predict the target of categorical or binary and in which case our dataset and the outcome trying to achieve suggest that this algorithm is one of the simplest to begin with.","metadata":{}},{"cell_type":"code","source":"classifier = LogisticRegression(random_state=123)\n\nclassifier.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(X_test)\ny_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix \ncm = confusion_matrix(y_test, y_pred)\n\nsns.heatmap(cm, annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LG_acc = accuracy_score(y_pred, y_test)*100\nprint(\"Logistic Regression Accuracy: {}\".format(LG_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LG_auc = roc_auc_score(y_test, y_pred)*100\nprint(\"Logistic Regression AUC: {}\".format(LG_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of the Classification Evaluation, the outcome of precision and recall aren't that great for True Negative values. If the prediction is to focus on only True Positive then these evaluation will be good enough. <br>\n\nSo I decided to try Accuracy and ROC/AUC. Accuracy focus on true results among the total numbers and it is good for balanced dataset and not skewed. AUC is good to measuring balanced performance of positive and negative classes. As we can see between Accuracy and AUC, Accuracy has the better result compare to AUC for predicting on customer churn. <br>","metadata":{}},{"cell_type":"markdown","source":"<br>","metadata":{}},{"cell_type":"markdown","source":"## Support Vector Machine ","metadata":{}},{"cell_type":"markdown","source":"#### Why not XGBoost but SVM?\nAt first, I was considering XGBoost but XGBoost is generally use on more large and complicated data prediction. Whereas our customer churn data is not that large and complicated, rather straightforward that consist more of categorical/classifications. <br>\n<br>\nIn the other hand, SVM method objective is to find a hyperplane which maximized the separation between data of different classes. I see this method can be effective as our customer churn data has quite some number of features and fit our objective of to predict if customer will Exited or Not Exited (2 different classes). \n","metadata":{}},{"cell_type":"code","source":"svc_model = SVC(random_state=123)\n\nsvc_model.fit(X_train, y_train)\npred_svm = svc_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix \ncm = confusion_matrix(y_test, pred_svm)\n\nsns.heatmap(cm, annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, pred_svm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM_acc = accuracy_score(pred_svm, y_test) * 100\nprint(\"SVM accuracy: {}\".format(SVM_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM_auc = roc_auc_score(y_test,pred_svm)*100\nprint(\"SVM AUC: {}\".format(SVM_auc))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Support Vector Machines looks pretty similar but better than Logistic Regression at the accuracy score. Let's look at my 3rd choice of the algorithm - Random Forest and it's prediction.","metadata":{}},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"markdown","source":"Random Forest adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model.","metadata":{}},{"cell_type":"code","source":"#Fitting Random Forest Classification\nrf_classifier = RandomForestClassifier(random_state=123)\n\nrf_classifier.fit(X_train, y_train)\n\npredict_rf =rf_classifier.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix \ncm = confusion_matrix(y_test, predict_rf)\n\nsns.heatmap(cm, annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, predict_rf))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Accuracy score\n\nrf_acc = accuracy_score(predict_rf, y_test)*100\nprint(\"Random Forest Accuracy: {}\".format(rf_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AUC\nrf_auc = roc_auc_score(y_test, predict_rf)*100\nprint(\"Random Forest AUC: {}\".format(rf_auc))","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Out of the 3 algorithms that I used for the prediction models, Random Forest did the best out of them. So I want to spent a bit more focus on see how can I make improvement to the prediction. <br>\n\nNext, I'm going to tune the model for improvement by using Cross Validation(GridSearch CV).\n<br>\n<br>","metadata":{}},{"cell_type":"markdown","source":"### Parameter Tuning","metadata":{}},{"cell_type":"markdown","source":"I choose GridSearch CV is because this Cross Validation model will gather all the possible combinations of parameter values are evaluated and the best combination is retained","metadata":{}},{"cell_type":"code","source":"rf_classifier = RandomForestClassifier(random_state=123)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_dict_rf = {'n_estimators' : [10, 20, 40, 50, 100], \n                 'max_depth': [5, 6, 7, 8, 9, 10]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_rf = GridSearchCV(param_grid= param_dict_rf, estimator= rf_classifier, cv=5, verbose= 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_rf.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_rf.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_rf = RandomForestClassifier(max_depth= 10, n_estimators=100, random_state=123)\n\nmodel_rf = best_rf.fit(X_train, y_train)\n\nresult_rf = model_rf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix \ncm = confusion_matrix(y_test, result_rf)\n\nsns.heatmap(cm, annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, result_rf))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_acc = accuracy_score(result_rf, y_test)*100\nprint(\"Random Forest Accuracy: {}\".format(rf_acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_auc = roc_auc_score(y_test, result_rf)*100\nprint(\"Random Forest AUC: {}\".format(rf_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although the improvement is not much but as we can see, the model prediction definitely improved compare to the one without the tuning.\n\nRandom Forest algorithm can help us to predict the chances of customer churn up to 86.5%. Overall it is pretty good model for the business to begin their study and prediction of customer churn.","metadata":{}},{"cell_type":"markdown","source":"### Feature Importance","metadata":{}},{"cell_type":"code","source":"len(model_rf.feature_importances_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_rf.feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feat_importances = pd.Series(model_rf.feature_importances_, index= X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances = pd.DataFrame(model_rf.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\nfeature_importances","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the Plot or Data Frame, it helps us to understand what are the main factors of customer churn based on the dataset features.\n<br>\n<br>\nWe can see and suggest that Age, Customers have 2 Products with the bank and Balance in the Bank are the top 3 most crucial factors of customer churn where their importance rates are pretty high compared to other features.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"- Random Forest Model is good prediction model for tackle this problem matter. But it can be improve better with some boosting methods. \n- If given more times, first thing will want to try on will be different ensemble methods and boosting algorithms to refine and improve the models. Also other different ML algorithms as those I chosen here consider the basic algorithms\n- 2 of my favourite Biggest takeaways from this work\n    - First ever ML model from scratch.\n    - Research and reference in Kaggle how others build their model and create their notebook for better understanding.\n","metadata":{}}]}