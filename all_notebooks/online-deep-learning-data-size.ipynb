{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"! pip install onn -q","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! ls ../input/human-activity-recognition-with-smartphones","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# arr = np.load('../input/cd1-dataset/cd1.npz')\n# arr.files","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X = arr['x_train']\n# y = arr['y_train']\n# y = np.argmax(y, axis=1)\n# X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/human-activity-recognition-with-smartphones/train.csv')\nlabel_map = {k:v for v, k in enumerate(set(train['Activity']))}\ntrain['Activity']  = train['Activity'].map(label_map)\n\ntest = pd.read_csv('../input/human-activity-recognition-with-smartphones/test.csv')\ntest['Activity']  = test['Activity'].map(label_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_map","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train['Activity'].values\nX_train = train.drop(['Activity'], axis=1).values\n\ny_val = test['Activity'].values\nX_val = test.drop(['Activity'], axis=1).values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_val.shape, y_train.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.vstack((X_train, X_val))\ny = np.concatenate((y_train, y_val))\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from onn.OnlineNeuralNetwork import ONN\nfrom tqdm.notebook import tqdm\n\n\ndef run(pct):\n        X_train, X_val, y_train, y_val = train_test_split(X, y , test_size=pct)\n        print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n        onn_network = ONN(features_size=562, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=500,\\\n                  n_classes=6, use_cuda=False)\n        for i in tqdm(range(len(X_train))):\n            onn_network.partial_fit(X_train[i].reshape(1, -1), y_train[i].reshape(1), show_loss=False)\n        y_pred = onn_network.predict(X_val)\n        print('Train Accuracy score = ', accuracy_score(onn_network.predict(X_train), y_train))\n        print('Valid Accuracy score = ', accuracy_score(y_pred, y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test size fraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"run(0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(0.6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run(0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}