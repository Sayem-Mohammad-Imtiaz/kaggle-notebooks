{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage import io\nfrom skimage.transform import  resize\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from os import listdir\nimport shutil\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = \"../input/hackerearth-deep-learning-identify-the-snake-breed/dataset/train.csv\"\ntest_csv = \"../input/hackerearth-deep-learning-identify-the-snake-breed/dataset/test.csv\"\ntrain_image_path = \"../input/hackerearth-deep-learning-identify-the-snake-breed/dataset/train\"\ntest_image_path = \"../input/hackerearth-deep-learning-identify-the-snake-breed/dataset/test\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_img_list = pd.read_csv(  train_csv, sep = \",\" )\ntest_df_img_list =  pd.read_csv(  test_csv, sep = \",\" )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_img_list.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure( figsize = (40,15), dpi = 30)\nsns.countplot( data = train_df_img_list, x = \"breed\", order = train_df_img_list[\"breed\"].value_counts() .index )\nplt.xticks( rotation = 90, fontsize = 30 )\nplt.yticks( fontsize = 30)\nplt.xlabel (\"breed\", fontsize = 30)\nplt.ylabel( \"Count\", fontsize = 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ( \"Number of class =\", len( train_df_img_list[\"breed\"].unique())  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Creating Data generator '''\ndef image_read ( path, img_width = 100,img_height = 100 ):\n    \n    data = io.imread( path)\n    return (resize(data/ 255,(img_width, img_height) ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_list = os.listdir(train_image_path)\nplt.imshow ( image_read ( path = train_image_path +\"/\"+file_list[0],\n                        img_width = 100,\n                        img_height = 100)\n           )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data generator, each call will spits out data to save memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_generator( train_file_name , path, img_height = 100, img_width = 100, batch_size = 300, cat_col = \"breed\", image_col =\"image_id\", file_format = \".jpg\" ):\n    \n    df = shuffle( pd.read_csv(train_file_name, sep =\",\") )\n    unique_breed  = list ( df[cat_col].unique() )\n    \n    while True:\n        \n        for each_chunk in pd.read_csv(train_file_name, chunksize=batch_size, sep =\",\"):\n\n            data  = np.zeros ( ( each_chunk.shape[0] , img_width, img_height, 3 )   )\n            label = np.zeros ( ( each_chunk.shape [0]  , len(unique_breed) )  ) \n\n            each_chunk= shuffle ( each_chunk )\n\n            for each_row in range( each_chunk.shape[0] ):\n\n                each_image =  path +\"/\"+ each_chunk.iloc[each_row][0] + file_format\n\n                each_breed =  each_chunk.iloc[each_row][1]\n\n                each_image_data = image_read (path = each_image, img_width = img_width ,img_height = img_height  )\n\n                data[each_row, :, :, 0 ] =  each_image_data[:,:,0]/255\n\n                data[each_row, :, :, 1 ] =  each_image_data[:,:,1]/255\n\n                data[each_row, :, :, 2 ] =  each_image_data[:,:,2]/255\n\n                label[each_row,unique_breed.index(each_breed) ] =  1\n\n            yield ( data, label )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Starting to build model"},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Required Tensorflow library for preocessing '''\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras import preprocessing\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_width = 90\nimg_height = 90\nbatch_size = 1000\nnumber_of_channel = 3 # RGB\nnumber_of_class = train_df_img_list[\"breed\"].unique()\nif ( train_df_img_list.shape[0] % batch_size ) ==0 :\n    \n    steps_per_epoch = int( train_df_img_list.shape[0]/batch_size)\n    \nelse:\n    steps_per_epoch = (train_df_img_list.shape[0] //batch_size ) + 1\n    \nnp.random.random(30)\ntf.random.set_seed(30)\n\ntrain_data_with_label = data_generator(train_file_name = train_csv , \n                            path =train_image_path, \n                            img_height = img_height, \n                            img_width = img_width, \n                            batch_size = batch_size, \n                            cat_col = \"breed\", \n                            image_col =\"image_id\", \n                            file_format = \".jpg\" )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model = Sequential()\n\ncnn_model.add ( Conv2D ( filters = 16, kernel_size = (2,2), strides = (1,1), padding = \"valid\",activation =\"relu\", input_shape = ( img_width,img_height, number_of_channel  ) ) )\ncnn_model.add (Conv2D  ( filters = 32, kernel_size = (2,2), strides = (2,2), padding = \"valid\",activation = \"elu\" )  )\ncnn_model.add (Conv2D  ( filters = 32, kernel_size = (2,2), strides = (1,1), padding = \"valid\",activation = \"elu\" )  )\ncnn_model.add (Dropout(0.2))\n\ncnn_model.add (Conv2D  ( filters = 32, kernel_size = (2,2), strides = (2,2), padding = \"valid\",activation = \"elu\" )  )\ncnn_model.add (Dropout(0.2))\ncnn_model.add (Conv2D  ( filters = 32, kernel_size = (2,2), strides = (2,2), padding = \"valid\",activation = \"elu\" )  )\n#cnn_model.add (Conv2D  ( filters = 64, kernel_size = (2,2), strides = (1,1), padding = \"valid\",activation = \"elu\" )  )\ncnn_model.add ( Flatten() )\ncnn_model.add ( Dense (units = 128, activation = \"relu\",use_bias = True  ) )\ncnn_model.add (Dropout(0.2))\ncnn_model.add ( Dense (units = 64, activation = \"relu\",use_bias = True  ) )\ncnn_model.add (Dropout(0.1))\ncnn_model.add ( Dense (units = 35, activation = \"softmax\",use_bias = False  ) )\ncnn_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model.compile (optimizer= \"adam\", loss = \"categorical_crossentropy\" , metrics = [\"categorical_accuracy\"] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnn_model.fit_generator(  train_data_with_label, steps_per_epoch=steps_per_epoch,epochs = 10, shuffle=  True ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"steps_per_epoch","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}