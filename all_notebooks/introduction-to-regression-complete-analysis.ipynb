{"cells":[{"metadata":{"_uuid":"4c47c9cd2d611be6ab3a7eb13eae9cad0bf7fd11"},"cell_type":"markdown","source":"# <center> Introduction to Regression - Complete Analysis on Wine data </center>"},{"metadata":{"_uuid":"41fa541624d8df2d758dc58a74dec5319b6df3e9"},"cell_type":"markdown","source":"- Report can be found [here](https://github.com/Abhishekmamidi123/Regression-Analysis)."},{"metadata":{"_uuid":"a49255d537ee5b21cc4575e3717dead35c7543ee"},"cell_type":"markdown","source":"## Problem Definition\n-  A wine dataset is provided. The task is to analyze data and build a regression model to predict the quality of the wine."},{"metadata":{"_uuid":"55df2ce4bc6be7229279667b9a6c83e973b95998"},"cell_type":"markdown","source":"## Abstract \n- The main goal of this report is to extract maximum knowledge from the Wine data in different ways. The data is analyzed and the plots are shown. A regression model is built to predict the quality of wine using the features provided. The assumptions of regression are also checked."},{"metadata":{"_uuid":"43d7291212244ed07cc6569e70237ee7723dcbd2"},"cell_type":"markdown","source":"## Methodology \n1. Description of data \n2. Preprocess data \n3. Visualize data \n4. Build a Regression model \n5. Check Regression Assumptions \n6. Goodness of fit \n7. Compare different Regression methods"},{"metadata":{"_uuid":"fe74b766376345464d5c42c818e49ab357ebd728"},"cell_type":"markdown","source":"## Description of data \n1. **Name of the data**: Wine data from UCI Machine learning repository \n2. **Number of data points**: 4898 \n3. **Number of features**: 11 \n4. **Target attribute**: Quality of wine \n5. **Range of target attribute**: 3 to 9"},{"metadata":{"trusted":true,"_uuid":"b75b65ffa7713ccf99460b46c7a909a08c67dd01","_kg_hide-input":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\nfrom IPython.display import HTML, display\nfrom IPython.core import display as ICD\nfrom plotly.offline import init_notebook_mode, iplot\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nimport numpy as np\nimport statsmodels.api as sm\nimport pylab\nimport scipy as sp\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn import neighbors\nfrom sklearn import linear_model\n\ninit_notebook_mode(connected=True)\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc0d6ef99373bb550b41ad54f04a039a36201672"},"cell_type":"markdown","source":"### Data"},{"metadata":{"trusted":true,"_uuid":"dcdab861908ff2daa0fb6a4274287657b6d1d87b","_kg_hide-input":true},"cell_type":"code","source":"PATH = '../input/'\nfilename = 'winequality-white.csv'\nwhite_data = pd.read_csv(PATH + filename)\n\ndata_head = white_data.head()\ncolorscale = [[0, '#4d004c'],[.5, '#f2e5ff'],[1, '#ffffff']]\ndf_table = ff.create_table(round(data_head.iloc[:,[0,1,2,3,4,5]], 3), colorscale=colorscale)\npy.iplot(df_table, filename='wine_quality')\ndf_table = ff.create_table(round(data_head.iloc[:,[6,7,8,9,10,11]], 3), colorscale=colorscale)\npy.iplot(df_table, filename='wine_quality')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dad833177c478c75db58c21dd9959edbbec27c8d"},"cell_type":"markdown","source":"### Features \n1. Fixed acidity \n2. Volatile acidity \n3. Citric acid \n4. Residual sugar \n5. Chlorides \n6. Free sulfur dioxide \n7. Total sulfur dioxide \n8. Density \n9. pH \n10. Sulphates \n11. Alcohol\n\n### Target Attribute \n- Quality of wine"},{"metadata":{"_uuid":"f592693aa2bf1578cc349f958d5ec90b1d6e13a4"},"cell_type":"markdown","source":"### Distribution of Target Attribute"},{"metadata":{"trusted":true,"_uuid":"53220262efd8244c7be9de082f73efa2c9677e12","_kg_hide-input":true},"cell_type":"code","source":"value_counts = white_data.quality.value_counts()\ntarget_counts = pd.DataFrame({'quality': list(value_counts.index), 'value_count': value_counts})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"488712f53d8ec26e9524d5728e31cece6d398bc7","_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\ng = sns.barplot(x='quality', y='value_count', data=target_counts, capsize=0.3, palette='spring')\ng.set_title(\"Frequency of target class\", fontsize=15)\ng.set_xlabel(\"Quality\", fontsize=13)\ng.set_ylabel(\"Frequency\", fontsize=13)\ng.set_yticks([0, 500, 1000, 1500, 2000, 2500])\nfor p in g.patches:\n    g.annotate(np.round(p.get_height(),decimals=2), \n                (p.get_x()+p.get_width()/2., p.get_height()), \n                ha='center', va='center', xytext=(0, 10), \n                textcoords='offset points', fontsize=14, color='black')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"931d9b36bf9344edf7bb36640095847b215b41a2"},"cell_type":"markdown","source":"#### Analysis:\n- The quality of wine ranges from 3 to 9 \n- The data is not balanced. The number of data points having quality 6 is very high and quality 3 and 9 are very low. \n- This may affect the model."},{"metadata":{"_uuid":"2e2e71c00f6a9412fb0040ff7c7ea7272ea70b93"},"cell_type":"markdown","source":"### Distribution of target attribute - Box plot"},{"metadata":{"trusted":true,"_uuid":"01e243e30528a351b9b292d3582a9e39b8ce6c92","_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10,3))\nsns.boxplot(data=white_data['quality'], orient='horizontal', palette='husl')\nplt.title(\"Distribution of target variable\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"663ee94802b4f4cefba55df0ba148665d9139590"},"cell_type":"markdown","source":"### Describe data"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d6c52d44ccb6f296ed729966da1e05fc33f05d1a"},"cell_type":"code","source":"white_data.describe().drop(columns=['quality'])\n\n# data_head = white_data.describe().drop(columns=['quality'])\n# data_head.columns = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',\n#        'chlorides', 'free_SO2', 'total_SO2', 'density',\n#        'pH', 'sulphates', 'alcohol']\n# colorscale = [[0, '#4d004c'],[.5, '#f2e5ff'],[1, '#ffffff']]\n# df_table = ff.create_table(round(data_head.iloc[:,[0,1,2,3,4,5,6,7,8,9,10]], 3), colorscale=colorscale)\n# py.iplot(df_table, filename='wine_quality')\n# df_table = ff.create_table(round(data_head.iloc[:,[6,7,8,9,10]], 3), colorscale=colorscale)\n# py.iplot(df_table, filename='wine_quality')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"991e6877c44a3c358afcffe7d8faa5319c443994"},"cell_type":"markdown","source":"## Preprocess data"},{"metadata":{"trusted":true,"_uuid":"ab00732b51732a04fd4b5a6b4a8894b60d444f24","_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.boxplot(data=white_data.drop(columns=['quality']), orient='horizontal', palette='husl')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c8a619e230ad926f4975b34a5d01237d4ac28c8"},"cell_type":"markdown","source":"#### Analysis:\n- If we observe the above boxplot, the range of features is different from each other. \n- We can normalize the data. All the variables range from 0 to 1 after normalization and don’t lose any information. "},{"metadata":{"_uuid":"969af041dd2deffd42bcf5a718ef74cc7230be55"},"cell_type":"markdown","source":"### Distribution of features - After normalization \n- This looks good than before and very easy to understand the distribution of data."},{"metadata":{"trusted":true,"_uuid":"fd0763434e8b8b458c93ae3878e6143c2b912bfa","_kg_hide-input":true},"cell_type":"code","source":"y = white_data['quality']\nwhite_data = white_data.loc[:, ~white_data.columns.isin(['quality'])]\n\nscaler = MinMaxScaler()\nscaled_values = scaler.fit_transform(white_data)\nwhite_data.loc[:,:] = scaled_values\n\nwhite_data['quality'] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd076be2f97ded4917033f54b1f9cfa360c3ce1a","_kg_hide-input":true},"cell_type":"code","source":"data_head = white_data.head()\ncolorscale = [[0, '#4d004c'],[.5, '#f2e5ff'],[1, '#ffffff']]\ndf_table = ff.create_table(round(data_head.iloc[:,[0,1,2,3,4,5]], 3), colorscale=colorscale, )\npy.iplot(df_table, filename='wine_quality')\ndf_table = ff.create_table(round(data_head.iloc[:,[6,7,8,9,10,11]], 3), colorscale=colorscale, )\npy.iplot(df_table, filename='wine_quality')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba1a8da079c96a07b9e70bca2eb9da95ca3b9dd5","_kg_hide-input":true},"cell_type":"code","source":"columns = list(white_data.columns)\nnew_column_names = []\nfor col in columns:\n    new_column_names.append(col.replace(' ', '_'))\nwhite_data.columns = new_column_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1f12443ed404abebbc4b32ca07b10f9ceca63d9","_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.boxplot(data=white_data.drop(columns=['quality']), orient='horizontal', palette='husl')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4323c1ca21cc83b3ee9c78dd85e1e71eef595608"},"cell_type":"markdown","source":"## Visualize data\n### Correlation between features "},{"metadata":{"trusted":true,"_uuid":"ae6b8aefbe870bff6c0b493173e0b4239587384f","_kg_hide-input":true},"cell_type":"code","source":"corr_matrix = white_data.corr().abs()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"113ef81dd5399cb0f53378e1448f8d477acf57a7"},"cell_type":"markdown","source":"#### Analysis:\n- The correlation between “density” and “residual sugar” is 0.84. \n- The correlation between “alcohol” and “density” is 0.78. \n- The correlation between “total sulfur dioxide” and “free sulfur dioxide” \nis 0.62. \n- These are the three pairs of features having a high correlation(>0.5)."},{"metadata":{"_uuid":"8be145f92b9fafac0340cfd3cc7ff62a4c33b247"},"cell_type":"markdown","source":"### Distribution of each feature"},{"metadata":{"trusted":true,"_uuid":"d1c88a50e2936e3ec7e040b2e94f40839106da3a","_kg_hide-input":true},"cell_type":"code","source":"features = white_data.copy(deep=True)\nfeatures['quality'] = y.astype('str').map({'3': 'Three', '4': 'Four', '5': 'Five', '6': 'Six', '7': 'Seven', '8': 'Eight', '9': 'Nine'})\nf, axes = plt.subplots(4, 3, figsize=(15, 10), sharex=True)\nsns.distplot(features[\"fixed_acidity\"], rug=False, color=\"skyblue\", ax=axes[0, 0])\nsns.distplot(features[\"volatile_acidity\"], rug=False, color=\"olive\", ax=axes[0, 1])\nsns.distplot(features[\"citric_acid\"], rug=False, color=\"gold\", ax=axes[0, 2])\nsns.distplot(features[\"residual_sugar\"], rug=False, color=\"teal\", ax=axes[1, 0])\nsns.distplot(features[\"chlorides\"], rug=False, ax=axes[1, 1])\nsns.distplot(features[\"free_sulfur_dioxide\"], rug=False, color=\"red\", ax=axes[1, 2])\nsns.distplot(features[\"total_sulfur_dioxide\"], rug=False, color=\"skyblue\", ax=axes[2, 0])\nsns.distplot(features[\"density\"], rug=False, color=\"olive\", ax=axes[2, 1])\nsns.distplot(features[\"pH\"], rug=False, color=\"gold\", ax=axes[2, 2])\nsns.distplot(features[\"sulphates\"], rug=False, color=\"teal\", ax=axes[3, 0])\nsns.distplot(features[\"alcohol\"], rug=False, ax=axes[3, 1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34c2921956c91acf63acaf9b08d09d149517bb09"},"cell_type":"markdown","source":"#### Analysis:\n- If we observe the distribution of all features, they follow a Normal distribution. \n- There is some fluctuation in the features “sulphates” and “alcohol”. "},{"metadata":{"trusted":true,"_uuid":"8f4cb39334a933ca6f285ec8e87bdb6fa2cfad1e","_kg_hide-input":true},"cell_type":"code","source":"upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.5)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3d2e8730f172ea406cf2fdaa8493aeabf9d2548","_kg_hide-input":true},"cell_type":"markdown","source":"### Pair plot between features\n- This is to understand the relation between features. "},{"metadata":{"trusted":true,"_uuid":"d3d7efbb1b83acf2ee8ba255b85a2e2f0aa4617e","_kg_hide-input":true},"cell_type":"code","source":"features = white_data.copy(deep=True)\nfeatures['quality'] = y.astype('str').map({'3': 'Three', '4': 'Four', '5': 'Five', '6': 'Six', '7': 'Seven', '8': 'Eight', '9': 'Nine'})\nsns.pairplot(features, diag_kind='kde', palette='husl', hue='quality')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f84fc52c7a7bf2234b458a833fabe4e2143901dd"},"cell_type":"markdown","source":"#### Analysis:\n- From this plot, we can see how different features are correlated with each other. \n- In the above plot, the features that are plotted on the x-axis and y-axis are in the given order itself. "},{"metadata":{"_uuid":"960b287bee6c04a17ef461d7207eff4a90121f64"},"cell_type":"markdown","source":"### Pair plot between correlated features"},{"metadata":{"trusted":true,"_uuid":"cc3aa7595eae46bff994751aa138e35ba876ce51","_kg_hide-input":true},"cell_type":"code","source":"features = white_data.copy(deep=True)\nfeatures['quality'] = y.astype('str').map({'3': 'Three', '4': 'Four', '5': 'Five', '6': 'Six', '7': 'Seven', '8': 'Eight', '9': 'Nine'})\nsns.pairplot(features, vars=to_drop, diag_kind='kde', palette='husl', hue='quality')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b574c7544ad2edad11359c161fda482c30c6d87"},"cell_type":"markdown","source":"#### Analysis:\n- As we have seen above in the correlation plot, there is a high correlation(>0.5) in between some of the features. \n- Here, we can visualize how these features are correlated. \n- If we observe carefully, we cannot separate the data points of different quality easily, because all the data points of various quality are overlapped."},{"metadata":{"_uuid":"2dcf076fcf0078a501cc81d90cda9eab8ac39829"},"cell_type":"markdown","source":"## Build a Regression model\n### Linear Regression using Gradient Descent\n- The method of Linear Regression that finds the coefficients of different features using Gradient Descent optimization, is fit to the data to see how independent variables are contributing to the dependent variable. \n- The below plot shows the coefficients of features(contribution). "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f78b78a36b3e711c92b1701bdbae8368ee1d6ab7"},"cell_type":"code","source":"model_reg = LinearRegression().fit(white_data.drop(columns=['quality']), y)\ny_true = white_data.quality\ny_pred = model_reg.predict(white_data.drop(columns=['quality']))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"75d51f75798c282bb036609575d51b63c71c338b"},"cell_type":"code","source":"column_names = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',\n       'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']\nregression_coefficient = pd.DataFrame({'Feature': column_names, 'Coefficient': model_reg.coef_}, columns=['Feature', 'Coefficient'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d9d673454a3dbb64305e71a8f0025dc00066cee3"},"cell_type":"code","source":"column_names = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar',\n       'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']\n\nplt.figure(figsize=(15,5))\ng = sns.barplot(x='Feature', y='Coefficient', data=regression_coefficient, capsize=0.3, palette='spring')\ng.set_title(\"Contribution of features towards target variable\", fontsize=15)\ng.set_xlabel(\"Feature\", fontsize=13)\ng.set_ylabel(\"Degree of Coefficient\", fontsize=13)\ng.set_yticks([-8, -6, -4, -2, 0, 2, 4, 6, 8])\ng.set_xticklabels(column_names)\nfor p in g.patches:\n    g.annotate(np.round(p.get_height(),decimals=2), \n                (p.get_x()+p.get_width()/2., p.get_height()), \n                ha='center', va='center', xytext=(0, 10), \n               textcoords='offset points', fontsize=14, color='black')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f3533bc36857ab7354339651cdb75371a2e9495"},"cell_type":"markdown","source":"### Ordinary Least Squares(OLS)\n- In statistics, ordinary least squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. \n- The OLS method corresponds to minimizing the sum of squared differences between the observed and predicted values. This minimization leads to the estimators of the parameters of the model. \n- The results of OLS Regression are shown below: "},{"metadata":{"trusted":true,"_uuid":"523a1c92ca2e0bf44c9611b9e48fb9d031f6b73d","_kg_hide-input":true},"cell_type":"code","source":"model_ols = ols(\"\"\"quality ~ fixed_acidity \n                        + volatile_acidity \n                        + citric_acid\n                        + residual_sugar \n                        + chlorides \n                        + free_sulfur_dioxide\n                        + total_sulfur_dioxide \n                        + density \n                        + pH \n                        + sulphates \n                        + alcohol\"\"\", data=white_data).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"849c9907e4ed25ce764dc513db4d86a1081b2f08","_kg_hide-input":true},"cell_type":"code","source":"model_summary = model_ols.summary()\nHTML(\n(model_ols.summary()\n    .as_html()\n    .replace('<th>Dep. Variable:</th>', '<th style=\"background-color:#c7e9c0;\"> Dep. Variable: </th>')\n    .replace('<th>Model:</th>', '<th style=\"background-color:#c7e9c0;\"> Model: </th>')\n    .replace('<th>Method:</th>', '<th style=\"background-color:#c7e9c0;\"> Method: </th>')\n    .replace('<th>No. Observations:</th>', '<th style=\"background-color:#c7e9c0;\"> No. Observations: </th>')\n    .replace('<th>  R-squared:         </th>', '<th style=\"background-color:#aec7e8;\"> R-squared: </th>')\n    .replace('<th>  Adj. R-squared:    </th>', '<th style=\"background-color:#aec7e8;\"> Adj. R-squared: </th>')\n    .replace('<th>coef</th>', '<th style=\"background-color:#ffbb78;\">coef</th>')\n    .replace('<th>std err</th>', '<th style=\"background-color:#c7e9c0;\">std err</th>')\n    .replace('<th>P>|t|</th>', '<th style=\"background-color:#bcbddc;\">P>|t|</th>')\n    .replace('<th>[0.025</th>    <th>0.975]</th>', '<th style=\"background-color:#ff9896;\">[0.025</th>    <th style=\"background-color:#ff9896;\">0.975]</th>'))\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4a7d6de42348c75f681ee3f583f6b0bf8fb3593"},"cell_type":"markdown","source":"#### Analysis:\n- The R-squared is 0.282 and Adjusted R-squared is 0.280. \n- If p-value > 0.05, we fail to reject the null hypothesis, otherwise we reject the null hypothesis.\n- The p-values of the features “citric acid” and “chlorides”, is greater than 0.05. Also, the contribution of these features is very less. \n- So, we can remove the remove the features from the data. \n- Let’s fit the model again and see if there would be any change. The results of OLS Regression are shown after the removal of these two features from the data. "},{"metadata":{"trusted":true,"_uuid":"128264ad201e9748628e5c4bf744e10fbc25bc7e","_kg_hide-input":true},"cell_type":"code","source":"model_ols = ols(\"\"\"quality ~ fixed_acidity \n                        + volatile_acidity \n                        + residual_sugar \n                        + free_sulfur_dioxide\n                        + total_sulfur_dioxide \n                        + density \n                        + pH \n                        + sulphates \n                        + alcohol\"\"\", data=white_data).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66fc9002487c4126508876bc37645ac40fe8ccfc","_kg_hide-input":true},"cell_type":"code","source":"model_summary = model_ols.summary()\nHTML(\n(model_ols.summary()\n    .as_html()\n    .replace('<th>Dep. Variable:</th>', '<th style=\"background-color:#c7e9c0;\"> Dep. Variable: </th>')\n    .replace('<th>Model:</th>', '<th style=\"background-color:#c7e9c0;\"> Model: </th>')\n    .replace('<th>Method:</th>', '<th style=\"background-color:#c7e9c0;\"> Method: </th>')\n    .replace('<th>No. Observations:</th>', '<th style=\"background-color:#c7e9c0;\"> No. Observations: </th>')\n    .replace('<th>  R-squared:         </th>', '<th style=\"background-color:#aec7e8;\"> R-squared: </th>')\n    .replace('<th>  Adj. R-squared:    </th>', '<th style=\"background-color:#aec7e8;\"> Adj. R-squared: </th>')\n    .replace('<th>coef</th>', '<th style=\"background-color:#ffbb78;\">coef</th>')\n    .replace('<th>std err</th>', '<th style=\"background-color:#c7e9c0;\">std err</th>')\n    .replace('<th>P>|t|</th>', '<th style=\"background-color:#bcbddc;\">P>|t|</th>')\n    .replace('<th>[0.025</th>    <th>0.975]</th>', '<th style=\"background-color:#ff9896;\">[0.025</th>    <th style=\"background-color:#ff9896;\">0.975]</th>'))\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be4e4b23561f20ca559046ca3dccf7d942b76aec"},"cell_type":"markdown","source":"- There is no change in the values of R-squared and there is an increase of 0.001 of Adjusted R-squared. So, there is no harm in removing those features from the data. Also, the contribution of these features to predict the quality of the wine is very less as shown before. Now, we are left with 9 features."},{"metadata":{"trusted":true,"_uuid":"e4c4bcb1257f93330fede8ce5e5cff13299ff3c9","_kg_hide-input":true},"cell_type":"code","source":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\ndef goodness(y_true, y_pred):\n    mape = mean_absolute_percentage_error(y_true, y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    r_squared = r2_score(y_true, y_pred)\n    return mape, mse, r_squared","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cde2b560748880c0b529b2ccc83f115aaec2ee7"},"cell_type":"markdown","source":"### Contribution of features after removing two features "},{"metadata":{"trusted":true,"_uuid":"1d2968d842f3e22e4c78e1c6fc7c275efd3d008d","_kg_hide-input":true},"cell_type":"code","source":"model = LinearRegression().fit(white_data.drop(columns=['quality', 'citric_acid', 'chlorides']), y)\ny_true = white_data.quality\ny_pred = model.predict(white_data.drop(columns=['quality', 'citric_acid', 'chlorides']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3847eb8593aeea93d864b0e9975e003c6d1bbc6","_kg_hide-input":true},"cell_type":"code","source":"column_names = ['fixed_acidity', 'volatile_acidity', 'residual_sugar',\n       'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']\nregression_coefficient = pd.DataFrame({'Feature': column_names, 'Coefficient': model.coef_}, columns=['Feature', 'Coefficient'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dc40fd24981995559e61da159efd562d1d6078b","_kg_hide-input":true},"cell_type":"code","source":"column_names = ['fixed_acidity', 'volatile_acidity', 'residual_sugar',\n       'free_SO2', 'total_SO2', 'density',\n       'pH', 'sulphates', 'alcohol']\n\nplt.figure(figsize=(15,5))\ng = sns.barplot(x='Feature', y='Coefficient', data=regression_coefficient, capsize=0.3, palette='spring')\ng.set_title(\"Contribution of features towards target variable\", fontsize=15)\ng.set_xlabel(\"Feature\", fontsize=13)\ng.set_ylabel(\"Degree of Coefficient\", fontsize=13)\ng.set_yticks([-8, -6, -4, -2, 0, 2, 4, 6, 8])\ng.set_xticklabels(column_names)\nfor p in g.patches:\n    g.annotate(np.round(p.get_height(),decimals=2), \n                (p.get_x()+p.get_width()/2., p.get_height()), \n                ha='center', va='center', xytext=(0, 10), \n               textcoords='offset points', fontsize=14, color='black')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4adffde21a1332421a4da0a40f2b055a6a993592"},"cell_type":"markdown","source":"- The coefficients of features have changed a little bit. "},{"metadata":{"_uuid":"6254a28b6b9c13ccf0b43bfd248f38afe31ecd19"},"cell_type":"markdown","source":"## Check Regression Assumptions \n1. Linearity \n2. Homoscedasticity \n3. Correlation of errors \n4. Normality of errors. "},{"metadata":{"_uuid":"448fee193dd45f3200ecb29e1eddb3de659256fa"},"cell_type":"markdown","source":"- Let’s check each condition using the predicted values and the errors/residuals. \n- Residuals are the difference between “true value” and the “predicted value”. \n\n**Note**: The two features “chlorides” and “citric acid” are removed from the data."},{"metadata":{"_uuid":"314be7595c77ab957dfcb87d59334f4bc4f6da66"},"cell_type":"markdown","source":"### Linearity \n- Plot partial regression plots to check linearity."},{"metadata":{"trusted":true,"_uuid":"96fff599a71e58b958be4df93788b02bc3366258","_kg_hide-input":true},"cell_type":"code","source":"error = y_true - y_pred\nerror_info = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'error': error}, columns=['y_true', 'y_pred', 'error'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d6bbe0af8a1c7ee8b70cf0322a7dfbb579b07f8","_kg_hide-input":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,12))\nfig = sm.graphics.plot_partregress_grid(model_ols, fig=fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71e8ddd156562d1e31db297c48fc8b6021b364f2"},"cell_type":"markdown","source":"### Analysis:\n- If we observe carefully, all the partial residual plots between the independent variable and dependent variable are linear.  \n- Linearity condition is satisfied. "},{"metadata":{"_uuid":"5e1a0f819903cde81068772d58b5323f3f849d17"},"cell_type":"markdown","source":"### Homoskedasticity \n- To check homoskedasticity, we plot the residuals vs predicted values/fitted values. \n- If we see any kind of funnel shape, we can say that there is heteroskedasticity. "},{"metadata":{"trusted":true,"_uuid":"c81201750043a01d961963734f1390f933726f93","_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(8,5))\ng = sns.regplot(x=\"y_pred\", y=\"error\", data=error_info, color='blue')\ng.set_title('Check Homoskedasticity', fontsize=15)\ng.set_xlabel(\"predicted values\", fontsize=13)\ng.set_ylabel(\"Residual\", fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8ce40e9173b68d9567f7dc36fe33972ed7ddddd"},"cell_type":"markdown","source":"#### Analysis:\n- The points are not random. Also, we can see the shape of a funnel to the right, which confirms that there is heteroskedasticity. \n- It means that the variance of Y across all X is not the same. \n- We can conclude that, Homoskedasticity condition doesn’t hold in this case. "},{"metadata":{"_uuid":"0c20a8cc21b73fdb29c3f7766c89ed380942c79a"},"cell_type":"markdown","source":"### Correlation of errors\n- If there is no correlation between errors, then the model is good."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"52b5664508e368d165ccc8960611b0a23b92b26d"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,5))\nax = error_info.error.plot()\nax.set_title('Uncorrelated errors', fontsize=15)\nax.set_xlabel(\"Data\", fontsize=13)\nax.set_ylabel(\"Residual\", fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc82094aa2d86b89f7ab848a1eaa609a32afba21"},"cell_type":"markdown","source":"#### Analysis:\n- If we observe, there is no correlation/pattern between errors. It is purely random. \n- We can also check this condition using the Durbin-Watson test: \n    - If DW = 2, then there is no correlation. \n    - If DW < 2, then the errors are positively correlated. \n    - If DW > 2, then the errors are negatively correlated. \n- If we perform Durbin-Watson test, the value of DW is 1.621. \n- According to the test, we can say that the errors are positively correlated.  \n- However, this is a point estimate for perfect uncorrelation of errors(DW=2). So, we won’t get DW as 2 on real data. If it around 2, then we can conclude that the errors are uncorrelated. "},{"metadata":{"_uuid":"099b5c1eade8285676ba09334d0bb742dffd949c"},"cell_type":"markdown","source":"### Normality of error terms\n- This can be checked by plotting probability probability plot(p-p plot) or Quantile-Quantile plot(Q-Q plot). "},{"metadata":{"_uuid":"c95520a3e11e65bffcf11c8b74c3ca0cc3fa0c62"},"cell_type":"markdown","source":"#### Probability-Probability plot"},{"metadata":{"trusted":true,"_uuid":"661c4c4b4967f09a1b6c81667e5f5ea9a05cd505","_kg_hide-input":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6,4))\n_ = sp.stats.probplot(error_info.error, plot=ax, fit=True)\nax.set_title('Probability plot', fontsize=15)\nax.set_xlabel(\"Theoritical Qunatiles\", fontsize=13)\nax.set_ylabel(\"Ordered Values\", fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3545a717b2c1316a16e4a1eb29648237d6b3443b"},"cell_type":"markdown","source":"### Quantile-Quantile plot"},{"metadata":{"trusted":true,"_uuid":"77b7f239a6bffe90b73b8ab54fbd4642d83d670d","_kg_hide-input":true},"cell_type":"code","source":"ax = sm.qqplot(error_info.error, line='45')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0736bc48aaaaa80f8b640a1b18c2ea9682581f2"},"cell_type":"markdown","source":"#### Analysis:\n- If we observe the above plots, we can conclude that the errors are following a Normal distribution, because the plot shows the fluctuation around the line and there is not much deviation. \n- The graph is linear."},{"metadata":{"trusted":true,"_uuid":"1f98a3507b95a8755be6515f204ac30a9f8c9b2e"},"cell_type":"markdown","source":"### Linear Regression Assumption: Multicollinearity\n- If the independent variables are independent of each other, then we say there is no multicollinearity. \n- This can be tested in different ways: \n1. **Correlation plot**: If we observe the plot, there is multicollinearity between variables. \n2. **Variation Inflation Factor**: With VIF > 10 there is an indication that multicollinearity may be present. With VIF > 100 there is certainly multicollinearity among the variables. \n- We can conclude that multicollinearity among variables exists. \n- If multicollinearity is found in the data, centring the data, that is deducting the mean score might help to solve the problem. Other alternatives to tackle the problems is conducting a **factor analysis**/**Principal Component Analysis(PCA)** and rotating the factors to ensure the independence of the factors in the linear regression analysis. \n- We can do the same analysis after applying PCA on the data. We can see some improvements in the model as there won’t be any multicollinearity. \n- The results of OLS Regression are shown below after transforming the feature variables using **PCA**. "},{"metadata":{"trusted":true,"_uuid":"53692b0c1941b6faf7aba146e8b977b7a5918fad","_kg_hide-input":true},"cell_type":"code","source":"pca = PCA()\ntransform_X = pca.fit_transform(white_data.drop(columns=['quality']), white_data.quality)\n\ncolumns = ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7',\n            'feature_8', 'feature_9', 'feature_10', 'feature_11']\ntransform_df = pd.DataFrame.from_records(transform_X)\ntransform_df.columns = columns\ntransform_df['quality'] = white_data.quality","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42e08094b37c340086af3c51a96cca94e56003ad","_kg_hide-input":true},"cell_type":"code","source":"model_ols_new = ols(\"\"\"quality ~ feature_1 \n                        + feature_2 \n                        + feature_3\n                        + feature_4 \n                        + feature_5 \n                        + feature_6 \n                        + feature_7 \n                        + feature_8 \n                        + feature_9 \n                        + feature_10 \n                        + feature_11\"\"\", data=transform_df).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7173cc5cfb6fc2347c8d0300c4e632c03a992c6","_kg_hide-input":true},"cell_type":"code","source":"model_summary = model_ols_new.summary()\nHTML(\n(model_ols_new.summary()\n    .as_html()\n    .replace('<th>Dep. Variable:</th>', '<th style=\"background-color:#c7e9c0;\"> Dep. Variable: </th>')\n    .replace('<th>Model:</th>', '<th style=\"background-color:#c7e9c0;\"> Model: </th>')\n    .replace('<th>Method:</th>', '<th style=\"background-color:#c7e9c0;\"> Method: </th>')\n    .replace('<th>No. Observations:</th>', '<th style=\"background-color:#c7e9c0;\"> No. Observations: </th>')\n    .replace('<th>  R-squared:         </th>', '<th style=\"background-color:#aec7e8;\"> R-squared: </th>')\n    .replace('<th>  Adj. R-squared:    </th>', '<th style=\"background-color:#aec7e8;\"> Adj. R-squared: </th>')\n    .replace('<th>coef</th>', '<th style=\"background-color:#ffbb78;\">coef</th>')\n    .replace('<th>std err</th>', '<th style=\"background-color:#c7e9c0;\">std err</th>')\n    .replace('<th>P>|t|</th>', '<th style=\"background-color:#bcbddc;\">P>|t|</th>')\n    .replace('<th>[0.025</th>    <th>0.975]</th>', '<th style=\"background-color:#ff9896;\">[0.025</th>    <th style=\"background-color:#ff9896;\">0.975]</th>'))\n)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_uuid":"52a8095392a1ae221b5379f4b790d1517a378e2d"},"cell_type":"markdown","source":"#### Analysis:\n- If we observe the p-values of transformed features, all the p-values are less than 0.05, which shows that multicollinearity problem is solved. "},{"metadata":{"trusted":true,"_uuid":"91ed091f1956772d5193b3edbf27dd5276f092a4","_kg_hide-input":true},"cell_type":"code","source":"r2_linear_regression = model_ols_new.rsquared\n\nmodel_ridge=linear_model.Ridge()\nmodel_ridge.fit(white_data.drop(columns=['quality']),white_data.quality)\ny_predict_ridge = model_ridge.predict(white_data.drop(columns=['quality']))\nr2_ridge = r2_score(y_true, y_predict_ridge)\n\nmodel_lasso=linear_model.Lasso()\nmodel_lasso.fit(white_data.drop(columns=['quality']),white_data.quality)\ny_predict_lasso = model_lasso.predict(white_data.drop(columns=['quality']))\nr2_score(y_true, y_predict_lasso)\n\nn_neighbors=5\nknn=neighbors.KNeighborsRegressor(n_neighbors,weights='uniform')\nknn.fit(white_data.drop(columns=['quality']),white_data.quality)\ny_predict_knn=knn.predict(white_data.drop(columns=['quality']))\nr2_knn = r2_score(y_true, y_predict_knn)\n\nreg = linear_model.BayesianRidge()\nreg.fit(white_data.drop(columns=['quality']),white_data.quality)\ny_pred_reg=reg.predict(white_data.drop(columns=['quality']))\nr2_bayesian = r2_score(y_true, y_pred_reg)\n\ndec = tree.DecisionTreeRegressor(max_depth=6)\ndec.fit(white_data.drop(columns=['quality']),white_data.quality)\ny1_dec=dec.predict(white_data.drop(columns=['quality']))\nr2_dt = r2_score(y_true, y1_dec)\n\nsvm_reg=svm.SVR()\nsvm_reg.fit(white_data.drop(columns=['quality']),white_data.quality)\ny1_svm=svm_reg.predict(white_data.drop(columns=['quality']))\nr2_svm = r2_score(y_true, y1_svm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b5551f50b00642f26b2fae976907f725caf2150"},"cell_type":"markdown","source":"## Discussion\n- We can do alot of changes to improve the accuracy of the model. Some of the conditions above are violated. If we can transform the variables accordingly, we can achieve good results. If we observe the R-squared score, it is 0.282. It is able to explain only 28% of the variance, which is poor. So, there is a scope to apply different methods to get better models. \n- I have applied different popular Regression methods on the data to compare the results we got. The below table shows the comparison of the R-squared of different methods."},{"metadata":{"trusted":true,"_uuid":"7c7e3af4b4eded53ab6aaeaa1b382a3c132c8be3","_kg_hide-input":true},"cell_type":"code","source":"r2_list = [r2_linear_regression, r2_ridge, r2_knn, r2_dt, r2_bayesian, r2_svm]\nr2_names = ['Linear Regression', 'Ridge Regression', 'KNN', 'Decision Tree', 'Bayesian Regression', 'SVM']\n\ncol = {'R-squared':r2_list, 'Method':r2_names}\ndf = pd.DataFrame(data=col, columns=['Method', 'R-squared'])\n\ndata_head = df\ncolorscale = [[0, '#4d004c'],[.5, '#f2e5ff'],[1, '#ffffff']]\ndf_table = ff.create_table(round(data_head.iloc[:,[0,1]], 3), colorscale=colorscale)\npy.iplot(df_table, filename='wine_quality')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e9a48716ae0f38113a5975fb22055e12a078cc2"},"cell_type":"markdown","source":"- If we compare R-square, KNN outperformed on all the Regression methods. Also, all the methods performed better than LinearRegression. So, we can conclude there is alot of scope to improve the Linear Regression model. "},{"metadata":{"trusted":true,"_uuid":"cb6ae9e736f2aca410619b4c1f0e5109f7c4d23a"},"cell_type":"markdown","source":"## Conclusion  \n- We have visualized wine dataset in all possible ways and they are shown in the form of plots.  \n- A Linear Regression model is built to predict the target variable. Some improvements have been done on the model by removing some features that are not contributing and the data is transformed using Principal Component Analysis(PCA). \n- The test of assumptions for Linear Regression is also checked and they are analyzed properly. \n- In the end, Linear Regression is compared with different other popular Regression methods, in which KNN performed well owhen compared to others. \n- There is alot of scope to increase the performance of Linear Regression model. \n- We can increase the samples to build a robust model. Also, we can add some more features that contribute to the wine quality. "},{"metadata":{"trusted":true,"_uuid":"bf21c737540a0766bdc0993235555bb116b095ce"},"cell_type":"markdown","source":"### Thank you for reading till the end.\n### Do Upvote if you find it useful. Feedback is always welcome. Please let me know in the comment section below."},{"metadata":{"trusted":true,"_uuid":"551232f2ec4a7e1099b62e544ba5aa5c17ed7e92"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}