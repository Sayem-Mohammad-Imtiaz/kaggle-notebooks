{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom sklearn.ensemble import AdaBoostClassifier\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport sys\nimport os\nimport random\nfrom pathlib import Path\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom keras import optimizers\nfrom keras.initializers import Constant\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization,LeakyReLU\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nfrom keras.utils import to_categorical\nfrom keras.layers.advanced_activations import LeakyReLU, PReLU\nimport tensorflow_addons as tfa\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_PATH = '../input/chinese-mnist/data/data/'\nIMAGE_WIDTH = 64\nIMAGE_HEIGHT = 64\nIMAGE_CHANNELS = 1\nRANDOM_STATE = 42\nTEST_SIZE = 0.2\nVAL_SIZE = 0.2\nBATCH_SIZE = 32\nNO_EPOCHS = 50\nPATIENCE = 5\nVERBOSE = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfc = pd.read_csv('/kaggle/input/chinese-mnist/chinese_mnist.csv')\nimages = np.array([np.asarray(Image.open('/kaggle/input/chinese-mnist/data/data/input_%d_%d_%d.jpg'%(x['suite_id'], x['sample_id'], x['code']))) for x in dfc.iloc])\npd.DataFrame(zip([9,10,100,1000,10000,100000000,0,1,2,3,4,5,6,7,8],dfc['character'].unique()), columns=['valor', 'caractere']).sort_values(by='valor')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = pd.read_csv('../input/chinese-mnist/chinese_mnist.csv')\nprint(data_df.shape) \ndata_df.sample(100).head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_files = list(os.listdir(IMAGE_PATH))\nprint(\"Number of image files: {}\".format(len(image_files)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_file_name(x):\n    \n    file_name = f\"input_{x[0]}_{x[1]}_{x[2]}.jpg\"\n    return file_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df[\"file\"] = data_df.apply(create_file_name, axis=1)\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_names = list(data_df['file'])\nprint(\"Matching image names: {}\".format(len(set(file_names).intersection(image_files))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_image_sizes(file_name):\n    image = skimage.io.imread(IMAGE_PATH + file_name)\n    return list(image.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tqdm.pandas()\nm = np.stack(data_df['file'].progress_apply(read_image_sizes))\ndf = pd.DataFrame(m,columns=['w','h'])\ndata_df = pd.concat([data_df,df],axis=1, sort=False)\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of suites: {data_df.suite_id.nunique()}\")\nprint(f\"Samples: {data_df.sample_id.unique()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, test_df = train_test_split(data_df, \n                                     test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=data_df[\"code\"].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, val_df = train_test_split(train_df, \n                                    test_size=VAL_SIZE, random_state=RANDOM_STATE, stratify=train_df[\"code\"].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train set rows: {}\".format(train_df.shape[0]))\nprint(\"Test  set rows: {}\".format(test_df.shape[0]))\nprint(\"Val   set rows: {}\".format(val_df.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_image(file_name):\n    image = skimage.io.imread(IMAGE_PATH + file_name)\n    image = skimage.transform.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT, 1), mode='reflect')\n    return image[:,:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def categories_encoder(dataset, var='character'):\n    X = np.stack(dataset['file'].apply(read_image))\n    y = pd.get_dummies(dataset[var], drop_first=False)\n    return X, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, y_train = categories_encoder(train_df)\nX_val, y_val = categories_encoder(val_df)\nX_test, y_test = categories_encoder(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\n\nmodel.add(Conv2D(filters = 64, kernel_size=(3,3), input_shape=(64, 64, 1), padding='same'))\nmodel.add(PReLU(alpha_initializer=Constant(value=0.25)))\nmodel.add(MaxPool2D(2))\n\n\nmodel.add(Conv2D(filters = 128, kernel_size=(3,3), padding='same'))\nmodel.add(PReLU(alpha_initializer=Constant(value=0.25)))\nmodel.add(MaxPool2D(2))\n\n\nmodel.add(Conv2D(filters = 160, kernel_size=(3,3),  padding='same'))\nmodel.add(PReLU(alpha_initializer=Constant(value=0.25)))\nmodel.add(MaxPool2D(2))\n\n\nmodel.add(Conv2D(filters = 256, kernel_size=(3,3), padding='same'))\nmodel.add(PReLU(alpha_initializer=Constant(value=0.25)))\nmodel.add(Conv2D(filters = 256, kernel_size=(3,3), padding='same'))\nmodel.add(PReLU(alpha_initializer=Constant(value=0.25)))\nmodel.add(MaxPool2D(2))\n\n\nmodel.add(Conv2D(filters = 384, kernel_size=(3,3),  padding='same'))\nmodel.add(PReLU(alpha_initializer=Constant(value=0.25)))\nmodel.add(Conv2D(filters = 384, kernel_size=(3,3), padding='same'))\nmodel.add(PReLU(alpha_initializer=Constant(value=0.25)))\nmodel.add(MaxPool2D(2))\n\n\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(1024))\nmodel.add(PReLU(alpha_initializer=Constant(value=0.25)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(15, activation='softmax'))\nmodel.add(PReLU(alpha_initializer=Constant(value=0.25)))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annealer = LearningRateScheduler(lambda x: 1e-3 * 0.99 ** (x+NO_EPOCHS))\nearlystopper = EarlyStopping(monitor='loss', patience=PATIENCE, verbose=VERBOSE)\ncheckpointer = ModelCheckpoint('best_model.h5',\n                                monitor='val_accuracy',\n                                verbose=VERBOSE,\n                                save_best_only=True,\n                                save_weights_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model  = model.fit(X_train, y_train,\n                  batch_size=BATCH_SIZE,\n                  epochs=NO_EPOCHS,\n                  verbose=1,\n                  validation_data=(X_val, y_val),\n                  callbacks=[earlystopper, checkpointer, annealer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_trace(x,y,ylabel,color):\n        trace = go.Scatter(\n            x = x,y = y,\n            name=ylabel,\n            marker=dict(color=color),\n            mode = \"markers+lines\",\n            text=x\n        )\n        return trace\n    \ndef plot_accuracy_and_loss(train_model):\n    hist = train_model.history\n    acc = hist['accuracy']\n    val_acc = hist['val_accuracy']\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    epochs = list(range(1,len(acc)+1))\n    #define the traces\n    \n    trace_ta = create_trace(epochs,acc,\"Training accuracy\", \"Green\")\n    trace_va = create_trace(epochs,val_acc,\"Validation accuracy\", \"Red\")\n    trace_tl = create_trace(epochs,loss,\"Training loss\", \"Blue\")\n    trace_vl = create_trace(epochs,val_loss,\"Validation loss\", \"Magenta\")\n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=('Training and validation accuracy',\n                                                             'Training and validation loss'))\n    #add traces to the figure\n    fig.append_trace(trace_ta,1,1)\n    fig.append_trace(trace_va,1,1)\n    fig.append_trace(trace_tl,1,2)\n    fig.append_trace(trace_vl,1,2)\n    #set the layout for the figure\n    fig['layout']['xaxis'].update(title = 'Epoch')\n    fig['layout']['xaxis2'].update(title = 'Epoch')\n    fig['layout']['yaxis'].update(title = 'Accuracy', range=[0,1])\n    fig['layout']['yaxis2'].update(title = 'Loss', range=[0,1])\n    #plot\n    iplot(fig, filename='accuracy-loss')\n\nplot_accuracy_and_loss(train_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_val, y_val, verbose=0)\nprint('Val loss:', score[0])\nprint('Val accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_accuracy_report(model):\n    predicted = model.predict(X_test)\n    test_predicted = np.argmax(predicted, axis=1)\n    test_truth = np.argmax(y_test.values, axis=1)\n    print(metrics.classification_report(test_truth, test_predicted, target_names=y_test.columns)) \n    test_res = model.evaluate(X_test, y_test.values, verbose=0)\n    print('Loss function: %s, accuracy:' % test_res[0], test_res[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_optimal = model\nmodel_optimal.load_weights('best_model.h5')\nscore = model_optimal.evaluate(X_test, y_test, verbose=0)\nprint(f'Best validation loss: {score[0]}, accuracy: {score[1]}')\n\ntest_accuracy_report(model_optimal)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}