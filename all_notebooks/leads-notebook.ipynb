{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom IPython.display import display\nfrom matplotlib.pyplot import xticks\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/leads-dataset/Leads.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = df.columns.str.lower().str.replace(' ','_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string_columns = list(df.dtypes[df.dtypes == 'object'].index)\n\nfor column in string_columns:\n    df[column] = df[column].str.lower().str.replace(' ', '_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Additional Data formatting\nfor column in ['asymmetrique_activity_index', 'asymmetrique_profile_index']:\n    df[column] = df[column].str.lower().str.replace('.', '_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select is a bad value, so we will replace it with NaN\ndf = df.replace('select', np.nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(df.loc[:,list(round(100*(df.isnull().sum()/len(df.index)), 2)>70)].columns, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['lead_quality'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_quality = df['lead_quality']\nsns.countplot(df['lead_quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['lead_quality'] = df['lead_quality'].replace(np.nan, 'not_sure')\ndf['lead_quality'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['lead_quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['totalvisits','page_views_per_visit','asymmetrique_activity_score', 'asymmetrique_profile_score']:\n    df[col] = df[col].fillna(0.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['asymmetrique_activity_index','asymmetrique_activity_score','asymmetrique_profile_index','asymmetrique_profile_score'],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in (df.dtypes[df.dtypes == 'object'].index):\n    description = df[col].describe()\n    display(description)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['x_education_forums', 'a_free_copy_of_mastering_the_interview', 'through_recommendations',\n         'search', 'newspaper_article', 'digital_advertisement'], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace(np.nan, 'not_answered', regex=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x = \"last_notable_activity\", hue = \"converted\", data = df)\nxticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_val = train_test_split(df_train_full, test_size=0.33, random_state=11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.converted.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val = df_val.converted.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_train['converted']\ndel df_val['converted']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ensure the training set has no missing values\ndf_train_full.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1 => Number of converted leads \ndf_train_full.converted.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global_mean = df_train_full.converted.mean()\nf'Conversion Rate => {global_mean}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = list(df.dtypes[df.dtypes == 'object'].index)\ncategorical.remove('prospect_id')\ncategorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical = list(df.dtypes[df.dtypes == 'int64'].index) + list(df.dtypes[df.dtypes == 'float64'].index)\nnumerical.remove('lead_number')\nnumerical.remove('converted')\nnumerical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full[categorical].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_categorical = []\nfor column in categorical:\n    if len(df[column].unique()) > 1:\n        new_categorical.append(column)\ncategorical = new_categorical\nnew_categorical.remove('newspaper')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full[categorical].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 24 columns being used in total\ndf_train_full[categorical + numerical].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_group_risk(df, column_name):\n    df_group = df.groupby(by=column_name).converted.agg(['mean'])\n    df_group['diff'] = df_group['mean'] - global_mean\n    df_group['risk'] = df_group['mean'] / global_mean\n    return df_group","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If the risk is lower than 1, the group has lower risks: the lead rate in this group is smaller than the global lead rate\n# (0.5) => two times less likely to convert\n\n# If the value is higher than 1, the group is risky: the lead rate in this group is higher than the global lead rate\n# (2) => two times more likely to convert\n\nfor col in categorical:\n    display(calculate_group_risk(df_train_full, col))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mutual_info_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def calculate_mi(series):\n    return mutual_info_score(series, df_train_full.converted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mi = df_train_full[categorical].apply(calculate_mi)\ndf_mi = df_mi.sort_values(ascending=False).to_frame(name='MI')\ndf_mi\n# Higher values means a higher degree of dependence, meaning the variable is useful for predicting the target\n# Lower values means that the target and categorical variable are independent, thus the variable is not as useful for predicting the target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Measure the dependency between a binary target variable and a numerical variable\n# Positive correlation means that when one variable goes up, the other variable tends to go up as well\n# Zero correlation means no relationship between variables: they are completely independent\n# Negative correlation occurs when one variable goes up while the other goes down\ndf_train_full[numerical].corrwith(df_train_full.converted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# People who spend more time on the website are more likely to be converted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dictionary of the Dataframe content\ntrain_dict = df_train[categorical + numerical].to_dict(orient='records')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction import DictVectorizer\n\ndv = DictVectorizer(sparse=False)\n\ndv.fit(train_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = dv.transform(train_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dv.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training the Logistic Regression Model\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver='liblinear', random_state=1)\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the model against the validation dataset\nval_dict = df_val[categorical + numerical].to_dict(orient='records')\n# Create the dictionaries\nX_val = dv.transform(val_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1-p, select all rows at index 1\ny_pred = model.predict_proba(X_val)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"converted_pred = y_pred >= 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(y_val == converted_pred).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bias Term\nmodel.intercept_[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Weights Vector\ndict(zip(dv.get_feature_names(), model.coef_[0].round(3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\n# Sigmoid function for Linear Regression\ndef sigmoid(score):\n    return 1 / (1 + math.exp(-score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bias Term base prediction %\nsigmoid(model.intercept_[0]) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the accuracy of our model\ny_pred = model.predict_proba(X_val)[:,1]\nconverted = y_pred >= 0.5\nf'Accuracy of Model: {(converted == y_val).mean()}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nthresholds = np.linspace(0,1,11)\nfor t in thresholds:\n    converted = y_pred >= t\n    acc = accuracy_score(y_val, converted)\n    print('%0.2f %0.3f' % (t, acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresholds = np.linspace(0, 1, 121)\naccuracies = []\nfor t in thresholds:\n    acc = accuracy_score(y_val, y_pred >= t)\n    accuracies.append(acc)\nplt.plot(thresholds, accuracies)\nplt.xlabel('threshold')\nplt.ylabel('accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_val)\nprint(y_pred)\nconfusion_matrix(y_val, converted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def confusion_table_create(y_val, y_pred, t = 0.5):\n    actual_churn = (y_val == 1)\n    actual_no_churn = (y_val == 0)\n    \n    predict_churn = (y_pred >= t)\n    predict_no_churn = (y_pred < t)\n \n    true_positive = (predict_churn & actual_churn).sum()\n    false_positive = (predict_churn & actual_no_churn).sum()\n \n    false_negative = (predict_no_churn & actual_churn).sum()\n    true_negative = (predict_no_churn & actual_no_churn).sum()\n    \n    return np.array([[true_negative, false_positive], [false_negative, true_positive]])\n\ndef print_confusion_table(confusion_table):\n    # Predicted False and the actual label is also False (TN)\n    print(f'True Negatives: {confusion_table[0,0]}')\n    # Predicted True but the actual label was False (FP)\n    print(f'False Positives: {confusion_table[0,1]}')\n    \n    # Predicted False but the actual label was True (FN)\n    print(f'False Negatives: {confusion_table[1,0]}')\n    # Predicted True and the actual label is also True (TP)\n    print(f'True Positives: {confusion_table[1,1]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_table = confusion_table_create(y_val, y_pred)\nprint_confusion_table(confusion_table)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_table / confusion_table.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of positive predictions that turned out correct (Based on Predictions) (TP / TP + FP)\ndef precision(confusion_table):\n    return confusion_table[1,1] / (confusion_table[1,1] + confusion_table[0,1])\n\n# Number of correctly positive examples among all positive examples (TP / TP + FN)\ndef recall(confusion_table):\n    return confusion_table[1,1] / (confusion_table[1,1] + confusion_table[1,0])\n\n# Fraction of false positives among all negatives (FP / FP + TN)\ndef false_positive_rate(confusion_table):\n    return confusion_table[0,1] / (confusion_table[0,0] + confusion_table[0,1])\n\n# Fraction of true positives among all positives (TP / TP + FN)\ndef true_positive_rate(confusion_table):\n    return confusion_table[1,1] / (confusion_table[1,1] + confusion_table[1,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Better the precision, the fewer false positives there are\nprecision(confusion_table) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Better the recall, the fewer false negatives there are\nrecall(confusion_table) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"false_positive_rate(confusion_table) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_positive_rate(confusion_table) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"false_positive_rate(confusion_table) * 100 + true_positive_rate(confusion_table) * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = []\n \nthresholds = np.linspace(0, 1, 101)\n \nfor t in thresholds:\n    tp = ((y_pred >= t) & (y_val == 1)).sum()\n    fp = ((y_pred >= t) & (y_val == 0)).sum()\n    fn = ((y_pred < t) & (y_val == 1)).sum()\n    tn = ((y_pred < t) & (y_val == 0)).sum()\n    scores.append((t, tp, fp, fn, tn))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores = pd.DataFrame(scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scores.columns = ['threshold','tp','fp','fn','tn']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the TPR and FPR for all values at once in the dataframe\ndf_scores['tpr'] = df_scores.tp / (df_scores.tp + df_scores.fn)\ndf_scores['fpr'] = df_scores.fp / (df_scores.fp + df_scores.tn)\ndf_scores[::10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(df_scores.threshold, df_scores.tpr, label='TPR')\nplt.plot(df_scores.threshold, df_scores.fpr, label='FPR')\nplt.legend()\nplt.xlabel('thresholds')\n# A small FPR indicates that the model makes very few mistakes predicting negative examples (FP (True but was False))\n# TPR should decrease slowly staying closing to 100% indicating true positives are predicted well (True and was True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nplt.plot(df_scores.fpr, df_scores.tpr)\nplt.plot([0, 1], [0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scikit-Learn ROC Curve calculation\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_val, y_pred)\n\nplt.figure(figsize=(5, 5))\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import auc\nauc(df_scores.fpr, df_scores.tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_val, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg = y_pred[y_val == 0]\npos = y_pred[y_val == 1]\n \nnp.random.seed(1)\n# Size is the size of the total np array, low and high are the thresholds of the random number\nneg_choice = np.random.randint(low=0, high=len(neg), size=10000)\npos_choice = np.random.randint(low=0, high=len(pos), size=10000)\n(pos[pos_choice] > neg[neg_choice]).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**K-fold cross validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(df, y, C):\n    cat = df[categorical + numerical].to_dict(orient='records')\n    dv = DictVectorizer(sparse=False)\n    dv.fit(cat)\n    \n    X = dv.transform(cat)\n    \n    model = LogisticRegression(solver='liblinear', C=C)\n    model.fit(X, y)\n    \n    return dv, model\n\n# Dataframe, DictVectorizer, Model\ndef predict(df, dv, model):\n    cat = df[categorical + numerical].to_dict(orient='records')\n    \n    X = dv.transform(cat)\n    y_pred = model.predict_proba(X)[:,1]\n    \n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nkfold = KFold(n_splits=10, shuffle=True, random_state=1)\n\nfor C in [0.001, 0.01, 0.1, 0.5, 1, 10]:\n    aucs = []\n    for train_idx, val_idx in kfold.split(df_train_full):\n        # access datafrane records by their numbers\n        df_train = df_train_full.iloc[train_idx]\n        df_val = df_train_full.iloc[val_idx]\n\n        y_train = df_train.converted.values\n        y_val = df_val.converted.values\n\n        dv, model = train(df_train, y_train, C)\n        y_pred = predict(df_val, dv, model)\n\n        auc = roc_auc_score(y_val, y_pred)\n        aucs.append(auc)\n    print('C=%s, auc = %0.3f ± %0.3f' % (C, np.mean(aucs), np.std(aucs)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nPickle model, dictvect export\n#with open('leads-model.bin', 'wb') as f_out:\n#    pickle.dump((dv, model), f_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}