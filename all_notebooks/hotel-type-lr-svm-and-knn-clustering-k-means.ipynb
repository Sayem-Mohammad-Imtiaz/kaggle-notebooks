{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First of all, we started by loading the data and importing the necessary libraries that we will use in the dataset preprocessing:","metadata":{}},{"cell_type":"code","source":"#Importing Packages\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing Data\ndf = pd.read_csv('../input/hotel-booking-demand/hotel_bookings.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the data is loaded, the number of columns and the number of data table rows were viewed. Then, we visualized a few lines of data to get a general idea of this data, and to see if there are any missing values, which will be represented by NaN:","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Verifiying Missing Values**","metadata":{}},{"cell_type":"markdown","source":"we check if there are missing values in our dataset, then we visualize some:","metadata":{}},{"cell_type":"code","source":"#Verifiying the existence of missing values\ndf.isnull().values.any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vizualizing some missing values\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#visualizing the percentages of missing values in each column\npercent_missing = df.isnull().sum() * 100 / len(df)\nmissing_value_df = pd.DataFrame({'percent_missing': percent_missing})\nmissing_value_df.sort_values('percent_missing', inplace=True)\nprint(missing_value_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dropping the rows that contain missing values except company and agent (except: company and agent)\ndf = df[df['children'].notna()]\ndf = df[df['country'].notna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**arrival year, month and day to arrival_date**","metadata":{}},{"cell_type":"markdown","source":"Then we merge the three columns 'arrival_date_year', 'arrival_date_month', 'arrival date day_of_month' into a column called “arrival_date”, containing the day, month and year of the client's arrival in datetime form. To do this, we run the following code:","metadata":{}},{"cell_type":"code","source":"#Transforming arrival_date_month to datetime type\ndf[\"arrival_date_month\"]=pd.to_datetime(df['arrival_date_month'],format='%B').dt.month","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#combine year and month and day in a datetime variable\ndf[\"arrival_date\"]=pd.to_datetime({\"year\":df[\"arrival_date_year\"].values,\"month\":df[\"arrival_date_month\"].values,\"day\":df[\"arrival_date_day_of_month\"].values})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Droping the year and month and day columns\ndf=df.drop(columns=['arrival_date_year','arrival_date_month','arrival_date_day_of_month'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing the shape of our dataframe (rows,columns) again\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing a sample of 10 rows of our dataframe\ndf.sample(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Verifiying that the timestamp of the variable reservation_status_date must occur after or at the same date as the input variable arrival_date**","metadata":{}},{"cell_type":"code","source":"#Visualizing the types of our dataframe's variables\ndf.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transforming the reservation_status_date variable type to Datetime \ndf[\"reservation_status_date\"]=pd.to_datetime(df[\"reservation_status_date\"], format = '%Y-%m-%d')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing the types of our dataframe's variables again\ndf.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing Extentions**","metadata":{}},{"cell_type":"markdown","source":"Cleaning: We propose to treat the missing values, to use the approach of filling each empty box with the median of the values of the column to which this empty box belongs, and we can extend this solution, by adding another column which will contain two values, True or False, to indicate if the value of the first column is original or it is calculated by the median. We implement this solution for the two columns “agent” and “company” as it is illustrated in the following figure:","metadata":{}},{"cell_type":"code","source":"#Filling null values in these two columns with the mean of values of each column\nfor column in ['agent','company']:\n    df[column] =df[column].fillna(df[column].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vizualizing the sum of missing values in each variable\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Filling null values in these two columns with the mean of values of each column\nfor column in ['arrival_date']:\n    df[column] =df[column].fillna(df[column].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Vizualizing the sum of missing values in each variable\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We check if there are duplicate lines, if so we opt to delete them, using the following command:","metadata":{}},{"cell_type":"code","source":"#Droping the duplicated values\ndf.drop_duplicates( inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transformation: in order to properly treat categorical variables, we propose the creation of columns among the number of categories for each variable. Each column is filled with the values 0 and 1. The value 0 replaces NULL, and 1 means that the corresponding row has this category. In our case; we first specify the categorical variables; and we transform them as follows:","metadata":{}},{"cell_type":"code","source":"#Transformation of categoriccal variables to numirical variables\ncategoricalV=[\"hotel\",\"meal\",\"country\",\"market_segment\",\"distribution_channel\",\"reserved_room_type\",\"assigned_room_type\",\"deposit_type\",\"customer_type\"]\ndf[categoricalV[1:11]]=df[categoricalV[1:11]].astype('category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[categoricalV[1:11]]=df[categoricalV[1:11]].apply(lambda x:LabelEncoder().fit_transform(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['hotel_Num']=LabelEncoder().fit_transform(df['hotel'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploratory Analysis**","metadata":{}},{"cell_type":"markdown","source":"Dataset summary statistics – Date variables","metadata":{}},{"cell_type":"markdown","source":"The first aim is to create summary statistics for the dataset. For date variables, we use the describe () method, with an additional attribute (in order to make the dates of the variables numeric to apply the method) as shown in the following figure:","metadata":{}},{"cell_type":"code","source":"#Create dataset summary statistics for Date variables\ndf[[\"reservation_status_date\",\"arrival_date\"]].describe(datetime_is_numeric=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"dataset summary statistics – Categorical variables","metadata":{}},{"cell_type":"markdown","source":"Secondly, we implement the describe () method, for the categorical variables already specified in the previous part. We notice that this time the output is different including std:","metadata":{}},{"cell_type":"code","source":"#Create dataset summary statistics for Categorical variables\ndf[categoricalV].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"dataset summary statistics – Integer and numeric variables","metadata":{}},{"cell_type":"markdown","source":"We specify the numeric variables, then we implement the describe () method again, as follows:","metadata":{}},{"cell_type":"code","source":"#Create dataset summary statistics for Integer and numeric variables\ndf[[\"lead_time\",\"arrival_date_week_number\",\"stays_in_weekend_nights\",\"stays_in_week_nights\",\"adults\",\"children\",\"babies\",\"is_repeated_guest\",\"previous_cancellations\",\"previous_bookings_not_canceled\",\"booking_changes\",\"days_in_waiting_list\",\"adr\",\"required_car_parking_spaces\",\"total_of_special_requests\"]].describe(datetime_is_numeric=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of hotel type for cancellation","metadata":{}},{"cell_type":"markdown","source":"the distribution is plotted once against the cancellation and once against the number of adults. The code for the two manipulation is as follows:","metadata":{}},{"cell_type":"code","source":"#Ploting the distribution of cancellation in each type of hotels\nplt.rcParams['figure.figsize'] = [10, 7]\nsns.set(style = 'white', font_scale = 1.3)\n# Plot\ndist = sns.countplot(df['hotel'], hue = 'is_canceled', data = df, palette = 'Set2');\ndist.set(title = \"Distribution of the hotel based on cancellation\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution of cancellation and Number of Adults","metadata":{}},{"cell_type":"code","source":"#Checking the distribution of cancellation and Number of Adults\ndist = sns.countplot('adults',data=df,hue='is_canceled');\ndist.set(title = \"Adults Cancellations\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Modeling****","metadata":{}},{"cell_type":"markdown","source":"From a demographic perspective, if we have precise data we will predict whether it is a resort hotel or a city hall. Supervised learning techniques will allow us to accomplish such a task, including Logistic Regression, KNN, SVM. In other words, the problem is purely a classification problem, which emphasizes segmentation of individuals based on the target variable hotel. This will help the hotel to divide the guests into groups based on the type of host. Which means a significant increase in profits and relevant revenue management.","metadata":{}},{"cell_type":"markdown","source":"***Logistic Regrssion***","metadata":{}},{"cell_type":"code","source":"#Importing the train_test_split module for spliting data\nfrom sklearn.model_selection import train_test_split\n#Importing datetime\nimport datetime as dt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transforming Datetime variables to numerical variables\ndf['numerical_larrival_date']=df['arrival_date'].map(dt.datetime.toordinal)\ndf['numerical_reservation_status_date']=df['reservation_status_date'].map(dt.datetime.toordinal)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transforming is_canceled to a numerical variable\ndf[\"is_canceled\"].replace({'not canceled': 0,'canceled':1}, inplace=True)\ndf[\"reservation_status\"].replace({'Canceled': 0,'Check-Out':1,'No-Show':2}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining X (target values) and Y (usefull columns)\nusefull_columns = df.columns.difference(['hotel','hotel_Num','arrival_date','reservation_status_date'])\nX = df[usefull_columns]\nY = df[\"hotel_Num\"].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Spliting data to train data and test data\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.3,random_state=150)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Test_size = 0.3 means that 30% of the initial data is dedicated to model testing, and the 70% is dedicated to model training.\nRandom_state means the degree of randomness with which we will divide our dataset","metadata":{}},{"cell_type":"code","source":"#Importing some needed metrics for evaluating the models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training our Logistic Regressing model \nlogisticR = LogisticRegression()\nlogisticR.fit(X_train,Y_train)\nY_pred= logisticR.predict(X_test)\nY_train_pred = logisticR.predict(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#metrics and accuracy score \nprint('Recall Score :',recall_score(Y_test,Y_pred))\nprint('Precision Score :',precision_score(Y_test,Y_pred))\nprint('F1 Score :',f1_score(Y_test,Y_pred))\nprint('-----------------------------------------------')\nprint('Accuracy Score :',accuracy_score(Y_test,Y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And we can see that our model's accuracy is 87%, which represents a good performance.","metadata":{}},{"cell_type":"code","source":"#Ploting the confusion matrix\nplot_confusion_matrix(logisticR,X_test,Y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***SVM - Support Vector Machine***","metadata":{}},{"cell_type":"code","source":"#Importing the needed packages for SVM algorithme\nfrom sklearn import svm\nfrom sklearn.svm import SVC","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaling = MinMaxScaler(feature_range=(-1,1)).fit(X_train)\nX_train = scaling.transform(X_train)\nX_test = scaling.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining an SVM classifier\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training the model\nY_pred = svclassifier.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#metrics and accuracy scores\nprint('Recall Score :',recall_score(Y_test,Y_pred))\nprint('Precision Score :',precision_score(Y_test,Y_pred))\nprint('F1 Score :',f1_score(Y_test,Y_pred))\nprint('-----------------------------------------------')\nprint('Accuracy Score :',accuracy_score(Y_test,Y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have as a result a classification rate of 90%, considered as a very good precision.","metadata":{}},{"cell_type":"code","source":"#Ploting the confusion matrix\nplot_confusion_matrix(svclassifier,X_test,Y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KNN - k-Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"#Importing the needed packages for KNN algorithme\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this section, we will plot the mean error for the predicted values of the test set for all K values between 1 and 40. first the error mean for all predicted values where K is between 1 and\n40, In each iteration, the average error for the predicted values of the set of\ntest is calculated and the result is added to the error list:","metadata":{}},{"cell_type":"code","source":"error = []\n\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, Y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i != Y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the use of the value 1 for K is the most optimal. In order to train the KNN algorithm, we rely on the use of Scikit-Learn. The first step is to import the KNeighborsClassifier class from from the sklearn.neighbors library. This class is initiated with a parameter, (n_neigbours). This is basically the value of K. The last step is to make predictions about our test data. To do this, we run the following script:","metadata":{}},{"cell_type":"code","source":"#Defining an KNN classifier and training the model\nclassifier = KNeighborsClassifier(n_neighbors=1)\nclassifier.fit(X_train, Y_train)\nY_pred = classifier.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Showing the metrics and accuracy scores\nprint('Recall Score :',recall_score(Y_test,Y_pred))\nprint('Precision Score :',precision_score(Y_test,Y_pred))\nprint('F1 Score :',f1_score(Y_test,Y_pred))\nprint('-----------------------------------------------')\nprint('Accuracy Score :',accuracy_score(Y_test,Y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results show that our KNN algorithm was able to rank the test set records with an accuracy of 94%, which is excellent given the high dimensionality of our dataset.","metadata":{}},{"cell_type":"code","source":"#Ploting the confusion matrix\nplot_confusion_matrix(classifier,X_test,Y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Concerning the accuracy, we had as result 94% for KNN, 90% for SVM, and 86% for Logistic Regression. The greatest value is that of KNN ... In other words with Knn 94% of our predictions will be correct, it therefore represents the best model to adopt. Which is logical, in the literature we find that when the training data is much bigger than the other features, KNN is better than SVM. Besides KNN is easy to implement. Yet KNN is slower in execution time than LR, but not slow enough than SVM. From a more global perspective, KNN and SVM support nonlinear solutions, and they are unparameterized where Parameterized Logistic Regression, deals with linear solutions. SVM is less computationally demanding than kNN and it is easier to interpret but can only identify a limited set of patterns. On the other hand, kNN can find very complex models but its output is more difficult to interpret.","metadata":{}},{"cell_type":"markdown","source":"***Clustering with K-Means***","metadata":{}},{"cell_type":"markdown","source":"After we used supervised algorithms in the first part, now we have considered an unsupervised problem, a clustering problem based on K-Means, and we will analyze the results of each cluster to identify the most profitable clients in our data set based on lead time and ADR. The first challenge that we encounter when we want to use clustering with K-means, is to determine the optimal number of clusters that we want to have as results. So first to determine the number of clusters, we used the Elbow method:","metadata":{}},{"cell_type":"code","source":"import sklearn.cluster as cluster","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_Short = df[['lead_time','adr']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K=range(1,12)\nwss = []\nfor k in K:\n    kmeans=cluster.KMeans(n_clusters=k,init=\"k-means++\")\n    kmeans=kmeans.fit(df_Short)\n    wss_iter = kmeans.inertia_\n    wss.append(wss_iter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mycenters = pd.DataFrame({'Clusters' : K, 'WSS' : wss})\nmycenters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x = 'Clusters', y = 'WSS', data = mycenters, marker=\"+\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To determine the optimal number of clusters, one must select the value of k after which the distortion begins to decrease linearly. Thus, we conclude that the optimal number of clusters for the data is 4.\nSo we ran the k-means algorithm based on lead_time and ADR with a number of clusters equal to 4, and we displayed the cluster centers:","metadata":{}},{"cell_type":"code","source":"kmeans = cluster.KMeans(n_clusters=4 ,init=\"k-means++\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans = kmeans.fit(df[['lead_time','adr']])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans.cluster_centers_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Clusters'] = kmeans.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we displayed the number of observations belonging to each cluster:","metadata":{}},{"cell_type":"code","source":"df['Clusters'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally we have displayed the clusters:","metadata":{}},{"cell_type":"code","source":"sns.lmplot(x=\"lead_time\", y=\"adr\",hue = 'Clusters',  data=df)\nplt.ylim(0, 600)\nplt.xlim(0, 800)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The clients with the lowest lead time and the highest ADR ie the clients that appear in the green cluster are considered to be the most profitable. While the red category shows the lowest ADR and the highest (least profitable) delivery time.\nWith regard to unsupervised learning in general - it is important to remember that this is largely a method of exploratory analysis - the goal is not necessarily to predict but rather to reveal information about data that may not have been taken into account before. For example, in our case after visualizing the graph, we can ask questions like: why some customers have a shorter delivery time than others? and are customers in certain countries more likely to match this profile? ect ...\nThese are all questions that the k-means clustering algorithm may not directly answer for us, but reducing the data into separate clusters provides a solid baseline to be able to ask questions like these.","metadata":{}},{"cell_type":"markdown","source":"**Conclusion**","metadata":{}},{"cell_type":"markdown","source":"In short, we learned three different ways to classify data using python for the supervised type (KNN, SVM, RL), and one for the unsupervised. We have found that, for the first type, KNN remains the best in terms of performance, for our case. And for the unsupervised, K-means allowed us to visualize the most profitable customers and the least profitable customers, based on the two variables lead_time (Number of days elapsed between the date of entry of the reservation in the PMS and the client arrival date) and adr (Average daily rate as defined by dividing the sum of all accommodation transactions by the total number of nights), and we used the result of this algorithm (which is under graph form) to ask specific questions about the variation in profitability of our customers, in order to give the hotel manager ideas to make their customers more profitable.\nOverall, the ideal model chosen for machine learning very often depends on the problem. There will be some datasets where KNN could fail miserably, so it is good to implement all the other models, for each problem, in order to judge the performance of each and choose the best model to adopt.\nIt all comes down to specifying the variables to be processed, and choosing the right machine learning model.","metadata":{}}]}