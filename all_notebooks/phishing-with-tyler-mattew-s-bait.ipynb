{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nimport sys\nimport os\n!pip install tldextract -q\nimport tldextract\nimport warnings\nimport regex as re\nimport eli5\nfrom typing import *\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](https://conectaja.proteste.org.br/wp-content/uploads/2020/05/phishing-970x472.jpg)conectaja.proteste.org.br","metadata":{}},{"cell_type":"markdown","source":"#Code by Tyler Sullivan and  Matthew Franglen  https://www.kaggle.com/tylersullivan/classifying-phishing-urls-three-models","metadata":{}},{"cell_type":"markdown","source":"Introduction\n\nIn this notebook TYLER SULLIVAN went over the preprocessing of URLs and compared three different binary classification models for determining if a URL is legitimate or used in a phishing attempt. He also looked at what features were most highly weighted in the most accurate model. As that was the first notebook he had created, Kagglers show your appreciation for TYLER's work:\nhttps://www.kaggle.com/tylersullivan/classifying-phishing-urls-three-models\n\nHe also thanked to MATTHEW FRANGLEN for improving the quality of HIS code!\n\nBy the way, Tyler is a Security Consultant. Therefore he knows a lot about Phishing.\n\nI just copied his Notebook in another Dataset and almost ruined the Code.  Upvote TYLER's work, not mine.\n\nhttps://www.kaggle.com/tylersullivan/classifying-phishing-urls-three-models","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn import svm\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom urllib.parse import urlparse\nfrom nltk.tokenize import RegexpTokenizer\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(r'/kaggle/input/phishing-data/combined_dataset.csv')\ndf.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_grp = df.groupby([\"domain\"])[[\"label\"]].sum().reset_index()\ndf_grp.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_url(domain: str) -> Optional[Dict[str, str]]:\n    try:\n        no_scheme = not domain.startswith('https://') and not domain.startswith('http://')\n        if no_scheme:\n            parsed_url = urlparse(f\"http://{domain}\")\n            return {\n                \"scheme\": None, # not established a value for this\n                \"netloc\": parsed_url.netloc,\n                \"path\": parsed_url.path,\n                \"params\": parsed_url.params,\n                \"query\": parsed_url.query,\n                \"fragment\": parsed_url.fragment,\n            }\n        else:\n            parsed_url = urlparse(domain)\n            return {\n                \"scheme\": parsed_url.scheme,\n                \"netloc\": parsed_url.netloc,\n                \"path\": parsed_url.path,\n                \"params\": parsed_url.params,\n                \"query\": parsed_url.query,\n                \"fragment\": parsed_url.fragment,\n            }\n    except:\n        return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_grp[\"parsed_url\"] = df_grp.domain.apply(parse_url)\ndf_grp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_grp = pd.concat([\n    df_grp.drop(['parsed_url'], axis=1),\n    df_grp['parsed_url'].apply(pd.Series)\n], axis=1)\ndf_grp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_grp = df_grp[~df_grp.netloc.isnull()]\ndf_grp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first meaningful bit of data to extract is the length of the URL.","metadata":{}},{"cell_type":"code","source":"df_grp[\"length\"] = df_grp.domain.str.len()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The TLD is then extracted using a python library, and if no TLD is present simply add 'None'.\n\ndf_grp[\"tld\"] = df_grp.netloc.apply(lambda nl: tldextract.extract(nl).suffix)\ndf_grp['tld'] = df_grp['tld'].replace('','None')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Next is a regex to determine if the URL is an IP address.\n\ndf_grp[\"is_ip\"] = df_grp.netloc.str.fullmatch(r\"\\d+\\.\\d+\\.\\d+\\.\\d+\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next few sections relate to certain punctuation in the URL which may be an indicator one way or another that a URL is malicious. My reasoning behind this is that typosquatted domains (which are almost always malicious) may contain this punctation to appear similar to a legitimate domain. There may also be more of each in the path of the URL for a legitimate URL as blogs often use underscores in a URL.","metadata":{}},{"cell_type":"code","source":"df_grp['domain_hyphens'] = df_grp.netloc.str.count('-')\ndf_grp['domain_underscores'] = df_grp.netloc.str.count('_')\ndf_grp['path_hyphens'] = df_grp.path.str.count('-')\ndf_grp['path_underscores'] = df_grp.path.str.count('_')\ndf_grp['slashes'] = df_grp.path.str.count('/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Full stops in the path could indicate that theres an attempt to fool a user into thinking a domain is legit. For example, attacker.com/paypal.com may be used to trick a user. Full stops may also be a sign of files in the URL such as shell.exe","metadata":{}},{"cell_type":"code","source":"df_grp['full_stops'] = df_grp.path.str.count('.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similar to the previous datapoint, getting the full stops in a subdomain will count how many subdomains are present. Lots may be another visual trick such as paypal.com.attacker.com/","metadata":{}},{"cell_type":"code","source":"def get_num_subdomains(netloc: str) -> int:\n    subdomain = tldextract.extract(netloc).subdomain \n    if subdomain == \"\":\n        return 0\n    return subdomain.count('.') + 1\n\ndf_grp['num_subdomains'] = df_grp['netloc'].apply(lambda net: get_num_subdomains(net))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As previous notebooks have shown, the lexical features of the URL will be important. In this instance, I have decided to separate the tokens from the path and the domain itself. My thinking here is that the same word in a path and domain may have very different meanings. By this i mean if you see 'paypal' in a URL path, it may be a malicious URL which is trying to seem legitimate, but 'paypal' in the domain may be more legitimate.","metadata":{}},{"cell_type":"code","source":"tokenizer = RegexpTokenizer(r'[A-Za-z]+')\ndef tokenize_domain(netloc: str) -> str:\n    split_domain = tldextract.extract(netloc)\n    no_tld = str(split_domain.subdomain +'.'+ split_domain.domain)\n    return \" \".join(map(str,tokenizer.tokenize(no_tld)))\n         \ndf_grp['domain_tokens'] = df_grp['netloc'].apply(lambda net: tokenize_domain(net))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_grp['path_tokens'] = df_grp['path'].apply(lambda path: \" \".join(map(str,tokenizer.tokenize(path))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_grp.columns.tolist()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The labels are now extracted and the URL column removed.\n\ndf_grp_y = df_grp['label'] #It was df_grp_y = df_grp['label'] But label Disappeared? Check columns tolist above\ndf_grp.drop('label', axis=1, inplace=True) #Where label disappeared? Label IS BACK!\ndf_grp.drop('domain', axis=1, inplace=True)\ndf_grp.drop('scheme', axis=1, inplace=True)\ndf_grp.drop('netloc', axis=1, inplace=True)\ndf_grp.drop('path', axis=1, inplace=True)\ndf_grp.drop('params', axis=1, inplace=True)\ndf_grp.drop('query', axis=1, inplace=True)\ndf_grp.drop('fragment', axis=1, inplace=True)\ndf_grp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Training\n\nWhen using pipelines and vectorizers, you need a converter to feed the vectorizer every word of that column. It cannot add the values one row at a time and so a converter class must be created.","metadata":{}},{"cell_type":"code","source":"class Converter(BaseEstimator, TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_frame):\n        return data_frame.values.ravel()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df_grp, df_grp_y, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The numeric features need their own pipeline to scale the data, MinMaxScaler was used as MultinomialNB needs no negative values to work.","metadata":{}},{"cell_type":"code","source":"numeric_features = ['length', 'domain_hyphens', 'domain_underscores', 'path_hyphens', 'path_underscores', 'slashes', 'full_stops', 'num_subdomains']\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', MinMaxScaler())])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The only categorical feature is TLD . (Those will be vectorized `domain_tokens` and `path_tokens`)  and OneHot encoding will be used for this. Interestingly there is no difference between using this or converting and using the TfidfVectorizer. However, using OneHot encoding makes the TLD obvious in the feature importance section.","metadata":{}},{"cell_type":"code","source":"categorical_features = ['tld', 'is_ip']\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CountVectorizer and TfidfVectorizer produce very similar results, but with the best performing model (spoiler its SVC) Tfidf slightly improved the score.","metadata":{}},{"cell_type":"code","source":"vectorizer_features = ['domain_tokens','path_tokens']\nvectorizer_transformer = Pipeline(steps=[\n    ('con', Converter()),\n    ('tf', TfidfVectorizer())])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to link all the transformers together in a ColumnTransformer, and create a pipeline for each classifier.","metadata":{}},{"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features),\n        ('domvec', vectorizer_transformer, ['domain_tokens']),\n        ('pathvec', vectorizer_transformer, ['path_tokens'])\n    ])\n\nsvc_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LinearSVC())])\n\nlog_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression())])\n\nnb_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', MultinomialNB())])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svc_clf.fit(X_train, y_train)\nlog_clf.fit(X_train, y_train)\nnb_clf.fit(X_train, y_train)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Results","metadata":{}},{"cell_type":"code","source":"def results(name: str, model: BaseEstimator) -> None:\n    preds = model.predict(X_test)\n\n    print(name + \" score: %.3f\" % model.score(X_test, y_test))\n    print(classification_report(y_test, preds))\n    labels = ['Good', 'Bad']\n\n    conf_matrix = confusion_matrix(y_test, preds)\n\n    font = {'family' : 'normal',\n            'size'   : 14}\n\n    plt.rc('font', **font)\n    plt.figure(figsize= (10,6))\n    sns.heatmap(conf_matrix, xticklabels=labels, yticklabels=labels, annot=True, fmt=\"d\", cmap='Greens')\n    plt.title(\"Confusion Matrix for \" + name)\n    plt.ylabel('True Class')\n    plt.xlabel('Predicted Class')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results(\"SVC\" , svc_clf)\nresults(\"Logistic Regression\" , log_clf)\nresults(\"Naive Bayes\" , nb_clf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see the SVC performs best out of the three followed by MultinomialNB. While Logistic Regression performs the worst, we can see it produces less false negatives than Naive Bayes.\n\nAlso if numerical features are removed, logistic regression performs better. I wouldn't know why this would be the case and would be interested to hear some ideas for it.","metadata":{}},{"cell_type":"markdown","source":"#Feature Importance\n\nFinally, to see what features are most strongly weighted to the SVC classifier I use eli5 to show this. It is worth noting that weights may be high for rarer features and should be taken with a grain of salt.","metadata":{}},{"cell_type":"code","source":"onehot_columns = list(svc_clf.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names(input_features=categorical_features))\ndomvect_columns = list(svc_clf.named_steps['preprocessor'].named_transformers_['domvec'].named_steps['tf'].get_feature_names())\npathvect_columns = list(svc_clf.named_steps['preprocessor'].named_transformers_['pathvec'].named_steps['tf'].get_feature_names())\nnumeric_features_list = list(numeric_features)\nnumeric_features_list.extend(onehot_columns)\nnumeric_features_list.extend(domvect_columns)\nnumeric_features_list.extend(pathvect_columns)\neli5.explain_weights(svc_clf.named_steps['classifier'], top=20, feature_names=numeric_features_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code by Olga Belitskaya https://www.kaggle.com/olgabelitskaya/sequential-data/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https://fonts.googleapis.com/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';</style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s</h1>\"\"\"%string))\n    \n    \ndhtml('Thank you Tyler Sullivan and Matthew Franglen for the script')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]}]}