{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"c9d7c5a1-a36d-0174-037a-389ff149d705"},"source":"# Tuning a kNN Classifier for Malignant Tumor Diagnosis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f78eba05-1850-c553-fed9-22fd52bbc08c"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import binarize\nfrom sklearn import metrics\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"907e6f76-d359-4263-fe83-5d306adfbb61"},"outputs":[],"source":"df = pd.read_csv(\"../input/data.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"86a61348-9059-7bd3-26ce-2b73b2f303dd"},"outputs":[],"source":"df.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f545b4e2-12e2-7692-7a52-e7a7c8ef48dd"},"outputs":[],"source":"df.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9fb95333-8257-e537-da4f-94990e605795"},"source":"**Remove Unnecessary Rows**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e268b18-9b91-d47b-be21-c84bcf27c174"},"outputs":[],"source":"del df['Unnamed: 32']\ndel df['id']"},{"cell_type":"markdown","metadata":{"_cell_guid":"64ed2ab4-fe9e-672b-9fc7-b900c9421390"},"source":"**Convert *diagnosis* to binary**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc70f46a-f1a4-b8b4-5aab-c8a51a9e19bf"},"outputs":[],"source":"d = {'M':1, 'B':0}\ny = df['diagnosis'].map(d).values\nX = df[df.columns[1:31]]"},{"cell_type":"markdown","metadata":{"_cell_guid":"41066831-eb26-27b8-2eef-afe7250b96b4"},"source":"**Split data** Open to suggestions as to how to split data here. I'm trying to use as much data as I can while holding out a reasonable amount for testing. In this case \"reasonable\" to me simply means that accuracy scores have ~ 1 percentage point resolution."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c49ed8eb-1683-b348-0409-82f57c230dad"},"outputs":[],"source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nprint(y_train.shape, y_test.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b332f477-f934-6ff6-9bd9-a4f9c0ca82ea"},"source":"## kNN Classifier\n- A good place to start, since it's a simple model and the dataset is not large."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2a97453-d333-87f5-d7c5-4c62186a1b24"},"outputs":[],"source":"knn = KNeighborsClassifier()\nk_values = range(1, 31)\nweight_values = ['uniform', 'distance']\nparam_dict = {'n_neighbors':k_values, 'weights':weight_values}\ngrid = GridSearchCV(knn, param_dict, cv=10, scoring='accuracy')\ngrid.fit(X_train, y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"323414c7-9788-820d-c612-f59aa67dfe7a"},"outputs":[],"source":"print(grid.best_score_)\nprint(grid.best_params_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"4d139c89-d64c-357b-1a2e-f6f972a757fa"},"source":"**Note:** Not bad for kNN. Admittedly I thought this model would perform much worse."},{"cell_type":"markdown","metadata":{"_cell_guid":"fd9aad2f-2580-8f44-5012-ac8f619b48ee"},"source":"**Split results by weight type** (*uniform* or *distance*) and visualize scores"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4cbbdfc2-5070-6218-bc35-9a5096decba0"},"outputs":[],"source":"%matplotlib inline\n\nuniform_scores = []\ndistance_scores = []\nall_scores = grid.cv_results_['mean_test_score']\nall_params = grid.cv_results_['params']\n\n#split\nfor i in range(len(all_scores)):\n    if all_params[i]['weights'] == 'uniform':\n        uniform_scores.append(all_scores[i])\n    else:\n        distance_scores.append(all_scores[i])\n        \n# Plot\nplt.plot(k_values, uniform_scores)\nplt.xlabel('K')\nplt.ylabel('Mean Validation Score')\nplt.title(\"Validation Score by K value, Uniform Weights\")\nplt.grid(True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4e19752-d6b4-e2bb-b47c-6fcc22fd2c12"},"outputs":[],"source":"plt.plot(k_values, distance_scores)\nplt.xlabel('K')\nplt.ylabel('Mean Validation Score')\nplt.title(\"Validation Score by K value, Distance Weights\")\nplt.grid(True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1b7e27fe-9bec-6c22-989f-1a504f9da686"},"source":"### Thoughts\n- After running this kernel a few times, it doesn't seem like there's an obvious difference between the two weight choices. Both have similar top *k* values, and neither out performs the other by much in terms of validation score.\n- If I had to choose, in light of the tie between the two metrics I suppose I would take uniform since it makes a for a computationally less intensive model."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"841a84fc-c62b-f4dc-5889-aac8811d5616"},"outputs":[],"source":"knn_dist = KNeighborsClassifier(n_neighbors=12, weights='distance')\nknn_unif = KNeighborsClassifier(n_neighbors=14, weights='uniform')\nknn_dist.fit(X_train, y_train)\nknn_unif.fit(X_train, y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f38b6997-073e-a9c2-a358-330d6423d4da"},"outputs":[],"source":"def model_accuracy(model, X, y):\n    y_pred = model.predict(X)\n    return metrics.accuracy_score(y, y_pred)\n\nacc_dist = model_accuracy(knn_dist, X_test, y_test)\nacc_unif = model_accuracy(knn_unif, X_test, y_test)\n\nprint(\"Best distance-weighted model test accuracy:\", acc_dist)\nprint(\"Best uniform-weighted model test accuracy:\", acc_unif)"},{"cell_type":"markdown","metadata":{"_cell_guid":"add064e9-4892-68a3-3819-db385f4c5e51"},"source":"#### Take a look at ROC\n- First we'll take a look at the distribution on probabilities assigned by the classifier.\n- Then we'll take a look at the ROC curve.\n- False negatives are more dangerous in this case, so look at what the threshold trade-offs look like"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b00ed242-cf3c-feab-36e1-e4b18ed7f252"},"outputs":[],"source":"y_prob_dist = knn_dist.predict_proba(X_test)\ny_prob_unif = knn_unif.predict_proba(X_test)\n\ndef plot_hist(y, title='', bins=10):\n    plt.hist(y, bins=bins)\n    plt.title(title)\n    plt.grid(True)\n\nplot_hist(y_prob_dist[:,1], title=\"Distance kNN Probability Scores\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b62f5cea-520e-3b80-31fa-78cb33d40f27"},"outputs":[],"source":"plot_hist(y_prob_unif[:,1], title=\"Uniform kNN Probability Scores\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"918ffb52-0b03-72d0-08aa-3ef18d5b751f"},"source":"**Note:** Both models have a tendency to favor a \"Benign\" diagnosis, which reflects the imbalance of diagnosis distributions in the data.\n**Note** Just eyeballing it, it appears as though setting the threshold even at a very low probability wouldn't push that many diagnoses into the \"Malignent\" category"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d0949a38-8157-942a-f956-0136294f19c7"},"outputs":[],"source":"# A finer grained histogram\nplot_hist(y_prob_dist[:,1], title=\"Distance Probability Scores\", bins=20)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9caf47fe-21dd-e380-8288-af303af18213"},"outputs":[],"source":"y_pred_dist = knn_dist.predict(X_test)\nmetrics.confusion_matrix(y_test, y_pred_dist)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cefcb87a-9d43-e78d-f447-239f15d79928"},"outputs":[],"source":"# plot ROC\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob_dist[:,1])\nplt.plot(fpr, tpr)\nplt.title(\"ROC curve for Distance-weighted kNN (k=12)\")\nplt.xlabel(\"False Positive Rate (1 - specificity)\")\nplt.ylabel(\"True Positive Rate\")\nplt.grid(True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5e41d711-5849-c5c0-7e0f-b0032039fa40"},"outputs":[],"source":"fpr"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2068ef43-c7a7-f5ee-86ff-2b5cb275d81f"},"outputs":[],"source":"thresholds"},{"cell_type":"markdown","metadata":{"_cell_guid":"bc194c54-8f90-a449-f88e-3f7d9d4dddba"},"source":"## Conclusions\n- By setting the threshold, the model can achieve a decent true positive rate of ~ 95%, with an overall accuracy of about 85% - 90%.\n- While this is decent performance, a false negative diagnosis is obviously potentially catastrophic for the patient, and is not tolerable, so we should look either to other methods, or to preprocessing data to get better performance on the dataset."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}