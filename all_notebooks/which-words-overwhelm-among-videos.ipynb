{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport holoviews as hv\nfrom datetime import datetime, timedelta\nimport json\nimport sqlite3\nfrom sqlalchemy import create_engine\nfrom urllib.parse   import quote\nfrom urllib.request import urlopen\nimport time\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk.corpus import wordnet\nhv.extension('bokeh')\n\nfm = pd.read_csv(\"../input/statistics-observation-of-random-youtube-video/count_observation_upload.csv\")\nfm2 = pd.read_csv(\"../input/statistics-observation-of-random-youtube-video/video_characteristics_upload.csv\")\nfm2 = fm2.drop('Unnamed: 0', axis = 1)\nfm = fm.drop('Unnamed: 0', axis = 1)\nfm = fm.set_index('index')\ndatetime_tran2 = lambda x : datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\nfm.loc[:,['commentCount', 'dislikeCount', 'favoriteCount', 'likeCount', 'viewCount']] =  fm.loc[:,['commentCount', 'dislikeCount', 'favoriteCount', 'likeCount', 'viewCount']].astype(np.float)\nfm['Time'] = fm['Time'].map(datetime_tran2)\nvideoId_list = list(fm.videoId.unique())\nvi_cat_dict = fm2.loc[:,['videoId','categoryId']].set_index('videoId').to_dict()['categoryId']\nfm['categoryId'] = fm['videoId'].map(vi_cat_dict)\nwith open('../input/youtube-new/US_category_id.json') as fb:\n    fb = fb.read()\n    categoryId_to_name = json.loads(fb)\ncategoryId_to_name2 = {}\nfor item in categoryId_to_name['items']:\n    categoryId_to_name2[np.float(item['id'])] = item['snippet']['title'] \nfm['categoryId'] = fm['categoryId'].map(categoryId_to_name2)\n%opts Bars [stack_index=1 xrotation=0 width=800 height=500 show_legend=False tools=['hover']]\n%opts Bars (color=Cycle('Category20'))","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"wordnet_lemmatizer = WordNetLemmatizer()\ndef treebank_tag_to_wordnet_pos2(treebank_tag):\n\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return ''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8c232485fc2a693bb1c1477d131ea37f67ccce0"},"cell_type":"markdown","source":"Music videos take 80 percent of the entire consumption which is screening actual video consumption, <br>\nSo In this kernel, we are going to exclude music videos which contains random words in their titles, disturbing the investigation.<br>\nAnd see which words among videos take major view counts \n"},{"metadata":{"_uuid":"4affee6bcac25062d8ec4912fe9a9b820cb5a6da"},"cell_type":"markdown","source":"Sort words by their entire viewcounts they obtained during the observation period. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e40ffdac502a6df22350d18f454dfd3acb0ba016"},"cell_type":"code","source":"vi_tit_dict = fm2.loc[:,['videoId','title']].set_index('videoId').to_dict()['title'] #create word to viewcount, title, and tag dictionary \nfm['title'] = fm['videoId'].map(vi_tit_dict)\nkingking = re.compile(r\"official|music|lyric|ft|mv\") #트레일러는 나중에 추가할까. \nnet_text = {} # lemmatization할때 대문자면 제대로 안됨. \nword2_title_dict = {}\ntag_checker = {}\nnet_text3 = {} \ntarget_fm = fm[fm['categoryId'] != 'Music'] #카테고리 music 인거 빼고 \ntarget_fm = target_fm.loc[:,['title','Time','viewCount_diff']]  #사이즈 줄이고. \nfor numnum, row in enumerate(target_fm.iterrows()):\n    title = row[1]['title']\n    viewC = row[1]['viewCount_diff']\n    infofo = row[1].iloc[1:]\n    if type(title) != str: #title이 str 아니면 빼고 \n        continue\n    title = title.lower()\n    if len(re.findall(kingking, title)) > 0:\n        continue  #music official lyric ft 이런말 들어가는 거 다 빼고. 정규표현식으로 따내야할 듯. \n    text = nltk.word_tokenize(title)\n    #text = list(map(lambda x:x.lower(),text))\n    pos_text = nltk.pos_tag(text)\n    dict__KEYS = list(net_text.keys())\n    for word, tag in pos_text:\n        changed_tag = treebank_tag_to_wordnet_pos2(tag)\n        if changed_tag != wordnet.NOUN: #명사만 헀고. pos tag 했을때 명사로 뜬 애들만. \n            continue\n        word2 = (wordnet_lemmatizer.lemmatize(word, pos = changed_tag))\n        if not word2 in dict__KEYS:\n            net_text[word2]  = []\n            word2_title_dict[word2] = []\n            tag_checker[word2] = []\n            net_text3[word2]  = []\n        net_text[word2].append(viewC)\n        word2_title_dict[word2].append(title)\n        tag_checker[word2].append(changed_tag)\n        net_text3[word2].append(infofo)\n    #print(\"{0} done\".format(numnum))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e4bbcc9c02e37fa35967d0aa55e1496db16a91c3"},"cell_type":"code","source":"mean_net_dict = {} #sum of view counts of each word related videos during the entire period \nfor net in net_text:\n    dirt = net_text[net]\n    dirt2 = [x for x in dirt if x >= 0] #sorting out error \n    smm = sum(dirt2)\n    mean_net_dict[net] = smm #구함. \nfinal_series = pd.Series(mean_net_dict).sort_values(ascending = False )\nfinal_series = final_series.reset_index()\nfinal_series.columns = ['word', 'viewCount_sum']\nfinal_series_ds = hv.Dataset(final_series, kdims = 'word', vdims = 'viewCount_sum')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5cb5e5c17aa7775a572527a0c76662f4cf501904"},"cell_type":"code","source":"hv.Bars(final_series_ds, label = \"Sort words by their entire viewcounts they obtained\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"49709b0a41788c459164bdbec8919163a00f1a6b"},"cell_type":"markdown","source":"What if look at words that have more than three related videos "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2062d590bfae690d26588b18492c4664ddcb2003"},"cell_type":"code","source":"mean_net_dict2 = {}\nfor word in word2_title_dict:\n    title = set(word2_title_dict[word])\n    if len(title) > 2:\n        mean_net_dict2[word] = mean_net_dict[word]\nfinal_series2 = pd.Series(mean_net_dict2).sort_values(ascending = False)\nfinal_series2 = final_series2.reset_index()\nfinal_series2.columns = ['word', 'viewCount_sum']\nfinal_series2_ds = hv.Dataset(final_series2, kdims = 'word', vdims = 'viewCount_sum')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1584068ac5ec5bbe69be6fa2c8818f97a71f2efb"},"cell_type":"code","source":"hv.Bars(final_series2_ds, label = \"Words that have more than three related videos\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6eb88cc525ae453e7da864c5b4aa1d85b307c41e"},"cell_type":"markdown","source":"'royale' has several related videos. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e20090d79566425c4f73b73facbb063bcebf3345"},"cell_type":"code","source":"set(word2_title_dict['royale'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"334ceb2799d63ee4a30da103671af3ee93768fec"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c9ea8ca88226d8878fd9fe6c37b1d0a85ad61296"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d6f5fb51f85a44e7ee5e8298f9f2420e4a47414"},"cell_type":"markdown","source":"Sort words by the number of related videos."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ecdb274b5086e859ca13bd4b0f670c3fd4aaaa63"},"cell_type":"code","source":"word_title_len = {}\nfor word in word2_title_dict:\n    word_title_len[word] = len(set(word2_title_dict[word]))\nword_title_len_df = pd.Series(word_title_len).sort_values(ascending= False).reset_index()\nword_title_len_df.columns = ['word', 'num_of_title']\nword_title_len_ds = hv.Dataset(word_title_len_df, kdims = 'word', vdims = 'num_of_title')\nhv.Bars(word_title_len_ds, label=\"Sort words by the number of related videos.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7c3d67c3d8b945906ef1487382fd728c7dc595e5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7d607a2582021493a5097b753498e8467bec3033"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"63b69a9eae4d5e5aa43f499d80955aa21cb6bfe5"},"cell_type":"markdown","source":"How about Top 10 words of each Time."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0131f63d6a0c0bc040396f17c8074c707a32d19e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"569b31fb1bba5ecd911bfc2ebabd13ffdc63d67b"},"cell_type":"code","source":"def top10(x):\n    return x.sort_values(by = 'viewCount_diff', ascending = False)[0:10]\ndf_listlist = []\nfor key in net_text3:\n    keydf = net_text3[key]\n    keydf = pd.concat(keydf, axis = 1).T.groupby('Time').sum()\n    keydf['word'] = key\n    df_listlist.append(keydf) #밑에랑 합쳤으면 좋겠는데 이게 단어별로 모여있어서. 밑에 과정을 거쳐서 \nword_tit = pd.concat(df_listlist, axis = 0).reset_index().groupby(['Time','word']).sum().reset_index()\nword_tit2 = word_tit.groupby('Time').apply(top10)\nword_tit2 = word_tit2.drop('Time',axis = 1).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f1c9d8a1a0994c0cd0ce9f631468d2d36c8d1680"},"cell_type":"code","source":"key_dimensions3   = [('Time', 'Time'), ('word', 'word')]\nvalue_dimensions3 = [('viewCount_diff', 'viewCount_diff')]\nmacro4 = hv.Table(word_tit2, key_dimensions3, value_dimensions3)\nmacro4.to.bars(['Time','word'], 'viewCount_diff', [], label = \"How about Top 10 words of each Time.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fd85687441536b419847c7b021d21a9aed1bd316"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10cbd77004956f275612fa370aec32a099523e0b"},"cell_type":"markdown","source":"Words that has more than three related videos"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fbea85e155d728451aaa3f8a5ab3dac9181f6cfb"},"cell_type":"code","source":"def title_more(x):\n    return x in list(mean_net_dict2.keys())\nword_tit3 = word_tit[word_tit.word.map(title_more)]\nword_tit3 = word_tit3.groupby('Time').apply(top10).drop('Time', axis = 1).reset_index()\nkey_dimensions32   = [('Time', 'Time'), ('word', 'word')]\nvalue_dimensions32 = [('viewCount_diff', 'viewCount_diff')]\nmacro42 = hv.Table(word_tit3, key_dimensions32, value_dimensions32)\nmacro42.to.bars(['Time','word'], 'viewCount_diff', [], label=\"Words that have more than three related videos\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3d97a79e7b496bb94d15ebe13820959653057950"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"12c2046bd9ee6587d0ff70d883ee36af5a474ec3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"19421325a65d91ea54ab5028a032d83403df5fb5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9cb1838b5f6fb35550fed69e0bece903eb21e3c2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}