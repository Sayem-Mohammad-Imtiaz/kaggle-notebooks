{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot\nfrom numpy import concatenate\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow.python.keras.layers import Dense, LSTM , Dropout\nfrom tensorflow.python.keras import Sequential\nfrom math import sqrt;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATASET PREPROCESSING**\n\n\n\n\nDataset Loading and dropping the rows with NA values.Plotting the graph for each columns. Further preprocessing includes:\n\n\n\n\n1. Feature Selection\n2. Label Encoding\n3. Data Normalization\n4. Converting to Time Series data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset preprocessing\n#datset loading dropping na values\n\ndataset= pd.read_csv('../input/beijing-pm25-data-data-set/PRSA_data_2010.1.1-2014.12.31.csv');\ndataset=dataset.dropna();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **1. Feature Selection**\nThere are 8 features important for the forecast: PM2.5, dew point, temperature, pressure, wind direction, wind speed and the cumulative number of hours of snow and rain. Hence, dropping other features."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=dataset.drop('No',axis=1);\ndataset=dataset.drop('year',axis=1);\ndataset=dataset.drop('month',axis=1);\ndataset=dataset.drop('day',axis=1);\ndataset=dataset.drop('hour',axis=1);\ndataset.head();\n\nvalues=dataset.values;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify columns to plot\ngroups = [0, 1, 2, 3, 5, 6, 7]\ni = 1\n# plotting each column\npyplot.figure()\nfor group in groups:\n\tpyplot.subplot(len(groups), 1, i)\n\tpyplot.plot(values[:, group])\n\tpyplot.title(dataset.columns[group], y=0.5, loc='right')\n\ti += 1\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **2.** **Label** **Encoding**\nThe Wind direction doesnâ€™t contain numerical values so label encoding is done.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# integer encode direction\nencoder = LabelEncoder()\nvalues[:,4] = encoder.fit_transform(values[:,4])\n# ensure all data is float\nvalues = values.astype('float32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **3. Data Normalization**\nData Normalization is done using MinMaxScaler function of sklearn."},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **4. Converting to Time Series Data**\nSince we use the LSTM neural network, we must sort the data according to the time. The dataset is transformed into a supervised learning problem. The weather variables for the hour to be predicted (t) are then removed. So, we have features for previous timestep (t-1) and for prediction of pollution PM2.5 taking its current timestep (t) data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = DataFrame(data)\n\tcols, names = list(), list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t\tif i == 0:\n\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n\t\telse:\n\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n\t# put it all together\n\tagg = concat(cols, axis=1)\n\tagg.columns = names\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# frame as supervised learning\nreframed = series_to_supervised(scaled, 1, 1)\n# drop columns we don't want to predict\nreframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\nprint(reframed.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting into train and test sets\nWe will only fit the model on the first 2 years(365* 24 * 2 hours) of data, then evaluate it on the remaining 3 years of data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and test sets\nvalues = reframed.values\nn_train_hours = 365 * 24*2\ntrain = values[:n_train_hours, :]\ntest = values[n_train_hours:, :]\n# split into input and outputs\ntrain_X, train_y = train[:, :8], train[:, 8]\ntest_X, test_y = test[:, :8], test[:, 8]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# reshaping input to be 3D [samples, timesteps, features]\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\ntest_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Designing Network**\nWe will define the LSTM with 100 neurons in the first hidden layer and a Dropout Layer of 0.3,Next there will be another hidden layer of 50 neurons and a Dropout of 0.2. Similarly, there will two more hidden layers with respective 0.2 Dropouts and 50 neurons. In Final Layer, 1 neuron in the output layer for predicting pollution. In the activation Function we used linear function, because of sequential dataset. In the batch size we used three days (24*3 Hours) data. Optimizer function we used Adam method. Loss function we used mean squared error. By monitoring the value of test data loss function, stop the training model when it is not decreasing, and save the current best model."},{"metadata":{"trusted":true},"cell_type":"code","source":"# design network\nmodel = Sequential()\nmodel.add(LSTM(100, return_sequences = True, input_shape=(train_X.shape[1], train_X.shape[2])))\nmodel.add(Dropout(0.3))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units = 50))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(1,activation='linear'))\n\nmodel.compile(loss='mse', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Fitting the Network**\nNetwork is fit with epochs size of 50 , batch size of 72."},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit network\nhistory = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Plotting the graph of Train Loss and Test Loss**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot history\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Making Prediction/Forecasting**\nForecasting the results and invert the scaling of the prediction and test data to check."},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a prediction\nyhat = model.predict(test_X)\ntest_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n\n# invert scaling for forecast\ninv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Calculating the RMSE and MAE values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate RMSE and MAE\nrmse = sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)\nmae = (mean_absolute_error(inv_y, inv_yhat))\nprint('Test MAE: %.3f' % mae)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Plotting the Graph of Actual vs Predicted**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Actual :', inv_y)\nprint('Predicted:', inv_yhat)\n# plot history\npyplot.plot(inv_y, label='Actual')\npyplot.plot(inv_yhat, label='Predicted')\npyplot.legend()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}