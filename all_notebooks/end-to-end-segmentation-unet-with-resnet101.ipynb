{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initial Setup:","metadata":{"id":"TWvVlFLWlyp_"}},{"cell_type":"markdown","source":"### Running the model for aprix 120 epochs reachs almost 90% accuracy:\n#### Here only 100 are used (small image size for faster training)","metadata":{}},{"cell_type":"markdown","source":"### 1. Install required packages","metadata":{"id":"T9FjtNfakZXH"}},{"cell_type":"code","source":"%%capture\n!pip install tensorflow_addons\n!pip install albumentations\n!pip install segmentation_models\n!pip install keras\n!sudo apt install zip unzip","metadata":{"id":"QZszDpT9kDGS","execution":{"iopub.status.busy":"2021-05-29T00:16:36.172996Z","iopub.execute_input":"2021-05-29T00:16:36.173337Z","iopub.status.idle":"2021-05-29T00:16:59.361661Z","shell.execute_reply.started":"2021-05-29T00:16:36.173305Z","shell.execute_reply":"2021-05-29T00:16:59.360473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Import needed modules","metadata":{"id":"Rq3WTvy-km7M"}},{"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras","metadata":{"id":"pJmHRULzu9BQ","outputId":"d853e86c-4e69-494a-a0bf-36dda29e7817","execution":{"iopub.status.busy":"2021-05-29T00:16:59.366233Z","iopub.execute_input":"2021-05-29T00:16:59.366531Z","iopub.status.idle":"2021-05-29T00:16:59.375611Z","shell.execute_reply.started":"2021-05-29T00:16:59.366501Z","shell.execute_reply":"2021-05-29T00:16:59.374802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt \nimport matplotlib.image as mpimg \nimport numpy as np \nimport pandas as pd\nimport random\n\nfrom PIL import Image\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow_datasets as tfds\nfrom functools import partial\n\nfrom keras.optimizers import *\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\n\n\nfrom keras import backend as keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nfrom glob import glob\n\nimport albumentations as A\nimport tensorflow_addons as tfa\n\nfrom segmentation_models import Unet\nfrom segmentation_models.metrics import iou_score\n\nimport cv2 as cv2","metadata":{"id":"HWdfGaUskLOr","execution":{"iopub.status.busy":"2021-05-29T00:16:59.379058Z","iopub.execute_input":"2021-05-29T00:16:59.379357Z","iopub.status.idle":"2021-05-29T00:16:59.388606Z","shell.execute_reply.started":"2021-05-29T00:16:59.379332Z","shell.execute_reply":"2021-05-29T00:16:59.387826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Create the DatasetManager Class \nThis class handles the reading, preprocessing, augmenting as well as displaying the data.\n\nThe get_dataset() method return a ready to use tensorflow dataset for training, validation and testing.","metadata":{"id":"XMrWU873kVav"}},{"cell_type":"code","source":"class DatasetManager:\n    def __init__(self, dataset_path, class_csv_file, bs = 10, augmentations = None):\n        self.dataset_path = dataset_path\n        self.class_csv_file = class_csv_file\n        self._class_df = None\n        self.label_dict = None\n        self.batchsize = bs\n        self.seed = 1\n        self.dataset_dirname  = None\n        self.dataset_raw = None\n        self.buffer_size = 10000\n        self.image_size = None\n        self.dataset_img_filenames = None \n        self.transforms_image = None\n        self.transforms_image_and_mask = None\n        self._init_manager()\n\n    def _init_manager(self):\n        self._prepare_labeldict()\n        if augmentations != None:\n            self.transforms_image = augmentations[\"img_augmentation\"]\n            self.transforms_image_and_mask = augmentations[\"img_mask_augmentation\"]\n\n    def _prepare_labeldict(self):\n        self._class_df = pd.read_csv(os.path.join(self.dataset_path, self.class_csv_file))\n        colors = self._class_df[[\"r\", \"g\", \"b\"]].values.tolist()\n        colors = [tuple(color) for color in colors]\n        category = self._class_df[[\"name\"]].values.tolist()\n        self.label_dict = {\"COLORS\": colors, \"CATEGORIES\": category}\n    \n    def show_label_encoding(self):\n        return self._class_df\n    \n    def get_label_info(self):\n        return self.label_dict\n    \n    def _augment_data(self, datapoint): \n        input_image = datapoint['image']\n        input_mask = datapoint['segmentation_mask']\n        return input_image, input_mask\n\n    def _process_data(self, image_path, mask_path):\n        image, mask = self._get_image(image_path), self._get_image(mask_path, mask=True)\n        if self.dataset_dirname == \"train\":\n            aug_img = tf.numpy_function(func=self._aug_training, inp=[image, mask], Tout=(tf.float32,tf.float32))\n            datapoint = self._normalize_img_and_colorcorrect_mask(aug_img[0],aug_img[1])\n            return datapoint[0], datapoint[1]\n        else:\n            aug_img = tf.numpy_function(func=self._aug_basic, inp=[image, mask], Tout=(tf.float32,tf.float32))\n            datapoint = self._normalize_img_and_colorcorrect_mask(aug_img[0],aug_img[1])\n            return datapoint[0], datapoint[1]\n\n    def _get_filenpaths(self): \n        dataset_img_filenames = tf.data.Dataset.list_files(self.dataset_path + self.dataset_dirname+\"/\"+ \"*.png\", seed=self.seed)\n        image_paths = os.path.join(self.dataset_path,self.dataset_dirname, \"*\")\n        mask_paths = os.path.join(self.dataset_path,self.dataset_dirname+\"_labels\", \"*\")\n        image_list = sorted(glob(image_paths))\n        mask_list = sorted(glob(mask_paths))\n        return image_list, mask_list     \n\n    def _get_image(self, image_path,  mask=False):\n        img = tf.io.read_file(image_path)\n        if not mask:\n            img = tf.cast(tf.image.decode_png(img, channels=3), dtype=tf.float32)\n        else:\n            img = tf.cast(tf.image.decode_png(img, channels=3), dtype=tf.float32)\n        return img\n\n    def _aug_training(self,image, mask):\n        # augment image and mask\n        img_mask_data = {\"image\":image, \"mask\":mask}\n        aug_image_and_mask = self.transforms_image_and_mask(**img_mask_data)\n        aug_img = aug_image_and_mask[\"image\"]\n        aug_mask = aug_image_and_mask[\"mask\"]\n        # augment image only\n        img_data = {\"image\":aug_img}\n        aug_data =  self.transforms_image(**img_data)\n        aug_img = aug_data[\"image\"]\n\n        aug_img = tf.cast(aug_img, tf.float32)\n        aug_img = tf.image.resize(aug_img, size=self.image_size)\n        aug_mask = tf.cast(aug_mask, tf.float32)\n        aug_mask = tf.image.resize(aug_mask, size=self.image_size)\n        aug_img = tf.clip_by_value(aug_img, 0,255)\n        return (aug_img, aug_mask)\n    \n    def _aug_basic(self,image, mask):\n        aug_img = tf.cast(image, tf.float32)\n        aug_img = tf.image.resize(aug_img, size=self.image_size)\n        aug_mask = tf.cast(mask, tf.float32)\n        aug_mask = tf.image.resize(aug_mask, size=self.image_size)\n        return (aug_img, aug_mask)\n\n    def _normalize_img_and_colorcorrect_mask(self,input_image, input_mask): \n        input_image = tf.cast(input_image, tf.float32) / 255.0\n        one_hot_map = []\n        for color in self.label_dict[\"COLORS\"]:\n            class_map = tf.reduce_all(tf.equal(input_mask, color), axis=-1)\n            one_hot_map.append(class_map)\n        one_hot_map = tf.stack(one_hot_map, axis=-1)\n        one_hot_map = tf.cast(one_hot_map, tf.float32)\n        return (input_image, one_hot_map)\n        \n    @tf.function    \n    def _set_shapes(self, img, mask):  \n        img.set_shape((self.image_size[0],self.image_size[1],3))\n        mask.set_shape((self.image_size[0],self.image_size[1],32))\n        return img,mask\n    \n    def _restore_original_mask_colors(self, mask):\n        new_mask = mask\n        h,w = new_mask.shape \n        new_mask = np.reshape(new_mask, (h*w,1))\n        dummy_mask = np.ndarray(shape=(h,w, 3))\n        dummy_mask =  np.reshape(dummy_mask, (h*w, 3))\n        for idx, pixel in enumerate(new_mask):\n            dummy_mask[idx] = np.asarray(data_manager.label_dict[\"COLORS\"][int(pixel)])\n        return np.reshape(dummy_mask, (h,w,3))/255.\n    \n    def _get_prepared_dataset(self):\n        if self.dataset_dirname == \"train\":\n            self.dataset = self.dataset.map(self._process_data, num_parallel_calls=self.AUTOTUNE).prefetch(self.AUTOTUNE)\n            self.dataset = self.dataset.map(self._set_shapes, num_parallel_calls=self.AUTOTUNE).shuffle(150).repeat().batch(self.batchsize ).prefetch(self.AUTOTUNE)\n            self.train_ds = self.dataset\n            return self.train_ds\n        elif self.dataset_dirname == \"val\":\n            self.dataset = self.dataset.map(self._process_data, num_parallel_calls=self.AUTOTUNE).prefetch(self.AUTOTUNE)\n            self.dataset = self.dataset.map(self._set_shapes, num_parallel_calls=self.AUTOTUNE).repeat().batch(self.batchsize ).prefetch(self.AUTOTUNE)\n            self.val_ds = self.dataset\n            return self.val_ds\n        elif self.dataset_dirname == \"test\":\n            self.dataset = self.dataset.map(self._process_data, num_parallel_calls=self.AUTOTUNE).prefetch(self.AUTOTUNE)\n            self.dataset = self.dataset.map(self._set_shapes, num_parallel_calls=self.AUTOTUNE).repeat().batch(1).prefetch(self.AUTOTUNE)\n            self.test_ds = self.dataset\n            return self.test_ds\n\n    def show_batch(self, ds, fsize = (15,5)):\n        image_batch, label_batch = next(iter(ds)) \n        image_batch = image_batch.numpy()\n        label_batch = label_batch.numpy()\n        for i in range(len(image_batch)):\n            fig, (ax1, ax2 )= plt.subplots(1, 2, figsize=fsize)\n            fig.suptitle('Image Label')\n            ax1.imshow(image_batch[i])\n            ax2.imshow(self._restore_original_mask_colors(np.argmax(label_batch[i], axis=-1)))\n\n    def get_dataset(self, dataset_dirname, image_size = (128,128)): \n        self.AUTOTUNE = tf.data.experimental.AUTOTUNE\n        self.image_size = image_size \n        self.dataset_dirname = dataset_dirname\n        self.image_list, self.mask_list  = self._get_filenpaths()\n        self.dataset = tf.data.Dataset.from_tensor_slices((self.image_list, self.mask_list))\n        return self._get_prepared_dataset()","metadata":{"id":"kInzAakXkTIP","execution":{"iopub.status.busy":"2021-05-29T00:16:59.391183Z","iopub.execute_input":"2021-05-29T00:16:59.391598Z","iopub.status.idle":"2021-05-29T00:16:59.430865Z","shell.execute_reply.started":"2021-05-29T00:16:59.391534Z","shell.execute_reply":"2021-05-29T00:16:59.429812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Preperation:","metadata":{"id":"83IJUWE2welb"}},{"cell_type":"code","source":"DATA_PATH = r\"../input/camvid/CamVid/\"\nCLASS_CSV_FILENAME = \"class_dict.csv\"\nBATCH_SIZE = 8\nNUM_PIXELS_SQRT  = 32*2 #Resnet expects integer multiplies of 32\n                        # Please adjust according to your GPU 32*11, 32*15 etc\nIM_SIZE = (NUM_PIXELS_SQRT, NUM_PIXELS_SQRT)\nOUTPUT_CHANNELS = 32","metadata":{"id":"Dv02UBLBikAw","execution":{"iopub.status.busy":"2021-05-29T00:16:59.434666Z","iopub.execute_input":"2021-05-29T00:16:59.434922Z","iopub.status.idle":"2021-05-29T00:16:59.445006Z","shell.execute_reply.started":"2021-05-29T00:16:59.434898Z","shell.execute_reply":"2021-05-29T00:16:59.443968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Prepare augmentations\nDue to the low amount of data (369 train images) and the complexity of the task (Semantic Segmentation) we use data augmentation to improve performance. Keep in mind that certain augmentations such as rotation and flipping must be applied to the images as well as corresponding masks. However, other augmentations like changing the brightness or contrast must only be applied to the images.","metadata":{"id":"yzIx8F1hix_Q"}},{"cell_type":"code","source":"image_mask_augmentations = A.Compose([A.Rotate(limit=20),\n                                      A.HorizontalFlip(p=0.3),\n                                      A.Resize(NUM_PIXELS_SQRT, NUM_PIXELS_SQRT, interpolation= cv2.INTER_NEAREST, p = 1),\n                                      A.RandomSizedCrop(min_max_height=(int(NUM_PIXELS_SQRT*0.5), int(NUM_PIXELS_SQRT*1)), \n                                                        height=NUM_PIXELS_SQRT, width=NUM_PIXELS_SQRT, p=0.8),\n                                      ])\n\nimage_augmentation =  A.Compose([A.RandomGamma(p=0.8)])\n\naugmentations = {\"img_augmentation\": image_augmentation, \"img_mask_augmentation\": image_mask_augmentations}","metadata":{"id":"FabAT6Yoi4Xs","execution":{"iopub.status.busy":"2021-05-29T00:16:59.446937Z","iopub.execute_input":"2021-05-29T00:16:59.447346Z","iopub.status.idle":"2021-05-29T00:16:59.456883Z","shell.execute_reply.started":"2021-05-29T00:16:59.447294Z","shell.execute_reply":"2021-05-29T00:16:59.456195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Get the datasets by instantiating a DatasetManager object and calling get_dataset() ","metadata":{"id":"t9-xN1WKmybX"}},{"cell_type":"code","source":"data_manager = DatasetManager(DATA_PATH, CLASS_CSV_FILENAME, BATCH_SIZE, augmentations = augmentations)\n\ntrain_ds = data_manager.get_dataset(\"train\",  image_size = IM_SIZE) \nval_ds = data_manager.get_dataset(\"val\", image_size = IM_SIZE) \ntest_ds = data_manager.get_dataset(\"test\", image_size = IM_SIZE) ","metadata":{"id":"83gchTbfmymw","execution":{"iopub.status.busy":"2021-05-29T00:16:59.458373Z","iopub.execute_input":"2021-05-29T00:16:59.458896Z","iopub.status.idle":"2021-05-29T00:16:59.892811Z","shell.execute_reply.started":"2021-05-29T00:16:59.458859Z","shell.execute_reply":"2021-05-29T00:16:59.892068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. (Optional) Investigate augmentation result samples.\nHint: Augmentation is only performed on the train_ds","metadata":{"id":"tk1HCm0YnZUl"}},{"cell_type":"code","source":" data_manager.show_batch(train_ds)","metadata":{"id":"wiDO_KoonhZR","outputId":"8acd90ca-420c-4a81-ed1a-adc33642ed97","execution":{"iopub.status.busy":"2021-05-29T00:16:59.895999Z","iopub.execute_input":"2021-05-29T00:16:59.896272Z","iopub.status.idle":"2021-05-29T00:17:08.268337Z","shell.execute_reply.started":"2021-05-29T00:16:59.896246Z","shell.execute_reply":"2021-05-29T00:17:08.267475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Prepare the model\nWe use a standard UNet architecture with a Resnet101 backbone pretrained on imagenet.\n\nCurrently the following architectures are supported:\n- Unet\n- FPN\n- Linknet\n- PSPNet\n\nFeel free to try different backbones:\n- resnet18, resnet34, resnet50, resnet101, resnet152 \n- seresnet18, seresnet34, seresnet50, seresnet101, seresnet152,seresnext50, seresnext101, senet154\n- resnext50, resnext101 \n- vgg16, vgg19 \n- densenet121, densenet169, densenet201 \n- inceptionresnetv2, inceptionv3 \n- mobilenet, mobilenetv2 \n- efficientnetb0, efficientnetb1, efficientnetb2, efficientnetb3, efficientnetb4, efficientnetb5, efficientnetb6, efficientnetb7\n","metadata":{"id":"uZGvUC8EprZW"}},{"cell_type":"code","source":"base_model = Unet(backbone_name='resnet101', encoder_weights='imagenet', encoder_freeze=True, activation=\"softmax\", classes =32)\nmodel = base_model\n\nmodel.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","metadata":{"id":"GnWzUCbQpwel","execution":{"iopub.status.busy":"2021-05-29T00:17:08.269951Z","iopub.execute_input":"2021-05-29T00:17:08.270302Z","iopub.status.idle":"2021-05-29T00:17:10.841707Z","shell.execute_reply.started":"2021-05-29T00:17:08.270264Z","shell.execute_reply":"2021-05-29T00:17:10.840941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Train the model","metadata":{"id":"HsjL_PKavm31"}},{"cell_type":"code","source":"rl = ReduceLROnPlateau(monitor='val_accuracy',factor=0.1, patience=8,verbose=1,mode=\"max\",min_lr=0.0001)\n\nEPOCHS = 100\nSTEPS_PER_EPOCH =369//BATCH_SIZE\nVALIDATION_STEPS = 100 // BATCH_SIZE\n\nmodel_history = model.fit(train_ds, epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_steps=VALIDATION_STEPS,\n                          validation_data=val_ds, callbacks=[rl])","metadata":{"id":"Rmr23PXsseoX","outputId":"ce56903c-03de-491d-c1e5-4c2615a04a1d","execution":{"iopub.status.busy":"2021-05-29T00:17:10.843057Z","iopub.execute_input":"2021-05-29T00:17:10.843432Z","iopub.status.idle":"2021-05-29T00:49:11.349348Z","shell.execute_reply.started":"2021-05-29T00:17:10.843397Z","shell.execute_reply":"2021-05-29T00:49:11.348625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions for upsampling and color correction","metadata":{"id":"YcLpKiyawIq7"}},{"cell_type":"code","source":"def upsample_img(img, size = (720,960)):\n    upsampled_img = tf.image.resize(img, size, antialias=True, method=tf.image.ResizeMethod.BILINEAR)  \n    return upsampled_img\n\ndef upsample_mask(mask, size = (720,960)):\n    upsampled_mask = np.argmax(mask, axis= -1)\n    upsampled_mask = tf.image.resize(np.expand_dims(upsampled_mask,-1), size, antialias=True, method=tf.image.ResizeMethod.BILINEAR)  \n    return reset_original_mask_colors(np.squeeze(upsampled_mask))\n\ndef reset_original_mask_colors(mask):\n    h,w = mask.shape \n    new_mask = np.reshape(mask, (h*w, 1))\n    dummy_mask = np.ndarray(shape=(h*w, 3))\n\n    for idx, pixel in enumerate(new_mask):\n        dummy_mask[idx] = np.asarray(data_manager.label_dict[\"COLORS\"][int(pixel)])\n    return np.reshape(dummy_mask, (h,w,3))/255","metadata":{"id":"TgHXGmGkRDZ9","execution":{"iopub.status.busy":"2021-05-29T00:49:11.351229Z","iopub.execute_input":"2021-05-29T00:49:11.351491Z","iopub.status.idle":"2021-05-29T00:49:11.362347Z","shell.execute_reply.started":"2021-05-29T00:49:11.351463Z","shell.execute_reply":"2021-05-29T00:49:11.361323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from operator import add","metadata":{"id":"LoW9tUDa4QGW","execution":{"iopub.status.busy":"2021-05-29T00:49:11.36362Z","iopub.execute_input":"2021-05-29T00:49:11.363953Z","iopub.status.idle":"2021-05-29T00:49:11.379066Z","shell.execute_reply.started":"2021-05-29T00:49:11.363919Z","shell.execute_reply":"2021-05-29T00:49:11.378404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_STEPS = 1 \nnum_test_imgs = 1 \nresults = model.predict(data_manager.val_ds, batch_size = 1, steps = TEST_STEPS)\n\nbatch_images = None\nbatch_masks = None\n\nfor img,mask in data_manager.val_ds.take(num_test_imgs): #16\n    batch_images=img\n    batch_masks=mask\n    batch_result = model.predict(batch_images)","metadata":{"id":"rEsW1vKwRJGW","execution":{"iopub.status.busy":"2021-05-29T00:49:11.380198Z","iopub.execute_input":"2021-05-29T00:49:11.380603Z","iopub.status.idle":"2021-05-29T00:49:15.438811Z","shell.execute_reply.started":"2021-05-29T00:49:11.380568Z","shell.execute_reply":"2021-05-29T00:49:15.437971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample prediction","metadata":{"id":"xHpu1etSwpbt"}},{"cell_type":"code","source":"batch_result = model.predict(batch_images)","metadata":{"id":"KJrdut-pX3vP","execution":{"iopub.status.busy":"2021-05-29T00:49:15.441335Z","iopub.execute_input":"2021-05-29T00:49:15.441718Z","iopub.status.idle":"2021-05-29T00:49:15.499939Z","shell.execute_reply.started":"2021-05-29T00:49:15.441679Z","shell.execute_reply":"2021-05-29T00:49:15.499202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(upsample_img(batch_images[0]))","metadata":{"id":"r8-QzOqnX3yN","outputId":"2f9e3442-a108-4974-d8bd-546d39954346","execution":{"iopub.status.busy":"2021-05-29T00:49:15.50114Z","iopub.execute_input":"2021-05-29T00:49:15.501596Z","iopub.status.idle":"2021-05-29T00:49:15.763234Z","shell.execute_reply.started":"2021-05-29T00:49:15.501541Z","shell.execute_reply":"2021-05-29T00:49:15.762507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(upsample_mask(batch_masks[0]))","metadata":{"id":"z3X7icT_X31S","outputId":"8f0173ce-c421-4ded-f03f-c396b13dd344","execution":{"iopub.status.busy":"2021-05-29T00:49:15.765408Z","iopub.execute_input":"2021-05-29T00:49:15.765774Z","iopub.status.idle":"2021-05-29T00:49:19.135122Z","shell.execute_reply.started":"2021-05-29T00:49:15.765737Z","shell.execute_reply":"2021-05-29T00:49:19.134104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(upsample_mask(batch_result[0]))","metadata":{"id":"2w65EFKIX895","outputId":"bb4225ee-9318-491b-c9c7-c6e821cf2752","execution":{"iopub.status.busy":"2021-05-29T00:49:19.139688Z","iopub.execute_input":"2021-05-29T00:49:19.142107Z","iopub.status.idle":"2021-05-29T00:49:22.52914Z","shell.execute_reply.started":"2021-05-29T00:49:19.142039Z","shell.execute_reply":"2021-05-29T00:49:22.528403Z"},"trusted":true},"execution_count":null,"outputs":[]}]}