{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Let's conduct some exploratory data analysis and sentiment analysis on the debate transcripts and see what we find.**\n\n**Credit**\n\nThanks to Branden Ciranni for posting the debate transcripts\nhttps://www.kaggle.com/brandenciranni/democratic-debate-transcripts-2020\n\nThanks to Oumainma Hourrance for posting the sentiment analysis training data\nhttps://www.kaggle.com/oumaimahourrane/imdb-reviews\n\n**Workflow**\n\n1. Exploratory data analysis\n2. Basic NLP on speech text\n3. Review word usage\n4. Train a model to detect sentiment\n5. Conduct sentiment analysis on speech text"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Input, Embedding, Dense, Dropout, BatchNormalization, Activation, Bidirectional, LSTM\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nimport re\nfrom operator import itemgetter\nimport collections\n\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.util import ngrams\nfrom nltk import pos_tag\nfrom nltk import RegexpParser\nnltk.download('stopwords')\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nfrom collections import defaultdict\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/democratic-debate-transcripts-2020/debate_transcripts_v3_2020-02-26.csv\", encoding=\"cp1252\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for null values\ndf.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop rows that do not contain a speaking time\ndf.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Review all sections\ndf.debate_section.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Review all speakers\ndf.speaker.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.speaker.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_total_speaking_time = df.groupby(df.speaker)[\"speaking_time_seconds\"].sum().sort_values()\n# Review mean and median total speaking time\ndf_total_speaking_time.mean(), df_total_speaking_time.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's drop speakers who had limited speaking time and view the remaining speakers\ndf = df[df.groupby(df.speaker)[\"speaking_time_seconds\"].transform(\"sum\") > 1100]\ndf.speaker.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,7))\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=15)\nplt.ylabel('Total Speaking Time', fontsize=20)\nplt.xlabel('Speaker', fontsize=20)\ndf.groupby(df.speaker)[\"speaking_time_seconds\"].sum().sort_values().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add a column for the speech with stop words and punctuation removed\nstop_words = set(nltk.corpus.stopwords.words('english'))\nfor word in [\"its\", \"would\", \"us\", \"then\", \"so\", \"it\", \"thats\", \"going\", \"also\"]:\n    stop_words.add(word)\ndf[\"speech_cleaned\"] = df[\"speech\"].apply(lambda x: \" \".join([re.sub(r'[^\\w\\d]','', item.lower()) for item in x.split() if re.sub(r'[^\\w\\d]','', item.lower()) not in stop_words]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Word Usage**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look the most words used. 'People' has been by far the most used word.\nt = Tokenizer()\nt.fit_on_texts(df.speech_cleaned)\ntop_20_words = sorted(t.word_counts.items(), key=itemgetter(1), reverse=True)[:20]\ntop_20_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a word cloud with the most used words\nwordcloud = WordCloud()\nwordcloud.generate_from_frequencies(dict(top_20_words))\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a tokenizer for each candidate in order to create a bag of words for each.\ntokenizers = defaultdict(Tokenizer)\nfor name in df.speaker.unique():\n    tokenizers[name].fit_on_texts(df.speech_cleaned[df.speaker == name])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for candidate in df.speaker.unique():\n    print(candidate , \"\\n\", sorted(tokenizers[candidate].word_counts.items(), key=itemgetter(1), reverse = True)[:10], \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"One thing that jumped out to me was how often Biden uses the word \"fact\", so I compared it across the other cadidates below. Out of curiousity, I also compared use of the word 'Trump'."},{"metadata":{"trusted":true},"cell_type":"code","source":"fact_dict = dict()\nfor candidate in df.speaker.unique():\n    if \"fact\" in tokenizers[candidate].word_index:\n        fact_dict[candidate] = tokenizers[candidate].word_counts[\"fact\"]\n    else:\n        fact_dict[candidate] = 0\nsorted(fact_dict.items(), key=itemgetter(1), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Healthcare appears to be a common word as well, let's see which candidates speak use the word 'healthcare' most often\nhealthcare_dict = dict()\nfor candidate in df.speaker.unique():\n    if \"healthcare\" in tokenizers[candidate].word_index:\n        healthcare_dict[candidate] = tokenizers[candidate].word_counts[\"healthcare\"]\n    else:\n        healthcare_dict[candidate] = 0\nsorted(healthcare_dict.items(), key=itemgetter(1), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unsurprisingly, Bernie Sanders and Elizabeth Warren use the word 'healthcare' the most. It is worth noting, that Michael Bloomberg did not use the word once."},{"metadata":{},"cell_type":"markdown","source":"**N-grams**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize all text in order\ntext = \"\"\ntokenized = list()\nfor speech in df.speech_cleaned:\n    text += \" \" + speech\ntokenized = text.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common bi-grams\nn_grams = collections.Counter(ngrams(tokenized, 2))\nn_grams.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common tri-grams\nn_grams = collections.Counter(ngrams(tokenized, 3))\nn_grams.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common quad-grams\nn_grams = collections.Counter(ngrams(tokenized, 4))\nn_grams.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common quint-grams\nn_grams = collections.Counter(ngrams(tokenized, 5))\nn_grams.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top Joe Biden N-grams"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize all Joe Biden text in order\ntext = \"\"\ntokenized = list()\nfor speech in df.speech_cleaned[df.speaker==\"Joe Biden\"]:\n    text += \" \" + speech\ntokenized = text.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common Biden bi-grams\nn_grams = collections.Counter(ngrams(tokenized, 2))\nn_grams.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common Biden tri-grams\nn_grams = collections.Counter(ngrams(tokenized, 3))\nn_grams.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top Bernie Sanders N-grams"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenize all Bernie Sanders text in order\ntext = \"\"\ntokenized = list()\nfor speech in df.speech_cleaned[df.speaker==\"Bernie Sanders\"]:\n    text += \" \" + speech\ntokenized = text.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common Bernie Sanders bi-grams\nn_grams = collections.Counter(ngrams(tokenized, 2))\nn_grams.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Most common Bernie Sanders tri-grams\nn_grams = collections.Counter(ngrams(tokenized, 3))\nn_grams.most_common(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sentiment Analysis**\n\nLet's train a model to detect sentiment, then examine how positive or negative each speaker is. For this, we use IMDB movie reviews as training data. A smaller dataset was selected to keep training time down, but it should be good enough to provide an overview."},{"metadata":{"trusted":true},"cell_type":"code","source":"# review the speech lengths\ndf_speech_length = df[\"speech\"].apply(lambda x: len(x.split()))\ndf_speech_length.hist(bins=30)\ndf_speech_length.mean(), df_speech_length.median(), np.percentile(df_speech_length, 80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set max sequence length\nmax_len = 150","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load reviews for sentiment analysis\ndf_reviews = pd.read_csv(\"../input/imdb-reviews/dataset.csv\", encoding=\"cp1252\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reviews.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create stemmer\nstemmer = PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Clean up review text\ndf_reviews[\"SentimentTextCleaned\"] = df_reviews[\"SentimentText\"].apply(lambda x: \" \".join([stemmer.stem(re.sub(r'[^\\w\\d]','', item.lower())) for item in x.split() if re.sub(r'[^\\w\\d]','', item.lower()) not in stop_words]))                                                                                                      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at cleaned up reviews\ndf_reviews[\"SentimentTextCleaned\"][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_tokenize = Tokenizer()\nreview_tokenize.fit_on_texts(df_reviews[\"SentimentTextCleaned\"])\nX_sentiment = pad_sequences(review_tokenize.texts_to_sequences(df_reviews[\"SentimentTextCleaned\"]), maxlen=max_len, padding=\"post\")\nY_sentiment = df_reviews[\"Sentiment\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build a model for sentiment analysis\nsentiment_model = Sequential([\n    Embedding(len(review_tokenize.word_index) + 1, 64),\n    Bidirectional(LSTM(32, return_sequences=True)),\n    Bidirectional(LSTM(16)),\n    Dense(64),\n    BatchNormalization(),\n    Activation(\"relu\"),\n    Dropout(.25),\n    Dense(16),\n    BatchNormalization(),\n    Activation(\"relu\"),\n    Dropout(.25),\n    Dense(2, activation=\"softmax\")\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_model.fit(X_sentiment, Y_sentiment, validation_split=.1, epochs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stem cleaned up speech in the debate data\ndf[\"speech_cleaned\"] = df[\"speech_cleaned\"].apply(lambda x: \" \".join([stemmer.stem(item) for item in x.split()]))\n# Review stemmed speech\ndf[\"speech_cleaned\"][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a column to show the sentiment of each speech\npredictions = []\nfor speech in df[\"speech_cleaned\"]:\n  prediction = sentiment_model.predict(pad_sequences(review_tokenize.texts_to_sequences([speech]), maxlen=max_len, padding=\"post\"))\n  predictions.append(prediction[0][1])\ndf[\"sentiment\"] = predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Review sentiment by speaker. The higher the number the more positive their speech is.\nplt.figure(figsize=(20,7))\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=15)\nplt.ylabel(\"Average Sentiment\", fontsize=20)\nplt.xlabel(\"Speaker\", fontsize=20)\ndf.groupby(df.speaker)[\"sentiment\"].mean().sort_values().plot.bar()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}