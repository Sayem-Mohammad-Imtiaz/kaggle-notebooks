{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Butterfly species clssification using Pytorch**\n\n![Image](https://www.thefactsite.com/wp-content/uploads/2016/12/butterfly-facts-702x347.jpg)\n\n### Dataset\n\n- Train, Test. Validation data set for 50 butterfly species. All images are 224 X 224 X 3 in jpg format .\n- Train set consists of 4955 images partitioned into 50 sub directories one for each species.\n- Test set consists of 250 images partitioned into 50 sub directories with 5 test images per species.\n- Valid set consists of 250 images partitioned into 50 sub directories with 5 validation images per species.","metadata":{}},{"cell_type":"markdown","source":"## Imports\n- Importing the necessay PyTorch libraries for the model and data.\n- Matplotlib and seaborn for visualizations.\n- Scikit-Learn for some performance metrics.","metadata":{}},{"cell_type":"code","source":"# libraries\nimport os\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nmatplotlib.rcParams['figure.facecolor'] = '#ffffff'\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as T\nfrom torch.utils.data import random_split\nfrom torchvision.utils import make_grid\nfrom tqdm.notebook import tqdm\n\nimport time\nimport pandas as pd\n\nfrom sklearn.metrics import classification_report,confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:54.295188Z","iopub.execute_input":"2021-07-24T14:58:54.295518Z","iopub.status.idle":"2021-07-24T14:58:54.301686Z","shell.execute_reply.started":"2021-07-24T14:58:54.295478Z","shell.execute_reply":"2021-07-24T14:58:54.3007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Preparation","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"../input/butterfly-images40-species/butterflies\"\n\ntrain_dir = os.path.join(DATA_DIR,\"train\")\nvalid_dir = os.path.join(DATA_DIR,\"valid\")\ntest_dir = os.path.join(DATA_DIR,\"test\")\n\nos.listdir(test_dir)[:4]","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:54.321083Z","iopub.execute_input":"2021-07-24T14:58:54.321371Z","iopub.status.idle":"2021-07-24T14:58:54.32953Z","shell.execute_reply.started":"2021-07-24T14:58:54.321344Z","shell.execute_reply":"2021-07-24T14:58:54.328478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(DATA_DIR + \"/train\")[:10]","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:54.333987Z","iopub.execute_input":"2021-07-24T14:58:54.334254Z","iopub.status.idle":"2021-07-24T14:58:54.341104Z","shell.execute_reply.started":"2021-07-24T14:58:54.33423Z","shell.execute_reply":"2021-07-24T14:58:54.340262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining transforms\n\nimagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\nimage_size = 224\nbatch_size = 32\n\ntrain_transforms = T.Compose([\n    T.Resize(image_size),\n    T.RandomHorizontalFlip(),\n    T.RandomRotation(20),\n    T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n    T.ToTensor(),\n    T.Normalize(*imagenet_stats)\n])\n\nval_test_transforms = T.Compose([\n    T.Resize(image_size),\n    T.ToTensor(),\n    T.Normalize(*imagenet_stats)\n])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:54.366292Z","iopub.execute_input":"2021-07-24T14:58:54.366534Z","iopub.status.idle":"2021-07-24T14:58:54.372491Z","shell.execute_reply.started":"2021-07-24T14:58:54.36651Z","shell.execute_reply":"2021-07-24T14:58:54.371436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datastes\ntrain_ds = ImageFolder(train_dir,train_transforms)\nval_ds = ImageFolder(valid_dir, val_test_transforms)\ntest_ds = ImageFolder(test_dir, val_test_transforms)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:54.376661Z","iopub.execute_input":"2021-07-24T14:58:54.377053Z","iopub.status.idle":"2021-07-24T14:58:54.472587Z","shell.execute_reply.started":"2021-07-24T14:58:54.377024Z","shell.execute_reply":"2021-07-24T14:58:54.471786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classes = train_ds.classes\nlen_classes = len(classes)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:54.473964Z","iopub.execute_input":"2021-07-24T14:58:54.474314Z","iopub.status.idle":"2021-07-24T14:58:54.48024Z","shell.execute_reply.started":"2021-07-24T14:58:54.474279Z","shell.execute_reply":"2021-07-24T14:58:54.479294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classes and indexes belonging to them\ntrain_ds.class_to_idx","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:54.482283Z","iopub.execute_input":"2021-07-24T14:58:54.482672Z","iopub.status.idle":"2021-07-24T14:58:54.491023Z","shell.execute_reply.started":"2021-07-24T14:58:54.482631Z","shell.execute_reply":"2021-07-24T14:58:54.490034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# No of images in train/test and valid sets\nprint(f\"Train : {len(train_ds)} \\nValidation : {len(val_ds)} \\nTest : {len(test_ds)}\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:54.492611Z","iopub.execute_input":"2021-07-24T14:58:54.492982Z","iopub.status.idle":"2021-07-24T14:58:54.50123Z","shell.execute_reply.started":"2021-07-24T14:58:54.492945Z","shell.execute_reply":"2021-07-24T14:58:54.500428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Loaders\ntrain_dl = DataLoader(train_ds, batch_size = batch_size, shuffle = True, num_workers = 3, pin_memory = True)\nval_dl = DataLoader(val_ds, batch_size = batch_size, shuffle = False, num_workers = 3, pin_memory = True)\ntest_dl = DataLoader(test_ds, batch_size = batch_size, shuffle = False, num_workers = 3, pin_memory = True)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:54.540934Z","iopub.execute_input":"2021-07-24T14:58:54.541373Z","iopub.status.idle":"2021-07-24T14:58:54.54656Z","shell.execute_reply.started":"2021-07-24T14:58:54.541333Z","shell.execute_reply":"2021-07-24T14:58:54.545578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Take a look at some of the images from train batch","metadata":{}},{"cell_type":"code","source":"# function to denormalize\ndef denormalize(images, means, stds):\n    means = torch.tensor(means).reshape(1, 3, 1, 1)\n    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n    return images * stds + means\n\n# to show the images\ndef show_images(img,label):\n    plt.figure(figsize = [20,14])\n    for i in range(25):\n        plt.subplot(5,5,i+1)\n        img[i] = denormalize(img[i], *imagenet_stats)\n        plt.imshow(img[i].permute(1,2,0))\n        plt.title(classes[label[i]])\n        plt.axis(\"off\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:54.548449Z","iopub.execute_input":"2021-07-24T14:58:54.548866Z","iopub.status.idle":"2021-07-24T14:58:54.559501Z","shell.execute_reply.started":"2021-07-24T14:58:54.548825Z","shell.execute_reply":"2021-07-24T14:58:54.558604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one batch \nimages,labels = iter(train_dl).next()\n\nprint(images.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:54.562771Z","iopub.execute_input":"2021-07-24T14:58:54.563193Z","iopub.status.idle":"2021-07-24T14:58:55.724792Z","shell.execute_reply.started":"2021-07-24T14:58:54.563167Z","shell.execute_reply":"2021-07-24T14:58:55.723705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show the images\nshow_images(images,labels)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:55.726508Z","iopub.execute_input":"2021-07-24T14:58:55.726765Z","iopub.status.idle":"2021-07-24T14:58:56.857825Z","shell.execute_reply.started":"2021-07-24T14:58:55.726737Z","shell.execute_reply":"2021-07-24T14:58:56.857039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **MODEL**","metadata":{}},{"cell_type":"code","source":"# A class we can extend to use in our model.\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}],{} train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch+1, \"last_lr: {:.5f},\".format(result['lrs'][-1]) if 'lrs' in result else '', \n            result['train_loss'], result['val_loss'], result['val_acc']))\n\n        \n        \ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:56.859407Z","iopub.execute_input":"2021-07-24T14:58:56.859831Z","iopub.status.idle":"2021-07-24T14:58:56.87265Z","shell.execute_reply.started":"2021-07-24T14:58:56.859795Z","shell.execute_reply":"2021-07-24T14:58:56.871768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build the model using a pre-trained model and changing the last layer\n\nfrom torchvision import models\n\nclass ButterFlyModel(ImageClassificationBase):\n    def __init__(self, num_classes, pretrained=True):\n        super().__init__()\n        # Use a pretrained model\n        self.network = models.googlenet(pretrained=True)\n        # Replace last layer\n        self.network.fc = nn.Linear(self.network.fc.in_features, num_classes)\n\n    def forward(self, xb):\n        return self.network(xb)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:56.874052Z","iopub.execute_input":"2021-07-24T14:58:56.874506Z","iopub.status.idle":"2021-07-24T14:58:56.88724Z","shell.execute_reply.started":"2021-07-24T14:58:56.87447Z","shell.execute_reply":"2021-07-24T14:58:56.886343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### To use a GPU","metadata":{}},{"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:56.888592Z","iopub.execute_input":"2021-07-24T14:58:56.889186Z","iopub.status.idle":"2021-07-24T14:58:56.898994Z","shell.execute_reply.started":"2021-07-24T14:58:56.889152Z","shell.execute_reply":"2021-07-24T14:58:56.898038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = get_default_device()\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:56.900312Z","iopub.execute_input":"2021-07-24T14:58:56.900841Z","iopub.status.idle":"2021-07-24T14:58:56.913693Z","shell.execute_reply.started":"2021-07-24T14:58:56.900803Z","shell.execute_reply":"2021-07-24T14:58:56.912864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\ntest_dl = DeviceDataLoader(test_dl,device)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:56.915024Z","iopub.execute_input":"2021-07-24T14:58:56.915622Z","iopub.status.idle":"2021-07-24T14:58:56.921175Z","shell.execute_reply.started":"2021-07-24T14:58:56.915587Z","shell.execute_reply":"2021-07-24T14:58:56.920216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Training the Model**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom tqdm.notebook import tqdm\n\n# for evaluation\n@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\n# training\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_losses = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history\n\n# get the learning rate\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\n# releasing resources after one epoch\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n\n    # Set up custom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,\n                                                steps_per_epoch=len(train_loader))\n\n    for epoch in range(epochs):\n        # Training Phase\n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n\n            # Gradient clipping\n            if grad_clip:\n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:56.924398Z","iopub.execute_input":"2021-07-24T14:58:56.92492Z","iopub.status.idle":"2021-07-24T14:58:56.940061Z","shell.execute_reply.started":"2021-07-24T14:58:56.924883Z","shell.execute_reply":"2021-07-24T14:58:56.939347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ButterFlyModel(len(train_ds.classes))\nto_device(model,device)\nprint(\"Model Built..\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:56.94204Z","iopub.execute_input":"2021-07-24T14:58:56.942459Z","iopub.status.idle":"2021-07-24T14:58:57.147778Z","shell.execute_reply.started":"2021-07-24T14:58:56.942422Z","shell.execute_reply":"2021-07-24T14:58:57.146831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# score of the model before the training process\n\nhistory = [evaluate(model, val_dl)]\nhistory","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:57.149211Z","iopub.execute_input":"2021-07-24T14:58:57.149552Z","iopub.status.idle":"2021-07-24T14:58:58.164609Z","shell.execute_reply.started":"2021-07-24T14:58:57.149522Z","shell.execute_reply":"2021-07-24T14:58:58.163701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining parameters for the model\nepochs = 15\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\n\nopt_func = torch.optim.Adam","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:58.169158Z","iopub.execute_input":"2021-07-24T14:58:58.171262Z","iopub.status.idle":"2021-07-24T14:58:58.178208Z","shell.execute_reply.started":"2021-07-24T14:58:58.171217Z","shell.execute_reply":"2021-07-24T14:58:58.17706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"code","source":"print(\"Starting Training .. ..\")\nstart = time.time()\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, val_dl, \n                         grad_clip=grad_clip, \n                         weight_decay=weight_decay, \n                         opt_func=opt_func)\nend = time.time()\nprint(f\"Finished training in {end-start} seconds..\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T14:58:58.1842Z","iopub.execute_input":"2021-07-24T14:58:58.186877Z","iopub.status.idle":"2021-07-24T15:10:10.112315Z","shell.execute_reply.started":"2021-07-24T14:58:58.186839Z","shell.execute_reply":"2021-07-24T15:10:10.111168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Testing**","metadata":{}},{"cell_type":"code","source":"evaluate(model,test_dl)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:10:28.831224Z","iopub.execute_input":"2021-07-24T15:10:28.831578Z","iopub.status.idle":"2021-07-24T15:10:29.667238Z","shell.execute_reply.started":"2021-07-24T15:10:28.831542Z","shell.execute_reply":"2021-07-24T15:10:29.666079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracies = [x[\"val_acc\"] for x in history]\nval_loss = [x[\"val_loss\"] for x in history]","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:10:34.838939Z","iopub.execute_input":"2021-07-24T15:10:34.839305Z","iopub.status.idle":"2021-07-24T15:10:34.84424Z","shell.execute_reply.started":"2021-07-24T15:10:34.839268Z","shell.execute_reply":"2021-07-24T15:10:34.843092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss = [x.get(\"train_loss\") for x in history]","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:10:40.214899Z","iopub.execute_input":"2021-07-24T15:10:40.215263Z","iopub.status.idle":"2021-07-24T15:10:40.220413Z","shell.execute_reply.started":"2021-07-24T15:10:40.215232Z","shell.execute_reply":"2021-07-24T15:10:40.219517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracies\n\nplt.plot(accuracies,marker = \"*\",c = \"green\")\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('Accuracy vs. No. of epochs');","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:10:41.951696Z","iopub.execute_input":"2021-07-24T15:10:41.952059Z","iopub.status.idle":"2021-07-24T15:10:42.137941Z","shell.execute_reply.started":"2021-07-24T15:10:41.952013Z","shell.execute_reply":"2021-07-24T15:10:42.136956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# losses\n\nplt.plot(train_loss, '-bx')\nplt.plot(val_loss, '-rx')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['Training', 'Validation'])\nplt.title('Loss vs. No. of epochs');","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:10:42.13972Z","iopub.execute_input":"2021-07-24T15:10:42.140079Z","iopub.status.idle":"2021-07-24T15:10:42.339025Z","shell.execute_reply.started":"2021-07-24T15:10:42.140042Z","shell.execute_reply":"2021-07-24T15:10:42.338059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# learning rates\n\nlrs = np.concatenate([x.get('lrs', []) for x in history])\nplt.plot(lrs)\nplt.xlabel('Batch no.')\nplt.ylabel('Learning rate')\nplt.title('Learning Rate vs. Batch no.');","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:10:43.964637Z","iopub.execute_input":"2021-07-24T15:10:43.964956Z","iopub.status.idle":"2021-07-24T15:10:44.106075Z","shell.execute_reply.started":"2021-07-24T15:10:43.96492Z","shell.execute_reply":"2021-07-24T15:10:44.105147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions v/s labels","metadata":{}},{"cell_type":"code","source":"# function to get prediction and labels\ndef makePrediction(dataset, num_images = 25):\n    predicted = []\n    actual = []\n    for i in range(num_images):\n        # getting label and image\n        img, label = dataset[i]\n        actual.append(label)\n        \n        # making prediction\n        img_batched = to_device(img.unsqueeze(0),device)\n        \n        _,pred = torch.max(model(img_batched), dim = 1)\n        pred = pred[0].item()\n        predicted.append(pred)\n        \n    return predicted,actual\n\n# function to denormalize and permute\ndef denPermute_test(images, means, stds):\n    means = torch.tensor(means).reshape(3, 1, 1)\n    stds = torch.tensor(stds).reshape(3, 1, 1)\n    images = images * stds + means\n    return images.permute(1,2,0)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:10:45.561863Z","iopub.execute_input":"2021-07-24T15:10:45.562229Z","iopub.status.idle":"2021-07-24T15:10:45.569123Z","shell.execute_reply.started":"2021-07-24T15:10:45.562198Z","shell.execute_reply":"2021-07-24T15:10:45.568263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred,actual = makePrediction(test_ds,len(test_ds))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:10:45.73124Z","iopub.execute_input":"2021-07-24T15:10:45.731562Z","iopub.status.idle":"2021-07-24T15:10:51.323725Z","shell.execute_reply.started":"2021-07-24T15:10:45.731533Z","shell.execute_reply":"2021-07-24T15:10:51.322883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotPredictions(testData, num=25):\n    plt.figure(figsize=[22,14])\n    for i in range(num):\n        plt.subplot(5,5,i+1)\n        image,_ = testData[i]\n        image = denPermute_test(image, *imagenet_stats)\n        plt.imshow(image)\n        plt.xlabel(f\"Actual : {classes[actual[i]]}\")\n        plt.ylabel(f\"Pred : {classes[pred[i]]}\")\n        plt.xticks([])\n        plt.yticks([])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:10:58.196672Z","iopub.execute_input":"2021-07-24T15:10:58.196999Z","iopub.status.idle":"2021-07-24T15:10:58.203594Z","shell.execute_reply.started":"2021-07-24T15:10:58.196969Z","shell.execute_reply":"2021-07-24T15:10:58.202139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotPredictions(test_ds)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:10:59.958728Z","iopub.execute_input":"2021-07-24T15:10:59.959105Z","iopub.status.idle":"2021-07-24T15:11:01.701274Z","shell.execute_reply.started":"2021-07-24T15:10:59.959072Z","shell.execute_reply":"2021-07-24T15:11:01.700494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion matrix","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=[25,14])\nsns.heatmap(confusion_matrix(pred,actual),annot=True, fmt = \"d\" ,cmap = \"Blues\");","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:11:15.994517Z","iopub.execute_input":"2021-07-24T15:11:15.994855Z","iopub.status.idle":"2021-07-24T15:11:25.472395Z","shell.execute_reply.started":"2021-07-24T15:11:15.994825Z","shell.execute_reply":"2021-07-24T15:11:25.471557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Classification Report**","metadata":{}},{"cell_type":"code","source":"print(classification_report(pred,actual))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T15:11:25.473888Z","iopub.execute_input":"2021-07-24T15:11:25.474326Z","iopub.status.idle":"2021-07-24T15:11:25.488301Z","shell.execute_reply.started":"2021-07-24T15:11:25.474284Z","shell.execute_reply":"2021-07-24T15:11:25.487331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **SUMMARY**\n- A classification task on a dataste containing images of different species of butterflies.\n- Prepare data into train,test and validation part using pytorch's built in modules.\n- Performinng data augmentation to potentially improve the performance of our model.\n- Defining classes and functions for ease of visulaizing the model imporovemnts.\n- Using a pre-trained model for better scores after trying CNNs and Simple Neural Nets.\n- Testing and visuakizing the results.\n- Dataset at : https://www.kaggle.com/gpiosenka/butterfly-images40-species\n- Helper functions from : https://jovian.ai/learn/deep-learning-with-pytorch-zero-to-gans","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}