{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DIABETES PREDICTION MODELLING","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Importing the Required Libraries","metadata":{}},{"cell_type":"code","source":"# basic analysis library\nimport sys\nimport numpy as np\nimport pandas as pd\n\n# visual eda library\nfrom pandas_profiling import ProfileReport\nimport webbrowser as web\n\n# visualization libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# feature engineering library\nfrom sklearn.preprocessing import StandardScaler\n\n# classification modelling libraries\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\n\n# performance measurement library\nfrom sklearn import metrics as m\n\n# enable display of complete array/dataframe/series\nnp.set_printoptions(threshold = sys.maxsize)\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\n# ignoring warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('All Required Libraries Imported')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/machine-learning-for-diabetes-with-python/diabetes_data.csv')\nprint(data.shape)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Analysis","metadata":{}},{"cell_type":"markdown","source":"We have already seen that the data contains only numerical features, let us check whether any of them are stored as text.","metadata":{}},{"cell_type":"code","source":"# classification of features\nnumerical = [var for var in data.columns if data[var].dtype != 'O' and var != 'Outcome']\ncategorical = [var for var in data.columns if data[var].dtype == 'O' and var != 'Outcome']\ntarget = ['Outcome']\nprint('There are', len(numerical), 'numerical variables')\nprint('There are', len(categorical), 'categorical variables')\nprint('There are', len(target), 'target variables')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So there are no features which are stored as text, so we can safely proceed. Let us see the summary statistics of the data.","metadata":{}},{"cell_type":"code","source":"# summary statistics of data\ndata.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number and percentage of null values in data\nsum_null = data.isnull().sum()\nmean_null = data.isnull().mean()\nnulls = pd.concat([sum_null, mean_null], axis = 1)\nnulls.rename(columns = {0:'count', 1:'percentage'}, inplace = True)\nnulls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above table, it is obvious that there are no null values in our data.","metadata":{}},{"cell_type":"code","source":"# identifying duplicate rows\ndata[data.duplicated()].shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nor there are any duplicate rows.","metadata":{}},{"cell_type":"code","source":"# correlation matrix of the data\nfigure = plt.figure(figsize = (10, 10))\ncorr_matrix = data[numerical].corr().round(2)\nsns.heatmap(data = corr_matrix, annot = True)\n\n# the less correlation, the better. More correlation means presence of duplication of features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above heatmap, it can be seen that there is a high correlation between Pregnancies and Age (about 54%). Since the data size is small, it is obvious and we can ignore this for now.","metadata":{}},{"cell_type":"code","source":"# distribution of all features\nfig, axes = plt.subplots(ncols = 4, nrows = 2, figsize = (20, 10))\n\nsns.kdeplot(data['Pregnancies'], ax = axes[0,0])\nsns.kdeplot(data['Glucose'], ax = axes[0,1])\nsns.kdeplot(data['BloodPressure'], ax = axes[0,2])\nsns.kdeplot(data['SkinThickness'], ax = axes[0,3])\nsns.kdeplot(data['Insulin'], ax = axes[1,0])\nsns.kdeplot(data['BMI'], ax = axes[1,1])\nsns.kdeplot(data['DiabetesPedigreeFunction'], ax = axes[1,2])\nsns.kdeplot(data['Age'], ax = axes[1,3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pairplot for data\nsns.pairplot(data[numerical])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Initial Model","metadata":{}},{"cell_type":"markdown","source":"Since there are no missing values, let's run a sample model to check on the performance of the model on the current data.","metadata":{}},{"cell_type":"code","source":"# splitting data into train and test datasets\nX = data.drop(['Outcome'], axis = 1)\ny = data['Outcome']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nX_train.shape, X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RandomForestClassifier model\nimodel = RandomForestClassifier()\nimodel.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training and testing accuracy\nprint('Training Accuracy:', imodel.score(X_train, y_train))\nprint('Testing Accuracy:', imodel.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training accuracy is 100%, which indicates that the model is clearly overfitting. The same would be for other tree based models. So tree based algorithms are not suitable for this type of data.","metadata":{}},{"cell_type":"code","source":"# LogisticRegression model\nimodel2 = LogisticRegression()\nimodel2.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training and testing accuracy\nprint('Training Accuracy:', imodel2.score(X_train, y_train))\nprint('Testing Accuracy:', imodel2.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here the training and testing accuracies are quite low, plus the testing accuracy is greater than the training accuracy. So we need to process and correct the data in order to improve the accuracies.","metadata":{}},{"cell_type":"markdown","source":"## Outliers Analysis","metadata":{}},{"cell_type":"code","source":"# copy of orignial data, so that original data does not get overridden\ndata_clean = data.copy()\ndata_clean.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# outlier distribution of all features\nfig, axes = plt.subplots(ncols = 4, nrows = 2, figsize = (20, 10))\n\nsns.boxplot(y = data_clean['Pregnancies'], ax = axes[0,0])\nsns.boxplot(y = data_clean['Glucose'], ax = axes[0,1])\nsns.boxplot(y = data_clean['BloodPressure'], ax = axes[0,2])\nsns.boxplot(y = data_clean['SkinThickness'], ax = axes[0,3])\nsns.boxplot(y = data_clean['Insulin'], ax = axes[1,0])\nsns.boxplot(y = data_clean['BMI'], ax = axes[1,1])\nsns.boxplot(y = data_clean['DiabetesPedigreeFunction'], ax = axes[1,2])\nsns.boxplot(y = data_clean['Age'], ax = axes[1,3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pregnancies","metadata":{}},{"cell_type":"code","source":"# boxplot of Pregnancies to check for outliers\nsns.boxplot(y = data_clean['Pregnancies'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary statistics of Pregnancies\nprint(data_clean['Pregnancies'].describe())\niqr_pr = data_clean['Pregnancies'].describe()['75%'] - data_clean['Pregnancies'].describe()['25%']\nprint('Inter Quartile Range ', iqr_pr)\nlower_limit_pr = data_clean['Pregnancies'].describe()['25%'] - (1.5 * iqr_pr)\nupper_limit_pr = data_clean['Pregnancies'].describe()['75%'] + (1.5 * iqr_pr)\nprint('Lower Limit ', lower_limit_pr)\nprint('Upper Limit ', upper_limit_pr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target level summary statistics\npreg_stats = pd.concat([data_clean[data_clean['Outcome'] == 0]['Pregnancies'].describe(), data_clean[data_clean['Outcome'] == 1]['Pregnancies'].describe()], axis = 1)\npreg_stats.columns = ['Outcome 0','Outcome 1']\npreg_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pregnancies can be 0, but it cannot be as high as the upper whisker value of 13, so it'd be best to replace outliers with the respective median values .","metadata":{}},{"cell_type":"code","source":"# replacing outliers with 2nd Quartile value\n\n# outcome 0\ndata_clean.loc[(data_clean['Pregnancies'] > upper_limit_pr) & (data_clean['Outcome'] == 0), 'Pregnancies'] = data_clean[data_clean['Outcome'] == 0]['Pregnancies'].describe()['50%']\n\n# outcome 1\ndata_clean.loc[(data_clean['Pregnancies'] > upper_limit_pr) & (data_clean['Outcome'] == 1), 'Pregnancies'] = data_clean[data_clean['Outcome'] == 1]['Pregnancies'].describe()['50%']\n\nsns.boxplot(y = data_clean['Pregnancies'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Glucose","metadata":{}},{"cell_type":"code","source":"# boxplot of Glucose to check for outliers\nsns.boxplot(y = data_clean['Glucose'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary statistics of Glucose\nprint(data_clean['Glucose'].describe())\niqr_gl = data_clean['Glucose'].describe()['75%'] - data_clean['Glucose'].describe()['25%']\nprint('Inter Quartile Range ', iqr_gl)\nlower_limit_gl = data_clean['Glucose'].describe()['25%'] - (1.5 * iqr_gl)\nupper_limit_gl = data_clean['Glucose'].describe()['75%'] + (1.5 * iqr_gl)\nprint('Lower Limit ', lower_limit_gl)\nprint('Upper Limit ', upper_limit_gl)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target level summary statistics\nglu_stats = pd.concat([data_clean[data_clean['Outcome'] == 0]['Glucose'].describe(), data_clean[data_clean['Outcome'] == 1]['Glucose'].describe()], axis = 1)\nglu_stats.columns = ['Outcome 0','Outcome 1']\nglu_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Glucose value cannot be 0, and it also cannot be as high as the maximum value (199), so we need to replace 0s with respective median values and upper (considerable) outliers with the respectuve 3rd quartile values.","metadata":{}},{"cell_type":"code","source":"# replacing outliers with quartile values\n\n# outcome 0\ndata_clean.loc[(data_clean['Glucose'] < data_clean['Glucose'].describe()['25%']) & (data_clean['Outcome'] == 0), 'Glucose'] = data_clean[data_clean['Outcome'] == 0]['Glucose'].describe()['50%']\ndata_clean.loc[(data_clean['Glucose'] > data_clean['Glucose'].describe()['75%']) & (data_clean['Outcome'] == 0), 'Glucose'] = data_clean[data_clean['Outcome'] == 0]['Glucose'].describe()['75%']\n\n# outcome 1\ndata_clean.loc[(data_clean['Glucose'] < data_clean['Glucose'].describe()['25%']) & (data_clean['Outcome'] == 1), 'Glucose'] = data_clean[data_clean['Outcome'] == 1]['Glucose'].describe()['50%']\ndata_clean.loc[(data_clean['Glucose'] > data_clean['Glucose'].describe()['75%']) & (data_clean['Outcome'] == 1), 'Glucose'] = data_clean[data_clean['Outcome'] == 1]['Glucose'].describe()['75%']\n\nsns.boxplot(y = data_clean['Glucose'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Blood Presssure","metadata":{}},{"cell_type":"code","source":"# boxplot of BloodPressure to check for outliers\nsns.boxplot(y = data_clean['BloodPressure'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary statistics of BloodPressure\nprint(data_clean['BloodPressure'].describe())\niqr_bp = data_clean['BloodPressure'].describe()['75%'] - data_clean['BloodPressure'].describe()['25%']\nprint('Inter Quartile Range ', iqr_bp)\nlower_limit_bp = data_clean['BloodPressure'].describe()['25%'] - (1.5 * iqr_bp)\nupper_limit_bp = data_clean['BloodPressure'].describe()['75%'] + (1.5 * iqr_bp)\nprint('Lower Whisker ', lower_limit_bp)\nprint('Upper Whisker ', upper_limit_bp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target level summary statistics\nbp_stats = pd.concat([data_clean[data_clean['Outcome'] == 0]['BloodPressure'].describe(), data_clean[data_clean['Outcome'] == 1]['BloodPressure'].describe()], axis = 1)\nbp_stats.columns = ['Outcome 0','Outcome 1']\nbp_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The BloodPressure value cannot be zero, and also it cannot be as high as the maximum values (122 and 114). So we will replace these small values with medians and higher values (>95) with 3rd quartile values.","metadata":{}},{"cell_type":"code","source":"# replacing outliers with median and 3rd quartile values\n\n# outcome 0\ndata_clean.loc[(data_clean['BloodPressure'] < data_clean['BloodPressure'].describe()['25%']) & (data_clean['Outcome'] == 0), 'BloodPressure'] = data_clean[data_clean['Outcome'] == 0]['BloodPressure'].describe()['50%']\ndata_clean.loc[(data_clean['BloodPressure'] > 95) & (data_clean['Outcome'] == 0), 'BloodPressure'] = data_clean[data_clean['Outcome'] == 0]['BloodPressure'].describe()['75%']\n\n# outcome 1\ndata_clean.loc[(data_clean['BloodPressure'] < data_clean['BloodPressure'].describe()['25%']) & (data_clean['Outcome'] == 1), 'BloodPressure'] = data_clean[data_clean['Outcome'] == 1]['BloodPressure'].describe()['50%']\ndata_clean.loc[(data_clean['BloodPressure'] > 95) & (data_clean['Outcome'] == 1), 'BloodPressure'] = data_clean[data_clean['Outcome'] == 1]['BloodPressure'].describe()['75%']\n\nsns.boxplot(y = data_clean['BloodPressure'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Skin Thickness","metadata":{}},{"cell_type":"code","source":"# boxplot of SkinThickness to check for outliers\nsns.boxplot(y = data_clean['SkinThickness'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary statistics of SkinThickness\nprint(data_clean['SkinThickness'].describe())\niqr_st = data_clean['SkinThickness'].describe()['75%'] - data_clean['SkinThickness'].describe()['25%']\nprint('Inter Quartile Range ', iqr_st)\nlower_limit_st = data_clean['SkinThickness'].describe()['25%'] - (1.5 * iqr_st)\nupper_limit_st = data_clean['SkinThickness'].describe()['75%'] + (1.5 * iqr_st)\nprint('Lower Limit ', lower_limit_st)\nprint('Upper Limit ', upper_limit_st)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target level summary statistics\nst_stats = pd.concat([data_clean[data_clean['Outcome'] == 0]['SkinThickness'].describe(), data_clean[data_clean['Outcome'] == 1]['SkinThickness'].describe()], axis = 1)\nst_stats.columns = ['Outcome 0','Outcome 1']\nst_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The SkinThickness value cannot be 0. So we replace this value with the median, and the outliers with the 3rd Quartile values.","metadata":{}},{"cell_type":"code","source":"# replacing outliers with quartile value\n\n# outcome 0\ndata_clean.loc[(data_clean['SkinThickness'] < data_clean['SkinThickness'].describe()['50%']) & (data_clean['Outcome'] == 0), 'SkinThickness'] = data_clean[data_clean['Outcome'] == 0]['SkinThickness'].describe()['50%']\ndata_clean.loc[(data_clean['SkinThickness'] > data_clean['SkinThickness'].describe()['75%']) & (data_clean['Outcome'] == 0), 'SkinThickness'] = data_clean[data_clean['Outcome'] == 0]['SkinThickness'].describe()['75%']\n\n# outcome 1\ndata_clean.loc[(data_clean['SkinThickness'] < data_clean['SkinThickness'].describe()['50%']) & (data_clean['Outcome'] == 1), 'SkinThickness'] = data_clean[data_clean['Outcome'] == 1]['SkinThickness'].describe()['50%']\ndata_clean.loc[(data_clean['SkinThickness'] > data_clean['SkinThickness'].describe()['75%']) & (data_clean['Outcome'] == 1), 'SkinThickness'] = data_clean[data_clean['Outcome'] == 1]['SkinThickness'].describe()['75%']\n\nsns.boxplot(y = data_clean['SkinThickness'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Insulin","metadata":{}},{"cell_type":"code","source":"# boxplot of Insulin to check for outliers\nsns.boxplot(y = data_clean['Insulin'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary statistics of Insulin\nprint(data_clean['Insulin'].describe())\niqr_in = data_clean['Insulin'].describe()['75%'] - data_clean['Insulin'].describe()['25%']\nprint('Inter Quartile Range ', iqr_in)\nlower_limit_in = data_clean['Insulin'].describe()['25%'] - (1.5 * iqr_in)\nupper_limit_in = data_clean['Insulin'].describe()['75%'] + (1.5 * iqr_in)\nprint('Lower Limit ', lower_limit_in)\nprint('Upper Limit ', upper_limit_in)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target level summary statistics\nin_stats = pd.concat([data_clean[data_clean['Outcome'] == 0]['Insulin'].describe(), data_clean[data_clean['Outcome'] == 1]['Insulin'].describe()], axis = 1)\nin_stats.columns = ['Outcome 0','Outcome 1']\nin_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are outliers above the upper whisker, but insulin level value cannot be 0. So we need to replace these 0s with the median and the outliers with 3rd quartile values. Notice that the median of Insulin where Outcome = 1 is 0, so we replace this value with the overall median value.","metadata":{}},{"cell_type":"code","source":"# replacing 0s with 2nd quartile value and outliers with 3rd quartile value\n\n# outcome 0\ndata_clean.loc[(data_clean['Insulin'] < data_clean['Insulin'].describe()['50%']) & (data_clean['Outcome'] == 0), 'Insulin'] = data_clean[data_clean['Outcome'] == 0]['Insulin'].describe()['50%']\ndata_clean.loc[(data_clean['Insulin'] > data_clean['Insulin'].describe()['75%']) & (data_clean['Outcome'] == 0), 'Insulin'] = data_clean[data_clean['Outcome'] == 0]['Insulin'].describe()['75%']\n\n# outcome 1\ndata_clean.loc[(data_clean['Insulin'] < data_clean['Insulin'].describe()['50%']) & (data_clean['Outcome'] == 1), 'Insulin'] = data_clean['Insulin'].describe()['50%']\ndata_clean.loc[(data_clean['Insulin'] > data_clean['Insulin'].describe()['75%']) & (data_clean['Outcome'] == 1), 'Insulin'] = data_clean[data_clean['Outcome'] == 1]['Insulin'].describe()['75%']\n\nsns.boxplot(y = data_clean['Insulin'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Body Mass Index (BMI)","metadata":{}},{"cell_type":"code","source":"# boxplot of BMI to check for outliers\nsns.boxplot(y = data_clean['BMI'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary statistics of BMI\nprint(data_clean['BMI'].describe())\niqr_bmi = data_clean['BMI'].describe()['75%'] - data_clean['BMI'].describe()['25%']\nprint('Inter Quartile Range ', iqr_bmi)\nlower_limit_bmi = data_clean['BMI'].describe()['25%'] - (1.5 * iqr_bmi)\nupper_limit_bmi = data_clean['BMI'].describe()['75%'] + (1.5 * iqr_bmi)\nprint('Lower Limit ', lower_limit_bmi)\nprint('Upper Limit ', upper_limit_bmi)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target level summary statistics\nbmi_stats = pd.concat([data_clean[data_clean['Outcome'] == 0]['BMI'].describe(), data_clean[data_clean['Outcome'] == 1]['BMI'].describe()], axis = 1)\nbmi_stats.columns = ['Outcome 0','Outcome 1']\nbmi_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BMI value cannot be 0. So we need to replace these outliers with the 1st quartile and 3rd quartile values.","metadata":{}},{"cell_type":"code","source":"# replacing 0s with 1st quartile value and outliers with 3rd quartile value\n\n# outcome 0\ndata_clean.loc[(data_clean['BMI'] < data_clean['BMI'].describe()['25%']) & (data_clean['Outcome'] == 0), 'BMI'] = data_clean[data_clean['Outcome'] == 0]['BMI'].describe()['25%']\ndata_clean.loc[(data_clean['BMI'] > data_clean['BMI'].describe()['75%']) & (data_clean['Outcome'] == 0), 'BMI'] = data_clean[data_clean['Outcome'] == 0]['BMI'].describe()['75%']\n\n# outcome 1\ndata_clean.loc[(data_clean['BMI'] < data_clean['BMI'].describe()['25%']) & (data_clean['Outcome'] == 1), 'BMI'] = data_clean[data_clean['Outcome'] == 1]['BMI'].describe()['25%']\ndata_clean.loc[(data_clean['BMI'] > data_clean['BMI'].describe()['75%']) & (data_clean['Outcome'] == 1), 'BMI'] = data_clean[data_clean['Outcome'] == 1]['BMI'].describe()['75%']\n\nsns.boxplot(y = data_clean['BMI'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Diabetes Pedigree Function","metadata":{}},{"cell_type":"code","source":"# boxplot of DiabetesPedigreeFunction to check for outliers\nsns.boxplot(y = data_clean['DiabetesPedigreeFunction'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary statistics of DiabetesPedigreeFunction\nprint(data_clean['DiabetesPedigreeFunction'].describe())\niqr_dpf = data_clean['DiabetesPedigreeFunction'].describe()['75%'] - data_clean['DiabetesPedigreeFunction'].describe()['25%']\nprint('Inter Quartile Range ', iqr_dpf)\nlower_limit_dpf = data_clean['DiabetesPedigreeFunction'].describe()['25%'] - (1.5 * iqr_dpf)\nupper_limit_dpf = data_clean['DiabetesPedigreeFunction'].describe()['75%'] + (1.5 * iqr_dpf)\nprint('Lower Limit ', lower_limit_dpf)\nprint('Upper Limit ', upper_limit_dpf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target level summary statistics\ndpf_stats = pd.concat([data_clean[data_clean['Outcome'] == 0]['DiabetesPedigreeFunction'].describe(), data_clean[data_clean['Outcome'] == 1]['DiabetesPedigreeFunction'].describe()], axis = 1)\ndpf_stats.columns = ['Outcome 0','Outcome 1']\ndpf_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The lower values are valid, but we still need to replace the outliers. We will replace these outliers with the 3rd quartile values.","metadata":{}},{"cell_type":"code","source":"# replacing outliers with 3rd quartile value\n\n# outcome 0\ndata_clean.loc[(data_clean['DiabetesPedigreeFunction'] > data_clean['DiabetesPedigreeFunction'].describe()['75%']) & (data_clean['Outcome'] == 0), 'DiabetesPedigreeFunction'] = data_clean[data_clean['Outcome'] == 0]['DiabetesPedigreeFunction'].describe()['75%']\n\n# outcome 1\ndata_clean.loc[(data_clean['DiabetesPedigreeFunction'] > data_clean['DiabetesPedigreeFunction'].describe()['75%']) & (data_clean['Outcome'] == 1), 'DiabetesPedigreeFunction'] = data_clean[data_clean['Outcome'] == 1]['DiabetesPedigreeFunction'].describe()['75%']\n\nsns.boxplot(y = data_clean['DiabetesPedigreeFunction'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Age","metadata":{}},{"cell_type":"code","source":"# boxplot of Age to check for outliers\nsns.boxplot(y = data_clean['Age'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary statistics of Age\nprint('Mean\\t', data_clean['Age'].mean())\nprint('Median\\t', data_clean['Age'].median())\nprint(data_clean['Age'].describe())\niqr_age = data_clean['Age'].describe()['75%'] - data_clean['Age'].describe()['25%']\nprint('Inter Quartile Range ', iqr_age)\nlower_limit_age = data_clean['Age'].describe()['25%'] - (1.5 * iqr_age)\nupper_limit_age = data_clean['Age'].describe()['75%'] + (1.5 * iqr_age)\nprint('Lower Limit ', lower_limit_age)\nprint('Upper Limit ', upper_limit_age)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# target level summary statistics\nage_stats = pd.concat([data_clean[data_clean['Outcome'] == 0]['Age'].describe(), data_clean[data_clean['Outcome'] == 1]['Age'].describe()], axis = 1)\nage_stats.columns = ['Outcome 0','Outcome 1']\nage_stats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the other values look valid, so we need to replace the outliers with the 3rd quartile values. Since the upper whisker value (66) itself is an outlier (according to boxlpot), we can reduce it to 60.","metadata":{}},{"cell_type":"code","source":"# replacing outliers with 3rd quartile value\n\n# outcome 0\ndata_clean.loc[(data_clean['Age'] > 60) & (data_clean['Outcome'] == 0), 'Age'] = data_clean[data_clean['Outcome'] == 0]['Age'].describe()['75%']\n\n# outcome 1\ndata_clean.loc[(data_clean['Age'] > 60) & (data_clean['Outcome'] == 1), 'Age'] = data_clean[data_clean['Outcome'] == 1]['Age'].describe()['75%']\n\nsns.boxplot(y = data_clean['Age'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that all the features have been cleaned, let us compare the distributions of the features before and after cleaning.","metadata":{}},{"cell_type":"code","source":"# distribution of all features before and after cleaning\nfig, axes = plt.subplots(ncols = 4, nrows = 2, figsize = (20, 10))\n\n# before cleaning\nsns.kdeplot(data['Pregnancies'], ax = axes[0,0])\nsns.kdeplot(data['Glucose'], ax = axes[0,1])\nsns.kdeplot(data['BloodPressure'], ax = axes[0,2])\nsns.kdeplot(data['SkinThickness'], ax = axes[0,3])\nsns.kdeplot(data['Insulin'], ax = axes[1,0])\nsns.kdeplot(data['BMI'], ax = axes[1,1])\nsns.kdeplot(data['DiabetesPedigreeFunction'], ax = axes[1,2])\nsns.kdeplot(data['Age'], ax = axes[1,3])\n\n# after cleaning\nsns.kdeplot(data_clean['Pregnancies'], ax = axes[0,0], color = 'green')\nsns.kdeplot(data_clean['Glucose'], ax = axes[0,1], color = 'green')\nsns.kdeplot(data_clean['BloodPressure'], ax = axes[0,2], color = 'green')\nsns.kdeplot(data_clean['SkinThickness'], ax = axes[0,3], color = 'green')\nsns.kdeplot(data_clean['Insulin'], ax = axes[1,0], color = 'green')\nsns.kdeplot(data_clean['BMI'], ax = axes[1,1], color = 'green')\nsns.kdeplot(data_clean['DiabetesPedigreeFunction'], ax = axes[1,2], color = 'green')\nsns.kdeplot(data_clean['Age'], ax = axes[1,3], color = 'green')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pairplot for cleaned data\nsns.pairplot(data_clean[numerical])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above pairplot shows a neater correlation of all features with each other compared to the earlier one. Now that all variables are corrected, let's standardize the data.","metadata":{}},{"cell_type":"markdown","source":"## Standardization","metadata":{}},{"cell_type":"code","source":"data_scal = data_clean.copy()\ndata_scal.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take a look at the data above. All the features are at different scales. This would definitely affect the performance of the model. So we need to transform the data so that all features are at a common scale. For this purpose, we use Standardization.","metadata":{}},{"cell_type":"code","source":"# splitting into train and test datasets\nX = data_scal.drop(['Outcome'], axis = 1)\ny = data_scal['Outcome']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nX_train.shape, X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fitting the scaler on the train data\nscaler = StandardScaler()\nscaler.fit(X_train[numerical])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transforming X_train and X_test\nX_train[numerical] = scaler.transform(X_train[numerical])\nX_test[numerical] = scaler.transform(X_test[numerical])\nX_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, all the features are now at a common scale. This would definitely help in the model performance. Now that our data is ready to be fed to the model, let us run the model.","metadata":{}},{"cell_type":"markdown","source":"## Feeding Processed Data to Model","metadata":{}},{"cell_type":"markdown","source":"As discussed in the beginning, tree-based algorithms (like Decision Tree, Random Forest, Gradient Boost etc.) are not suitable for this data as they overfit on this data. So we start with Logistic Regression.","metadata":{}},{"cell_type":"code","source":"# Logistic Regression model\nmodel_lr = LogisticRegression()\nmodel_lr.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training and testing accuracy\nprint('Logistic Training Accuracy:', model_lr.score(X_train, y_train))\nprint('Logistic Testing Accuracy:', model_lr.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracies have improved very much compared to the initial model we ran at the beginning. So it can be said that our data was cleaned and scaled properly and effectively.","metadata":{}},{"cell_type":"markdown","source":"Let us feed the data to few other models as well and check their performance.","metadata":{}},{"cell_type":"code","source":"# Support Vector model\nmodel_svm = SVC()\nmodel_svm.fit(X_train, y_train)\n\n# training and testing accuracy\nprint('SVM Training Accuracy:', model_svm.score(X_train, y_train))\nprint('SVM Testing Accuracy:', model_svm.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNeighborsClassifier model\nmodel_knn = KNeighborsClassifier()\nmodel_knn.fit(X_train, y_train)\n\n# training and testing accuracy\nprint('KNN Training Accuracy:', model_knn.score(X_train, y_train))\nprint('KNN Testing Accuracy:', model_knn.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MLPClassifier model\nmodel_mlp = MLPClassifier()\nmodel_mlp.fit(X_train, y_train)\n\n# training and testing accuracy\nprint('MLP Training Accuracy:', model_mlp.score(X_train, y_train))\nprint('MLP Testing Accuracy:', model_mlp.score(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"Now that we have trained and tested for the accuracies, let's tune the hyperparameters by using the GridSearchCV function.","metadata":{}},{"cell_type":"code","source":"# best parameter function\ndef print_results(y_pred, model):\n    print('\\nBest Parameters:',model.best_params_)\n    print('\\nPrediction Metrics:\\n')\n    print('Training Accuracy:', model.score(X_train, y_train))\n    print('Testing Accuracy:', model.score(X_test, y_test))\n    print('Precision:', m.precision_score(y_test, y_pred, average = 'weighted'))\n    print('Recall:', m.recall_score(y_test, y_pred, average = 'weighted'))\n    print('F1-Score:', m.f1_score(y_test, y_pred, average = 'weighted'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression Model\nmodel_lr = LogisticRegression(random_state = 0)\nparams_lr = {'C':[1,5,10,50], 'solver':['newton-cg','lbfgs','liblinear','sag','saga'], 'max_iter':[50,100,500]}\ngrid_lr = GridSearchCV(model_lr, params_lr, scoring = 'accuracy', cv = 5, verbose = 5, n_jobs = -1, return_train_score = True)\ngrid_lr.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best parameters\ny_pred_lr = grid_lr.predict(X_test)\nprint_results(y_pred_lr, grid_lr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Model\nmodel_svm = SVC(random_state = 0)\nparams_svm = {'C':[1,5,10,50], 'kernel':['rbf','poly','sigmoid','linear']}\ngrid_svm = GridSearchCV(model_svm, params_svm, scoring = 'accuracy', cv = 5, verbose = 5, n_jobs = -1, return_train_score = True)\ngrid_svm.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best parameters\ny_pred_svm = grid_svm.predict(X_test)\nprint_results(y_pred_svm, grid_svm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K-Nearest Neighbors Model\nmodel_knn = KNeighborsClassifier()\nparams_knn = {'n_neighbors':[5,10,20,50]}\ngrid_knn = GridSearchCV(model_knn, params_knn, scoring = 'accuracy', cv = 5, verbose = 5, n_jobs = -1, return_train_score = True)\ngrid_knn.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best parameters\ny_pred_knn = grid_knn.predict(X_test)\nprint_results(y_pred_knn, grid_knn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MLP Classifier\nmodel_nn = MLPClassifier(random_state = 0)\nparams_nn = {'solver':['lbfgs','sgd','adam'], 'hidden_layer_sizes':[(50,50,50),(50,100,50),(100,)], \n             'learning_rate':['constant','invscaling','adaptive'], 'activation':['identity','logistic','tanh','relu']}\ngrid_nn = GridSearchCV(model_nn, params_nn, scoring = 'accuracy', cv = 5, verbose = 5, n_jobs = -1, return_train_score = True)\ngrid_nn.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best parameters\ny_pred_nn = grid_nn.predict(X_test)\nprint_results(y_pred_nn, grid_nn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"# performance metrics dataframe\nperf_mets = pd.DataFrame({'Model':['LR','SVM','KNN','MLP'],\n                          'Training Accuracy':[grid_lr.score(X_train, y_train), grid_svm.score(X_train, y_train), \n                                               grid_knn.score(X_train, y_train), grid_nn.score(X_train, y_train)],\n                          'Testing Accuracy':[grid_lr.score(X_test, y_test), grid_svm.score(X_test, y_test), \n                                               grid_knn.score(X_test, y_test), grid_nn.score(X_test, y_test)],\n                          'Precision':[m.precision_score(y_test, y_pred_lr, average = 'weighted'), \n                                      m.precision_score(y_test, y_pred_svm, average = 'weighted'), \n                                      m.precision_score(y_test, y_pred_knn, average = 'weighted'),\n                                      m.precision_score(y_test, y_pred_nn, average = 'weighted')],\n                          'Recall':[m.recall_score(y_test, y_pred_lr, average = 'weighted'), \n                                   m.recall_score(y_test, y_pred_svm, average = 'weighted'), \n                                   m.recall_score(y_test, y_pred_knn, average = 'weighted'), \n                                   m.recall_score(y_test, y_pred_nn, average = 'weighted')],\n                          'F1-Score':[m.f1_score(y_test, y_pred_lr, average = 'weighted'), \n                                     m.f1_score(y_test, y_pred_svm, average = 'weighted'), \n                                     m.f1_score(y_test, y_pred_knn, average = 'weighted'), \n                                     m.f1_score(y_test, y_pred_nn, average = 'weighted')]\n                         }).set_index('Model')\n\nperf_mets_perc = perf_mets.style.format({'Training Accuracy': '{:,.2%}'.format,\n                                    'Testing Accuracy': '{:,.2%}'.format,\n                                    'Precision': '{:,.2%}'.format,\n                                    'Recall': '{:,.2%}'.format,\n                                    'F1-Score': '{:,.2%}'.format})\nperf_mets_perc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# comparing the performance metrics\n\nfig, axes = plt.subplots(ncols = 3, nrows = 2, figsize = (15, 10))\n\nsns.barplot(x = perf_mets.index, y = perf_mets['Training Accuracy'], ax = axes[0,0], order = perf_mets.sort_values('Training Accuracy', ascending = False).index)\nsns.barplot(x = perf_mets.index, y = perf_mets['Testing Accuracy'], ax = axes[0,1], order = perf_mets.sort_values('Testing Accuracy', ascending = False).index)\nsns.barplot(x = perf_mets.index, y = perf_mets['Precision'], ax = axes[0,2], order = perf_mets.sort_values('Precision', ascending = False).index)\nsns.barplot(x = perf_mets.index, y = perf_mets['Recall'], ax = axes[1,0], order = perf_mets.sort_values('Recall', ascending = False).index)\nsns.barplot(x = perf_mets.index, y = perf_mets['F1-Score'], ax = axes[1,1], order = perf_mets.sort_values('F1-Score', ascending = False).index)\n\nfig.delaxes(axes[1,2])\n\nfor i in range(2):\n    for j in range(3):\n        for bar in axes[i,j].patches:\n            axes[i,j].annotate(format(bar.get_height(), '.2%'), (bar.get_x() + bar.get_width() / 2, bar.get_height()), ha = 'center', va = 'center', size = 15, xytext = (0, 8), textcoords = 'offset points')\n\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above plots shows the performance metrics of each model in decreasing order of their magnitudes. As per the plots, we can see that MLP Classifer performs better than the other algorithms.","metadata":{}},{"cell_type":"markdown","source":"## Summary","metadata":{}},{"cell_type":"markdown","source":"In this notebook, \n1. We have analyzed the data, the number of numerical and categorial variables and also the summary statistics of the dataset.\n2. We have run an initial model to check the performance of the model on the data.\n3. We have analyzed the outliers and handled them effectively with suitable techniques.\n4. We have standardized the features so that all features are on a common scale.\n5. We have run the models on the corrected and standardized data and have found out its performance metrics.\n6. We have tuned the hyperparameters of the models to get the model with best set of hyperparameters.","metadata":{}}]}