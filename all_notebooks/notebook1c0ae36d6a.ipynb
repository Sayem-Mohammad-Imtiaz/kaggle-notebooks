{"cells":[{"metadata":{"_uuid":"24b43777-c7da-4363-bf9d-426c7f8b9ebf","_cell_guid":"0ab1f69c-c842-41c0-9a52-2227c861a583","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\n# # TIME SERIES ANALYSIS\n\n# \n# \n\n# In[ ]:\n\n\n# Import the libraries\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import plotly.express as px\nfrom sklearn import linear_model  # will be using for plotting trend line\nfrom sklearn.preprocessing import MinMaxScaler # for normalizing data\nfrom sklearn.cluster import KMeans\nfrom tqdm.autonotebook import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\n# In[ ]:\n\n\n# Import the data\ndf = pd.read_csv(\"C:/Users/ANUSHKA/Desktop/Technocolabs Mini Project/data.csv\")\ndf_genre = pd.read_csv(\"C:/Users/ANUSHKA/Desktop/Technocolabs Mini Project/data_by_genres.csv\")\n\ndf_year = pd.read_csv(\"C:/Users/ANUSHKA/Desktop/Technocolabs Mini Project/data_by_year.csv\")\ndf_genre2 = pd.read_csv(\"C:/Users/ANUSHKA/Desktop/Technocolabs Mini Project/data_w_genres.csv\")\n# View the shape and columns names\nprint(df.shape)\ndf.columns\n\n\n# In[ ]:\n\n\n# Check for missing values\ndf.isnull().sum()\n\n\n# In[ ]:\n\n\n# Drop unneccessary columns\ndf.drop([\"id\", \"key\", \"mode\", \"explicit\", \"release_date\"], axis=1, inplace=True)\ndf.head()\n\n\n# In[ ]:\n\n\ncorr = df[[\"acousticness\",\"danceability\",\"energy\", \"instrumentalness\", \n           \"liveness\",\"tempo\", \"valence\", \"loudness\", \"speechiness\"]].corr()\n\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, annot=True)\n\n\n# Song Trends\n\n# In[ ]:\n\n\nyear_avg = df[[\"acousticness\",\"danceability\",\"energy\", \"instrumentalness\", \n               \"liveness\",\"tempo\", \"valence\", \"loudness\", \"speechiness\", \"year\"]].\\\ngroupby(\"year\").mean().sort_values(by=\"year\").reset_index()\n\nyear_avg.head()\n\n\n# In[ ]:\n\n\n# Create a line plot\nplt.figure(figsize=(14,8))\nplt.title(\"Song Trends Over Time\", fontdict={\"fontsize\": 15})\n\nlines = [\"acousticness\",\"danceability\",\"energy\", \n         \"instrumentalness\", \"liveness\", \"valence\", \"speechiness\"]\n\nfor line in lines:\n    ax = sns.lineplot(x='year', y=line, data=year_avg)\n    \n    \nplt.ylabel(\"value\")\nplt.legend(lines)\n\n\n# # DIFFERENTIATE GENRES\n\n# In[ ]:\n\n\n# lets perform clustering\n# data(columns) we will we using\nsong_features = pd.DataFrame()\n# normalizer instance\nscaler = MinMaxScaler()\nfor col in df.iloc[:,:-1].columns:      # excluding year col i.e, of int64 type\n    if df[col].dtypes in ['float64', 'int64']:\n        # adding normalized col\n        scaler.fit(df[[col]])\n        song_features[col] = scaler.transform(df[col].values.reshape(-1,1)).ravel()\n\n\n# In[ ]:\n\n\n# first we would like to know that how many cluster or to say Genres can be clustered \n# with less SSE(Sum of Squared Error) we will use \"Elbow method\" to find out \n\n# KMeans instance\nkm = KMeans()\nk_rng = range(1,200)  # k value\nsse = [] # sse value for each k\nfor i in k_rng:\n    km = KMeans(n_clusters = i)\n    km.fit(song_features.sample(1000))\n    # calculating sse\n    sse.append(km.inertia_) \n    \n# due to less computation power I am unable to use whole data \n# I guess 1000 sample of whole data can depict actual\n\n\n# In[ ]:\n\n\nplt.plot(k_rng,sse)\nplt.xlabel('K value')\nplt.ylabel('SSE Error')\nplt.title('Best K value')\n# plt.ylim(0,400)\n# plt.xlim(0,100)\nplt.show()\n\n\n# In[ ]:\n\n\n# looks like 25 is good value of K\nkm = KMeans(n_clusters=25)\npredicted_genres = km.fit_predict(song_features)\n\n\n# In[ ]:\n\n\nsong_features['predicted_genres'] = predicted_genres\nsong_features['predicted_genres'] = song_features['predicted_genres'].apply(lambda x: 'Genre'+ str(x))\n\n\n# In[ ]:\n\n\nsong_features.sample(10)\n\n\n# In[ ]:\n\n\n# lets see how many songs falls in each Genre and which Genre have more songs\ngenres_grp = song_features.groupby(['predicted_genres']).size()\nplt.figure(figsize=(10,6))\ngenres_grp.sort_values(ascending=True).plot.barh(color='red')\nplt.xlabel('Total Songs')\nplt.title('Genre Ranking')\nplt.show()\n\n\n# # RECOMMEND ARTISTS\n\n# In[ ]:\n\n\n# reading artists data\nartists_df = pd.read_csv(\"C:/Users/ANUSHKA/Desktop/Technocolabs Mini Project/data_by_artist.csv\")\nartists_df = artists_df.rename(columns={\"count\": \"playCount\"})\n\n\n# In[ ]:\n\n\n# we will replace each feature with its Genre for our convience and for easy tracking\nartists_df.iloc[:,1:-1] = scaler.fit_transform(artists_df.iloc[:,1:-1])\nkm = KMeans(n_clusters=25)\nartists_df['genres'] = km.fit_predict(artists_df.iloc[:,1:-1])\nartists_df = artists_df.iloc[:,[0,-3,-2,-1]]\nartists_df.head()\n\n\n# In[ ]:\n\n\n# lets create our own user list with his rating and add to artists data\nartists_df['user_id'] = np.random.randint(1000,1400,len(artists_df))\nartists_df['rating'] = np.random.randint(1,6,len(artists_df))\nartists_df.head()\n\n\n# In[ ]:\n\n\n# lets create our recommender system\ndef recommend_me(user):\n    \"\"\"This function will recommend artists to any user with its genre profile\"\"\"\n    # first we will choose user top liked genres\n    fav_genre = artists_df[artists_df['user_id']==user].sort_values(by=['rating','playCount'], ascending=False)['genres'][:5]\n    fav_genre = list(dict.fromkeys(fav_genre)) # removing duplicate if exits\n    \n    # lets clear out the artists from list whose songs has been listened by the user\n    listened_artist = artists_df.index[artists_df['artists'].isin(['Johann Sebastian Bach','Frédéric Chopin'])].tolist()\n    \n    # rest data\n    remaining_artist = artists_df.drop(listened_artist, axis=0)\n    CanBeRecommened =  remaining_artist[remaining_artist['genres'].isin(fav_genre)]\n    \n    # now lets sort our artists whose are popular in this user favorite genre\n    CanBeRecommened = CanBeRecommened.sort_values(by=['rating','playCount',], ascending=False)[['artists', 'genres', 'rating', 'playCount']][:5]\n    \n    # output will contain artists name, genres, other useres rating and song played count\n    return CanBeRecommened\n\n\n# In[ ]:\n\n\n# lets recommend this user some artists\nrecommend_me(1012)\n\n\n# In[ ]:\n\n\n# lets check which genre is user fav and did he get same recommended\nartists_df[artists_df.user_id==1012].sort_values(by='rating')['genres'].unique()\n\n\n# # POPULARITY PREDICTION\n\n# In[ ]:\n\n\nfeatures = ['year', 'danceability', 'energy', 'loudness', 'tempo']\ntracks_data = df.copy()\nfeatures_tracks_data = df[features]\n\n\n# In[ ]:\n\n\nclass Artist: \n    def __init__(self, name, popularity): \n        self.name = name\n        self.popularity = popularity\n        \n        \nclass Track: \n    def __init__(self, name, artists, popularity): \n        self.name = name\n        self.artists = artists\n        self.popularity = popularity   \n        \n        \n\ntracks = []\n\nnames = tracks_data.name.values\nartists_names = tracks_data.artists.values\npopularity = tracks_data.popularity.values\n\nfor index in range(len(names)): \n    track = Track(names[index], artists_names[index], popularity[index])\n    tracks.append(track)\n    \n    \nartists = []\nartists_names_done = []\nartists_popularities = []\n\nfor artists_str in tqdm(artists_names): \n    artists_sub_list = artists_str[1:-1].split(', ')\n    \n    track_pop = 0\n    for artist in artists_sub_list: \n        \n        if artist in artists_names_done: \n            a = [x for x in artists if x.name == artist][0]\n            artist_pop = a.popularity\n            \n        else: \n            songs_pop = [x.popularity for x in tracks if artist in x.artists]\n            artist_pop = sum(songs_pop) / len(songs_pop)\n            artists_names_done.append(artist)\n            a = Artist(artist, artist_pop)\n            artists.append(a)\n        \n        track_pop += artist_pop\n        \n    track_pop /= len(artists_sub_list)\n    artists_popularities.append(track_pop)\n    \nartists_popularities = np.asarray(artists_popularities)\n\nprint(artists_popularities.max())\n\n\n# In[ ]:\n\n\nscaler = MinMaxScaler()\nscaler.fit(features_tracks_data)\nfeatures_tracks_data = scaler.transform(features_tracks_data)\n\nprint(features_tracks_data.shape) \nfeatures_tracks_data = np.column_stack((artists_popularities / 100, features_tracks_data))\nprint(features_tracks_data.shape)\n\ny_tracks_data = tracks_data.popularity.values / 100\n\nX_train, X_test, y_train, y_test = train_test_split(features_tracks_data, y_tracks_data, test_size=0.2, random_state=27)\n\n\n# In[ ]:\n\n\nfor column in range(X_train.shape[1]): \n    print(X_train[:, column].min(), X_train[:, column].max())\n\n\n# In[ ]:\n\n\nclf = RandomForestRegressor()\nclf.fit(X_train, y_train)\n\n\n# In[ ]:\n\n\npreds = clf.predict(X_test)\n\naccuracy = clf.score(X_test, y_test)\nprint(\"Test Accuracy: {:.4f}\".format(accuracy*100))\n\naverage_error = (abs(y_test - preds)).mean()\nprint(\"{:.4f} average error\".format(average_error))\n\n\n# In[ ]:\n\n\nfor index in range(len(preds[:100])): \n    \n    pred = preds[index]\n    actual = y_test[index]\n    \n    print(\"Actual / Predicted: {:.4f} / {:.4f}\".format(actual, pred))\n\n\n# # Deployment Using Streamlit\n\n# In[ ]:\n\n\nimport streamlit as st\n\n\n\n# In[ ]:\n\n\n\n\n\n# In[ ]:\n\n\n\n\n\n# In[ ]:\n\n\n\n\n\n# In[ ]:","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}