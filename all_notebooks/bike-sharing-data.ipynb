{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Importing require libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Setting Format\npd.options.display.float_format = '{:.5f}'.format\npd.options.display.max_columns = None\npd.options.display.max_rows = None\nnp.random.seed(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing our data\n\nday = pd.read_csv(\"../input/bike-sharing-dataset/day.csv\")\nhour = pd.read_csv(\"../input/bike-sharing-dataset/hour.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Description of our data\n\nwith open('../input/bike-sharing-dataset/Readme.txt', 'r') as txt:\n    print(txt.read())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As per given information these data was transformed\n# So lets do transfrom them back to their real format to get better understanding of data\n\nday['temp'] = day['temp']*41\nhour['temp'] = hour['temp']*41\n\nday['atemp'] = day['atemp']*50\nhour['atemp'] = hour['atemp']*50\n\nday['hum'] = day['hum']*100\nhour['hum'] = hour['hum']*100\n\nday['windspeed'] = day['windspeed']*67\nhour['windspeed'] = hour['windspeed']*67","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.info() #Checking data type","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day.isna().sum() # is their any Null vales?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.isna().sum()# is their any Null vales?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day.describe().T #Lets look at Mean, median and Standard Deviation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These columns should be Category not int\n\ncol = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n       'workingday', 'weathersit']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def change_dtype(data, col):\n    for i in col:\n        if i in data.columns.to_list():\n            data[i] = data[i].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in col:\n    print(\"Name of {} col\".format(i)) #Name of Col\n    print(\"No. of NUnique\", hour[i].nunique()) #Total Nunique Values\n    print(\"Unique Values\", hour[i].unique())# All unique vales\n    print('*'*30) # to make differnce i each col\n    print()\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"change_dtype(day, col) #Changing Col\nchange_dtype(hour, col) #Changing Col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#How they look after transformation\n\nfor i in col:\n    print(\"Name of {} col\".format(i)) #Name of Col\n    print(\"No. of NUnique\", hour[i].nunique()) #Total Nunique Values\n    print(\"Unique Values\", hour[i].unique())# All unique vales\n    print('*'*30) # to make differnce i each col\n    print()\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_instant(data):\n    data.drop(['instant'], axis=1, inplace=True)\n    \ndrop_instant(day)\ndrop_instant(hour)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"mean and median in \"**Day**\" data is approxminately nearby except in case of \"**Casual**\"   "},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"mean and median in \"**hour**\" data is approxminately nearby except in case of \"**Casual, registered and cnt**\"   "},{"metadata":{},"cell_type":"markdown","source":"## Lets do some Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in day.select_dtypes(include='int'):\n    sns.distplot(day[i]) #Lets check how data is distributed\n    plt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in hour.select_dtypes(include='int'):\n    sns.distplot(hour[i]) #Lets check how data is distributed\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in day.select_dtypes(include='int'):\n    sns.boxplot(day[i]) #Is their any outlier\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in hour.select_dtypes(include='int'):\n    sns.boxplot(hour[i]) #Is their any outlier\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Casual, Registered and cnt have outlier and need to be fix\n\nPoint to be noted that cnt is sum total of casual and Registered."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in day.select_dtypes(include='float'):\n    sns.distplot(day[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in hour.select_dtypes(include='float'):\n    sns.distplot(hour[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in day.select_dtypes(include='float'):\n    sns.boxplot(day[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in hour.select_dtypes(include='float'):\n    sns.boxplot(hour[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(day.corr()) #How are data is related to each other","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day.corr()['cnt'] #Co-relation with Tagret Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.corr()['cnt'] #Co-relation with Tagret Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"day.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_df_name(df):\n    '''\n    This Function returns the name of a dateset\n    '''\n    name =[x for x in globals() if globals()[x] is df][0]\n    return name\n\n\ndef plot_stack_bar_chart(data, col, name):\n    plt.figure(figsize=(12,8)) #Size of a PLot\n    p1 = plt.bar(data[col].unique(),  # the x locations for the groups\n                data.groupby([col])['casual'].sum()) # Count of casual per season\n\n    p2 = plt.bar(data[col].unique(),  # the x locations for the groups\n                data.groupby([col])['registered'].sum(), # Count of Registered per season\n                 bottom = data.groupby([col])['casual'].sum()) # Count of casual per season\n\n    plt.ylabel('Count')\n    plt.title(\"Count by Casual and Registered for each {} in {} Data\".format(col, get_df_name(data)))\n    plt.xticks(data[col].unique(), name) # Name of unique values in columns\n    plt.legend((p1[0], p2[0]), ('Casual', 'Registered')) #setting legends as per target\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's Visualize and understand **Day** Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(day, 'season', ('1:springer', '2:summer', '3:fall', '4:winter'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(day, 'yr', ('2011', '2012'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(day, 'mnth', [str(i) for i in day['mnth'].unique()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(day, 'holiday', ('Yes', 'No'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(day, 'weekday', [str(i) for i in day['weekday'].unique()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(day, 'workingday', ('Yes', 'No'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(day, 'weathersit', ('Clear', 'Mist', 'Light Snow', 'Rain'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's Visualize and understand Hour Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(hour, 'season', ('1:springer', '2:summer', '3:fall', '4:winter'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(hour, 'yr', ('2011', '2012'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(hour, 'mnth', [str(i) for i in hour['mnth'].unique()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(hour, 'hr', [str(i) for i in hour['hr'].unique()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(hour, 'holiday', ('Yes', 'No'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(hour, 'weekday', [str(i) for i in hour['weekday'].unique()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(hour, 'workingday', ('Yes', 'No'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_stack_bar_chart(hour, 'weathersit', ('Clear', 'Mist', 'Light Snow', 'Rain'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.barplot(x = hour['holiday'], y = hour['cnt'],hue = hour['season'])\nplt.title('Holiday wise distribution of counts')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.barplot(x = hour['workingday'], y = hour['cnt'],hue = hour['season'])\nplt.title('Working Day wise distribution of counts')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,10))\nsns.barplot(x = hour['mnth'], y = hour['cnt'], hue = hour['season'])\nplt.title('Month wise distribution of counts')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.barplot(x = hour['weathersit'], y = hour['cnt'],hue = hour['season'])\nplt.title('Weather Situation wise distribution of counts')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding and Replacing outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(hour['hum'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(hour['windspeed'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.describe(include='all').T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def treat_outlier_iqr(data, col):\n    \n    #Finding 25 and 75 Quantile\n    q25, q75 = np.percentile(data[col], 25), np.percentile(data[col], 75)\n    # Inter Quantile Range\n    iqr = q75-q25\n    #Minimum and Maximum Range\n    min_r, max_r = q25-(iqr*1.5), q75+(iqr*1.5)\n    #Replacing Outliers with Mean\n    data.loc[data.loc[:, col] < min_r, col] = data[col].mean()\n    data.loc[data.loc[:, col] > max_r, col] = data[col].mean()\n    \n    return sns.boxplot(data[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treat_outlier_iqr(hour, 'hum') # Treating Outliers in Hum Column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"treat_outlier_iqr(hour, 'windspeed') # Treating Outliers in Hum Column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(hour.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above we can determine that\n\ntemp and atemp are highly Correlated\n\ncasual, Registered and Cnt are highly correlated as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = hour.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.describe(include='all').T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking Multi - Collinearity "},{"metadata":{"trusted":true},"cell_type":"code","source":"y = hour['cnt']\nx = hour.drop(['cnt', 'dteday'], axis=1) # removing cnt and dteday\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\n#Adding Constant to data\nX = add_constant(x)\n\n# Checking ViF for Multi-Collinearity\n\npd.Series([variance_inflation_factor(X.values, i) \n           for i in range(X.shape[1])], \n              index=X.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# So Temp and atemp is showing Multi-Collinarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_eng(data, col):\n    data['temp_and_atemp'] = (data['temp'] + data['atemp'])/2 #Average the column to remove multicollinearity\n    data['dteday'] = data['dteday'].astype('datetime64') # Converting column to datetime64\n    data['day'] = data['dteday'].astype('datetime64').dt.day # Extrating day from date\n    data['day'] = data['day'].astype('category') #Converting day to category\n    data.drop(['casual', col, 'dteday', 'temp', 'atemp'], axis=1, inplace=True) #Droping all the irrelevant column\n\n#Transforming Hour Data\nfeature_eng(hour, 'registered')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking to see if their is still any multicollinearity\n\ny = hour['cnt']\nx = hour.drop(['cnt'], axis=1)\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nX = add_constant(x)\n\npd.Series([variance_inflation_factor(X.values, i) \n           for i in range(X.shape[1])], \n              index=X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.describe(include='all').T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting all category columns to list\ncat_col = hour.select_dtypes(include='category').columns.to_list()\n\n#Printing all unique values in Category Columnns\nfor i in cat_col:\n    print(\"Name of {} col\".format(i))\n    print(\"No. of NUnique\", hour[i].nunique())\n    print(\"Unique Values\", hour[i].unique())\n    print('*'*30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Categorical_transformation(data):\n    '''\n    Transforming all Categorical Columns to int\n    '''\n    cat = data.select_dtypes(include='category').columns.to_list()\n    for i in cat:\n        data[i] = data[i].astype('int64')\n    return \"Successful\"\n\nCategorical_transformation(hour)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = hour['cnt']\nx = hour.drop(['cnt'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for Spliting Data and Hyperparameter Tuning \nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n#Importing Machine Learning Model\nfrom catboost import CatBoostRegressor\nfrom sklearn import ensemble\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRFRegressor, XGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\n\n#statistical Tools\nfrom sklearn import metrics\n\n#To tranform data\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spliting data into Training and Testing\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = {}\nrmse = {}\nexplained_variance = {}\nmax_error = {}\nMAE = {}\n\ndef train_model(model, model_name):\n    print(model_name) # Printing model name\n    model.fit(x_train,y_train) # fitting the defined model\n    pred = model.predict(x_test) # predicting our data\n\n    acc = metrics.r2_score(y_test, pred)*100 #Checking R2_Score\n    accuracy[model_name] = acc # Saving R2_Score to dict.\n    print('R2_Score',acc)\n\n    met = np.sqrt(metrics.mean_squared_error(y_test, pred)) #Calculating RMSE\n    print('RMSE : ', met) \n    rmse[model_name] = met #Saving RMSE\n\n    var = (metrics.explained_variance_score(y_test, pred)) #Calculating explained_variance_score\n    print('Explained_Variance : ', var)\n    explained_variance[model_name] = var #Saving explained_variance_score\n\n    error = (metrics.max_error(y_test, pred)) #Calculating Max_Error\n    print('Max_Error : ', error)\n    max_error[model_name] = error #Saving Max_Error\n    \n    err = metrics.mean_absolute_error(y_test, pred) #Calculating mean_absolute_error\n    print(\"Mean Absolute Error\", err)\n    MAE[model_name] = err #Saving mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBRegressor(n_jobs = 4, n_estimators = x.shape[0], max_depth = 5)\n\n#Training Model\ntrain_model(xgb, \"Xtreme Gradient\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training Model\ngbr = ensemble.GradientBoostingRegressor(learning_rate=0.01, n_estimators=1000, \n                                         max_depth=5, min_samples_split=8) # Gradient Boosting Model\n\ntrain_model(gbr, \"Gradient Boost\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training Model\ncat = CatBoostRegressor(verbose=0, n_estimators = x_train.shape[0]) #Cat Booting Regression model\n\ntrain_model(cat, \"Cat Boost\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbr = LGBMRegressor(n_estimators = x_train.shape[0], learning_rate=0.01, max_depth=12, \n                     objective='tweedie', num_leaves=15, n_jobs = 4) #Light Gradient Boosting Model\n\n#Training Model\ntrain_model(lgbr, 'Light Gradient Boost')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training Model\nlr = LinearRegression(normalize = True, n_jobs=4) #Linear Regression Model\n\ntrain_model(lr, \"Linear Regression\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training Model\nrfc = ensemble.RandomForestRegressor(n_estimators=1000, bootstrap=True, min_samples_leaf=100, \n                                     n_jobs=-1, min_samples_split=8, max_depth=6) #Random Forest Bagging Model\n\ntrain_model(rfc, \"Random Forest\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training Model\nada = ensemble.AdaBoostRegressor(n_estimators=1000, learning_rate=0.01) #Adaptive Boosting Bodel\n\ntrain_model(ada, \"Ada Boost\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training Model\ndtr = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100) # Decision tree model\n\ntrain_model(dtr, \"Decision Tree\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training Model\nmlp = MLPRegressor(hidden_layer_sizes=(200,2), learning_rate='adaptive', max_iter=400) #Multi-Layer Percepton Regression model\n\ntrain_model(mlp, \"Multi-layer Perceptron\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training Model\nknn = KNeighborsRegressor(n_neighbors=10, n_jobs=4, leaf_size=50) # K Nearest Neighbors Regressor model\n\ntrain_model(knn, \"K Nearest Neighbors\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training model using Deep Learning Keras Library\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.layers import Dense\n\nmodel = keras.Sequential([\n    layers.Dense(256, activation = tf.keras.layers.ELU(), input_shape=[x_train.shape[1]]), #Input Layer\n    layers.Dense(256, activation=tf.keras.layers.ELU()), #Hidden Layer\n    layers.Dense(16, activation = 'relu'), #Hidden Layer\n    layers.Dense(4, activation = 'relu'), #Hidden Layer\n    layers.Dense(1) #Output Layer\n   ])\n\n# Compile the network :\nmodel.compile(loss = tf.keras.losses.MeanSquaredError(), \n                 optimizer = 'adam', metrics = tf.keras.metrics.RootMeanSquaredError())\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nz = scaler.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating check point to retreive best weights\nfrom keras.callbacks import ModelCheckpoint\n\ncheckpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \ncheckpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\ncallbacks_list = [checkpoint]\n\n# Fitting the model\nhistory = model.fit(x, y, epochs=200, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating Dataframe to check history\nhistory_df = pd.DataFrame(history.history)\n\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Loss\")\nhistory_df.loc[:, ['root_mean_squared_error', 'val_root_mean_squared_error']].plot(title=\"Root Mean Square Error\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load wights file of the best model :\nwights_file = './Weights-021--5126.26074.hdf5' # choose the best checkpoint \n\nmodel.load_weights(wights_file) # load it\nmodel.compile(loss = tf.keras.losses.MeanSquaredError(), \n                 optimizer = 'adam', metrics = tf.keras.metrics.RootMeanSquaredError())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(model, \"NN Model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Light Gradient Boosting Model is giving the Best Result Lowest Mean Squared Error"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}