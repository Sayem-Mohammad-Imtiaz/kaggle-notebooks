{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nprint(tf.__version__)\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Functions\n\n- Let's built some useful function to use in the next steps "},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is a simple function to plot time x axis values y axis\n\ndef plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Inputs: data series, size of the window,The size of the batches to use when training,\n# the size of the shuffle buffer, which determines how the data will be shuffled.\n#\n# Expand the dimensions of the series to work with the LSTM's\n# Create dataset ds from the series\n# Slice the data up into the appropriate windows, shifted by one time set.\n# keep them all the same size by setting drop remainder to true.\n# flatten the data into chunks in the size of our window_size + 1.\n# Shuffle it with shuffle buffer that speeds things up with large datasets\n# Return the dataset that batched into the selected batch size \n\ndef windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can use this function to make some prediction by using the trained model \n\ndef model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration and Preparation\n\n###  This data tracks sunspots on a monthly basis from 1749 until 2018. \n### Sunspots do have seasonal cycles approximately every 11 years."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_sun = pd.read_csv('/kaggle/input/sunspots/Sunspots.csv')\nprint(df_sun.shape)\ndf_sun.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's visualize the data\nseries = df_sun['Monthly Mean Total Sunspot Number']\ntime = df_sun['Unnamed: 0']\n\nplt.figure(figsize=(10, 6))\nplot_series(time, series)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split Train-Test Dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take 80% of the data as train set\nsplit_time = 2500\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's define random_seeds and the variables\n# Clear keras session \ntf.keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nshuffle_buffer_size = 1000\nwindow_size = 64\nbatch_size = 128\n\n# Use windowed_dataset function to make dataset suitable\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\nprint(train_set)\nprint(x_train.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We use \"lr_schedule\" to see which \"learning rate\" is optimum \n# Run the model with less epoch to visualize \"learning rate\" vs \"loss\"\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n                    lambda epoch: 1e-8 * 10**(epoch/20))\n# Optimizer and loos parameters\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\nloss = tf.keras.losses.Huber()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Build and Fit the model\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=64, kernel_size=5,\n                          strides=1, padding='causal',\n                          activation='relu',\n                          input_shape=[None, 1]),\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    tf.keras.layers.LSTM(128, return_sequences=True),\n    tf.keras.layers.Dense(32, activation=\"relu\"),\n    tf.keras.layers.Dense(16, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n\nmodel.compile(loss=loss, optimizer=optimizer, metrics=['mae'])\n\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's visualize \"learning rate\" vs \"loss\"\nplt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-3, 0, 80])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This suggests the best learning rate for this network will be around \"7e-5\""},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Let's rerun the model with the optimul learning rate\ntf.keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nshuffle_buffer_size = 1000\nwindow_size = 64\nbatch_size = 64\n\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n\noptimizer = tf.keras.optimizers.SGD(lr=7e-6, momentum=0.9)\nloss = tf.keras.losses.Huber()\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                          strides=1, padding='causal',\n                          activation='relu',\n                          input_shape=[None, 1]),\n    tf.keras.layers.LSTM(64, return_sequences=True),\n    tf.keras.layers.LSTM(32, return_sequences=True),\n    tf.keras.layers.Dense(32, activation=\"relu\"),\n    tf.keras.layers.Dense(16, activation=\"relu\"),\n    tf.keras.layers.Dense(1),\n    tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\n\nmodel.compile(loss=loss, optimizer=optimizer, metrics=['mae'])\n\nhistory = model.fit(train_set, epochs=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\nprint(\"First 10 Predictions :\",\"\\n\", rnn_forecast[:10])\nprint('')\nprint(\"mae : \", tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy())\n\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}