{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reading the data**","metadata":{}},{"cell_type":"code","source":"#To read the Twitter dataset:\n# df=pd.read_csv('../input/twitter-and-reddit-sentimental-analysis-dataset/Twitter_Data.csv')\n#To read the Reddit Dataset:\ndf=pd.read_csv('../input/twitter-and-reddit-sentimental-analysis-dataset/Reddit_Data.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Convert the Dataset into a TF-IDF Matrix**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words={'english'})\ncheck=[]\nfor x in df.clean_comment:\n    check.append(x)\nX = vectorizer.fit_transform(df['clean_comment'].values.astype('U'))  ## Even astype(str) would work\n# X = vectorizer.fit_transform(check)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Print the shape of the matrix","metadata":{}},{"cell_type":"code","source":"print(X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prints the values of the first row of the TF-IDF matrix","metadata":{}},{"cell_type":"code","source":"print(X[0,])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Apply K-Means algorithm to the TF-IDF matrix**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding the optimal value of k","metadata":{}},{"cell_type":"code","source":"Sum_of_squared_distances = []\nK = range(2,10)\nfor k in K:\n    km = KMeans(n_clusters=k, max_iter=200, n_init=10)\n    km = km.fit(X)\n    Sum_of_squared_distances.append(km.inertia_)\n\nplt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Forming the clusters","metadata":{}},{"cell_type":"code","source":"true_k = 8\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=200, n_init=10)\nmodel.fit(X)\nlabels=model.labels_\nwiki_cl=pd.DataFrame(list(zip(df.clean_comment,labels)),columns=['title','cluster'])\nprint(wiki_cl.sort_values(by=['cluster']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualization of clusters**","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\nresult={'cluster':labels,'wiki':df.clean_comment}\nresult=pd.DataFrame(result)\nfor k in range(0,true_k):\n    s=result[result.cluster==k]\n    text=s['wiki'].str.cat(sep=' ')\n    text=text.lower()\n    text=' '.join([word for word in text.split()])\n    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n    print('Cluster: {}'.format(k))\n    print('Titles')\n    titles=wiki_cl[wiki_cl.cluster==k]['title']\n    titles=titles[0:5]\n#     titles=df.clean_text[]\n    print(titles.to_string(index=False))\n    plt.figure()\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TESTING**","metadata":{}},{"cell_type":"code","source":"Y=vectorizer.transform([\"congress vs bjp\"])\nprediction = model.predict(Y)\nprint(prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y=vectorizer.transform([\"india is the second most populated country\"])\nprediction = model.predict(Y)\nprint(prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y=vectorizer.transform([\"what you have learnt is never a waste\"])\nprediction = model.predict(Y)\nprint(prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y=vectorizer.transform([\"Is amazon prime better than netflix\"])\nprediction = model.predict(Y)\nprint(prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}