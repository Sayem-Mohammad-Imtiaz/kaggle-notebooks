{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **SENTIMENT ANALYSIS USING LSTM**\n\nThe objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n\nFormally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.","metadata":{}},{"cell_type":"code","source":"#importing libraries\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-07-07T12:31:10.513207Z","iopub.execute_input":"2021-07-07T12:31:10.513536Z","iopub.status.idle":"2021-07-07T12:31:16.881295Z","shell.execute_reply.started":"2021-07-07T12:31:10.513492Z","shell.execute_reply":"2021-07-07T12:31:16.88047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing the dataset\nds = pd.read_csv(\"../input/twitter-sentiment-analysis-hatred-speech/train.csv\")\nds.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T12:31:16.882709Z","iopub.execute_input":"2021-07-07T12:31:16.883035Z","iopub.status.idle":"2021-07-07T12:31:17.00662Z","shell.execute_reply.started":"2021-07-07T12:31:16.883Z","shell.execute_reply":"2021-07-07T12:31:17.005879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking for null values\nds.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T12:31:17.008361Z","iopub.execute_input":"2021-07-07T12:31:17.008701Z","iopub.status.idle":"2021-07-07T12:31:17.019266Z","shell.execute_reply.started":"2021-07-07T12:31:17.008665Z","shell.execute_reply":"2021-07-07T12:31:17.018318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**NO NULL VALUES FOUND**","metadata":{}},{"cell_type":"code","source":"#defining dependent and independent vectors\n#taking only title for prediction\nx = ds.iloc[:,2:3]\ny = ds['label']","metadata":{"execution":{"iopub.status.busy":"2021-07-07T12:31:17.021233Z","iopub.execute_input":"2021-07-07T12:31:17.021594Z","iopub.status.idle":"2021-07-07T12:31:17.028036Z","shell.execute_reply.started":"2021-07-07T12:31:17.021558Z","shell.execute_reply":"2021-07-07T12:31:17.027085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-07T13:26:02.758447Z","iopub.execute_input":"2021-07-07T13:26:02.758805Z","iopub.status.idle":"2021-07-07T13:26:02.768219Z","shell.execute_reply.started":"2021-07-07T13:26:02.758773Z","shell.execute_reply":"2021-07-07T13:26:02.767141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#checking number of real and fake news\nsns.countplot(x = 'label',data = ds)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T12:31:17.02939Z","iopub.execute_input":"2021-07-07T12:31:17.029761Z","iopub.status.idle":"2021-07-07T12:31:17.148408Z","shell.execute_reply.started":"2021-07-07T12:31:17.029726Z","shell.execute_reply":"2021-07-07T12:31:17.147498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**AS YOU CAN SEE O HAVE (~ 30000) VALUES AND 1 HAVE (~ 2500) VALUES**","metadata":{}},{"cell_type":"code","source":"#Text Cleaning and preprocessing\n\ncleaned = []\nfor i in range(0,len(ds)):\n    \n    #removing words any other than (a-z) and (A-Z)\n    text = re.sub('[^a-zA-Z]',' ', x['tweet'][i])\n    \n    #converting all words into lower case\n    text = text.lower()\n    \n    #tokenizing \n    text = text.split()\n    \n    #stemming and removing stopwords\n    ps = PorterStemmer()\n    text = [ps.stem(words) for words in text if words not in stopwords.words('english')]\n    text = ' '.join(text)\n    cleaned.append(text)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T12:31:17.149775Z","iopub.execute_input":"2021-07-07T12:31:17.150112Z","iopub.status.idle":"2021-07-07T12:32:13.648935Z","shell.execute_reply.started":"2021-07-07T12:31:17.150079Z","shell.execute_reply":"2021-07-07T12:32:13.648073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#cleaned text\ncleaned[:5]","metadata":{"execution":{"iopub.status.busy":"2021-07-07T12:32:13.650207Z","iopub.execute_input":"2021-07-07T12:32:13.650538Z","iopub.status.idle":"2021-07-07T12:32:13.657182Z","shell.execute_reply.started":"2021-07-07T12:32:13.650504Z","shell.execute_reply":"2021-07-07T12:32:13.656296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DATA IS NOW READY FOR ONE HOT ENCODING**\n\nOur motive here is to create an embedding layer of texts for the LSTM, OneHot encoding prepares our text array into a format required by embedding layer.","metadata":{}},{"cell_type":"code","source":"#taking dictionary size 5000\nvocab_size = 5000\n\n#one hot encoding\none_hot_dir = [one_hot(words,vocab_size) for words in cleaned]\n\n#length of all rows should be equal therefore applying padding\n#this will adjust size by adding 0 at staring of the shorter rows\nembedded_layer = pad_sequences(one_hot_dir,padding = 'pre')\nembedded_layer","metadata":{"execution":{"iopub.status.busy":"2021-07-07T12:32:13.66002Z","iopub.execute_input":"2021-07-07T12:32:13.660364Z","iopub.status.idle":"2021-07-07T12:32:14.243487Z","shell.execute_reply.started":"2021-07-07T12:32:13.66033Z","shell.execute_reply":"2021-07-07T12:32:14.242515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**OUR MATRIX IS NOW READY FOR THE LSTM**","metadata":{}},{"cell_type":"code","source":"#converting into numpy arrays.\nx = np.array(embedded_layer)\ny = np.array(y)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T12:32:14.245046Z","iopub.execute_input":"2021-07-07T12:32:14.245388Z","iopub.status.idle":"2021-07-07T12:32:14.25248Z","shell.execute_reply.started":"2021-07-07T12:32:14.245352Z","shell.execute_reply":"2021-07-07T12:32:14.251642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitting the Dataset into Train and Test set\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T12:32:14.253808Z","iopub.execute_input":"2021-07-07T12:32:14.25419Z","iopub.status.idle":"2021-07-07T12:32:14.266361Z","shell.execute_reply.started":"2021-07-07T12:32:14.254152Z","shell.execute_reply":"2021-07-07T12:32:14.265672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import regularizers\n#creating model using LSTM\nmodel = Sequential()\n\n#taking number features as 64\nmodel.add(Embedding(vocab_size,64,input_length = len(embedded_layer[0])))\n#model.add(Dropout(0.4))\n\n#adding LSTM layers with 128 neurons\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.4))\n\n#adding output layer \nmodel.add(Dense(1,activation=\"sigmoid\"))\n\n#compiling the model\nmodel.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n\n#summary of model\nmodel.summary()\n\n#training the model\nmodel.fit(x_train, y_train, validation_data = (x_test,y_test), epochs = 5, batch_size = 32)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T13:23:33.036842Z","iopub.execute_input":"2021-07-07T13:23:33.037166Z","iopub.status.idle":"2021-07-07T13:24:08.958043Z","shell.execute_reply.started":"2021-07-07T13:23:33.037136Z","shell.execute_reply":"2021-07-07T13:24:08.957245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicting and getting accuracy\ny_pred = model.predict(x_test)\ny_pred = (y_pred > 0.5)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T13:24:42.683674Z","iopub.execute_input":"2021-07-07T13:24:42.684017Z","iopub.status.idle":"2021-07-07T13:24:43.3234Z","shell.execute_reply.started":"2021-07-07T13:24:42.683985Z","shell.execute_reply":"2021-07-07T13:24:43.322436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-07T13:24:46.32892Z","iopub.execute_input":"2021-07-07T13:24:46.329264Z","iopub.status.idle":"2021-07-07T13:24:46.360969Z","shell.execute_reply.started":"2021-07-07T13:24:46.329234Z","shell.execute_reply":"2021-07-07T13:24:46.359947Z"},"trusted":true},"execution_count":null,"outputs":[]}]}