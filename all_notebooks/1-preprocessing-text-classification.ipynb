{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport urllib.request\nimport keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input,Dense,Flatten,GlobalAveragePooling1D,Embedding,SimpleRNN,LSTM\nfrom sklearn.model_selection import train_test_split\n\nraw_text = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\",encoding=\"latin1\")\nprint(raw_text.shape)\n\nplt.figure(figsize=(8,8))\nsns.countplot(data=raw_text,x=\"v1\")\nplt.show()\n\nraw_text.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_text.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\nraw_text['v1'] = raw_text['v1'].replace(['ham','spam'],[0,1])\nraw_text.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(raw_text.info())\nprint(f\"\\nAny NA values? >> {raw_text.isnull().any()}\")\nprint(\"\\n\",raw_text.v2.nunique())\nraw_text.drop_duplicates(subset=['v2'],inplace=True)\nprint(f\"Total rows >> {raw_text.shape[0]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = raw_text.v2\nlabel = raw_text.v1\n\nWORD_SIZE = 5000\n\ntokenizer = Tokenizer(num_words=WORD_SIZE)\ntokenizer.fit_on_texts(data)\nsequences = tokenizer.texts_to_sequences(data)\nsequences[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_to_index = tokenizer.word_index\nindex_to_word = tokenizer.index_word","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(word_to_index))\n\nvocab_size= WORD_SIZE+1\nprint(f\"단어 집합의 크기:{vocab_size}\")\n\nmax_len = max(len(l)for l in sequences)\nprint(f\"최대 문장 길이(단어수):{max_len}\")\n\nsequence_size = 180\ndata =  pad_sequences(sequences,maxlen=sequence_size,padding='post',truncating='post')\nprint(data.shape)\ndata[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train Test Split\n\ntrain_data,test_data,train_label,test_label = train_test_split(data,label,stratify=label)\nprint(f\"shape of train data >> {train_data.shape}\")\nprint(f\"shape of test data >> {test_data.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Without RNN\nword_vec_size=64\n\ndef create_simple_model():\n    X = Input(shape=[sequence_size])\n    H = Embedding(input_dim=vocab_size,output_dim=word_vec_size,input_length=sequence_size)(X)\n    H = GlobalAveragePooling1D()(H)\n    H = Dense(word_vec_size)(H)\n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\nsimple_model = create_simple_model()\nhist = simple_model.fit(train_data,train_label,batch_size=32,validation_split=0.2,epochs=10)\nevaluation = simple_model.evaluate(test_data,test_label)\nsimple_model.summary()\n\n\ndef plot_graph(hist):\n    fig = plt.figure(figsize=(12,8))\n    ax1 = fig.add_subplot(1,2,1)\n    ax1.plot(range(len(hist.history['loss'])),hist.history['loss'],'bo--',label='train_loss')\n    ax1.plot(range(len(hist.history['loss'])),hist.history['val_loss'],'ro--',label='val_loss')\n    plt.legend()\n\n    ax2 = fig.add_subplot(1,2,2)\n    ax2.plot(range(len(hist.history['accuracy'])),hist.history['accuracy'],'bo--',label='train_acc')\n    ax2.plot(range(len(hist.history['accuracy'])),hist.history['val_accuracy'],'ro--',label='val_acc')\n    plt.legend()\n    plt.show()\n\nplot_graph(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# With RNN (SimpleRNN)\nhidden_size=64\n\ndef create_RNN_model():\n    X = Input(shape=[sequence_size])\n    H = Embedding(input_dim=vocab_size,output_dim=word_vec_size,input_length=sequence_size)(X)\n    H = SimpleRNN(cell_size)(H)\n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\nrnn_model = create_RNN_model()\nhist = rnn_model.fit(train_data,train_label,batch_size=32,validation_split=0.2,epochs=10)\nevaluation = rnn_model.evaluate(test_data,test_label)\n\nplot_graph(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# With LSTM\n\ndef create_lstm_model():\n    X = Input(shape=[sequence_size])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size)(X)\n    H = LSTM(hidden_size)(H)\n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\nlstm = create_lstm_model()\nhist = lstm.fit(train_data,train_label,epochs=5,batch_size=16,validation_split=0.2)\nevaluation = lstm.evaluate(test_data,test_label)\n\nplot_graph(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# many-to-many LSTM\n\ndef create_many2many_lstm_model():\n    X = Input(shape=[sequence_size])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size)(X)\n    H = LSTM(hidden_size,return_sequences=True)(H)\n    Y = keras.layers.TimeDistributed(Dense(1,activation='sigmoid'))(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\nlstm = create_many2many_lstm_model()\nhist = lstm.fit(train_data,train_label,epochs=8,batch_size=16,validation_split=0.2)\nevaluation = lstm.evaluate(test_data,test_label)\n\nplot_graph(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# many-to-many RNN\n\ndef create_many2many_rnn_model():\n    X = Input(shape=[sequence_size])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size)(X)\n    H = SimpleRNN(hidden_size,return_sequences=True)(H)\n    Y = keras.layers.TimeDistributed(Dense(1,activation='sigmoid'))(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\nrnn = create_many2many_rnn_model()\nhist = rnn.fit(train_data,train_label,epochs=8,batch_size=16,validation_split=0.2)\nevaluation = rnn.evaluate(test_data,test_label)\n\nplot_graph(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacked many-to-one LSTM\n\ndef create_stacked_simple_LSTM():\n    X = Input(shape=[sequence_size])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size)(X)\n    H = LSTM(hidden_size,return_sequences=True)(H)\n    H = LSTM(hidden_size,return_sequences=False)(H)\n    Y = Dense(1,activation='sigmoid')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n    \n    return model\n\nlstm = create_stacked_simple_LSTM()\nhist = lstm.fit(train_data,train_label,epochs=10,validation_split=0.2,batch_size=16)\nevaluation = lstm.evaluate(test_data,test_label)\n\nplot_graph(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stacked many-to-many LSTM\n\ndef create_stacked_many_to_many_LSTM():\n    X = Input(shape=[sequence_size])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size)(X)\n    H = LSTM(hidden_size,return_sequences=True)(H)\n    H = LSTM(hidden_size,return_sequences=True)(H)\n    Y = keras.layers.TimeDistributed(Dense(1,activation='sigmoid'))(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\nlstm = create_stacked_many_to_many_LSTM()\nhist = lstm.fit(train_data,train_label,epochs=10,validation_split=0.2,batch_size=16)\nevaluation = lstm.evaluate(test_data,test_label)\n\nplot_graph(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bidirectional + stacked + many-to-many LSTM\n\nfrom keras.layers import Bidirectional\n\ndef create_bi_stacked_model():\n    X = Input(shape=[sequence_size])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size)(X)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    Y = keras.layers.TimeDistributed(Dense(1,activation='sigmoid'))(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\nlstm = create_bi_stacked_model()\nhist = lstm.fit(train_data,train_label,epochs=6,validation_split=0.2,batch_size=16)\nevaluation = lstm.evaluate(test_data,test_label)\n\nplot_graph(hist)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}