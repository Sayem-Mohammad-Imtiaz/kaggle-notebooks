{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport time\n\nimport torch\nfrom torchtext import data\nimport torch.nn as nn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training\n    - https://www.kaggle.com/swarnabha/pytorch-text-classification-torchtext-lstm"},{"metadata":{"trusted":true},"cell_type":"code","source":"sent = pd.read_csv(\"../input/sentiment140/training.1600000.processed.noemoticon.csv\", encoding =\"ISO-8859-1\" , names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])#.head(10000)\nsent.drop(columns=['ids','date','flag','user'], inplace=True)\nprint(\"There are {} rows and {} columns in train file\".format(sent.shape[0],sent.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# light cleaning\ndef normalise_text (text):\n    text = text.str.lower() # lowercase\n    text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n    text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n    text = text.str.replace(r\"@\",\"\")\n    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n    text = text.str.replace(\"\\s{2,}\", \" \")\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent[\"text\"]=normalise_text(sent[\"text\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 75% training , 25% validation\ntrain_df, valid_df = train_test_split(sent, train_size = .75, random_state = 1 )\nprint(\"Train/valid shapes :\", train_df.shape, valid_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 42\n\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\nLABEL = data.LabelField(dtype = torch.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataFrameDataset(data.Dataset):\n\n    def __init__(self, df, fields, is_test=False, **kwargs):\n        examples = []\n        for i, row in df.iterrows():\n            label = row.target if not is_test else None\n            text = row.text\n            examples.append(data.Example.fromlist([text, label], fields))\n\n        super().__init__(examples, fields, **kwargs)\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    @classmethod\n    def splits(cls, fields, train_df, val_df=None, test_df=None, **kwargs):\n        train_data, val_data, test_data = (None, None, None)\n        data_field = fields\n\n        if train_df is not None:\n            train_data = cls(train_df.copy(), data_field, **kwargs)\n        if val_df is not None:\n            val_data = cls(val_df.copy(), data_field, **kwargs)\n        if test_df is not None:\n            test_data = cls(test_df.copy(), data_field, True, **kwargs)\n\n        return tuple(d for d in (train_data, val_data, test_data) if d is not None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fields = [('text',TEXT), ('label',LABEL)]\n\ntrain_ds, val_ds = DataFrameDataset.splits(fields, train_df=train_df, val_df=valid_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets look at a random example\nprint(vars(train_ds[15]))\n\n# Check the type \nprint(type(train_ds[15]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"137eaa9eadbf47da7362794a2d0152a4855db04b"},"cell_type":"code","source":"MAX_VOCAB_SIZE = 25000\n\nTEXT.build_vocab(train_ds, \n                 max_size = MAX_VOCAB_SIZE, \n                 vectors = 'glove.6B.200d',\n                 unk_init = torch.Tensor.zero_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LABEL.build_vocab(train_ds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 512\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator = data.BucketIterator.splits(\n    (train_ds, val_ds), \n    batch_size = BATCH_SIZE,\n    sort_within_batch = True,\n    device = device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0057c657aa1fe65fca683ac97df2cede9a4f7325"},"cell_type":"code","source":"# Hyperparameters\nnum_epochs = 20\nlearning_rate = 0.001\n\nINPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 200\nHIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.2\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] # padding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LSTM architecture\nclass LSTM_net(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 bidirectional, dropout, pad_idx):\n        \n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n        \n        self.rnn = nn.LSTM(embedding_dim, \n                           hidden_dim, \n                           num_layers=n_layers, \n                           bidirectional=bidirectional, \n                           dropout=dropout)\n        \n        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n        \n        self.fc2 = nn.Linear(hidden_dim, 1)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text, text_lengths):\n        \n        embedded = self.embedding(text)\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n        \n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        \n        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n        output = self.fc1(hidden)\n        output = self.dropout(self.fc2(output))\n                \n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LSTM_net(INPUT_DIM, \n            EMBEDDING_DIM, \n            HIDDEN_DIM, \n            OUTPUT_DIM, \n            N_LAYERS, \n            BIDIRECTIONAL, \n            DROPOUT, \n            PAD_IDX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_embeddings = TEXT.vocab.vectors\nmodel.embedding.weight.data.copy_(pretrained_embeddings)\n\n#  to initiaise padded to zeros\nmodel.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\nmodel.embedding.weight.data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03e87004b8803019eb1ab08ac8435b01162e69c4"},"cell_type":"code","source":"model.to(device) # Neural Network to GPU\n\n\n# Loss and optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def binary_accuracy(preds, y):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    \"\"\"\n\n    #round predictions to the closest integer\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float() #convert into float for division \n    acc = correct.sum() / len(correct)\n    return acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training function \ndef train(model, iterator):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        text, text_lengths = batch.text\n        \n        optimizer.zero_grad()\n        predictions = model(text, text_lengths).squeeze(1)\n        loss = criterion(predictions, batch.label)\n        acc = binary_accuracy(predictions, batch.label)\n\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, iterator):\n    \n    epoch_acc = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for batch in iterator:\n            text, text_lengths = batch.text\n            predictions = model(text, text_lengths).squeeze(1)\n            acc = binary_accuracy(predictions, batch.label)\n            \n            epoch_acc += acc.item()\n        \n    return epoch_acc / len(iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t = time.time()\nloss=[]\nacc=[]\nval_acc=[]\n\nfor epoch in range(num_epochs):\n    \n    #training\n    train_loss, train_acc = train(model, train_iterator)\n    \n    #validation\n    valid_acc = evaluate(model, valid_iterator)\n    \n    # to print results\n    #print(f'Epoch {epoch}')\n    #print('--------')\n    #print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    #print(f'\\t Val. Acc: {valid_acc*100:.2f}% \\n')\n    \n    loss.append(train_loss)\n    acc.append(train_acc)\n    val_acc.append(valid_acc)\n    \nprint(f'time:{time.time()-t:.3f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3299a7f04480735cda7d18a3b5875d2ca3d62d0","_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"# save model\ntorch.save(model.state_dict(), \"model.pth\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"#save performance per epoch\nperformance = pd.DataFrame({\"Train Loss\":loss, \"Train acc\":acc, \"Val acc\":valid_acc})\nperformance.to_csv(\"performance.csv\", index=False)\nperformance.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotly libraries\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.subplots import make_subplots\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tain and Val accuracies\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=performance.index.values, y=performance['Train acc'].values,\n                    mode='lines+markers',\n                    name='train accuracy'))\nfig.add_trace(go.Scatter(x=performance.index.values, y=performance['Val acc'].values,\n                    mode='lines+markers',\n                    name='valid accuracy'))\nfig.add_trace(go.Scatter(x=performance.index.values, y=performance['Train Loss'].values,\n                    mode='lines+markers',\n                    name='train loss'))\n\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}