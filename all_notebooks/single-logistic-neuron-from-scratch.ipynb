{"cells":[{"outputs":[],"metadata":{"_cell_guid":"4b4bb32b-1696-455c-b193-5db0362f5fb8","_uuid":"94a50cab20408878bf03e1df9cb0418d8827dd82","collapsed":true},"execution_count":null,"source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n%matplotlib inline","cell_type":"code"},{"metadata":{"_cell_guid":"7705ab6e-69a2-4463-ae6c-ae0a846ea7d7","_uuid":"36b0949e3663c3e55cabd48d7c002b2993b78252"},"source":"We'll implement logistic regression to solve the gender voice classification problem.  \nIn this, the logistic unit can be considered as the most simplistic neural net with just one neuron.  \nSo we'll implement logistic regession using the forward + backprop philosophy of neural networks.   \n\nLets import the data into a dataframe first and have a look.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"75819a89-2c36-485f-a2f3-fc847c639727","_uuid":"c3e39b16ad60ad071536f0f6fd0caaa16497b29a"},"execution_count":null,"source":"df = pd.read_csv('../input/voice.csv')\ndf.head()","cell_type":"code"},{"metadata":{"_cell_guid":"6c1130e7-0ea0-42ca-8f09-651041f12042","_uuid":"ea657b0d73bb868fee3ae068ef79b6b1b8d6b221"},"source":"Next, lets check for any null records.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"10ca24ee-7188-4980-a366-914655141973","_uuid":"43dbf3b17d5cf0ce544c51a05866fa291d72aec2"},"execution_count":null,"source":"np.where(pd.isnull(df))","cell_type":"code"},{"metadata":{"_cell_guid":"2e5a167f-18ab-4adc-b1f9-1653c34596a4","_uuid":"dfd26c58b793a552bddadc0c199de4e146f08cbb"},"source":"Great!, there are no null records.  \nNow for a bit of preprocessing.  \nLets first separate the features and labels.  ","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"4703c850-edd6-46ed-b43a-debf9d709ef5","_uuid":"f509e6146c05dc6015334975345d4e16725df7a4","collapsed":true},"execution_count":null,"source":"X = df.drop(\"label\", axis=1)\nY = df[\"label\"]","cell_type":"code"},{"metadata":{"_cell_guid":"3968c7d3-1147-426a-8bfb-3830523683bf","_uuid":"bfbd0a8e9bae51dca6d5af44b7141ab6246156ca"},"source":"Normalize the features uing scikit-learn's standardscaler","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"beebacf0-b616-4950-8dec-cec0e630a0f1","_uuid":"cb20a8a2a1d7cee823d607a2eac3780e63210d6d","collapsed":true},"execution_count":null,"source":"scaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","cell_type":"code"},{"metadata":{"_cell_guid":"cb102094-47de-408c-b3af-9d2ef46e4474","_uuid":"7372952a2fc96a4ba44a4f8e5fc1dc262f4c66e9"},"source":"Convert the labels into ones and zeroes as they are strings","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"712c7d4d-df93-46b8-bb5e-5574700a5bc9","_uuid":"c45318b61578705b4fad3784c82bccb49ef3828f","collapsed":true},"execution_count":null,"source":"gender_encoder = LabelEncoder()\nY = gender_encoder.fit_transform(Y)","cell_type":"code"},{"metadata":{"_cell_guid":"9118fada-0523-435f-95a6-b85c78d3a5ab","_uuid":"9571e5ef9b4432b2fe1d7840b76bd95db74bd58d"},"source":"All the steps above can be encapsulated in methods","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"10067584-4e02-414c-835c-7fec84d882ad","_uuid":"1728925ff7ea459764c620c4562f8d882017beb2","collapsed":true},"execution_count":null,"source":"def normalize_features(features):\n    scaler = StandardScaler()\n    scaler.fit(features)\n    return scaler.transform(features)","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"15b652e1-c34d-4a07-a403-6c94d06d8a37","_uuid":"41c3b34100aa3aa84ae9a21ea23d7098a319be43","collapsed":true},"execution_count":null,"source":"def encode_gender(Y):\n    gender_encoder = LabelEncoder()\n    return gender_encoder.fit_transform(Y)","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"2d50b208-e3cd-4a71-a3e4-5ebd45cc889f","_uuid":"e224a1dd332200ac0dbb2d3bcd7f53466c7f9a6e","collapsed":true},"execution_count":null,"source":"def read_data(path):\n    df = pd.read_csv(path)\n    X = df.drop(\"label\", axis=1)\n    Y = df[\"label\"]\n    X = normalize_features(X)\n    Y = encode_gender(Y)\n    Y = Y.reshape(Y.shape[0], 1)\n    return X,Y","cell_type":"code"},{"metadata":{"_cell_guid":"be82e2bf-e5cf-4134-ae8f-07a4dece42fb","_uuid":"dfda87adac506548ed88a9f6906d475e5dea518d"},"source":"Lets test this all together now","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"399da132-436d-490d-8268-536c9df77867","_uuid":"4b333ee9f7d2b84b5ba079f22be4034e4616186c"},"execution_count":null,"source":"X,Y = read_data('../input/voice.csv')\nassert(X.shape == (3168, 20))\nassert(Y.shape == (3168, 1))\nassert(Y.dtype == np.dtype('int64'))","cell_type":"code"},{"metadata":{"_cell_guid":"d6380e8d-2603-45c1-8007-2919634ba635","_uuid":"0298c6ee1b469c0b56fd80fb77364706d025183c"},"source":"We now need to split the data into training and test sets.  \nWe'll use the excellent train_test_split method provided by scikit-learn.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"96b5c1b5-4f11-4e2b-a3a0-de648fdcf81b","_uuid":"e816d968b8c86ccd7d078ee4f197d9e4adf2dbe9"},"execution_count":null,"source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)","cell_type":"code"},{"metadata":{"_cell_guid":"a0dff317-51e8-4e2b-98fa-588a0ee6c3e5","_uuid":"7c288589402d96a38b2de2b43102e133d7bad36e"},"source":"Since we need to implement logistic regression from scratch, lets first define the sigmoid function.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"7d43bd11-e919-4cc1-888b-088841423bf0","_uuid":"fc94c645b7262ec01760ebbbf4cce6179ff52c5c","collapsed":true},"execution_count":null,"source":"def sigmoid(z):\n    s = 1 / (1 + np.exp(-z))\n    return s","cell_type":"code"},{"metadata":{"_cell_guid":"3c06540f-7d48-4ddd-81ec-9c7d811fd158","_uuid":"795b5daed9cb7873397e95e66f2511d1337ec63a"},"source":"We need to initialize weights and the bias values to zeros.\nSo lets have a method for that.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"a57b1b70-9cbd-483c-927a-81e4ef17ec77","_uuid":"e15b436ab0f8cd1ed5491dd8cbf584e8bf72dc97","collapsed":true},"execution_count":null,"source":"def initialize_weight_and_bias(dimension):\n    w = np.zeros((dimension,1))\n    b = 0\n    return w, b","cell_type":"code"},{"metadata":{"_cell_guid":"87890f41-b74f-41f3-8169-c96a0b0809c0","_uuid":"9f0b526166b4ae96884be6f22d635e38a2be71a2"},"source":"Lets now implement forward prop which is essentially defined as:  \n$A = \\sigma(w^T X + b)$","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"b88f5c1f-a769-40e5-858e-eddb45eb07fb","_uuid":"0f088a3877a2f56bc78895a551dc58badbe6d034","collapsed":true},"execution_count":null,"source":"def forward_prop(X, Y, w, b):\n    return sigmoid(np.dot(w.T, X) + b)","cell_type":"code"},{"metadata":{"_cell_guid":"02754667-f08a-47aa-93b3-73f4ea4cbdef","_uuid":"f588a0a20ef80f434cec8a4e8f54056366028c7e"},"source":"The cost function is defined as:  \n\n$J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$   \n\nwhere m is the total number of samples","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"27fc6e02-9cef-4718-8ebf-04f8782daaa5","_uuid":"8232d0087cedf1527ac0361f68eb5096972fe326","collapsed":true},"execution_count":null,"source":"def compute_cost(A, Y, m):\n    cost = (-1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n    return cost","cell_type":"code"},{"metadata":{"_cell_guid":"d134dca3-48ce-4101-92f4-67ab1d5312a3","_uuid":"fb7914e557e252e6246f36f69d3991a05533c58d"},"source":"In the backpropogation step, we'll calculate the gradients defines as:   \n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"b434bdb0-0326-4d8d-b645-e0e3ffd0043b","_uuid":"c1ccaeb804f4f934a3a00d59a925159045fee71e","collapsed":true},"execution_count":null,"source":"def back_prop(X, A, Y, m):\n    dw = (1 / m) * np.dot(X, (A - Y).T)\n    db = (1 / m) * np.sum(A - Y)\n    return {\"dw\": dw, \"db\": db}","cell_type":"code"},{"metadata":{"_cell_guid":"e98dfb71-04fb-489b-a9dd-d58de6fb747a","_uuid":"fbaf1dfed355059601e90a1128d08be229db3f47"},"source":"Now lets integrate all these steps together and implement gradient descent.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"ac6872c6-83aa-4872-af71-19295d2da379","_uuid":"533e8bccfc5a177b61fafa395859a1c68a513f04","collapsed":true},"execution_count":null,"source":"def optimize(w, b, X, Y, num_iterations, learning_rate):\n    costs = []\n    m = X.shape[1]\n\n    for i in range(num_iterations):\n\n        A = forward_prop(X, Y, w, b)\n        cost = compute_cost(A, Y, m)\n        grads = back_prop(X, A, Y, m)\n\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n\n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n\n        if i % 100 == 0:\n            costs.append(cost)\n\n\n    params = {\"w\": w,\n              \"b\": b}\n\n    grads = {\"dw\": dw,\n             \"db\": db}\n\n    return params, grads, costs","cell_type":"code"},{"metadata":{"_cell_guid":"041dd721-45b9-4e5a-abf1-382f0212a615","_uuid":"7c8e4559a3401f2660fd8a57d4f9b580d018f2e5"},"source":"We can now use the weights and bias learned to make predictions in our training and test sets.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"778b9730-f46d-4a8e-9f36-f229cbaaac96","_uuid":"fec481a60b14c2e488eb9ae6fed2564d8287ca5e","collapsed":true},"execution_count":null,"source":"def predict(w, b, X):\n    m = X.shape[1]\n    Y_prediction = np.zeros((1, m))\n    w = w.reshape(X.shape[0], 1)\n\n    A = sigmoid(np.dot(w.T, X) + b)\n    for i in range(A.shape[1]):\n\n        if A[0, i] <= 0.5:\n            Y_prediction[0, i] = 0\n        else:\n            Y_prediction[0, i] = 1\n\n    return Y_prediction","cell_type":"code"},{"metadata":{"_cell_guid":"d58b323d-8ea0-45ff-a8be-95f3a619554e","_uuid":"42d21252aa4c6be76f65543e255e16520e981fd5"},"source":"Finally, we can hook this whole implementation as a model and run it for a specified number of iterations.","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"9ffa5f44-0fe0-40b8-85a2-10a3ae4cb13e","_uuid":"cd47fef1b3cfeb8640bf715708cdb23eab9f5fd3","collapsed":true},"execution_count":null,"source":"def model(X_train, Y_train, X_test, Y_test, num_iterations=100, learning_rate=0.005):\n\n    w, b = initialize_weight_and_bias(X_train.shape[0])\n\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate)\n\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n\n    Y_prediction_test = predict(w, b, X_test)\n    Y_prediction_train = predict(w, b, X_train)\n\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n\n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test,\n         \"Y_prediction_train\": Y_prediction_train,\n         \"w\": w,\n         \"b\": b,\n         \"learning_rate\": learning_rate,\n         \"num_iterations\": num_iterations}\n    return d","cell_type":"code"},{"metadata":{"_cell_guid":"11ace259-3807-476a-b48b-8001a8c5b5e9","_uuid":"da33439d3ef35819e1f4532e14a658acfece8649"},"source":"Lets run this model for 2000 iterations with a learning rate of 0.01","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"17da26f1-7c61-4902-aa62-310ac6852bb7","_uuid":"af030ce2cb377e948729abae104c4d8091e94e02"},"execution_count":null,"source":"result = model(X_train.T, Y_train.T, X_test.T, Y_test.T, num_iterations = 2000, learning_rate = 0.01)","cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"e1f70535-8e89-4410-b42d-630dabff6f98","_uuid":"45cf7b21b8643a2d81b004063956329aff426e8b","collapsed":true},"execution_count":null,"source":"","cell_type":"code"}],"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","version":"3.6.1","name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","file_extension":".py","nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":1}