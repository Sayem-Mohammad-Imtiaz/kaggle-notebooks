{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/200000-jeopardy-questions/JEOPARDY_CSV.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"checking unique values in the Round column, there are four unique values Jeopardy, Final Jeopardy, Double Jeopardy, Tiebreaker."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[' Round'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see the countplot for each of these Round Type, we can clearly see that majority of the samples are from Jeopardy or Double Jeopardy, and very few from Final Jeopardy and almost negligible from Tiebreaker"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=' Round',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The samples from category Jeopardy are 107384"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[' Round'] == 'Jeopardy!'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The samples from category Double Jeopardy are 105912"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[' Round'] == 'Double Jeopardy!'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The samples from category Final Jeopardy are 3631"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[' Round'] == 'Final Jeopardy!'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The samples from category Tiebreaker are only 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[' Round'] == 'Tiebreaker'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 27995 different unique categories in the 'Category' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[' Category'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"checking if there are any null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values but some values in the 'Value' column are filled with the string 'None'."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df[' Value'] == 'None']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping rows containing 'None' Values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[df[' Value'] == 'None'].index,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 'Value' column has the string of the value which also contains $ sign and columns, so removing the signs and converting the string to interger value in a new column 'ValueNum'"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ValueNum'] = df[' Value'].apply(\n    lambda value: int(value.replace('$', '').replace(',','')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ValueNum'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 145 unique values in the ValueNum column so it makes a lot of different categories to classify"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ValueNum'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Binning the values if the value is smaller than 1000, then we round to the nearest hundred. Otherwise, if it's between 1000 and 10k, we round it to nearest thousand. If it's greater than 10k, then we round it to the nearest 10-thousand."},{"metadata":{"trusted":true},"cell_type":"code","source":"def binning(value):\n    if value < 1000:\n        return np.round(value, -2)\n    elif value < 10000:\n        return np.round(value, -3)\n    else:\n        return np.round(value, -4)\n\ndf['ValueBins'] = df['ValueNum'].apply(binning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, Now we have 21 different values to classify instead of 145"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ValueBins'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets just take a look at the 'Question' Column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[' Question']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comment_words = '' \nstopwords = set(STOPWORDS) \n\nfor val in df[' Question']: \n      \n    val = str(val) \n    tokens = val.split() \n\n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n                     \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Building a Random Forest Model\nSince the data is huge, but for our convenience lets take 10,000 random samples from the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample = df.sample(n=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_sample[' Question']\ny = df_sample['ValueBins']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use a Random Forest Classifier model with Grid Searching for finding the best hyperparameters from our dictionary of parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\n\nRFC=RandomForestClassifier(max_features=\"sqrt\")\nparameters={ \"max_depth\":[5,8,25], \n             \"min_samples_split\":[1,2,5], \"n_estimators\":[800,1200]}\nfrom sklearn.model_selection import GridSearchCV\nclf = GridSearchCV(RFC, parameters)\nfrom sklearn.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(stop_words='english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = tfidf.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clf.cv_results_['params'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clf.cv_results_['rank_test_score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that these are the best parameters from the parameter dict"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(clf.cv_results_['params'][-2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So this is how we get the best parameters and we can use these parameters to train over the complete data to get the best results!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}