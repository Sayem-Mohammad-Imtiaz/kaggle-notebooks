{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"../input/Pokemon.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83bf8ab778f9be169bc0f69d4be58ff0b1b9ac79"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"411334b089480d9e74652de2b8dc5086bdb113d3"},"cell_type":"markdown","source":"I will use numeric featues therefore drop them."},{"metadata":{"trusted":true,"_uuid":"8a5dc906eaf9bf2984d31ce2379df02198d37c3e"},"cell_type":"code","source":"data.drop([\"#\",\"Name\",\"Type 1\",\"Type 2\"],axis=1,inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45f8c88829053f22137a9f3ee731613195c632ab"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6af092f632d30d870fb9031ee06124ad5db50689"},"cell_type":"markdown","source":"   True =1 , False=0. This is how i want to use my class."},{"metadata":{"trusted":true,"_uuid":"4ca1c7069885fa808fddbd3cae4be1282fc69637"},"cell_type":"code","source":"data.Legendary=[1 if each==True else 0 for each in data.Legendary]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edb90f0d63c77de27fd96e346967a9388fcd1a46"},"cell_type":"code","source":"y=data.Legendary.values #class\nx_data=data.drop([\"Legendary\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c7a0a9be592cf28cbfba3918e56cc23cdf5fadf"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39aad6b766734bfff8dfc05369df7ed22b8773b4"},"cell_type":"markdown","source":"Because of the numeric differences i will normalize data."},{"metadata":{"trusted":true,"_uuid":"9c154561351feac5e307cdcf69422023dd187c4c"},"cell_type":"code","source":"x=(x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f1e688a389fe0b34c3a5aefe3937426a27f54d2"},"cell_type":"markdown","source":"I will split my train and test data."},{"metadata":{"trusted":true,"_uuid":"563bae59648f6ab3810cacdd9fed3034f4358f53"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28aed71a79ed3c3db609157396805abfc5eb8200"},"cell_type":"markdown","source":"We will use some algorithm to create model and evaluate accuracy of them."},{"metadata":{"trusted":true,"_uuid":"c2a819e3f5ee997b522b78ae3b58e4347e1c3f3b"},"cell_type":"code","source":"alg_acc={} # to keep accuracies","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adcc5d7c49c8abd4a027e49064dfa3e797ac5cf1"},"cell_type":"markdown","source":"**KNearestNeighbor**"},{"metadata":{"_uuid":"689b37c273c7652cfb59ed6db4819c231e21fea6"},"cell_type":"markdown","source":"Before fitting my data i want to find best K value, so i will make a for loop."},{"metadata":{"trusted":true,"_uuid":"91e8efd26a0dcf5983d53b8aa4b1eb88347a410b"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nscores=[]\nfor each in range(1,10):\n    knn_t=KNeighborsClassifier(n_neighbors=each)\n    knn_t.fit(x_train,y_train)\n    scores.append(knn_t.score(x_test,y_test))\nplt.plot(range(1,10),scores)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1fedcbf060f44754a1c595dabe537b984a31cbe"},"cell_type":"markdown","source":"K=2 gives best accuracy so i will use it to create my model."},{"metadata":{"trusted":true,"_uuid":"9b5d2cdf6059092c9c53d719dd5bf977daf080fb"},"cell_type":"code","source":"knn=KNeighborsClassifier(n_neighbors=2)\nknn.fit(x_train,y_train)\nprint(\"accuracy is\",knn.score(x_test,y_test))\nalg_acc[\"knn\"]=knn.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"311b8d8739dd4b59f937d91c01b41a0af4d263f4"},"cell_type":"markdown","source":"To understand and evaluate accuracy i will use confusion matrix."},{"metadata":{"trusted":true,"_uuid":"c0d4443b75bce6815b359781b3b4d023d10904f8"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ny_pre=knn.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pre)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc99ee705060fcfce3de628f0da6ee40d5ec541d"},"cell_type":"markdown","source":"   Let visulization confusion matrix"},{"metadata":{"trusted":true,"_uuid":"de610102ae268d091addcd8fc384423ae3bfb015"},"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(cm,annot=True,fmt=\".0f\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"befaf208e92b20eacb9e839ab236943484c164ff"},"cell_type":"markdown","source":"According to confusion matrix heatmap. We predict 149  of non-legendary pokemon correctly, only 1 wrong. It's good accuracy. However, there are 10 legendary pokemon, our model predict 5 of them correctly, 5 wrong. 50% accuracy. The reason of low accuracy is we dont have enough legendary pokemon at data."},{"metadata":{"trusted":true,"_uuid":"3f57861d3ae93ea0a91ade528828e214b1c51098"},"cell_type":"code","source":"np.count_nonzero(y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6c8def4ab4e6dc51a37efa1f7c54d831a47bece"},"cell_type":"markdown","source":"We used 55 legendary pokemon at data to train our model. not enough."},{"metadata":{"_uuid":"60fb54308be9299d9d04819192645cd1af877700"},"cell_type":"markdown","source":"**Support Vector Machine**"},{"metadata":{"trusted":true,"_uuid":"44152ebe7f675841b4c0ff818e0b0a2f1ae165e5"},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm=SVC(random_state=42)\nsvm.fit(x_train,y_train)\nalg_acc[\"svm\"]=svm.score(x_test,y_test)\nprint(\"Support Vector Machine test accuracy is:\",svm.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4081e5b85f7d5e46e486c37fd03f55761928353"},"cell_type":"markdown","source":"**Naive Bayes**"},{"metadata":{"trusted":true,"_uuid":"1e0ccaa750f7c01e45e64510b3a0b7648741eb56"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(x_train,y_train)\nalg_acc[\"nb\"]=nb.score(x_test,y_test)\nprint(\"Naive Bayes test accuracy is:\",nb.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60a3936218ae45f222359205ea9778cd91d01bd4"},"cell_type":"markdown","source":"**Decision Tree**"},{"metadata":{"trusted":true,"_uuid":"47707580142af1dd485e1556fd8c9cdf21dde4fb"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nalg_acc[\"dt\"]=dt.score(x_test,y_test)\nprint(\"Decision Tree test accuracy is:\",dt.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9ce5950ecce6f1035d6316dff75d0ba5a174a29"},"cell_type":"markdown","source":"**Random Forest**"},{"metadata":{"_uuid":"e83397327df3b6a8fd574e9431e7423093baf1f0"},"cell_type":"markdown","source":"Random forest has hyperparameter called as n_estimators. It means tree number. To find best hyperparameter value, we can use for loop."},{"metadata":{"trusted":true,"_uuid":"47cfbe32888bbf6795872a43d0c6f5811e8c29f4"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nscores2=[]\nfor each in range(100,1000,100):\n    rf_t=RandomForestClassifier(n_estimators=each,random_state=42)\n    rf_t.fit(x_train,y_train)\n    scores2.append(rf_t.score(x_test,y_test))\nplt.plot(range(100,1000,100),scores2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff9013828029d63d52ba009421ca28dabfdad06b"},"cell_type":"markdown","source":"100 is a good value for n_estimators hyperparamater."},{"metadata":{"trusted":true,"_uuid":"00f8550ff6ebce00b1d0785c6fabf839f019acd8"},"cell_type":"code","source":"rf=RandomForestClassifier(n_estimators=100,random_state=42)\nrf.fit(x_train,y_train)\nalg_acc[\"rf\"]=rf.score(x_test,y_test)\nprint(\"Random Forest test accuracy is:\",rf.score(x_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cec861c819ab9646e18a658e9a130acfb6ec10c"},"cell_type":"code","source":"label=alg_acc.keys()\nscores=alg_acc.values()\nplt.plot(label,scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d721e9c7bf33b814c5bbee8fdeeb3758ad236e07"},"cell_type":"markdown","source":"**Conclusion**"},{"metadata":{"_uuid":"7686c89f28505c7d0635e49b7d9b66362050e842"},"cell_type":"markdown","source":"As you see; Knn and random forest gave best accuracies at our data."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}