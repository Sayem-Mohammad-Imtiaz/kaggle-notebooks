{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nimport spacy\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom wordcloud import WordCloud\nfrom unidecode import unidecode\nfrom string import punctuation\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom spacy.lang.pt.stop_words import STOP_WORDS\n\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introdução"},{"metadata":{},"cell_type":"markdown","source":"Um dos problemas do NLP está na qualidade dos dados, principalmente quando se trabalha com textos não formais (como redes sociais). Além disso a quantidade de material replicável em português é escasso, o que atrapalha bastante na modelagem. Sendo assim, o objetivo deste notebook é criar ferramentas para o pré-processamento dos textos."},{"metadata":{},"cell_type":"markdown","source":"# Funções para Pré-Processamento"},{"metadata":{},"cell_type":"markdown","source":"## Limpeza do texto"},{"metadata":{},"cell_type":"markdown","source":"A primeira função, e talvez a mais importante, é para limpar o texto. Nesta etapa é tratado tudo o que pode atrapalhar no processo de modelagem. Além disso, acredito que ela possa ser utilizada em qualquer situação quando se trata de NLP.\n\nNa função abaixo são realizados os seguintes passos para limpeza do texto:\n\n- Colocar o texto em caixa baixa (letras minúsculas);\n- Excluir citações - útil para textos retirados de redes sociais;\n- Excluir acentuação das palavras;\n- Excluir html tags para textos retirados de fóruns e afins;\n- Excluir números;\n- Excluir URL's;\n- Excluir pontuação e caracteres especiais.\n\nExistem outras tratativas que podem ser adicionadas, mas algumas possuem restrições em questão de instalação (só funcionar no linux por exemplo) ou só ter disponível em inglês. Um exemplo é a função Speller da biblioteca autocorrect que corrige erros de digitação (com um erro logicamente) e só funciona em inglês."},{"metadata":{"trusted":true},"cell_type":"code","source":"def limpar_texto(text):\n    \n    # Colocando todas as letras do texto em caixa baixa:\n    text = text.lower()\n    # Excluindo citações com @:\n    text = re.sub('@[^\\s]+', '', text)\n    # Excluindo acentuação das palavras:\n    text = unidecode(text)\n    # Excluindo html tags, como <strong></strong>:\n    text = re.sub('<[^<]+?>','', text)\n    # Excluindo os números:\n    text = ''.join(c for c in text if not c.isdigit())\n    # Excluindo URL's:\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)\n    # Excluindo pontuação:\n    text = ''.join(c for c in text if c not in punctuation)\n    \n    # Retornando o texto tratado tokenizado:\n    \n    return word_tokenize(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Testando a função:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# O texto abaixo contém todas as situações para que seja feita a limpeza:\n\ntexto = \"\"\"\n<strong>Olá</strong> @usuario, vamos testar a função #clean_text?\nCaso tenha dúvidas, uma boa pesquisa no www.google.com pode ajudar!\nMesmo que você tenha que pesquisar 100 vezes!\n\"\"\"\n\ntexto_limpo = limpar_texto(texto)\nprint(texto_limpo)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remoção das Palavras de Parada (Stop Words)"},{"metadata":{},"cell_type":"markdown","source":"As palavras de parada são aquelas que, dependendo do caso, podem ser consideradas irrelevantes para o conjunto de resultados a ser exibido em uma busca realizada em uma *search engine*. Por exemplo: o, para, com, foi.\n\n**Quando removê-las de um texto?**\n\nQuando se remove as palavras de parada geralmente o texto perde o seu contexto (ou sentido), o que pode atrapalhar alguns algoritmos de descoberta, principalmente aqueles que trabalham com redes neurais. Então é importante ter atenção.\n\nAgora, quando montamos um **saco de palavras** (Bag-of-Words) as palavras de maior frequência provavelmente serão palavras de parada. Portanto, neste caso, removê-las é uma boa prática.\n\n**Diferenças de linguagens e bibliotecas**\n\nCada linguagem possui as suas palavras de parada e a qualidade da remoção depende de vários fatores, dentre eles o quão correto está escrito o texto. Então, dependendo de onde ele foi coletado (redes sociais por exemplo), poderão aparecer ruídos após a remoção.\nAs duas principais bibliotecas que possuem funções para remoção de palavras de parada são spacy e nltk. Além disso, é possível (e recomendado pelo menos considerar) criar a própria lista de palavras para remover do texto nesta etapa."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removendo as stopwords utilizando a lista do nltk e do spacy:\n\nsw = list(set(stopwords.words('portuguese') + list(STOP_WORDS)))\n\ndef remove_stop_words(texts, stopwords = sw):\n      \n    new_texts = list()\n    \n    for word in texts:\n        if word not in stopwords:\n            new_texts.append(''.join(word))\n\n    return new_texts\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos ver a nossa lista de palavras de parada:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Agora vamos aplicá-la no texto de saída da função de limpeza:"},{"metadata":{"trusted":true},"cell_type":"code","source":"texto_sem_stop_words = remove_stop_words(texto_limpo)\nprint(texto_sem_stop_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Se estivéssemos criando um modelo a palavra 'cleantext' talvez não fosse importante. Vamos adicioná-la na nossa lista de palavras de palavra rodar novamente a função:"},{"metadata":{"trusted":true},"cell_type":"code","source":"texto_sem_stop_words = remove_stop_words(texto_limpo, sw + ['cleantext'])\nprint(texto_sem_stop_words)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lematização e stemização"},{"metadata":{},"cell_type":"markdown","source":"Lematização e stemização são processos que reduzem as palavras. A vantagem de utilizá-los é a redução do vocabulário e abstração de significado. No caso do texto que estamos usando como exemplo, as palavras 'pesquisa' e 'pesquisar' poderiam ser reduzidas, pois em questão de significado elas agregam de forma igual no processo de modelagem.\n\n- A lematização reduz a palavra ao seu lema, que é a forma no masculino e singular. No caso de verbos o lema é o infinitivo. Por exemplo, as palavras \"menino\", \"meninos\", \"menininhos\" são todas formas do lema: \"menino\".\n\n- A stemização reduz a palavra ao seu radical. Por exemplo, as palavras \"menino\", \"meninos\", \"menininhos\" são todas formas do lema: \"menin\".\n\nUm dos problemas de utilizar estas funções, principalmente a lematização, é que ele pode retornar alguns resultados estranhos, podendo perder o contexto do original. Mas mesmo assim o contexto ficará melhor do que extrair o radical.\n\nEm termos de velocidade de processamento, extrair os radicais vai fazer com que mais palavras se agrupem e, consequentemente, uma possível modelagem fique mais rápida. Mas não quer dizer que a qualidade da predição vai ficar boa."},{"metadata":{"trusted":true},"cell_type":"code","source":"# primeiramente é necessário realizar a instalação abaixo:\n\n!python -m spacy download pt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vamos criar uma função que mostra o texto original, a interpretação - da função - semântica dela e o lema\n\nnlp = spacy.load(\"pt\")\n\ndef verificar_lemma(words):\n    \n    text = \"\"\n    pos = \"\"\n    lemma = \"\"\n    for word in nlp(words):\n        text += word.text + \"\\t\"\n        pos += word.pos_ + \"\\t\"\n        lemma += word.lemma_ + \"\\t\"\n\n    print(text)\n    print(pos)\n    print(lemma)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos verificar como ficaria a extração do lema em algumas frases:"},{"metadata":{"trusted":true},"cell_type":"code","source":"verificar_lemma('o sentido desta frase está errado')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neste caso a função extraiu erroneamente o lema da palavra **sentido**, pois ela é um substantivo neste caso e o lema seria sentido. Além disso, separou a palavra desta em **d** e **esta**, sendo que o lema seria **deste**. Para as demais palavras a extração do lema funcionou corretamente."},{"metadata":{"trusted":true},"cell_type":"code","source":"verificar_lemma('você está se sentindo bem?')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neste caso a função extraiu o lema corretamente. O problema é que em um dataset com as duas frases, a palavra sentido seria agrupada, sendo que o lema das duas não é o mesmo, o que poderia acarretar em problemas em um algoritmo de aprendizagem. Agora vamos ver os radicais."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Vamos criar uma função que mostra o texto original e o stem de cada palavra\n\ndef verificar_radical(words):\n    \n    stemmer = nltk.stem.SnowballStemmer('portuguese')\n    text = \"\"\n    stem = \"\"\n    \n    for word in word_tokenize(words):\n\n        text += word + \"\\t\"\n        stem += stemmer.stem(word) + \"\\t\"\n    \n    print(text)\n    print(stem)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos verificar a extração do radical com as mesmas frases que utilizamos na função de lema:"},{"metadata":{"trusted":true},"cell_type":"code","source":"verificar_radical('o sentido desta frase esta errado')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"verificar_radical('você está se sentindo bem?')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Novamente a palavra **sentido** seria agrupada, mas neste caso, como é o radical, está correto."},{"metadata":{},"cell_type":"markdown","source":"Para concluir, utilizar ou não as funções de lema e radical é de escolha de quem está desenvolvendo. Caso seja escolhido por utilizar, aconselho dar uma atenção na qualidade das extrações."},{"metadata":{},"cell_type":"markdown","source":"## Nuvem de palavras"},{"metadata":{},"cell_type":"markdown","source":"Uma nuvem de palavras é uma boa maneira de verificar quais palavras são mais ou menos relevantes em um dataset. Trata-se de um gráfico que *plota* as palavras indicando através de seu tamanho a quantidade de vezes que apareceu em todos os textos. Outra utilidade é adicionar novas palavras na lista de stop words que por ventura podem aparecer como relevante e não estarem na lista.\n\nAbaixo temos uma função que cria uma nuvem de palavras:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def nuvem_palavras(textos):\n    \n    # Juntando todos os textos na mesma string\n    todas_palavras = ' '.join([texto for texto in textos])\n    # Gerando a nuvem de palavras\n    nuvem_palvras = WordCloud(width= 800, height= 500,\n                              max_font_size = 110,\n                              collocations = False).generate(todas_palavras)\n    # Plotando nuvem de palavras\n    plt.figure(figsize=(24,12))\n    plt.imshow(nuvem_palvras, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vamos testar as funções de nuvem de palavras, CountVectorizer e TfidfVectorizer em um dataset de reviews traduzidos do imdb:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/imdb-ptbr/imdb-reviews-pt-br.csv', nrows=1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vamos ver as primeiras cinco linhas do dataset:\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construindo a nuvem de palavras:\nnuvem_palavras(df[\"text_pt\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Podemos observar que aparecem várias palavras de parada como relevantes no dataset. Portanto seria interessante removê-las, talvez adicionando a palavra \"filme\", pois se trata de *reviews* de filmes."},{"metadata":{},"cell_type":"markdown","source":"## CountVectorizer e TfidfVectorizer"},{"metadata":{},"cell_type":"markdown","source":"Após a realização da limpeza dos textos é necessário transformar o dataset número para aplicar algum algoritmo de aprendizagem. Para tal, vamos apresentar duas técnicas: **CountVectorizer** e **TfidfVectorizer**.\n\nO CountVectorizer agrupa todas as palavras e faz uma contagem da frequência de cada uma.\n\nO TfidfVectorizer é um índice no qual o valor aumenta proporcionalmente à contagem das palavras, mas é compensado pela frequência da palavra no corpus (conjunto de todas as palavras do dataset). Este é o IDF, que significa *inverse document frequency part* - parte inversa da frequência no documento. A vantagem de usá-lo está no fato de que ele vai dar menos importância para palavras como as *stopwords*.\n\nAbaixo temos funções para as duas técnicas:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def countvectorizer(textos):\n\n    vect = CountVectorizer()\n    text_vect = vect.fit_transform(textos)\n    \n    return text_vect\n\ndef tfidfvectorizer(textos):\n    \n    vect = TfidfVectorizer(max_features=50)\n    text_vect = vect.fit_transform(textos)\n    \n    return text_vect","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neste caso não iremos aplicá-las, pois não estamos interessados neste notebook em avançar para a etapa de criação de modelos, mas sim em preparar um dataset limpo para tal. Ao invés disso, vamos juntar todas as funções em uma classe e aplicá-las ao dataset do imdb."},{"metadata":{},"cell_type":"markdown","source":"# Classe de Pré-processamento para NLP"},{"metadata":{"trusted":true},"cell_type":"code","source":"class preprocess_nlp(object):\n    \n    def __init__(self, texts, stopwords = True, lemma=False, stem=False, wordcloud=True, numeric='tfidf'):\n        \n        self.texts = texts\n        self.stopwords = stopwords\n        self.lemma = lemma\n        self.stem = stem\n        self.wordcloud = wordcloud\n        self.numeric = numeric\n        self.new_texts = None\n        self.stopwords_list = list()\n        \n    def clean_text(self):\n\n        new_texts = list()\n\n        for text in self.texts:\n\n            text = text.lower()\n            text = re.sub('@[^\\s]+', '', text)\n            text = unidecode(text)\n            text = re.sub('<[^<]+?>','', text)\n            text = ''.join(c for c in text if not c.isdigit())\n            text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)\n            text = ''.join(c for c in text if c not in punctuation)\n            new_texts.append(text)\n        \n        self.new_texts = new_texts\n\n    def create_stopwords(self):\n        \n        stop_words = list(set(stopwords.words('portuguese') + list(STOP_WORDS)))\n        \n        for word in stop_words:\n\n            self.stopwords_list.append(unidecode(word))\n       \n    \n    def add_stopword(self, word):\n        \n        self.stopwords_list += [word]\n        \n\n    def remove_stopwords(self):\n\n        new_texts = list()\n\n        for text in self.new_texts:\n\n            new_text = ''\n\n            for word in word_tokenize(text):\n\n                if word.lower() not in self.stopwords_list:\n\n                    new_text += ' ' + word\n\n            new_texts.append(new_text)\n\n        self.new_texts = new_texts\n\n\n    def extract_lemma(self):\n        \n        nlp = spacy.load(\"pt\")\n        new_texts = list()\n\n        for text in self.texts:\n\n            new_text = ''\n\n            for word in nlp(text):\n\n                new_text += ' ' + word.lemma_\n\n            new_texts.append(new_text)\n        \n        self.new_texts = new_texts\n    \n\n    def extract_stem(self):\n\n        stemmer = nltk.stem.SnowballStemmer('portuguese')\n        new_texts = list()\n\n        for text in self.texts:\n\n            new_text = ''\n\n            for word in word_tokenize(text):\n\n                new_text += ' ' + stemmer.stem(word)\n\n            new_texts.append(new_text)\n\n        self.new_texts = new_texts\n    \n\n    def word_cloud(self):\n\n        all_words = ' '.join([text for text in self.new_texts])\n        word_cloud = WordCloud(width= 800, height= 500,\n                               max_font_size = 110,\n                               collocations = False).generate(all_words)\n        plt.figure(figsize=(24,12))\n        plt.imshow(word_cloud, interpolation='bilinear')\n        plt.axis(\"off\")\n        plt.show()\n        \n\n    def countvectorizer(self):\n\n        vect = CountVectorizer()\n        text_vect = vect.fit_transform(self.new_texts)\n\n        return text_vect\n    \n\n    def tfidfvectorizer(self):\n\n        vect = TfidfVectorizer(max_features=50)\n        text_vect = vect.fit_transform(self.new_texts)\n\n        return text_vect\n    \n    \n    def preprocess(self):\n\n        self.clean_text()\n        \n        if self.stopwords == True:\n            self.create_stopwords()\n            self.remove_stopwords()\n            \n        if self.lemma == True:\n            self.extract_lemma()\n        \n        if self.stem == True:\n            self.extract_stem() \n        \n        if self.wordcloud == True:\n            self.word_cloud()\n        \n        if self.numeric == 'tfidf':\n            text_vect = self.tfidfvectorizer()\n        elif self.numeric == 'count':\n            text_vect = self.countvectorizer()\n        else:\n            print('metodo nao mapeado!')\n            exit()\n            \n        return text_vect","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Na criação desta classe coloquei alguns argumentos opcionais em relação à quais funções serão executadas no pré-processamento. O padrão será remover as palavras de parada, fazer a limpeza do texto, apresentar a nuvem de palavras e aplicar o TfidfVectorizer.\n\nAgora vamos aplicar na base do imdb:"},{"metadata":{"trusted":true},"cell_type":"code","source":"prepro = preprocess_nlp(df['text_pt'], numeric='count')\n#adicionando as palavras filme e filmes na lista de palavras de parada, pois elas são irrelevantes neste contexto\nprepro.add_stopword('filme') \nprepro.add_stopword('filmes') \nsparse_matrix = prepro.preprocess()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusão"},{"metadata":{},"cell_type":"markdown","source":"Neste documento abordamos algumas funções de pré-processamento de textos para análise de sentimento onde foi possível notar que existe uma gama enorme de possibilidades para tal. Ao final, criamos uma classe, sem aprofundar muito em cada função, para realizar a tratativa de um dataframe de textos com algumas opções e aplicamos em um dataset selecionado.\nA partir daí é possível realizar estudos exploratórios e preditivos na base resultante."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}