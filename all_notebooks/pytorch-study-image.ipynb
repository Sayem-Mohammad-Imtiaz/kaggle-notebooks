{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Pytorch Study Image\nThis notebook is intended as a study of the basics of PyTorch, specificaly when dealing with images.  \nI will first build a Neural Network with fully connected layers to classify the MNIST dataset. Then I will use a Convolutional Neural Network for classification of the CIFAR-10 dataset.  \nNote that this is not intended to have a great performance, but to get a better understanding of how PyTorch works, so I will use only some basic tools and will not perform any optimization on the parameters, rather I will use the predefined settings or use some commonly used parameters.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nfrom torch import nn #API for building neural networks\nfrom torch.utils.data import Dataset, DataLoader #Imports the Dataset and Dataloader classes\nimport os\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-22T11:29:13.514473Z","iopub.execute_input":"2021-06-22T11:29:13.514849Z","iopub.status.idle":"2021-06-22T11:29:14.656151Z","shell.execute_reply.started":"2021-06-22T11:29:13.514764Z","shell.execute_reply":"2021-06-22T11:29:14.655234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Using {} device'.format(device)) #Use GPU if available","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:29:14.657644Z","iopub.execute_input":"2021-06-22T11:29:14.657964Z","iopub.status.idle":"2021-06-22T11:29:14.726987Z","shell.execute_reply.started":"2021-06-22T11:29:14.65793Z","shell.execute_reply":"2021-06-22T11:29:14.725866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Required hyperparameters\nlearning_rate = 0.001\nepochs = 30","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:29:14.729117Z","iopub.execute_input":"2021-06-22T11:29:14.729758Z","iopub.status.idle":"2021-06-22T11:29:14.736223Z","shell.execute_reply.started":"2021-06-22T11:29:14.729684Z","shell.execute_reply":"2021-06-22T11:29:14.735323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we can call these functions to train and evaluate our model.","metadata":{}},{"cell_type":"markdown","source":"Note how the test score starts higher than the train score and the test loss lower than the train loss. That is beacause of the batch training.  \nFor training, we calculate the average over all the batches, but after every batch we update the weights, so, in principle (and as demonstrated by the graph), the model learns something after each batch. So after each epoch we have already updated the model 120 times (60000 samples/500 batch) to calculate the test score and loss.","metadata":{}},{"cell_type":"markdown","source":"### Image data\nAlthough we used the MNIST dataset above, we used only linear layers. For images it is usual to use convolutional layers, so the model can capture spatial patterns.  \nAlso, the MNIST dataset consists of grayscale images, but in most cases we will deal with color images, so below we will construct a convolutional neural network for classification of color images using the CIFAR-10 dataset.","metadata":{}},{"cell_type":"markdown","source":"First we will create the dataset. We need to create a .csv file with the names of the files and its respective label.","metadata":{}},{"cell_type":"markdown","source":"With this code I have created the annotations for the training and test images\n\n    import csv\n    \n    train_folder = '../input/cifar10-pngs-in-folders/cifar10/train'  \n    test_folder = '../input/cifar10-pngs-in-folders/cifar10/test'  \n    labels_dict = {  \n    'airplane': 0,  \n    'horse': 1,  \n    'truck': 2,  \n    'automobile': 3,  \n    'ship': 4,  \n    'dog': 5,  \n    'bird': 6,  \n    'frog': 7,  \n    'cat': 8,  \n    'deer': 9  \n    }  \n\n    with open('../train_annotations.csv', mode='w') as csv_file:  \n        csv_writer = csv.writer(csv_file)  \n        for folder in os.listdir(train_folder):  \n            for file in os.listdir(train_folder + '/' + folder):  \n                label = str(labels_dict[folder])  \n                path = train_folder + '/' + folder + '/' + file  \n                csv_writer.writerow([path, label])  \n\n    with open('../test_annotations.csv', mode='w') as csv_file:  \n        csv_writer = csv.writer(csv_file)  \n        for folder in os.listdir(train_folder):  \n            for file in os.listdir(train_folder + '/' + folder):  \n                label = str(labels_dict[folder])  \n                path = train_folder + '/' + folder + '/' + file  \n                csv_writer.writerow([path, label])","metadata":{}},{"cell_type":"code","source":"from torchvision.io import read_image\n\nclass ImageDataset(Dataset):\n    def __init__(self, annotations_file):\n        self.annotations = pd.read_csv(annotations_file, header=None, \n                                       names=['Path', 'Label'], delimiter=',')\n    \n    def __len__(self):\n        return(len(self.annotations))\n    \n    def __getitem__(self, index):\n        path = self.annotations['Path'][index]\n        label = torch.tensor(self.annotations['Label'][index]).float()\n        img = read_image(path).float()\n        return(img, label)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:29:14.738077Z","iopub.execute_input":"2021-06-22T11:29:14.73845Z","iopub.status.idle":"2021-06-22T11:29:14.885442Z","shell.execute_reply.started":"2021-06-22T11:29:14.738395Z","shell.execute_reply":"2021-06-22T11:29:14.884517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = ImageDataset('../input/cifarannotations/train_annotations.csv')\ntest_dataset = ImageDataset('../input/cifarannotations/test_annotations.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:29:14.886787Z","iopub.execute_input":"2021-06-22T11:29:14.887179Z","iopub.status.idle":"2021-06-22T11:29:15.034725Z","shell.execute_reply.started":"2021-06-22T11:29:14.887138Z","shell.execute_reply":"2021-06-22T11:29:15.033865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want to zero mean our dataset, so we will computer the mean for each channel ","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint('Using {} device'.format(device)) #Use GPU if available","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:29:15.03603Z","iopub.execute_input":"2021-06-22T11:29:15.036421Z","iopub.status.idle":"2021-06-22T11:29:15.0423Z","shell.execute_reply.started":"2021-06-22T11:29:15.036386Z","shell.execute_reply":"2021-06-22T11:29:15.040494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ntrain_loader = DataLoader(train_dataset, batch_size=500, shuffle=True, num_workers=8)\ntest_loader = DataLoader(test_dataset, batch_size=500, shuffle=True, num_workers=8)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:29:15.043929Z","iopub.execute_input":"2021-06-22T11:29:15.044547Z","iopub.status.idle":"2021-06-22T11:29:15.051624Z","shell.execute_reply.started":"2021-06-22T11:29:15.044507Z","shell.execute_reply":"2021-06-22T11:29:15.0504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ch0, ch1, ch2 = 0,0,0\nfor img, label in train_loader:\n    ch0 += img[:,0,:,:].mean(dim=0)\n    ch1 += img[:,1,:,:].mean(dim=0)\n    ch2 += img[:,2,:,:].mean(dim=0)\ndiv = len(train_dataset)/train_loader.batch_size\nch0 = ch0/div\nch1 = ch1/div\nch2 = ch2/div\nmean_img = torch.stack((ch0,ch1,ch2)).to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:29:15.055025Z","iopub.execute_input":"2021-06-22T11:29:15.055463Z","iopub.status.idle":"2021-06-22T11:30:03.948746Z","shell.execute_reply.started":"2021-06-22T11:29:15.055423Z","shell.execute_reply":"2021-06-22T11:30:03.947719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.ConvNet = nn.Sequential(\n        nn.Conv2d(in_channels=3, out_channels=10, kernel_size=5, padding=2), #32 x 32 x 20\n        nn.ReLU(),\n        nn.MaxPool2d(2, 2), # 16 x 16 x 20\n        nn.Conv2d(in_channels=10, out_channels=10, kernel_size=5, padding=2), #16 x 16 x 20 \n        nn.ReLU(),\n        nn.Conv2d(in_channels=10, out_channels=10, kernel_size=5, padding=2), #16 x 16 x 20 \n        nn.ReLU(),\n        nn.MaxPool2d(2, 2), # 8 x 8 x 20\n        nn.Flatten(),\n        nn.Linear(8*8*10, 10) #First number is number of inputs, second is the number of outputs\n        )\n    def forward(self, x):\n        logits = self.ConvNet(x)\n        return(logits)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:46:19.773227Z","iopub.execute_input":"2021-06-22T11:46:19.77357Z","iopub.status.idle":"2021-06-22T11:46:19.781426Z","shell.execute_reply.started":"2021-06-22T11:46:19.773535Z","shell.execute_reply":"2021-06-22T11:46:19.780288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = NeuralNetwork().to(device) #Needs to be stored somewhere. Use GPU for speed.\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:46:22.203884Z","iopub.execute_input":"2021-06-22T11:46:22.204236Z","iopub.status.idle":"2021-06-22T11:46:22.214112Z","shell.execute_reply.started":"2021-06-22T11:46:22.204204Z","shell.execute_reply":"2021-06-22T11:46:22.213054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from prettytable import PrettyTable\n\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if not parameter.requires_grad: continue\n        param = parameter.numel()\n        table.add_row([name, param])\n        total_params+=param\n    print(table)\n    print(f\"Total Trainable Params: {total_params}\")\n    return(total_params)\n    \ntotal_parameters = count_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:46:23.936242Z","iopub.execute_input":"2021-06-22T11:46:23.936558Z","iopub.status.idle":"2021-06-22T11:46:23.943612Z","shell.execute_reply.started":"2021-06-22T11:46:23.936531Z","shell.execute_reply":"2021-06-22T11:46:23.942749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loss = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:46:24.524664Z","iopub.execute_input":"2021-06-22T11:46:24.524967Z","iopub.status.idle":"2021-06-22T11:46:24.529526Z","shell.execute_reply.started":"2021-06-22T11:46:24.52494Z","shell.execute_reply":"2021-06-22T11:46:24.528428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, dataloader, model_loss, optimizer, mean=0):\n    size = len(dataloader.dataset)\n    total_loss = 0\n    total_correct = 0\n    model.train()\n    for X, y in dataloader: #Gets batch each iteration until it runs out of data\n        X = X.to(device)\n        X = X - mean\n        y = y.to(device)\n        #Forward pass\n        pred = model(X) #Returns the logits of the last layer\n        loss = model_loss(pred, y.long())\n        total_loss += loss\n        total_correct += (pred.argmax(1) == y).sum().item()\n        #Backward pass\n        optimizer.zero_grad() #Sets all the gradients to 0, so it can be computed for this epoch\n        loss.backward() #Backpropagates through the loss function\n        optimizer.step() #Backpropagates and updates the weights of the model\n        \n    avg_loss = total_loss/size\n    score = total_correct/size\n    return(avg_loss, score)\n        \ndef test_model(model, dataloader, model_loss, mean=0):\n    size = len(dataloader.dataset)\n    total_loss = 0\n    total_correct = 0\n    model.eval()\n    for X,y in dataloader:\n        X = X.to(device)\n        X = X-mean\n        y = y.to(device)\n        pred = model(X) #Logits of the last layer\n        loss = model_loss(pred, y.long())\n        total_loss += loss.item() #This .item() get the value in the tensor. Avoids memory consuptiom\n        total_correct += (pred.argmax(1) == y).sum().item() #by not storing the computational graph \n    avg_loss = total_loss/size\n    score = total_correct/size\n    return(avg_loss, score)\n\ndef train_and_test(model, train_loader, test_loader, model_loss, optimizer, mean, epochs):\n    train_losses, train_scores, test_losses, test_scores = [], [], [], []\n    for epoch in range(epochs):\n        print('Epoch:', epoch)\n        train_loss, train_score = train_model(model, train_loader, model_loss, optimizer, mean_img)\n        test_loss, test_score = test_model(model, test_loader, model_loss, mean_img)\n        train_losses.append(train_loss)\n        train_scores.append(train_score)\n        test_losses.append(test_loss)\n        test_scores.append(test_score)\n        print('Train loss: {:.2f}       Test loss: {:.2f}'.format(train_loss, test_loss))\n        print('Train score: {:.2f}%    Test score: {:.2f}%'.format(train_score*100, test_score*100))\n        print('='*20)\n\n    return(train_losses, test_losses, train_scores, test_scores)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:04.006047Z","iopub.execute_input":"2021-06-22T11:30:04.006483Z","iopub.status.idle":"2021-06-22T11:30:04.020733Z","shell.execute_reply.started":"2021-06-22T11:30:04.006446Z","shell.execute_reply":"2021-06-22T11:30:04.019839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot the train and test errors and losses\ndef plot_trainxtest(train_losses, test_losses, train_scores, test_scores):\n    fig, axs = plt.subplots(2,1, sharex=True)\n    axs[0].plot(train_scores, label='Train')\n    axs[0].plot(test_scores, label='Test')\n    axs[0].legend(loc='lower right')\n    axs[0].set_ylabel('Score')\n    axs[1].plot(train_losses)\n    axs[1].plot(test_losses)\n    axs[1].set_xlabel('Epoch')\n    axs[1].set_ylabel('Loss')\n    fig.suptitle('Train x Test')\n    fig.subplots_adjust(hspace = .001)\n    axs[0].set_xticklabels(())\n    axs[0].title.set_visible(False)\n    fig.show()\n    print('Final test score: ', test_scores[-1])","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:30:04.022023Z","iopub.execute_input":"2021-06-22T11:30:04.022681Z","iopub.status.idle":"2021-06-22T11:30:04.034023Z","shell.execute_reply.started":"2021-06-22T11:30:04.022644Z","shell.execute_reply":"2021-06-22T11:30:04.033187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 20\nplot_trainxtest(*train_and_test(model, train_loader, test_loader, model_loss, optimizer, mean_img, epochs))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T11:46:31.39721Z","iopub.execute_input":"2021-06-22T11:46:31.397533Z","iopub.status.idle":"2021-06-22T11:53:29.429041Z","shell.execute_reply.started":"2021-06-22T11:46:31.397505Z","shell.execute_reply":"2021-06-22T11:53:29.428227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), '.\\CIFAR_model')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:10:40.47071Z","iopub.execute_input":"2021-06-22T12:10:40.471038Z","iopub.status.idle":"2021-06-22T12:10:40.483192Z","shell.execute_reply.started":"2021-06-22T12:10:40.471005Z","shell.execute_reply":"2021-06-22T12:10:40.482348Z"},"trusted":true},"execution_count":null,"outputs":[]}]}