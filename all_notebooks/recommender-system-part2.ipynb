{"cells":[{"metadata":{},"cell_type":"markdown","source":"### So here we are using the data which we generated in the last part","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![recommend](https://miro.medium.com/max/2560/1*dOM8OeGZq6FkquXQq-l7HA.jpeg)","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/generated-user-data-for-recommendation/file3.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['rating'].value_counts()[:50].plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SURPISE - is a Python scikit building and analyzing recommender systems that deal with explicit rating data.\n\nThe name SurPRISE (roughly :) ) stands for Simple Python RecommendatIon System Engine.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from surprise import Reader\nfrom surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import train_test_split\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reader = Reader()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = Dataset.load_from_df(df[['UserId', 'ArticleId_served', 'rating']], reader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = data.build_full_trainset()\nprint('Number of users: ',dataset.n_users,'\\n')\nprint('Number of items: ',dataset.n_items)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### With the Surprise library, we will benchmark the following algorithms:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise import SVD\nfrom surprise import SVDpp\nfrom surprise import NMF\nfrom surprise import NormalPredictor\nfrom surprise import KNNBaseline\nfrom surprise import KNNBasic\nfrom surprise import KNNWithMeans\nfrom surprise import BaselineOnly\nfrom surprise import CoClustering\n\nbenchmark = []\n# Iterate over all algorithms\nfor algorithm in [SVD(), SVDpp(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), BaselineOnly(), CoClustering()]:\n    # Perform cross validation\n    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)\n    \n    # Get results & append algorithm name\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n    benchmark.append(tmp)\n    \npd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> There are few more like coclustering , slopeone, KNNWithZScore . feel free to use them as well ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## SVD","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise.model_selection import GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lr_all --> learning rate for all parameters\n\nreg_all --> The regularization term for all parameters","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Tuning SVD parameters with GridSearchCV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n              'reg_all': [0.4, 0.6]}\ngs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n\ngs.fit(data)\n\n# best RMSE score\nprint(gs.best_score['rmse'])\n\n# best RMSE score\nprint(gs.best_score['mae'])\n\n# combination of parameters that gave the best RMSE score\nprint(gs.best_params['rmse'])\n\n# combination of parameters that gave the best MAE score\nprint(gs.best_params['mae'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> After tuning rmse reduced to 1.419594","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svd = SVD(n_factors= 50, reg_all=0.05)\nsvd.fit(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PREDICTIONS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svd.predict(2,4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svd.predict(2,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = df['ArticleId_served'].unique()\nlen(r)\nlist_of_articles = r.tolist()\nlist_of_articles[:5]\n\nrec = []\nfor i in list_of_articles:\n    predicted_rating = svd.predict(2, i)\n    rec.append(predicted_rating)\n    \nrec[:10]    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## BaselineOnly","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Tuning BaselineOnly parameters with GridSearchCV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'bsl_options':{'method': ['als','sgd'],'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n              'reg_all': [0.4, 0.6]}}\n\n# bsl_options = {'method': ['als','sgd'],'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n#               'reg_all': [0.4, 0.6]}\nbsl_algo = BaselineOnly()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gs = GridSearchCV(BaselineOnly, param_grid, measures=['rmse', 'mae'], cv=3)\n\ngs.fit(data)\n\n# best RMSE score\nprint(gs.best_score['rmse'])\n\n# combination of parameters that gave the best RMSE score\nprint(gs.best_params['rmse'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> After tuning rmse reduced to 1.4265558","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bsl_options = {'method': 'sgd', 'n_epochs': 5, 'lr_all': 0.002, 'reg_all': 0.4}\nalgo = BaselineOnly(bsl_options=bsl_options)\n\nalgo.fit(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PREDICTIONS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"algo.predict(2,4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"algo.predict(2,5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r = df['ArticleId_served'].unique()\nlen(r)\nlist_of_articles = r.tolist()\nlist_of_articles[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rec = []\nfor i in list_of_articles:\n    predicted_rating = algo.predict(2, i)\n    rec.append(predicted_rating)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rec[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Despite its merits, RMSE can be quite detached from the ultimate goal of evaluating item ranking experience, since aperfectly  ranked  solution  can  score  arbitrarily  badly  on  an RMSE scale by having scores on the wrong scale, e.g., out of bounds, or just very close to each other.\n\nThe RMSE metric has another issue, particularly importantin our context:  it assumes numerical rating values.  Thus, it shares all the discussed disadvantages of such an assumption.\n\n*First, it cannot express rating scales which vary among differ-ent users*. \n\n*Second, it cannot be applied in cases where ratings are  ordinal*.  \n\nThus,  besides  using  RMSE  we  also  employ  aranking-oriented metric which is free of the aforementioned issues","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" The quality metric we use measures the proportion of well ranked items pairs, denoted by <font color='red'>FCP (for Fraction of Concordant Pairs).</font>\n \n FCP=nc/(nc+nd)\n \n a measure that generalizes the known AUC metric into non-binary ordered outcomes\n \nname of paper(section 5.2) --> <font color='blue'>***Collaborative Filtering on Ordinal User Feedback***</font> by <font color='green'>**Yehuda Koren**</font>  and <font color='green'>**Joseph Sil**</font>","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from surprise import SVD\nfrom surprise import SVDpp\nfrom surprise import SlopeOne\nfrom surprise import NMF\nfrom surprise import NormalPredictor\nfrom surprise import KNNBaseline\nfrom surprise import KNNBasic\nfrom surprise import KNNWithMeans\nfrom surprise import KNNWithZScore\nfrom surprise import BaselineOnly\nfrom surprise import CoClustering\n\nbenchmark = []\n# Iterate over all algorithms\nfor algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n    # Perform cross validation\n    results = cross_validate(algorithm, data, measures=['RMSE','FCP'], cv=3, verbose=False)\n    \n    # Get results & append algorithm name\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n    benchmark.append(tmp)\n    \npd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')  \npd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_fcp') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To be continue........","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}