{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n%matplotlib inline\npd.pandas.set_option(\"display.max_columns\", None)\n# pd.pandas.set_option(\"display.max_rows\", None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/income-qualification/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for the columns that has the null values in them\ncolumn_nan = [feature for feature in train_data.columns if train_data[feature].isnull().any() == True]\nprint(column_nan)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for numerical features\nnumerical_features = [feature for feature in train_data.columns if train_data[feature].dtype != 'object']\ntrain_data[numerical_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking for categorical features\ntrain_data.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### checking the biases in the DATA\n\nsince it is classification problem so we can check the biasness in the data by grouping them all on the target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.groupby('Target')['Target'].count())\nsns.countplot(train_data.Target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from above bar chart we can clearly observe that the is biased towards a single class 4"},{"metadata":{},"cell_type":"markdown","source":"##### Oversampling on the biased data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# since the data seems to be baised so we will first we will do over sampling of the data so to get the balanced data\n# minority specifies the sampler to resmaple those data which are less in count and \n# to reach them at the same count with the other majority class\n\noversampler = RandomOverSampler() #sampling_strategy='minority')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_over, y_over = oversampler.fit_resample(train_data.drop('Target', axis=1), train_data['Target'])\nX_over.shape, y_over.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_over = X_over.merge(y_over, left_index=True, right_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape, dataset_over.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's again check the biasnes of data\nsns.countplot(dataset_over.Target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Checking if all members of the family has the same poverty level\n* Checking if there is a house without a family head."},{"metadata":{"trusted":true},"cell_type":"code","source":"same_poverty = 0\nno_family_head = 0\n\nfor idhogar in dataset_over['idhogar'].unique():\n    if len(dataset_over[dataset_over['idhogar'] == idhogar]['Target'].unique()) == 1:\n        same_poverty += 1\n    if (dataset_over[dataset_over['idhogar'] == idhogar]['parentesco1'] == 0).all():\n        no_family_head += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Family with the same poverty level:', same_poverty)\nprint('Family with the diff poverty level:', len(dataset_over['idhogar'].unique()) - same_poverty)\nprint('House without a Family head:', no_family_head)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Count how many null values are existing in columns."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"dataset_over[column_nan].isnull().sum() / dataset_over.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"columns with zero variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"zero_var_col = [feature for feature in dataset_over.columns if len(dataset_over[feature].unique()) == 1]\nprint('columns with zero variance', zero_var_col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing columns with zero variance\ndataset_over.drop(['elimbasu5'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will be performing Feature Engineering on the copy of the original data\ntrain_df = dataset_over.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since dependency column decimal values so that we will convert into integer value using ceil value. \n# But before that we will have to convert the yes and no values to integer value\n# print(train_df['dependency'].unique())\ntrain_df.loc[train_df['dependency'] == 'no', 'dependency'] = 0\n# print(train_df['dependency'].unique())\ntrain_df.loc[train_df['dependency'] == 'yes', 'dependency'] = train_df[train_df['dependency'] != 'yes']['dependency'].astype('float').mean()\n# print(train_df['dependency'].unique())\ntrain_df['dependency'] = train_df['dependency'].astype('float').apply(np.ceil)\n# print(train_df['dependency'].unique())\ntrain_df['dependency'].plot.box()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since we have some outliers in the dependency column so we will treat them by updating those rows from mean(including non-zero values only) which have outliers\n# train_df = train_df[train_df['dependency'] < 4]\ntrain_df.loc[train_df['dependency']>3, 'dependency'] = np.ceil(train_df[train_df['dependency']>0]['dependency'].mean())\ntrain_df['dependency'].plot.box()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the rent paid has nan values as well as 0. This might be the case where the household people are the owner of that house. So we will be updating the nan value from 0\n\nAnd the dataset is also having some outliers so we will treat them by updating them from mean value of that column"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['v2a1'].fillna(0, inplace=True)\ntrain_df[train_df['v2a1'] > 0]['v2a1'].plot.box()\nplt.show()\ntrain_df[(train_df['v2a1'] < 330000) & (train_df['v2a1'] > 0)]['v2a1'].plot.box()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Treating outliers from the mean value of that column excluding zeros and null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[train_df['v2a1'] > 350000, 'v2a1'] = np.round(train_df[train_df['v2a1'] > 0]['v2a1'].mean())\ntrain_df[train_df['v2a1'] > 0]['v2a1'].plot.box()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing uneccessary features from the dataset\n\nfeature_remove = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin',\n       'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']\ntrain_df.drop(feature_remove, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Set poverty level of the members and the head of the house within a family.\n\nupdating the value of target from the most probable poverty level of the house."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for idhogar in train_df['idhogar'].unique():\n    train_df.loc[train_df['idhogar'] == idhogar, 'Target'] = stats.mode(train_df[train_df['idhogar'] == idhogar]['Target']).mode[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_null = [feature for feature in train_df.columns if train_df[feature].isnull().any() == True]\nprint(column_null)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"apart from the meaneduc rest og the columns are not creating too much significance on the target so we will delete those columns from the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(['v18q1', 'rez_esc'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['meaneduc'].fillna(0, inplace=True)\ntrain_df.drop(['Id', 'idhogar'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.select_dtypes('object').head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df['edjefe'].unique())\ntrain_df.loc[train_df['edjefe'] == 'no', 'edjefe'] = 0\ntrain_df.loc[train_df['edjefe'] == 'yes', 'edjefe'] = train_df[train_df['edjefe'] != 'yes']['edjefe'].astype('float').mean()\nprint(train_df['edjefe'].unique())\ntrain_df['edjefe'] = train_df['edjefe'].astype('float').apply(np.ceil)\nsns.boxplot(train_df['edjefe'])\nplt.show()\ntrain_df.loc[train_df['edjefe'] > 15, 'edjefe'] = np.ceil(train_df[train_df['edjefe']>0]['edjefe'].mean())\nsns.boxplot(train_df['edjefe'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_df['edjefa'].unique())\ntrain_df.loc[train_df['edjefa'] == 'no', 'edjefa'] = 0\ntrain_df.loc[train_df['edjefa'] == 'yes', 'edjefa'] = train_df[train_df['edjefa'] != 'yes']['edjefa'].astype('float').mean()\nprint(train_df['edjefa'].unique())\ntrain_df['edjefa'] = train_df['edjefa'].astype('float').apply(np.ceil)\nprint(train_df['edjefa'].unique())\nsns.boxplot(train_df['edjefa'])\nplt.show()\ntrain_df.loc[train_df['edjefa'] > 15, 'edjefa'] = np.ceil(train_df[train_df['edjefa']>0]['edjefa'].mean())\nsns.boxplot(train_df['edjefa'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(train_df['meaneduc'])\nplt.show()\ntrain_df.loc[train_df['meaneduc'] > 17, 'meaneduc'] = np.ceil(train_df[train_df['meaneduc']>0]['meaneduc'].mean())\nsns.boxplot(train_df['meaneduc'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca.fit(train_df)\npca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df.drop(['Target'], axis=1)\ny = train_df['Target'].values\npca = PCA(n_components=10)\nX_pca = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_pca[:1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_model = RandomForestClassifier(n_estimators = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = rfc_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(y_test, y_pred))\nprint(precision_score(y_test, y_pred, average='weighted'))\nprint(recall_score(y_test, y_pred, average='weighted'))\nprint(f1_score(y_test, y_pred, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(rfc_model, X, y, cv=10)\nprint(cv_score)\nprint('Accuracy after cross validation: ', cv_score.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}