{"cells":[{"metadata":{"_uuid":"d3016d4d6802774d5d9ba7e6fc8ed4d5639c2a50"},"cell_type":"markdown","source":"# Regression Revision with the World Happiness Report Dataset\n  **Content:**\n  1. [Introduction](#1)\n      1. [Dataset](#2)\n      1. [Plan of action](#2)\n  1. [Exploring the data](#2)  \n      1. [Example](#2)\n      1. [Your Turn](#2)\n  1. [Data Preprocessing](#2)\n  1. [Linear Regression](#3)\n      1. [Train the model and predict results](#2)\n      1. [Measure Accurracy](#2)\n      1. [Visualisation](#2)\n  1. [Polynomial Regression](#4)\n      1. [Train the model and predict results](#2)\n      1. [Measure Accurracy](#2)\n      1. [Visualisation](#2)\n  1. [Random Forest](#5)\n      1. [Your Turn](#2)"},{"metadata":{"_uuid":"ea9fde7b60fd9c84e5937dee879d4f9d4e57ba8a"},"cell_type":"markdown","source":"  ## Introduction\n   We will have a look at the regression models once again. Github Repository for the relevant code can be found [here](https://github.com/KacperKubara/USAIS_Workshops/tree/master/Regression). Previous USAIS presentations on regression models can be found [here](https://drive.google.com/open?id=1k5twAgSVGUl8CGjw0qmQitPf6ij0YrIj)"},{"metadata":{"_uuid":"a1a175dd8761f9019eb46d08fdb242c989978b89"},"cell_type":"markdown","source":"  ### Dataset\n  [World Happiness Report Dataset](https://www.kaggle.com/unsdsn/world-happiness/home) will be used to implement our Machine Learning models. I would suggest to follow the link first to understand the dataset a bit better.  "},{"metadata":{"_uuid":"0ff79047594d343210bddcce55e2a386995e2ba3"},"cell_type":"markdown","source":" ### Plan of action  \n  Before we dive into training and testing the model, we will take a step back and have a closer look on the dataset. Starting from simple dataset visualisations, we are going to decide how to format the data for the Machine Learning model. Then we will try using few different Regression models and change their parameters to see what performs the best."},{"metadata":{"_uuid":"6e88a2cf42348d2da1bbd4c388872113b3b44cff"},"cell_type":"markdown","source":"## Exploring the data   \n### Example\nBefore plugging in the data to the Machine Learning model it is good to see what the dataset contains and try to guess what features can be important for the model. We will do few simple visualisations of the dataset to understand it better."},{"metadata":{"_uuid":"8d88512e915e7d2c435bc0e82235782a8d00b25e"},"cell_type":"markdown","source":"Importing the packages and listing the available datasets"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualisations\n\nimport os\nprint(os.listdir(\"../input\")) # print all the files from the \"input\" directory (we provided this directory! When using locally inject path to your files) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4e35e47c76317533f3e33f23d615be8f41e02f1"},"cell_type":"markdown","source":"As we can see from the output above, we have 3 .csv files to import:"},{"metadata":{"trusted":true,"_uuid":"67f73cc7efcacbc6c474b3ec22d7d86b3b2c50d2"},"cell_type":"code","source":"dataset_2015 = pd.read_csv(\"../input/2015.csv\") # Read csv file\ndataset_2016 = pd.read_csv(\"../input/2016.csv\")\ndataset_2017 = pd.read_csv(\"../input/2017.csv\")\n\ndataset_2015 = dataset_2015.sample(frac=1).reset_index(drop=True) # randomly shuffle the rows (for the purpose of our exercise later)\ndataset_2016 = dataset_2016.sample(frac=1).reset_index(drop=True)\ndataset_2017 = dataset_2017.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66aa0f23df1bcbab2930f89077c6397d7d24d148"},"cell_type":"markdown","source":"Let's get a better understanding of how the data looks like!"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"9eae387230846fce89fd6c3b80ed19e1db322ed7"},"cell_type":"code","source":"pd.set_option('display.max_columns', None) #  Ensures that all columns will be displayed\nprint(dataset_2015.head(3)) # Prints first 3 entries\nprint('\\n\\n')\n\nprint('COLUMN NAMES')\nprint(dataset_2015.columns) # Prints the column names\nprint(\"NUMBER OF COLUMNS: \" + str(len(dataset_2015.columns))) # Prints no. columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"444109312d9d7356d7553d86b01fed60cbadece3"},"cell_type":"markdown","source":"We can see how the variables are correlated to themselves by using the correlation matrix. Correlation matrix simply shows how each feature is correlated to another one. '1' stands for the biggest positive correlation (increase in x_1 leads to increase in x_2), '-1' stands for biggest negative correlation (increase in x_1 leads to decrease in x_2) and '0' for no correlation at all.  \n More information about correlation matrix can be found [here](https://www.datascience.com/blog/introduction-to-correlation-learn-data-science-tutorials)."},{"metadata":{"trusted":true,"_uuid":"cfa6cf7cea00d5e8f77001adfa2d414b55af371f"},"cell_type":"code","source":"import seaborn as sns # Library for more fancy plots\ncorr= dataset_2015.corr() # Creates correlation matrix\nsns.heatmap(corr, xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values) # Creats a heatmap based on correlation matrix","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"661eaf5113c93d930d56edcd0efd413664fc84dc"},"cell_type":"markdown","source":"Since we are going to predict the Happiness Score, we should look for the brightest or the darkest cells under the column **''Happiness Score\"** . Thus, features worth trying in our future model should include **Economy (GDP per capita)**, **Family** and **Health (Life Expectancy)**. "},{"metadata":{"_uuid":"6ff9cda55b528c44fe6619925d4eb340d23f3cbb"},"cell_type":"markdown","source":"### **Your Turn**  \nYou can visualise and analyse the dataset in many ways. The more time you spend on understanding the dataset, the more likely you will come up with a better choice of the ML model.  \n ** Task for you: **\n1.  Display the countries with the highest Happiness score in 2015 using the Bar graph (Look [here]() to get a better idea on how to do it)"},{"metadata":{"trusted":true,"_uuid":"41b04bc01e4a594b1df984d6fbbc526ac6ab16a5","scrolled":true},"cell_type":"code","source":"\"The code below will be hidden: \"\nhappiness_by_country = dataset_2015[['Country', 'Happiness Score']] \nhappiness_by_country = happiness_by_country.sort_values(by = ['Happiness Score'], ascending = False)\nhappiness_by_country.head() # Check if the dataframe is alright\n\n# happiness_by_country = happiness_by_country.sample(frac=1)\nhappiness_by_country = happiness_by_country.head(5)\nsns.barplot(x = happiness_by_country['Country'], y = happiness_by_country['Happiness Score'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2605e7b577c157839fa444645ddba3186b27fc2e"},"cell_type":"markdown","source":"## Data Preprocessing  \n To train the Machine Learning Model we need to provide the data  in a correct format. It is worth looking at the documentation of the specific ML model to understand how preprocess the data. In general, the steps we are going to go through prepare the data for the Regression models are :\n 1. **Choose variable which you want to predict and features to train the model on:**\n Simply speaking 'x' are the model features and 'y' is the output we are going to predict\n \n 1. **Split the dataset into train and test data:**\nOn the train data we can train model. Test data help us measure how accurate the model is. There are different accuracy metrics for regression models, for example **Mean Squared Error (MSE)** or **Mean Absolute Error (MAE)**. In this tutorial we will stick to **MSE** as it is a quite popular choice for regression models. More information about the accuracy metrics can be found in Sklearn [documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics).\n\n 1. **Scale/Normalise the data**:\nWe need to scale all of the numerical features so each of them have the same significance during the model training. For example, is feature x_1 has values of magnitude 1000 and feature x_2 of magnitude 10, x_2 won't contribute much to the training of the model. Awesome explanation can be found [here](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e).\n"},{"metadata":{"_uuid":"ba2fc74ec2ec303412b45308569fa1aceb685b3f"},"cell_type":"markdown","source":" 1. **Choose variable which you want to predict and features to train the model on:**"},{"metadata":{"trusted":true,"_uuid":"121a3eb0132b4a2b335b60d7c0e3334b29484e1e"},"cell_type":"code","source":"# Choosing variable to be predicted\ny = dataset_2015['Happiness Score'] # Happiness Score chosen as a value to be predicted","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8a37d93d0d20ff04af2cbb8eec68152e264fe86"},"cell_type":"markdown","source":"### **Your Turn**  \nChoose the features you want to include in your model. If the features are not numerical ( E.g. column 'Country' has a string value). There should be an additional preprocessing step (LabelEncoder and OneHotEncoder in Sklearn). I've uploaded the template in the [Github repo](https://github.com/KacperKubara/USAIS_Workshops/tree/master/Other) if you want to include the categorical data in your model.   \n ** Task for you: **  \n  **1. Choose the feature you want to train the model on. Use LabelEncoder and OneHotEncoder if you want to include the categorical data as well.**"},{"metadata":{"trusted":true,"_uuid":"8970949f18e120fab827d0dbbfbf794ea8d77d2b"},"cell_type":"code","source":"\"\"\"Code below will be hidden\"\"\"\nx = dataset_2015[['Economy (GDP per Capita)', 'Family',\n       'Health (Life Expectancy)']] # Include more features if you think that it might improve the models accuracy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"780612c29ead741859ff3ca3a62c4e8730a38d47"},"cell_type":"markdown","source":"2. **Split the dataset into train and test data:**"},{"metadata":{"trusted":true,"_uuid":"e287c348f6616ae63baff5c7c3547b2c61658a24"},"cell_type":"code","source":"# Split data into train and test dataset\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18663169ba2342687b22426b32caf5a16c94a297"},"cell_type":"markdown","source":" 3. **Scale/Normalise the data**:"},{"metadata":{"trusted":true,"_uuid":"628f485b4f1ac73c00005d09f632813797c470d0","scrolled":true},"cell_type":"code","source":"# Scale/Normalise the data\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nsc_y = StandardScaler()\n\n# Unscaled data - we will need it for Random Forest Regression and plotting the LinearRegression model\nx_train_unscaled = x_train.copy()\nx_test_unscaled  = x_test.copy()\ny_train_unscaled = y_train.copy()\n\n# Scale the data\nx_train = sc_x.fit_transform(x_train)\nx_test  = sc_x.transform(x_test)\ny_train = y_train.values.reshape(-1, 1)\ny_train = sc_y.fit_transform(y_train)\n\nprint('SCALED X_TRAIN:')\nprint(x_train[:5])\nprint('\\nSCALED Y_TRAIN:')\nprint(y_train[:5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f94de78b564d402ac55d721b1732812e483ca8ce"},"cell_type":"markdown","source":"## Linear Regression  \nLinear Regression is the simplest ML model. It is simply the best-fitting line for the dataset. Quick explanation can be found in our previous [presentation](https://www.beautiful.ai/deck/-LXYxxcxN4Hu0yzhjV6Y/AI-Society-Launch) (starts from the 10th slide): "},{"metadata":{"_uuid":"1e7ed5a99b8abab4b39294c17b6e8b2bdc213661"},"cell_type":"markdown","source":"### Train the model and predict results  \n "},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7bc7cd0961368d81386eee0e3c7da7a6022afc76"},"cell_type":"code","source":"# Train Model\nfrom sklearn.linear_model import LinearRegression\nregr = LinearRegression()\nregr.fit(x_train, y_train) # training the model - simple as that huh\n\n# Predict Results\ny_pred = regr.predict(x_test)\ny_pred = sc_y.inverse_transform(y_pred)\nprint('PREDICTED VALUES (UNSCALED): ')\nprint(y_pred[:5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3e9a4a2f2aa9a7bd6eb7b4d7be40b9ed66662e4"},"cell_type":"markdown","source":"### Measure Accuracy"},{"metadata":{"trusted":true,"_uuid":"18e7f3fe78f96eb42a540e144a088822b9f2993c"},"cell_type":"code","source":"# Measure Accuracy\nfrom sklearn.metrics import mean_squared_error\nacc = mean_squared_error(y_test, y_pred) # Mean Squared Error to measure the accuracy\nprint('ACCURACY(MSE): ')\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a94569778bb9b9cdd28b585a8947d7f1e8dc3de"},"cell_type":"markdown","source":"### **Your Turn**  \nYou can also visualise the results on 2D plot having one feature as 'X' and another one as 'Y'.\n ** Task for you: **  \n  **1. Visualise the results with a plot (e.g. line or scatter plot)**"},{"metadata":{"trusted":true,"_uuid":"a3440ce0c7d6259b388fef9743b7e9231019b3d3"},"cell_type":"code","source":"plt.title(\"Linear Regression Prediction\")\nplt.xlabel(\"Some Feature\")\nplt.ylabel(\"Happiness Score\")\n# Values in blue are those predicted by the model\nplt.scatter(x_test_unscaled['Family'], y_pred, color = 'b')\n# Values in red are orignal dataset points\nplt.scatter(x_test_unscaled['Family'], y_test, color = 'r') \n# Display graph\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47ccf19f89d070e25a1e3d23aa6e1d36a23b0bae"},"cell_type":"markdown","source":"## Polynomial Regression\n  Polynomial regression is quite similar to Linear regression. The only difference is that it will fit **polynomial function** instead of the **linear one**. In fact, you can see in code below that the code is still based on the **LinearRegression** Class"},{"metadata":{"_uuid":"54a06270f734e2067aa2c4ef4d9604933e94e07a"},"cell_type":"markdown","source":"### Train the model and predict results  "},{"metadata":{"trusted":true,"_uuid":"63b4ae1a2055e2e95c1c477fa1d92fc8ebb80ebc"},"cell_type":"code","source":"# Import libraries for the Polynomial Regression model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Creates separate variables for each of the polynomial degree of the feature \npoly_reg = PolynomialFeatures(degree = 3)\nx_poly_train = poly_reg.fit_transform(x_train)\nx_poly_test  = poly_reg.fit_transform(x_test)\npoly_reg.fit(x_poly_train, y_train)\n\n# Fit the polynomial features to the LinearRegression model\nlin_reg = LinearRegression()\nlin_reg.fit(x_poly_test, y_test)\n\n# Predict the result\ny_pred = lin_reg.predict(x_poly_test)\n\n# Unscale the data\ny_pred = sc_y.inverse_transform(y_pred)\nprint('PREDICTED VALUES (UNSCALED): ')\nprint(y_pred[:5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e3a897b0a43b7b4a2400b7b01d20754c2fe8c7f"},"cell_type":"markdown","source":"### Measure Accuracy"},{"metadata":{"trusted":true,"_uuid":"9306a694f36e6235a6417bd3694df8fee97e633b"},"cell_type":"code","source":"# Measure Accuracy\nfrom sklearn.metrics import mean_squared_error\nacc = mean_squared_error(y_test, y_pred)\n\nprint('ACCURACY(MSE): ')\nprint(acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ae63eba7ac8288178e2cc571a7975a6f12c6fd1"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"c26c47bd7bb4e92a76775f73f847f7809a3f986e"},"cell_type":"markdown","source":"## Random Forest  \n  \n"},{"metadata":{"_uuid":"9479e614a5fe425612d9f0948b21a0566a72775d"},"cell_type":"markdown","source":"Some description here"},{"metadata":{"_uuid":"c33110e6d8685b92be0d29f27f286c7c62e47e7a"},"cell_type":"markdown","source":"### **Your Turn**  \n ** Task for you: **  \n  **1. Create a Random Forest Regression Model and fit it into the dataset. **  \n  **1. Plot the results as in the linear regression example above **\n"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6d263d3e044ca0a1db2ceab601e2c8987d7652ae"},"cell_type":"code","source":"\"\"\"Code below will be hidden\"\"\"\n# Train Model\nfrom sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor(n_estimators = 10, random_state = 42)\n\n# For the RandomForest we don't have to scale the data\n# which is due to the different algorithm being used\n\n# Fit the model\nregr.fit(x_train_unscaled, y_train_unscaled)\n\n# Predict Results\ny_pred = regr.predict(x_test_unscaled)\n\n# Predict the values\nprint('PREDICTED VALUES (UNSCALED): ')\nprint(y_pred[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"209a4d6d0862ff5355676fc9a472148fd6952294"},"cell_type":"code","source":"# Measure Accuracy\nfrom sklearn.metrics import mean_squared_error\nacc = mean_squared_error(y_test, y_pred)\n\nprint('ACCURACY(MSE): ')\nprint(acc)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}