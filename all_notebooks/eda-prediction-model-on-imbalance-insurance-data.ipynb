{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## file handling libraries\nimport sys\nimport pandas as pd\nimport os \nimport numpy as np\n## importing libraries for\n## Vizualization\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n## loading datasets\ndataFrame = pd.read_csv('/kaggle/input/imbalanced-data-practice/aug_train.csv')\ntestFrame = pd.read_csv('/kaggle/input/imbalanced-data-practice/aug_test.csv')\n\n## plotting correlations\nplt.figure(figsize= (10, 7))\nsb.heatmap(dataFrame.corr(), annot = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## for the purpouse our Analysis lets divide the training data into\n## 2 parts : those who buy and those who does not buy the policy\n## and then see what causing this to happen.\n\nthoseWhoBought = dataFrame[dataFrame['Response'] == 1]\nthoseWhoDoesNotBought = dataFrame[dataFrame['Response'] == 0]\n\nplt.pie ([len(thoseWhoBought), len(thoseWhoDoesNotBought)], labels = ['those who bought Policy', \n                                                                      'those who doesnt bought Policy'],autopct='%1.2f%%')\n\n## only 16.38 % people from the survey bought \n## while 83.62 % does not\n\n## This means and model which says nobody bought\n## policy is 83.62 accuracte !!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets comapritively study bought of these categories\n## we start this by age distribution\nplt.figure(figsize = (14, 5))\n\nplt.subplot(1, 2, 1)\nsb.distplot(thoseWhoBought['Age'], kde = False)\nplt.title('Age distribution : those who bought')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nsb.distplot(thoseWhoDoesNotBought['Age'], kde = False)\nplt.title('Age dist those who doesnt bought')\nplt.grid(True)\n\n## This give us a key insigth that\n## that people in the early are not\n## as likely to buy insurance as those\n## in late years (30 - 55)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let see what role gender has to play\nplt.figure(figsize = (14, 5))\n\nplt.subplot(1, 2, 1)\nplt.pie([len (thoseWhoBought[thoseWhoBought['Gender'] == 'Male']),\n        len (thoseWhoBought[thoseWhoBought['Gender'] == 'Female'])], labels = ['Males', \n                                                                               'Females'],autopct='%1.2f%%')\nplt.title('Gender distribution : those who bought')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.pie([len(thoseWhoDoesNotBought[thoseWhoDoesNotBought['Gender']== 'Male']),\n        len(thoseWhoDoesNotBought[thoseWhoDoesNotBought['Gender']== 'Female'])],labels = ['Males', \n                                                                               'Females'],autopct='%1.2f%%' )\nplt.title('Gender distribution: those who doesnt bougth')\nplt.grid(True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## taking about driving lisence \nplt.figure(figsize = (14, 5))\n\nplt.subplot(1, 2, 1)\nplt.pie([len (thoseWhoBought[thoseWhoBought['Driving_License'] == 1]),\n        len (thoseWhoBought[thoseWhoBought['Driving_License'] == 0 ])], labels = ['have license', \n                                                                               'doesnt have lincese'],autopct='%1.2f%%')\nplt.title('Lincese distribution : those who bought')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.pie([len(thoseWhoDoesNotBought[thoseWhoDoesNotBought['Driving_License']== 1]),\n        len(thoseWhoDoesNotBought[thoseWhoDoesNotBought['Driving_License']== 0 ])],labels = ['have license', \n                                                                               'doesnt have license'],autopct='%1.2f%%' )\nplt.title('Lincese distribution: those who doesnt bougth')\nplt.grid(True)\n\n\n## This means almost every one has a lisence\n## Also it isn't easy to get insured without a\n## lisence (hence we may safely exclude this feature form our model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let see about regions\nplt.figure(figsize = (14, 5))\nplt.subplot(1, 2, 1)\nsb.distplot(thoseWhoBought['Region_Code'], kde = False)\nplt.title('Region_Code distribution : those who bought')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nsb.distplot(thoseWhoDoesNotBought['Region_Code'], kde = False)\nplt.title('Region_code: those who doesnt bought')\nplt.grid(True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## one Last thing before moving to model training part\n## let see premium distribution  \n\nplt.figure(figsize = (10, 5))\nplt.subplot(1, 2, 1)\nsb.distplot(thoseWhoBought['Annual_Premium'], kde = False)\nplt.title('Premium distribution : those who bought')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nsb.distplot(thoseWhoDoesNotBought['Annual_Premium'], kde = False)\nplt.title('Premium: those who doesnt bought')\nplt.grid(True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets preprocess data\n\n## features we are selecting\nfeaturesNeeded = ['Gender', 'Age', 'Region_Code', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', \n                 'Annual_Premium','Policy_Sales_Channel', 'Vintage' ]\n\n## features that are string must be mapped to int value\nfeatureNeedsToBeMapped = ['Gender', 'Vehicle_Age', 'Vehicle_Damage']\n\n## ids and labels for training\nlabelsForPrediction = testFrame['id']\ntrainingResponses = np.array (dataFrame['Response'], dtype = 'float64')\n\n\nfor feature in featureNeedsToBeMapped :\n    uvals = set(dataFrame[feature])\n    umap = {}\n    i = 0\n    \n    for val in uvals:\n        umap[val] = i\n        i += 1\n    \n    dataFrame[feature] = dataFrame[feature].map(umap)\n    testFrame[feature] = testFrame[feature].map(umap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainVector = np.array (dataFrame[featuresNeeded], dtype = 'float64')\ntestVector = np.array (testFrame[featuresNeeded], dtype = 'float64' )\n\n## let us standardize thes values \nfrom sklearn.preprocessing import StandardScaler\nstdsc = StandardScaler()\n\ntrainVector = stdsc.fit_transform(trainVector)\ntestVector = stdsc.transform(testVector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## before proceeding to model training lets \n## do dimentionality reduction and plot the \n## scatter of who have bought the policy and\n## who does not\n\nfrom sklearn.decomposition import PCA\npca  = PCA(n_components = 3)\npca.fit(trainVector)\n\ntransformedFeatures = pca.transform(trainVector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets make this a dataFrame\ndimReducedDataFrame = pd.DataFrame(transformedFeatures)\ndimReducedDataFrame = dimReducedDataFrame.rename(columns = { 0: 'V1', 1 : 'V2', 2 : 'V3'})\ndimReducedDataFrame['Response'] = trainingResponses\n\n## Plotting this\nplt.figure(figsize = (10, 5))\nsb.scatterplot(data = dimReducedDataFrame, x = 'V1', y = 'V2',hue = 'Response')\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let us split our dataset\nfrom sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(trainVector, trainingResponses, test_size = 0.1, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets try model stacking:\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport time\n\nestimators = [('gb1', GradientBoostingClassifier(n_estimators= 100) ),\n              ('gb2', GradientBoostingClassifier(n_estimators= 50) ),\n              ('rf1', RandomForestClassifier(n_estimators= 200) ),\n              ('rf2', RandomForestClassifier(n_estimators= 50) ),\n              ('dt1', DecisionTreeClassifier()),\n              ('bg1', BaggingClassifier())\n              \n             ]\n\nclf = StackingClassifier(\n    estimators=estimators, final_estimator = LogisticRegression())\n\n\nstartTime = time.perf_counter()\nclf.fit(xtrain, ytrain)\nendTime = time.perf_counter()\n\nprint ('Time taken to train : {} seconds'.format(endTime - startTime))\n## nearly 26 min needed to train stacked classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nprint (confusion_matrix(ytest, clf.predict(xtest)))\nprint (accuracy_score(ytest, clf.predict(xtest)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets just also try Knn classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 7)\nknn.fit(xtrain, ytrain)\n\n\nconfusion_matrix(ytest, knn.predict(xtest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## lets also train a neural network\n## this model has an accuracy of about 53 %\n## when predicting those who doesnt buy the policy\n## which is not that great !!! \nimport tensorflow as tf\nnum, featureSize = xtrain.shape\n\nmodel = tf.keras.models.Sequential([ tf.keras.layers.Dense(45, activation='relu',input_shape = (featureSize, )),\n                                 tf.keras.layers.Dense(30, activation='relu'),\n                                 tf.keras.layers.Dropout(0.2),tf.keras.layers.Dense(45, activation='relu'),\n                                 tf.keras.layers.Dense(1 ,activation='sigmoid')])\n\nmodel.compile(optimizer = tf.keras.optimizers.SGD(learning_rate= 0.3), loss = 'binary_crossentropy', metrics=['accuracy'])\nretObj = model.fit(xtrain, ytrain, validation_data=(xtest, ytest), epochs= 10, batch_size= 100) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def AnnPredictions(predVec, model) :\n  predictions = model.predict(predVec).flatten()\n  predictions[predictions < 0.5] = 0\n  predictions[predictions >= 0.5] = 1\n\n  return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(ytest, AnnPredictions(xtest, model))\n## imbalance prevail !!!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## but rember we do have pca clustered data which has only 3 dims\n## lets check that out\n\nnewTrainingVector = np.array(dimReducedDataFrame[['V1', 'V2', 'V3']], dtype = 'float64')\nnewLabelVector = np.array(dimReducedDataFrame['Response'], dtype = 'float64')\nstdc2 = StandardScaler()\n\nnewTrainingVector = stdc2.fit_transform(newTrainingVector)\n\nxtrainN, xtestN, ytrainN, ytestN = train_test_split(newTrainingVector, newLabelVector, random_state = 42, test_size = 0.1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num, featureSize = xtrainN.shape\n\nmodel = tf.keras.models.Sequential([ tf.keras.layers.Dense(45, activation='relu',input_shape = (featureSize, )),\n                                 tf.keras.layers.Dense(30, activation='relu'),\n                                 tf.keras.layers.Dropout(0.2),tf.keras.layers.Dense(45, activation='relu'),\n                                 tf.keras.layers.Dense(1 ,activation='sigmoid')])\n\nmodel.compile(optimizer = tf.keras.optimizers.SGD(learning_rate= 0.3), loss = 'binary_crossentropy', metrics=['accuracy'])\nretObj = model.fit(xtrainN, ytrainN, validation_data=(xtestN, ytestN), epochs= 10, batch_size= 100) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = clf.predict(testVector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## let also do pca clustering and visualize this\npca1  = PCA(n_components = 3)\npca1.fit(testVector)\n\ntransformedFeatures1 = pca1.transform(testVector)\ndimReducedDataFrame1 = pd.DataFrame(transformedFeatures1)\ndimReducedDataFrame1 = dimReducedDataFrame1.rename(columns = { 0: 'V1', 1 : 'V2', 2 : 'V3'})\ndimReducedDataFrame1['Response'] = predictions\n\n## Plotting this\nplt.figure(figsize = (10, 5))\nsb.scatterplot(data = dimReducedDataFrame1, x = 'V1', y = 'V2',hue = 'Response')\nplt.grid(True)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###########################################################################\n# work in progress                                                        #\n# To try underSampling, and oversampling                                  #\n###########################################################################","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}