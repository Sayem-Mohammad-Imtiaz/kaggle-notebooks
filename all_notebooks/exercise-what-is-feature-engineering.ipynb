{"cells":[{"cell_type":"markdown","metadata":{},"source":"**This notebook is an exercise in the [Feature Engineering](https://www.kaggle.com/learn/feature-engineering) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/what-is-feature-engineering).**\n\n---\n"},{"cell_type":"markdown","metadata":{},"source":"# The Ames Housing Dataset #\n\nIn the exercises you'll complete a feature engineering project with the [*Ames Housing*](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) dataset. The *House Prices Getting Started* bonus lesson reproduces this work on the same dataset used in our *House Prices* Getting Started competition. After completing this course, you'll have a starter notebook ready to extend with ideas of your own.\n\nIn this first exercise, you'll do a complete iteration of feature development: understand the dataset, create a baseline model, create a derived feature, and compare performance.\n\nRun this next cell to set everything up!"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.feature_engineering_new.ex1 import *\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n\ndef score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\", \"object\"]):\n        X[colname], _ = X[colname].factorize()\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n\n\ndf = pd.read_csv(\"../input/fe-course-data/ames.csv\")"},{"cell_type":"markdown","metadata":{},"source":"Here are five features from the *Ames* dataset."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"features = [\n    \"OverallQual\",\n    \"CentralAir\",\n    \"GrLivArea\",\n    \"Neighborhood\",\n    \"Fireplaces\",\n]\ndf[features].head(10)"},{"cell_type":"markdown","metadata":{},"source":"# 1) Determine Data Types\n\nCan you name the data type of each of these features? Record your answers as a `string`, one of: `\"continuous\"`, `\"discrete\"`, `\"ordinal\"`, `\"nominal\"`, or `\"binary\"`."},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":0},"outputs":[],"source":"# YOUR CODE HERE: Enter your answer as a string, like:\n# feature = \"continuous\"\noverall_qual = ____\ncentral_air = ____\ngr_liv_area = ____\nneighborhood = ____\nfireplaces = ____\n\n\n# Check your answer\nq_1.check()"},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":0},"outputs":[],"source":"# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()"},{"cell_type":"markdown","metadata":{},"source":"Often, the author of a dataset which will tell you the intended types and values of each feature in the dataset's documentation. You can see the author's documentation of this dataset by running the cell below:"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# Uncomment and run to see data documentation\n!cat \"../input/fe-course-data/DataDocumentation.txt\""},{"cell_type":"markdown","metadata":{},"source":"-------------------------------------------------------------------------------\n\nEffective feature engineering makes use of prominent relationships in the dataset. Data visualization is one of the best ways to discover these relationships. Now you'll use Seaborn to discover some important things about the *Ames* data. (Check our our [Data Visualization](https://www.kaggle.com/learn/data-visualization) course, too!)\n\nYou can see the relationship a feature has to the target with a *scatterplot*. Take a look at scatterplots for `YearBuilt` and `MoSold` relative to `SalePrice`:"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"fig, axs = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\naxs[0] = sns.scatterplot(x=\"YearBuilt\", y=\"SalePrice\", data=df, ax=axs[0])\naxs[1] = sns.scatterplot(x=\"MoSold\", y=\"SalePrice\", data=df, ax=axs[1])"},{"cell_type":"markdown","metadata":{},"source":"# 2) Discover Relationships\n\nDoes there appear to be a significant relationship between either feature and the target? If so, does the relationship appear to be linear (best fit with a line)?\n\nAfter you've thought about your answer, run the following cell for a solution."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# View the solution (Run this cell to receive credit!)\nq_2.check()"},{"cell_type":"markdown","metadata":{},"source":"-------------------------------------------------------------------------------\n\nThe number of bathrooms in a home is often important to prospective home-buyers. The *Ames* data contains four such features:\n- `FullBath`\n- `HalfBath`\n- `BsmtFullBath`\n- `BsmtHalfBath`\n\n# 3) Create a New Feature\n\nCreate a new feature `TotalBaths` that describes the *total* number of bathrooms in a home. There are a couple answers that might be reasonable. Can you think of a way that's better than just summing the features up?"},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":0},"outputs":[],"source":"X = df.copy()\ny = X.pop('SalePrice')\n\n\n# YOUR CODE HERE\nX[\"TotalBaths\"] = ____\n\n\n# Check your answer\nq_3.check()"},{"cell_type":"code","execution_count":null,"metadata":{"lines_to_next_cell":0},"outputs":[],"source":"# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()"},{"cell_type":"markdown","metadata":{},"source":"-------------------------------------------------------------------------------\n\nNow compare the performance of XGBoost on *Ames* with and without the `TotalBaths` feature. (The `score_dataset` function performs 5-fold cross-validation with XGBoost using with the RMSLE metric, the same metric used in the *House Prices* competition.)"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"X_base = df.copy()\ny_base = X_base.pop(\"SalePrice\")\n\nbaseline_score = score_dataset(X_base, y_base)\nnew_score = score_dataset(X, y)\n\nprint(f\"Score Without New Feature: {baseline_score:.4f} RMSLE\")\nprint(f\"Score With New Feature: {new_score:.4f} RMSLE\")"},{"cell_type":"markdown","metadata":{},"source":"# 4) Feature Selection\n\nBased on the performance of XGBoost with and without the additional feature, would you decide to keep or discard the new feature? Or is there not enough information to decide?\n\nAfter you've thought about you're answer, run the next cell for the solution."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# View the solution (Run this cell to receive credit!)\nq_4.check()"},{"cell_type":"markdown","metadata":{},"source":"# Iterating on Feature Sets #\n\nYou've just worked through a complete iteration of feature development: discovery, creation, validation, and selection. In most machine learning projects, you'll likely go through many such iterations before arriving at your final, best feature set.\n\nIn the next lesson, you'll learn about *feature utility*, a way of scoring features for their potential usefulness -- a big help when you're just getting started with a new dataset!\n\n# Keep Going #\n\n[**Discover useful features**](https://www.kaggle.com/ryanholbrook/mutual-information) with mutual information."},{"cell_type":"markdown","metadata":{},"source":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/) to chat with other Learners.*"}],"metadata":{"jupytext":{"cell_metadata_filter":"-all","formats":"ipynb"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":4}