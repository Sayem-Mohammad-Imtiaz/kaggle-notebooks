{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries:","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# for preprocessing the data:\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom sklearn.model_selection import train_test_split\n\n# importing the neural network libraries:\nfrom keras.optimizers import *\nfrom keras.losses import binary_crossentropy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# importing a classifier from xgboost:\nfrom xgboost import XGBClassifier\n\n# importing metrics to measure our accuracy:\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing the Data:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Reading the data:","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/tictactoe/tic-tac-toe.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking if there are any NaN values:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Defining the labels to predict on:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['class']\ndata.drop(['class'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Encoding the labels or converting them to numerical form since categorical values cannot be used in algorithms:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"label = LabelEncoder()\n\ny = label.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cbe = CatBoostEncoder()\ndata = cbe.fit_transform(data, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dividing the dataset into training and test sets:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test, ytrain, ytest = train_test_split(data, y,\n                                              test_size=0.4, train_size=0.6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and Predicting:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    Dense(256, activation='relu', input_shape=(9,)),\n    Dense(128, activation='relu'),\n    Dense(128, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(8, activation='relu'),\n    Dense(1, activation='sigmoid')\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(metrics=['accuracy'], loss='binary_crossentropy', optimizer='Adam')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train, ytrain, epochs=40,\n          validation_data=(test, ytest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Concluding with fitting the neural network, I can say that it is good but we can do better since in my opinion a 1000 examples aren't exactly enough to train a plain artificial neural network with and get tinkerable results (pardon me if I am wrong I am new to this too!). The accuracy we got is almost 95% on the training set while the accuracy on the test set is 96% . ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets try predicting with XGBoost Classifier now:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xg = XGBClassifier(n_estimators=350)\n\nxg.fit(train, ytrain)\n\nxgPreds = xg.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(xgPreds, ytest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think that 98% accuracy on a classifier is descent enough! (pun intended)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}