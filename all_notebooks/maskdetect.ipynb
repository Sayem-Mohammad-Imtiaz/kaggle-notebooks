{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Face Mask Detection**\n\nIn this notebook I am going to detect the face covering on a person. I have a dataset of 20 different labels like face_with_mask, no_mask, hijab, helmet, sunglasss, hat, etc."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport os\nimport tensorflow as tf\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_path=os.path.join(\"/kaggle/input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/images\")\nanno_path=os.path.join(\"/kaggle/input/face-mask-detection-dataset/Medical mask/Medical mask/Medical Mask/annotations\")\ntrain_df = pd.read_csv('../input/face-mask-detection-dataset/train.csv')\nsubmission = pd.read_csv('../input/face-mask-detection-dataset/submission.csv')\nprint(len(os.listdir(img_path)))\nprint(len(os.listdir(anno_path)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sorting images based on indexes\nimages=os.listdir(img_path)\nannot=os.listdir(anno_path)\nimages.sort()\nannot.sort()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Train/Test Split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images=images[1698:]\ntest_images=images[:1698]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_images)+len(test_images)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us visualize a few examples!"},{"metadata":{"trusted":true},"cell_type":"code","source":"img=plt.imread(os.path.join(img_path,train_images[3]))\nplt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img=plt.imread(os.path.join(img_path,train_images[10]))\nplt.imshow(img)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['name'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Total classes\ntrain_df['classname'].nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Listing all different categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['classname'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.catplot(x='classname',kind='count',data=train_df,orient=\"h\",height=10,aspect=2)\nax.fig.suptitle('Count of Classnames',fontsize=16,color=\"r\")\nax.fig.autofmt_xdate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This shows a single picture might contain more than one person"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['name'].value_counts().max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us find and plot this image"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby(['name']).count().sort_values(['x1'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us define a function to draw boxes around the faces"},{"metadata":{"trusted":true},"cell_type":"code","source":"box=[]\nfor i in range(len(train_df)):\n    arr=[]\n    for j in train_df.iloc[i][['x1','x2','y1','y2']]:\n        arr.append(j)\n    box.append(arr)\ntrain_df['box']=box","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_boxes(id):\n    boxes=[]\n    for i in train_df[train_df[\"name\"]==str(id)][\"box\"]:\n        boxes.append(i)\n    return boxes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(get_boxes(train_images[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image = '1914.jpg'\nimg=plt.imread(os.path.join(img_path, image))\nfig, ax = plt.subplots(1)\nax.imshow(img)\nboxes=get_boxes(image)\nfor box in boxes:\n    rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],linewidth=2,edgecolor='r',facecolor='none')\n    ax.add_patch(rect)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Preparation**\n\nCropping the images to the get face images. These facial shots will be used for model training for the classification task"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_size=50\nx=[]\ny=[]\nfor i in range(len(train_df)):\n    arr=[]\n    for item in train_df.iloc[i]:\n        arr.append(item)\n    img = cv2.imread(os.path.join(img_path, arr[0]), cv2.IMREAD_GRAYSCALE)\n    img = img[arr[2]:arr[4], arr[1]:arr[3]]\n    try:\n        img = cv2.resize(img, (img_size, img_size))\n    except:\n        continue\n    x.append(img)\n    y.append(arr[5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with_mask =['face_with_mask']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a seperate copy of data\nx_mask=[]\ny_mask=[]\nfor i in range(len(x)):\n    x_mask.append(x[i])\n    y_mask.append(y[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding the labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlr = LabelEncoder()\ny_mask = lr.fit_transform(y_mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalizing the data\nx_mask=tf.keras.utils.normalize(x_mask,axis=1)\nx_mask=np.array(x_mask).reshape(-1,50,50,1)\nfrom keras.utils import to_categorical\ny_mask = to_categorical(y_mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.shape(x_mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Input, Dropout,Flatten, Conv2D\nfrom tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D, LeakyReLU\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.constraints import UnitNorm\n\nfrom IPython.display import SVG, Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val,Y_train,Y_val=train_test_split(x_mask, y_mask,train_size=0.95,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 32\ndatagen_train = ImageDataGenerator(horizontal_flip=True, rotation_range=20,brightness_range=(0., 2.), shear_range=40)\n\ntrain_generator = datagen_train.flow(X_mask, Y_mask, batch_size=batch_size, shuffle=True)\n#val_generator = datagen_train.flow(X_val, Y_val,batch_size=batch_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\n# 1 - Convolution\nmodel.add(Conv2D(64,(3,3), padding='same', input_shape=(50,50,1)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 2nd Convolution layer\nmodel.add(Conv2D(128,(5,5), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 3rd Convolution layer\nmodel.add(Conv2D(256,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 4th Convolution layer\nmodel.add(Conv2D(512,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(1,1)))\n\n# 5th Convolution layer\nmodel.add(Conv2D(1024,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# 6th Convolution layer\nmodel.add(Conv2D(2048,(3,3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flattening\nmodel.add(Flatten())\n\n# Fully connected layer 1st layer\nmodel.add(Dense(1024))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\n# Fully connected layer 2nd layer\nmodel.add(Dense(512))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(20, activation='softmax'))\n\nopt = Adam(lr=0.0005)\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install visualkeras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import visualkeras\nvisualkeras.layered_view(model, to_file='output.png').show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 70\nsteps_per_epoch = train_generator.n//train_generator.batch_size\n\ncallbacks=[ReduceLROnPlateau(monitor='train_loss', patience=2, verbose=1), \n           ]\n\nhistory = model.fit(train_generator,steps_per_epoch=steps_per_epoch, \n                    epochs=epochs, callbacks=[callbacks])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train/Val loss plot"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n\naxes[0].plot(history.history['loss'], label='train')\n#axes[0].plot(history.history['val_loss'], label='val')\naxes[0].set_title('loss')\naxes[0].legend()\naxes[1].plot(history.history['accuracy'], label='train')\n#axes[1].plot(history.history['val_accuracy'], label='val')\naxes[1].set_title('accuracy')\naxes[1].legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install mtcnn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To detect and localize the facial images from the input images we will be using the MTCNN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from mtcnn.mtcnn import MTCNN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(pic):\n    cvNet = cv2.dnn.readNetFromCaffe('../input/caffepretrainedfacemodel/architecture.txt',\n                                     '../input/caffepretrainedfacemodel/weights.caffemodel')\n    #detector=MTCNN()\n    img = plt.imread(pic)\n    inp=cv2.imread(pic, cv2.IMREAD_GRAYSCALE)\n    (h, w) = inp.shape[:2]\n    blob = cv2.dnn.blobFromImage(cv2.resize(img.copy(), (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n    cvNet.setInput(blob)\n    detections = cvNet.forward()\n    for i in range(0, detections.shape[2]):\n        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n        (startX, startY, endX, endY) = box.astype(\"int\")\n        frame = inp[startY:endY, startX:endX]\n        try:\n            im = cv2.resize(frame,(50,50))\n        except:\n            continue\n        confidence = detections[0, 0, i, 2]\n        if confidence > 0.8:\n            cv2.rectangle(img, (startX, startY), (endX, endY), (0, 255, 0), 2)\n            im = im.reshape(-1,50,50,1)\n            pred=model.predict(im)\n            pred=np.argmax(pred)\n            if pred==14 or pred==15:\n                pred=5\n            elif pred==3:\n                pred=2\n            pred=lr.inverse_transform([pred])[0]\n\n    return img,pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax=plt.subplots(1,5,figsize=(20,20))\na=361\nfor i in range(a,366):\n    img,pred=predict(os.path.join(img_path, test_images[i]))\n    ax[i-a].imshow(img)\n    ax[i-a].set_title(pred)\n    ax[i-a].set_yticklabels([])\n    ax[i-a].set_xticklabels([])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d={}\nd['Class']=lr.classes_\nd['Probability']=a.flatten()\nd=pd.DataFrame.from_dict(d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='Class', y='Probability', data=d)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Model\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\nactivations = activation_model.predict(x_mask[23].reshape(1,50,50,1))\n \ndef display_activation(activations, col_size, row_size, act_index): \n    activation = activations[act_index]\n    activation_index=0\n    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*2.5,col_size*1.5))\n    for row in range(0,row_size):\n        for col in range(0,col_size):\n            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n            ax[row][col].set_yticklabels([])\n            ax[row][col].set_xticklabels([])\n            activation_index += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_activation(activations, 5, 5, 19)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(x[16])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(model.layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax=plt.subplots(1,20,figsize=(15,10))\nfor i in range(10,30):\n    ax[i-10].imshow(x_mask[i],cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.classes_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}