{"cells":[{"metadata":{},"cell_type":"markdown","source":" ## Breast Cancer\n"},{"metadata":{},"cell_type":"markdown","source":"<a href=\"https://ibb.co/f2WVf3d\"><img src=\"https://i.ibb.co/TkGD6QM/images.jpg\" alt=\"images\" border=\"0\"></a>"},{"metadata":{},"cell_type":"markdown","source":"<font color='red'> <br>\n\n* [A. Problem Understanding](#1)\n    * [Data Describtion](#2)\n    * [Exploratory Data Analysis(EDA)](#3)    \n* [B. Logistic Regression](#4)\n    * [Creating Parameters](#5)\n    * [Forward and Backward Propagation](#6)\n    * [Implementing Update Parameters](#7)\n    * [Prediction Parameter](#8)\n* [C. Logistec Regression with Sklearn](#9)\n   \n  [CONCLUSION](#10)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n## A. Problem Understanding\n\nDespite a great deal of public awareness and scientific research, breast cancer continues to be the most common cancer and the second largest cause of cancer deaths among women. Approximately 12% of U.S. women will be diagnosed with breast cancer, and 3.5% will die of it. The annual mortality rate of approximately 28 deaths per 100,000 women has remained nearly constant over the past 20 years. "},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n## Data Describtion"},{"metadata":{},"cell_type":"markdown","source":"*  ID number\n*  Diagnosis (M = malignant, B = benign)\n*  Ten real-valued features are computed for each cell nucleus:\n*  radius (mean of distances from center to points on the perimeter)\n*  texture (standard deviation of gray-scale values)\n*  perimeter\n*  area\n*  smoothness (local variation in radius lengths)\n*  compactness (perimeter^2 / area - 1.0)\n*  concavity (severity of concave portions of the contour)\n*  concave points (number of concave portions of the contour)\n*  symmetry\n*  fractal dimension (\"coastline approximation\" - 1)\n Note: Mean, Etandard Error (SE) and Worst (mean of the three largest values) of these features are obtained from each image, resulting in 30 features. For example, the third column is Mean Radius, column 13 is Radius SE, column 23 is Worst Radius. All feature values are stored with four significant numbers."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/breast-cancer/breast-cancer.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['id','Unnamed: 32'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a> <br>\nExploratory Data Analysis(EDA)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2= data[['diagnosis','radius_mean','texture_mean','perimeter_mean','area_mean',\n                 'smoothness_mean','area_worst','concavity_mean','concave points_mean',\n                 'symmetry_mean','fractal_dimension_mean']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"color_list = ['cyan' if i=='M' else 'lime' for i in data2.loc[:,'diagnosis']]\npd.plotting.scatter_matrix(data2.loc[:, data2.columns != 'diagnosis'],\n                           c=color_list,\n                           figsize= [20,20],\n                           diagonal='hist',\n                           alpha=0.5,\n                           s = 200,\n                           marker = '*',\n                           edgecolor= \"black\")\n                                        \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Values of 'Benign' and 'Malignant' cancer cells\ndata.diagnosis.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualization\nsns.countplot(x=\"diagnosis\", data=data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.jointplot(data.radius_mean, data.smoothness_mean, kind=\"kde\", size=7)\nplt.savefig('graph.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,25))\nsns.heatmap(data.corr(),annot=True,cmap='RdBu_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n## B. Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"<a href=\"http://ibb.co/c574qx\"><img src=\"http://preview.ibb.co/cxP63H/5.jpg\" alt=\"5\" border=\"0\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\nprint(data.info())\n#Firstly M and B values update 0 and zero. Because not using string.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalization\n#This is a formul>>   (x - min(x))/(max(x)-min(x))\nx = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape\n# change value (matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# %30 testing %70 training ///  random constant=42\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a> <br>\n## Creating Parameters"},{"metadata":{},"cell_type":"markdown","source":"* Parameters are weight and bias.\n* Weights: coefficients of each pixels\n* Bias: intercept\n* z = (w.t)x + b => z equals to (transpose of weights times input x) + bias\n* y_head = sigmoid(z)\n* Sigmoid function makes z between zero and one so that is probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"# dimension =feauture values\n\ndef initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0  #float values\n    return w,b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"look = np.full((6,1),0.01)\nprint(look)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sigmoid formule and graphic\n\n<a href=\"https://ibb.co/Njcnpqj\"><img src=\"https://i.ibb.co/jMnZGYM/images.png\" alt=\"images\" border=\"0\"></a>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's calculating z\n# z = np.dot(w.T,x_train)+b\n\ndef sigmoid(z):\n    \n    y_head = 1/(1+ np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sigmoid(-6))\nprint(sigmoid(0))\nprint(sigmoid(6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a> <br>\n## Forward and Backward Propagation"},{"metadata":{},"cell_type":"markdown","source":"Now if our cost is going to be a mistake. we must create backward and forward propagation."},{"metadata":{},"cell_type":"markdown","source":"Loss formule\n<a href=\"https://imgbb.com/\"><img src=\"https://image.ibb.co/eC0JCK/duzeltme.jpg\" alt=\"duzeltme\" border=\"0\"></a>"},{"metadata":{},"cell_type":"markdown","source":"Cost\n<a href=\"http://imgbb.com/\"><img src=\"http://image.ibb.co/dAaYJH/7.jpg\" alt=\"7\" border=\"0\"></a>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape[1] #for scaling","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_backward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]  \n    \n    # backward propagation\n    #weight turev\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] \n    #bias turev\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]           \n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"7\"></a> <br>\n## Implementing Update Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <a id=\"8\"></a> <br>\n## Prediction Parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 30\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 3, num_iterations = 300)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a> <br>\n## C.Logistec Regression with Sklearn\n\nWith the Sklearn library, we can find the result you found above in a much easier way."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a> <br>\n\n##  CONCLUSION"},{"metadata":{},"cell_type":"markdown","source":"Thank you for your votes and comments\n\nIf you have any suggest, May you write for me, I will be happy to hear it."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}