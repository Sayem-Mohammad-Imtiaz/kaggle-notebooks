{"cells":[{"metadata":{"id":"CazISR8X_HUG"},"cell_type":"markdown","source":"# 1.3 Multiple Linear Regression & Backward Elimination.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For better understanding of current noebook for beginners go through the links:\n\n [1.1 Data Preprocessing](http://www.kaggle.com/saikrishna20/data-preprocessing-tools)\n\n\n[1.2 Simple linear Regression](https://www.kaggle.com/saikrishna20/1-2-simple-linear-regression) \n\nIt basically tells u about the preprocessing & Linear Regression which will help u in understanding this notebook better","execution_count":null},{"metadata":{"id":"pOyqYHTk_Q57"},"cell_type":"markdown","source":"## Importing the libraries","execution_count":null},{"metadata":{"id":"T_YHJjnD_Tja","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"id":"vgC61-ah_WIz"},"cell_type":"markdown","source":"## Importing the dataset","execution_count":null},{"metadata":{"id":"UrxyEKGn_ez7","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/50-startups/50_Startups.csv')\nX = dataset.iloc[:, :-1].values # features\ny = dataset.iloc[:, -1].values # target","execution_count":null,"outputs":[]},{"metadata":{"id":"GOB3QhV9B5kD","outputId":"f26c2ce7-cbb1-48e9-ec41-0a9bbcc3722a","executionInfo":{"status":"ok","timestamp":1592297595101,"user_tz":-330,"elapsed":1690,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"X[:5]","execution_count":null,"outputs":[]},{"metadata":{"id":"VadrvE7s_lS9"},"cell_type":"markdown","source":"## Encoding categorical data","execution_count":null},{"metadata":{"id":"wV3fD1mbAvsh","trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(drop='first'), [3])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))\n# we don't need to del the one dummy varible column in the dataframe for linear regression\n# but for other model we need to use n-1 dummy columns if there are n unique values in a particular column\n# hence to avoid confussion we can del the first column of dummy columns created. ","execution_count":null,"outputs":[]},{"metadata":{"id":"4ym3HdYeCGYG","outputId":"ae256b74-ece8-4a7e-c524-902f2d3b52e5","executionInfo":{"status":"ok","timestamp":1592297596101,"user_tz":-330,"elapsed":2639,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"print(X)\n# The dummy variables are always created in the first columns.","execution_count":null,"outputs":[]},{"metadata":{"id":"WemVnqgeA70k"},"cell_type":"markdown","source":"## Splitting the dataset into the Training set and Test set","execution_count":null},{"metadata":{"id":"Kb_v_ae-A-20","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"id":"k-McZVsQBINc"},"cell_type":"markdown","source":"## Training the Multiple Linear Regression model on the Training set","execution_count":null},{"metadata":{"id":"ywPjx0L1BMiD","outputId":"1666846b-80aa-4c7c-dc09-b187ccb31dca","executionInfo":{"status":"ok","timestamp":1592297596102,"user_tz":-330,"elapsed":2622,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"xNkXL1YQBiBT"},"cell_type":"markdown","source":"## Predicting the Test set results","execution_count":null},{"metadata":{"id":"VsGFojgaSCIZ","trusted":true},"cell_type":"code","source":"y_pred = regressor.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"SiBZXENISF2g","outputId":"21d1c262-a3b6-45d4-f376-0e104b5d44c6","executionInfo":{"status":"ok","timestamp":1592297596104,"user_tz":-330,"elapsed":2595,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"id":"Oo2qxpRRSKh1"},"cell_type":"markdown","source":"y_pred is an numpy array of one row","execution_count":null},{"metadata":{"id":"TQKmwvtdBkyb","outputId":"10e6ce3d-971a-47e4-cbee-52b849635d38","executionInfo":{"status":"ok","timestamp":1592297596105,"user_tz":-330,"elapsed":2584,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"np.set_printoptions(precision=2) # only two decimals after point\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","execution_count":null,"outputs":[]},{"metadata":{"id":"6rHtIbRMSZj7"},"cell_type":"markdown","source":"In the output above we have all the predicted values from model on the left side and the real values on the right side. ","execution_count":null},{"metadata":{"id":"SeL8kbZzqoja","outputId":"19a6c58c-baba-4869-a27c-05bcdec6a360","executionInfo":{"status":"ok","timestamp":1592297596108,"user_tz":-330,"elapsed":2570,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"id":"hY7BujuISxX_"},"cell_type":"markdown","source":"# Significance level - Backward elimination\n\nWe have different techniques to find out the features which have the maximum effect on the output.\n\nHere we are going to look at the Backward elimination.\n\nIn this process we need to add one column of ones in the starting of the column.\n\nIn backward elimination we delete the value one by one whose significance level is less.\n\ni.e In general we have a P-value and a significance level\n\nP_value = 1 - (minus) significane level\n\nor in other terms\n\np_value+ significance level = 1\n\nif P_value is high significance level is less.\n\nHence we will be deleating features one by one whose P_value is high which means it has less significance level.\n\nBy eliminating process we get to the values which are of most significance","execution_count":null},{"metadata":{"id":"rof_OzIqpSGc","outputId":"3c32ddfe-5a0a-4a72-f1dc-3ba24a02346c","executionInfo":{"status":"ok","timestamp":1592297596109,"user_tz":-330,"elapsed":2558,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"# Building the optimal model using Backward Elimination\nimport statsmodels.api as sm\nX = np.append(arr = np.ones((50, 1)).astype(float), values = X, axis = 1)\nprint(X)\nX_opt = np.array(X[:, [0, 1, 2, 3, 4, 5]], dtype=float)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"b6EMvqLGscIV","outputId":"a6fcf6c2-eac1-44df-d52d-041901b9b3af","executionInfo":{"status":"ok","timestamp":1592297596110,"user_tz":-330,"elapsed":2545,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"X_opt","execution_count":null,"outputs":[]},{"metadata":{"id":"BOtufJk5026q"},"cell_type":"markdown","source":"the varibale whose p value is greater of all and is more than significance level 0.05 is deleted as it means it has less significance on the outcome.","execution_count":null},{"metadata":{"id":"_iPawXL6sTWf","outputId":"425b1336-0f46-4dcb-93ba-0f933a71b8b6","executionInfo":{"status":"ok","timestamp":1592297596111,"user_tz":-330,"elapsed":2537,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"model = sm.OLS(endog = y, exog = X_opt)\nregressor_OLS = model.fit()\nregressor_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"kVyJQduzWLiH"},"cell_type":"markdown","source":"**R squared – It tells about the goodness of the fit. It ranges between 0 and 1. The closer the value to 1, the better it is. It explains the extent of variation of the dependent variables in the model. However, it is biased in a way that it never decreases(even on adding variables).**\n\n\n**Adj Rsquared – This parameter has a penalising factor(the no. of regressors) and it always decreases or stays identical to the previous value as the number of independent variables increases. If its value keeps increasing on removing the unnecessary parameters go ahead with the model or stop and revert.**\n\n\n**F statistic – It is used to compare two variances and is always greater than 0. It is formulated as v12/v22. In regression, it is the ratio of the explained to the unexplained variance of the model.\nAIC and BIC – AIC stands for Akaike’s information criterion and BIC stands for Bayesian information criterion Both these parameters depend on the likelihood function L.**\n\n\n**Skew – Informs about the data symmetry about the mean.**\n\n\n**Kurtosis – It measures the shape of the distribution i.e.the amount of data close to the mean than far away from the mean.**\n\n\n**Omnibus – D’Angostino’s test. It provides a combined statistical test for the presence of skewness and kurtosis.**\n\n\n**Log-likelihood – It is the log of the likelihood function.**","execution_count":null},{"metadata":{"id":"0vxtmtJjpSWA","outputId":"f543eb15-1858-4e9a-ff5d-6e1a4562fb89","executionInfo":{"status":"ok","timestamp":1592297596111,"user_tz":-330,"elapsed":2526,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"X_opt = X[:, [0, 1, 3, 4, 5]]\nX_opt = np.array(X[:, [0, 1, 3, 4, 5]], dtype=float)\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"kHj99avDpSmj","outputId":"f0c8916b-c24d-495b-afa4-b35badd5c727","executionInfo":{"status":"ok","timestamp":1592297596112,"user_tz":-330,"elapsed":2516,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"X_opt = np.array(X[:, [0,3, 4, 5]], dtype=float)\n#X_opt = X[:, [0, 3, 4, 5]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"eDP6Nf88ps3W","outputId":"5b9e8df8-dbf4-437a-fb4f-780efe230254","executionInfo":{"status":"ok","timestamp":1592297596112,"user_tz":-330,"elapsed":2507,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"X_opt = np.array(X[:, [0, 3, 5]], dtype=float)\n#X_opt = X[:, [0, 3, 5]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"9G5XXBzdpvJM","outputId":"d0f70af9-1853-4217-aaf1-5815fb6f22a0","executionInfo":{"status":"ok","timestamp":1592297596113,"user_tz":-330,"elapsed":2497,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"\n\nX_opt = np.array(X[:, [0,3]], dtype=float)\n#X_opt = X[:, [0, 3]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nregressor_OLS.summary()","execution_count":null,"outputs":[]},{"metadata":{"id":"5p136WrJFBLj"},"cell_type":"markdown","source":"### Making a single prediction (for example the profit of a startup with R&D Spend = 160000, Administration Spend = 130000, Marketing Spend = 300000 and State = 'California')","execution_count":null},{"metadata":{"id":"p7mdgRSMFsqH","outputId":"525d3302-c588-4618-ef0c-a0792ab37b9e","executionInfo":{"status":"ok","timestamp":1592297596113,"user_tz":-330,"elapsed":2484,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"print(regressor.predict([[ 0, 0, 160000, 130000, 300000]]))","execution_count":null,"outputs":[]},{"metadata":{"id":"TXf9NlRFGlht"},"cell_type":"markdown","source":"Therefore, our model predicts that the profit of a Californian startup which spent 160000 in R&D, 130000 in Administration and 300000 in Marketing is $ 181566,92.\n\n**Important note 1:** Notice that the values of the features were all input in a double pair of square brackets. That's because the \"predict\" method always expects a 2D array as the format of its inputs. And putting our values into a double pair of square brackets makes the input exactly a 2D array. Simply put:\n\n$1, 0, 0, 160000, 130000, 300000 \\rightarrow \\textrm{scalars}$\n\n$[1, 0, 0, 160000, 130000, 300000] \\rightarrow \\textrm{1D array}$\n\n$[[1, 0, 0, 160000, 130000, 300000]] \\rightarrow \\textrm{2D array}$\n\n**Important note 2:** Notice also that the \"California\" state was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the second row of the matrix of features X, \"California\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, not the last three ones, because the dummy variables are always created in the first columns.","execution_count":null},{"metadata":{"id":"soirA4ugKRTL"},"cell_type":"markdown","source":"## Getting the final linear regression equation with the values of the coefficients","execution_count":null},{"metadata":{"id":"HtuCxTkwKVPa","outputId":"4a10e650-73c8-41f5-969b-da45d4f3a320","executionInfo":{"status":"ok","timestamp":1592297596114,"user_tz":-330,"elapsed":2476,"user":{"displayName":"Sai Krishna","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj7otYF7KfPHoQ27NTuCcjxev75Gpq4J5DwGTw3Lw=s64","userId":"14839971739298272557"}},"trusted":true},"cell_type":"code","source":"print(regressor.coef_)\nprint(regressor.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"id":"mMXYYf3IKcnk"},"cell_type":"markdown","source":"Therefore, the equation of our multiple linear regression model is:\n\n$$\\textrm{Profit} = 86.6 \\times \\textrm{Dummy State 1} - 873 \\times \\textrm{Dummy State 2} + 786 \\times \\textrm{Dummy State 3} - 0.773 \\times \\textrm{R&D Spend} + 0.0329 \\times \\textrm{Administration} + 0.0366 \\times \\textrm{Marketing Spend} + 42467.53$$\n\n**Important Note:** To get these coefficients we called the \"coef_\" and \"intercept_\" attributes from our regressor object. Attributes in Python are different than methods and usually return a simple value or an array of values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Like this notebook then upvote it.\n\n\n# Need to improve it then comment below.\n\n\n# * Enjoy Machine Learning.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}