{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Here is a proposal of a model in order to classify images of rail, between 2 classes, with insulation joint or without. This is my first CNN model and I am interested in all kinds of feedback.","metadata":{}},{"cell_type":"markdown","source":"Insulation joints separate track circuits which aim at detecting and then locating the trains on the network. They play a key role to ensure safety when trains operate to prevent collision.","metadata":{}},{"cell_type":"markdown","source":"# Packages","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport tqdm\nfrom typing import Tuple\n\nimport numpy as np\nimport pandas as pd\n\nimport cv2\nimport matplotlib.pyplot as plot\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\n\nfrom IPython.display import SVG","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:32:28.936927Z","iopub.execute_input":"2021-07-17T08:32:28.937326Z","iopub.status.idle":"2021-07-17T08:32:34.571894Z","shell.execute_reply.started":"2021-07-17T08:32:28.937277Z","shell.execute_reply":"2021-07-17T08:32:34.571112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parameters\n\nAll the images do not have the same size. There are 2 different sizes. Inasmuch as the 2 sizes are quite big, a new image shape has been chosen, smaller than the 2 initial shapes, but not too small to keep enough details in the images (in particular the joints).","metadata":{}},{"cell_type":"code","source":"image_dir_path = \"/kaggle/input/insulation-joint-training-set-prorail/trainset_insulation_joint/images/\"\nlabel_csv_path = \"/kaggle/input/insulation-joint-training-set-prorail/trainset_insulation_joint/labels.csv\"\n\nimage_shape = (200,200)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:32:34.576173Z","iopub.execute_input":"2021-07-17T08:32:34.576449Z","iopub.status.idle":"2021-07-17T08:32:34.580679Z","shell.execute_reply.started":"2021-07-17T08:32:34.576424Z","shell.execute_reply":"2021-07-17T08:32:34.579826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get data","metadata":{}},{"cell_type":"markdown","source":"## Get dataframe of all images with their corresponding label","metadata":{}},{"cell_type":"code","source":"data_df = pd.read_csv(label_csv_path, sep=\";\")","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:32:34.582032Z","iopub.execute_input":"2021-07-17T08:32:34.582586Z","iopub.status.idle":"2021-07-17T08:32:34.648168Z","shell.execute_reply.started":"2021-07-17T08:32:34.582549Z","shell.execute_reply":"2021-07-17T08:32:34.647464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:32:34.649982Z","iopub.execute_input":"2021-07-17T08:32:34.650236Z","iopub.status.idle":"2021-07-17T08:32:34.673824Z","shell.execute_reply.started":"2021-07-17T08:32:34.650212Z","shell.execute_reply":"2021-07-17T08:32:34.672948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get height and width\nAll images do not have the same height and width. In order to keep all sizes in all datasets, height and width are first computed.","metadata":{}},{"cell_type":"code","source":"data_df[\"height\"] = 0\ndata_df[\"width\"] = 0\nfor image_file in tqdm.tqdm(data_df[\"filepath\"]):\n    image = cv2.imread(os.path.join(image_dir_path, image_file), 0) # in order to read it as a grayscale image\n    height, width = image.shape\n    data_df.loc[data_df[\"filepath\"] == image_file, \"height\"] = height\n    data_df.loc[data_df[\"filepath\"] == image_file, \"width\"] = width","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:32:34.677825Z","iopub.execute_input":"2021-07-17T08:32:34.678119Z","iopub.status.idle":"2021-07-17T08:41:13.887967Z","shell.execute_reply.started":"2021-07-17T08:32:34.678092Z","shell.execute_reply":"2021-07-17T08:41:13.887124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df[[\"height\", \"width\"]].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:41:13.891715Z","iopub.execute_input":"2021-07-17T08:41:13.891981Z","iopub.status.idle":"2021-07-17T08:41:13.906821Z","shell.execute_reply.started":"2021-07-17T08:41:13.891953Z","shell.execute_reply":"2021-07-17T08:41:13.90618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Few examples\nPositive and negative examples are displayed with the two different sizes. For every image, a new image with the new shape is also displayed in order to check if the joint is still visible after resizing.","metadata":{}},{"cell_type":"code","source":"example_filepath_list = []\nexample_filepath_list.extend(\n    random.sample(\n        list(\n            data_df.loc[\n                (data_df[\"label\"] == \"n\") & (data_df[\"height\"] == 851),\n                \"filepath\"\n            ].values\n        ),\n        2\n    )\n)\nexample_filepath_list.extend(\n    random.sample(\n        list(\n            data_df.loc[\n                (data_df[\"label\"] == \"n\") & (data_df[\"height\"] == 1066),\n                \"filepath\"\n            ].values\n        ),\n        2\n    )\n)\nexample_filepath_list.extend(\n    random.sample(\n        list(\n            data_df.loc[\n                (data_df[\"label\"] == \"p\") & (data_df[\"height\"] == 851),\n                \"filepath\"\n            ].values\n        ),\n        2\n    )\n)\nexample_filepath_list.extend(\n    random.sample(\n        list(\n            data_df.loc[\n                (data_df[\"label\"] == \"p\") & (data_df[\"height\"] == 1066),\n                \"filepath\"\n            ].values\n        ),\n        2\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:41:13.909614Z","iopub.execute_input":"2021-07-17T08:41:13.909855Z","iopub.status.idle":"2021-07-17T08:41:13.956226Z","shell.execute_reply.started":"2021-07-17T08:41:13.90983Z","shell.execute_reply":"2021-07-17T08:41:13.955307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plot.subplots(len(example_filepath_list), 2)\nf.subplots_adjust(0, 0, 1.5 * 2, 1.5 * len(example_filepath_list))\nfor index in range(len(example_filepath_list)):\n    label = data_df.loc[\n        data_df[\"filepath\"] == example_filepath_list[index], \"label\"\n    ].values[0]\n    image = cv2.imread(\n        os.path.join(image_dir_path, example_filepath_list[index]), 0\n    )\n    ax[index, 0].imshow(image)\n    ax[index, 0].set_title(label)\n    image_resized = cv2.resize(image, image_shape, interpolation=cv2.INTER_AREA)\n    ax[index, 1].imshow(image_resized)\n    ax[index, 1].set_title(label)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:41:13.95951Z","iopub.execute_input":"2021-07-17T08:41:13.959786Z","iopub.status.idle":"2021-07-17T08:41:18.511036Z","shell.execute_reply.started":"2021-07-17T08:41:13.959759Z","shell.execute_reply":"2021-07-17T08:41:18.509868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split data into 3 datasets: train, dev and test","metadata":{}},{"cell_type":"code","source":"# Stratify column in order to keep same proportion of labels and sizes in all datasets\ndata_df[\"stratify\"] = (\n    data_df[\"label\"]\n    + \"_\"\n    + data_df[\"height\"].astype(str)\n    + \"_\"\n    + data_df[\"width\"].astype(str)\n)\nprint(data_df[\"stratify\"].value_counts())\n\n# Train+dev and test\nindices_train_dev, indices_test = train_test_split(\n    data_df.index, test_size=0.1, shuffle=True, stratify=data_df[\"stratify\"],\n)\n\n# Train and dev\nindices_train, indices_dev = train_test_split(\n    data_df.loc[indices_train_dev, :].index,\n    test_size=0.1,\n    shuffle=True,\n    stratify=data_df.loc[indices_train_dev, \"stratify\"],\n)\n\n# Add dataset column\ndata_df[\"dataset\"] = \"\"\ndata_df.loc[indices_test, \"dataset\"] = \"test\"\ndata_df.loc[indices_dev, \"dataset\"] = \"dev\"\ndata_df.loc[indices_train, \"dataset\"] = \"train\"\nprint(data_df[\"dataset\"].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:41:18.512576Z","iopub.execute_input":"2021-07-17T08:41:18.51297Z","iopub.status.idle":"2021-07-17T08:41:18.78684Z","shell.execute_reply.started":"2021-07-17T08:41:18.512926Z","shell.execute_reply":"2021-07-17T08:41:18.785921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Oversample the positive label of the train dataset","metadata":{}},{"cell_type":"code","source":"# First multiply all positive images equally\nnb_class_n = (data_df.loc[indices_train, \"label\"] == \"n\").sum()\nnb_class_p = (data_df.loc[indices_train, \"label\"] == \"p\").sum()\n\noversampling_of_p = int(np.floor(nb_class_n / nb_class_p) - 1)\n\ndata_df_oversampling = pd.concat(\n    [data_df.loc[(data_df[\"dataset\"] == \"train\") & (data_df[\"label\"] == \"p\")].copy()]\n    * oversampling_of_p\n)\ndata_df_oversampling[\"dataset\"] = \"train_oversampling\"\ndata_df = data_df.append(data_df_oversampling, ignore_index=True)\n\n# Then complete to have the same number of positive and negative images in the training set by randomly choosing the positive images\nnb_class_n = (\n    data_df.loc[\n        (\n            (data_df[\"dataset\"] == \"train\")\n            | (data_df[\"dataset\"] == \"train_oversampling\")\n        ),\n        \"label\",\n    ]\n    == \"n\"\n).sum()\nnb_class_p = (\n    data_df.loc[\n        (\n            (data_df[\"dataset\"] == \"train\")\n            | (data_df[\"dataset\"] == \"train_oversampling\")\n        ),\n        \"label\",\n    ]\n    == \"p\"\n).sum()\n\nadditionnal_indices_for_oversampling = random.sample(\n    data_df.loc[\n        (data_df[\"dataset\"] == \"train\") & (data_df[\"label\"] == \"p\")\n    ].index.to_list(),\n    nb_class_n - nb_class_p,\n)\n\ndata_df_oversampling = data_df.loc[additionnal_indices_for_oversampling].copy()\ndata_df_oversampling[\"dataset\"] = \"train_oversampling\"\ndata_df = data_df.append(data_df_oversampling, ignore_index=True)\nprint(data_df[[\"dataset\", \"label\"]].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:41:18.788263Z","iopub.execute_input":"2021-07-17T08:41:18.788822Z","iopub.status.idle":"2021-07-17T08:41:18.925917Z","shell.execute_reply.started":"2021-07-17T08:41:18.788762Z","shell.execute_reply":"2021-07-17T08:41:18.924966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate datasets of images with data augmentation for the training dataset","metadata":{}},{"cell_type":"code","source":"train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale=1.0 / 255,\n    width_shift_range=0.3, # because the joint is usually horizontally in the center of the image, however it could be close to the top or the bottom hence no height_shift_range\n    fill_mode=\"nearest\",\n    horizontal_flip=True,\n    vertical_flip=True,\n)\nval_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255)\n\n# common kwargs for all datasets\nkwargs = {\n    \"directory\": image_dir_path,\n    \"x_col\": \"filepath\",\n    \"y_col\": \"label\",\n    \"weight_col\": None,\n    \"target_size\": image_shape,\n    \"color_mode\": \"grayscale\",\n    \"class_mode\": \"binary\",\n    \"batch_size\": 256,\n}\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    data_df.loc[\n        (data_df[\"dataset\"] == \"train\") | (data_df[\"dataset\"] == \"train_oversampling\")\n    ],\n    **kwargs\n)\nvalidation_generator = val_datagen.flow_from_dataframe(\n    data_df.loc[(data_df[\"dataset\"] == \"dev\")],\n    **kwargs\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:41:18.927505Z","iopub.execute_input":"2021-07-17T08:41:18.928012Z","iopub.status.idle":"2021-07-17T08:41:28.010368Z","shell.execute_reply.started":"2021-07-17T08:41:18.927969Z","shell.execute_reply":"2021-07-17T08:41:28.009336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN model","metadata":{}},{"cell_type":"markdown","source":"## Architecture\n\nI had trouble to simultaneously get a parsimonious and reproducible model.\n\nSince all the images look quite similar (the rail is always vertical, the joint always horizontal) and there are only 2 classes, I first thought that a small model would be more appropriate. I manage to get quite good results. However I had trouble to get similar result when trying to retrain the model. Sometimes, the new performances were very bad compared to the best one. This make me think that the model was overfitting but I was not able to get better results with smaller or more regularized models.\n\nFor this reason I decided to build a bigger model. Although it seems to be a bit of overfitting during training, this is the only way I found to get more reproducible models. Two different training would result in similar performance. I do not have a lot of experience in ML, but to me, a good architecture should be robust to seed, hence the following proposal of architecture.","metadata":{}},{"cell_type":"code","source":"regu_coef = 0.004\ndropout_cnn = 0\ndropout_fc = 0.4\n\nmodel = tf.keras.models.Sequential()\n\nmodel.add(\n    tf.keras.layers.Conv2D(\n        8,\n        kernel_size=(3, 3),\n        activation=\"relu\",\n        input_shape=(*image_shape, 1),\n        kernel_regularizer=tf.keras.regularizers.l2(regu_coef),\n        bias_regularizer=tf.keras.regularizers.l2(regu_coef),\n        kernel_initializer=tf.keras.initializers.he_normal(),\n    )\n)\nmodel.add(tf.keras.layers.Dropout(dropout_cnn))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(\n    tf.keras.layers.Conv2D(\n        8,\n        kernel_size=(3, 3),\n        activation=\"relu\",\n        kernel_regularizer=tf.keras.regularizers.l2(regu_coef),\n        bias_regularizer=tf.keras.regularizers.l2(regu_coef),\n        kernel_initializer=tf.keras.initializers.he_normal(),\n    )\n)\nmodel.add(tf.keras.layers.Dropout(dropout_cnn))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(\n    tf.keras.layers.Conv2D(\n        8,\n        kernel_size=(3, 3),\n        activation=\"relu\",\n        kernel_regularizer=tf.keras.regularizers.l2(regu_coef),\n        bias_regularizer=tf.keras.regularizers.l2(regu_coef),\n        kernel_initializer=tf.keras.initializers.he_normal(),\n    )\n)\nmodel.add(tf.keras.layers.Dropout(dropout_cnn))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(\n    tf.keras.layers.Conv2D(\n        2,\n        kernel_size=(3, 3),\n        activation=\"relu\",\n        kernel_regularizer=tf.keras.regularizers.l2(regu_coef),\n        bias_regularizer=tf.keras.regularizers.l2(regu_coef),\n        kernel_initializer=tf.keras.initializers.he_normal(),\n    )\n)\nmodel.add(tf.keras.layers.Dropout(dropout_fc))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(\n    tf.keras.layers.Dense(\n        10,\n        activation=\"relu\",\n        kernel_regularizer=tf.keras.regularizers.l2(regu_coef),\n        bias_regularizer=tf.keras.regularizers.l2(regu_coef),\n        kernel_initializer=tf.keras.initializers.he_normal(),\n    )\n)\nmodel.add(tf.keras.layers.Dropout(dropout_cnn))\nmodel.add(\n    tf.keras.layers.Dense(\n        1,\n        activation=\"sigmoid\",\n        kernel_regularizer=tf.keras.regularizers.l2(regu_coef),\n        bias_regularizer=tf.keras.regularizers.l2(regu_coef),\n        kernel_initializer=tf.keras.initializers.he_normal(),\n    )\n)\n\ninitial_learning_rate = 0.001\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps=2000, decay_rate=0.96, staircase=False\n)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    metrics=[\n        \"accuracy\",\n        tf.keras.metrics.Precision(name=\"precision\"),\n        tf.keras.metrics.Recall(name=\"recall\"),\n    ],\n)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:41:28.011884Z","iopub.execute_input":"2021-07-17T08:41:28.012453Z","iopub.status.idle":"2021-07-17T08:41:30.631095Z","shell.execute_reply.started":"2021-07-17T08:41:28.012408Z","shell.execute_reply":"2021-07-17T08:41:30.630389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\nI only did a few epochs with a quite big learning rate (which partially explains the high variability on the dev set). More epochs with a smaller learning rate help to get a better model.","metadata":{}},{"cell_type":"code","source":"callback = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=10, mode=\"min\", restore_best_weights=True\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:41:30.632356Z","iopub.execute_input":"2021-07-17T08:41:30.632689Z","iopub.status.idle":"2021-07-17T08:41:30.639336Z","shell.execute_reply.started":"2021-07-17T08:41:30.632653Z","shell.execute_reply":"2021-07-17T08:41:30.638468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(\n    train_generator,\n    epochs=60,\n    validation_data=validation_generator,\n    callbacks=[callback],\n    workers = 12,\n    max_queue_size = 16\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T08:41:30.640541Z","iopub.execute_input":"2021-07-17T08:41:30.640921Z","iopub.status.idle":"2021-07-17T12:18:14.724541Z","shell.execute_reply.started":"2021-07-17T08:41:30.64088Z","shell.execute_reply":"2021-07-17T12:18:14.709518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255)\ntest_generator = test_datagen.flow_from_dataframe(\n    data_df.loc[(data_df[\"dataset\"] == \"test\")],\n    directory=image_dir_path,\n    x_col=\"filepath\",\n    y_col=\"label\",\n    weight_col=None,\n    target_size=image_shape,\n    color_mode=\"grayscale\",\n    class_mode=\"binary\",\n    batch_size=64,\n    shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:18:14.742259Z","iopub.execute_input":"2021-07-17T12:18:14.742643Z","iopub.status.idle":"2021-07-17T12:18:22.324577Z","shell.execute_reply.started":"2021-07-17T12:18:14.742601Z","shell.execute_reply":"2021-07-17T12:18:22.322333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:18:22.325968Z","iopub.execute_input":"2021-07-17T12:18:22.330297Z","iopub.status.idle":"2021-07-17T12:18:39.70778Z","shell.execute_reply.started":"2021-07-17T12:18:22.330231Z","shell.execute_reply":"2021-07-17T12:18:39.707095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_prod_pred = model.predict(test_generator)\ny_pred = np.copy(y_prod_pred)\ny_pred[y_pred > 0.5] = 1\ny_pred[y_pred <= 0.5] = 0\ny_true = test_generator.classes","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:18:39.710602Z","iopub.execute_input":"2021-07-17T12:18:39.710867Z","iopub.status.idle":"2021-07-17T12:18:51.545582Z","shell.execute_reply.started":"2021-07-17T12:18:39.710841Z","shell.execute_reply":"2021-07-17T12:18:51.544777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:18:51.54899Z","iopub.execute_input":"2021-07-17T12:18:51.549276Z","iopub.status.idle":"2021-07-17T12:18:51.574153Z","shell.execute_reply.started":"2021-07-17T12:18:51.549249Z","shell.execute_reply":"2021-07-17T12:18:51.573505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prob_true, prob_pred = calibration_curve(y_true, y_prod_pred, n_bins=20, strategy=\"quantile\")\nf, ax = plot.subplots(1, 1)\nax.plot(prob_pred, prob_true, linestyle=\":\", marker=\"+\", label=\"model calibration curve\")\nax.plot(\n    [0, 1], [0, 1], label=\"calibrated curve\",\n)\nax.set_xlabel(\"Mean predicted value\")\nax.set_ylabel(\"Mean true value\")\nax.legend([\"model calibration curve\", \"calibrated curve\"])","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:18:51.576915Z","iopub.execute_input":"2021-07-17T12:18:51.577158Z","iopub.status.idle":"2021-07-17T12:18:51.738327Z","shell.execute_reply.started":"2021-07-17T12:18:51.577134Z","shell.execute_reply":"2021-07-17T12:18:51.737564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explainability","metadata":{}},{"cell_type":"markdown","source":"## Activation maximization from the training set\nGet the first 9 image parts of the training set that maximize the activation of each filter. This gives insights of the patterns detected by the model in order to predict the final class.","metadata":{}},{"cell_type":"code","source":"def locs_of_previous_layer(i, j, padding, kernel_size, strides):\n    if padding == \"valid\":\n        shift = (0, 0)\n    elif padding == \"same\":\n        shift = tuple([-int(np.floor(x / 2)) for x in kernel_size])\n    return (\n        i * strides[0] + shift[0],\n        i * strides[0] + shift[0] + kernel_size[0] - 1,\n        j * strides[1] + shift[1],\n        j * strides[1] + shift[1] + kernel_size[1] - 1,\n    )\n\n\ndef feature_map_loc_to_input_loc(model, layer_name, i, j):\n    previous_layer = False\n    x_min = i\n    x_max = i\n    y_min = j\n    y_max = j\n    for layer in reversed(model.layers):\n        if layer.name == layer_name:\n            previous_layer = True\n        if previous_layer:\n            if isinstance(layer, tf.keras.layers.Conv2D):\n                strides = layer.strides\n                padding = layer.padding\n                kernel_size = layer.kernel_size\n                x_min, _, y_min, _ = locs_of_previous_layer(\n                    x_min, y_min, padding, kernel_size, strides\n                )\n                _, x_max, _, y_max = locs_of_previous_layer(\n                    x_max, y_max, padding, kernel_size, strides\n                )\n            elif isinstance(layer, tf.keras.layers.MaxPooling2D):\n                strides = layer.strides\n                padding = layer.padding\n                kernel_size = layer.pool_size\n                x_min, _, y_min, _ = locs_of_previous_layer(\n                    x_min, y_min, padding, kernel_size, strides\n                )\n                _, x_max, _, y_max = locs_of_previous_layer(\n                    x_max, y_max, padding, kernel_size, strides\n                )\n    return x_min, x_max, y_min, y_max\n\ndef get_filter_highest_activations(model, dev_generator, nb_images_per_filter=9):\n    # submodel\n    conv_layers_outputs = [\n        (layer.name, layer.output)\n        for layer in model.layers\n        if isinstance(layer, tf.keras.layers.Conv2D)\n    ]\n    activations_model = tf.keras.models.Model(\n        model.inputs,\n        outputs=[conv_layer_output[1] for conv_layer_output in conv_layers_outputs],\n    )\n\n    # output initialization\n    max_activations = {}\n    for layer_id in range(len(activations_model.output)):\n        max_activations[layer_id] = {}\n        for channel_id in range(activations_model.output[layer_id].shape[-1]):\n            max_activations[layer_id][channel_id] = {\"activation\": [], \"input\": []}\n\n    # get max activations\n    for ii in range(len(dev_generator)):\n        (image_batch, _) = next(dev_generator)\n        predictions = activations_model.predict(image_batch)\n        for layer in range(len(activations_model.output)):\n            for channel in range(activations_model.output[layer].shape[-1]):\n                layer_name = conv_layers_outputs[layer][0]\n                new_shape = 1\n                for size in predictions[layer].shape[:3]:\n                    new_shape *= size\n                highest_values = tf.sort(\n                    tf.reshape(predictions[layer][:, :, :, channel], new_shape)\n                )[-nb_images_per_filter:]\n                for activation in highest_values:\n                    if len(\n                        max_activations[layer][channel][\"activation\"]\n                    ) == nb_images_per_filter and activation.numpy() > min(\n                        max_activations[layer][channel][\"activation\"]\n                    ):\n                        index_to_remove = max_activations[layer][channel][\n                            \"activation\"\n                        ].index(min(max_activations[layer][channel][\"activation\"]))\n                        del max_activations[layer][channel][\"activation\"][\n                            index_to_remove\n                        ]\n                        del max_activations[layer][channel][\"input\"][index_to_remove]\n\n                    if (\n                        len(max_activations[layer][channel][\"activation\"])\n                        < nb_images_per_filter\n                    ):\n                        max_activations[layer][channel][\"activation\"].append(\n                            activation.numpy()\n                        )\n                        loc = tf.where(\n                            predictions[layer][:, :, :, channel] == activation\n                        )[\n                            0\n                        ]  # take the first one if several\n                        x_min, x_max, y_min, y_max = feature_map_loc_to_input_loc(\n                            activations_model,\n                            layer_name,\n                            loc[1].numpy(),\n                            loc[2].numpy(),\n                        )\n                        if x_max - x_min != y_max - y_min:\n                            break\n                        max_activations[layer][channel][\"input\"].append(\n                            image_batch[\n                                loc[0].numpy(), x_min : x_max + 1, y_min : y_max + 1, 0\n                            ]\n                        )\n\n    # create IO output\n    all_layer_io = []\n    for layer in max_activations.keys():\n        layer_io = []\n        nb_channels = len(max_activations[layer])\n        for channel in range(nb_channels):\n            size = max_activations[layer][channel][\"input\"][0].shape[0]\n            fig = plot.figure(figsize=(size / 5, size / 5))\n            for index in range(len(max_activations[layer][channel][\"input\"])):\n                ax = fig.add_subplot(\n                    np.sqrt(nb_images_per_filter),\n                    np.sqrt(nb_images_per_filter),\n                    index + 1,\n                )\n                ax.imshow(\n                    max_activations[layer][channel][\"input\"][index],\n                    cmap=\"gray\",\n                )\n                ax.get_xaxis().set_visible(False)\n                ax.get_yaxis().set_visible(False)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:18:51.739756Z","iopub.execute_input":"2021-07-17T12:18:51.740117Z","iopub.status.idle":"2021-07-17T12:18:51.778514Z","shell.execute_reply.started":"2021-07-17T12:18:51.740078Z","shell.execute_reply":"2021-07-17T12:18:51.777759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0 / 255)\nkwargs = {\n    \"directory\": image_dir_path,\n    \"x_col\": \"filepath\",\n    \"y_col\": \"label\",\n    \"weight_col\": None,\n    \"target_size\": image_shape,\n    \"color_mode\": \"grayscale\",\n    \"class_mode\": \"binary\",\n    \"batch_size\": 64,\n}\ndev_generator = dev_datagen.flow_from_dataframe(\n    data_df.loc[\n        data_df[\"dataset\"] != \"train_oversampling\"\n    ],\n    **kwargs\n)\n\nget_filter_highest_activations(model, dev_generator)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:18:51.780076Z","iopub.execute_input":"2021-07-17T12:18:51.780564Z","iopub.status.idle":"2021-07-17T12:26:07.909465Z","shell.execute_reply.started":"2021-07-17T12:18:51.780522Z","shell.execute_reply":"2021-07-17T12:26:07.908332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Grad CAM and guided grad CAM\n\nGrad CAM and Guided Grad CAM are used on a single (positive) example to highlight their potential to explain the prediction of the model on a single instance.","metadata":{}},{"cell_type":"code","source":"def get_last_conv2d_name(model: tf.keras.models.Sequential) -> str:\n    \"\"\"\n    Get the name of the last convolutional layer\n    \"\"\"\n    for layer in reversed(model.layers):\n        if isinstance(layer, tf.keras.layers.Conv2D):\n            return layer.name\n    raise ValueError(\n        \"There is no Conv2D layer in the model. \"\n        \"This is then not possible to retrieve the name of a Conv2D layer.\"\n    )\n\n    \ndef compute_grad_cam(\n    model: tf.keras.models.Sequential, image: np.ndarray, image_shape: Tuple[int]\n) -> np.ndarray:\n    \"\"\"\n    Compute grad CAM of a given image and model.\n    \"\"\"\n    last_cnn_layer = get_last_conv2d_name(model)\n\n    # Create a graph that outputs target convolution and output\n    grad_model = tf.keras.models.Model(\n        model.inputs, [model.get_layer(last_cnn_layer).output, model.output]\n    )\n\n    # Get the prediction for target class\n    with tf.GradientTape() as tape:\n        last_conv_outputs, predictions = grad_model(image)\n        y_pred_prob = predictions[:, 0]  # prediction for the target class\n    grads = tape.gradient(y_pred_prob, last_conv_outputs)[\n        0\n    ]  # only one image to predict\n\n    # Average gradients spatially\n    weights = tf.reduce_mean(grads, axis=(0, 1))\n\n    # Build a ponderated map of filters according to gradients importance\n    last_conv_output = last_conv_outputs[0]  # only one image to predict\n    grad_cam = tf.reduce_sum(tf.multiply(weights, last_conv_output), axis=-1)\n\n    # Final Grad CAM computation: resize and ReLU\n    grad_cam = cv2.resize(grad_cam.numpy(), image_shape)\n    grad_cam = np.maximum(grad_cam, 0)\n    return grad_cam\n\n@tf.custom_gradient\ndef guided_relu(x):\n    def grad(dy):\n        return tf.multiply(\n            tf.multiply(tf.where(dy > 0, 1.0, 0.0), tf.where(x > 0, 1.0, 0.0)), dy\n        )\n\n    result = tf.nn.relu(x)\n    return result, grad\n\ndef compute_guided_backpropagation(\n    model: tf.keras.models.Sequential, image: np.ndarray\n) -> tf.python.framework.ops.EagerTensor:\n    \"\"\"\n    Compute the guided backpropagation with the modified relu activation\n    \"\"\"\n    model_with_guided_relu = tf.keras.models.clone_model(model)\n    model_with_guided_relu.set_weights(model.get_weights())\n\n    layer_list = [\n        layer for layer in model_with_guided_relu.layers if hasattr(layer, \"activation\")\n    ]\n    for layer in layer_list:\n        if layer.activation == tf.keras.activations.relu:\n            layer.activation = guided_relu\n\n    input_data = tf.Variable(tf.cast(image, tf.float32))\n    with tf.GradientTape() as tape:\n        predictions = model_with_guided_relu(input_data)\n        loss = predictions[:, 0]\n    guided_backpropagation = tape.gradient(loss, input_data)[\n        0\n    ]  # only one image to predict\n    return guided_backpropagation","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:26:07.911155Z","iopub.execute_input":"2021-07-17T12:26:07.911522Z","iopub.status.idle":"2021-07-17T12:26:07.933641Z","shell.execute_reply.started":"2021-07-17T12:26:07.91148Z","shell.execute_reply":"2021-07-17T12:26:07.932537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get image of test set and preprocess it\nimage_example_path = os.path.join(\n    image_dir_path,\n    data_df.loc[\n        (data_df[\"dataset\"] == \"test\")\n        & (data_df[\"label\"] == \"p\"),\n    \"filepath\"\n    ].values[0],\n)\nimage_example = tf.keras.preprocessing.image.load_img(\n    image_example_path, target_size=image_shape, color_mode=\"grayscale\"\n)\nimage_example = tf.keras.preprocessing.image.img_to_array(image_example)\nimage_example = np.expand_dims(image_example, axis=0)\nimage_example = image_example / 255","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:26:07.935595Z","iopub.execute_input":"2021-07-17T12:26:07.936088Z","iopub.status.idle":"2021-07-17T12:26:07.968619Z","shell.execute_reply.started":"2021-07-17T12:26:07.936047Z","shell.execute_reply":"2021-07-17T12:26:07.967857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.predict(image_example)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:26:07.97004Z","iopub.execute_input":"2021-07-17T12:26:07.970534Z","iopub.status.idle":"2021-07-17T12:26:08.099493Z","shell.execute_reply.started":"2021-07-17T12:26:07.970499Z","shell.execute_reply":"2021-07-17T12:26:08.098639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grad_cam = compute_grad_cam(model, image_example, image_shape)\nheatmap = (grad_cam - grad_cam.min()) / (grad_cam.max() - grad_cam.min())\nguided_backpropagation = compute_guided_backpropagation(model, image_example)\n\nfig = plot.figure(figsize=(8, 16))\nax = fig.add_subplot(1, 3, 1)\nax.imshow(image_example[0, :, :, 0], cmap=\"gray\")\nax = fig.add_subplot(1, 3, 2)\nax.imshow((image_example[0, :, :, 0] * heatmap[:, :]), cmap=\"gray\")\nax = fig.add_subplot(1, 3, 3)\nax.imshow((guided_backpropagation[:, :, 0] * heatmap[:, :]), cmap=\"gray\")","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:26:08.100859Z","iopub.execute_input":"2021-07-17T12:26:08.101205Z","iopub.status.idle":"2021-07-17T12:26:08.696108Z","shell.execute_reply.started":"2021-07-17T12:26:08.101168Z","shell.execute_reply":"2021-07-17T12:26:08.695409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Limit of the trained model\nThe model has been trained on a dataset on images. Using this model on an images that differs from this dataset may be irrelevant. Below, I constructed an image that maximize the activation of one of the two filters of the last layer. The prediction is 1 although it seems clear that the image does not include a joint.","metadata":{}},{"cell_type":"code","source":"def build_submodel(\n    model: tf.keras.models.Sequential,\n    conv_layer_name: str,\n    add_input_over_all_reals: bool = True,\n    change_relu: bool = True,\n) -> tf.keras.models.Sequential:\n    \"\"\"\n    Build a submodel of the original model.\n\n    The output of the submodel is the output of the Conv2D layer which name is provided in argument.\n\n    Since the image pixel values are between 0 and 1 (after normalization),\n    an additional layer may be added at the beginning implementing a sigmoid\n    to map all the real to values between 0 and 1.\n\n    The derivative of the ReLU is 0 for negative values. This may prevent the optimization to work.\n    It is possible to change the ReLU activation by a Leaky ReLU one in order to make the optimization works.\n    \"\"\"\n    # Create a copy because activations may be changed\n    model_copy = tf.keras.models.clone_model(model)\n    model_copy.set_weights(model.get_weights())\n\n    # Create submodel\n    submodel = tf.keras.models.Model(\n        model_copy.inputs, model_copy.get_layer(conv_layer_name).output\n    )\n\n    # Change activation\n    if change_relu:\n        layer_list = [\n            layer for layer in submodel.layers if hasattr(layer, \"activation\")\n        ]\n        for layer in layer_list:\n            if layer.activation == tf.keras.activations.relu:\n                layer.activation = tf.nn.leaky_relu\n\n    # Add new input over all reals in order to restrain the image value over ]0,1[\n    if add_input_over_all_reals:\n        input_layer_over_all_reals = tf.keras.Input(shape=(*image_shape, 1))\n        x = tf.keras.layers.Activation(tf.keras.activations.sigmoid)(\n            input_layer_over_all_reals\n        )\n        output = submodel(x)\n        submodel = tf.keras.models.Model(input_layer_over_all_reals, output)\n\n    return submodel\n\n\ndef maximize_ouput(\n    submodel: tf.keras.models.Sequential,\n    channel_id: int,\n    epochs: int,\n    nb_activation_to_max: str = \"single\",\n    learning_rate: float = 1.0,\n) -> (np.array, list, list):\n    \"\"\"\n    Maximize the output of the provided model by changing the input.\n    \"\"\"\n    # Initialize random normal noise on R\n    input_data = np.random.normal(size=(1, *image_shape, 1))\n    # Cast random noise from np.float64 to tf.float32 Variable because we will compute the derivative\n    input_data = tf.Variable(tf.cast(input_data, tf.float32))\n\n    # get maximum size to look at during optimization\n    if nb_activation_to_max == \"single\":\n        max_size = 0\n    elif nb_activation_to_max == \"all\":\n        max_size = input_data.shape[1]\n    else:\n        raise ValueError(f\"Unknown input argument {nb_activation_to_max}.\")\n\n    # Iterate gradient ascents\n    mean_activation_history = []\n    grad_history = []\n    for _ in range(epochs):\n        with tf.GradientTape() as tape:\n            output = submodel(input_data)\n            if nb_activation_to_max == \"single\":\n                mean_activation = tf.reduce_mean(output[:, 0, 0, channel_id])\n            elif nb_activation_to_max == \"all\":\n                mean_activation = tf.reduce_mean(output[:, :, :, channel_id])\n            mean_activation_history.append(mean_activation)\n        grads = tape.gradient(mean_activation, input_data)\n        if tf.norm(grads) < 1e-5:\n            print(\"Optimization stopped since gradient is almost flat.\")\n            break\n        grad_history.append(tf.norm(grads))\n        input_data.assign_add(grads * learning_rate)\n        if nb_activation_to_max == \"single\":\n            max_size_tmp = tf.math.reduce_max(tf.where(grads[0, :, :, 0]))\n            max_size = max(max_size_tmp, max_size)\n        if len(mean_activation_history) >= 5:\n            if mean_activation_history[-1] < min(mean_activation_history[-5:-2]):\n                print(\"Optimization stopped since activation did not increase.\")\n                break\n    return (\n        input_data[0, : max_size + 1, : max_size + 1, 0],\n        mean_activation_history,\n        grad_history,\n    )","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:26:08.697429Z","iopub.execute_input":"2021-07-17T12:26:08.697793Z","iopub.status.idle":"2021-07-17T12:26:08.721559Z","shell.execute_reply.started":"2021-07-17T12:26:08.697759Z","shell.execute_reply":"2021-07-17T12:26:08.72063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submodel = build_submodel(model, \"conv2d_3\", True, True)\n\ninput_value_on_real, activation_history, grad_history = maximize_ouput(\n    submodel,\n    1,\n    1000,\n    nb_activation_to_max=\"single\",\n    learning_rate=0.01,\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-17T12:26:08.723482Z","iopub.execute_input":"2021-07-17T12:26:08.72394Z","iopub.status.idle":"2021-07-17T12:26:17.162295Z","shell.execute_reply.started":"2021-07-17T12:26:08.723901Z","shell.execute_reply":"2021-07-17T12:26:17.161388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_data = np.zeros(shape=(1, *image_shape, 1))\ninput_data[0, :38, :38, 0] = tf.math.sigmoid(input_value_on_real)\ninput_data[0, 38 : 2 * 38, :38, 0] = tf.math.sigmoid(input_value_on_real)\ninput_data[0, :38, 38 : 2 * 38, 0] = tf.math.sigmoid(input_value_on_real)\n\n\nkwargs = {\"vmin\": 0, \"vmax\": 1}\nfig = plot.figure(figsize=(6, 6))\nax = fig.add_subplot(1, 1, 1)\nax.imshow(input_data[0, :, :, 0], cmap=\"gray\", **kwargs)\nfig.suptitle(f\"{model.predict(input_data)[0][0]:.2%}\")\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-17T13:15:24.837425Z","iopub.execute_input":"2021-07-17T13:15:24.837838Z","iopub.status.idle":"2021-07-17T13:15:24.898138Z","shell.execute_reply.started":"2021-07-17T13:15:24.83779Z","shell.execute_reply":"2021-07-17T13:15:24.895403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}