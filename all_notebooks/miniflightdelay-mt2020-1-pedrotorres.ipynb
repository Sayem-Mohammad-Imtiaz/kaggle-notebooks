{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Mini flight delay prediction"},{"metadata":{},"cell_type":"markdown","source":"### Kaggle imports and directory/path configurations"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imports for preprocessing and evaluation"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# Evaluation metrics\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Open and display dataframes"},{"metadata":{},"cell_type":"markdown","source":"#### Train dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load dataframe\ntrain_df = pd.read_csv('../input/mini-flight-delay-prediction/flight_delays_train.csv')\n\n# Display dataframe's head\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display dataframe info\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load test dataframe\ntest_df = pd.read_csv('../input/mini-flight-delay-prediction/flight_delays_test.csv')\n\n# Display test dataframe's head\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display test dataframe info\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, both train and test dataframes have no missing values, so we can proceed to treat their data with that in mind."},{"metadata":{},"cell_type":"markdown","source":"## Data handling"},{"metadata":{},"cell_type":"markdown","source":"### Map carrier, origin/destination airport codes and delayed to numeric attributes"},{"metadata":{},"cell_type":"markdown","source":"#### Train DF"},{"metadata":{"trusted":true},"cell_type":"code","source":"# UniqueCarrier\nuc_labels = train_df.UniqueCarrier.unique().tolist()\nlabel_dict_uc_train = {}\nfor index, possible_label in enumerate(uc_labels):\n    label_dict_uc_train[possible_label] = index\n\n# Origin\norigin_labels = train_df.Origin.unique().tolist()\nlabel_dict_origin_train = {}\nfor index, possible_label in enumerate(origin_labels):\n    label_dict_origin_train[possible_label] = index\n\n# Dest\ndest_labels = train_df.Dest.unique().tolist()\nlabel_dict_dest_train = {}\nfor index, possible_label in enumerate(dest_labels):\n    label_dict_dest_train[possible_label] = index\n\n# Mapping 'UniqueCarrier', 'Origin' and 'Dest'\ntrain_df['UniqueCarrier'] = train_df.UniqueCarrier.replace(label_dict_uc_train)\ntrain_df['Origin'] = train_df.Origin.replace(label_dict_origin_train)\ntrain_df['Dest'] = train_df.Dest.replace(label_dict_dest_train)\n\n# Map 'dep_delayed...' to 1/0 and save it to 'delayed' variable\ndelayed = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test DF"},{"metadata":{"trusted":true},"cell_type":"code","source":"# UniqueCarrier\nuc_labels = test_df.UniqueCarrier.unique().tolist()\nlabel_dict_uc_test = {}\nfor index, possible_label in enumerate(uc_labels):\n    label_dict_uc_test[possible_label] = index\n\n# Origin\norigin_labels = test_df.Origin.unique().tolist()\nlabel_dict_origin_test = {}\nfor index, possible_label in enumerate(origin_labels):\n    label_dict_origin_test[possible_label] = index\n\n# Dest\ndest_labels = test_df.Dest.unique().tolist()\nlabel_dict_dest_test = {}\nfor index, possible_label in enumerate(dest_labels):\n    label_dict_dest_test[possible_label] = index\n\n# Map 'dep_delayed...' to 1/0 and save it to 'delayed' variable\ntest_df['UniqueCarrier'] = test_df.UniqueCarrier.replace(label_dict_uc_test)\ntest_df['Origin'] = test_df.Origin.replace(label_dict_origin_test)\ntest_df['Dest'] = test_df.Dest.replace(label_dict_dest_test)\n\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Clean attributes related to date and cast them to int"},{"metadata":{},"cell_type":"markdown","source":"#### Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing the 'c-' from the data related to dates\nmonth = train_df['Month'].str.split('-')\ntrain_df['Mon']=month.apply(lambda x:int(x[1]))\n\nday = train_df['DayofMonth'].str.split('-')\ntrain_df['DOM']=day.apply(lambda x:int(x[1]))\n\ndow = train_df['DayOfWeek'].str.split('-')\ntrain_df['DOW']=dow.apply(lambda x:int(x[1]))\n\n# Drop redundant columns\ntrain_df = train_df.drop(['Month', 'DayofMonth', 'DayOfWeek'], axis=1)\n\n# Rename columns to 'Month', 'Day' and 'DayOfWeek'\ntrain_df.rename(columns={'Mon': 'Month',  'DOM': 'DayOfMonth',\n                         'DOW': 'DayOfWeek'}, inplace=True)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing the 'c-' from the data related to dates\nmonth = test_df['Month'].str.split('-')\ntest_df['Mon']=month.apply(lambda x:int(x[1]))\n\nday = test_df['DayofMonth'].str.split('-')\ntest_df['DOM']=day.apply(lambda x:int(x[1]))\n\ndow = test_df['DayOfWeek'].str.split('-')\ntest_df['DOW']=dow.apply(lambda x:int(x[1]))\n\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Convert 'DepTime' to 'timedelta' and cast it to numeric"},{"metadata":{},"cell_type":"markdown","source":"#### Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate hours and minutes into their respective columns\ntrain_df['DepHour'] = train_df['DepTime']//100\ntrain_df['DepHour'].replace(to_replace=[24,25], value=0, inplace=True)\n\ntrain_df['DepMinute'] = train_df['DepTime']%100\n\n# Save the time in minutes\ntrain_df['Minutes'] = train_df['DepMinute'] + train_df['DepHour']*60\n\n# Convert time to 'timedelta'\ntrain_df['Time'] = pd.to_timedelta(train_df['Minutes'], unit='m')\n\n# Drop irrelevant columns\ntrain_df = train_df.drop(['DepHour', 'DepMinute', 'Minutes', 'DepTime'], axis=1)\n\n# Cast 'datetime' to numeric\ntrain_df['Time'] = pd.to_numeric(train_df['Time'], downcast='float')\n\n# Rename column 'Time' to 'DepTime'\ntrain_df.rename(columns={'Time' : 'DepTime'}, inplace=True)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separate hours and minutes into their respective columns\ntest_df['DepHour'] = test_df['DepTime']//100\ntest_df['DepHour'].replace(to_replace=[24,25], value=0, inplace=True)\n\ntest_df['DepMinute'] = test_df['DepTime']%100\n\n# Save the time in minutes\ntest_df['Minutes'] = test_df['DepMinute'] + test_df['DepHour']*60\n\n# Convert time to 'timedelta'\ntest_df['Time'] = pd.to_timedelta(test_df['Minutes'], unit='m')\n\n# Cast 'datetime' to numeric\ntest_df['Time'] = pd.to_numeric(test_df['Time'], downcast='float')\n\n# Drop redundant/irrelevant columns\ntest_df = test_df.drop(['Month', 'DayofMonth', 'DayOfWeek', 'DepHour',\n                          'DepMinute', 'Minutes', 'DepTime'], axis=1)\n\n# Rename columns to 'Month', 'Day', 'DayOfWeek' and 'DepTime'\ntest_df.rename(columns={'Mon': 'Month',  'DOM': 'DayOfMonth',\n                         'DOW': 'DayOfWeek', 'Time': 'DepTime'}, inplace=True)\n\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is mostly treated. I'll be normalizing the values afterwards, as I am using PyCaret to compare the classifiers and it already does it in its runtime."},{"metadata":{},"cell_type":"markdown","source":"## Automated model test using PyCaret"},{"metadata":{},"cell_type":"markdown","source":"### Split between modeling and validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data (the split is stratified by default)\ndata = train_df.sample(frac=0.75, random_state=31415)\n\ndata_unseen = train_df.drop(data.index)\ndata.reset_index(inplace=True, drop=True)\ndata_unseen.reset_index(inplace=True, drop=True)\n\nprint('Data for Modeling: ' + str(data.shape))\nprint('Unseen Data For Predictions: ' + str(data_unseen.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Install and import PyCaret"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pycaret\nfrom pycaret.classification import *","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_cls101 = setup(data = data, target = 'dep_delayed_15min', session_id=27182,\n                   numeric_features = ['UniqueCarrier', 'Origin', 'Dest',\n                                       'Distance', 'Month', 'DayOfMonth',\n                                       'DayOfWeek', 'DepTime'],\n                   data_split_stratify=True, silent=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model compairson"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = compare_models()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the best classifiers (according to those metrics) are CatBoost, LGBM and Extreme Gradient Boosting, with LGBM being quicker than both, while obtaining similar scores.\n\nSince this submission is for a course I'm taking, I'll further evaluate LGBM, Random Forest, Gradient Boosting and Decision Tree, as we have studied the last three and LGBM has one of the best scores, while being quick to model.\n\nI won't be evaluating CatBoost or Extreme Gradient Boosting as these two take too long to run, as well as SVM and MLP, even though we have studied them, they too take a long time to run, but don't offer good results (I've run them beforehand)."},{"metadata":{},"cell_type":"markdown","source":"#### LGBM"},{"metadata":{},"cell_type":"markdown","source":"Creating the model with PyCaret is as simple as the following code suggests. Then, we can further evaluate the results and scores by plotting the confusion matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create model for evaluation\nlgbm = create_model('lightgbm')\n\n# Plot the confusion matrix\nplot_model(lgbm, plot='confusion_matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune the model's hyperparameters\ntuned_lgbm = tune_model(lgbm)\n\n# Plot the tuned model's confusion matrix\nplot_model(tuned_lgbm, plot='confusion_matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create model for evaluation\nrfc = create_model('rf')\n\n# Plot the confusion matrix\nplot_model(rfc, plot='confusion_matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune the model's hyperparameters\ntuned_rfc = tune_model(rfc)\n\n# Plot the tuned model's confusion matrix\nplot_model(tuned_rfc, plot='confusion_matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create model for evaluation\ngbc = create_model('gbc')\n\n# Plot the confusion matrix\nplot_model(gbc, plot='confusion_matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune the model's hyperparameters\ntuned_gbc = tune_model(gbc)\n\n# Plot the tuned model's confusion matrix\nplot_model(tuned_gbc, plot='confusion_matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Decision tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create model for evaluation\ndtc = create_model('dt')\n\n# Plot the confusion matrix\nplot_model(dtc, plot='confusion_matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tune the model's hyperparameters\ntuned_dtc = tune_model(dtc)\n\n# Plot the confusion matrix\nplot_model(tuned_dtc, plot='confusion_matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, all selected models suffer to predict delays with accuracy. I've tested both CatBoost and Extreme Gradient Boosting outside of this scope and they respond just the same (you can do the same if you want to check for yourself).\n\nIt is also notable, that both LGBM and Gradient Boosting respond somewhat well to hyperparameters tuning, while Random Forest just classifies everything as won't have any delays and Decision Tree start to generalize towards this path.\n\nIt should be noted, though, that Decision Tree (with default parameters) have the best results towards predicting delays, but then suffer slightly to predict non delayed flights.\n\nMy approach will be to try and predict more accurately non delayed flights, as, with previous tests, I couldn't find a model that predicted the delays well. I'm going to use Gradient Boosting Classifier, as a colleague used this same method with PyCaret, as well, and choose to use LGBM."},{"metadata":{},"cell_type":"markdown","source":"## Data handling: normalization"},{"metadata":{},"cell_type":"markdown","source":"#### Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the column with truth values\ntrain_df = train_df.drop(['dep_delayed_15min'], axis=1)\n\n# Save columns names\natt = list(train_df.columns.values)\n\n# Normalize\ntrain_values = train_df.values\nscaler = preprocessing.MinMaxScaler()\nvalues_scaled = scaler.fit_transform(train_values)\n\n# Save to new dataframe\ntrain_scaled_df = pd.DataFrame(values_scaled,columns=att)\n\ntrain_scaled_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save columns names\ntest_att = list(test_df.columns.values)\n\n# Normalize\ntest_values = test_df.values\ntest_scaler = preprocessing.MinMaxScaler()\ntest_values_scaled = test_scaler.fit_transform(test_values)\n\n# Save to new dataframe\ntest_scaled_df = pd.DataFrame(test_values_scaled,columns=test_att)\n\ntest_scaled_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boost Classifier"},{"metadata":{},"cell_type":"markdown","source":"### Print tuned hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print tuned hyperparameters\nprint(tuned_gbc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train model using the whole training dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import gradient boost classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Save normalized train dataframe values to 'train_data' variable\ntrain_data = train_scaled_df.values\n\ngbc_model = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse',\n                                       init=None, learning_rate=0.036,\n                                       loss='deviance', max_depth=7,\n                                       max_features=1.0, max_leaf_nodes=None,\n                                       min_impurity_decrease=0,\n                                       min_impurity_split=None,\n                                       min_samples_leaf=5, min_samples_split=9,\n                                       min_weight_fraction_leaf=0.0,\n                                       n_estimators=190, n_iter_no_change=None,\n                                       presort='deprecated', subsample=0.3,\n                                       tol=0.0001, validation_fraction=0.1,\n                                       verbose=0, warm_start=False)\n\n# Fit data from training dataset\ngbc_model.fit(train_data, delayed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Try and predict the results with the above model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get values from scaled test dataframe\ntest_sdf_values = test_scaled_df.values\n\n# Predict values using the trained GB classifier\npredicted = gbc_model.predict(test_sdf_values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.Series(predicted)\nsubmission_df = submission_df.map(lambda label: 'N' if label==0 else 'Y')\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}