{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Datos de nutrición del restaurante Subway","metadata":{}},{"cell_type":"markdown","source":"# Ejercicio: Técnicas de Análisis\r\n\r\nEn esta práctica vamos a realizar un estudio de los indicadores nutricionales de los diferentes productos vendidos en Subway, la conocida empresa de comida rápida especializada en sandwiches y bocadillos.\r\n\r\nDatos fuente:\r\n\r\nhttps://www.kaggle.com/davinm/subway-restaurant-nutrition-data\r\n\r\nTambién se pueden ver directamente en su web oficial: https://www.subway.com/en-US/MenuNutrition/Nutrition/NutritionGrid","metadata":{}},{"cell_type":"markdown","source":"# Descripción de los datos\r\n\r\nEl conjunto de datos que vamos a utilizar contiene preparados según la receta estándar (pan de trigo de 9 granos con lechuga, tomates, cebollas, pimientos verdes y pepinos). La información nutricional de todos los bocadillos se basa en las recetas recomendadas por el chef. Dsiponemos de las siguientes caracerísticas para cada uno de los preparados:\r\n\r\n`Product`: Nombre del preparado.\r\n\r\n`Category`: Categoría a la que pertenece: Sandwich, Salad, Breakfast o 'Wrap', entre otros.\r\n\r\n`Serving Size (g)`: Peso del preparado.\r\n\r\n`Calories`: Calorías.\r\n\r\n`Total Fat (g)`: Grasas total.\r\n\r\n`Saturated Fat (g)`: Grasas saturadas.\r\n\r\n`Trans Fat (g)`: Ácido graso.\r\n\r\n`Cholesterol (mg)`: Colesterol.\r\n\r\n`Sodium (mg)`: Sodio.\r\n\r\n`Carbohydrates (g)`: Hidratos de carbono.\r\n\r\n`Dietary Fiber (g)`: Fibra.\r\n\r\n`Sugars (g)`: Azúcares.\r\n\r\n`Protein (g)`: Proteínas.\r\n\r\n`Vitamin A % DV`: Vitamina A.\r\n\r\n`Vitamin C % DV`: Vitamina C.\r\n\r\n`Calcium % DV`: Calcio.\r\n\r\n`Iron % DV`. Hierro.\r\n\r\n","metadata":{}},{"cell_type":"markdown","source":"![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)","metadata":{}},{"cell_type":"markdown","source":"# Importamos todas las librerías necesarias para realizar el estudio","metadata":{}},{"cell_type":"code","source":"import pandas as pd\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\n\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\n\r\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\r\nfrom sklearn.cluster import KMeans\r\nfrom sklearn.model_selection import KFold, RepeatedKFold, train_test_split\r\nfrom sklearn.neighbors import KNeighborsRegressor\r\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\r\nfrom sklearn.svm import SVR, SVC\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nfrom sklearn.metrics import precision_score\r\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\r\n\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:14.102862Z","iopub.execute_input":"2021-07-23T15:54:14.103294Z","iopub.status.idle":"2021-07-23T15:54:15.653035Z","shell.execute_reply.started":"2021-07-23T15:54:14.103209Z","shell.execute_reply":"2021-07-23T15:54:15.651927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)","metadata":{}},{"cell_type":"markdown","source":"# Análisis exploratorio","metadata":{}},{"cell_type":"markdown","source":"Importamos el dataset, en formato csv, a un dataframe de la librería Pandas.","metadata":{}},{"cell_type":"code","source":"subway = pd.read_csv('../input/subway-restaurant-nutrition-data/exported_data.csv', sep=\",\", dtype=str, encoding='iso-8859-1')\nsubway.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.654557Z","iopub.execute_input":"2021-07-23T15:54:15.654884Z","iopub.status.idle":"2021-07-23T15:54:15.662768Z","shell.execute_reply.started":"2021-07-23T15:54:15.654852Z","shell.execute_reply":"2021-07-23T15:54:15.659563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Renombramos la columna \"Unnamed: 0\", que contiene el nombre del preparado, por \"Product\".","metadata":{}},{"cell_type":"code","source":"subway = subway.rename(columns={'Unnamed: 0':'Product'})","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.663733Z","iopub.status.idle":"2021-07-23T15:54:15.66424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos cuántos productos distintos se preparan en el restaurante, y el número de características total (entre las que se encuentran el nombre del preparado y la categoría), como valores discretos (categorías), y sus características nutricionales (valores numéricos). Como se puede observar, disponemos de información de 135 preparados con 15 valores nutricionales asociados (especificados anteriormente en la descripción de los datos), además del nombre y la categoría mencionados.","metadata":{}},{"cell_type":"code","source":"subway.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.665686Z","iopub.status.idle":"2021-07-23T15:54:15.666469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Revisamos la distribución de preparados por categorías. ","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(subway['Category'].value_counts(dropna=False))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.667847Z","iopub.status.idle":"2021-07-23T15:54:15.668518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos que lo que la categoría que más variedad de preparados tiene son los sandwiches, seguido de los rollos (wraps) y las ensaladas.\r\n\r\nVemos la distribución en una gráfica de barras.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16, 7))\r\nsns.countplot(subway[\"Category\"])\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.669964Z","iopub.status.idle":"2021-07-23T15:54:15.670629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Valores ausentes\r\n\r\nComprobamos si en el dataset existen valores no informados en alguna de las características. Como se puede observar, no encontramos ninguno, por lo que no necesitamos realizar ninguna acción al respecto.","metadata":{}},{"cell_type":"code","source":"subway.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.671979Z","iopub.status.idle":"2021-07-23T15:54:15.672666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comprobamos el tipo de datos de cada característica:","metadata":{}},{"cell_type":"code","source":"subway.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.674155Z","iopub.status.idle":"2021-07-23T15:54:15.674794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Como se puede observar, a pesar de que tenemos valores numéricos en todas las características correspondientes a la información nutricional de los preparados, al importar los datos desde csv las columnas (series) del dataframe se crearon como objetos. Los convertimos:","metadata":{}},{"cell_type":"code","source":"subway['Serving Size (g)'] = subway['Serving Size (g)'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Calories'] = subway['Calories'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Total Fat (g)'] = subway['Total Fat (g)'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Saturated Fat (g)'] = subway['Saturated Fat (g)'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Trans Fat (g)'] = subway['Trans Fat (g)'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Cholesterol (mg)'] = subway['Cholesterol (mg)'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Sodium (mg)'] = subway['Sodium (mg)'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Carbohydrates (g)'] = subway['Carbohydrates (g)'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Dietary Fiber (g)'] = subway['Dietary Fiber (g)'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Sugars (g)'] = subway['Sugars (g)'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Protein (g)'] = subway['Protein (g)'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Vitamin A % DV'] = subway['Vitamin A % DV'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Vitamin C % DV'] = subway['Vitamin C % DV'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Calcium % DV'] = subway['Calcium % DV'].str.replace('.', '').str.replace(',', '.').astype(float)\r\nsubway['Iron % DV'] = subway['Iron % DV'].str.replace('.', '').str.replace(',', '.').astype(float)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.676373Z","iopub.status.idle":"2021-07-23T15:54:15.677118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Y comprobamos si el cambio ha surtido efecto:","metadata":{}},{"cell_type":"code","source":"subway.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.678418Z","iopub.status.idle":"2021-07-23T15:54:15.679055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para finalizar la parte de análisis exploratorio, mostramos los estadísticos más relevantes de los valores nutricionales:","metadata":{}},{"cell_type":"code","source":"subway.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.680338Z","iopub.status.idle":"2021-07-23T15:54:15.68097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Y vemos, por ejemplo, cómo se distribuyen las calorías en un diagrama de caja con un diagrama de enjambre superpuesto para cada una de las categorías.","metadata":{}},{"cell_type":"code","source":"subway_cal = subway[['Product', 'Category', 'Calories']].pivot(index=['Product'], columns='Category', values='Calories').reset_index()\r\n\r\nplt.figure(figsize=(20, 10))\r\nsns.boxplot(data=subway_cal)\r\nsns.stripplot(data=subway_cal, color='darkblue', alpha=0.7)\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.682222Z","iopub.status.idle":"2021-07-23T15:54:15.682855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Matriz de correlación\r\n\r\nLa matriz de correlación nos permite comprender la relación entre las diferentes características de nuestro conjunto de datos. Cada fila y columna representa una variable, y cada valor de esta matriz es el coeficiente de correlación entre las variables representadas por la fila y columna correspondientes.\r\n\r\nEl valor de cada casilla representa la correlación entre pares de variables:\r\n\r\n* Un valor positivo grande (cercano a 1,0) indica una fuerte correlación positiva, es decir, si el valor de una de las variables aumenta, el valor de la otra variable aumenta también.\r\n\r\n* Un valor negativo grande (cercano a -1,0) indica una fuerte correlación negativa, es decir, que el valor de una de las variables disminuye al aumentar el de la otra y viceversa.\r\n\r\n* Un valor cercano a 0 (tanto positivo como negativo) indica la ausencia de cualquier correlación entre las dos variables, y por lo tanto esas variables son independientes entre sí.\r\n\r\nEsta información es importante para el preprocesamiento en aprendizaje automático cuando se desea reducir la dimensionalidad de un dato de alta dimensión (no lo vemos aquí).\r\n","metadata":{}},{"cell_type":"code","source":"mat = subway[['Serving Size (g)', 'Calories', 'Total Fat (g)', 'Saturated Fat (g)', 'Trans Fat (g)'\r\n            , 'Cholesterol (mg)', 'Sodium (mg)', 'Carbohydrates (g)', 'Dietary Fiber (g)', 'Sugars (g)'\r\n            , 'Protein (g)', 'Vitamin A % DV', 'Vitamin C % DV', 'Calcium % DV', 'Iron % DV']].corr().abs()\r\n\r\nmask = np.triu(np.ones_like(mat, dtype=bool))\r\nmat_masked = mat.mask(mask)  # Pone a NaN todo lo que aparezca como True en la máscara\r\n\r\nfig, ax = plt.subplots(figsize=(10,7)) \r\nsns.heatmap(mat_masked, annot=True, ax=ax)\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.684163Z","iopub.status.idle":"2021-07-23T15:54:15.684796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Por poner un ejemplo, se puede observar una alta correlación en las calorías respecto al tamaño del preparado o a los carbohidratos, de un 75% y un 86%, respectivamente. Parece claro que cuanto más grande es un preparado o más hidratos de carbono tiene, la cantidad de calorías será mayor. Otro ejemplo podría ser la alta correlación entre proteína y colesterol.\r\n\r\nEste es el punto de partida, pero puede ocurrir que obtengamos valores que nos lleven a pensar que existe cierta relación pero que no sean consecuencia directa, sobre todo en conjuntos de datos pequeños, por lo que habría que estudiarlas en más profundidad.","metadata":{}},{"cell_type":"markdown","source":"Vemos ahora cómo correlacionan las calorías y el tamaño con los diferentes tipos de preparados.\r\n\r\nEn la creación de la matriz de correlación, las variables cualitativas no nos van a aportar información sobre su posible correlación con el resto de campos numéricos. Para poder manejarlo, hacemos las modificaciones necesarias para convertirlas en variables binarias, que sí nos ofrecen esa posibilidad.\r\n\r\nPara ello, creamos la función get_categories para pasar las posibles categorías de los preparados a columnas binarias.","metadata":{}},{"cell_type":"code","source":"def get_categories(data):\r\n\r\n    cat_Sandwich = False\r\n    cat_Salad = False\r\n    cat_Breakfast = False\r\n    cat_Extra = False\r\n    cat_Wrap = False\r\n    cat_Bread = False\r\n    cat_Cheese = False\r\n    cat_Extras = False\r\n    cat_Sauces = False\r\n    cat_Veggies = False\r\n    cat_Protein = False\r\n    cat_Seasonings = False\r\n\r\n    if data['Category'] == 'Sandwich':\r\n        cat_Sandwich = True\r\n    elif data['Category'] == 'Salad':\r\n        cat_Salad = True\r\n    elif data['Category'] == 'Breakfast':\r\n        cat_Breakfast = True\r\n    elif data['Category'] == 'Extra':\r\n        cat_Extra = True\r\n    elif data['Category'] == 'Wrap':\r\n        cat_Wrap = True\r\n    elif data['Category'] == 'Bread':\r\n        cat_Bread = True\r\n    elif data['Category'] == 'Cheese':\r\n        cat_Cheese = True\r\n    elif data['Category'] == 'Extras':\r\n        cat_Extras = True\r\n    elif data['Category'] == 'Sauces':\r\n        cat_Sauces = True\r\n    elif data['Category'] == 'Veggies':\r\n        cat_Veggies = True\r\n    elif data['Category'] == 'Protein':\r\n        cat_Protein = True\r\n    elif data['Category'] == 'Seasonings':\r\n        cat_Seasonings = True\r\n\r\n    return pd.Series([cat_Sandwich, cat_Salad, cat_Breakfast, cat_Extra, cat_Wrap, cat_Bread\r\n                    , cat_Cheese, cat_Extras, cat_Sauces, cat_Veggies, cat_Protein, cat_Seasonings])","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.68616Z","iopub.status.idle":"2021-07-23T15:54:15.68678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subway_cat = subway.copy()\r\nsubway_cat[['cat_Sandwich'\r\n          , 'cat_Salad'\r\n          , 'cat_Breakfast'\r\n          , 'cat_Extra'\r\n          , 'cat_Wrap'\r\n          , 'cat_Bread'\r\n          , 'cat_Cheese'\r\n          , 'cat_Extras'\r\n          , 'cat_Sauces'\r\n          , 'cat_Veggies'\r\n          , 'cat_Protein'\r\n          , 'cat_Seasonings']] = subway_cat.apply(get_categories, axis=1)\r\nsubway_cat[subway_cat['Category'] == 'Breakfast'].head()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.688211Z","iopub.status.idle":"2021-07-23T15:54:15.688841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subway_cat['Cal_per_g'] = subway_cat['Calories'] / subway_cat['Serving Size (g)']\r\n\r\nmat = subway_cat[['Cal_per_g'\r\n          , 'cat_Sandwich'\r\n          , 'cat_Salad'\r\n          , 'cat_Breakfast'\r\n          , 'cat_Extra'\r\n          , 'cat_Wrap'\r\n          , 'cat_Bread'\r\n          , 'cat_Cheese'\r\n          , 'cat_Extras'\r\n          , 'cat_Sauces'\r\n          , 'cat_Veggies'\r\n          , 'cat_Protein'\r\n          , 'cat_Seasonings']].corr().abs()\r\n\r\nmask = np.triu(np.ones_like(mat, dtype=bool))\r\nmat_masked = mat.mask(mask)  # Pone a NaN todo lo que aparezca como True en la máscara\r\n\r\nfig, ax = plt.subplots(figsize=(10,7)) \r\nsns.heatmap(mat_masked, annot=True, ax=ax)\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.690161Z","iopub.status.idle":"2021-07-23T15:54:15.69078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos destacar que las categorías 'Salad' y 'Veggies' son las que mayor relación tienen con las calorías, lo que es bastante sorprendente.","metadata":{}},{"cell_type":"markdown","source":"![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)","metadata":{}},{"cell_type":"markdown","source":"# Clustering\r\n\r\nEn este apartado aplicaremos una estrategia de clustering al conjunto de datos con el objetivo de descubrir nuevas relaciones entre las columnas.\r\n\r\nPara ello emplearemos el algoritmo Kmeans el cual trabaja iterativamente para asignar a cada muestra uno de los “K” grupos basado en sus características. Son agrupados en base a la similitud de sus columnas.\r\n\r\nLos grupos se van definiendo de manera “orgánica”, es decir que se va ajustando su posición en cada iteración del proceso, hasta que converge el algoritmo. Una vez hallados los centroides debemos analizarlos para ver cuales son sus características únicas, frente a la de los otros grupos. Estos grupos son las etiquetas que genera el algoritmo.\r\n\r\nPara reducir la complejidad del problema reduciremos el número de columnas del dataset.","metadata":{}},{"cell_type":"code","source":"# Crear instancia del encoder\r\nlabelencoder = LabelEncoder()\r\n\r\nclustering_dataset = subway.copy()\r\n\r\nsns.pairplot(clustering_dataset, hue='Category',size=4,vars=['Calories', 'Cholesterol (mg)', 'Carbohydrates (g)', 'Sugars (g)'],kind='scatter')\r\n","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.692116Z","iopub.status.idle":"2021-07-23T15:54:15.692759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para seleccionar el número de clusters empleamos la regla del codo, la cual determina que el número óptimo de clusters se encuentra en el punto donde la curva deja de mejorar notablemente:","metadata":{}},{"cell_type":"code","source":"clustering_dataset['Category'] = labelencoder.fit_transform(subway['Category'])\r\n\r\nX = np.array(clustering_dataset[['Calories', 'Cholesterol (mg)', 'Carbohydrates (g)', 'Sugars (g)']])\r\ny = np.array(clustering_dataset['Category'])\r\n\r\nN_clusters = range(1, 10)\r\nkmeans = [KMeans(n_clusters=i) for i in N_clusters]\r\nscore = [kmeans[i].fit(X).score(X) for i in range(len(kmeans))]\r\n\r\nplt.plot(N_clusters,score)\r\nplt.xlabel('Número del clusters')\r\nplt.ylabel('Score')\r\nplt.title('Curva del codo')\r\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.693925Z","iopub.status.idle":"2021-07-23T15:54:15.694578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Elegimos K = 5, por lo que dividiremos el conjunto de datos en cinco clústers.\r\n\r\nVemos algunas de sus características en 3D:","metadata":{}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=5).fit(X)\r\nlabels = kmeans.predict(X)\r\n\r\nC = kmeans.cluster_centers_\r\ncolors=[\"orange\",\"green\",\"blue\", \"red\", \"brown\"]\r\nvalues=[]\r\nfor row in labels:\r\n    values.append(colors[row])\r\n\r\nfig = plt.figure()\r\nax = Axes3D(fig)\r\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=values,s=60)\r\nax.scatter(C[:, 0], C[:, 1], C[:, 2], marker='.', c=colors, s=1000)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.695718Z","iopub.status.idle":"2021-07-23T15:54:15.696371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Si agregamos el clúster al que pertenece cada clase al dataset, quedaría de la siguiente manera:","metadata":{}},{"cell_type":"code","source":"clustering_dataset['Class']  = labels\r\nclustering_dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.697715Z","iopub.status.idle":"2021-07-23T15:54:15.698372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)","metadata":{}},{"cell_type":"markdown","source":"# Regresión","metadata":{}},{"cell_type":"markdown","source":"Un modelo de regresión es un modelo matemático que busca determinar la relación entre una variable dependiente (Y), con respecto a otras variables, llamadas explicativas o independientes (X).\r\n\r\nExisten pocos datos en nuestro dataset, por lo que vamos a utilizar Cross Validation con el fin de dividir los datos disponibles en subconjuntos, de modo que el ajuste y la predicción del modelo se puedan realizar en subconjuntos independientes. Debido a la baja cantidad de preparados, las pruebas con KFold() finalizaban a menudo sin llegar a converger a una solución adecuada, por lo que se ha optado en realizar K-Fold varias veces mediante RepeatedKFold() para conseguir dicha convergencia.\r\n\r\nAplicamos la regresión sobre la característica \"Calories\". Intentamos predecir qué cantidad de calorías tendrá un producto a partir del resto de valores nutricionales.\r\n\r\nPara evaluar los modelos utilizamos la métrica de \"Varianza capturada\" ($R^2$), que nos proporciona la cantidad de variabilidad que tiene el modelo. Un $R^2 = 1$ se considera una predicción perfecta.\r\n\r\nLos algoritmos de regresión aplicados son los siguientes:\r\n\r\n- `KNeighborsRegressor`: Regresión basada en los K vecinos más cercanos.\r\n- `LinearRegression`: Consiste en la obtención de la función de distribución lineal.\r\n- `SVR`: Implementación basada en máquinas de vector soporte.\r\n- `RandomForestRegressor`: Basado en hacer varios DecisionTrees(es un Ensemble). Se consigue mediante bootstrappingy selección aleatoria de dimensiones para cada uno de los árboles.\r\n","metadata":{}},{"cell_type":"code","source":"#Hacemos copia auxiliar para no tener que leer todo el tiempo el mismo conjunto de datos\r\naux = subway.drop(columns=[\"Product\", \"Category\"])\r\naux.dropna(axis=0, inplace=True)\r\naux = aux.reset_index(drop=True)\r\n\r\n#Preparamos las etiquetas y las eliminamos \r\nlabels = subway[\"Calories\"]\r\naux.drop([\"Calories\"], axis=1, inplace=True)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(aux, labels, test_size=0.2, random_state=33)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.699625Z","iopub.status.idle":"2021-07-23T15:54:15.700278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_SPLITS = 5\r\nN_REPEATS = 10\r\n\r\n#Declaramos cross validation\r\ncv = RepeatedKFold(n_splits=N_SPLITS, n_repeats = N_REPEATS) \r\nprint(cv)\r\n\r\n# Variable acumuladora (cuatro posiciones, una por modelo)\r\nmean_score = [0.0, 0.0, 0.0, 0.0]\r\nfor train_index, test_index in cv.split(X_train):\r\n\r\n    # Seleccionamos los datos\r\n    x_train_cv, y_train_cv = X_train.iloc[train_index], y_train.iloc[train_index]\r\n    x_test_cv, y_test_cv = X_train.iloc[test_index], y_train.iloc[test_index]\r\n\r\n    # Declaramos los modelos\r\n    model_KNN = KNeighborsRegressor(n_neighbors=2)\r\n    model_LiR = LinearRegression()\r\n    model_SVR = SVR(max_iter=1000, C=50, kernel=\"poly\", coef0=2)\r\n    model_RF = RandomForestRegressor(criterion=\"mse\", bootstrap=True)\r\n\r\n    # Entrenamos los modelos\r\n    model_KNN.fit(x_train_cv, y_train_cv)\r\n    model_LiR.fit(x_train_cv, y_train_cv)\r\n    model_SVR.fit(x_train_cv, y_train_cv)\r\n    model_RF.fit(x_train_cv, y_train_cv)\r\n\r\n    # Sumamos el resultado de la iteración\r\n    mean_score[0] = mean_score[0] + model_KNN.score(x_test_cv,y_test_cv)\r\n    mean_score[1] = mean_score[1] + model_LiR.score(x_test_cv,y_test_cv)\r\n    mean_score[2] = mean_score[2] + model_SVR.score(x_test_cv,y_test_cv)\r\n    mean_score[3] = mean_score[3] + model_RF.score(x_test_cv,y_test_cv)\r\n\r\n    #print(\"Valor R2 (KNeighborsRegressor): \", model_KNN.score(x_test,y_test))\r\n    #print(\"Valor R2 (LinearRegression): \", model_LiR.score(x_test,y_test))\r\n    #print(\"Valor R2 (SVR): \", model_SVR.score(x_test,y_test))\r\n    #print(\"Valor R2 (RandomForestRegressor): \", model_RF.score(x_test,y_test))\r\n","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.701567Z","iopub.status.idle":"2021-07-23T15:54:15.702222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vemos los resultados obtenidos para cada uno de los algoritmos de regresión:","metadata":{}},{"cell_type":"code","source":"#Obtenemos la media de los resultados\r\nprint(\"RESULTADOS TRAIN:\")\r\nprint(\"Resultado R2 (promedio) con Cross validation (KNeighborsRegressor): \", np.round(mean_score[0]/(N_SPLITS * N_REPEATS),2))\r\nprint(\"Resultado R2 (promedio) con Cross validation (LinearRegression): \", np.round(mean_score[1]/(N_SPLITS * N_REPEATS),2))\r\nprint(\"Resultado R2 (promedio) con Cross validation (SVR): \", np.round(mean_score[2]/(N_SPLITS * N_REPEATS),2))\r\nprint(\"Resultado R2 (promedio) con Cross validation (RandomForestRegressor): \", np.round(mean_score[3]/(N_SPLITS * N_REPEATS),2))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.703496Z","iopub.status.idle":"2021-07-23T15:54:15.704313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El modelo que mejor consigue capturar la varianza para el conjunto de datos de entrenamiento es `LinearRegression`, con un 92% de explicabilidad de las calorías respecto al resto de valores nutricionales.","metadata":{}},{"cell_type":"code","source":"#Obtenemos las puntuaciones (R2) de los resultados de las predicciones\r\nprint(\"RESULTADOS TEST (datos no vistos previamente):\")\r\nprint(\"Resultado R2 (promedio) con Cross validation (KNeighborsRegressor): \", np.round(model_KNN.score(X_test,y_test),2))\r\nprint(\"Resultado R2 (promedio) con Cross validation (LinearRegression): \", np.round(model_LiR.score(X_test,y_test),2))\r\nprint(\"Resultado R2 (promedio) con Cross validation (SVR): \", np.round(model_SVR.score(X_test,y_test),2))\r\nprint(\"Resultado R2 (promedio) con Cross validation (RandomForestRegressor): \", np.round(model_RF.score(X_test,y_test),2))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.705657Z","iopub.status.idle":"2021-07-23T15:54:15.706345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"El mejor modelo se mantiene si los aplicamos sobre el conjunto de datos de test (datos no vistos previamente por el modelo). El `LinearRegression`, con un 94% de explicabilidad. Sin embargo, en este caso `SVR` adelanta a `RandomForestRegressor`, que realiza mejores predicciones sobre estos datos. En ambos casos, `KNeighborsRegressor` es el que más errores comete.","metadata":{}},{"cell_type":"markdown","source":"![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)","metadata":{}},{"cell_type":"markdown","source":"# Clasificación\r\n\r\n","metadata":{}},{"cell_type":"markdown","source":"Cuando lo que pretendemos es identificar una clase o etiqueta asociada a una instancia, utilzamos modelos de clasificación.\r\n\r\nEn este caso utilizamos las métricas de accuracy y precision:\r\n- `accuracy`: Todas las predicciones correctas respecto al total: (TP+TN)/(TP+FP+FN+TN)\r\n- `precision`: Cuántos positivos son reales: TP/(TP+FP)\r\n\r\nLos algoritmos de regresión aplicados son los siguientes:\r\n\r\n- `LogisticRegression`: Regresión logística. Consiste en convertir una regresión lineal en una curva logarítmica. Utiliza un algoritmo OvR (One-vs-Rest) para el entrenamiento.\r\n- `SVC`: Implementación basada en máquinas de vector soporte para clasificación. Se apoyan en las muestras para crear un margen que maximice la separación entre clases.\r\n","metadata":{}},{"cell_type":"code","source":"#Hacemos copia auxiliar para no tener que leer todo el tiempo el mismo conjunto de datos\r\naux = subway.drop(columns=[\"Product\"])\r\n\r\n# Separamos etiquetas de dimensiones\r\nlabels = aux[\"Category\"]\r\naux = aux.drop(columns=[\"Category\"])\r\n\r\n#Escalamos los datos\r\nscaler = MinMaxScaler()\r\naux = pd.DataFrame(scaler.fit_transform(aux), columns=aux.columns)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(aux, labels, test_size=0.2, random_state=33)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.707193Z","iopub.status.idle":"2021-07-23T15:54:15.707598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_SPLITS = 5\r\nN_REPEATS = 10\r\n\r\n#Declaramos cross validation\r\ncv = RepeatedKFold(n_splits=N_SPLITS, n_repeats = N_REPEATS) \r\n\r\n#Para cada fold\r\n#Variable acumuladora\r\nmean_score = [0.0, 0.0]\r\niteration = 0\r\nprecision = [0, 0]\r\nfor train_index, test_index in cv.split(X_train):\r\n     \r\n     iteration = iteration + 1\r\n\r\n     #Seleccionamos los datos\r\n     x_train_cv, y_train_cv = X_train.iloc[train_index], y_train.iloc[train_index]\r\n     x_test_cv, y_test_cv = X_train.iloc[test_index], y_train.iloc[test_index]\r\n\r\n     #Declaramos modelo\r\n     model_LR = LogisticRegression(C=100, max_iter=10000)\r\n     model_SVC = SVC(gamma=\"auto\", C=10000)\r\n\r\n     model_LR.fit(x_train_cv, y_train_cv)\r\n     model_SVC.fit(x_train_cv, y_train_cv)\r\n\r\n     mean_score[0] = mean_score[0] + model_LR.score(x_test_cv, y_test_cv)\r\n     mean_score[1] = mean_score[1] + model_SVC.score(x_test_cv, y_test_cv)\r\n\r\n     # print(\"Iteration \", iteration)\r\n     # print(\"Accuracy (LogisticRegression): \", np.round(model_LR.score(x_test_cv,y_test_cv),2))\r\n     # print(\"Accuracy (SVC): \", np.round(model_SVC.score(x_test_cv,y_test_cv),2))\r\n\r\n     # Calculamos precision en cada fold\r\n     y_predicted_LR = model_LR.predict(x_test_cv)\r\n     y_predicted_SVC = model_SVC.predict(x_test_cv)\r\n\r\n     # Con precision_score podemos saber la precisión para cada una de las clases\r\n     precision[0] = precision[0] + precision_score(y_test_cv, y_predicted_LR, average='weighted', labels=list(subway['Category'].unique()))\r\n     precision[1] = precision[1] + precision_score(y_test_cv, y_predicted_SVC, average='weighted', labels=list(subway['Category'].unique()))\r\n\r\n     # print(\"Precission (LogisticRegression): \", precision[0])\r\n     # print(\"Precission (SVC): \", precision[1])\r\n     # print(\"-----------------\")\r\n\r\n\r\n# Obtenemos la media de los resultados\r\n# Accuracy:\r\nresultado_LR_acc = np.round(mean_score[0]/(N_SPLITS * N_REPEATS),2)\r\nresultado_SVC_acc = np.round(mean_score[1]/(N_SPLITS * N_REPEATS),2)\r\n# Precision:\r\nresultado_LR_pre = np.round(precision[0]/(N_SPLITS * N_REPEATS),2)\r\nresultado_SVC_pre = np.round(precision[1]/(N_SPLITS * N_REPEATS),2)\r\n\r\n# E imprimimos\r\nprint(\"RESULTADOS TRAIN:\")\r\n# Accuracy:\r\nprint(\"Accuracy con Cross validation (LogisticRegression): \", resultado_LR_acc) \r\nprint(\"Precision con Cross validation (LogisticRegression): \", resultado_LR_pre)  \r\n# Precision:\r\nprint(\"Accuracy con Cross validation (SVC): \", resultado_SVC_acc)    \r\nprint(\"Precision con Cross validation (SVC): \", resultado_SVC_pre)   \r\n ","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.708891Z","iopub.status.idle":"2021-07-23T15:54:15.709345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obtenemos métricas similares para ambos modelos, manteniendo un accuracy y una precisión en torno al 80%.","metadata":{}},{"cell_type":"code","source":"#Obtenemos las puntuaciones (R2) de los resultados de las predicciones\r\nprint(\"RESULTADOS TEST (datos no vistos previamente):\")\r\nprint(\"Accuracy con Cross validation (LogisticRegression): \", np.round(model_LR.score(X_test,y_test),2))\r\nprint(\"Accuracy con Cross validation (SVC): \", np.round(model_SVC.score(X_test,y_test),2))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.710339Z","iopub.status.idle":"2021-07-23T15:54:15.71074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para datos no vistos en el entrenamiento, el modelo `SVC` mantiene un accuracy similar al obtenido respecto a los datos de entrenamiento, por lo que el modelo ha aprendido a generalizar correctamente. Sin embargo, el accuracy del modelo de regresión logística ha descendido en aproximadamente un 10%, por lo que es superado por `SVC`. ","metadata":{}},{"cell_type":"markdown","source":"![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)","metadata":{}},{"cell_type":"markdown","source":"# Ensembles\r\n\r\nSon combinaciones de modelos de aprendizaje simples que mejoran el rendimiento. Debemos tener en cuenta cuánto penaliza la combinación de estos modelos al tiempo de predicción (en paralelo si son completamente independientes unos de otros) respecto a los modelos simples. El esfuerzo adicional está justificado solo si el rendimiento es considerablemente mejor que el de los modelos simples.\r\n\r\nPreparamos el conjunto de datos:\r\n","metadata":{}},{"cell_type":"code","source":"#Hacemos copia auxiliar para no tener que leer todo el tiempo el mismo conjunto de datos\r\naux = subway.drop(columns=[\"Product\"])\r\n\r\n# Separamos etiquetas de dimensiones\r\nlabels = aux[\"Category\"]\r\naux = aux.drop(columns=[\"Category\"])\r\n\r\n#Escalamos los datos\r\nscaler = MinMaxScaler()\r\naux = pd.DataFrame(scaler.fit_transform(aux), columns=aux.columns)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(aux, labels, test_size=0.2, random_state=33)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.711706Z","iopub.status.idle":"2021-07-23T15:54:15.712138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=X_train[['Calories', 'Carbohydrates (g)']]\r\ny=y_train.to_numpy()\r\n\r\nsc = StandardScaler()\r\nsc.fit(X)\r\nX_scaled = pd.DataFrame(sc.transform(X),columns = X.columns)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.713212Z","iopub.status.idle":"2021-07-23T15:54:15.713609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Y probamos con bagging y boosting:","metadata":{}},{"cell_type":"markdown","source":"## Bagging\r\n\r\nCuando usamos bagging, también combinamos varios modelos de machine learning. La forma de conseguir que los errores se compensen entre sí, es que cada modelo se entrena con subconjuntos del conjunto de entrenamiento. Estos subconjuntos se forman eligiendo muestras aleatoriamente (con repetición) del conjunto de entrenamiento.\r\n\r\nLos modelos que vamos a combinar son de tipo `DecisionTreeClassifier`: crean una estructura de árbol basada en reglas lógicas. Parten de un nodo raíz (el que mejor separación aporte), hasta que llegan a las hojas, donde se tiene la etiqueta. Podemos especificar diferentes parámetros, donde dos de los más críticos son la profundidad máxima de los árboles, y el número máximo de nodos por hoja.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\r\n\r\nnum_models=10\r\nbagging = BaggingClassifier(DecisionTreeClassifier(max_depth=10, max_leaf_nodes=10),n_estimators=num_models,max_samples=0.5, max_features=0.5)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.714675Z","iopub.status.idle":"2021-07-23T15:54:15.715119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = bagging.fit(X_scaled.to_numpy(), y)\r\n\r\ntrain_err = (clf.predict(X_scaled.to_numpy()) != y).mean()\r\nprint(f'Train error: {train_err:.1%}')","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.716303Z","iopub.status.idle":"2021-07-23T15:54:15.716714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En este caso, cambiar la profundidad no nos ayuda demasiado a reducir el error. Lo que sí se ha notado es que aumentar el número de hojas hasta seis nos ha permitido reducir el error, aunque seguir aumentando el número máximo una vez alcanzado este punto ya no proporciona una mejora sustancial.\r\n\r\nVisualizamos los árboles generados y las distintas lógicas aplicadas en cada uno de ellos:","metadata":{}},{"cell_type":"code","source":"nrows=int(num_models/2)\r\nncols = 2\r\n\r\nfig, axes = plt.subplots(figsize=(8, num_models*3),\r\n                             nrows=nrows,\r\n                             ncols=ncols,\r\n                             sharex=True,\r\n                             dpi=100)\r\nfor r in range(nrows):\r\n  for c in range(ncols):\r\n    i=r*ncols+c\r\n    plot_tree(bagging.estimators_[i], class_names=list(subway['Category'].unique()), filled=True,ax=axes[r][c])\r\n    axes[r][c].title.set_text(\"Model \" + str (i) + \" with feature \" + str(bagging.estimators_features_[i][0]))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.717865Z","iopub.status.idle":"2021-07-23T15:54:15.718328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para finalizar el análisis del ensemble, visualilzamos la matriz de confusión y observamos las métricas obtenidas.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\r\n\r\npredictions = bagging.predict(X_test[['Calories', 'Carbohydrates (g)']])\r\n\r\nprint(\"Confusion Matrix:\")\r\nprint(confusion_matrix(y_test, predictions))\r\n\r\nprint(\"Classification Report\")\r\nprint(classification_report(y_test, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.719499Z","iopub.status.idle":"2021-07-23T15:54:15.719926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No hemos conseguido mejorar las predicciones que mediante la utilización de modelos simples. El ensemble ha aprendido a predecir sandwiches sobre todo. Sabemos que la categoría Sandwich es la que más se repite en el conjunto de datos, por lo que claramente se ha producido overfitting y las métricas obtenidas son bajas. \r\n\r\nPara solucionarlo, debemos aumentar las muestras o balancear el conjunto de datos. Sin embargo, tenemos categorías para las que solamente existe un producto, y el balanceo aquí se hace complicado.","metadata":{}},{"cell_type":"markdown","source":"## Boosting","metadata":{}},{"cell_type":"markdown","source":"En el boosting, cada modelo intenta arreglar los errores de los modelos anteriores. Por ejemplo, en el caso de clasificación, el primer modelo tratará de aprender la relación entre los atributos de entrada y el resultado. Seguramente cometerá algunos errores. Así que el segundo modelo intentará reducir estos errores. Esto se consigue dándole más peso a las muestras mal clasificadas y menos peso a las muestras bien clasificadas. ","metadata":{}},{"cell_type":"code","source":"X=X_train[['Calories', 'Carbohydrates (g)']]\r\ny=y_train.to_numpy()\r\n\r\nsc = StandardScaler()\r\nsc.fit(X)\r\nX_scaled = pd.DataFrame(sc.transform(X),columns = X.columns)","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.720853Z","iopub.status.idle":"2021-07-23T15:54:15.721277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\r\n\r\ngradientboost = GradientBoostingClassifier(n_estimators=100,learning_rate=1, max_depth=1, random_state=0).fit(X_scaled, y)\r\n\r\ntrain_err = (gradientboost.predict(X_scaled.to_numpy()) != y).mean()\r\nprint(f'Train error: {train_err:.1%}')","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.722384Z","iopub.status.idle":"2021-07-23T15:54:15.722794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\r\n\r\npredictions = gradientboost.predict(X_test[['Calories', 'Carbohydrates (g)']])\r\n\r\nprint(\"Confusion Matrix:\")\r\nprint(confusion_matrix(y_test, predictions))\r\n\r\nprint(\"Classification Report\")\r\nprint(classification_report(y_test, predictions))","metadata":{"execution":{"iopub.status.busy":"2021-07-23T15:54:15.723829Z","iopub.status.idle":"2021-07-23T15:54:15.724245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Al igual que ocurría con bagging, los resultados no son todo lo buenos que esperábamos. El ensemble ha acabado con overfitting y necesitamos balancear o aumentar el conjunto de datos de entrenamiento para realizar mejores predicciones.","metadata":{}},{"cell_type":"markdown","source":"# Conclusiones\r\n\r\nHemos analizado el conjunto de datos y las relaciones entre sus características, y hemos logrado obtener cinco clústers o segmentos que nos van a permitir realizar análisis más específicos (líneas futuras).\r\n\r\nHemos logrado predecir las calorías de los preparados en función del resto de propiedades nutricionales de los preparados con resultados satisfactorios. Sin embargo, el conjunto de datos es bastante limitado y se hace difícil clasificar las categorías a las que pertenecen dichos preparados. Hemos visto que la relación de los valores nutricionales con las categorías es mucho menor que la obtenida con las calorías.\r\n\r\nPor último, hemos realizado el ejercicio de combinar modelos simples de clasificación (ensembles) para intentar mejorar los resultados pobres en la predicción de la categoría, con resultados de nuevo poco convincentes. Como se ha comentado, necesitaríamos aumentar el número de muestras o añadir características adicionales para conseguir un mejor resultado.\r\n","metadata":{}}]}