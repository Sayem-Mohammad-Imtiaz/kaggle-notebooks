{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Let's do some basics import and CountVectorizer so to call the transform() function on one or more documents as needed to encode each as a vector\nfrom os import path\nfrom pandas import DataFrame\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's import some NLP modules such as PorterStemmer, SnowballStemmer, WordNetLemmetizer\n# download vader_lexicon, and stopwords\n\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import LancasterStemmer\nfrom nltk.stem import WordNetLemmatizer    # Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nnltk.download('vader_lexicon')\nnltk.download('stopwords')\nnltk.download('wordnet')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's import some visualization modules\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import style\nimport matplotlib.colors","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import wordcloud   # Sentiment-based Word Clouds\nfrom wordcloud import WordCloud, STOPWORDS \nfrom PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change and set directory to kaggle/input\n\nos.chdir('/kaggle/input')\nos.getcwd()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's read IMDB Dataset and store it into a dataframe \"df\"\n\ndf=pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv',header=0,error_bad_lines=True,encoding='utf8')\n\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at our table\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's define a function \"sc\" to run sentimental analysis on the text \"review\" and return the compound value (-1 to +1)\ndef sc(x):\n    score=SentimentIntensityAnalyzer().polarity_scores(x)\n    return score['compound']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Let's apply the compound score of our sentimental analysis to \"review\" storing the results in a new column \"SentScore\" through \n# map function\n\ndf[\"SentScore\"]=df[\"review\"].map(sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's look at our updated table \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's define a function \"sc\" to run sentimental analysis on the text \"review\" and return the compound value (-1 to +1)\n\n\ndef sca(lb):\n    if lb >= .6:\n        return \"Very Good\"\n    elif (lb > .2) and (lb < .6):\n        return \"Good\"\n    elif (lb > -.2) and (lb < .2):\n        return \"Average\"\n    elif (lb > -.6) and (lb < -.2):\n        return \"Disappointing\"\n     \n    else:\n        return \"Regrettable\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we insert a column to indicate the class of the review (\"Very Good\" , \"Good\", \"Average\", \"Disappointing\", \"Regrettable\")\n\ndf[\"SentClass\"]=df[\"SentScore\"].map(sca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check our updated table\n\ndf.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We define a function for which relatively to the \"sentiment\" column, positive=1 | negative=0\n\ndef num(lb):\n    if lb == 'positive':\n        return 1   \n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's create a new column \"sentiment_bin\" applying the function above using .map\n\ndf[\"sentiment_bin\"]=df[\"sentiment\"].map(num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the updated table\n\ndf.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similarly to what we did above, for the SentScore results (-1 to +1) we define a function for which a value >= 0 equals 1(positive), else 0(negative)\n\ndef numscore(lb):\n    if lb >= 0:\n        return 1     \n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's create a new column \"SentScore_bin\" applying the function above using .map\n\ndf[\"SentScore_bin\"]=df[\"SentScore\"].map(numscore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the updated table\n\ndf.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's do now some TEXT ADJUSTMENTS / CLEANING","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make text lower case\ndf[\"review\"]  = df[\"review\"].str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove digits from text\ndef Remove_digit(text):\n    result = re.sub(r\"\\d\", \"\", text)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove HTML from text\ndef remove_html(text):\n    result = re.sub(r'<.*?>','',text) # Find out anything that is in between < & > symbol \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove special text characters\ndef remove_spl(text):\n    result = re.sub(r'\\W',' ',text) \n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Link words with similar meaning to one word (in context)\ndef lem_word(text):\n    result= WordNetLemmatizer().lemmatize(text)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's apply all of the above functions to the text column \"review\"\n\ndf[\"review\"]  = df[\"review\"].apply(Remove_digit)\ndf[\"review\"]  = df[\"review\"].apply(remove_html)\ndf[\"review\"]  = df[\"review\"].apply(remove_spl)\ndf[\"review\"]  = df[\"review\"].apply(lem_word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the updated table\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's store the adjusted text to the object 'corpus1' and transform it into a List\ncorpus1=df['review'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create an object \"corpus\" that includes the first 1000 values of the list 'corpus1', otherwise the machine could take too long to run the command\n\ncorpus=corpus1[ :1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count Vectorisation\n# I have defined ngram range to be unigrams and bigrams (it starts with one word and goes up to two when vectorizing)\n\nfrom sklearn.feature_extraction import text\n\ncv = text.CountVectorizer(input=corpus,ngram_range=(1,2),stop_words='english')\nmatrix = cv.fit_transform(corpus)\n\n# I am converting the matrix_cv into a dataframe \ncorpus2 = pd.DataFrame(matrix.toarray(), columns=cv.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a snapshot at the data\ncorpus2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One thing to notice here is the dimension of this data\n# We have 1000 documents (rows) which is consistent with the selected amount of rows of our list corpus \n# and 110012 columns which is humangous. We have a created a giant matrix\n\n# It is noticeable that many features contain 0, since not all words willbe present across documents of the corpus(2)\n\ncorpus2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TF-IDF, Term Frequency and Inverse Document Freq\n# We run a TF-IDF representation on the same corpus, same like before and also this time removing the english stop_words\n\n\ntf = text.TfidfVectorizer(input=corpus, ngram_range=(1,2),stop_words='english')\n\nmatrix1 = tf.fit_transform(corpus)\n\n# I am converting the matrix1 into a dataframe X\nX = pd.DataFrame(matrix1.toarray(), columns=tf.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at our matrix X\n\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's set our y to be the first 1000 values of the column SentScore_bin (based on our sentiment analysis)\n\ny = df['SentScore_bin'][:1000].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at the array 'y'\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to try and run the RandomForest Classifier on X= vectorized matrix and y= SentScore_bin\n# using the fit transformation of the tf - idf matrix to array =X\n\n# Let's split X and y in training and testing data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's set the RandomForestClassifier and set the parameters\n# Let's fit the model on X and y training data\n\nfrom sklearn.ensemble import RandomForestClassifier\ntext_classifier=RandomForestClassifier(bootstrap=False, criterion=\"gini\", max_features=0.3, min_samples_leaf=4, min_samples_split=9, n_estimators=100)\ntext_classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's run the prediction on the X test data and store them into the object 'predictions'\n\npredictions = text_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can see that running the RANDOM FOREST CLASSIFIER we get an accuracy score of 68%. NOT amazing!\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n \nprint(confusion_matrix(y_test,predictions))  \nprint(classification_report(y_test,predictions))  \nprint(accuracy_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try LOGISTIC REGRESSION","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training the model\nlr=LogisticRegression(C=1.0,class_weight=None,dual=False,fit_intercept=True,intercept_scaling=1,l1_ratio=None,max_iter=100,\nmulti_class='auto',n_jobs=None,penalty='l2',random_state=23,solver='lbfgs',tol=0.0001,verbose=0,warm_start=False)\n#Fitting the model for tfidf features\nlr_tfidf=lr.fit(X_train,y_train)\nprint(lr_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Predicting the model for tfidf features\nlr_tfidf_predict=lr.predict(X_test)\nprint(lr_tfidf_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy score running a LOGISTIC REGRESSION is pretty low.....only 61.5%!\n\n#Accuracy score for tfidf features\nlr_tfidf_score=accuracy_score(y_test,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification report for tfidf features\nlr_tfidf_report=classification_report(y_test,lr_tfidf_predict,target_names=['0','1'])\nprint(lr_tfidf_report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GRADIENT BOOSTING CLASSIFIER","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf=GradientBoostingClassifier(n_estimators=80,random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nmod=GridSearchCV(clf,param_grid={'n_estimators': [80,100,120,140,160]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf=GradientBoostingClassifier(n_estimators=100,random_state=23)\nclf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_imp=pd.Series(clf.feature_importances_)\nfeature_imp.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's now repeat the operations setting the first 1000 values of our column 'sentiment_bin' as our \"y\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df['sentiment_bin'][:1000].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We split again in training and test data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RandomForestClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\ntext_classifier=RandomForestClassifier(bootstrap=False, criterion=\"gini\", max_features=0.3, min_samples_leaf=4, min_samples_split=9, n_estimators=100)\ntext_classifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = text_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It seems to be more accurate with a score of 78%!\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n \nprint(confusion_matrix(y_test,predictions))  \nprint(classification_report(y_test,predictions))  \nprint(accuracy_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try again LOGISTIC REGRESSION\n\nfrom sklearn.linear_model import LogisticRegression\n#training the model\nlr=LogisticRegression(C=1.0,class_weight=None,dual=False,fit_intercept=True,intercept_scaling=1,l1_ratio=None,max_iter=100,\nmulti_class='auto',n_jobs=None,penalty='l2',random_state=23,solver='lbfgs',tol=0.0001,verbose=0,warm_start=False)\n#Fitting the model for tfidf features\nlr_tfidf=lr.fit(X_train,y_train)\nprint(lr_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Predicting the model for tfidf features\nlr_tfidf_predict=lr.predict(X_test)\nprint(lr_tfidf_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic regression with y=sentiment_bin gives us the highest accuracy----81%! Not bad\n\n#Accuracy score for tfidf features\nlr_tfidf_score=accuracy_score(y_test,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification report for tfidf features\nlr_tfidf_report=classification_report(y_test,lr_tfidf_predict,target_names=['0','1'])\nprint(lr_tfidf_report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try with the GRADIENT BOOSTING CLASSIFIER\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf=GradientBoostingClassifier(n_estimators=80,random_state=23)\nclf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nmod=GridSearchCV(clf,param_grid={'n_estimators': [80,100]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n                           learning_rate=0.1, loss='deviance', max_depth=3,\n                           max_features=None, max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=2,\n                           min_weight_fraction_leaf=0.0, n_estimators=100,\n                           n_iter_no_change=None, presort='deprecated',\n                           random_state=23, subsample=1.0, tol=0.0001,\n                           validation_fraction=0.1, verbose=0,\n                           warm_start=False)\nclf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(X_test,y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}