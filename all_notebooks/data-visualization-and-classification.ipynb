{"cells":[{"metadata":{"_cell_guid":"ed563d85-5b61-4383-8c73-b8a5fbc933ce","_uuid":"e5609efbc04fb45bfceb8591df49cb9ebb56e02e","collapsed":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom sklearn import svm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nimport random\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n#import data\ndata = pd.read_csv('../input/data.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d523ab60-2032-40a4-bcd6-b737260a96c9","_uuid":"2885edce35e6dd14c656fc40528d1611779578e6"},"cell_type":"markdown","source":"The feature **Unnamed: 32** contains lots of NaN values, so I decide to drop them. In addition, feature **id** is not related to breast cancer, so it will be removed."},{"metadata":{"_cell_guid":"fe86e296-1375-4d34-b77a-2e5def5de899","_uuid":"b4578c5e7eaf82032bf02001282563553526ead2","collapsed":true,"trusted":true},"cell_type":"code","source":"data.drop('Unnamed: 32', axis=1,inplace = True)\ndata.drop('id', axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3479f14f-fcee-4e0d-8247-ac1d3b04ec53","_uuid":"6f6366f0b9873cfe546c3b2df6ba292b71091d20","collapsed":true,"trusted":true},"cell_type":"code","source":"#plot the PCC figure\ncorr = data.corr(method = 'pearson')\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(10, 275, as_cmap=True)\n\nsns.heatmap(corr, cmap=cmap, square=True,\n            linewidths=0.5, cbar_kws={\"shrink\": 0.5}, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0fdf6d78-2932-4671-af1b-db610725a9a2","_uuid":"84f1b3aed33a2d6834c4810b8e3fc797906f17fe"},"cell_type":"markdown","source":"Some features show strong correlation with each other. In order to reduce the dimensions, some features are dropped. For features **radius**, **perimeter** and **area**, I choose **area**. For **concavity, concave point** and **compatiness**, I choose concavity. For [**texture_worst, texture_mean**] and [**area_worst, area_mean**], I choose **texture_mean** and **texture_mean**. Therefore, 17 features are left."},{"metadata":{"_cell_guid":"f8000e91-29ee-4079-ba40-9a81b7d20f3e","_uuid":"a49c3fecaad163b1f0d74c9751ef293f4a701a31","collapsed":true,"trusted":true},"cell_type":"code","source":"#drop those related features\ndrop_features = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se',\n              'perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst',\n              'compactness_se','concave points_se','texture_worst','area_worst']\ndata1 = data.drop(drop_features, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e802a713-017c-4cee-8dad-39c36f7f50ea","_uuid":"e4e80e6bc1e454067881be9cbc2b45b6ce096542","collapsed":true,"trusted":true},"cell_type":"code","source":"#replace M and B in 'diagnosis' with 1 and 0 respectively for later classification problem\ndata1[\"class\"] = data['diagnosis'].map({'M':1, 'B':0})\ndata1 = data1.drop('diagnosis', axis=1, inplace=True)\nx = data1.copy(deep = True)\nx = x.drop('class', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"58cfa133-ffed-480b-8431-83acb78a1e08","_uuid":"1f7411d1f3e9e4d0331976e9611c3bddb81d5b16"},"cell_type":"markdown","source":"## Feature Selection"},{"metadata":{"_cell_guid":"5bc8ad95-f7bc-4df1-9499-643e3d83598c","_uuid":"30d361c78d2298ff8690ce975c60754b9b1ba617","collapsed":true,"trusted":true},"cell_type":"code","source":"x = x\ny = data1['class'] \n#calculate the scores for each features in order to find out which features are more important.\nfeature_ranking = SelectKBest(chi2, k=5)\nfit = feature_ranking.fit(x, y)\n\nfmt = '%-8s%-20s%s'\n\nprint(fmt % ('', 'Scores', 'Features'))\nfor i, (score, feature) in enumerate(zip(feature_ranking.scores_, x.columns)):\n    print(fmt % (i, score, feature))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"178a4471-f301-4ab8-8515-faf2b5c05d97","_uuid":"ca7534ecb222f93acaa9e45bf8e3f97f5ae6a432"},"cell_type":"markdown","source":"We can see that features: **area_mean, area_se, texture_mean, concavity_worst, concavity_mean** are the best 5 features."},{"metadata":{"_cell_guid":"b33ac4e8-f0d7-45c7-bda4-173871f76920","_uuid":"f679979bb5a7ea46dab27990b3eb3712e3813799"},"cell_type":"markdown","source":"## Min Max Normalization\nFor machine learning methods, it is beneficial to do the normalization first. Before normaliztion, the ranges of different features are quite different. After min-max normalization, the interval is between [0, 1]. This makes the values invariant to rigid displacement of coordinates. However, it may encounter an out-of-bounds error if a future input case for normalization falls outside of the original data range."},{"metadata":{"_cell_guid":"e7e9ff6e-dc80-4295-b14f-b8c257453f7c","_uuid":"7ede610d96584d793260f35c19069b9b9b4d70cc","collapsed":true,"trusted":true},"cell_type":"code","source":"data_norm = (data1 - data1.min()) / (data1.max() - data1.min())\ndata_norm.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"639db115-5952-41eb-be27-84252e07d6a8","_uuid":"f228b3e6d30b8918ccc9ad36fb578d28c652ee86"},"cell_type":"markdown","source":"## PCA"},{"metadata":{"_cell_guid":"8b5c78cf-da56-42a1-97fd-67b558bcfb9d","_uuid":"4b05a9fa57563f7c8d568397b8e720c2ce8c545a","collapsed":true,"trusted":true},"cell_type":"code","source":"X_norm = data_norm\ny_norm = data_norm['class']\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n#PCA model, after normalization\npca = PCA(n_components=2)\nX_r = pca.fit(X_norm).transform(X_norm)\n\n#get the first class\ndata_0 = []\nfor i, label in enumerate(y_norm):\n    if label == 0:\n        data_0.append(X_r[i].tolist())\n        \ndata_0_array = np.asarray(data_0)\n #get the second class\ndata_1 = []\nfor i, label in enumerate(y_norm):\n    if label == 1:\n        data_1.append(X_r[i].tolist())\n        \ndata_1_array = np.asarray(data_1)\n #plot these two classes in one single plot\nax1.scatter(x=data_0_array[:,0], y=data_0_array[:,1], c='purple', label='Benign')\nax1.legend()\nax1.scatter(x=data_1_array[:,0], y=data_1_array[:,1], c='yellow', label='Malignant')\nax1.legend()\nax1.set_title('Principal Component Analysis after normalization (PCA)')\nax1.set_xlabel('1st principal component')\nax1.set_ylabel('2nd principal component')\n\n#PCA model, before normalization\n\nX = data1\ny = data1['class']\n\npca = PCA(n_components=2)\nX_r1 = pca.fit(X).transform(X)\n\ndata1_0 = []\nfor i, label in enumerate(y):\n    if label == 0:\n        data1_0.append(X_r1[i].tolist())\n        \ndata1_0_array = np.asarray(data1_0)\n\ndata1_1 = []\nfor i, label in enumerate(y):\n    if label == 1:\n        data1_1.append(X_r1[i].tolist())\n        \ndata1_1_array = np.asarray(data1_1)\n\nax2.scatter(x=data1_0_array[:,0], y=data1_0_array[:,1], c='purple', label='Benign')\nax2.legend()\nax2.scatter(x=data1_1_array[:,0], y=data1_1_array[:,1], c='yellow', label='Malignant')\nax2.legend()\nax2.set_title('Principal Component Analysis before normalization (PCA)')\nax2.set_xlabel('1st principal component')\nax2.set_ylabel('2nd principal component')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"75f59e4d-e8f2-48a8-ae11-f94fc234d670","_uuid":"f396fcdd9041c158c5c1ba4dedb84a5e396f6f3d"},"cell_type":"markdown","source":"From these two figures, it is very clear that normalization is quite essential for PCA. Before normalization, the distance between two classes is very close. Some of them are overlapped, so it is hard to classify them. However, after normalization, the distance is large enough to easily distinguish two classes."},{"metadata":{"_cell_guid":"83fafd12-de68-44cd-908b-bef90b8cfde8","_uuid":"468f4c79a24330ad5beae4f6c8acd0184829c409","collapsed":true,"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_r, y, test_size=0.33, random_state=42)\nprint(X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a65c98f4-da25-4191-80a4-069e0367f1be","_uuid":"acbdc7799adbc92b50ea54ddf542f9bd8dbdb17a"},"cell_type":"markdown","source":"## KNN after PCA"},{"metadata":{"_cell_guid":"97fcc3ae-27f8-431f-9735-62c46775aeeb","_uuid":"c25678081ca0f94aba4fc38c1f7a784f15c8c266","collapsed":true,"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train, y_train)\npred = knn.predict(X_test)\nprint(accuracy_score(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a78b86a7-96f9-42b5-816f-39d041319648","_uuid":"ee88c1cf5050bee07544475d946f62bb1bca7831","collapsed":true,"trusted":true},"cell_type":"code","source":"confusion_matrix = confusion_matrix(y_test, pred)\nprint(confusion_matrix)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0d8e893a-d767-4e6b-9063-e8744ab37499","_uuid":"1442f3b03fe473659d9eb862fdab1049d9ccdacd"},"cell_type":"markdown","source":"The confusion matrix means that 121+67 test samples are correctly classified. 0 test sample is incorrectly classified, so the accuracy is 100%."},{"metadata":{"_cell_guid":"388481e3-a4e5-45b4-a151-f53a77d796a9","_uuid":"18a37ab3cd48989ec8ec6d429ff3efc207df1c09","collapsed":true,"trusted":true},"cell_type":"code","source":"plt.scatter(X_test[:, 0], X_test[:, 1], c=pred, label=pred)\nplt.title('Classification using KNN', fontsize=12)\nplt.xlabel('1st principal component')\nplt.ylabel('2nd principal component')\nplt.legend(labels=pred)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fd0dd9e6-2dec-4b7e-b5ae-f47ad60eaea6","_uuid":"09cc170f849ad5ccf7a1b8c00795fa5b9c6f0522"},"cell_type":"markdown","source":"## SVM after PCA"},{"metadata":{"_cell_guid":"8d567683-64b2-4b51-9c2e-64219c01a996","_uuid":"8cea96eafc2bc52a32ed892e82a4d2431289f650","collapsed":true,"trusted":true},"cell_type":"code","source":"clf = svm.SVC(kernel='linear', C = 1.0)\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\nprint((accuracy_score(y_test, pred)))\n\nplt.scatter(X_test[:, 0], X_test[:, 1], c=pred, label=pred)\nplt.title('Classification using SVM', fontsize=12)\nplt.xlabel('1st principal component')\nplt.ylabel('2nd principal component')\nplt.legend(labels=pred)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5722e495-7c57-4870-8e1b-5e11ac9f8c95","_uuid":"f4bf15dd033b5eb2951342c6a147ed540bed1008"},"cell_type":"markdown","source":"## Naive Bayes Classifier"},{"metadata":{"_cell_guid":"51fdc74e-992b-49c1-894d-b1e8dcaf53d3","_uuid":"6d93e5a4a5cc0136bda86a6c7cc609ccc99710ae","collapsed":true,"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_r, y, test_size=0.33, random_state=42)\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred= gnb.predict(X_test)\n\nscore = accuracy_score(y_test, y_pred, normalize = True)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9e26fd84-dbbc-4d71-b8bc-6654936ff387","_uuid":"a718b9f7c8575b20008850073818e7f0755cc57c"},"cell_type":"markdown","source":"The above comparision is based on dimensional reduction. But what if I do the classification without reducing the dimensions?"},{"metadata":{"_cell_guid":"07bc10d0-2e7c-4dc4-b633-b55d456dcc7e","_uuid":"e52c5a60a3770f9d5c5f45d629d47b2a582bc27e"},"cell_type":"markdown","source":"## KNN without dimensional reduction"},{"metadata":{"_cell_guid":"58270287-0fa5-492e-828f-250274908006","_uuid":"1c5cb74698357225b447e2882d82984aab48c96c","collapsed":true,"trusted":true},"cell_type":"code","source":"X = data_norm\ny = data_norm['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train, y_train)\npred = knn.predict(X_test)\nprint(accuracy_score(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8016293-bad1-45be-b73e-ed3b3fd8d174","_uuid":"bd63f4b78c6af77c2ea4a433a0effda1348e76d3"},"cell_type":"markdown","source":"## SVM without dimensional reduction"},{"metadata":{"_cell_guid":"e2dbb944-2b94-448e-bc54-07511b76ed4d","_uuid":"dfeb2df1a95ed71ca5298ce8752530a91b43bfed","collapsed":true,"trusted":true},"cell_type":"code","source":"clf = svm.SVC(kernel='linear', C = 1.0)\nclf.fit(X_train, y_train)\npred = clf.predict(X_test)\nprint((accuracy_score(y_test, pred)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5cc80c15-5969-4ad4-8012-ad8685cf3416","_uuid":"2faf04b54aed0c23ca685594637e5512183b75d6","collapsed":true,"trusted":true},"cell_type":"code","source":"sns.countplot(data1['class'],label=\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79fc733a-0834-4dcd-abc6-8c06416367cf","_uuid":"be49ea20531a81c42a2a6a3e97fdfba64c14003a"},"cell_type":"markdown","source":"## Naive Bayes Classifier without dimensional reduction"},{"metadata":{"_cell_guid":"4246c381-84a9-4d9a-91eb-28a407d255d2","_uuid":"f2f597fe3fdb149a12e8a86dde119c74fff10941","collapsed":true,"trusted":true},"cell_type":"code","source":"gnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\nscore = accuracy_score(y_test, y_pred, normalize = True)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"74b48dae-18fa-48e5-9a1b-a6113a82d4a8","_uuid":"becbc5156264353e3609dd1caf25c20ae35141a1"},"cell_type":"markdown","source":"## Logistic regression without dimensional reduction"},{"metadata":{"_cell_guid":"d41dc437-c130-46d4-9d5b-a75533aaf02c","_uuid":"269bdb9a07ef2261c5ddf18eed6311ab455f4e4a","collapsed":true,"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\ny_pred = logreg.predict(X_test)\nlog_score = logreg.score(X_test, y_test)\nprint(log_score)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"71f223f3-6bc1-4ab9-992c-ab409c6c8d85","_uuid":"64079b9f3d2aebcf992142beabf72813bd3c4e52"},"cell_type":"markdown","source":"KNN, SVM, Logistic Regression and Naive Bayes Classifier all get a very high accuracy. I guess one of the reasons is that the training samples are not large enough. Besides, the features that we chosen are very related to the classification. As long as the features are chosen correctly, the classification accuracy would be higher and dimension reduction doesn't matter a lot in this case."},{"metadata":{"_cell_guid":"6dcb96ab-cd4b-450d-93bd-58bea7d15cc9","_uuid":"7d952ec8db1b729101a949521a0cec379e1cc833","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}