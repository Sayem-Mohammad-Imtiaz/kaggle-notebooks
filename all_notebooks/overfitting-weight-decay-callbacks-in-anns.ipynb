{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"id\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(\"id\",axis=1, inplace=True)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(\"Unnamed: 32\",axis=1, inplace=True)\ndf\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le=LabelEncoder()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le.fit_transform(df[\"diagnosis\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"diagnosis\"]=le.fit_transform(df[\"diagnosis\"])\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"diagnosis\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),cmap=\"inferno\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=df.drop(\"diagnosis\",axis=1)\nX=X.values\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=df[\"diagnosis\"]\ny=y.values\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sc=StandardScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=sc.fit_transform(X_train)\nX_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test=sc.transform(X_test)\nX_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential # creates`Sequential` groups a linear stack of layers into a `tf.keras.Model`.\nfrom tensorflow.keras.layers import Dense #regularly connected neurons)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann=Sequential()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann.add(Dense(units=30, activation=\"relu\")) # here we add a dense layer with 30 neurons","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann.add(Dense(units=15, activation=\"relu\")) # here we add a dense layer with 15 neurons","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The activation function is a mathematical “gate” in between the input feeding the current neuron and its output going to the next layer. It can be as simple as a step function that turns the neuron output on and off, depending on a rule or threshold. Or it can be a transformation that maps the input signals into output signals that are needed for the neural network to function.\n\n\nTypes of Activation Function:\n\n1.Sigmoid / Logistic\n\n2.TanH / Hyperbolic Tangent\n\n3.ReLU (Rectified Linear Unit)\n\n4.Softmax","metadata":{}},{"cell_type":"code","source":"ann.add(Dense(1,activation=\"sigmoid\"))  # here we add output layer with 1 neuron","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" optimizer:this determines how we want to perform the gradient descent like adam optimizer loss= represents cost function we want to use\n\nType of Optimizers:\n\n1.Gradient Descent\n\n2.Stochastic Gradient Descent\n\n3.Mini-Batch Gradient Descent\n\n4.Adam:The intuition behind the Adam is that we don’t want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search.\n\n\nChoosing an optimizer: Keep in mind what kind of problem we are trying to solve:\n\nFor a multi-class classification problem:\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\nFor a binary classification problem:\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\nFor a mean squared error regression problem:\n\nmodel.compile(optimizer='rmsprop', loss='mse')","metadata":{}},{"cell_type":"code","source":"ann.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(ann.history.history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(ann.history.history).plot(figsize=(15,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann.evaluate(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann.evaluate(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test[[1]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann.predict(X_test[[1]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test[[1]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=ann.predict(X_test)\npredictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df=pd.DataFrame(predictions, columns=[\"Predictions\"])\npredictions_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_df=pd.DataFrame(y_test, columns=[\"Diagnosis\"])\ny_test_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comparison_df=pd.concat([predictions_df, y_test_df], axis=1)\ncomparison_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann2=Sequential()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann2.add(Dense(units=1, activation=\"relu\")) # here we add a dense layer with 1 neuron","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann2.add(Dense(1,activation=\"sigmoid\")) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann2.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann2.evaluate(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann2.evaluate(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann3=Sequential()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to see the overfitting:","metadata":{}},{"cell_type":"code","source":"ann3.add(Dense(units=100, activation=\"relu\")) # here we add a dense layer with 100 neurons","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann3.add(Dense(units=100, activation=\"relu\")) # here we add a dense layer with 100 neurons","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann3.add(Dense(units=100, activation=\"relu\")) # here we add a dense layer with 100 neurons","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann3.add(Dense(1,activation=\"sigmoid\")) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann3.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann3.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann3.evaluate(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann3.evaluate(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=ann3.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann3.history.history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(ann3.history.history).plot(figsize=(15,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see an expected shape of an overfit model where test accuracy increases to a point and then begins to decrease again.","metadata":{}},{"cell_type":"markdown","source":"An overfit model should show accuracy increasing on both train and test and at some point accuracy drops on the test dataset but continues to rise on the training dataset.","metadata":{}},{"cell_type":"markdown","source":"Keras provides a weight regularization API that allows you to add a penalty for weight size to the loss function.\n\nThree different regularizer instances are provided; they are:\n\nL1: Sum of the absolute weights.\nL2: Sum of the squared weights.\nL1L2: Sum of the absolute and the squared weights.","metadata":{}},{"cell_type":"markdown","source":"The most common type of regularization is L2, also called simply “weight decay,” with values often on a logarithmic scale between 0 and 0.1, such as 0.1, 0.001, 0.0001, etc.","metadata":{}},{"cell_type":"code","source":"from keras.regularizers import l2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann4=Sequential()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann4.add(Dense(100,activation=\"relu\", kernel_regularizer=l2(0.001)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann4.add(Dense(units=100, activation=\"relu\",kernel_regularizer=l2(0.001)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann4.add(Dense(units=100, activation=\"relu\",kernel_regularizer=l2(0.001)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann4.add(Dense(1,activation=\"sigmoid\")) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann4.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=ann4.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(ann4.history.history).plot(figsize=(15,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks to weight decay we got %100 accuracy in the test set after 65 epoch, after that there happens a overfitting.\n\nWe will use early stopping to prevent this problem:","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann5=Sequential()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann5.add(Dense(100,activation=\"relu\", kernel_regularizer=l2(0.001)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann5.add(Dense(units=100, activation=\"relu\",kernel_regularizer=l2(0.001)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann5.add(Dense(units=100, activation=\"relu\",kernel_regularizer=l2(0.001)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann5.add(Dense(1,activation=\"sigmoid\")) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ann5.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=ann5.fit(x= X_train, y= y_train, batch_size=32, epochs=100, validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor=\"val_accuracy\",patience=10)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(ann5.history.history).plot(figsize=(15,10))\n# Now we stopped the training after gettin %100 accuracy in the test set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we will compare the performance of the deep learning with a normal machine learning model","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtree=DecisionTreeClassifier()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dtree.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions2=dtree.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,predictions2)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can obviously see, the deep learning model easily outperform normal machine learning algorithms.","metadata":{}}]}