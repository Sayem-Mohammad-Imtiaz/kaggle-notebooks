{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Description of Data : About one in seven U.S. adults has diabetes now, according to the Centers for Disease Control and Prevention. But by 2050, that rate could skyrocket to as many as one in three. With this in mind, learning how to use Machine Learning to help us predict Diabetes. Let’s get started!"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#importing pakages\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing dataset\ndataset = pd.read_csv('/kaggle/input/machine-learning-for-diabetes-with-python/diabetes_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(dataset.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.groupby('Outcome').hist(figsize=(9,9))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#splitting the independent and dependent variables.\nX = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,8].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#counting the outcome of the diabetes study.\nsns.countplot(dataset['Outcome'],label='Count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.groupby('Outcome').size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing train test split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(dataset.iloc[:,dataset.columns != 'Outcome'],\n                                                dataset['Outcome'],stratify=dataset['Outcome'],random_state=66)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"k-Nearest Neighbors The k-NN algorithm is arguably the simplest machine learning algorithm. Building the model consists only of storing the training data set. To make a prediction for a new data point, the algorithm finds the closest data points in the training data set — its “nearest neighbors.”\n\nFirst, Let’s investigate whether we can confirm the connection between model complexity and accuracy:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing KNN classifier\nfrom sklearn.neighbors import KNeighborsClassifier\ntraining_accuracy = []\ntest_accuracy = []\nneighbors_settings = range(1,100)\nfor n_neighbors in neighbors_settings:\n    knn = KNeighborsClassifier(n_neighbors= n_neighbors)\n    knn.fit(X_train, Y_train)\n    training_accuracy.append(knn.score(X_train,Y_train))\n    test_accuracy.append(knn.score(X_test,Y_test))\nplt.plot(neighbors_settings, training_accuracy, label='training accuracy')\nplt.plot(neighbors_settings, test_accuracy, label='test accuracy')\nplt.xlabel('Accuracy')\nplt.ylabel('n_neighbors')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#k value can be taken as 19 as seen from the above graph\nknn = KNeighborsClassifier(n_neighbors=19)\nknn.fit(X_train, Y_train)\nprint('Accuracy of KNN classifier on training set: {:.2f}'.format(knn.score(X_train, Y_train)))\nprint('Accuracy of KNN classifier on test set: {:.2f}'.format(knn.score(X_test, Y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nactual =Y_test\npredicted =knn.predict(X_test)\nresults = confusion_matrix(actual, predicted)\nprint('Confusion Matrix')\nprint(results)\nprint('Accuracy Score :', accuracy_score(actual, predicted))\nprint('Report')\nprint(classification_report(actual,predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing roc curve and roc curve score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr,tpr):\n  plt.plot(fpr, tpr, color = 'orange', label = 'ROC')\n  plt.plot([0,1], [0,1],color='darkblue',linestyle='--')\n  plt.title('Receiver Operating Charactersticks (ROC) Curve')\n  plt.xlabel('False Positive Value')\n  plt.ylabel('True Positive Rate')\n  plt.legend()\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting the probablity by KNN classifier\nprobs = knn.predict_proba(X_test)\nprobs[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = probs[:,1]\nprobs[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"auc = roc_auc_score(Y_test,probs)\nprint('AUC: %.2f' %auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = roc_curve(Y_test, probs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the roc curve\nplot_roc_curve(fpr, tpr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}