{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing necessary fuctions\n\nimport numpy as np\nimport pandas as pd \nfrom scipy.stats import skew, boxcox_normmax\nfrom scipy.special import boxcox1p\nfrom sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, validation_curve, cross_validate\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom copy import copy\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso, LassoCV\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import mutual_info_classif, SelectKBest, chi2, RFE\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\nimport sys\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a96aa9f1bdc024325b220afbfa568e2c2fca354"},"cell_type":"markdown","source":"**I. Data Acquisition**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"#Read data file directly from Kaggle\ndef read_data(input_path):\n    raw_data = pd.read_csv(input_path, keep_default_na=False, na_values=['_'])\n    return raw_data\n\nraw_data = read_data(\"../input/turnover.csv\")\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f0bb0081f95e22290919673da7cd4f6d3d6be41"},"cell_type":"code","source":"# Differentiate categorical from numerical variables, even though some are encoded already\ncat_raw_data = raw_data[['Work_accident',\n                         'left',\n                         'promotion_last_5years',\n                         'sales',\n                         'salary']]\n\nnumerical_columns = list(set(raw_data.columns) - set(cat_raw_data.columns))\nnum_raw_data = raw_data.loc[:,numerical_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"708c935db8d54c8ec72b78deb90974bb097d0c0c"},"cell_type":"code","source":"# Define functions to be used later in the model   \n# Ensure no null values in the data frame \ndef ensure_no_nulls(df):\n    return df.isnull().T.any().T.sum()\n\ndef onehot_encode(df):\n    numeric_dtypes = ['int16','int32','int64','float16','float32','float64']\n    for categorical_column in cat_raw_data:\n        if cat_raw_data[categorical_column].dtype in numeric_dtypes:\n            new_df = cat_raw_data\n    for categorical_column in cat_raw_data:\n        if cat_raw_data[categorical_column].dtype not in numeric_dtypes:\n            new_df = pd.concat([new_df,pd.get_dummies(df[categorical_column],\n                                                      prefix=categorical_column)],axis=1)\n    return new_df\n\n# Determine which variables present skewness\ndef feature_skewness(df):\n    feature_skew = df.apply(\n        lambda x: skew(x)).sort_values(ascending=False)\n    skews = pd.DataFrame({'skew':feature_skew})\n    return feature_skew\n\n#If the previous function detects skewness, implement Box Cox transformation to make distribution normal\ndef fix_skewness(df):\n    feature_skew = feature_skewness(df)\n    high_skew = feature_skew[abs(feature_skew) > 0.5]\n    skew_index = high_skew.index\n    for i in skew_index:\n        df[i] = boxcox1p(df[i], boxcox_normmax(df[i]+1))\n    skew_features = df.apply(\n        lambda x: skew(x)).sort_values(ascending=False)\n    skews = pd.DataFrame({'skew':skew_features})\n    return df\n\n# Obtain model accuracy and confusion matrix for each \ndef score_model(data, seed=1000):\n    X = data.loc[:, data.columns != 'left']\n    y = data.loc[:, 'left']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n    regr = linear_model.LogisticRegression(penalty='l1')\n    regr.fit(X_train, y_train)\n    y_pred = regr.predict(X_test)\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\n    print(\"Accuracy: %1.4f\" % (accuracy_score(y_test, y_pred)))\n\n# Obtain specific accuracy score \ndef print_accuracy(data, seed = 1000):\n    X = data.loc[:, data.columns != 'left']\n    y = data.loc[:, 'left']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n    regr = linear_model.LogisticRegression(penalty='l1')\n    regr.fit(X_train, y_train)\n    y_pred = regr.predict(X_test)\n    print(\"Accuracy: %1.4f\" % (accuracy_score(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c463320a15bf4f10d7b6c9e0b96056dfed955ab"},"cell_type":"markdown","source":"**II. Data Understanding**"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"85dd70911bb191c7fa29f112441f3b8f9c8b1dca"},"cell_type":"code","source":"# Ensure no null values in the data frame \nassert(ensure_no_nulls(raw_data) == 0)\n\n# Check distribution of categorical variables\nfor i in cat_raw_data.columns:\n    dist = cat_raw_data[i].value_counts()/len(cat_raw_data)\n    print(dist)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68957d1740b2f291c2f024a578e521467a10b038"},"cell_type":"markdown","source":"There are no null values in the data set. \nValues considered 'rare' in the data set (occurrence < 5%):\n    - Has been promoted in the past 5 years\n    - Is working in either HR or management\n This will be important further along the feature engineering process. "},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"e23e461e3970df8ed27c58c63c8975856405304d"},"cell_type":"code","source":"# Understanding distribution of numerical variables \nnum_raw_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a32e46c600bbf61ece59c0564de3c6aa82d9d22"},"cell_type":"code","source":"fig,ax = plt.subplots(2,3, figsize=(10,10))               # 'ax' has references to all the four axes\nsns.distplot(raw_data['satisfaction_level'], ax = ax[0,0]) \nsns.distplot(raw_data['last_evaluation'], ax = ax[0,1]) \nsns.distplot(raw_data['number_project'], ax = ax[0,2]) \nsns.distplot(raw_data['average_montly_hours'], ax = ax[1,0]) \nsns.distplot(raw_data['time_spend_company'], ax = ax[1,1]) \nsns.distplot(raw_data['promotion_last_5years'], ax = ax[1,2])\n \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c963df9503b8ccff469344a8656ba4ddcf8fc006"},"cell_type":"code","source":"# Target which variables \nraw_data.satisfaction_level = raw_data.satisfaction_level.astype('category')\nax = pd.Series((raw_data[raw_data.left==0].satisfaction_level.value_counts()/len(raw_data.left))*100).sort_index().plot(kind='bar',color='b',figsize=(35,10),alpha=0.4)\npd.Series((raw_data[raw_data.left==1].satisfaction_level.value_counts()/len(raw_data.left))*100).sort_index().plot(kind='bar',color='r',figsize=(35,10),alpha = 0.4, ax=ax)\nax.legend([\"Left = 0\", \"Left = 1\"])\nplt.title('Occurence of Churn for all satisfaction levels',fontsize=20)\nplt.rcParams.update({'font.size': 22})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30675dc89f0255119521660e1917ab2b532575d7"},"cell_type":"code","source":"plt.figure(figsize=(40,5))\nraw_data.satisfaction_level = raw_data.last_evaluation.astype('category')\n((raw_data[raw_data.left==1].last_evaluation.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='r',alpha = 0.4)\n((raw_data[raw_data.left==0].last_evaluation.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='b',alpha=0.4)\nax.legend([\"Left = 0\", \"Left = 1\"])\nplt.title('Occurences of Churn for all last evaluations scores')\nplt.rcParams.update({'font.size': 22})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"052ba4c1f76ca5baa43b2cef675f01e28f521a16"},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nax = ((raw_data[raw_data.left==0].salary.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='b',alpha = 0.4)\n((raw_data[raw_data.left==1].salary.value_counts().sort_index()/len(raw_data.left)*100)).plot(kind='bar',color='r',alpha = 0.4, ax= ax)\nax.legend([\"Left = 0\", \"Left = 1\"])\nplt.title('Occurences of Churn for all salary levels')\nplt.rcParams.update({'font.size': 10})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"acfc75fb08d4f056812c3ee5c5eb7de1e2646f2f"},"cell_type":"code","source":"plt.figure(figsize=(5,2))\n((raw_data[raw_data.left==0].number_project.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='b',alpha = 0.4)\n((raw_data[raw_data.left==1].number_project.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='r',alpha = 0.4)\nax.legend([\"Left = 0\", \"Left = 1\"])\nplt.title('Occurences of Churn for all number of projects')\nplt.rcParams.update({'font.size': 10})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7a565995be4598f29379a2d196a98f766bbe784"},"cell_type":"code","source":"plt.figure(figsize=(40,5))\n((raw_data[raw_data.left==0].average_montly_hours.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='b',alpha=0.4)\n((raw_data[raw_data.left==1].average_montly_hours.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='r',alpha=0.4)\nax.legend([\"Left = 0\", \"Left = 1\"])\nplt.title('Occurences of Churn for all monthly hours')\nplt.rcParams.update({'font.size': 10})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0893385ae8ada7d1acb0915a95cdadb1d6564fe4"},"cell_type":"code","source":"plt.figure(figsize=(10,5))\n((raw_data[raw_data.left==0].time_spend_company.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='b',alpha = 0.4)\n((raw_data[raw_data.left==1].time_spend_company.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='r',alpha=0.4)\nax.legend([\"Left = 0\", \"Left = 1\"])\nplt.title('Occurences of Churn for all seniority levels')\nplt.rcParams.update({'font.size': 10})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58cde6d303b278186d8fae81ed696ca097af2f2c"},"cell_type":"code","source":"plt.figure(figsize=(5,3))\n((raw_data[raw_data.left==0].Work_accident.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='b', alpha = 0.4)\n((raw_data[raw_data.left==1].Work_accident.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='r',alpha=0.4)\nax.legend([\"Left = 0\", \"Left = 1\"])\nplt.title('Occurences of Churn according to work accidents')\nplt.rcParams.update({'font.size': 10})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2122ceb0f07bd4c3427112bb5542c6c10b6d9c8c"},"cell_type":"code","source":"plt.figure(figsize=(5,3))\n((raw_data[raw_data.left==0].promotion_last_5years.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='b', alpha = 0.4)\n((raw_data[raw_data.left==1].promotion_last_5years.value_counts().sort_index()/len(raw_data.left))*100).plot(kind='bar',color='r',alpha=0.4)\nplt.title('Occurences of Churn according to promotions')\nplt.rcParams.update({'font.size': 10})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f240cfbacd2cd4abd41c54074df3199f25120a5"},"cell_type":"code","source":"plt.figure(figsize=(5,3))\n((raw_data[raw_data.left==0].sales.value_counts()/len(raw_data.left))*100).plot(kind='bar',color='b', alpha = 0.4)\n((raw_data[raw_data.left==1].sales.value_counts()/len(raw_data.left))*100).plot(kind='bar',color='r',alpha=0.4)\nplt.title('Occurences of Churn across all departments')\nplt.rcParams.update({'font.size': 10})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d134ffcdccf0dc5a180855c975f20516509853c9"},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.heatmap(raw_data.corr(),cbar=True,fmt =' .2f', annot=True, cmap='coolwarm')\nsns.set(font_scale=1.4)\ncmap = sns.diverging_palette(h_neg=210, h_pos=350, s=90, l=30, as_cmap=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b5306020ca74a5b0088c9c57d21291ed66a1502"},"cell_type":"markdown","source":"**III. Data Preparation**"},{"metadata":{"trusted":true,"_uuid":"76e5627b0a53ef4676084e9f8947c13feede982a"},"cell_type":"code","source":"lkup = {\"low\": 0, \"medium\": 1, \"high\": 2}\nraw_data['sal_num'] = raw_data['salary'].map(lkup)\nraw_data.drop('salary', inplace=True, axis=1)\nnum_raw_data['sal_num'] = raw_data['sal_num']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c445540bc9b7529eba5b19b541ea6d4cf1185f36"},"cell_type":"code","source":"cat_raw_data.drop('salary', inplace=True, axis=1)\nraw_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f572124dc3a10944ab3c57394e180204021cfcf"},"cell_type":"code","source":"## Data Preparation\nfeature_skewness(num_raw_data)\nfix_skewness(num_raw_data).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b6f0441314e13979fd9b1cb6176fbf5736025ab"},"cell_type":"code","source":"onehot_encode(raw_data).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8b1834a56ccffac1a438e9c36d0aff47c865edc"},"cell_type":"code","source":"#Merge numerical and categorical variables in the same data frame\ndataset = fix_skewness(num_raw_data).merge(onehot_encode(raw_data).drop(['sales'],axis=1), how='outer', left_index=True, right_index=True)\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fc8e391947d9ed83e8db3d72035a26066c907c9"},"cell_type":"markdown","source":"**IV. Establish Baseline Model**"},{"metadata":{"trusted":true,"_uuid":"3ae0b8389b85287d76eba6594dc7be716bfe2fb5"},"cell_type":"code","source":"## Establish Baseline Model with Logistic Regression:\nscore_model(dataset)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f3b00917613f48fcab161f5b0c0866945938e62"},"cell_type":"markdown","source":"**V. Feature Engineering - Feature Creation**"},{"metadata":{"trusted":true,"_uuid":"d5ebc3168cac2261c20c9e24b935527f17672ed2"},"cell_type":"code","source":"## Feature Engineering\n\n# Feature Construction\n# Standardize numerical columns since they are measured on different scales\ndef standardize(df):\n    df.iloc[:, 0:5] = StandardScaler().fit_transform(df.iloc[:, 0:5])\n    return df\n\n# Cluster personnel between functional and support activities\ndef cluster_functions(df):\n    df['support_fuction'] = \"\"\n    filter_col = [col for col in df if col.startswith('sales_')]\n    for i in range(len(df[filter_col])):\n        if df.at[i,'sales_accounting'] == 1 or df.at[i,'sales_hr'] == 1 or df.at[i,'sales_management'] == 1:\n            df['support_fuction'] = 1\n        else:\n            df['support_fuction'] = 0\n    return df\n\n# Other functions needed in feature engineering\n# Remove outliers, still for target 'left'\ndef remove_outliers(df):\n    X = df.drop(['left'], axis=1)\n    y = df.left.reset_index(drop=True)\n    ols = sm.OLS(endog = y, exog = X)\n    fit = ols.fit()\n    test = fit.outlier_test()['bonf(p)']\n    outliers = list(test[test<1e-3].index)\n    df.drop(df.index[outliers])\n    return df\n\n# Binning a few variables (with pandas library)\ndef bin_variables_quartiles(df):\n    variables=['average_montly_hours','satisfaction_level','last_evaluation']\n    for field in variables:\n        min=df[field].min()-1\n        q1=df[field].quantile(0.25)\n        q2=df[field].quantile(0.50)\n        q3=df[field].quantile(0.75)\n        max=df[field].max()+1\n        bins = [min, q1, q2, q3, max]\n        group_names = ['1Q', '2Q', '3Q', '4Q']\n        df[\"BINNED_\"+field]= pd.cut(df[field], bins, labels=group_names)\n    new_df = df[df.columns[-3:]]\n    numeric_dtypes = ['int16','int32','int64','float16','float32','float64']\n    for i in new_df.columns:\n        if new_df[i].dtype not in numeric_dtypes:\n            new_df = pd.concat([new_df,pd.get_dummies(new_df[i],prefix=i)],axis=1)\n    new_df = df.merge(new_df.drop(['BINNED_average_montly_hours','BINNED_satisfaction_level','BINNED_last_evaluation'],\n                                  axis=1), how='outer', left_index=True, right_index=True) \n    new_df = new_df.drop(['BINNED_average_montly_hours','BINNED_satisfaction_level','BINNED_last_evaluation'],axis=1)\n    return new_df\n\n#Construct bins appropriate for the dataset\n\ndef bin_variables(df):\n    bins = [0, 0.11, 0.35, 0.46, 0.71, 0.92,1.0]\n    df['satisfaction_level_bin'] = pd.cut(df.satisfaction_level,bins)\n    bins = [0, 0.47, 0.48, 0.65, 0.88, 0.89,1.0]\n    df['last_evaluation_bin'] = pd.cut(df.last_evaluation,bins)\n    lkup = { 3: \"low\", 4 : \"medium\", 5 : \"medium\",  2: \"high\", 6: \"high\", 7: \"Very high\"}\n    df['number_project_cat'] = df['number_project'].map(lkup)\n    bins = [96, 131, 165, 178, 179, 259, 287]\n    df['average_montly_hours_bin'] = pd.cut(df.average_montly_hours,bins)\n    bins = [0, 0.8, 0.9, 0.98, 1.04, 1.1]\n    df['time_spend_company_bin'] = pd.cut(df.time_spend_company,bins)\n    df = pd.concat([df, pd.get_dummies(df['satisfaction_level_bin'],prefix='sts', prefix_sep='_')], axis=1)\n    df.drop('satisfaction_level', inplace=True, axis=1)\n    df.drop('satisfaction_level_bin', inplace=True, axis=1)\n    df = pd.concat([df, pd.get_dummies(df['last_evaluation_bin'],prefix='le', prefix_sep='_')], axis=1)\n    df.drop('last_evaluation_bin', inplace=True, axis=1)\n    df.drop('last_evaluation', inplace=True, axis=1)\n    df = pd.concat([df, pd.get_dummies(df['number_project_cat'],prefix='np', prefix_sep='_')], axis=1)\n    df.drop('number_project_cat', inplace=True, axis=1)\n    df.drop('number_project', inplace=True, axis=1)\n    df = pd.concat([df, pd.get_dummies(df['average_montly_hours_bin'],prefix='am', prefix_sep='_')], axis=1)\n    df.drop('average_montly_hours_bin', inplace=True, axis=1)\n    df.drop('average_montly_hours', inplace=True, axis=1)\n    df = pd.concat([df, pd.get_dummies(df['time_spend_company_bin'],prefix='tsc', prefix_sep='_')], axis=1)\n    df.drop('time_spend_company_bin', inplace=True, axis=1)\n    df.drop('time_spend_company', inplace=True, axis=1)\n    return df\n\n# Find the features with underrepresented attributes\ndef under_represented_features(df):\n    under_rep = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if ((zeros / len(df)) * 100) > 95.0:\n            under_rep.append(i)\n    df.drop(under_rep, axis=1, inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"670e3c72b5e6284ce95df92d3dd5eb4275e6fef6"},"cell_type":"code","source":"# Check for improvements over Baseline\nscore_model(dataset.copy())\nscore_model(standardize(dataset.copy()))\nscore_model(cluster_functions(dataset.copy()))\nscore_model(under_represented_features(dataset.copy()))\nscore_model(bin_variables(dataset.copy()))\nscore_model(bin_variables_quartiles(dataset.copy()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7207722198506996b991b75f63121554396f4932","scrolled":true},"cell_type":"code","source":"dataset = bin_variables(dataset)\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bba3a15c6fc494d4e651ad32de8e4e6875bd815b"},"cell_type":"markdown","source":"**VI. Feature Engineering - Feature Selection**"},{"metadata":{"trusted":true,"_uuid":"d3c41a616dd7252c9373063cb9ad7049af07e75a"},"cell_type":"code","source":"## Feature Selection\nX = dataset.drop(labels=[\"left\"],axis=1)\nY = dataset['left']\n\n# Stepwise function\ndef stepwise_selection(X, Y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    \"\"\" Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n    \"\"\"\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            model = sm.OLS(Y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.argmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(Y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n\n# Information Gain Function\ndef information_gain(df):\n    res = dict(zip(df.drop(labels=[\"left\"],axis=1).columns,\n                   mutual_info_classif(X, Y, discrete_features=True)))\n    res = sorted(res.items(), key=lambda x: x[1], reverse=True)\n    return res\n\n# Chi-squared Stats, for top 5 most significant categorical variables\ndef get_chi2_test(df):\n    chi2_dataset = df.drop(labels=['left'],axis=1)\n    x = chi2_dataset\n    Y = dataset['left']\n    # Feature extraction\n    test = SelectKBest(score_func=chi2, k=10)\n    fit = test.fit(x, Y)\n    # Summarize scores\n    np.set_printoptions(precision=3)\n    print(fit.scores_)\n    features = fit.transform(x)\n    # Summarize selected features\n    print(features[0:10,:])\n    # Obtain the column labels\n    x.columns[test.get_support(indices=True)]\n    vector_names = list(x.columns[test.get_support(indices=True)])\n    print(vector_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13d3230c15ec1af6c3ec3cca49200e572df06ece","scrolled":false},"cell_type":"code","source":"X = dataset.drop(labels=[\"left\"],axis=1)\nY = dataset['left']\nresult = stepwise_selection(X, Y)\n\nprint('resulting features:')\nprint(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"3a6df6ff5eb16bdf1ad31f4a22c0023acc27b61f"},"cell_type":"code","source":"information_gain(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"547f190b0d815f98393ef07e56fafed28c95003a","scrolled":true},"cell_type":"code","source":"# Perform Chi-Squared test for most significant categorical variables\nget_chi2_test(dataset)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6c77f41b660bd23a792c6e81e3294c7d0b64356","scrolled":false},"cell_type":"code","source":"# Recursive Feature Elimination\nmodel = LogisticRegression(class_weight='balanced')\nrfe = RFE(model)\nfit = rfe.fit(X, Y)\nprint(\"Num Features: %s\" % (fit.n_features_))\nprint(\"Selected Features: %s\" % (fit.support_))\nprint(\"Feature Ranking: %s\" % (fit.ranking_))\n\ndef get_column_names(lst):\n    l = []\n    for i in range(len(lst)):\n        if lst[i] == True:\n            l.append(X.columns[i])\n    return l\n\nget_column_names(fit.support_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cf707e316194440cede1381fd356e442071e5bdd","scrolled":false},"cell_type":"code","source":"for i in range(1,41):\n    lst = list(get_column_names(fit.support_))[:i]\n    lst.extend(['left'])\n    dataset_1 = dataset.loc[:,lst]\n    print('With %s variables:'%i)\n    print_accuracy(dataset_1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8eb4f64908fafb5c1c645a08d556582408f5f4d"},"cell_type":"markdown","source":"**VII. Model Evaluation**"},{"metadata":{"trusted":true,"_uuid":"db024f1dab72e8f5845abb16a96515b653ca15fb"},"cell_type":"code","source":"# Lasso Regression\n\nimport math \nimport matplotlib.pyplot as plt \nimport pandas as pd\nimport numpy as np\n# difference of lasso and ridge regression is that some of the coefficients can be zero i.e. some of the features are \n# completely neglected\nfrom sklearn.linear_model import Lasso\n#print cancer_df.head(3)\nX = dataset.drop(labels=[\"left\"],axis=1)\nY = dataset['left']\nX_train,X_test,y_train,y_test=train_test_split(X,Y, test_size=0.3, random_state=31)\nlasso = Lasso()\nlasso.fit(X_train,y_train)\ntrain_score=lasso.score(X_train,y_train)\ntest_score=lasso.score(X_test,y_test)\ncoeff_used = np.sum(lasso.coef_!=0)\nprint(\"training score:\", train_score)\nprint(\"test score: \", test_score)\nprint(\"number of features used: \", coeff_used)\nlasso001 = Lasso(alpha=0.01, max_iter=10e5)\nlasso001.fit(X_train,y_train)\ntrain_score001=lasso001.score(X_train,y_train)\ntest_score001=lasso001.score(X_test,y_test)\ncoeff_used001 = np.sum(lasso001.coef_!=0)\nprint(\"training score for alpha=0.01:\", train_score001)\nprint(\"test score for alpha =0.01: \", test_score001)\nprint(\"number of features used: for alpha =0.01:\", coeff_used001)\nlasso00001 = Lasso(alpha=0.0001, max_iter=10e5)\nlasso00001.fit(X_train,y_train)\ntrain_score00001=lasso00001.score(X_train,y_train)\ntest_score00001=lasso00001.score(X_test,y_test)\ncoeff_used00001 = np.sum(lasso00001.coef_!=0)\nprint(\"training score for alpha=0.0001:\", train_score00001)\nprint(\"test score for alpha =0.0001: \", test_score00001)\nprint (\"number of features used: for alpha =0.0001:\", coeff_used00001)\nlasso0000001 = Lasso(alpha=0.000001, max_iter=10e5)\nlasso0000001.fit(X_train,y_train)\ntrain_score0000001=lasso0000001.score(X_train,y_train)\ntest_score0000001=lasso0000001.score(X_test,y_test)\ncoeff_used0000001 = np.sum(lasso0000001.coef_!=0)\nprint(\"training score for alpha=0.000001:\", train_score0000001)\nprint(\"test score for alpha =0.000001: \", test_score0000001)\nprint (\"number of features used: for alpha =0.000001:\", coeff_used0000001)\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\nlr_train_score=lr.score(X_train,y_train)\nlr_test_score=lr.score(X_test,y_test)\nprint(\"LR training score:\", lr_train_score)\nprint(\"LR test score: \", lr_test_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"926069c37684ec487afb865ad832be1c1fc1b47d"},"cell_type":"code","source":"# Cross-Validation with Dataset \nX = dataset.drop(labels=[\"left\"],axis=1)\ny = dataset['left']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\ncv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=101)\npipeline = make_pipeline(LogisticRegression(n_jobs=-1)).fit(X_train, y_train)\nscores = cross_val_score(pipeline, X_train, y_train, scoring=\"accuracy\", cv=cv)\n\nprint('Obtained {} positive accuracy scores'.format(len(scores[scores > 0.0])))\nprint('Best CV accuracy: {:.4f}'.format((max(scores)),digits=4))\nprint('Avg. CV accuracy: {:.4f} +/- {:.04}'.format(np.mean(scores[scores > 0.0]),np.std(scores[scores > 0.0])))\nprint('Accuracy in hold-out dataset: {:.4f}'.format(pipeline.score(X_test,y_test),4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90d94c2e9b7f8f90ed06457b6a5a68b84e5191b8"},"cell_type":"code","source":"pipeline = make_pipeline(LogisticRegression(n_jobs=-1)).fit(X_train, y_train)\npipeline.fit(X_train, y_train)\ntraining_score = pipeline.score(X_test, y_test)\n\nprint('Accuracy from entire-dataset estimator: {:.4f}'.format(training_score))\n\n# Obtain scores and estimators from different splits and use the best one.\nscores = cross_validate(pipeline,X_train, y_train,scoring=['accuracy'],cv=5,return_estimator=True)\nsplit_scores = [scores['estimator'][i].score(X_test, y_test) for i in range(len(scores))]\nindex_best = split_scores.index(max(split_scores))\nprint('Best estimator accuracy score: {:.4f}'.format(split_scores[index_best],4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b0b9879506999ab2790a79db4bc8a5f85fbd23d"},"cell_type":"code","source":"# Confusion Matrix for Polynomial of Degree 2\ndef score_model_pol(data, seed=1000):\n    pol = PolynomialFeatures(2)\n    X = data.loc[:, data.columns != 'left']\n    y = data.left.values\n    X_train, X_test, y_train, y_test = train_test_split(pol.fit_transform(X.as_matrix()), y, test_size=0.30, random_state=101)\n    # Create regression object\n    regr = linear_model.LogisticRegression(penalty='l1')\n    regr.fit(X_train, y_train)\n    y_pred = regr.predict(X_test)\n    print(confusion_matrix(y_test, y_pred))\n    print(classification_report(y_test, y_pred))\n    print(\"Accuracy: %1.4f\" % (accuracy_score(y_test, y_pred)))\n    \nscore_model_pol(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2aead4afc6d9673faa7b8c2194ecf29cc4110352"},"cell_type":"code","source":"def score_model2(data, seed=666, splits=20):\n    X = data.loc[:, data.columns != 'left']\n    y = data.loc[:, 'left']\n    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20,random_state=seed)\n    regr = linear_model.LogisticRegression()\n    scores = cross_val_score(regr,X_train, y_train,scoring='accuracy', cv=splits)\n    return scores\n\ndef score_model3(data, seed=666, splits = 20):\n    X = dataset.drop(labels=[\"left\"],axis=1)\n    y = dataset['left']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)\n    cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=101)\n    pipeline = make_pipeline(PolynomialFeatures(degree=2, include_bias=False),LogisticRegression(n_jobs=-1)).fit(X_train, y_train)\n    scores = cross_val_score(pipeline, X_train, y_train, scoring=\"accuracy\", cv=splits)\n    return scores\n\nnum_times = 10\nscores2 = [np.mean(score_model2(dataset, seed=i)) for i in range(num_times)]\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.scatter(range(len(scores2)), scores2,\nlabel='$\\sigma$ = {:.3f}'.format(np.std(scores2)))\nplt.legend(loc='best')\nplt.title('New scoring function with k-fold cv.')\n\nnum_times = 10\nscores3 = [np.mean(score_model3(dataset, seed=i)) for i in range(num_times)]\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,2)\nplt.scatter(range(len(scores3)), scores3,\nlabel='$\\sigma$ = {:.3f}'.format(np.std(scores3)))\nplt.legend(loc='best')\nplt.title('New scoring function with k-fold cv with polynomial of degree 2.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"577954110f543b17f04ea70bd9be17faca4941af"},"cell_type":"code","source":"## Cross-Validation\nX = dataset.drop(labels=[\"left\"],axis=1)\ny = dataset['left']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\ncv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=101)\npipeline = make_pipeline(PolynomialFeatures(degree=2, include_bias=False),LogisticRegression(n_jobs=-1)).fit(X_train, y_train)\nscores = cross_val_score(pipeline, X_train, y_train, scoring=\"accuracy\", cv=cv)\n\nprint('Obtained {} positive accuracy scores'.format(len(scores[scores > 0.0])))\nprint('Best CV accuracy: {:.4f}'.format((max(scores)),digits=4))\nprint('Avg. CV accuracy: {:.4f} +/- {:.04}'.format(np.mean(scores[scores > 0.0]),np.std(scores[scores > 0.0])))\nprint('Accuracy in hold-out dataset: {:.4f}'.format(pipeline.score(X_test,y_test),4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6dd04fb75218fbbb90c1f35e7347a14adaf039c"},"cell_type":"code","source":"pipeline = Pipeline([('polynomials', PolynomialFeatures(degree=2, include_bias=False)),\n                     ('logistic_regression', LogisticRegression(n_jobs=-1))])\npipeline.fit(X_train, y_train)\ntraining_score = pipeline.score(X_test, y_test)\n\nprint('Accuracy from entire-dataset estimator: {:.4f}'.format(training_score))\n\n# Obtain scores and estimators from different splits and use the best one.\nscores = cross_validate(pipeline,X_train, y_train,scoring=['accuracy'],cv=5,return_estimator=True)\nsplit_scores = [scores['estimator'][i].score(X_test, y_test) for i in range(len(scores))]\nindex_best = split_scores.index(max(split_scores))\nprint('Best estimator accuracy score: {:.4f}'.format(split_scores[index_best],4))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}