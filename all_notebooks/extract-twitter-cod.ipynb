{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install PySastrawi","metadata":{"_uuid":"48f462c6-9f62-4c07-8219-40517b1ebfc9","_cell_guid":"693ea604-612e-4b14-b307-6cb6a50c4d46","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-06-30T15:06:32.759494Z","iopub.execute_input":"2021-06-30T15:06:32.759947Z","iopub.status.idle":"2021-06-30T15:06:40.812826Z","shell.execute_reply.started":"2021-06-30T15:06:32.759893Z","shell.execute_reply":"2021-06-30T15:06:40.811056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd \nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:06:40.815233Z","iopub.execute_input":"2021-06-30T15:06:40.815643Z","iopub.status.idle":"2021-06-30T15:06:40.820964Z","shell.execute_reply.started":"2021-06-30T15:06:40.815604Z","shell.execute_reply":"2021-06-30T15:06:40.819788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"#data1 keyword \"kurir cod\" 1 Juni-14 Juni\n#data2 keyword \"kurir cod\" 1 Mei-31 Mei\n#data3 keyword \"sistem cod shopee\" 1 Mei-14 Juni\n#data4 keyword \"sistem cod tokopedia\" 1 Mei-14 Juni\n#data5 keyword \"sistem cod lazada\" 1 Mei-14 Juni\n#data6 keyword \"sistem cod marketplace\" 1 Mei-14 Juni\n#data7 keyword \"cod @shopeeid\" 1 Mei-14 Juni\n#data8 keyword \"hapus cod\" 1 Mei-14 Juni\n#data9 keyword \"@bukuakik\" 1 Mei-14 Juni\n\ndf1 = pd.read_json(r'../input/raw-data-tweet-cod/data1.json',lines=True)\ndf2 = pd.read_json(r'../input/raw-data-tweet-cod/data2.json',lines=True)\ndf3 = pd.read_json(r'../input/raw-data-tweet-cod/data3.json',lines=True)\ndf4 = pd.read_json(r'../input/raw-data-tweet-cod/data4.json',lines=True)\ndf5 = pd.read_json(r'../input/raw-data-tweet-cod/data5.json',lines=True)\ndf6 = pd.read_json(r'../input/raw-data-tweet-cod/data6.json',lines=True)\ndf7 = pd.read_json(r'../input/raw-data-tweet-cod/data7.json',lines=True)\ndf8 = pd.read_json(r'../input/raw-data-tweet-cod/data8.json',lines=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:06:40.823324Z","iopub.execute_input":"2021-06-30T15:06:40.823662Z","iopub.status.idle":"2021-06-30T15:06:41.848023Z","shell.execute_reply.started":"2021-06-30T15:06:40.823633Z","shell.execute_reply":"2021-06-30T15:06:41.84662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = df1.where(pd.notnull(df1), None)\ndf2 = df2.where(pd.notnull(df2), None)\ndf3 = df3.where(pd.notnull(df3), None)\ndf4 = df4.where(pd.notnull(df4), None)\ndf5 = df5.where(pd.notnull(df5), None)\ndf6 = df6.where(pd.notnull(df6), None)\ndf7 = df7.where(pd.notnull(df7), None)\ndf8 = df8.where(pd.notnull(df8), None)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:06:41.850678Z","iopub.execute_input":"2021-06-30T15:06:41.851212Z","iopub.status.idle":"2021-06-30T15:06:41.938412Z","shell.execute_reply.started":"2021-06-30T15:06:41.851158Z","shell.execute_reply":"2021-06-30T15:06:41.937058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:06:41.940238Z","iopub.execute_input":"2021-06-30T15:06:41.940719Z","iopub.status.idle":"2021-06-30T15:06:41.963351Z","shell.execute_reply.started":"2021-06-30T15:06:41.940664Z","shell.execute_reply":"2021-06-30T15:06:41.961859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extractTweets(df, df_source):\n    for i in range(len(df_source)):\n\n        tweet_date = df_source['date'][i]\n        content = df_source['content'][i]\n        tweet_id = df_source['id'][i]\n        user_id = df_source['user'][i]['id']\n        user_username = df_source['user'][i]['username']\n        user_is_verified = df_source['user'][i]['verified']\n        user_created = df_source['user'][i]['created']\n        user_followers = df_source['user'][i]['followersCount']\n        user_statuses_count = df_source['user'][i]['statusesCount']\n        tweet_reply_count = df_source['replyCount'][i]\n        tweet_retweet_count = df_source['retweetCount'][i]\n        tweet_like_count = df_source['likeCount'][i]\n        tweet_lang = df_source['lang'][i]\n        quoted_tweet_id = df_source['quotedTweet'][i]\n        replied_tweet_id = df_source['inReplyToTweetId'][i]\n\n        if(df_source['inReplyToUser'][i] != None):\n            replied_user = df_source['inReplyToUser'][i]['id']\n        else:\n            replied_user = None\n\n        mentioned_users_id = []\n        mentioned_users_username = []\n        if(df_source['mentionedUsers'][i] != None):\n            for mentioned_users in df_source['mentionedUsers'][i]:\n                mentioned_users_id.append(mentioned_users['id'])\n                mentioned_users_username.append(mentioned_users['username'])\n        else: \n            mentioned_users_id = None\n            mentioned_users_username = None\n\n        if(df_source['place'][i] != None):\n            tweet_place = df_source[\"place\"][i]['name']\n            tweet_country = df_source[\"place\"][i]['country']\n        else: \n            tweet_place = None\n            tweet_country = None\n\n        a_row = pd.Series([tweet_date, content, tweet_id, user_id, user_username, user_is_verified, user_created, user_followers, user_statuses_count, tweet_reply_count, tweet_retweet_count, tweet_like_count, tweet_lang, quoted_tweet_id, replied_tweet_id, replied_user, mentioned_users_id, mentioned_users_username, tweet_place, tweet_country])\n        row_df = pd.DataFrame([a_row], index = [tweet_id])\n        df = pd.concat([df, row_df])\n    \n    index = df.index\n    is_duplicate = index.duplicated(keep=\"first\")\n    not_duplicate = ~is_duplicate\n    df = df[not_duplicate]\n    \n    print(sum(is_duplicate),\"data with non-unique indices deleted\")\n    print(sum(not_duplicate),\"data successfully extracted\")\n    print(\"total data: \", len(df))\n    print(\"\\n\")\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:06:41.96491Z","iopub.execute_input":"2021-06-30T15:06:41.965477Z","iopub.status.idle":"2021-06-30T15:06:41.983554Z","shell.execute_reply.started":"2021-06-30T15:06:41.965433Z","shell.execute_reply":"2021-06-30T15:06:41.982304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#del tweets_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:06:41.985128Z","iopub.execute_input":"2021-06-30T15:06:41.985447Z","iopub.status.idle":"2021-06-30T15:06:42.005049Z","shell.execute_reply.started":"2021-06-30T15:06:41.985416Z","shell.execute_reply":"2021-06-30T15:06:42.003637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_df = pd.DataFrame()\ntweets_df = extractTweets(tweets_df, df1)\ntweets_df = extractTweets(tweets_df, df2)\ntweets_df = extractTweets(tweets_df, df3)\ntweets_df = extractTweets(tweets_df, df4)\ntweets_df = extractTweets(tweets_df, df5)\ntweets_df = extractTweets(tweets_df, df6)\ntweets_df = extractTweets(tweets_df, df7)\ntweets_df = extractTweets(tweets_df, df8)\n\ntweets_df = tweets_df.rename(columns={0: \"tweet_date\", \n                                        1: \"content\", \n                                        2: \"tweet_id\", \n                                        3: \"user_id\",\n                                        4: \"user_username\",\n                                        5: \"user_is_verified\",\n                                        6: \"user_created\",\n                                        7: \"user_statuses_count\",\n                                        8: \"user_followers\",\n                                        9: \"tweet_reply_count\",\n                                        10: \"tweet_retweet_count\",\n                                        11: \"tweet_like_count\",\n                                        12: \"tweet_lang\",\n                                        13: \"quoted_tweet_id\",\n                                        14: \"replied_tweet_id\",\n                                        15: \"replied_user\",\n                                        16: \"mentioned_users_id\",\n                                        17: \"mentioned_users_username\",\n                                        18: \"tweet_place\",\n                                        19: \"tweet_country\"}, errors=\"raise\")\ntweets_df = tweets_df.reset_index(drop=True)\ntweets_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:06:42.007822Z","iopub.execute_input":"2021-06-30T15:06:42.008345Z","iopub.status.idle":"2021-06-30T15:08:37.612362Z","shell.execute_reply.started":"2021-06-30T15:06:42.0083Z","shell.execute_reply":"2021-06-30T15:08:37.61078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:37.615679Z","iopub.execute_input":"2021-06-30T15:08:37.616246Z","iopub.status.idle":"2021-06-30T15:08:37.663631Z","shell.execute_reply.started":"2021-06-30T15:08:37.61619Z","shell.execute_reply":"2021-06-30T15:08:37.662738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alay_dict = pd.read_csv(\"../input/indonesian-abusive-and-hate-speech-twitter-text/new_kamusalay.csv\", encoding = 'latin-1', header=None)\nalay_dict = alay_dict.rename(columns=  {0: 'original',\n                                        1: 'replacement'})\nstopword_dict = pd.read_csv('../input/indonesian-stoplist/stopwordbahasa.csv', header=None)\nstopword_dict = stopword_dict.rename(columns={0: 'stopword'})","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:37.665009Z","iopub.execute_input":"2021-06-30T15:08:37.665334Z","iopub.status.idle":"2021-06-30T15:08:37.724845Z","shell.execute_reply.started":"2021-06-30T15:08:37.665303Z","shell.execute_reply":"2021-06-30T15:08:37.72358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-processing","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nimport pytz","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:37.726301Z","iopub.execute_input":"2021-06-30T15:08:37.726652Z","iopub.status.idle":"2021-06-30T15:08:37.730884Z","shell.execute_reply.started":"2021-06-30T15:08:37.726614Z","shell.execute_reply":"2021-06-30T15:08:37.729985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_date(dt_object):\n    tz = pytz.timezone(\"Asia/Jakarta\")\n    local_dt = dt_object.astimezone(tz)\n    return local_dt.strftime(\"%d %b %Y\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:37.73265Z","iopub.execute_input":"2021-06-30T15:08:37.733087Z","iopub.status.idle":"2021-06-30T15:08:37.746499Z","shell.execute_reply.started":"2021-06-30T15:08:37.733042Z","shell.execute_reply":"2021-06-30T15:08:37.745491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets_df[\"tweet_date\"] = tweets_df[\"tweet_date\"].apply(convert_date)\nprint(tweets_df[\"tweet_date\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:37.748145Z","iopub.execute_input":"2021-06-30T15:08:37.748743Z","iopub.status.idle":"2021-06-30T15:08:38.029439Z","shell.execute_reply.started":"2021-06-30T15:08:37.748707Z","shell.execute_reply":"2021-06-30T15:08:38.028466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()\n\n#lowercase\ndef lowercase(text):\n    return text.lower()\n\ndef remove_unnecessary_char(text):\n    text = re.sub('\\n',' ',text) # Remove every '\\n'\n    text = re.sub('rt',' ',text) # Remove every retweet symbol\n    text = re.sub('((@[^\\s]+)|(#[^\\s]+))',' ',text)\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n    text = re.sub('  +', ' ', text) # Remove extra spaces\n    return text\n    \ndef remove_nonaplhanumeric(text):\n    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n    return text\n\nalay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\ndef normalize_alay(text):\n    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n\ndef remove_stopword(text):\n    text = ' '.join(['' if word in stopword_dict.stopword.values else word for word in text.split(' ')])\n    text = re.sub('  +', ' ', text) # Remove extra spaces\n    text = text.strip()\n    return text\n\ndef stemming(text):\n    return stemmer.stem(text)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:38.031144Z","iopub.execute_input":"2021-06-30T15:08:38.031602Z","iopub.status.idle":"2021-06-30T15:08:38.082277Z","shell.execute_reply.started":"2021-06-30T15:08:38.031541Z","shell.execute_reply":"2021-06-30T15:08:38.081057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function test\ntesting_tweet = 'wkwkw berwawasan DUNIAWI abis RT @_rizkaAnggie \"The quick brown fox jumps over the lazy dog\" www.wikipedia.org #mantap_gan'\n\n\nprint(testing_tweet, \"\\t (original tweet)\")\ntesting_tweet = lowercase(testing_tweet)\nprint(testing_tweet, \"\\t (lowercase)\")\ntesting_tweet = remove_unnecessary_char(testing_tweet)\nprint(testing_tweet, \"\\t (remove_unnecessary_char)\")\ntesting_tweet = remove_nonaplhanumeric(testing_tweet)\nprint(testing_tweet, \"\\t (remove_nonaplhanumeric)\")\ntesting_tweet = normalize_alay(testing_tweet)\nprint(testing_tweet, \"\\t (normalize_alay)\")\ntesting_tweet = remove_stopword(testing_tweet)\nprint(testing_tweet, \"\\t (remove_stopword)\")\ntesting_tweet = stemming(testing_tweet)\nprint(testing_tweet, \"\\t (stemming)\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:38.083572Z","iopub.execute_input":"2021-06-30T15:08:38.083879Z","iopub.status.idle":"2021-06-30T15:08:38.112479Z","shell.execute_reply.started":"2021-06-30T15:08:38.083851Z","shell.execute_reply":"2021-06-30T15:08:38.111094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(text):\n    text = lowercase(text) # 1\n    text = remove_unnecessary_char(text) # 2\n    text = remove_nonaplhanumeric(text) # 3\n    text = normalize_alay(text) # 4\n    text = remove_stopword(text) # 5\n    text = stemming(text) # 6\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:38.114555Z","iopub.execute_input":"2021-06-30T15:08:38.115031Z","iopub.status.idle":"2021-06-30T15:08:38.133596Z","shell.execute_reply.started":"2021-06-30T15:08:38.114984Z","shell.execute_reply":"2021-06-30T15:08:38.132361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tweets_df['content'][2])\nprint(preprocess(tweets_df['content'][2]))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:38.135374Z","iopub.execute_input":"2021-06-30T15:08:38.13582Z","iopub.status.idle":"2021-06-30T15:08:38.155967Z","shell.execute_reply.started":"2021-06-30T15:08:38.135769Z","shell.execute_reply":"2021-06-30T15:08:38.1546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_tweets_df = pd.DataFrame()\nclean_tweets_df['tweet_id'] = tweets_df['tweet_id']\nclean_tweets_df['tweet'] = tweets_df['content'].apply(preprocess)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:38.157623Z","iopub.execute_input":"2021-06-30T15:08:38.158469Z","iopub.status.idle":"2021-06-30T15:08:54.559415Z","shell.execute_reply.started":"2021-06-30T15:08:38.15841Z","shell.execute_reply":"2021-06-30T15:08:54.558137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_tweets_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:54.561117Z","iopub.execute_input":"2021-06-30T15:08:54.561438Z","iopub.status.idle":"2021-06-30T15:08:54.581979Z","shell.execute_reply.started":"2021-06-30T15:08:54.561409Z","shell.execute_reply":"2021-06-30T15:08:54.580679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# wordcloud","metadata":{}},{"cell_type":"code","source":"from os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:54.583606Z","iopub.execute_input":"2021-06-30T15:08:54.583971Z","iopub.status.idle":"2021-06-30T15:08:54.647517Z","shell.execute_reply.started":"2021-06-30T15:08:54.583916Z","shell.execute_reply":"2021-06-30T15:08:54.646481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \" \".join(review for review in clean_tweets_df.tweet)\nprint (\"There are {} words in the combination of all review.\".format(len(text)))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:54.648924Z","iopub.execute_input":"2021-06-30T15:08:54.649282Z","iopub.status.idle":"2021-06-30T15:08:54.660268Z","shell.execute_reply.started":"2021-06-30T15:08:54.649247Z","shell.execute_reply":"2021-06-30T15:08:54.658807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords = set(STOPWORDS)\nstopwords.update([\"nya\", \"ya\", \"kayak\", \"sih\", \"deh\", \"yang\", \"biar\"])\n\nwordcloud = WordCloud(stopwords=stopwords, max_font_size=100, max_words=200, background_color=\"black\", width=1200, height=800, colormap=\"Blues\").generate(text)\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:54.664349Z","iopub.execute_input":"2021-06-30T15:08:54.664901Z","iopub.status.idle":"2021-06-30T15:08:58.009751Z","shell.execute_reply.started":"2021-06-30T15:08:54.664865Z","shell.execute_reply":"2021-06-30T15:08:58.008734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud.to_file(\"wordcloud1.png\")","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:58.014041Z","iopub.execute_input":"2021-06-30T15:08:58.014414Z","iopub.status.idle":"2021-06-30T15:08:58.868588Z","shell.execute_reply.started":"2021-06-30T15:08:58.014377Z","shell.execute_reply":"2021-06-30T15:08:58.867475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"import csv","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:58.870484Z","iopub.execute_input":"2021-06-30T15:08:58.870829Z","iopub.status.idle":"2021-06-30T15:08:58.875243Z","shell.execute_reply.started":"2021-06-30T15:08:58.870791Z","shell.execute_reply":"2021-06-30T15:08:58.874089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lexicon_positive = dict()\nwith open('../input/inset-lexicon/positive.csv') as csvfile:\n    reader = csv.reader(csvfile, delimiter=',')\n    next(reader)\n    for row in reader:\n        lexicon_positive[row[0]] = int(row[1])\n\nlexicon_negative = dict()\nwith open('../input/inset-lexicon/negative.csv') as csvfile:\n    reader = csv.reader(csvfile, delimiter=',')\n    next(reader)\n    for row in reader:\n        lexicon_negative[row[0]] = int(row[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:58.877231Z","iopub.execute_input":"2021-06-30T15:08:58.877593Z","iopub.status.idle":"2021-06-30T15:08:58.932393Z","shell.execute_reply.started":"2021-06-30T15:08:58.877562Z","shell.execute_reply":"2021-06-30T15:08:58.931432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentiment_analysis_lexicon_indonesia(text):\n    #for word in text:\n    score = 0\n    for word in text:\n        if (word in lexicon_positive):\n            score = score + lexicon_positive[word]\n    for word in text:\n        if (word in lexicon_negative):\n            score = score + lexicon_negative[word]\n    polarity=''\n    if (score > 0):\n        polarity = 'positive'\n    elif (score < 0):\n        polarity = 'negative'\n    else:\n        polarity = 'neutral'\n    return score, polarity","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:58.933614Z","iopub.execute_input":"2021-06-30T15:08:58.934096Z","iopub.status.idle":"2021-06-30T15:08:58.940423Z","shell.execute_reply.started":"2021-06-30T15:08:58.934048Z","shell.execute_reply":"2021-06-30T15:08:58.939195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_tweets_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:58.942157Z","iopub.execute_input":"2021-06-30T15:08:58.942533Z","iopub.status.idle":"2021-06-30T15:08:58.967889Z","shell.execute_reply.started":"2021-06-30T15:08:58.942438Z","shell.execute_reply":"2021-06-30T15:08:58.966828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_words = pd.Series()\nclean_tweets_df.insert(len(clean_tweets_df.columns), 'tweet_words', '')\n\nfor i, text in enumerate(clean_tweets_df['tweet']):\n    list_words=[]\n    for word in clean_tweets_df['tweet'][i].split():\n        list_words.append(word)\n        \n    clean_tweets_df['tweet_words'][i] = list_words \nclean_tweets_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:08:58.969283Z","iopub.execute_input":"2021-06-30T15:08:58.969616Z","iopub.status.idle":"2021-06-30T15:09:03.365199Z","shell.execute_reply.started":"2021-06-30T15:08:58.969584Z","shell.execute_reply":"2021-06-30T15:09:03.364065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = clean_tweets_df['tweet_words'].apply(sentiment_analysis_lexicon_indonesia)\nresults = list(zip(*results))\nclean_tweets_df['polarity_score'] = results[0]\nclean_tweets_df['polarity'] = results[1]\nprint(clean_tweets_df['polarity'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:09:03.366549Z","iopub.execute_input":"2021-06-30T15:09:03.366888Z","iopub.status.idle":"2021-06-30T15:09:03.442578Z","shell.execute_reply.started":"2021-06-30T15:09:03.366856Z","shell.execute_reply":"2021-06-30T15:09:03.44132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_tweets_df[\"raw_tweet\"] = tweets_df['content']","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:09:03.444274Z","iopub.execute_input":"2021-06-30T15:09:03.444714Z","iopub.status.idle":"2021-06-30T15:09:03.451206Z","shell.execute_reply.started":"2021-06-30T15:09:03.444665Z","shell.execute_reply":"2021-06-30T15:09:03.45017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', 3000)\npositive_tweets = clean_tweets_df[clean_tweets_df['polarity'] == 'positive']\npositive_tweets = positive_tweets[['raw_tweet', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=False).reset_index(drop = True)\npositive_tweets.index += 1\npositive_tweets[0:10]","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-06-30T15:09:03.452664Z","iopub.execute_input":"2021-06-30T15:09:03.453026Z","iopub.status.idle":"2021-06-30T15:09:03.483955Z","shell.execute_reply.started":"2021-06-30T15:09:03.452988Z","shell.execute_reply":"2021-06-30T15:09:03.482945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"negative_tweets = clean_tweets_df[clean_tweets_df['polarity'] == 'negative']\nnegative_tweets = negative_tweets[['raw_tweet', 'polarity_score', 'polarity']].sort_values(by = 'polarity_score', ascending=True).reset_index(drop = True)\nnegative_tweets.index += 1\nnegative_tweets[0:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:09:03.485581Z","iopub.execute_input":"2021-06-30T15:09:03.486021Z","iopub.status.idle":"2021-06-30T15:09:03.509373Z","shell.execute_reply.started":"2021-06-30T15:09:03.485974Z","shell.execute_reply":"2021-06-30T15:09:03.508148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:09:03.511207Z","iopub.execute_input":"2021-06-30T15:09:03.51165Z","iopub.status.idle":"2021-06-30T15:09:04.390499Z","shell.execute_reply.started":"2021-06-30T15:09:03.511602Z","shell.execute_reply":"2021-06-30T15:09:04.389588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_tweets_df[\"date\"]  = clean_tweets_df.tweet_id.map(tweets_df.set_index('tweet_id')['tweet_date'])","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:09:04.391675Z","iopub.execute_input":"2021-06-30T15:09:04.392137Z","iopub.status.idle":"2021-06-30T15:09:04.406776Z","shell.execute_reply.started":"2021-06-30T15:09:04.392099Z","shell.execute_reply":"2021-06-30T15:09:04.405154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_tweets_df","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:09:04.408607Z","iopub.execute_input":"2021-06-30T15:09:04.409214Z","iopub.status.idle":"2021-06-30T15:09:04.641832Z","shell.execute_reply.started":"2021-06-30T15:09:04.409176Z","shell.execute_reply":"2021-06-30T15:09:04.641002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def groupbyPolarity(g):\n    # do computation\n    return g.groupby(['polarity']).count()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:09:04.643198Z","iopub.execute_input":"2021-06-30T15:09:04.643735Z","iopub.status.idle":"2021-06-30T15:09:04.647554Z","shell.execute_reply.started":"2021-06-30T15:09:04.6437Z","shell.execute_reply":"2021-06-30T15:09:04.646762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_count = clean_tweets_df[[\"date\",\"polarity\",\"tweet\"]].groupby(['date','polarity']).count().reset_index()\ndf_count['date'] = pd.to_datetime(df_count.date)\ndf_count.sort_values(by='date')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:09:04.648759Z","iopub.execute_input":"2021-06-30T15:09:04.649274Z","iopub.status.idle":"2021-06-30T15:09:04.697331Z","shell.execute_reply.started":"2021-06-30T15:09:04.649241Z","shell.execute_reply":"2021-06-30T15:09:04.695793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_count = df_count.pivot(index=\"date\", columns=\"polarity\", values=\"tweet\")\ndf_count.plot(figsize=(16,8))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T15:09:04.699112Z","iopub.execute_input":"2021-06-30T15:09:04.699529Z","iopub.status.idle":"2021-06-30T15:09:05.147207Z","shell.execute_reply.started":"2021-06-30T15:09:04.699493Z","shell.execute_reply":"2021-06-30T15:09:05.145762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{},"execution_count":null,"outputs":[]}]}