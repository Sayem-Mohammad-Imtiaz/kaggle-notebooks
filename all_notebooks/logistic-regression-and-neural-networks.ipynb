{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py","name":"python","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.1"}},"nbformat":4,"nbformat_minor":0,"cells":[{"metadata":{"collapsed":false,"_cell_guid":"b317a81b-59d6-4807-8fed-aa5950be75f0","_execution_state":"idle","_uuid":"cbff7aedb20b9c41a474d0c0f5fdb3800849b80d"},"source":"Hi, this is my first kernel on Kaggle. I didn't look at any of the state of the art methods used for this dataset (still haven't), just went in with a purpose of implementing the algorithms which I have studied, while maintaining the tradeoff between bias and variance. People new to the whole scikit-learn and machine learning world are the ones most likely to benefit from this kernel.","execution_count":null,"cell_type":"markdown","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"6ee1e30e-3d24-4414-a5db-2ce1695c20ba","_execution_state":"idle","_uuid":"26ade93bddc2a5ff9ecc9a4dad0873d227573a25"},"source":"The dataset contains 30 attributes of patients - and a class label telling whether the tumour is malignant or benign. There are a total of 357 benign objects and 212 malignant objects. We start by loading the packages, importing the dataset and mapping 'B' and 'M' class labels to integers, and separating the attributes and class labels.","execution_count":null,"cell_type":"markdown","outputs":[]},{"metadata":{"_cell_guid":"99b466e3-afac-43f8-9c2a-ca72b5edb978","_execution_state":"idle","_uuid":"f158fcce65d2762f290b7b2d5be6c7ab40dd6b71","trusted":false},"source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nfrom sklearn import linear_model #for logistic regression\nfrom sklearn.neural_network import MLPClassifier #for neural network\nfrom sklearn.model_selection import KFold, GridSearchCV, cross_val_score, cross_val_predict, validation_curve \n#GridSearchCV is used to optimize parameters of the models used\n#the other modules and functions \nfrom sklearn.ensemble import VotingClassifier #for creating ensembles of classifiers\n\ndf = pd.read_csv('../input/data.csv', skiprows=[0], header=None)\ndf = df.replace({'B':0, 'M':1})\nx = df.iloc[:,2:] \ny = df.iloc[:,1]\nprint (x.shape, y.shape)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"853671d8-c653-41d2-a264-f17162a1b4a3","_execution_state":"idle","_uuid":"e59122e7e29dce2f372c84078173b90f1d254001"},"source":"Now, we normalize the data. Normalization is a bit of a \"controversial\" subject (for lack of a better term). I tried to research on this a bit by looking at questions on [quora][1] and [stackexchange][2]. If you're using regularization, it makes sense to normalize your input, while at the same time, you should not normalize if you are trying to interpret and explain the coefficients and relate them to the features. Since this kernel is aimed at being more of an introduction to solving problems using scikit-learn and pandas, I decided to not focus on excessive explorations of coefficients. In hindsight, it seems I could have thought more on whether to normalize or not.\n\n\n  [1]: https://www.quora.com/Should-input-data-to-logistic-regression-be-normalized\n  [2]: https://stats.stackexchange.com/questions/189652/is-it-a-good-practice-to-always-scale-normalize-data-for-machine-learning","execution_count":null,"cell_type":"markdown","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"3c018123-cb71-46e7-b549-de6af2ce60fd","_execution_state":"idle","_uuid":"cebe3d89d645a771621e854a792aef01c2e4d2df","trusted":false},"source":"x_mean = x.mean()\nx_std = x.std()\nx_norm = (x - x_mean)/x_std\nprint (x_norm.shape)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"095fabb8-c0da-4d07-ac05-b793ac9777c2","_execution_state":"idle","_uuid":"6847abc632e56853970b99453afa2bd45f1ffa8e"},"source":"We start by using a simple logistic regression model, and use K fold cross validation to get the accuracy on the dataset. K fold cross validation is a method used to prevent overfitting (A situation where your model fits the training data *too* well but does not generalize well enough to data which is outside the training set). \n\nHere, our training data is divided into 5 parts, model is generated for the 4 parts, and tested on the 5th part. This is done 5 times by using different combinations of these parts as training and test sets, and eventually an average of all these models is used to get the final accuracy. \n\nFind out more about cross validation [here][1] and scikit specific information here\n\n\n  [1]: https://en.wikipedia.org/wiki/Cross-validation_(statistics)","execution_count":null,"cell_type":"markdown","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"5b02476e-fad9-41ce-8498-35f1ebbd25d7","_execution_state":"idle","_uuid":"1093b46846c393829713296655dc8db382fd897f","trusted":false},"source":"logreg = linear_model.LogisticRegression()\nkfold = KFold(n_splits=5,random_state=7)\ncv_results = cross_val_score(logreg, x_norm, y, cv=kfold)\nprint (cv_results.mean()*100, \"%\")","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"cdced4c4-7bad-421e-8512-87f719888acc","_execution_state":"idle","_uuid":"02fa6892f153ad0256eaa4f8e33eb1d965c6d2b8"},"source":"So, we can see that using logistic regression gave us an accuracy of 97.7% on the dataset.\n\nNext, we optimize the parameters of our model. For logistic regression, it makes sense to look at the parameter C, which is the inverse of the regularization parameter. The lower the value of C, the higher we penalize the coefficients of our logistic regression.","execution_count":null,"cell_type":"markdown","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"8e2ee4b7-d3ce-4b4e-8aa3-f50ddbf50222","_execution_state":"idle","_uuid":"4dcb5be57b4f37dc8b721a474faf53b4edea8efb","trusted":false},"source":"logreg = linear_model.LogisticRegression()\nparam_grid = {\"C\":[0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]}\ngrid = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=kfold)\ngrid.fit(x_norm,y)\nprint (grid.best_estimator_)\nprint (grid.best_score_*100, \"%\")","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"dd4947aa-b838-4bbb-9bde-04a89708ac96","_execution_state":"idle","_uuid":"9f9aa8e08869ed963dad05d3930083429d047dda"},"source":"As we can see, our accuracy has increased slightly to 97.89%. \n\nLets now look at the validation curve and confirm that we're not overfitting. For this, we need the individual training score and test scores (here, by test score I mean the average of scores on the 5 validation sets) for each of our 5 \"folds\", and plot them by varying C. For those values of C which give us a low training and high test score, we have high bias, and our model \"underfits\" the dataset. \n\nAt some point, the test score starts decreasing with increase in value of C, and this is said to be \"overfitting\" of the dataset (because our model fits the training data too well, but fails to generalize on the test set). The middle ground, where the test score is highest, is the value of C we are looking for.","execution_count":null,"cell_type":"markdown","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"7888c0f8-5dce-4cb6-afcd-da0f3866e032","_execution_state":"idle","_uuid":"7b92104f2628e4948a6dbc887c9ff7d1b3f1cc7f","trusted":false},"source":"#plot validation curve\nnum_splits = 5\nnum_C_values = 10 # we iterate over 10 possible C values\nlogreg = linear_model.LogisticRegression()\nkfold = KFold(n_splits=5,random_state=7)\nC_values = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\ntrain_scores, valid_scores = validation_curve(logreg, x_norm, y, \"C\", C_values, cv=kfold)\ntrain_scores = pd.DataFrame(data=train_scores, index=np.arange(0, num_C_values), columns=np.arange(0,num_splits)) \nvalid_scores = pd.DataFrame(data=valid_scores, index=np.arange(0, num_C_values), columns=np.arange(0,num_splits)) \nplt.semilogx(C_values, train_scores.mean(axis=1), label='training score')\nplt.semilogx(C_values, valid_scores.mean(axis=1), label='test score')\nplt.xlabel('C')\nplt.legend()","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"0662368a-1cec-431f-a1e2-608e063cae95","_execution_state":"idle","_uuid":"81f0d6c34e56bc0b18e9166357c8c167ba13b6bb"},"source":"As we can see, the optimum point is at C=0.1, where we get an accuracy of 97.89%.\n\nNow we move on to the next classifier - Neural Network. We choose 'lbfgs' solver, which works better on small datasets. For the architecture of the neural network, I decided to use 1 hidden layer (which is the standard for most NN problems). \n\nFor the number of hidden units, initially I tried to do this experimentally, I tried 5, 10, 15 and the default for scikit (which is 100). 100 hidden units seemed to be giving the best accuracy.  But then I researched a bit more on this. I was confused about the role cross validation, regularization and number of hidden layers play in overfitting of the data. According to [this][1] post on the statistics stackexchange website, cross validation and regularization will reduce the amount of overfitting in your model, but it does not guarantee that there will be no overfitting. \n\nSo I decided to use a \"better\" method to decide the number of hidden nodes. According to [this][2] post, the mean of number of input output layers is a good approximation of the number of hidden layers to use. So, let us go ahead with 15 hidden neurons. Even though this gives a lesser score while using cross validation (compared to 100 hidden units), on combining the logistic regressiong and neural network model, using 15 hidden neurons instead of 100 gives a better accuracy. Thus we can <s>claim</s> hypothesize (since I did not exhaustively vary the weights of the 2 classifiers, only did it manually) that a 15 hidden neuron network is better and more \"general\" predictor model than the 100 hidden neuron neural network.\n\n  [1]: https://stats.stackexchange.com/questions/193661/is-cross-validation-enough-to-prevent-overfitting\n  [2]: https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw","execution_count":null,"cell_type":"markdown","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"f5831994-7c0f-4693-b092-24f392ddaf0f","_execution_state":"idle","_uuid":"6420e9e761f2ed21baa8c8a1d511e7401a505e7a","trusted":false},"source":"clf = MLPClassifier(solver='lbfgs', random_state=1, activation='logistic', hidden_layer_sizes=(15,))\nkfold = KFold(n_splits=5,random_state=7)\ncv_results = cross_val_score(clf, x_norm, y, cv=kfold)\nprint (cv_results.mean()*100, \"%\")","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"7565eeb3-7227-4dd0-b83d-32f451eba9f7","_execution_state":"idle","_uuid":"ef35d38bdb3b435b60cf521048b0a76c28e6c7a6"},"source":"On optimizing the parameter \"alpha\" (regularization parameter for neural network) in a similar way to what we did with the regularization parameter in logistic regression.","execution_count":null,"cell_type":"markdown","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"e6e10ce3-618f-4a6a-9e6f-d2cda07cc91f","_execution_state":"idle","_uuid":"426e794a885932353bbfd8710565e5fd4f9d7850","trusted":false},"source":"clf = MLPClassifier(solver='lbfgs', random_state=1, activation='logistic',  hidden_layer_sizes=(15,))\nparam_grid = {\"alpha\":10.0 ** -np.arange(-4, 7)}\ngrid = GridSearchCV(estimator=clf, param_grid=param_grid, cv=kfold)\ngrid.fit(x_norm,y)\nprint (grid.best_estimator_)\nprint (grid.best_score_*100, \"%\")","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"1a9aef67-515b-4550-b27a-33901321dced","_execution_state":"idle","_uuid":"8c6e545ac91bf874dc9f16f3d39eb733142745c3"},"source":"Thus, alpha = 1.0 gives an optimal accuracy of 97.7%.\n\nBoth models (Logistic regression and neural network models) seem to be giving a good accuracy. Lets see the misclassified examples of both models to figure out if we can combine them in some way.","execution_count":null,"cell_type":"markdown","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"630e6869-8be8-48cf-9900-a9d96894d949","_execution_state":"idle","_uuid":"9c8e90d80f4a8eff542bb4fe54bd029f6fecf310","trusted":false},"source":"logreg = linear_model.LogisticRegression(C=0.1)\nkfold = KFold(n_splits=5,random_state=7)\ncv_results = cross_val_score(logreg, x_norm, y, cv=kfold)\npredicted = cross_val_predict(logreg, x_norm, y, cv=kfold)\ndiff = predicted - y\nmisclass_indexes = diff[diff != 0].index.tolist()\nprint (misclass_indexes)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"ac4de4dc-787e-4a58-a23c-48d311a7388f","_execution_state":"idle","_uuid":"b3b73ad797cce025d92af43fa7f15e8f35fd6f1d","trusted":false},"source":"clf = MLPClassifier(solver='lbfgs', random_state=1, activation='logistic', alpha=1.0, hidden_layer_sizes=(15,))\nkfold = KFold(n_splits=5,random_state=7)\ncv_results = cross_val_score(clf, x_norm, y, cv=kfold)\npredicted = cross_val_predict(clf, x_norm, y, cv=kfold)\ndiff = predicted - y\nmisclass_indexes = diff[diff != 0].index.tolist()\nprint (misclass_indexes)","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"d74f299a-deb3-4b5c-a860-874d5c367b6f","_execution_state":"idle","_uuid":"9018042e589dee52a3b827558eb25fe548147005"},"source":"9 objects are misclassified by both classifiers, but we can improve the overall accuracy by using a combination of the 2 classfiers and assigning weights. (If we had 3 classifiers we would also have considered a majority voting ensemble). I played around a bit with the weights manually, and assigning a weight of 2 to logistic regression and 1 for the neural network gave the best accuracy (although not by a huge margin). While its tempting to relate this with the fact that logistic regression had a slightly better accuracy, and claim that this is a \"logical\" way to choose the weights, I am pretty sure that assigning the weights in this way is also some sort of \"overfitting\".","execution_count":null,"cell_type":"markdown","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"9d4f5882-04d3-47a9-a740-ed460fa4d307","_execution_state":"idle","_uuid":"52f096af6d50f38f230df7af7bc2ddb0ce37f795","trusted":false},"source":"clf1 = linear_model.LogisticRegression(C=0.1)\nclf2 = MLPClassifier(solver='lbfgs', alpha=1.0,hidden_layer_sizes=(15,), random_state=1, activation='logistic')\neclf = VotingClassifier(estimators=[('lr', clf1), ('nn', clf2)], voting='soft', weights=[2,1])\ncv_results = cross_val_score(eclf, x_norm, y, cv=kfold)\nprint (cv_results.mean()*100, \"%\")","execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"collapsed":false,"_cell_guid":"c1526ea8-dc6e-411f-add4-89fe369e1cf8","_execution_state":"idle","_uuid":"d48f1ff50d2dc4747f85b11fd876a92b1bdfdf33"},"source":"So, we observe that the combined classifier improves our overall accuracy to 98.24% \n\nThat's it for this kernel. I learnt a lot while working on this dataset, definitely made my share of mistakes. I would really appreciate any sort of feedback that you might have. For people new to the field, I hope this could be of some help to you in understanding the general flow of working through a dataset to solve a machine learning problem.","execution_count":null,"cell_type":"markdown","outputs":[]}]}