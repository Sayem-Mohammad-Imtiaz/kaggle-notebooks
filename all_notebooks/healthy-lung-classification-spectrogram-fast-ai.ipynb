{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nIn general, stethoscope is an acoustic medical instrument that be used by the doctor to diagnose the problem in the heart or lungs.\n\nHowever, with the rise of deep learning in the field of Image Processing and Audio Processing, I believe deep learning model can achieved the acceptable level of disease diagnosis. If we can get to that state, stethoscope can help the patient or anyone who want to monitor their health, and inform the doctor if they find something anormaly. It might also be very helpful in this period of pandemic where we need to limit the contact as much as possible \n\n## Technical Disscussion\n\nI've played with this dataset and found it is very unbalance. We have so many subjects with COPD and very small sample of others classes. I've tried to classify the disease. Even though reaching quite high accuracy in total (~ 95%), there are some classes which has very low accuracy. That make me change idea to classify healthy/unhealthy first.\n\nI use Xresnet18 to classify spectrogram image with carefully splitting classes in train/valid dataset and use oversampling for handling unbalance dataset. The code is quite neat thanks to fast.ai","metadata":{}},{"cell_type":"code","source":"! pip install -Uqq fastai\n! pip install -qq torchaudio==0.7.0\n! pip install -qq librosa","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *\nimport torchaudio\nimport pathlib\nimport librosa\nfrom IPython.display import Audio\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nwarnings.filterwarnings(\"ignore\")\n%config Completer.use_jedi = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Helper method for getting audio files, getting label and configuration for audio processing","metadata":{}},{"cell_type":"code","source":"mypath = \"../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/\"\nfilenames = get_files(mypath, extensions='.wav')\nfilenames","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_diag = pd.read_csv(\"../input/respiratory-sound-database/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv\",header=None) # patient diagnosis file\np_diag.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# configuration for audio processing\nn_fft=1024\nhop_length=256\ntarget_rate=44100\nnum_samples=int(target_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Method for labelling sample (Healthy/Unhealthy)\ndef get_y(path): \n    desease = p_diag[p_diag[0] == int(path.stem[:3])][1].values[0]\n    if desease == \"Healthy\":\n        return \"Healthy\"\n    else : \n        return \"Unhealthy\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Method for getting all audio files, I get file withc rate 44100 Hz only because resampling take so much time :( \ndef get_items(path): \n    fns = [fn for fn in get_files(path, extensions='.wav') if torchaudio.load_wav(fn)[1] == target_rate]\n    return fns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Helper method to tranform audio array to Spectrogram\nau2spec = torchaudio.transforms.MelSpectrogram(sample_rate=target_rate,n_fft=n_fft, hop_length=hop_length, n_mels=256)\nampli2db = torchaudio.transforms.AmplitudeToDB()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_x(path, target_rate=target_rate, num_samples=num_samples*2):\n    x, rate = torchaudio.load_wav(path)\n    if rate != target_rate: \n        x = torchaudio.transforms.Resample(orig_freq=rate, new_freq=target_rate, resampling_method='sinc_interpolation')(x)\n    x = x[0] / 32768\n    x = x.numpy()\n    sample_total = x.shape[0]\n    randstart = random.randint(target_rate, sample_total-target_rate*3)\n    x = x[randstart:num_samples+randstart]\n    x = librosa.util.fix_length(x, num_samples)\n    torch_x = torch.tensor(x)\n    spec = au2spec(torch_x)\n    spec_db = ampli2db(spec)\n    spec_db = spec_db.data.squeeze(0).numpy()\n    spec_db = spec_db - spec_db.min()\n    spec_db = spec_db/spec_db.max()*255\n    return spec_db","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Getting all files and labels\nitems = get_items(mypath)\nlabels = [get_y(item) for item in items]\nCounter(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the labels's counter above, we reconfirm that the dataset is very unbalance","metadata":{}},{"cell_type":"markdown","source":"the train_test_split method below here is to guarantee the classes is spllited equally in train and validation set, too avoid the problem that we just have Unhealthy samples in validations set. Details can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n","metadata":{}},{"cell_type":"code","source":"test_size=0.3\nsplitter = TrainTestSplitter(test_size=test_size, random_state=42, stratify=labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"db = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_x=get_x,\n    get_y=get_y,\n    splitter=splitter,\n    item_tfms=[Resize(256)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dsets = db.datasets(items)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dsets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To overcome the problem of unbalance dataset, we try to weight the probability of classes's apperance using WeightDataLoader","metadata":{}},{"cell_type":"code","source":"count = Counter(labels)\nwgts = [1/count[dsets.vocab[label]] for img, label in dsets.train]\nwgts[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each weight in the list above is the probability of each file will appear in a batch. ","metadata":{}},{"cell_type":"code","source":"dls = db.dataloaders(items, num_workers=2, dl_type=WeightedDL, wgts=wgts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To reconfirm we have balance classes in each batch. We try to get one batch and see if the distribution of each class is equal","metadata":{}},{"cell_type":"code","source":"x, y = dls.one_batch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(y)/len(y)\n## ~50% => we are fine here","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls.show_batch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## We use xresnet18 as model\nlearn = cnn_learner(dls, xresnet18, metrics=error_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## model learning\nlearn.fine_tune(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.show_results()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}