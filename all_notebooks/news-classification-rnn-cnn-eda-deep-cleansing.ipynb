{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"    LinkedIn : https://www.linkedin.com/in/nikenamelia/\n    Github   : https://github.com/nikenaml\n\n\n# Info Dataset - AG News Classification Dataset\n### Description\nThe AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. \n\nConsists of class ids 1-4 where \n- 1-World\n- 2-Sports \n- 3-Business \n- 4-Sci/Tech\n\nhttps://www.kaggle.com/amananandrai/ag-news-classification-dataset","metadata":{}},{"cell_type":"code","source":"# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport string\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_rows', 2000)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-30T02:16:30.126205Z","iopub.execute_input":"2021-06-30T02:16:30.126638Z","iopub.status.idle":"2021-06-30T02:16:30.141317Z","shell.execute_reply.started":"2021-06-30T02:16:30.1266Z","shell.execute_reply":"2021-06-30T02:16:30.140151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# join data\n\nfrom glob import glob\n\nfilename = 'dataset_news.csv'\n\nwith open(filename, 'a') as singleFile:\n    first_csv = True\n    for csv in glob('../input/ag-news-classification-dataset/*.csv'):\n        if csv == filename:\n            pass\n        else:\n            header = True\n            for line in open(csv, 'r'):\n                if first_csv and header:\n                    singleFile.write(line)\n                    first_csv = False\n                    header = False\n                elif header:\n                    header = False\n                else:\n                    singleFile.write(line)\n    singleFile.close()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:16:30.143167Z","iopub.execute_input":"2021-06-30T02:16:30.143578Z","iopub.status.idle":"2021-06-30T02:16:30.715968Z","shell.execute_reply.started":"2021-06-30T02:16:30.143539Z","shell.execute_reply":"2021-06-30T02:16:30.714832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"./dataset_news.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:16:30.718077Z","iopub.execute_input":"2021-06-30T02:16:30.718616Z","iopub.status.idle":"2021-06-30T02:16:31.334203Z","shell.execute_reply.started":"2021-06-30T02:16:30.718562Z","shell.execute_reply":"2021-06-30T02:16:31.332936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Undestanding","metadata":{}},{"cell_type":"markdown","source":"## EDA and get insights","metadata":{}},{"cell_type":"code","source":"# total data\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:16:31.33624Z","iopub.execute_input":"2021-06-30T02:16:31.336669Z","iopub.status.idle":"2021-06-30T02:16:31.343726Z","shell.execute_reply.started":"2021-06-30T02:16:31.336623Z","shell.execute_reply":"2021-06-30T02:16:31.342547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data info\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:16:31.345551Z","iopub.execute_input":"2021-06-30T02:16:31.346004Z","iopub.status.idle":"2021-06-30T02:16:31.399766Z","shell.execute_reply.started":"2021-06-30T02:16:31.345936Z","shell.execute_reply":"2021-06-30T02:16:31.398577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# categories\ndata['Class Index'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:16:31.401174Z","iopub.execute_input":"2021-06-30T02:16:31.401526Z","iopub.status.idle":"2021-06-30T02:16:31.410138Z","shell.execute_reply.started":"2021-06-30T02:16:31.401489Z","shell.execute_reply":"2021-06-30T02:16:31.409004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation\n\n","metadata":{}},{"cell_type":"code","source":"# join columns\ndata['news'] = data['Title'] + ' ' + data['Description']\n\n# rename columns\ndata = data.rename(columns = {'Class Index': 'category'}, inplace = False)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:16:31.412701Z","iopub.execute_input":"2021-06-30T02:16:31.413036Z","iopub.status.idle":"2021-06-30T02:16:31.543704Z","shell.execute_reply.started":"2021-06-30T02:16:31.412992Z","shell.execute_reply":"2021-06-30T02:16:31.542581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# delete columns (unused column)\ndata = data.drop(data.columns[[1, 2]], axis=1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:16:31.54583Z","iopub.execute_input":"2021-06-30T02:16:31.546147Z","iopub.status.idle":"2021-06-30T02:16:31.594025Z","shell.execute_reply.started":"2021-06-30T02:16:31.546118Z","shell.execute_reply":"2021-06-30T02:16:31.592888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# define labels\nlabels = ['World News', 'Sports News', 'Business News', 'Sci/Tech News']\n\nax = sns.countplot(data.category)\nplt.xlabel('category news')\nax.set_xticklabels(labels);","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:16:31.595265Z","iopub.execute_input":"2021-06-30T02:16:31.595589Z","iopub.status.idle":"2021-06-30T02:16:32.407033Z","shell.execute_reply.started":"2021-06-30T02:16:31.595556Z","shell.execute_reply":"2021-06-30T02:16:32.405695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"# removing punctuation down character\ndef removePunctuationDown(strs):\n    remove = '!#$%&\\()+,-./:;<=>?@[\\\\]_{|}~'\n    pattern = r\"[{}]\".format(remove)\n    h = re.sub(pattern, \" \", strs)\n    return h\n\n# removing punctuation up character\ndef removePunctuationUp(strs):\n    remove = ',\"\\^`*'\n    # remove = ',\"\\'^`*'\n    pattern = r\"[{}]\".format(remove)\n    h = re.sub(pattern, \"\", strs)\n    return h\n\n# replace other special character\ndef replace(strs):\n    strs = strs.replace('\\\\t',' ').replace('\\\\n',' ').replace('\\\\u',' ').replace('\\\\',' ')\n    strs = strs.replace('\\n',' ')\n    strs = strs.replace('\\t','')\n    strs = strs.encode('utf-8').decode('ascii', 'ignore')\n    return strs\n\n# remove emoji\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002500-\\U00002BEF\"  # chinese char\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           u\"\\U0001f926-\\U0001f937\"\n                           u\"\\U00010000-\\U0010ffff\"\n                           u\"\\u2640-\\u2642\"\n                           u\"\\u2600-\\u2B55\"\n                           u\"\\u200d\"\n                           u\"\\u23cf\"\n                           u\"\\u23e9\"\n                           u\"\\u231a\"\n                           u\"\\ufe0f\"  # dingbats\n                           u\"\\u3030\"\n                               #convert data input to Unicode\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:16:32.408424Z","iopub.execute_input":"2021-06-30T02:16:32.408758Z","iopub.status.idle":"2021-06-30T02:16:32.418643Z","shell.execute_reply.started":"2021-06-30T02:16:32.408727Z","shell.execute_reply":"2021-06-30T02:16:32.417185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply Cleaning Data\ndata['news'] = [i.lower() for i in data.news] #lower case\ndata['remove_emoji'] = data['news'].apply(remove_emoji) #remov emoji\ndata['hastags'] = [re.findall(r'\\B#\\w*[a-zA-Z0-9]+\\w*', i) for i in data.remove_emoji] #save hastags\ndata['remove_email'] = [re.sub(r'\\S*@\\S*\\s?','',i) for i in data.remove_emoji] #remove email\ndata['remove_hashtag'] = [re.sub(\"#[A-Za-z0-9_]+\",'',i) for i in data.remove_emoji] #remove hastags\ndata['remove_special_character'] = data['remove_hashtag'].replace(r'http\\s+|www.\\s+','',regex=True).replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True) #remove link\ndata['remove_special_character'] = [re.sub(r'&lt;/?[a-z]+&gt;','',i) for i in data.remove_special_character] #remove_special_character\ndata['remove_special_character'] = [re.sub('<.*?>+', '', i) for i in data.remove_special_character] #remove special character\ndata['text_clean'] = [removePunctuationDown(i) for i in data.remove_special_character] #remove punc down\ndata['text_clean'] = [removePunctuationUp(i) for i in data.text_clean] #remove punc up\ndata['text_clean'] = [replace(j) for j in data.text_clean] #remove \\n \\t","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:16:32.419988Z","iopub.execute_input":"2021-06-30T02:16:32.420343Z","iopub.status.idle":"2021-06-30T02:16:44.077127Z","shell.execute_reply.started":"2021-06-30T02:16:32.420311Z","shell.execute_reply":"2021-06-30T02:16:44.076028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.tail()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:16:44.078542Z","iopub.execute_input":"2021-06-30T02:16:44.078856Z","iopub.status.idle":"2021-06-30T02:16:44.098779Z","shell.execute_reply.started":"2021-06-30T02:16:44.078825Z","shell.execute_reply":"2021-06-30T02:16:44.097447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom Cleansing Data\n\nCustom for remove specific word","metadata":{}},{"cell_type":"code","source":"def text_clean(text):    \n    ## Decontract text            \n    # specific\n    text = re.sub(r\"won\\'t\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"won\\’t\", \"will not\", text)\n    text = re.sub(r\"can\\’t\", \"can not\", text)\n    text = re.sub(r\"\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'clock\", \"f the clock\", text)\n    text = re.sub(r\"\\'cause\", \" because\", text)\n    \n    # general\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"n\\’t\", \" not\", text)\n    text = re.sub(r\"\\’re\", \" are\", text)\n    text = re.sub(r\"\\’s\", \" is\", text)\n    text = re.sub(r\"\\’ll\", \" will\", text)\n    text = re.sub(r\"\\’t\", \" not\", text)\n    text = re.sub(r\"\\’ve\", \" have\", text)\n    text = re.sub(r\"\\’m\", \" am\", text)\n    text = re.sub(r\"\\’\", \"\\'\", text)\n    \n    # remove all puctuation\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text) #remove digit/number\n    \n    # remove all special characters\n    text = re.sub(r'\\W', ' ', text)\n\n    # remove break\n    text = re.sub('[‘’“”…]', '', text)\n    \n    # remove prefixed 'b'\n    text = re.sub(r'^b\\s+', '', text)\n    \n    # substituting multiple spaces with single space\n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    \n    return text\n\ndata['text_clean'] = data['text_clean'].apply(lambda x: text_clean(x))","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:16:44.100691Z","iopub.execute_input":"2021-06-30T02:16:44.1012Z","iopub.status.idle":"2021-06-30T02:17:02.354784Z","shell.execute_reply.started":"2021-06-30T02:16:44.10115Z","shell.execute_reply":"2021-06-30T02:17:02.353363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:17:02.356039Z","iopub.execute_input":"2021-06-30T02:17:02.356347Z","iopub.status.idle":"2021-06-30T02:17:02.375619Z","shell.execute_reply.started":"2021-06-30T02:17:02.356309Z","shell.execute_reply":"2021-06-30T02:17:02.374296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# select columns\ndata = data[['news', 'text_clean', 'category']]\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:17:02.376871Z","iopub.execute_input":"2021-06-30T02:17:02.377181Z","iopub.status.idle":"2021-06-30T02:17:02.47037Z","shell.execute_reply.started":"2021-06-30T02:17:02.377152Z","shell.execute_reply":"2021-06-30T02:17:02.469099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lemmatization","metadata":{}},{"cell_type":"code","source":"# import and download packages\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:17:02.472026Z","iopub.execute_input":"2021-06-30T02:17:02.472472Z","iopub.status.idle":"2021-06-30T02:18:23.45756Z","shell.execute_reply.started":"2021-06-30T02:17:02.472408Z","shell.execute_reply":"2021-06-30T02:18:23.456235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import library\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = nltk.stem.WordNetLemmatizer()\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# define tag and lemmatization function\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return None\n\ndef lemmatize_sentence(sentence):\n    #tokenize the sentence and find the POS tag for each token\n    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n    #tuple of (token, wordnet_tag)\n    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n    lemmatized_sentence = []\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            #if there is no available tag, append the token as is\n            lemmatized_sentence.append(word)\n        else:\n            #else use the tag to lemmatize the token\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n    return \" \".join(lemmatized_sentence)\n\n\n\n# lemmatizing\ndata['text_lemma'] = data['text_clean'].apply(lambda x: lemmatize_sentence(x))\n\ndata = data[['text_clean','text_lemma','category']]\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:18:23.459219Z","iopub.execute_input":"2021-06-30T02:18:23.45968Z","iopub.status.idle":"2021-06-30T02:24:40.152792Z","shell.execute_reply.started":"2021-06-30T02:18:23.459648Z","shell.execute_reply":"2021-06-30T02:24:40.151667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stopword","metadata":{}},{"cell_type":"code","source":"stop = stopwords.words('english')\n\ndata['text_preprocess'] = data['text_lemma'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:24:40.15417Z","iopub.execute_input":"2021-06-30T02:24:40.154499Z","iopub.status.idle":"2021-06-30T02:24:48.771797Z","shell.execute_reply.started":"2021-06-30T02:24:40.154465Z","shell.execute_reply":"2021-06-30T02:24:48.771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Process","metadata":{}},{"cell_type":"code","source":"#convert dataframe to values\ndata_process = data['text_preprocess'].values\nlabel = data['category'].apply(lambda x:x-1).values","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:24:48.774982Z","iopub.execute_input":"2021-06-30T02:24:48.775287Z","iopub.status.idle":"2021-06-30T02:24:48.851064Z","shell.execute_reply.started":"2021-06-30T02:24:48.775256Z","shell.execute_reply":"2021-06-30T02:24:48.850142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view data array\ndata_process[0:10]","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:24:48.852972Z","iopub.execute_input":"2021-06-30T02:24:48.853642Z","iopub.status.idle":"2021-06-30T02:24:48.860027Z","shell.execute_reply.started":"2021-06-30T02:24:48.853591Z","shell.execute_reply":"2021-06-30T02:24:48.859082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# view label array\nlabel","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:24:48.861331Z","iopub.execute_input":"2021-06-30T02:24:48.861756Z","iopub.status.idle":"2021-06-30T02:24:48.87468Z","shell.execute_reply.started":"2021-06-30T02:24:48.861724Z","shell.execute_reply":"2021-06-30T02:24:48.873834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Splitting","metadata":{}},{"cell_type":"code","source":"# split data into training and validation\n\nfrom sklearn.model_selection import train_test_split\n\ntext_train, text_test, label_train, label_test = train_test_split(data_process, label, test_size=0.2, shuffle=True,  stratify=label, random_state=42)\n\ntext_train.shape, text_test.shape, label_train.shape, label_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:24:48.876037Z","iopub.execute_input":"2021-06-30T02:24:48.876598Z","iopub.status.idle":"2021-06-30T02:24:48.985847Z","shell.execute_reply.started":"2021-06-30T02:24:48.876562Z","shell.execute_reply":"2021-06-30T02:24:48.984845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# tokenizer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n \ntokenizer = Tokenizer(num_words=10000, oov_token='x')\ntokenizer.fit_on_texts(text_train) \ntokenizer.fit_on_texts(text_test)\n\nword_index = tokenizer.word_index\ntotal_words = len(word_index)+1\n \nsekuens_train = tokenizer.texts_to_sequences(text_train)\nsekuens_test = tokenizer.texts_to_sequences(text_test)\n \npadded_train = pad_sequences(sekuens_train, maxlen=100) \npadded_test = pad_sequences(sekuens_test, maxlen=100)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:24:48.987124Z","iopub.execute_input":"2021-06-30T02:24:48.987434Z","iopub.status.idle":"2021-06-30T02:24:57.683796Z","shell.execute_reply.started":"2021-06-30T02:24:48.987404Z","shell.execute_reply":"2021-06-30T02:24:57.682683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'train datashape : {padded_train.shape}')\nprint(f'test datashape : {padded_test.shape}')\ntotal_words","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:24:57.685265Z","iopub.execute_input":"2021-06-30T02:24:57.685893Z","iopub.status.idle":"2021-06-30T02:24:57.694683Z","shell.execute_reply.started":"2021-06-30T02:24:57.685834Z","shell.execute_reply":"2021-06-30T02:24:57.693668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import packages\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers,Sequential\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:24:57.69609Z","iopub.execute_input":"2021-06-30T02:24:57.696442Z","iopub.status.idle":"2021-06-30T02:24:57.707952Z","shell.execute_reply.started":"2021-06-30T02:24:57.696408Z","shell.execute_reply":"2021-06-30T02:24:57.706817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=total_words,output_dim=128,input_length=100),\n    tf.keras.layers.LSTM(128),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(4, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', metrics=['accuracy'], loss='sparse_categorical_crossentropy')\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:24:57.709411Z","iopub.execute_input":"2021-06-30T02:24:57.709792Z","iopub.status.idle":"2021-06-30T02:24:58.549644Z","shell.execute_reply.started":"2021-06-30T02:24:57.709758Z","shell.execute_reply":"2021-06-30T02:24:58.547473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callback\n\nclass myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('accuracy')>0.91 and logs.get('val_accuracy')>0.91):\n      self.model.stop_training = True\n      print(\"\\nThe accuracy of the training set and the validation set has reached > 91%!\")\ncallbacks = myCallback()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:24:58.551067Z","iopub.execute_input":"2021-06-30T02:24:58.551344Z","iopub.status.idle":"2021-06-30T02:24:58.560796Z","shell.execute_reply.started":"2021-06-30T02:24:58.551316Z","shell.execute_reply":"2021-06-30T02:24:58.559418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\ntime_array = []\n\nstart_time = time.time()\n\n# model fit\nhistory = model.fit(padded_train, label_train, \n                    batch_size=256, \n                    epochs=30, \n                    validation_data=(padded_test, label_test),\n                    verbose=2, callbacks=[callbacks],\n                    validation_steps=5, steps_per_epoch=25)\n\nprint(\"--- %.2f menit ---\" % ((time.time() - start_time)/60))\ntime_array.append(time.time() - start_time)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:24:58.562534Z","iopub.execute_input":"2021-06-30T02:24:58.563024Z","iopub.status.idle":"2021-06-30T02:30:33.925858Z","shell.execute_reply.started":"2021-06-30T02:24:58.56299Z","shell.execute_reply":"2021-06-30T02:30:33.925054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Visualization Accuracy and Loss Each Epoch\n","metadata":{}},{"cell_type":"code","source":"# define function plot visualization training and validation process\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    print(\"Training Accuracy: %.2f\" % acc[-1])\n    print(\"Training Loss: %.2f\" % loss[-1])\n    print(\"Validation Accuracy: %.2f\" % val_acc[-1])\n    print(\"Validation Loss: %.2f\" % val_loss[-1] + '\\n\\n')\n\n    # make a visualization of the results of the neural network between loss, validation loss and accuracy, validation accuracy\n    plt.figure(figsize=(15, 5))\n\n    # create visualizations for accuracy values in the training and validation process\n    plt.subplot(1, 2, 1)\n    plt.plot(acc, 'b', label='Train acc')\n    plt.plot(val_acc, 'r', label='Validation acc')\n    plt.title('Train and validation accuracy Visualization')\n    plt.xlabel(\"Jumlah Epochs\")\n    plt.legend()\n\n    # create visualizations for loss values in the training and validation process\n    plt.subplot(1, 2, 2)\n    plt.plot(loss, 'b', label='Train loss')\n    plt.plot(val_loss, 'r', label='Validation loss')\n    plt.title('Train and validation loss Visualization')\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:31:32.659633Z","iopub.execute_input":"2021-06-30T02:31:32.66004Z","iopub.status.idle":"2021-06-30T02:31:32.67203Z","shell.execute_reply.started":"2021-06-30T02:31:32.660009Z","shell.execute_reply":"2021-06-30T02:31:32.670626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(history)","metadata":{"execution":{"iopub.status.busy":"2021-06-30T02:31:44.725244Z","iopub.execute_input":"2021-06-30T02:31:44.725641Z","iopub.status.idle":"2021-06-30T02:31:45.05285Z","shell.execute_reply.started":"2021-06-30T02:31:44.725611Z","shell.execute_reply":"2021-06-30T02:31:45.051903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}