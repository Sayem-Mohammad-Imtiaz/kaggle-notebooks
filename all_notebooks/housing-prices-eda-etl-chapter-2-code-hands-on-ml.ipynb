{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nhousing = pd.read_csv(\"../input/housing-handsonml-chapter2/housing.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"ocean_proximity\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline \n#only in a Jupyter notebook\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#From scratch methid to split data based on length\nimport numpy as np\ndef split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, test_set = split_train_test(housing, 0.2)\nprint(len(train_set), \"train +\", len(test_set), \"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#better way to split data using hash values - this needs a distinct identifier column, for which index or \n#combination of lat and longitude can be used\n\nimport hashlib\n\ndef test_set_check(identifier, test_ratio, hash):\n    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio\n\ndef split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))\n    return data.loc[~in_test_set], data.loc[in_test_set]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_with_id = housing.reset_index() # adds an `index` column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best package for test set splitting\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creatinf 5 categories of meidam income for stratified splitting\nhousing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\nhousing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stratified splitting\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing[\"income_cat\"].value_counts() / len(housing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing the additionam median value category column that was added\nfor set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#copying the train set to work, so that OG train set is not disturbed\nhousing = strat_train_set.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#gepgraphical scatter plot\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#setting alpha for better visualisation\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The radius of each circle represents the districtâ€™s population and the color represents the price . Using predefined color map to map housing prices\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]/100, label=\"population\", figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,) \n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Correlation Matrix\ncorr_matrix = housing.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#using pandas plot to certain more correlated attributes to check visually\nfrom pandas.plotting import scatter_matrix\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#zooming on the one with mediam income and house value\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",alpha=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating custom variables to check if they are better than the existing ones\n#One should always reiterating this step when some pattern is noticed\nhousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Seeing correlations again to check if customs performed better\n#bedrooms per room seems better clearly than just bedrooms or rooms\ncorr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making x and y - training and label datasets\nhousing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Missing value handling - three options\nhousing.dropna(subset=[\"total_bedrooms\"]) # option 1 - get rid of NA rows\nhousing.drop(\"total_bedrooms\", axis=1) # option 2 - get rid of the column itself\nmedian = housing[\"total_bedrooms\"].median() # option 3 - impute median values\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputer function to imputr median values\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='median')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remiving the categorical value because imputer only works on Numerical values\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer.fit(housing_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputed values\nimputer.statistics_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_num.median().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transforming train set with imputed values\nX = imputer.transform(housing_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#converting the above numpy array to a dataframe\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#label encoding the categorial column - ocean proximity\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\nhousing_cat = housing[\"ocean_proximity\"]\nhousing_cat_encoded = encoder.fit_transform(housing_cat)\nhousing_cat_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(encoder.classes_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#one hot encoding better than label, fit_transform() expects a 2D array, but housing_cat_encoded is a 1D array, so we need to reshape it:17\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nhousing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\nhousing_cat_1hot\n#output is a scipy matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#coverting to numpy array\nhousing_cat_1hot.toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can apply both transformations (from text categories to integer categories, then from integer categories to one-hot vectors) in one shot using the LabelBinarizer class:\nfrom sklearn.preprocessing import LabelBinarizer\nencoder = LabelBinarizer()\nhousing_cat_1hot = encoder.fit_transform(housing_cat)\nhousing_cat_1hot\n\n#this returns a dense NumPy array by default. You can get a sparse matrix instead by passing sparse_output=True to the LabelBinarizer constructor.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"#Feature Scaling - very important. all columns should be on similar scale. Y does not need to be scaled\n'''\nTwo mwthods - minmax scaler and Standardization\nMin-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled\nso that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max\nminus the min. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a\nfeature_range hyperparameter that lets you change the range if you donâ€™t want 0â€“1 for some reason.\nStandardization is quite different: first it subtracts the mean value (so standardized values always have a\nzero mean), and then it divides by the variance so that the resulting distribution has unit variance. Unlike\nmin-max scaling, standardization does not bound values to a specific range, which may be a problem for\nsome algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However,\nstandardization is much less affected by outliers. For example, suppose a district had a median income\nequal to 100 (by mistake). Min-max scaling would then crush all the other values from 0â€“15 down to 0â€“\n0.15, whereas standardization would not be much affected. Scikit-Learn provides a transformer called\nStandardScaler for standardization.\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Self defined pipeline function\nfrom sklearn.base import BaseEstimator, TransformerMixin\nrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n        def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n            self.add_bedrooms_per_room = add_bedrooms_per_room\n        def fit(self, X, y=None):\n            return self # nothing else to do\n        def transform(self, X, y=None):\n            rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n            population_per_household = X[:, population_ix] / X[:, household_ix]\n            if self.add_bedrooms_per_room:\n                bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n                return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n            else:\n                return np.c_[X, rooms_per_household, population_per_household]\n            \nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if we could feed a Pandas DataFrame directly into our pipeline, instead of having ,\n#to first manually extract the numerical columns into a NumPy array. There is nothing in Scikit-Learn to\n#handle Pandas DataFrames,19 but we can write a custom transformer for this task\n\n#self defined pipeline function\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Self defined label binarier class\nclass MyLabelBinarizer(TransformerMixin):\n    def __init__(self, *args, **kwargs):\n        self.encoder = LabelBinarizer(*args, **kwargs)\n    def fit(self, x, y=0):\n        self.encoder.fit(x)\n        return self\n    def transform(self, x, y=0):\n        return self.encoder.transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pipeline code - basically all the preprocessing steps together - imputing, scaling, encoding for both numerical and categorical columns\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([('imputer', SimpleImputer(strategy = \"median\")),\n                        ('attribs_adder', CombinedAttributesAdder()),\n                        ('std_scaler', StandardScaler()),\n                        ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\n\nfrom sklearn.pipeline import FeatureUnion\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nnum_pipeline = Pipeline([\n                         ('selector', DataFrameSelector(num_attribs)),\n                         ('imputer', SimpleImputer(strategy = \"median\")),\n                         ('attribs_adder', CombinedAttributesAdder()),\n                         ('std_scaler', StandardScaler()),\n                        ])\n\ncat_pipeline = Pipeline([('selector', DataFrameSelector(cat_attribs)), \n                         ('label_binarizer', MyLabelBinarizer()),\n                        ])\n\nfull_pipeline = FeatureUnion(transformer_list = [(\"num_pipeline\", num_pipeline), \n                                                 (\"cat_pipeline\", cat_pipeline),\n                                                ])\n\n# And we can now run the whole pipeline simply:\n#to get the finally prepared dataset\nhousing_prepared = full_pipeline.fit_transform(housing)\nhousing_prepared","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_prepared.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trying linear regression on the transformed data\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seeing some predictions on test values\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Labels:\", list(some_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seeing MSE of the linear regression model - not good - definitely underfitting- need to try a more complex model \nfrom sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#trying decision tree\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#seeinf rmse\n#overfits excessively giving 0 rmse\nhousing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nCross Validation :Cross-validation is a technique for evaluating ML models by training several ML models on subsets \nof the available input data and evaluating them on the complementary subset of the data. \nUse cross-validation to detect overfitting, ie, failing to generalize a pattern.\n\nYou use the k-fold cross-validation method to perform cross-validation. \nIn k-fold cross-validation, you split the input data into k subsets of data (also known as folds). \nYou train an ML model on all but one (k-1) of the subsets, and then evaluate the model on the subset that was not used for training. \nThis process is repeated k times, with a different subset reserved for evaluation (and excluded from training) each time.\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#doing cross validation to avoid overfitting and beter analysyse train-test RMSE\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#decision tree results after cross validation - still performs poorly\ndisplay_scores(tree_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear regression with cross validation - still performs poorly\nlin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random forest regression = ensemble/combination of many decision trees to form a better model \nfrom sklearn.ensemble import RandomForestRegressor\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)\n\n#result metrics of the random forest model  - much better than linear regression and decision tree moseld -RMSE is so much less\nhousing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random forest with Cross validation\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n\nforest_rmse_scores = np.sqrt(-forest_scores)\n\ndisplay_scores(forest_rmse_scores)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"#pickle to save models\nfrom sklearn.externals import joblib\njoblib.dump(my_model, \"my_model.pkl\")\n# and later...\nmy_model_loaded = joblib.load(\"my_model.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grid search on random forest - to hyper parameter tune the model \n# and find the best parameter combination that gives the most accurate predictions\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [\n{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\nscoring='neg_mean_squared_error')\ngrid_search.fit(housing_prepared, housing_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#listing the best parameters\ngrid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding the nest estimator model\ngrid_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#listing all grid search models with their RMSEs\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"'''\nAlternatives to GridSearchCV is \n- Randomised Search CV where it picks random combinations to find bet values\n- Ensemble Methods\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#listing feature importances of all independent variables\nfeature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#combining feature importances with parameter names and listing in desecending order\nextra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\ncat_one_hot_attribs = list(encoder.classes_)\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying the final best model that came from grid search\nfinal_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#printing the final best RMSE - lowest found so far\nfinal_rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nThe idea is to find the model with the right complexity and then keep hyperparameter tuning it to find the best model\n'''","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"'''\npresent your solution (highlighting what you have\nlearned, what worked and what did not, what assumptions were made, and what your systemâ€™s limitations\nare), document everything, and create nice presentations with clear visualizations and easy-to-remember\nstatements\n'''","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}