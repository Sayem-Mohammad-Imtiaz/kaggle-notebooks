{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nspam = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\",encoding='latin-1')\nspam.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam.drop([\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam.columns=['Labels','Message']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the class distribution\nspam.Labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert label to a numerical variable\nspam['label_num'] = spam.Labels.map({'ham':0, 'spam':1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spam.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = spam.Message\ny = spam.label_num","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split X and y into training and testing sets\n# by default, it splits 75% training and 25% test\n# random_state=1 for reproducibility\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. import and instantiate CountVectorizer (with the default parameters)\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# 2. instantiate CountVectorizer (vectorizer)\nvect = CountVectorizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_dtm = vect.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. transform testing data (using fitted vocabulary) into a document-term matrix\nX_test_dtm = vect.transform(X_test)\nX_test_dtm\n\n# you can see that the number of columns, 7456, is the same as what we have learned above in X_train_dtm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. import\nfrom sklearn.naive_bayes import MultinomialNB\n\n# 2. instantiate a Multinomial Naive Bayes model\nnb = MultinomialNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. train the model \n\nnb.fit(X_train_dtm, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. make class predictions for X_test_dtm\ny_pred_class = nb.predict(X_test_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate accuracy of class predictions\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the confusion matrix\nmetrics.confusion_matrix(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print message text for the false positives (ham incorrectly classified as spam)\n\nX_test[(y_pred_class==1) & (y_test==0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print message text for the false negatives (spam incorrectly classified as ham)\nX_test[(y_pred_class==0) & (y_test==1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# example false negative\nX_test[3132]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate AUC\ny_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\nmetrics.roc_auc_score(y_test, y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. import\nfrom sklearn.linear_model import LogisticRegression\n\n# 2. instantiate a logistic regression model\nlogreg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3. train the model using X_train_dtm\nlogreg.fit(X_train_dtm, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4. make class predictions for X_test_dtm\ny_pred_class = logreg.predict(X_test_dtm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate predicted probabilities for X_test_dtm (well calibrated)\ny_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\ny_pred_prob","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate accuracy\nmetrics.accuracy_score(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate AUC\nmetrics.roc_auc_score(y_test, y_pred_prob)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Without removing  English stop words\nvect1 = CountVectorizer()\n\nX_train_1 = vect1.fit_transform(X_train)\n\nX_train_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove English stop words\nvect1 = CountVectorizer(stop_words='english')\n\nX_train_1 = vect1.fit_transform(X_train)\n\nX_train_1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# include 1-grams and 2-grams\n\n# how to differentiate between \"Happy\", \"Not Happy\", \"Very Happy\"\nvect2 = CountVectorizer(ngram_range=(1, 2))\n\nX_train_2 = vect2.fit_transform(X_train)\n\nX_train_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ignore terms that appear in more than 50% of the documents\nvect3 = CountVectorizer(max_df=0.5)\n\nX_train_3 = vect3.fit_transform(X_train)\n\nX_train_3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only keep terms that appear in at least 2 documents\nvect4 = CountVectorizer(min_df=2)\n\nX_train_4 = vect4.fit_transform(X_train)\n\nX_train_4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vect_combined= CountVectorizer(stop_words='english',ngram_range=(1, 2),min_df=2,max_df=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_c = vect_combined.fit_transform(X_train)\nX_test_c = vect_combined.transform(X_test)\n\nX_train_c","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. import\nfrom sklearn.naive_bayes import MultinomialNB\n\n# 2. instantiate a Multinomial Naive Bayes model\nnb = MultinomialNB()\n\nnb.fit(X_train_c, y_train)\n\ny_pred_class = nb.predict(X_test_c)\n\nmetrics.confusion_matrix(y_test, y_pred_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}