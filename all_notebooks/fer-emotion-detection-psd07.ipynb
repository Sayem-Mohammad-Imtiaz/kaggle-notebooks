{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Convolutionnel Neural Networks For Facial Expression Recognition \n****\nEmotion detection from facial expression is one of the most active research fields, and plays a huge part in today’s technology. It can be implemented using machine learning algorithms, although these can’t provide  a hundred percent accurate solution since facial expression are not always the same and they depend on the person, the brightness, the position, and so on. This Notebook, presents an implementation of a deep learning algorithm for emotion detection using Convolutional Neural Network on a preprocessed images from a dataset FER. The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centered and occupies about the same amount of space in each image. The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\nOur choice of using CNN for this matter is based on the fact that this algorithm performs better than other solutions. Also, to conduct this experiment we have used a dataset which is a mix of other datasets like JAFFE and that was provided by Kaggle in the context of a competition.\n\n****\nThis work was made by:\n    * Nasr Abdelhamid \n                        abdelnasr7@gmail.com\n    * Omar Harchich \n                        omar.harchich@gmail.com\nSupervised by:\n    * Professor Elhannani Assmaa.\n    * kjhk Fatima Zahra Salmam.\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import tarfile\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Conv2D, Activation, MaxPool2D, Flatten, Dropout, BatchNormalization\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fce1cf17c2aff52474c87db45652109f0ac7e723"},"cell_type":"code","source":"df = pd.read_csv('../input/facial-expression-recognitionferchallenge/fer2013/fer2013/fer2013.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4902acabb0cadb89adb9cf5755617237b5d9ffa0"},"cell_type":"code","source":"ls ../input/facial-expression-recognitionferchallenge/fer2013/fer2013","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0067ff5deea7866c568bc42dc43fee81e6ae794"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b9d43b4ded4d0fae29a9c8393d8b837ed4657bc"},"cell_type":"code","source":"ls ../input/facial-expression-recognitionferchallenge/fer2013/fer2013","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59e1fb0765fc49eeea94d9fa8a952883a514227e"},"cell_type":"code","source":"df[\"Usage\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4de64f72e6005666c5d4513f2bb885a8b284e62c"},"cell_type":"code","source":"train = df[[\"emotion\", \"pixels\"]][df[\"Usage\"] == \"Training\"]\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ac71cb108fd3c05fc75a6866d16b68151ffe7ad"},"cell_type":"code","source":"train['pixels'] = train['pixels'].apply(lambda im: np.fromstring(im, sep=' '))\nx_train = np.vstack(train['pixels'].values)\ny_train = np.array(train[\"emotion\"])\nx_train.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f1efabacf8c9716c75fb4fe70af2dd138426183"},"cell_type":"code","source":"public_test_df = df[[\"emotion\", \"pixels\"]][df[\"Usage\"]==\"PublicTest\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c76d821014985e3eb02c8542eed80aee73b8b422"},"cell_type":"code","source":"public_test_df[\"pixels\"] = public_test_df[\"pixels\"].apply(lambda im: np.fromstring(im, sep=' '))\nx_val = np.vstack(public_test_df[\"pixels\"].values)\ny_val = np.array(public_test_df[\"emotion\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc686df08372a66620e54fe991a68c5135e6abfa"},"cell_type":"code","source":"x_train = x_train.reshape(-1, 48, 48, 1)\nx_val = x_val.reshape(-1, 48, 48, 1)\nx_train.shape, x_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff51dffea2ab0f62cdb78ec626ea37cd70ca41b1"},"cell_type":"code","source":"y_train = np_utils.to_categorical(y_train)\ny_val = np_utils.to_categorical(y_val)\ny_train.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac6c87d92b5aca7b491fbcff053325bafb0301a4"},"cell_type":"code","source":"import seaborn as sns\nplt.figure(0, figsize=(12,6))\nfor i in range(1, 13):\n    plt.subplot(3,4,i)\n    plt.imshow(x_train[i, :, :, 0], cmap=\"gray\")\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71789d384eaf4c6f22a463a3f5c433617ef557a5"},"cell_type":"markdown","source":"# Build The Model"},{"metadata":{"trusted":true,"_uuid":"987fce0f4b238e40230df1ee6d5f1cd87db7799a"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.layers.core import Activation\nfrom keras.layers.core import Flatten\nfrom keras.layers.core import Dropout\nfrom keras.layers.core import Dense\nfrom keras import backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa2ba6524b072a3ad59575f4aa1f4893f33753f4"},"cell_type":"code","source":"def buildModel(width, height, depth):\n\t\t# initialize the model along with the input shape to be\n\t\t# \"channels last\" and the channels dimension itself\n\t\tmodel = Sequential()\n\t\tchanDim = -1\n\t\t# CONV => RELU => POOL\n\t\tmodel.add(Conv2D(64, 3, data_format=\"channels_last\", kernel_initializer=\"he_normal\", \n                 input_shape=(48, 48, 1)))\n\t\tmodel.add(Activation(\"relu\"))\n\t\tmodel.add(BatchNormalization(axis=chanDim))\n\t\tmodel.add(MaxPooling2D(pool_size=(3, 3)))\n\t\tmodel.add(Dropout(0.25))\n\n\t\t# (CONV => RELU) * 2 => POOL\n\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n\t\tmodel.add(Activation(\"relu\"))\n\t\tmodel.add(BatchNormalization(axis=chanDim))\n\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n\t\tmodel.add(Activation(\"relu\"))\n\t\tmodel.add(BatchNormalization(axis=chanDim))\n\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\t\tmodel.add(Dropout(0.25))\n\n\t\t# (CONV => RELU) * 2 => POOL\n\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n\t\tmodel.add(Activation(\"relu\"))\n\t\tmodel.add(BatchNormalization(axis=chanDim))\n\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n\t\tmodel.add(Activation(\"relu\"))\n\t\tmodel.add(BatchNormalization(axis=chanDim))\n\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\t\tmodel.add(Dropout(0.25))\n\n\t\t# first (and only) set of FC => RELU layers\n\t\tmodel.add(Flatten())\n\t\tmodel.add(Dense(1024))\n\t\tmodel.add(Activation(\"relu\"))\n\t\tmodel.add(BatchNormalization())\n\t\tmodel.add(Dropout(0.25))\n\n\t\t# softmax classifier\n\t\tmodel.add(Dense(7))\n\t\tmodel.add(Activation(\"softmax\"))\n\n\t\t# return the constructed network architecture\n\t\treturn model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35f7af5325883246152abbb202a03d6c0a3c7b21"},"cell_type":"code","source":"# batch size, and image dimensions\nEPOCHS = 100\nINIT_LR = 1e-3\nBS = 32","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef941607df38ae5289d4df16fa148f577264f245"},"cell_type":"code","source":"model = buildModel(width=48, height=48,depth=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0643b3d8823d6652328eb8e44c13e2d9001d7f2"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"973ce8f7a910a169c045aa6e0f2425590f9d1b94"},"cell_type":"code","source":"from keras.optimizers import Adam\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77974b3331165f148eb15542920b116925249f3a"},"cell_type":"code","source":"opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7383c2630eea3f94850e60701d659fd34b1f7f05"},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\naug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n\theight_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n\thorizontal_flip=True, fill_mode=\"nearest\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c693c4cee4e6fcda0d6fe202da0215e3514aa5b"},"cell_type":"code","source":"# run model\nhist = model.fit_generator( aug.flow(x_train, y_train, batch_size=BS), epochs=EPOCHS,\n                 shuffle=True,\n                 steps_per_epoch=len(x_train) // BS,\n                 validation_data=(x_val, y_val),\n                 verbose=1)\n\n# save model to json\nmodel_json = model.to_json()\nwith open(\"face_model.json\", \"w\") as json_file:\n    json_file.write(model_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e866df18dbd2108820f9cc084d382047519a8451"},"cell_type":"code","source":"plt.figure(figsize=(14,3))\nplt.subplot(1, 2, 1)\nplt.suptitle('Optimizer : Adam', fontsize=10)\nplt.ylabel('Loss', fontsize=16)\nplt.plot(hist.history['loss'], color='b', label='Training Loss')\nplt.plot(hist.history['val_loss'], color='r', label='Validation Loss')\nplt.legend(loc='upper right')\n\nplt.subplot(1, 2, 2)\nplt.ylabel('Accuracy', fontsize=16)\nplt.plot(hist.history['acc'], color='b', label='Training Accuracy')\nplt.plot(hist.history['val_acc'], color='r', label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ba581e9b4a15b9381073c6133aa22ce98c0e8de"},"cell_type":"code","source":"import os\nimage_test = \"../input/image-test/test/test\"\nos.mkdir('test_pretraitement')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a670bf95c8117e2c6e9136e8d3c1880e40ee63e4"},"cell_type":"code","source":"import glob\nimport cv2 as cv\nfrom keras.preprocessing.image import img_to_array\ntest_pretraitement = 'test_pretraitement'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3b3dbc3a05df4b829f6ece94b5cdb911bd042f3"},"cell_type":"code","source":"data_test = {}\nlabels_test = {}\nface_cascade = cv.CascadeClassifier('../input/haarcascade/haarcascade_frontalface_default.xml')\ni = 0\nfor img in glob.glob(image_test+\"/*.jpg\"):\n    image = cv.imread(img)\n    name = img.split('/')[-1]\n    \n    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY) # convert to greyscale\n    height, width = image.shape[:2]\n    faces = face_cascade.detectMultiScale(gray_image, 1.3, 1)\n    if isinstance(faces, tuple):\n        resized_image = cv.resize(gray_image, (48, 48))\n        cv.imwrite(test_pretraitement+'/'+name,resized_image)\n    #print(faces)\n    elif isinstance(faces, np.ndarray):\n        for (x,y,w,h) in faces:\n            if w * h < (height * width) / 3:\n                resized_image = cv.resize(gray_image, (48, 48)) \n                cv.imwrite(test_pretraitement+'/'+name,resized_image)\n            else:\n                \n                #cv.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n                roi_gray = gray_image[y:y+h, x:x+w]\n                #print(len(roi_gray))\n                resized_image = cv.resize(roi_gray, (48, 48))\n                cv.imwrite(test_pretraitement+'/'+name, resized_image)\n    image = resized_image.astype(\"float\") / 255.0\n    image = img_to_array(image)\n    image = np.expand_dims(image, axis=0)\n    data_test[name] = image\n    #data.append(img_to_array(resized_image))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a4c5d4f8ba421f20393858d89834278a2cd1d7f"},"cell_type":"code","source":"data_predict = {}\nfor key,value in data_test.items():\n    predict = model.predict(value)\n    idx = np.argmax(predict)\n    #l= lb.classes_[idx]\n    data_predict[key] = idx\n    #print(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbf9eb01a3966fe4ae412023d0a42a8961249a84"},"cell_type":"code","source":"final_data = pd.DataFrame(list(data_predict.items()),\n                      columns=['Image','Emotion'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5865743ec32cd7492454573e8b294444493d9e3f"},"cell_type":"code","source":"#pd.read_csv('submissionsref.csv')\nmapping_emotion = {0:'anger', 1:'disgust', 2:'fear', 3:'happiness', 6:'neutral', 4:'sadness', 5:'surprise'}\nfinal_data['Emotion'] = final_data['Emotion'].map(mapping_emotion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9cf7c2b219efc6b298ff7f96a415e555264d6857"},"cell_type":"code","source":"final_data.to_csv('submissionsref.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1020f12fb363822a5207cde2180318123a109ac"},"cell_type":"code","source":"rd_df = pd.read_csv('submissionsref.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"069de81df2f199774edaaa33a26c368a3574dda0"},"cell_type":"code","source":"rd_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32851a78339d4a20f17a3b35b58b6a3b0b896746"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}