{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Context\nSince its inception in 2008, Airbnb has disrupted the traditional hospitality industry as more travellers decide to use Airbnb as their primary means of accommodation. Airbnb offers travellers a more unique and personalized way of accommodation and experience.\n\n\n## Task Details\nAs of October 2020, this data set contains nearly 226029 Airbnb listings in U.S. The purpose of this task is to predict the price of U.S. Airbnb rentals based on the data provided and any external dataset(s) with relevant information.\n\nExpected Submission\nUsers should submit a CSV file with each listing from the data set and the model-predicted price :\n\nid,  price <br>\n49091, 83 <br>\n50646, 81 <br>\n56334, 69 <br>\n...\n\n","metadata":{}},{"cell_type":"code","source":"# from zipfile import ZipFile\n# import os\n# \n# with ZipFile('us-airbnb-open-data.zip') as f :\n#     f.extractall(path ='Airbnb-data')\n#     \n# data_dir = 'Airbnb-data'\n# os.listdir(data_dir)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:32:49.506515Z","iopub.execute_input":"2021-08-06T20:32:49.507125Z","iopub.status.idle":"2021-08-06T20:32:49.511837Z","shell.execute_reply.started":"2021-08-06T20:32:49.507006Z","shell.execute_reply":"2021-08-06T20:32:49.510898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#csv_path = data_dir + '/AB_US_2020.csv'\n#csv_path","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:32:49.513517Z","iopub.execute_input":"2021-08-06T20:32:49.514109Z","iopub.status.idle":"2021-08-06T20:32:49.526008Z","shell.execute_reply.started":"2021-08-06T20:32:49.514064Z","shell.execute_reply":"2021-08-06T20:32:49.524401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\npd.options.display.max_columns = 200\npd.options.display.max_rows = 200\n\ndata = pd.read_csv('../input/us-airbnb-open-data/AB_US_2020.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:32:49.528125Z","iopub.execute_input":"2021-08-06T20:32:49.5287Z","iopub.status.idle":"2021-08-06T20:32:51.123477Z","shell.execute_reply.started":"2021-08-06T20:32:49.528663Z","shell.execute_reply":"2021-08-06T20:32:51.122307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = 13, 10","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:32:51.124946Z","iopub.execute_input":"2021-08-06T20:32:51.125432Z","iopub.status.idle":"2021-08-06T20:32:52.007429Z","shell.execute_reply.started":"2021-08-06T20:32:51.125396Z","shell.execute_reply":"2021-08-06T20:32:52.006308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(data.corr(), annot = True, fmt = \".2f\", cmap = 'Blues');","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:32:52.008859Z","iopub.execute_input":"2021-08-06T20:32:52.009292Z","iopub.status.idle":"2021-08-06T20:32:53.089755Z","shell.execute_reply.started":"2021-08-06T20:32:52.009251Z","shell.execute_reply":"2021-08-06T20:32:53.088647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBRegressor\nimport numpy as np\n\n\n# train & target :\nX = data.drop('price', axis = 1)\ny = data['price']\n\n# split the data first :\nX_train, X_valid, y_train, y_valid = tts(X, y, test_size=0.25,\n                                         random_state = 42)\n\n\n#1. identify numerical and categorical cols :\nnumerical_cols = [cname for cname in X.columns if \n                X[cname].dtype in ['int64', 'float64']]\n\ncategorical_cols = [cname for cname in X.columns if\n                    X[cname].nunique() < 50 and \n                    X[cname].dtype in ['object', 'bool']]\n\n\n#2. set up pipelines to transform numerical & categorical data :\n# Also pipelines are valuable for cleaning up machine learning code and avoiding errors, \n# and are especially useful for workflows with sophisticated data preprocessing. \n\nnumerical_transformer = SimpleImputer(strategy='constant')\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n    ('onehot', OneHotEncoder(sparse = False, handle_unknown = 'ignore'))\n])\n\n\n#3. Tranforming columns both numeric & categorical :\npreprocessor = ColumnTransformer(transformers = [\n    ('num',  numerical_transformer, numerical_cols),\n    ('cat', categorical_transformer, categorical_cols)\n])\n\n\n#4. Import model and set up Hyperparameteres : \nmodel = XGBRegressor(base_score = 0.5, booster = 'gbtree', colsample_bylevel = 1,\n                     colsample_bynode = 1, colsample_bytree = 0.6, gamma = 0.0, gpu_id = -1,\n                     importance_type = 'gain', interaction_constraints = '',\n                     learning_rate = 0.02, max_delta_step = 0, max_depth = 4,\n                     min_child_weight = 0.0, n_estimators = 1250, n_jobs = 0, \n                     num_parallel_tree = 1, random_state = 0,\n                     reg_alpha = 0, reg_lambda = 1, scale_pos_weight = 1, subsample=0.8,\n                     tree_method = 'exact', validate_parameters = 1, verbosity=None )\n\n# These hyperparameters are found after performing GridSearchCV\n# Not mentioning the process here, since its was too lengthy.\n\n\n#5. Putting it together\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\nclf.fit(X_train, y_train, model__verbose=False) \npreds = clf.predict(X_valid)\n\n\nprint('RMSE:', mean_squared_error(y_valid, preds, squared = False))","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:32:53.091333Z","iopub.execute_input":"2021-08-06T20:32:53.091722Z","iopub.status.idle":"2021-08-06T20:36:25.2841Z","shell.execute_reply.started":"2021-08-06T20:32:53.091685Z","shell.execute_reply":"2021-08-06T20:36:25.283215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = clf.predict(X)\nop = pd.DataFrame({'id':X.id,\n                  'Price':preds})\n\nop.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-08-06T20:36:25.287999Z","iopub.execute_input":"2021-08-06T20:36:25.290073Z","iopub.status.idle":"2021-08-06T20:36:28.224476Z","shell.execute_reply.started":"2021-08-06T20:36:25.290026Z","shell.execute_reply":"2021-08-06T20:36:28.223701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}