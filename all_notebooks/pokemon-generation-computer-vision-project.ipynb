{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pokemon_Computer_Vision_Project"},{"metadata":{},"cell_type":"markdown","source":"Coded by Luna McBride. This dataset only comes with types, but I intend to add generation number to the mix, in an attempt to see if the various claims about how obviously new, and thus \"bad\", the newer ones are."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport tensorflow as tf #Import tensorflow in order to use Keras\nfrom keras import backend as K #Used for trying to clear memory, but I could still not clear enough RAM\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences #Add padding to help the Keras Sequencing\nimport tensorflow.keras.layers as L #Import the layers as L for quicker typing\nfrom tensorflow.keras.optimizers import Adam #Pull the adam optimizer for usage\n\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy #Loss function being used\nfrom sklearn.model_selection import train_test_split #Train Test Split\nfrom tensorflow.keras.preprocessing import image #Add image handling, because I am looking at images\nfrom PIL import Image #Pillow images, as that is the format Keras uses\n\nfrom keras.models import Sequential #Sequential\nfrom keras.layers import Conv2D, MaxPooling2D #Load 2d layers\nfrom keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization #Load important layers\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pokemon = pd.read_csv(\"../input/pokemon-images-and-types/pokemon.csv\") #Load the Pokemon\npokemon.head() #Take a peek at the pokemon","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Check for Nulls"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pokemon.isnull().any()) #Print if there are any nulls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like the issue is the dataset has nulls for pokemon that are not dual type. I think I will go with the gen 1 strategy and have the second type as the same as the first. For example, Charmander was considered a fire fire type in the code of Pokemon Red."},{"metadata":{"trusted":true},"cell_type":"code","source":"#DualTypeMaker: Makes a pokemon a dual type of the same type if their second type is null\n#Input: The two types it is\n#Output: the type type2 will become\ndef dualTypeMaker(type1, type2):\n    if pd.isnull(type2): #If the second type is null\n        return type1 #Make the second type the same as the first\n    return type2 #Leave the already dual type alone\n\npokemon[\"Type2\"] = pokemon.apply(lambda x: dualTypeMaker(x[\"Type1\"], x[\"Type2\"]), axis = 1) #Set the second type of the pokemon\npokemon.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Add a row for the Pokemon generation"},{"metadata":{},"cell_type":"markdown","source":"The generation is essentially the set of games they were introduced. I happen to know this already, but I will type them here for those who do not know:\n\nGen 1: Pokemon 1-151 (Red and Green/Blue)\n\nGen 2: Pokemon 152-251 (Gold and Silver)\n\nGen 3: Pokemon 252-386 (Ruby and Sapphire)\n\nGen 4: Pokemon 387-493 (Diamond and Pearl)\n\nGen 5: Pokemon 494-649 (Black and White)\n\nGen 6: Pokemon 650-721 (X and Y)\n\nGen 7: Pokemon 722-809 (Sun and Moon)\n\nThis Dataset only goes through generation 7."},{"metadata":{"trusted":true},"cell_type":"code","source":"gen = [] #A list to hold the generation numbers\nlength = len(pokemon) #Get the number of pokemon in the dataset\n\n#A for loop to put the generation into the list, going based on the list above -1 because the index starts at 0\nfor i in range(0, length):\n    if i<=150:\n        gen.append(1)\n    elif i<=250:\n        gen.append(2)\n    elif i<=385:\n        gen.append(3)\n    elif i<=492:\n        gen.append(4)\n    elif i<=648:\n        gen.append(5)\n    elif i<=720:\n        gen.append(6)\n    else:\n        gen.append(7)\n        \n#Get the pokemon that boarder each generation so we can make sure there were no mistakes\nboarderPokemon = [\"mew\", \"chikorita\", \"celebi\", \"treecko\", \"deoxys-normal\", \"turtwig\", \"arceus\", \"victini\", \"genesect\", \"chespin\",\n                 \"volcanion\", \"rowlet\", \"melmetal\"]\n        \npokemon[\"Gen\"] = gen #Insert the generation numbers into the dataset\n\npokemon[\"Gen\"] = pokemon[\"Gen\"].astype(str) #Change the gen to strings\n\nprint(\"Name, Generation\\n\") #Print the format of the pokemon prints\n\n#For each pokemon, find the boarder pokemon and print their name and generation number\nfor i in range(0, length):\n    if pokemon[\"Name\"][i] in boarderPokemon: #If the pokemon is a boarder pokemon\n        print(pokemon[\"Name\"][i], \", \", pokemon[\"Gen\"][i]) #Print the name and generation of that pokemon\n\npokemon.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Judging by the boarder pokemon, all generation numbers were assigned correctly. Forme names are still a problem, but I will fix that later, because the image names have the forme too"},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Get the Images"},{"metadata":{},"cell_type":"markdown","source":"Image Processing: https://keras.io/api/preprocessing/image/"},{"metadata":{"trusted":true},"cell_type":"code","source":"pokemonImages = [] #A list to hold the image data, which will then be put into the dataframe\ndatagen = image.ImageDataGenerator()\n\n#For each pokemon, get and convert their image\nfor i in range(0, length):\n    pokemonName = pokemon[\"Name\"][i] #Get the pokemon name\n    \n    try: #Try to load the image assuming it is a png\n        path = \"../input/pokemon-images-and-types/images/images/{}.png\".format(pokemonName) #Get the image based on the pokemon\n        img = image.load_img(path, target_size = None, interpolation = \"nearest\") #Load the image of the pokemon\n    except: #Catch the issue if the image is not a png, try a jpg\n        path = \"../input/pokemon-images-and-types/images/images/{}.jpg\".format(pokemonName) #Get the image based on the pokemon\n        img = image.load_img(path, target_size = None, interpolation = \"nearest\") #Load the image of the pokemon\n    \n    pokemonImages.append(path) #Append the image to the image list\n    \npokemon[\"Image\"] = pokemonImages #Put the image list into a new column \"Image\"\npokemon.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split the images and gens into train and test\nimageTrain, imageTest, genTrain, genTest = train_test_split(pokemon[\"Image\"], pokemon[\"Gen\"], test_size = 0.33)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainDf = pd.DataFrame(imageTrain) #Put the training images into a dataframe\ntrainDf[\"Gen\"] = genTrain #Add the generation to the dataframe\ntrain = datagen.flow_from_dataframe(trainDf, x_col = \"Image\", y_col = \"Gen\") #Make a flow variable for keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testDf = pd.DataFrame(imageTest) #Put the testing images into a dataframe\ntestDf[\"Gen\"] = genTest #Add the generation to the dataframe\ntest = datagen.flow_from_dataframe(testDf, x_col = \"Image\", y_col = \"Gen\") #Make a flow variable for keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Create the Model"},{"metadata":{},"cell_type":"markdown","source":"Note, we are checking for generation labeling accuracy here"},{"metadata":{},"cell_type":"markdown","source":"Sources: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html , https://keras.io/api/preprocessing/image/#imagedatagenerator-class"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.backend.clear_session() #Clear any previous model building\n\nepoch = 2 #Number of runs through the data\nbatchSize = 8 #The number of items in each batch\nwidth = 256 #The width of the images\nheight = 256 #The height of the images\nchannels = 3 #The number of channels (RGB)\n\nmodel = Sequential() #Add a sequential to the model\nmodel.add(L.Lambda(lambda x: x, input_shape = (width, height, channels))) #Put the input into a lambda, because it would not work for some reason in the Conv2D\nmodel.add(Conv2D(32, (3, 3))) #Add a convolutional image layer\nmodel.add(BatchNormalization()) #Normalize the data\nmodel.add(Activation('relu')) #Make the activation relu to discourage negative units\nmodel.add(MaxPooling2D(pool_size=(2, 2))) #Max pool the data to keep the most important characteristics\n\nmodel.add(Conv2D(32, (3, 3))) #Add a convolutional image layer\nmodel.add(Activation('relu')) #Make the activation relu to discourage negative units\nmodel.add(MaxPooling2D(pool_size=(2, 2))) #Max pool the data to keep the most important characteristics\n\nmodel.add(Conv2D(64, (3, 3))) #Add a bigger convolutional image layer, layering activations\nmodel.add(Activation('relu')) #Make the activation relu to discourage negative units\nmodel.add(MaxPooling2D(pool_size=(2, 2))) #Max pool the data to keep the most important characteristics\n\nmodel.add(Flatten()) #Make the layers flat to apply the characteristics into one slot\nmodel.add(Dense(64)) #Add a dense layer to track activation\nmodel.add(Activation('relu')) #Make the activation relu to discourage negative units\nmodel.add(Dropout(0.2)) #Have a 0.2 dropout to prevent overfitting\nmodel.add(Dense(1)) #Add another dense layer to finish the lot\nmodel.add(Activation('sigmoid')) #Make the activation sigmoid \n\nmodel.compile(loss = 'binary_crossentropy', #Make the loss binary to fit with the sigmoid endpoint\n              optimizer = 'rmsprop', #Use root mean squared prop to optimize (adam came to similar results at much slower speeds)\n              metrics = ['accuracy']) #Track the accuracy of the model\n\nhistory = model.fit_generator(train, epochs = epoch) #Fit the model to the data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Make Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"loss, accuracy = model.evaluate(test) #Get the loss and Accuracy based on the tests\n\n#Print the loss and accuracy\nprint(\"Test Loss: \", loss)\nprint(\"Test Accuracy: \", accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"85.7% accuracy is pretty telling. That means that there is something to each generation of Pokemon that set the designs apart, rather than just an old new split. I wanted to delve deeper into actual predictions and classification reports, but ram went through the roof, even with a 0.01 test size. Nothing I could find online fixed it, so I am going to say that the fact that the model can predict the exact generation number with 85% accuracy tells that each generation has distinct designs that cannot be shoved into a new/old split."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}