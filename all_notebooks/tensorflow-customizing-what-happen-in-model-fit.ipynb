{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sklearn\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow_datasets as tfds\nimport pathlib","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:46.675607Z","iopub.execute_input":"2021-07-24T09:25:46.675951Z","iopub.status.idle":"2021-07-24T09:25:46.680195Z","shell.execute_reply.started":"2021-07-24T09:25:46.675923Z","shell.execute_reply":"2021-07-24T09:25:46.679444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepering the dataset","metadata":{}},{"cell_type":"code","source":"data_path = \"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\"\ndata = pd.read_csv(data_path)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:46.681755Z","iopub.execute_input":"2021-07-24T09:25:46.682225Z","iopub.status.idle":"2021-07-24T09:25:46.749552Z","shell.execute_reply.started":"2021-07-24T09:25:46.682195Z","shell.execute_reply":"2021-07-24T09:25:46.748821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 201 missing values for bmi feature \ndata.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:46.75101Z","iopub.execute_input":"2021-07-24T09:25:46.751391Z","iopub.status.idle":"2021-07-24T09:25:46.760431Z","shell.execute_reply.started":"2021-07-24T09:25:46.751363Z","shell.execute_reply":"2021-07-24T09:25:46.759636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#no duplicated rows\nif data.duplicated().sum() == 0:\n    print(\"No duplicated rows\")\nelse:\n    print(\"There are duplicated rows in the data\")","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:46.761773Z","iopub.execute_input":"2021-07-24T09:25:46.762202Z","iopub.status.idle":"2021-07-24T09:25:46.782229Z","shell.execute_reply.started":"2021-07-24T09:25:46.762173Z","shell.execute_reply":"2021-07-24T09:25:46.780882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"There are {}  examples is this dataset, before dropping the rows containing null values\".format(len(data)))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:46.783593Z","iopub.execute_input":"2021-07-24T09:25:46.783882Z","iopub.status.idle":"2021-07-24T09:25:46.789486Z","shell.execute_reply.started":"2021-07-24T09:25:46.783854Z","shell.execute_reply":"2021-07-24T09:25:46.78824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# so I decide to fill the missing values \n\ndata.fillna(data.median(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:46.790774Z","iopub.execute_input":"2021-07-24T09:25:46.791156Z","iopub.status.idle":"2021-07-24T09:25:46.807632Z","shell.execute_reply.started":"2021-07-24T09:25:46.791125Z","shell.execute_reply":"2021-07-24T09:25:46.806455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5 categorical variables\ncat_variables = ['gender', 'hypertension', 'heart_disease', 'ever_married',\n                'work_type', 'work_type', 'Residence_type', \"smoking_status\"] \n\nfor variable in cat_variables:\n    data[variable] = data[variable].astype('category')","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:46.808701Z","iopub.execute_input":"2021-07-24T09:25:46.808967Z","iopub.status.idle":"2021-07-24T09:25:46.823912Z","shell.execute_reply.started":"2021-07-24T09:25:46.80894Z","shell.execute_reply":"2021-07-24T09:25:46.822648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:46.825204Z","iopub.execute_input":"2021-07-24T09:25:46.825491Z","iopub.status.idle":"2021-07-24T09:25:46.917908Z","shell.execute_reply.started":"2021-07-24T09:25:46.825464Z","shell.execute_reply":"2021-07-24T09:25:46.917124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['gender'][data['gender'] == 'Other']\ndata.drop(3116, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:46.922284Z","iopub.execute_input":"2021-07-24T09:25:46.923221Z","iopub.status.idle":"2021-07-24T09:25:46.933428Z","shell.execute_reply.started":"2021-07-24T09:25:46.923178Z","shell.execute_reply":"2021-07-24T09:25:46.932444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = data[['gender',  'age', 'hypertension', 'heart_disease','ever_married',\n          'work_type', 'Residence_type', 'avg_glucose_level', 'bmi', 'smoking_status']]\ny = data['stroke']\n\n\nx_train_full, x_test, y_train_full, y_test = train_test_split(x, y, shuffle=True, test_size=0.2,\n                                                              stratify=y,\n                                                              random_state=100)\n\nx_train, x_val, y_train, y_val = train_test_split(x_train_full, y_train_full,\n                                                  shuffle=True, test_size=0.2,\n                                                  stratify=y_train_full,\n                                                  random_state=100)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:46.938026Z","iopub.execute_input":"2021-07-24T09:25:46.938348Z","iopub.status.idle":"2021-07-24T09:25:46.960901Z","shell.execute_reply.started":"2021-07-24T09:25:46.938322Z","shell.execute_reply":"2021-07-24T09:25:46.95987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features = ['age', 'avg_glucose_level', 'bmi']\ncategorical_features = ['gender', 'hypertension', 'heart_disease','ever_married',\n                        'work_type', 'Residence_type', 'smoking_status']\n\n\npreprocessing_pipeline = ColumnTransformer([\n    ('num', MinMaxScaler(), numerical_features),\n    ('cat', OneHotEncoder(), categorical_features)\n])","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:46.96468Z","iopub.execute_input":"2021-07-24T09:25:46.964973Z","iopub.status.idle":"2021-07-24T09:25:46.972302Z","shell.execute_reply.started":"2021-07-24T09:25:46.964945Z","shell.execute_reply":"2021-07-24T09:25:46.971216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train_prepered = preprocessing_pipeline.fit_transform(x_train)\nx_val_prepered = preprocessing_pipeline.transform(x_val)\nx_test_prepered = preprocessing_pipeline.transform(x_test)\n\ny_train = np.array(y_train)\ny_val = np.array(y_val)\ny_test = np.array(y_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:46.973489Z","iopub.execute_input":"2021-07-24T09:25:46.973907Z","iopub.status.idle":"2021-07-24T09:25:47.015943Z","shell.execute_reply.started":"2021-07-24T09:25:46.973874Z","shell.execute_reply":"2021-07-24T09:25:47.014985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_dataset = tf.data.Dataset.from_tensor_slices((tf.constant(x_train_prepered), tf.constant(y_train)))\ntraining_dataset = training_dataset.shuffle(512).batch(64).prefetch(1)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((tf.constant(x_val_prepered), tf.constant(y_val)))\nval_dataset = val_dataset.shuffle(256).batch(64).prefetch(1)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((tf.constant(x_train_prepered), tf.constant(y_train)))\ntest_dataset = test_dataset.shuffle(256).batch(64).prefetch(1)","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:47.018219Z","iopub.execute_input":"2021-07-24T09:25:47.018648Z","iopub.status.idle":"2021-07-24T09:25:47.067432Z","shell.execute_reply.started":"2021-07-24T09:25:47.018604Z","shell.execute_reply":"2021-07-24T09:25:47.066533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom training loop","metadata":{}},{"cell_type":"code","source":"def base_model():\n    \n    inputs = tf.keras.Input(shape=(22,))\n    x = tf.keras.layers.Dense(512, activation='relu')(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    x = tf.keras.layers.Dense(256, activation='relu',\n                             kernel_regularizer=tf.keras.regularizers.l2())(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Dense(64, activation='relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    model_output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    \n    model = tf.keras.models.Model(inputs=inputs, outputs=model_output)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:47.068653Z","iopub.execute_input":"2021-07-24T09:25:47.069171Z","iopub.status.idle":"2021-07-24T09:25:47.076566Z","shell.execute_reply.started":"2021-07-24T09:25:47.069139Z","shell.execute_reply":"2021-07-24T09:25:47.075487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 5\nbatch_size = 64\nn_steps = len(x_train_prepered) // batch_size\n\noptimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\nloss_fn = tf.keras.losses.BinaryCrossentropy()\nmetric = tf.keras.metrics.Accuracy()\nval_metrics = tf.keras.metrics.Accuracy()","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:47.077856Z","iopub.execute_input":"2021-07-24T09:25:47.078331Z","iopub.status.idle":"2021-07-24T09:25:47.124655Z","shell.execute_reply.started":"2021-07-24T09:25:47.0783Z","shell.execute_reply":"2021-07-24T09:25:47.123655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = base_model()\n\nepochs_train_losses = []\nepochs_val_losses = []\nepochs_train_acc = []\nepochs_val_acc = []\n\nfor epoch in range(1, n_epochs + 1):\n    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n    \n    #apply gradients\n    training_losses = []\n    for step, (x_batch_train, y_batch_train) in enumerate(training_dataset):\n        #calculate the gradiants with regards to the model trainable_weights\n        with tf.GradientTape() as tape:\n            #forward pass\n            logits = model(x_batch_train)\n            loss = loss_fn(y_batch_train, logits)\n            training_losses.append(loss)\n        # backward pass \n        #calculate the gradiants\n        grads = tape.gradient(loss, model.trainable_weights)\n        #modifying the trainable_weights\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n        \n        #calculating rhe ACC after modifying the  trainable_weights\n        metric(y_batch_train, tf.argmax(logits, axis=1, output_type=tf.int32))\n    \n    # the metric measurement on the epoch\n    train_acc = metric.result()\n    epochs_train_acc.append(train_acc)\n    \n    #The mean of the losses of the batches give us the mean loss for each epoch\n    losses_train_mean = np.mean(training_losses)\n    epochs_train_losses.append(losses_train_mean)\n    \n    #calculating the validation loss and MAE\n    val_losses = []\n    for x_val, y_val in val_dataset:\n        val_logits = model(x_val)\n        val_loss = loss_fn(y_val, val_logits)\n        val_losses.append(val_loss)\n        val_metrics(y_val, tf.argmax(val_logits, axis=1, output_type=tf.int32))\n     \n    val_acc = val_metrics.result()\n    epochs_val_acc.append(val_acc)\n    \n    losses_val_mean = np.mean(val_losses)\n    epochs_val_losses.append(losses_val_mean)\n    \n    print(\"Trainig Loss: {}-------Training Accuracy: {}\".format(epochs_train_losses[-1], train_acc))\n    print(\"Validation Loss: {}-------Validation Accuracy: {}\".format(epochs_val_losses[-1], val_acc))\n    print(\"\\n\")\n    \n    #reset the metrics after each epoch\n    metric.reset_states()\n    val_metrics.reset_states()\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-24T09:25:47.125741Z","iopub.execute_input":"2021-07-24T09:25:47.125984Z","iopub.status.idle":"2021-07-24T09:25:55.688342Z","shell.execute_reply.started":"2021-07-24T09:25:47.12596Z","shell.execute_reply":"2021-07-24T09:25:55.686786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate the model on the test dataset","metadata":{}},{"cell_type":"code","source":"test_accuracy = tf.keras.metrics.Accuracy()\n\nfor (x, y) in test_dataset:\n    # training=False is needed only if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    logits = model(x, training=False)\n    prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n    test_accuracy(prediction, y)\n\nprint(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))","metadata":{"execution":{"iopub.status.busy":"2021-07-24T09:25:55.68948Z","iopub.execute_input":"2021-07-24T09:25:55.689737Z","iopub.status.idle":"2021-07-24T09:25:56.040562Z","shell.execute_reply.started":"2021-07-24T09:25:55.689713Z","shell.execute_reply":"2021-07-24T09:25:56.039288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}