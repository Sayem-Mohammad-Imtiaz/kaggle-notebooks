{"nbformat_minor":1,"metadata":{"language_info":{"pygments_lexer":"ipython3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","name":"python","file_extension":".py","version":"3.6.3"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"nbformat":4,"cells":[{"metadata":{"_uuid":"d6af7efc104d0be8dbb7eed97017a3036b1278e0","_cell_guid":"631db98c-f0bd-450c-8662-f0747f44b384"},"source":"# Introduction\n\nIn [the previous notebook](https://www.kaggle.com/residentmario/gaming-cross-validation-and-hyperparameter-search/notebook) I introduced the concepts of cross validation and hyperparameter search (if you don't know what those are, or need a refresher, start there). I implemented grid search by hand in [a separate notebook](https://www.kaggle.com/residentmario/nyc-buildings-part-2-feature-scales-grid-search/), but the technique that we implemented in that notebook are built into `scikit-learn` as the `GridSearchCV` (of course!).\n\nIn this notebook we'll look at how this work can be managed by a `scikit-learn` [pipeline](http://scikit-learn.org/stable/modules/pipeline.html).","cell_type":"markdown"},{"source":"import pandas as pd\nsurvey = pd.read_csv(\n    \"../input/BoilingSteam_com_LinuxGamersSurvey_Q1_2016_Public_Sharing_Only.csv\"\n).loc[1:]\n\nimport numpy as np\n\nspend = survey.loc[:, ['LinuxGamingHoursPerWeek', 'LinuxGamingSpendingPerMonth']].dropna()\nspend = spend.assign(\n    LinuxGamingHoursPerWeek=spend.LinuxGamingHoursPerWeek.map(lambda v: int(v) if str.isdigit(v) else np.nan),\n    LinuxGamingSpendingPerMonth=spend.LinuxGamingSpendingPerMonth.map(lambda v: float(v) if str.isdecimal(v) else np.nan)\n).dropna()\n\nX = spend['LinuxGamingHoursPerWeek'].values[:, np.newaxis]\ny = spend['LinuxGamingSpendingPerMonth'].values[:, np.newaxis]","metadata":{"_uuid":"b61771e8b32ddd7d7521fe2747b48548b3e0fdaa","collapsed":true,"_cell_guid":"df96f7ad-ad7c-4a95-8e8e-3458c95e63f6"},"execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_uuid":"a2f608d9e5a28836bf0c2c28005c407b85d4caac","_cell_guid":"44fc41ef-ccb0-4d50-afa5-c8451dcfbdfa"},"source":"# Pipe-in'\n\nYou can declare a pipeline directly as an object, using the `Pipeline` constructor, or use the `make_pipeline` convenience function (which we will use). Recall that for our model in the previous notebook we used polynomial regression and performed hyperparameter search over the degrees of our variables.\n\nOrganizing that into a simple pipeline is easy.","cell_type":"markdown"},{"source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\npipe = make_pipeline(PolynomialFeatures(), LinearRegression())","metadata":{"_uuid":"cba3069d9a11af606085b481c8e8e98a56f16053","collapsed":true,"_cell_guid":"a8dc4782-2a6c-470d-82ca-e915f458ce2e"},"execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_uuid":"d69cadb9853ffc864d5cce38075db65a3b17feb5","_cell_guid":"7a931f04-6ffb-4e19-8dc5-f10ccd865619"},"source":"You can get the steps you are using with `steps` or `named_steps` utility methods.","cell_type":"markdown"},{"source":"pipe.steps","metadata":{"_uuid":"20125cd31e45040a3db0790952f0cedc58084590","collapsed":true,"_cell_guid":"bd592df6-766f-4485-9f59-2124316d70b4"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"pipe.named_steps","metadata":{"_uuid":"f67a065f41c93bcd6d760c46d9b319681df22a8a","collapsed":true,"_cell_guid":"e464b009-bda7-472f-8423-76894dfae57d"},"execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_uuid":"f5fba57f06d18ad60fcfdf4c2deec897ee675da9","_cell_guid":"54ea4c99-6b2c-4521-b0f8-8263ba77705f"},"source":"Setting a parameter is weird, though. You have to take the name of the step, add a `__`, and then specify what you want to mutate that step's parameter to.","cell_type":"markdown"},{"source":"pipe.set_params(polynomialfeatures__degree=3)","metadata":{"_uuid":"7c6cf68a6e8e710e74136b2f62d8125f666567eb","collapsed":true,"_cell_guid":"fd5abef9-b849-4ada-902a-9fab106070ad"},"execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_uuid":"352748ef97dcbfa9cb4770a4a37c7ea9fe6b134e","_cell_guid":"3543b026-6931-4fa1-9cfb-4d4518f02898"},"source":"We can use `fit` and `predict` on the pipeline to get a result, as usual.","cell_type":"markdown"},{"source":"pipe.fit(X, y)\ny_hat = pipe.predict(X)\n\n# Plot the result.\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nsortorder = np.argsort(spend['LinuxGamingHoursPerWeek'].values)\ns = spend['LinuxGamingHoursPerWeek'].values[sortorder]\ny_hat = y_hat[sortorder]\nplt.plot(s, y_hat)\nplt.scatter(spend['LinuxGamingHoursPerWeek'], \n            spend['LinuxGamingSpendingPerMonth'], \n            color='black')","metadata":{"_uuid":"e6727618c9b021cc43824027b9c4867c9c5e386c","collapsed":true,"_cell_guid":"ea8405de-4879-4265-b659-2a7d7cf7c396"},"execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_uuid":"bc2f961713652c33f1c63e6079e36dd381ce5897","_cell_guid":"63ddd10a-5ae3-4948-b83b-0a8de0772286"},"source":"A pipeline technically needs to consist of a string of n transformations (classes that implement a callable `transform` method) and then a single model at the end (implementing `fit` and `predict`). That means that it is very easy to write our own functions for insertion in a pipeline, if we are so inclined.\n\nRunning a grid search occurs outside of the pipeline, *on* the pipeline. Getting the result is as simple as throwing the pipe into `GridSearchCV`.","cell_type":"markdown"},{"source":"from sklearn.grid_search import GridSearchCV\n\npipe = make_pipeline(PolynomialFeatures(), LinearRegression())\nparam_grid = dict(polynomialfeatures__degree=list(range(1, 5)))\ngrid_search = GridSearchCV(pipe, param_grid=param_grid)","metadata":{"_uuid":"b1db88a90701faf593845b629be4a37764eab772","collapsed":true,"_cell_guid":"9cceb8cc-58c3-4d7e-9007-efb65a16de1b"},"execution_count":null,"cell_type":"code","outputs":[]},{"source":"grid_search.fit(X, y)","metadata":{"_uuid":"2b6862f8c80ea369f40e3432f6fcb40f355e1b92","collapsed":true,"_cell_guid":"1b223abd-ac2f-4444-85d0-cc70f9660b20"},"execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_uuid":"40089adfdaa885bcdb2ef868970b3be8e8de3e28","_cell_guid":"9e8690e7-9b83-4013-837b-05decff70611"},"source":"The resulting model is stored in the `estimator` parameter.","cell_type":"markdown"},{"source":"grid_search.estimator","metadata":{"_uuid":"fe336ba01b903cd37a65bffde44b65ec582cf0d8","collapsed":true,"_cell_guid":"7a502588-0645-44b1-a23a-44480fda09ba"},"execution_count":null,"cell_type":"code","outputs":[]},{"metadata":{"_uuid":"076877ac2f2af43480ea5a8d8c75f471ae6e9fa0","_cell_guid":"3b9f72de-7fe5-49bc-8e13-1d3ecf8a1cf0"},"source":"## Discussion\n\nFor simple models like this one the benefit of using a pipeline is relatively small. However, for increasingly complex models, the organizational \"struts\" that the pipeline provides are increasingly useful.\n\nThe `scikit-learn` documentation lists these three benefits to using pipelines:\n\n> **Convenience and encapsulation**\n>\n> You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n>\n> **Joint parameter selection**\n>\n> You can grid search over parameters of all estimators in the pipeline at once.\n>\n> **Safety**\n>\n> Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n\nConvenience and encapsulation is an obvious one. The purported benefit of joint parameter selection is interesting; using a pipeline unlocks the ability to do more complicated grid searches, like, for example, mixing and matching what kinds of transforms we apply to the dataset *before* running (and potentially searching through) the model.\n\nThe point on safety is hardest to explain. Basically, using a pipeline helps defend against a difficult-to-escape form of model leakage known as \"knowledge leakage\". This warrents another notebook: for more on that, [read this notebook](https://www.kaggle.com/residentmario/leakage-especially-knowledge-leakage/).\n\n## Feature Union\n\nIt's also worth briefly mentioning the `FeatureUnion` class. This class provides a way of combining transforms without having to place an estimator at the end (e.g. it's a `pipeline[:-1]`).","cell_type":"markdown"}]}