{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport datetime\nfrom sklearn import preprocessing ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Instructions\n1. We will be conducting the entire assignment through this notebook. You will be entering your code in the cells provided, and any explanation and details asked in markdown cells. \n2. You are free to add more code and markdown cells for describing your answer, but make sure they are below the question asked and not somewhere else. \n3. The notebook needs to be submitted on LMS. You can find the submission link [here](https://lms.iiitb.ac.in/moodle/mod/assign/view.php?id=13932). \n4. The deadline for submission is **5th October, 2020 11:59PM**."},{"metadata":{},"cell_type":"markdown","source":"# Data import\nThe data required for this assignment can be downloaded from the following [link](https://www.kaggle.com/dataset/e7cff1a2c6e29e18684fe6b077d3e4c42f9a7ae6199e01463378c60fe4b4c0cc), it's hosted on kaggle. Do check directory paths on your local system.  "},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/alcoholism/student-mat.csv\")\nfifadata = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/fifa18/data.csv\")\naccidata1 = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/accidents/accidents_2005_to_2007.csv\")\naccidata2 = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/accidents/accidents_2009_to_2011.csv\")\naccidata3 = pd.read_csv(\"../input/iiitb-ai511ml2020-assignment-1/Assignment/accidents/accidents_2012_to_2014.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part - 1\n## Alcohol Consumption Data\nThe following data was obtained in a survey of students' math course in secondary school. It contains a lot of interesting social, gender and study information about students. \n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Try to visualize correlations between various features and grades and see which features have a significant impact on grades. \nTry to engineer the three grade parameters (G1, G2 and G3) as one feature for such comparisons.\n\n"},{"metadata":{},"cell_type":"markdown","source":"So there is no null values in this dataset,tehrefore we can move on to finding the features with signifcant impact\non grades"},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata.isnull().sum()\nalcdata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to get a sorted correlation plot, based on the target column specified ( decreasing)\ndef CorrPlotLargest(df, target):\n    k = 10\n    numerical_feature_columns = list(df._get_numeric_data().columns)\n    cols = df[numerical_feature_columns].corr().nlargest(k, target)[target].index\n    cm = df[cols].corr()\n    plt.figure(figsize=(10,6))\n    return sns.heatmap(cm, annot=True, cmap = 'viridis')\n\n#Function to get a sorted correlation plot, based on the target column specified (increasing)\ndef CorrPlotSmallest(df, target):\n    k = 10\n    numerical_feature_columns = list(df._get_numeric_data().columns)\n    cols = df[numerical_feature_columns].corr().nsmallest(k-1, target)[target].index\n    cols = cols.insert(0,target)\n    cm = df[cols].corr()\n    plt.figure(figsize=(10,6))\n    return sns.heatmap(cm, annot=True, cmap = 'viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>So as we have to merge the 3 grades column lets just take avg for now as we can't give\nmore preference ourselves for now</b> <br> Making a single column as grade avg also helps in analysis as now we only neeed to compare the features to this variable and not all the tests."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nalcdata['G_avg'] = (alcdata['G1'] + alcdata['G2'] + alcdata['G3'])/3\n\n#extracting features\nalcdata_features = alcdata.iloc[:,:-4]\n\n#extracting target\nalcdata_target = alcdata.iloc[:,-1]\n\n#drawing a correlation matrix for the numeric data or non object data sorted on the basis of max correlation with g_avg\nsubplot1 = CorrPlotLargest(alcdata,'G_avg')\n\n#drawing a correlation matrix for the numeric data or non object data sorted on the basis of negative correlation with g_avg\nsubplot2 = CorrPlotSmallest(alcdata,'G_avg')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>After seeing this we dont get much insight, some things is that all the grades G1,G2,G3 are pretty correlated to each other, also the failures are negatively correlated with the grades as it should be. So let's see some of the top things correlated with grades.</p>\n<ul>\n<li>So if you father's or mother's education is high you can get good grades which seems correct</li>\n<li>Your grades decrease as you age(this isn't a very strong relation but I guess as studies get harder it is difficult to score so not unexpected)</li>\n    <li>Also if you have more studytime your grade is better as well.</li>\n<li>And if you goout or travel more, your grades are expected to be low which is also consistent</li>\n    <li>And if you have more failures, your grades would be lesser </li>\n \n</ul>\n<b>These were some of the highest correlations that could be seen from the given heatmap</b>"},{"metadata":{},"cell_type":"markdown","source":"<p> Now let's see a distribution of g_avg w.r.t to all the above features to get a more better insight"},{"metadata":{"trusted":true},"cell_type":"code","source":"#As the amount of ppl in same age is different lets compare expectation value of marks in each age gp\nExpectedMarks_With_Age = alcdata.groupby('age').apply(lambda x : x['G_avg']/len(x)).groupby('age').sum()\nsns.jointplot(ExpectedMarks_With_Age.index,ExpectedMarks_With_Age.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see there is an apparent down slope except for at age 20, let's see the reason why is that."},{"metadata":{"trusted":true},"cell_type":"code","source":"ExpectedMarks_With_Age[20] \nalcdata[alcdata['age'] == 20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well it seems there were 2 very good performers in the age gp of 20 which we could say were kind of like outliers to the data set that we had and are making the mean score of age gp 20 skyrocket.\n"},{"metadata":{},"cell_type":"markdown","source":"Next we are checking the relation of grades with father's and mother's education and how it correlates with the \ngrade"},{"metadata":{"trusted":true},"cell_type":"code","source":"rel = ['Medu','Fedu','studytime']\nfor col in rel:\n    ex = alcdata.groupby(col)['G_avg'].mean()\n    #print(col)\n    #print(ex.index,ex.values)\n    sns.jointplot(ex.index,ex.values)\n    \n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All of the above factors except for some outliers at the value where mother and father have an education level of 0 , we \nsee an apparent increase in grades as father's of mother's education increase. We can also see the same relation with study time as well."},{"metadata":{},"cell_type":"markdown","source":"### 2. If there is a need for encoding some of the features,  how would you go  about it? \nWould you consider combining certain encodings together ?\n"},{"metadata":{},"cell_type":"markdown","source":"Now that we have looked at the *numerical categories* lets take a look at the *categorical data* as well with the help of one hot encoding."},{"metadata":{"trusted":true},"cell_type":"code","source":"alcdata.describe()\nobj_alcdata = alcdata_features.select_dtypes(include=['object']).copy()\n#obj_alcdata\n\nonehot_objects = pd.get_dummies(obj_alcdata)\nonehot_objects\n#These are all the categorical items\n#for col in onehot_objects.columns:\nmeans = []\n\n#print(alcdata.iloc[onehot_objects[onehot_objects[col] == 1].index]['G_avg'].sum())\nfor col in onehot_objects.columns:\n    #print(alcdata.iloc[onehot_objects[onehot_objects[col] == 1].index]['G_avg'].sum(),len(onehot_objects[col]))\n    means.append(alcdata.iloc[onehot_objects[onehot_objects[col] == 1].index]['G_avg'].sum()/len(onehot_objects[onehot_objects[col]==1]))\n\nprint(len(means))\nplt.figure(figsize = (20,8))\ng = sns.barplot(data=onehot_objects,ci=None)\nplt.xticks(rotation = 'vertical')\nplt.ylabel(\"Probability of occurence of a particular categorical gp\")\n#onehot_objects[onehot_objects[onehot_objects.columns] == 1].sum()[onehot_objects.columns[1]]/len(onehot_objects[onehot_objects.columns[1]])\n\nfor index, row in alcdata.iterrows():\n    g.text(index,onehot_objects[onehot_objects[onehot_objects.columns] == 1].sum()[onehot_objects.columns[index]]/len(onehot_objects[onehot_objects.columns[index]]), round(means[index],1), color='black', ha=\"center\")\n#A plot of probabilities of the entity being in one of the following categories","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is plot of average marks in comparision to the categorical datasets. Y axis shows the probability of occurence of a particular dataset, X, axis shows the dataset and the value on top of the bar plots tells the mean score in each category."},{"metadata":{},"cell_type":"markdown","source":"#### So we can make some statements after saying this like\n1. School doesnt affect the grades by a lot\n2. Even though there are more females in school , men score higher as compared to females\n3. Rural or Urban living conditions affect the grade by quite a lot.\n4. Apparently if the parents are apart the student are scoring more, but it could be due to some specially high cases in it as the set with parents apart is pretty low as compared to the parents together\n5. Mother's and Father's job plays a major role in upgrading the grades, with the best combination being mother in *healthcare* and father in *teaching* profession.\n6. Student who have a father or mother are likely to score compared to with other guardian.\n7. So if a student has extra educational support the student scores lesser in the exams, hmmmmmm interseting,same with family educational support.\n8. Extra paid classes don't help the student much.\n9. If a student wants to pursue higher education the data shows , that they have scored much better grades than those who don't which makes sense as if they don't want to pursue higher there is no reason for them to get good grades here in the first place, maybe they are taking over some father's buisness or such.\n10. Internet helps a student to learn more and hence score more in the exams."},{"metadata":{},"cell_type":"markdown","source":"Now to improve the features as some of the features only have 2 binary otuputs we can clearly do label encoding in them,\nas for the others Im going with one hot for them as It doesnt increase the features like that by a lot"},{"metadata":{"trusted":true},"cell_type":"code","source":"le = preprocessing.LabelEncoder()\ncols = alcdata.nunique()\nnew_cols = []\nother_cols = []\n\n\nfor col in alcdata.columns:\n        if alcdata.nunique()[col] == 2:\n            new_cols.append(col)\n        else:\n            if(alcdata[col].dtype == 'object'):\n                other_cols.append(col)\nle = preprocessing.LabelEncoder()\nfor col in new_cols:\n    alcdata[col] = le.fit_transform(alcdata[col].astype(str))\n\nnew_obj_data = pd.get_dummies(alcdata[other_cols])\n\nnew_obj_data\n\nalcdata = alcdata.merge(new_obj_data,on = alcdata.index)\n\nalcdata.drop(other_cols,inplace = True)\n\nalcdata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### 3. Try to find out how family relation(famrel) and parents cohabitation(Pstatus) affect grades of students. \n"},{"metadata":{"trusted":false},"cell_type":"code","source":"alcdata['Pstatus'] = alcdata['Pstatus'].apply(lambda x: x.replace('A','1')).apply(lambda x: x.replace('T','0')).astype(int)\nrel_alcdata = alcdata[['famrel','Pstatus','G3']].groupby('famrel')\n\n\nfinal_val = rel_alcdata['Pstatus'].sum()/rel_alcdata['Pstatus'].size()\nfinal_val\nsns.barplot(final_val.index,final_val.values)\nplt.ylabel('PStatus')\n#enter code/answer in this cell. You can add more code/markdown cells below for your answer. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that the probability that has a poor family condition the probability that his parents are apart is quite high, so there is clear correlation between these 2 columns. There are an increase towards the end, lets see it in next maybe what is the reason."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n### 4. Figure out which features in the data are skewed, and propose a way to remove skew from all such columns. "},{"metadata":{"trusted":false},"cell_type":"code","source":"#enter code/answer in this cell. You can add more code/markdown cells below for your answer.\n#alcdata_features.dtypes\n#alcdata_features['health']\n#sns.barplot(alcdata_target.index,alcdata_target['G3'].values)\n#plt.figure(figsize = (15,5))\n#alcdata_features.mean()\npossible_skews = alcdata.loc[:, alcdata.dtypes != 'object']\nabs(possible_skews.kurtosis()) > 0.5\nsns.pairplot(data = possible_skews)\n\n#Features which show that they have a skew when saw kurtosis\n#   traveltime,failures,famrel,Dalc,Walc,absences\n\nplt.show()\n\n\n#possible_skews['absences'].apply(lambda x: np.power(x,0.02)).apply(lambda x:x - possible_skews['absences'].mean()).hist(bins = 50,figsize = (5,5))\n#sns.boxplot(alcdata_features.absences,ax=ax[1])\nplt.hist(possible_skews['absences'].apply(lambda x: np.power(x,0.45)),bins=40)\n\n#Since cotegoricl data don;t come from the normal distribution therefore there is no concept of skew in these variables\n#Thus the only variable here witha skew is absence/ all others either are categorical / have minimal kurtosis / \n#Graph doesnt show much skew\n#plt.hist(np.power(alcdata.Walc,1),bins=40)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"possible_skews['absences'].apply(lambda x: np.power(x,0.45)).kurtosis()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"possible_skews.hist(bins = 50,figsize = (15,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot(alcdata)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part - 2\n## FIFA 2019  Data\n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Which clubs are the most economical? How did you decide that?"},{"metadata":{},"cell_type":"markdown","source":"This is just a test column as some values for bayern munich werent upto the mark"},{"metadata":{"trusted":false},"cell_type":"code","source":"fifadata.iloc[fifadata['Value'].apply(lambda x: x.replace('€','')).apply(lambda x: x.replace('M','e06')).apply(lambda x: x.replace('K','e03')).astype(float).sort_values(ascending = False).index[10:]][['Name','Value','Release Clause','Club']][fifadata['Club'] == \"FC Bayern München\"].head(100)\n#fifadata['Release Clause'].apply(lambda x: x.replace('€','')).apply(lambda x: x.replace('M','e06')).apply(lambda x: x.replace('K','e03')).astype(float)\n#fifadata['Release Clause'] = fifadata['Release Clause'].apply(lambda x: x.replace('€','')).apply(lambda x: x.replace('M','e06')).apply(lambda x: x.replace('K','e03')).astype(float)\n\n#remove_nan = fifadata.copy()\nnull_data = fifadata[fifadata['Release Clause'].isnull()]\nnon_null_data = fifadata[fifadata['Release Clause'].notnull()]\nnon_null_data['Release Clause'] = non_null_data['Release Clause'].apply(lambda x: x.replace('€','')).apply(lambda x: x.replace('M','e06')).apply(lambda x: x.replace('K','e03')).astype(float)\ngped_data = non_null_data.groupby('Overall')\n#non_null_data.head()\ngped_data = gped_data['Release Clause'].mean()\n#gped_data\n#null_data['Release Clause'] = gped_data[null_data['Overall']].values\n#null_data['Release Clause']\n#fifadata[fifadata['Release Clause'].isnbull()].apply(lambda x: x)\n#fifadata[\"Release Clause\"].fillna(null_data[\"Release Clause\"], inplace=True)\n\n#gped_data[88]\n\n#fifadata\n#CorrPlotLargest(remove_nan,'Release Clause')\n#So i guess International Reputation and Overall are pretty good standards to get the release clause\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"above cell is later used in next column to replace the 0 or null value in the release claluse with the mean of the players with same overall as that player, due to high correlation between those columns."},{"metadata":{},"cell_type":"markdown","source":"So my intution for the most economical club would be one which even whn it goes backrupt and has to sell all its players\nhas the maximum amount of money. So when a player is sold the minimum amount that is recieved by the club is the release \nclause and value here is assumed by me as the money spent by the club to get that player. \nTherefore for me economical:\n    \n                                             Release Clause - (Value + Wages)\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"#So my intution for the most economical club would be one which has enough money for it to have \n#highest release clause when summed over all the players which yields us the following list\n\n\nfifadata['Release Clause'].replace(np.nan,'€0.0M',inplace=True)\n#fifadata[fifadata['Release Clause'] == '€0.0M']\ntestdata = fifadata.copy()\ntestdata.head()\ntestdata.head()\ntestdata['Release Clause'] = testdata['Release Clause'].apply(lambda x: x.replace('€','')).apply(lambda x: x.replace('M','e06')).apply(lambda x: x.replace('K','e03')).astype(float)\ntestdata['Value'] = testdata['Value'].apply(lambda x: x.replace('€','')).apply(lambda x: x.replace('M','e06')).apply(lambda x: x.replace('K','e03')).astype(float)\ntestdata['Wage'] = testdata['Wage'].apply(lambda x: x.replace('€','')).apply(lambda x: x.replace('M','e06')).apply(lambda x: x.replace('K','e03')).astype(float)\n\nnull_data['Release Clause'] = gped_data[null_data['Overall']].values\n#null_data['Release Clause']\n#fifadata[fifadata['Release Clause'].isnull()].apply(lambda x: x)\n#fifadata[\"Release Clause\"].fillna(null_data[\"Release Clause\"], inplace=True)\n\ntestdata[testdata['Release Clause'] == 0]['Release Clause'] = null_data['Release Clause']\n#testdata.head()\nclub_data = testdata[['Wage','Value','Release Clause','Club']].groupby('Club')\nAmountPerClub = club_data['Release Clause'].apply(lambda x: x.sum()) - club_data['Value'].apply(lambda x: x.sum()) - club_data['Wage'].apply(lambda x: x.sum())   \nAmountPerClub\nfinal_values = AmountPerClub.apply(lambda x: (x - AmountPerClub.values.min())/AmountPerClub.values.max())\nfinal_values =AmountPerClub.sort_values(ascending = False)\nfig, axs = plt.subplots(3,1,figsize=(17,15))\nfig.tight_layout()\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=1.5)\naxs[0].tick_params(axis='x', rotation=90)\nsns.barplot(final_values[:100].index,final_values[:100].values,ax=axs[0])\naxs[1].tick_params(axis='x', rotation=90)\nsns.barplot(final_values[100:200].index,final_values[100:200].values,ax=axs[1])\naxs[2].tick_params(axis='x', rotation=90)\nsns.barplot(final_values[200:300].index,final_values[200:300].values,ax=axs[2])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"fifadata['Club'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here some of the players dont have a club value as well lets keep it for later to relate club value with other things. But it is potentially very difficult to predict this value. "},{"metadata":{},"cell_type":"markdown","source":"### 2. What is the relationship between age and individual potential of the player? How does age influence the players' value? At what age does the player exhibit peak pace ?"},{"metadata":{"trusted":false},"cell_type":"code","source":"corrMatrix = testdata[['Age','Potential','Value']].corr()\n#plt.figure(figsize = (20,20))\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"from this correlation matrix above we can see that there is a negative correlation between the age and potential i.e the potential of a player is decreasing as the age increases. Now as for the max value, lets do this in the next cell"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#fifadata['Age'].isnull().sum()\ngp_data = fifadata[['Age','Potential']].groupby('Age')\nfv = gp_data['Potential'].max()\nsns.barplot(fv.index,fv.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The decrease plot is shown here. The max has been takes as we are seeing the peak performance at a particular age gp."},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.jointplot(fifadata['Potential'],fifadata['Age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.jointplot(testdata['Age'],testdata['Value'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph shows that a player's value is max at some point between 25-30 and follows a gaussian pattern"},{"metadata":{"trusted":false},"cell_type":"code","source":"fifadata.columns\n#'Acceleration','SprintSpeed', 'Agility' ,  'Stamina'\nfifadata['Acceleration'].fillna(fifadata['Acceleration'].median(),inplace = True)\nfifadata['SprintSpeed'].fillna(fifadata['SprintSpeed'].median(),inplace = True)\nfifadata['Agility'].fillna(fifadata['Agility'].median(),inplace = True)\nfifadata['Stamina'].fillna(fifadata['Stamina'].median(),inplace = True)\nfifadata[['Acceleration','SprintSpeed', 'Agility' ,  'Stamina']].isnull().sum()\n#fifadata[['SprintSpeed','Age']].groupby('Age').mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. What skill sets are helpful in deciding a player's potential? How do the traits contribute to the players' potential? "},{"metadata":{"trusted":false},"cell_type":"code","source":"testdata = fifadata.copy()\n#fifadata['Wage'].isnull().sum()\nplt.figure(figsize = (30,4))\ntestdata['Wage'] = testdata['Wage'].apply(lambda x: x.replace('€','')).apply(lambda x: x.replace('M','e06')).apply(lambda x: x.replace('K','e03')).astype(float)\n\n#fifadata.info()\nCorrPlotLargest(testdata,'Potential')\n#CorrPlotSmallest(testdata,'Potential')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So here we can see there is quite a high correlation:\n\nPotential V traits: +ve correlation with Overall , Reactions, Composure, International Reputation, Special and so on"},{"metadata":{"trusted":false},"cell_type":"code","source":"cols = testdata.columns\nfig,ax = plt.subplots(5,1,figsize = (20,20))\nsns.barplot(testdata['Overall'],testdata['Potential'],ax = ax[0])\nsns.barplot(testdata['Reactions'],testdata['Potential'],ax = ax[1])\nsns.barplot(testdata['Composure'],testdata['Potential'],ax = ax[2])\nsns.barplot(testdata['International Reputation'],testdata['Potential'],ax = ax[3])\n#sns.barplot(testdata['Special'],testdata['Potential'],ax = ax[4])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see in all of them there is an apparent increase in the Potential along witht he respective features"},{"metadata":{"trusted":false},"cell_type":"code","source":"cols = testdata.columns\nfig,ax = plt.subplots(5,1,figsize = (20,20))\nsns.barplot(testdata['BallControl'],testdata['Potential'],ax = ax[0])\nsns.barplot(testdata['Skill Moves'],testdata['Potential'],ax = ax[1])\nsns.barplot(testdata['LongPassing'],testdata['Potential'],ax = ax[2])\nsns.barplot(testdata['Dribbling'],testdata['Potential'],ax = ax[3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Which features directly contribute to the wages of the players?"},{"metadata":{"trusted":false},"cell_type":"code","source":"CorrPlotLargest(testdata,'Wage')\n#CorrPlotSmallest(testdata,'Wage')\n\n#enter code/answer in this cell. You can add more code/markdown cells below for your answer. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. What is the age distribution in different clubs? Which club has most players young?"},{"metadata":{},"cell_type":"markdown","source":"Here I am assuming young age refers to below 21, and on basis of that I am finding the count of players below that age\nin different clubs"},{"metadata":{"trusted":false},"cell_type":"code","source":"club_data = fifadata[['Age','Club']].groupby('Club')\nyoung_age = 21\nageVclub = club_data['Age'].apply((lambda x: len(x[x<=young_age])))\nfinal_values = ageVclub.sort_values(ascending = False)\nfig, axs = plt.subplots(3,1,figsize=(17,18))\nfig.tight_layout()\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=2)\naxs[0].tick_params(axis='x', rotation=90)\nsns.barplot(final_values[:100].index,final_values[:100].values,ax=axs[0])\naxs[1].tick_params(axis='x', rotation=90)\nsns.barplot(final_values[100:200].index,final_values[100:200].values,ax=axs[1])\naxs[2].tick_params(axis='x', rotation=90)\nsns.barplot(final_values[200:300].index,final_values[200:300].values,ax=axs[2])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus we can see that the Clubs like FC NORdjessallen, Ajax etc. have invested more into building their team comprised of younger players"},{"metadata":{},"cell_type":"markdown","source":"# Part - 3\n## UK Road Accidents Data\n\n\nThe UK government amassed traffic data from 2000 and 2016, recording over 1.6 million accidents in the process and making this one of the most comprehensive traffic data sets out there. It's a huge picture of a country undergoing change."},{"metadata":{},"cell_type":"markdown","source":"### 1. The very first step should be to merge all the 3 subsets of the data."},{"metadata":{},"cell_type":"markdown","source":"Since all the dataframes have exactly the same columns which I saw individually, we can simply concat the three sets \nand work on these sets individually according to our will."},{"metadata":{"trusted":false},"cell_type":"code","source":"print(accidata1.columns == accidata2.columns)\nprint(accidata2.columns == accidata1.columns)\nprint(accidata2.columns == accidata3.columns)\nprint(accidata3.columns == accidata2.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata = pd.concat([accidata1,accidata2,accidata3])\n#Since on initial analysis it displayed itself as a entirely null column\naccidata = accidata.drop('Junction_Detail',axis = 1)\n#len(accidata)\n#accidata.isnull().sum()\n#accidata.info()\n\n\n\n\n#Again the number of null values in Longitude and Latitude seemed to be pretty low so I just straight away removed them \n#so that it doesn't cause trouble in later stages\naccidata = accidata.dropna(axis=0,thresh = 28)\n\n#Removing the null values of those variable that are very hard to predict and are pretty low\naccidata = accidata[accidata['Special_Conditions_at_Site'].notna()]\n#accidata = accidata[accidata['LSOA_of_Accident_Location'].notna()]\n\n#Creating a new category unknown for categorical variable that have a high amount of nulll values\naccidata['LSOA_of_Accident_Location'] = np.where(accidata['LSOA_of_Accident_Location'].isnull(),\"Unknown_location\",accidata['LSOA_of_Accident_Location'])\naccidata['Junction_Control'] = np.where(accidata['Junction_Control'].isnull(),\"Unknown_Junction\",accidata['Junction_Control'])\n\n#Filling with the max imputed value\naccidata['Pedestrian_Crossing-Human_Control'] = accidata['Pedestrian_Crossing-Human_Control'].fillna('None within 50 metres')\naccidata['Pedestrian_Crossing-Physical_Facilities'] = accidata['Pedestrian_Crossing-Physical_Facilities'].fillna('No physical crossing within 50 meters')\n\n#Removing all the rest of the null values\naccidata = accidata.dropna()\n\n#Setting date to be a python timestamp\naccidata['Date'] = accidata['Date'].apply(lambda x: time.mktime(datetime.datetime.strptime(str(x),\"%d/%m/%Y\").timetuple()))\n\n#Very high correlation with date so can be removed\naccidata = accidata.drop('Year',axis = 1)\n\naccidata.notna().sum()\ntest = accidata['Accident_Severity']\n#enter code/answer in this cell. You can add more code/markdown cells below for your answer. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above column is a mixture of merge of all the columns and we are preprocessing the data as well. More details have been written somewhat in the comments."},{"metadata":{"trusted":false},"cell_type":"code","source":"accidata['LSOA_of_Accident_Location'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"accidata['Junction_Control'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting a Correlation Mapping between the different features"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (15,15))\ncorrMatrix = accidata.corr()\nsns.heatmap(corrMatrix, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now since there 1:1 correlation between longitude and Location_Eastern_OSGR and some with latitude and \nLocation_Northing_OSGR, we can remove one of the 2 columns in both."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking for skew\naccidata[['Location_Easting_OSGR']]\naccidata['Location_Easting_OSGR'].kurtosis()\naccidata[['Location_Northing_OSGR']]\naccidata['Location_Northing_OSGR'].kurtosis()\n\n#Location Northing Shows a bit of skew so updating it to not have skew\naccidata['Location_Northing_OSGR'] = accidata['Location_Northing_OSGR'].apply(lambda x: np.log(x))\n\n#accidata","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#removing longitude and latitude as they are the same as the other 2 and above we have update the skew in both and stored\n#here in this column\naccidata = accidata.loc[:, accidata.columns != 'Longitude']\naccidata = accidata.loc[:, accidata.columns != 'Latitude']\naccidata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just some columns to see the plot of eastern and checking if there is no skew there"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Location\naccidata['Location_Easting_OSGR'].apply(lambda x : np.power(x,1)).hist(bins = 1000,figsize = (5,5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pd.set_option('display.max_columns', 50)\naccidata.head(150)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. What are the number of casualties in each day of the week? Sort them in descending order. "},{"metadata":{"trusted":false},"cell_type":"code","source":"#Pretty much a simple group by, sum and then sort\nWeekday_Deaths = accidata.groupby('Day_of_Week')['Number_of_Casualties'].sum()\nWeekday_Deaths = Weekday_Deaths.sort_values(ascending = False)\nlabels = ['Mon','Tue','Wed','Thur','Fri','Sat','Sun']\ng = sns.barplot(Weekday_Deaths.index,Weekday_Deaths.values,order = Weekday_Deaths.index)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. On each day of the week, what is the maximum and minimum speed limit on the roads the accidents happened?"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Here you can change the 'Road_Type' column to see how exactly that feature changes speed limit.\nMaxSpeedLimit_Week = accidata.groupby(['Day_of_Week','Road_Type'])['Speed_limit'].max()\nMinSpeedLimit_Week = accidata.groupby(['Day_of_Week','Road_Type'])['Speed_limit'].min()\n\nfig, axs = plt.subplots(14,1,figsize=(15,50))\n\nfor i in range(1,8):\n    if((i-1)%2 == 0):\n        sns.barplot(MaxSpeedLimit_Week[i].index,MaxSpeedLimit_Week[i].values,ax = axs[i-1])\n        axs[i-1].set_ylabel('Max Speed on Week Day' + str(i))\n        sns.barplot(MinSpeedLimit_Week[i].index,MinSpeedLimit_Week[i].values,ax = axs[i])\n        axs[i].set_ylabel('Min Speed on Week Day' + str(i))\n\nfor i in range(1,8):\n    if((i-1)%2 != 0):\n        sns.barplot(MaxSpeedLimit_Week[i].index,MaxSpeedLimit_Week[i].values,ax = axs[7 + i-1])\n        axs[7 + i-1].set_ylabel('Max Speed on Week Day' + str(i))\n        sns.barplot(MinSpeedLimit_Week[i].index,MinSpeedLimit_Week[i].values,ax = axs[7 + i])\n        axs[7 + i].set_ylabel('Min Speed on Week Day' + str(i))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pretty much consistent data around the week, except for a change in single carriagewat min and max speed changing around\ndifferent days of the week, and the same in One way streen but only in the even week days and Sunday(7)."},{"metadata":{},"cell_type":"markdown","source":"### 4. What is the importance of Light and Weather conditions in predicting accident severity? What does your intuition say and what does the data portray?"},{"metadata":{"trusted":false},"cell_type":"code","source":"val = accidata['Light_Conditions'].value_counts()\nsns.barplot(val.index,val.values)\nplt.xticks(rotation = 'vertical')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"val = accidata['Weather_Conditions'].value_counts()\nsns.barplot(val.index,val.values)\nplt.xticks(rotation = 'vertical')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Severity_Light = accidata.groupby('Accident_Severity')['Light_Conditions'].value_counts()\nfig,axs = plt.subplots(2,1,figsize = (10,10))\n#fig.tight_layout()\n#plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=2)\nseverity_index = [item[0] for item in Severity_Light.index] \nlight_index = [item[1] for item in Severity_Light.index] \nsns.barplot(severity_index[:-10],Severity_Light.values[:-10],light_index[:-10],ax = axs[0])\nsns.barplot(severity_index,Severity_Light.values,light_index,ax = axs[1])\n\nplt.xticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>So we can see from the first plot that most of the accidents happened when the daylight were present followed by \npresent and lit , which was not expected by me as well it makes sense for more accidents to happen in night when ppl \ncan't see.</p>\n<p>Seeing the second plot tells us that as accidents generally tend to be more towards a severity of 3 than compared to 2 and 1 and all of them follow the same trend as above.</p>\n"},{"metadata":{},"cell_type":"markdown","source":"### 5. To predict the severity of the accidents which columns do you think are unnecessary and should be dropped before implementing a regression model. Support your statement using relevant plots and hypotheses derived from them."},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (20,5))\nCorrPlotLargest(accidata,'Accident_Severity')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well this is just sad we dont have any good correlation, but they dont tell causaality so lets look at categorical data\nthey might provide some more insite."},{"metadata":{"trusted":true},"cell_type":"code","source":"accidata_categorical = accidata[accidata.columns[accidata.dtypes == object]]\ntarget = accidata['Accident_Severity']\n#accidata_categorical\ndf = pd.merge(accidata_categorical,target,on = accidata_categorical.index)\ngped_data = df.groupby('Accident_Severity')\nfig,axs = plt.subplots(len(accidata_categorical.columns),1,figsize = (10,10))\nfig.tight_layout()\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=2)\nkey = 0\nfor i in accidata_categorical.columns:\n    val = gped_data[i].value_counts()\n    sns.barplot(val.index,val.values,ax = axs[key])\n    key += 1\nplt.show()\n#Severity_Light = accidata_categorical.groupby('Road_Type')['Light_Conditions'].value_counts()\n#fig,axs = plt.subplots(2,1,figsize = (10,10))\n#fig.tight_layout()\n#plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=2)\n#severity_index = [item[0] for item in Severity_Light.index] \n#light_index = [item[1] for item in Severity_Light.index] \n#sns.barplot(severity_index[:-10],Severity_Light.values[:-10],light_index[:-10],ax = axs[0])\n#sns.barplot(severity_index,Severity_Light.values,light_index,ax = axs[1])\n\n#plt.xticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"accidata['Number_of_Casualties'].isnull().sum()\n\n#below is a set of data that needs to be removed\n#accidata['Date'] = pd.to_datetime(val, errors='coerce', cache=False).strftime('%m/%d/%Y')\naccidata['Date'] = accidata['Date'].apply(lambda x: time.mktime(datetime.datetime.strptime(str(x),\"%d/%m/%Y\").timetuple()))\n\n#too much corelation with date\naccidata = accidata.drop('Year',axis = 1)\n#accidata = accidata.drop('Year',axis = 1)\n\n#There is no need for index or time there is realy low correaltion with them\naccidata = accidata.drop('Accident_Index',axis = 1)\naccidata = accidata.drop('Time',axis = 1)\n\n#Im using label encoder for all of the features\nle = preprocessing.LabelEncoder()\ncols = accidata.select_dtypes(include=['object']).columns\nfor col in cols:\n    accidata[col] = le.fit_transform(accidata[col].astype(str))\n\naccidata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. Implement a basic Logistic Regression Model using scikit learn with cross validation = 5, where you predict the severity of the accident (Accident_Severity). Note that here your goal is not to tune appropriate hyperparameters, but to figure out what features will be best to use."},{"metadata":{"trusted":false},"cell_type":"code","source":"testdata = accidata.copy()\ntestdata1 = accidata.copy()\ntestdata2 = accidata.copy()\n\ntestdata[testdata['Accident_Severity'] < 3] = 0\ntestdata[testdata['Accident_Severity'] == 3] = 1\ntestdata['Accident_Severity'].value_counts()\n\ntestdata1[testdata1['Accident_Severity'] != 2] = 0\ntestdata1[testdata1['Accident_Severity'] == 2] = 1\ntestdata1['Accident_Severity'].value_counts()\n\n\ntestdata2[testdata2['Accident_Severity'] != 1] = 0\ntestdata2[testdata2['Accident_Severity'] == 1] = 1\ntestdata2['Accident_Severity'].value_counts()\n\n#testdata1 = accidata[accidata['Accident_Severity'] < 3]\ntestdata['Accident_Severity'].value_counts()\n#testdata2 = accidata[accidata['Accident_Severity'] < 3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#accidata[accidata['Accident_Severity'] > 1] = 1\nfrom sklearn.model_selection import train_test_split\n\ntestdata['Accident_Severity'].value_counts()\ny = accidata['Accident_Severity']\nX = accidata.loc[:, accidata.columns != 'Accident_Severity']\nX_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size=0.3)\n\nX_scaled = preprocessing.scale(X_train)\nlen(Y_test3)\n#enter code/answer in this cell. You can add more code/markdown cells below for your answer. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlogreg1 = LogisticRegressionCV(cv=5, n_jobs = -1,random_state=0,multi_class = \"multinomial\").fit(X_scaled,Y_train)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test = preprocessing.scale(X_test)\nX_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"logreg1.score(X_test,Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}