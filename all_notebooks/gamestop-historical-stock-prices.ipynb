{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas_profiling import ProfileReport\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/gamestop-historical-stock-prices/GME_stock.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"profile = ProfileReport(data, title=\"Pandas Profiling Report\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting the date column into datetime index\ndata['date'] = pd.to_datetime(data['date'])\ndata.set_index('date',inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.index)\nprint('\\nUnique dates in our data: ', len(data.index.unique()), 'Days')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have now created a column for each category, we can see there no longer repeated values in the Datetime Index. \n\n## Generating a complete Index and Setting Frequency\nSince we are using daily data, we would like to set a daily frequency. We see our data has a length of 4773 days. By subtracting the smallest date from the largest date, we can tell there are some days missing:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\nUnique dates in our data: ', len(data.index.unique()), 'Days')\nour_date_range = data.index.max() - data.index.min()\n\n# Calculate number of days in date range\nprint('Total days in our date range:', our_date_range.days, 'Days')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_index = pd.date_range(data.index.min(), data.index.max())\ndata_new = data.reindex(new_index, fill_value=0)\ndata_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_weekly = data_new.resample('W').sum()\nprint('Weekly Sales')\nprint(sales_weekly.head(), '\\n')\n\nsales_monthly = data_new.resample('M').sum()\nprint('Monthly Sales')\nprint(sales_monthly.head(), '\\n')\n\nsales_quarterly = data_new.resample('Q').sum()\nprint('Quarterly Sales')\nprint(sales_quarterly.head(), '\\n')\n\nsales_annual = data_new.resample('Y').sum()\nprint('Annual Sales')\nprint(sales_annual.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_quarterly['close_price'].plot(figsize=(13,5))\nsales_monthly['close_price'].plot(figsize=(13,5))\nsales_weekly['close_price'].plot(figsize=(13,5), title='Close Price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_quarterly['open_price'].plot(figsize=(13,5))\nsales_monthly['open_price'].plot(figsize=(13,5))\nsales_weekly['open_price'].plot(figsize=(13,5), title='Open Price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_quarterly['open_price'].plot(figsize=(13,5))\nsales_monthly['open_price'].plot(figsize=(13,5))\nsales_weekly['open_price'].plot(figsize=(13,5), title='Open Price')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the data from december 2020 to january 2021 to view the trend\nfig = px.line(data, x=data.index, y=data.columns, \n              range_x=['2020-12-01','2021-01-28'],\n              title='Plot of values for December 20 and January 21')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the total amount traded\ndata['total_amount_traded'] = data['open_price']*data['volume']\n\nfig = px.line(data, x=data.index, y=data.total_amount_traded,\n              title='Plot of total amount traded')\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the exponential moving average for the opening price\ndata['EWMA12'] = data['open_price'].ewm(span=12).mean()\n#data[['open_price','EWMA12']].plot(figsize=(16,8))\nfig = px.line(data[['EWMA12']], x=data.index, y=data.open_price,\n              title='Moving average of opening price')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting candlestick chart\nfig = go.Figure(data=[go.Candlestick(x=data.index,\n                open=data['open_price'],\n                high=data['high_price'],\n                low=data['low_price'],\n                close=data['close_price'])])\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(data, plotSize, textSize):\n    data = data.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    data = data.dropna('columns')\n    data = data[[col for col in data if data[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(data)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    data = data[columnNames]\n    ax = pd.plotting.scatter_matrix(data, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = data.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotScatterMatrix(data, 18, 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model development\n\n## LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing required libraries\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, SimpleRNN, Activation\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating dataframe\ndata = data.sort_index(ascending=True, axis=0)\ntrain_data = pd.DataFrame(index=range(0,len(data)),columns=['Date', 'Close'])\nfor i in range(0,len(data)):\n    train_data['Date'][i] = data.index[i]\n    train_data['Close'][i] = data['close_price'][i]\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#setting index\ntrain_data['Date'] = pd.to_datetime(train_data['Date'])\ntrain_data.set_index('Date',inplace=True)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating train and test sets\ndataset = train_data.values\n\ntrain = dataset[0:3773,:]\nvalid = dataset[1000:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature scaling\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting dataset into x_train and y_train for 60 timesteps\nx_train, y_train = [], []\nfor i in range(60,len(train)):\n    x_train.append(scaled_data[i-60:i,0])\n    y_train.append(scaled_data[i,0])\nx_train, y_train = np.array(x_train), np.array(y_train)\n\nx_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the LSTM model\nregressor = Sequential()\n\nregressor.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units = 50))\nregressor.add(Dropout(0.2))\n\nregressor.add(Dense(units = 1))\n\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\nregressor.fit(x_train, y_train, epochs = 100, batch_size = 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting values, using past 60 from the train data\ninputs = train_data[len(train_data) - len(valid) - 60:].values\ninputs = inputs.reshape(-1,1)\ninputs  = scaler.transform(inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = []\nfor i in range(60,inputs.shape[0]):\n    X_test.append(inputs[i-60:i,0])\nX_test = np.array(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\nclosing_price_pred = regressor.predict(X_test)\nclosing_price_pred = scaler.inverse_transform(closing_price_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms=np.sqrt(np.mean(np.power((valid-closing_price_pred),2)))\nrms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting\ntrain = train_data[:3773]\nvalid = train_data[1000:]\nvalid['Predictions'] = closing_price_pred\nplt.plot(train['Close'])\nplt.plot(valid[['Close','Predictions']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Simple RNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the Simple RNN model\nregressor = Sequential()\n\nregressor.add(SimpleRNN(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))\nregressor.add(Dropout(0.2))\n\nregressor.add(SimpleRNN(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\nregressor.add(SimpleRNN(units = 50))\nregressor.add(Dropout(0.2))\n\nregressor.add(Dense(units = 1))\n\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\nregressor.fit(x_train, y_train, epochs = 100, batch_size = 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting values, using past 60 from the train data\ninputs = train_data[len(train_data) - len(valid) - 60:].values\ninputs = inputs.reshape(-1,1)\ninputs  = scaler.transform(inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = []\nfor i in range(60,inputs.shape[0]):\n    X_test.append(inputs[i-60:i,0])\nX_test = np.array(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\nclosing_price_pred = regressor.predict(X_test)\nclosing_price_pred = scaler.inverse_transform(closing_price_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rms=np.sqrt(np.mean(np.power((valid-closing_price_pred),2)))\nrms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting\ntrain = train_data[:3773]\nvalid = train_data[1000:]\nvalid['Predictions'] = closing_price_pred\nplt.plot(train['Close'])\nplt.plot(valid[['Close','Predictions']])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}