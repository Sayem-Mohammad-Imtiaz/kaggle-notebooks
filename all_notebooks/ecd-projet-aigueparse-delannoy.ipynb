{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Projet ECD - Alexis AIGUEPARSE & Jonas DELANNOY\n\nSource des données : https://www.kaggle.com/hwaitt/tennis-20112019#atp.csv\n\n## 1. Présentation des données\nCe notebook a pour objectif d'analyser des données receuillies auprès de 216 981 matches de Tennis ATP joués entre 2011 et 2019. Ces données regroupe des informations sur les résultats des matches, les caractérisques des joueurs, les paramètres d'avant-match...Nous souhaiterons, au cours de notre analyse, répondre aux questions suivantes :\n- Quel est l'âge optimal pour gagner au tennis ? \n- Jouer le jour de son anniversaire modifie-t-il significativement les chances de victoire ?\n- Est-ce que la fréquence de jeu joue un rôle primordial dans les chances de victoire ?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Préparation des données\nCette phase de pré-étude a pour objectif de préparer et formater les données afin de les rendre plus adaptées à nos besoins.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# importation des différentes librairies\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt # plotting\n\n# Parcours des jeux de données\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# lectures des données\nnRowsRead = None # nombre de lignes lues, 'None' pour lire le fichier entierement\ndf = pd.read_csv('/kaggle/input/tennis-20112019/atp.csv', delimiter=',', nrows = nRowsRead)\ndf.dataframeName = 'atp.csv'\n\n# sélection des données\ndf1 = df[['ID', 'GRes_1', 'GRes_2', 'Age_CUR_1', 'Age_CUR_2', 'IsBirthDay_CUR_1', 'IsBirthDay_CUR_2', 'DaysFromLast_CUR_1', 'DaysFromLast_CUR_2', 'TotalPointsWon_1', 'TotalPointsWon_2', 'Serve1stWon_1', 'Serve1stWon_2', 'Serve2ndWon_1', 'Serve2ndWon_2', 'Surface']]\ndf2 = df[['ID', 'GRes_1', 'Age_CUR_1', 'IsBirthDay_CUR_1', 'DaysFromLast_CUR_1', 'TotalPointsWon_1', 'Serve1stWon_1', 'Serve2ndWon_1', 'Surface']]\ndf3 = df[['ID', 'GRes_2', 'Age_CUR_2', 'IsBirthDay_CUR_2', 'DaysFromLast_CUR_2', 'TotalPointsWon_2', 'Serve1stWon_2', 'Serve2ndWon_2', 'Surface']]\n\ndef isCurOlder(ages):\n    if ages[0] > ages[1]:\n        return 1\n    else:\n        return 0\n\ndf2 = df2.assign(isOlder=pd.Series(map(isCurOlder, df1[['Age_CUR_1', 'Age_CUR_2']].to_numpy())))\ndf3 = df3.assign(isOlder=pd.Series(map(isCurOlder, df1[['Age_CUR_2', 'Age_CUR_1']].to_numpy())))\n\n# création d'un jeu de données fusionné\ndf3 = df3.rename(columns={\"GRes_2\": \"GRes_1\", \"Age_CUR_2\": \"Age_CUR_1\", \"IsBirthDay_CUR_2\": \"IsBirthDay_CUR_1\", \"DaysFromLast_CUR_2\": \"DaysFromLast_CUR_1\", \"TotalPointsWon_2\": \"TotalPointsWon_1\", \"Serve1stWon_2\": \"Serve1stWon_1\", \"Serve2ndWon_2\": \"Serve2ndWon_1\"})\ndfmerged = df2.append(df3)\n\n# suppression des NaN, non supporté par sklearn\ndfmerged = dfmerged[dfmerged['ID'].notna()]\ndfmerged = dfmerged[dfmerged['GRes_1'].notna()]\ndfmerged = dfmerged[dfmerged['Age_CUR_1'].notna()]\ndfmerged = dfmerged[dfmerged['IsBirthDay_CUR_1'].notna()]\ndfmerged = dfmerged[dfmerged['DaysFromLast_CUR_1'].notna()]\ndfmerged = dfmerged[dfmerged['TotalPointsWon_1'].notna()]\ndfmerged = dfmerged[dfmerged['Serve1stWon_1'].notna()]\ndfmerged = dfmerged[dfmerged['Serve2ndWon_1'].notna()]\ndfmerged = dfmerged[dfmerged['Surface'].notna()]\n\n# informations sur nos données\nnRow, nCol = dfmerged.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Analyse exploratoire unidimensionnelle\nCette phase a pour objectif de mieux comprendre nos données à l'aide de différents diagrammes et indicateurs. Nous pourrons ainsi remarquer la présence de données anormales à écarter.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1) Vérifications d'usage\n#print(df1.dtypes)\n#print(df1.shape)\n#print(df1.count())\n#print(df1.describe())\n\n# 2) Histogramme & Boxplot & Secteur pour l'âge des joueurs\nprint(dfmerged.Age_CUR_1.describe())\nplt.figure()\ndfmerged.Age_CUR_1.plot.hist()\nplt.show()\n\nplt.figure()\ndfmerged.Age_CUR_1.plot(kind=\"box\")\nplt.show()\n\n# 2) Boxplot & Secteur pour les anniversaires\nprint(dfmerged.IsBirthDay_CUR_1.describe())\n\nprint(\"Nombre de joueurs jouant le jour de leur anniversaire : \" + str(list(dfmerged.IsBirthDay_CUR_1).count(1)))\n\nplt.figure()\ndfmerged.IsBirthDay_CUR_1.value_counts().plot.pie(figsize=[5,5])\nplt.show()\n\n# 2) Boxplot & Secteur pour la fréquence de jeu\nprint(dfmerged.DaysFromLast_CUR_1.describe())\nplt.figure()\ndfmerged.DaysFromLast_CUR_1.plot.hist()\nplt.show()\n\nplt.figure()\ndfmerged.DaysFromLast_CUR_1.plot(kind=\"box\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyse exploratoire sur l'âge\nLa moyenne de l'âge des joueurs est à environ 24 ans comme nous pouvions nous en douter. On voit bien sur l'histogramme que la grande proportion des joueurs se situe entre 18 et 30 ans. Or l'écart type reste faible (4,41), ce qui indique que la majorité des joueurs ont entre 20 et 28 ans.\nCependant nous pouvons observer sur la boîte à moustache concernant l'âge qu'il existe de nombreuses valeurs en dehors de la boîte considérées donc comme \"écartées\".  Le joueur le plus jeune enregistré dans le jeu de données a 12 ans et le plus âgé a 65 ans.\n\n\n### Analyse exploratoire sur les anniversaires\nLa probabilité qu'un joueur joue le jour de son anniversaire est (compte tenu des 365 jours annuel) faible. Nous avons donc décidé dans un premier temps de calculer le nombre d'occurrences de ce phénomène. Nous avons déterminé que 1121 joueurs ont joué le jour de leur anniversaire. Bien que ce chiffre est faible compte tenu du nombre de données disponibles, nous considérons qu'il est suffisant pour l'analyser sous certains points.\n\n### Analyse exploratoire sur la fréquence de jeu\nL'analyse unidimensionnelle sur la fréquence de jeu n'a rien donné. En effet, le jeu de données possède des valeurs trop écartées. Par exemple, on voit sur l'histogramme que la quasi totalité des joueurs a eu un match il y a moins de 300 jours environ or nos données présentent des valeurs jusqu'à 3500 jours. Le boxplot vérifie ces observations. Nous allons donc réaliser à nouveau notre analyse en écartant les valeurs lointaines :\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfmergedDayReduced = dfmerged[dfmerged['DaysFromLast_CUR_1']<=31]\nprint(dfmergedDayReduced.DaysFromLast_CUR_1.describe())\nplt.figure()\ndfmergedDayReduced.DaysFromLast_CUR_1.plot.hist()\nplt.show()\n\nplt.figure()\ndfmergedDayReduced.DaysFromLast_CUR_1.plot(kind=\"box\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ainsi nous pouvons voir que 90% des joueurs ont joué il y a moins de 31 jours. La moyenne du nombre de jours écoulés depuis le dernier match est de 4.7 jours environ pour les joueurs qui ont joué il y a moins de 31 jours alors qu'elle était de 20 jours sur l'ensemble des données. Globalement, nous pouvons dire qu'en moyenne les joueurs du jeu de données ont joué les 7 derniers jours (on a un 3ème quartile à 8 sur le jeu de données global).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4. Analyse exploratoire bidimensionnelle\nNous poursuivons notre analyse descriptive avec l'analyse bidimensionnelle pour explorer les données, les comprendre et peut-être découvrir des phénomènes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Discrétisation de la variable âge","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = range(12, 65)\ndfmerged['discretizedAges'] = pd.cut(dfmerged['Age_CUR_1'], bins=bins, labels=bins[:-1])\nprint(dfmerged)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyse des variables Age et Fréquence de jeu","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(dfmerged.Age_CUR_1, dfmerged.DaysFromLast_CUR_1, s=2, edgecolor = 'none')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(dfmerged.discretizedAges, dfmerged.DaysFromLast_CUR_1, s=10, edgecolor = 'none', marker = '.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"D'après le nuage de points global, on semble distinguer une tendance. Plus les personnes sont âgées et plus le dernier match joué serait récent ? Cette observation nous étonne, nous choisissons donc d'adapter nos données.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(bins[:-1], dfmerged.groupby('discretizedAges').DaysFromLast_CUR_1.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"D'après ce nuage de points qui considère les moyennes sur chaque tranche d'âge, nous distinguons une nouvelle tendance. Il semble que plus le joueur est âgé et plus son dernier match joué est lointain. \n\nNous allons maintenant voir s'il existe une relation entre ces deux variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Calcul du coéfficient de corrélation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfmerged.DaysFromLast_CUR_1 = dfmerged.DaysFromLast_CUR_1.fillna(0.0)\ndataCor = dfmerged[['Age_CUR_1', 'DaysFromLast_CUR_1']]\nprint(dataCor.corr(method='pearson'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"D'après nos analyses, il ne semble pas que les variables Age et Fréquence de jeu soient corrélées. On ne peut donc pas dire que l'âge influe sur la fréquence de jeu et inversement selon nos données.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Analyse des variables Anniversaire et Performance\n\nAfin d'analyser les performances des joueurs le jour de leur anniversaire, nous proposons de proceder à un test de chi² d'indépendance. Nous considérons que les performances d'un joueur sont mesurables à l'aide du nombre de points gagnés au premier et second service et du nombre total de points gagnés. C'est pourquoi nous analyserons les probabilités de chacun de ces parametres sachant que le joueur fete son anniversaire, ou non.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Nous posons l'hypothese nulle H0 : \"Jouer le jour de son anniversaire n'augmente pas les performances\". Nous admettons un seuil d'acceptabilité de 0.05.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats\n\n# Premier service gagnant\ndftmp = dfmerged.groupby('IsBirthDay_CUR_1').Serve1stWon_1.mean()\nprint(dftmp)\n# calcul du chi²\nserve1stWonAverage = dfmerged.Serve1stWon_1.mean()\nprint(scipy.stats.chisquare(f_obs=dftmp.to_numpy(), f_exp=serve1stWonAverage))\n\n# Second service gagnant\ndftmp = dfmerged.groupby('IsBirthDay_CUR_1').Serve2ndWon_1.mean()\nprint(dftmp)\n# calcul du chi²\nserve2ndWonAverage = dfmerged.Serve2ndWon_1.mean()\nprint(scipy.stats.chisquare(f_obs=dftmp.to_numpy(), f_exp=serve2ndWonAverage))\n\n# Total des points gagnés\ndftmp = dfmerged.groupby('IsBirthDay_CUR_1').TotalPointsWon_1.mean()\nprint(dftmp)\n# calcul du chi²\nTotalPointsWonAverage = dfmerged.TotalPointsWon_1.mean()\nprint(scipy.stats.chisquare(f_obs=dftmp.to_numpy(), f_exp=TotalPointsWonAverage))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Le calcul des p-valeurs ont révélées être strictement supérieures au seuil fixé. Ainsi, nous pouvons conclure que l'hypothese nulle n'est pas rejetable et donc que jouer le jour de son anniversaire d'augmente pas les performances de maniere statistiquement significative.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Analyse des variables Age et Victoire","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dfmerged)\n\nboxplot = dfmerged.boxplot(column='discretizedAges', by='GRes_1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### D'autres indicateurs...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfmergedOnlyVictory = dfmerged[dfmerged['GRes_1']==1.0]\ndfmergedOnlyDefeat = dfmerged[dfmerged['GRes_1']==0.0]\n\nprint(dfmergedOnlyVictory.Age_CUR_1.mean())\nprint(dfmergedOnlyDefeat.Age_CUR_1.mean())\n\nprint(dfmergedOnlyVictory.Age_CUR_1.median())\nprint(dfmergedOnlyDefeat.Age_CUR_1.median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Globalement l'analyse de ces deux variables ne nous donne pas beaucoup d'informations, les boîtes à moustache sont très semblables et les moyennes et médiannes sont quasiment égales. On ne peut rien déduire de cette analyse à part qu'il y a autant de défaites que de victoires quelque soit l'âge.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.graphics.mosaicplot import mosaic\n\nprint(dfmerged.groupby('isOlder').GRes_1.value_counts())\nmosaic(dfmerged, ['isOlder', 'GRes_1'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"La représentation graphique de l'importance de l'âge sur la victoire nous révèle une indépendance parfaite entre l'age et la victoire. En effet, aucune corrélation entre ces 2 variables n'est décelable.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Premier bilan\nAprès l'analyse descriptive, nous nous rendons compte que les données ne collent pas vraiment avec les questions que l'on se pose. En effet, il ne semble pas qu'il y ai de relation directe entre par exemple l'âge et la victoire ou encore entre la fréquence de jeu et la victoire. Nous allons donc étendre notre recherche pour dégager les tendances les plus remarquables. Nous allons tout de même répondre à la question sur l'âge optimal cependant nous abondonnons nos deux dernières questions.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5. Clustering et Réduction de dimension\n### Clustering\nNotre objectif est d'identifier des groupes d'observation ayant des caractéristiques similaires. Pour cela, on souhaite que les individus d'un même groupe se ressemblent le plus possible et qu'à l'inverse, les indidividus dans des groupes différents se démarquent le plus possible.\nAinsi cela nous permet d'identifier des structures sous-jacentes dans les données, de résumer des comportements, d'affecter de nouveaux individus à des catégories.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Préparation des données - Méthode des centres mobiles\n\nPour appliquer l'algorithme des centres mobiles permettant de classifier nos individus, nous allons changer nos variables. Nous prendrons seulement des variables numériques que nous normaliserons par la suite. Nous fusionnons nos variables concernant les deux joueurs d'un match pour classifier les joueurs en particulier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfCluster1 = df[['Aces_A_1', 'Aces_L5_1', 'BreakPointsConvertedPCT_A_1', 'BreakPointsConvertedPCT_L5_1', 'BreakPointsTotal_A_1', \n                'BreakPointsTotal_L5_1', 'ReceivingPointsWonPCT_A_1', 'ReceivingPointsWonPCT_L5_1', 'Serve1stPCT_A_1', 'Serve1stWonPCT_A_1', \n                'Serve2ndWonPCT_L5_1', 'Serve2ndWonPCT_A_1', 'Age_CUR_1', 'TotalPointsWon_A_1', 'TotalPointsWon_L5_1']]\n\ndfCluster2 = df[['Aces_A_2', 'Aces_L5_2', 'BreakPointsConvertedPCT_A_2', 'BreakPointsConvertedPCT_L5_2', 'BreakPointsTotal_A_2', \n                'BreakPointsTotal_L5_2', 'ReceivingPointsWonPCT_A_2', 'ReceivingPointsWonPCT_L5_2', 'Serve1stPCT_A_2', 'Serve1stWonPCT_A_2',\n                'Serve2ndWonPCT_L5_2', 'Serve2ndWonPCT_A_2', 'Age_CUR_2', 'TotalPointsWon_A_2', 'TotalPointsWon_L5_2']]\n\ndfCluster2 = dfCluster2.rename(columns={\"Aces_A_2\": \"Aces_A_1\", \"Aces_L5_2\": \"Aces_L5_1\", \"BreakPointsConvertedPCT_A_2\": \"BreakPointsConvertedPCT_A_1\",\n                                        \"BreakPointsConvertedPCT_L5_2\": \"BreakPointsConvertedPCT_L5_1\", \"BreakPointsTotal_A_2\": \"BreakPointsTotal_A_1\",\n                                        \"BreakPointsTotal_L5_2\": \"BreakPointsTotal_L5_1\", \"ReceivingPointsWonPCT_A_2\": \"ReceivingPointsWonPCT_A_1\",\n                                        \"ReceivingPointsWonPCT_L5_2\": \"ReceivingPointsWonPCT_L5_1\", \"Serve1stPCT_A_2\": \"Serve1stPCT_A_1\", \"Serve1stWonPCT_A_2\": \"Serve1stWonPCT_A_1\",\n                                        \"Serve2ndWonPCT_L5_2\": \"Serve2ndWonPCT_L5_1\", \"Serve2ndWonPCT_A_2\": \"Serve2ndWonPCT_A_1\", \"Age_CUR_2\": \"Age_CUR_1\", \n                                        \"TotalPointsWon_A_2\": \"TotalPointsWon_A_1\", \"TotalPointsWon_L5_2\": \"TotalPointsWon_L5_1\"})\n\ndfCluster = dfCluster1.append(dfCluster2)\n\ndfCluster = dfCluster[dfCluster['Aces_A_1'].notna()]\ndfCluster = dfCluster[dfCluster['Aces_L5_1'].notna()]\ndfCluster = dfCluster[dfCluster['BreakPointsConvertedPCT_A_1'].notna()]\ndfCluster = dfCluster[dfCluster['BreakPointsConvertedPCT_L5_1'].notna()]\ndfCluster = dfCluster[dfCluster['BreakPointsTotal_A_1'].notna()]\ndfCluster = dfCluster[dfCluster['BreakPointsTotal_L5_1'].notna()]\ndfCluster = dfCluster[dfCluster['ReceivingPointsWonPCT_A_1'].notna()]\ndfCluster = dfCluster[dfCluster['ReceivingPointsWonPCT_L5_1'].notna()]\ndfCluster = dfCluster[dfCluster['Serve1stPCT_A_1'].notna()]\ndfCluster = dfCluster[dfCluster['Serve1stWonPCT_A_1'].notna()]\ndfCluster = dfCluster[dfCluster['Serve2ndWonPCT_L5_1'].notna()]\ndfCluster = dfCluster[dfCluster['Serve2ndWonPCT_A_1'].notna()]\ndfCluster = dfCluster[dfCluster['Age_CUR_1'].notna()]\ndfCluster = dfCluster[dfCluster['TotalPointsWon_A_1'].notna()]\ndfCluster = dfCluster[dfCluster['TotalPointsWon_L5_1'].notna()]\n\n# informations sur nos données\nnRow, nCol = dfCluster.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Obtention du nombre de cluster\n\nL'un des inconvénients principal de l'algorithme des centres mobiles est qu'il faut fixer le nombre de cluster **avant** l'exécution de l'algorithme. Nous allons donc calculer la variance expliquée en fonction du nombre de clusters (heuristique du coude) pour déterminer le nombre de cluster qui donne le plus d'informations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.vq import kmeans\nfrom scipy.spatial.distance import cdist,pdist\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA\nfrom matplotlib import cm\n\ndfKMeans = dfCluster.sample(n=500, random_state=24, replace=False)\n\n# normalisation des données avant K Means\ndfKMeans_scaled = normalize(dfKMeans)\ndfKMeans_scaled = pd.DataFrame(dfKMeans_scaled, columns=dfKMeans.columns)\n\n# réduction à 2 dimensions (PCA)\npca = PCA(n_components=2).fit(dfKMeans_scaled)\nX = pca.transform(dfKMeans_scaled)\n\n# clustering avec K Means\nnb_clusters = 20\nnums_clusters = range(1,nb_clusters+1)\n\nkmeans_out = [kmeans(X,k) for k in nums_clusters]\ncentroids = [cent for (cent,var) in kmeans_out]\ndist_kmeans = [cdist(X, cent, 'euclidean') for cent in centroids]\ncIdx = [np.argmin(D,axis=1) for D in dist_kmeans]\ndist = [np.min(D,axis=1) for D in dist_kmeans]\n\n# calcul des sommes de carrés intra et inter classes\nwithin_sum_squares = [sum(d**2) for d in dist]\nsum_squares = sum(pdist(X)**2)/X.shape[0]\nbetween_sum_squares = sum_squares - within_sum_squares\n\n# représentation graphique\nkIdx = 8     \nclr = cm.rainbow( np.linspace(0,1,10) ).tolist()\nmrk = 'os^p<dvh8>+x.'\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(nums_clusters, between_sum_squares/sum_squares*100, 'b*-')\nax.plot(nums_clusters[kIdx], between_sum_squares[kIdx]/sum_squares*100, marker='o', markersize=12, \n    markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\nax.set_ylim((0,100))\nplt.grid(True)\nplt.xlabel('Nombre de clusters')\nplt.ylabel('Pourcentage de variance expliquée (%)')\nplt.title('Méthode du coude - KMeans clustering')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Maintenant que nous connaissons le nombre de clusters adéquat nous pouvons lancer notre algorithme.\n\nPour pouvoir lancer l'algorithme, nous utilisons une méthode permettant de posséder un meilleur ensemble de centres initiaux pour pouvoir exécuter K Means sur l'ensemble des données.\nNous allons donc récupérer un échantillon du jeu de données (20 individus), puis nous allons éxécuter des petites itérations du K Means dessus.\nNous répéterons ces itérations avec un certain nombre de centroïdes initialisés de manière aléatoire et nous suivrons l'amélioration de la mesure (somme des carrés à l'intérieur d'un groupe) pour déterminer la qualité de l'appartenance à un groupe.\n\n\nLes derniers centroïdes associés au processus d'itération d'initialisation aléatoire des centroïdes qui offrent la plus faible inertie sont l'ensemble de centroïdes que nous allons reporter à notre processus complet de regroupement des ensembles de données.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfKMeans = dfCluster\n\ndata_sample = dfKMeans.sample(n=100, random_state=24, replace=False)\n\n# normalisation des données avant K Means\ndata_scaled = normalize(data_sample)\ndata_scaled = pd.DataFrame(data_scaled, columns=data_sample.columns)\n\n# initialisation de variables\nNUM_CLUSTERS = 9\nNUM_ITER = 3\nNUM_ATTEMPTS = 5\n\n# exécution du premier K Means\nkm = KMeans(n_clusters=NUM_CLUSTERS, init='random', max_iter=1, n_init=1)\nkm.fit(data_scaled)\n\nprint('Inertie sur l\\'échantillon:', km.inertia_)\n\nfinal_cents = []\nfinal_inert = []\n\n# on boucle sur 5 tentatives pour trouver les meilleurs centroïdes\nfor sample in range(NUM_ATTEMPTS):\n    km = KMeans(n_clusters= NUM_CLUSTERS, init='random', max_iter=1, n_init=1) \n    km.fit(data_scaled)\n    inertia_start = km.inertia_\n    inertia_end = 0\n    cents = km.cluster_centers_\n        \n    for iter in range(NUM_ITER):\n        km = KMeans(n_clusters = NUM_CLUSTERS, init=cents, max_iter=1, n_init=1)\n        km.fit(data_scaled)\n        inertia_end = km.inertia_\n        cents = km.cluster_centers_\n\n    final_cents.append(cents)\n    final_inert.append(inertia_end)\n    print('Difference entre inertie finale et initiale: ', inertia_start-inertia_end)\n\n# on récupère les meilleurs centroïdes\nbest_cents = final_cents[final_inert.index(min(final_inert))]\nprint(\"Meilleurs centroïdes trouvés:\", best_cents)\n    \n# on exécute K Means sur l'ensemble des données avec les meilleurs centroïdes à l'initialisation\ndata_fullScaled = normalize(dfKMeans)\ndata_fullScaled = pd.DataFrame(data_fullScaled, columns=dfKMeans.columns)\nfullKMeans = KMeans(n_clusters=NUM_CLUSTERS, init=best_cents, max_iter=100, n_init=1, verbose=1)\nfullKMeans.fit(data_fullScaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Réduction de dimension - Analyse en composantes principales\n\nPour visualiser notre clustering, nous allons réaliser une analyse en composantes principale pour réduire le nombre de dimension à 2 dans un premier temps puis à 3 dans un second temps.\n\n### Visualisation 2D","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca2D = PCA(2)\npca2D.fit(data_fullScaled)\nprojected = pca2D.fit_transform(data_fullScaled)\nplt.scatter(projected[:, 0], projected[:, 1],\n            c=fullKMeans.labels_, edgecolor='none', alpha=0.5,\n            cmap=plt.cm.get_cmap('rainbow', 10))\nplt.xlabel('F1')\nplt.ylabel('F2')\nplt.title('Visualisation 2D des 9 clusters')\nplt.colorbar();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualisation 3D\nMalheureusement, la visualisation 3D ne passe pas à l'échelle. Nous avons donc choisi de réutiliser l'échantillon qui a servi à entrainer le K Means précédemment.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(3)\npca.fit(data_scaled)\npca_data = pd.DataFrame(pca.transform(data_scaled))\n\nfrom matplotlib import colors as mcolors \nimport math \n\ncolors = list(zip(*sorted(( \n                    tuple(mcolors.rgb_to_hsv( \n                          mcolors.to_rgba(color)[:3])), name) \n                     for name, color in dict( \n                            mcolors.BASE_COLORS, **mcolors.CSS4_COLORS \n                                                      ).items())))[1] \n    \nskips = math.floor(len(colors[5 : -5])/NUM_CLUSTERS) \ncluster_colors = colors[5 : -5 : skips] \n\nfrom mpl_toolkits.mplot3d import Axes3D \n   \nfig = plt.figure(figsize=(15, 13)) \n\nax = fig.add_subplot(111, projection = '3d') \nax.scatter(pca_data[0], pca_data[1], pca_data[2],  \n           c = list(map(lambda label : cluster_colors[label], \n                                            km.labels_))) \n   \nstr_labels = list(map(lambda label:'% s' % label, km.labels_)) \n   \nlist(map(lambda data1, data2, data3, str_label: \n        ax.text(data1, data2, data3, s = str_label, size = 12.5, \n        zorder = 20, color = 'k'), pca_data[0], pca_data[1], \n        pca_data[2], str_labels)) \n   \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Autre méthode - Classification hiérarchique avec technique d'agglomération","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Nous aurions pu également utiliser une autre technique de clustering : la classification hiérarchique avec agglomération. Cette technique vise à utiliser un dendogramme puis d'agglomérer géographiquement (avec distance auclidienne) les clusters par la suite.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_sample_agg = dfCluster.sample(n=100, random_state=24, replace=False)\n\ndata_sample_agg = data_sample_agg[data_sample_agg['BreakPointsConvertedPCT_A_1'] != 0]\ndata_sample_agg = data_sample_agg[data_sample_agg['Serve2ndWonPCT_A_1'] != 0]\n\nimport scipy.cluster.hierarchy as shc\nplt.figure(figsize=(10, 7))  \nplt.title(\"Dendrograms\")  \ndend = shc.dendrogram(shc.linkage(data_sample_agg, method='ward'))\n\nfrom sklearn.cluster import AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters=9, affinity='euclidean', linkage='ward')  \ncluster.fit_predict(data_sample_agg)\n\nplt.figure(figsize=(9, 7))  \nplt.scatter(data_sample_agg['BreakPointsConvertedPCT_A_1'], data_sample_agg['Serve2ndWonPCT_A_1'], c=cluster.labels_) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analyse globale du clustering\n### V-Test et interprétation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Pour interpreter notre clustering, nous réalisons désormais une mesure du V-Test qui nous permet de comparer la moyenne d'un cluster sur une variable à la moyenne de la population totale.\n\n(PS : Nous n'avons pas eu le temps de réaliser cette partie avec du code propre)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"globalVariance = data_sample.var()\nglobalMean = data_sample.mean()\nnCluster = len(data_sample.index)\nrows = ['Aces_A_1','Aces_L5_1','BreakPointsConvertedPCT_A_1','BreakPointsConvertedPCT_L5_1','BreakPointsTotal_A_1','BreakPointsTotal_L5_1',\n        'ReceivingPointsWonPCT_A_1','ReceivingPointsWonPCT_L5_1','Serve1stPCT_A_1','Serve1stWonPCT_A_1','Serve2ndWonPCT_L5_1','Serve2ndWonPCT_A_1','Age_CUR_1',\n        'TotalPointsWon_A_1','TotalPointsWon_L5_1']\n\n# fonction permettant de calculer la valeur v-test\ndef vTest(data):\n    mean = data[0]\n    var = data[1]\n    globalMean = data[2]\n    n = float(data[3])\n    nCluster = float(data[4])\n\n    result = (mean-globalMean) / (np.sqrt( ((nCluster - n)/(nCluster - 1)) * (var/n) ))\n    return result\n\n# cluster 1\ngroup1_data = data_sample[km.labels_==0]\ngroup1_mean = group1_data.mean()\ngroup1_mean = group1_mean.to_frame()\ngroup1 = group1_mean\nsymbols = ['mean']\ngroup1.columns = symbols\n\ngroup1['var'] = globalVariance\ngroup1['globalMean'] = globalMean\ngroup1['n'] = len(group1_data.index)\ngroup1['nCluster'] = len(data_sample.index)\ntab = pd.Series(map(vTest, group1[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\ngroup1['vtest_values'] = pd.Series(map(vTest, group1[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\nidx=0\nfor val in tab:\n    index = rows[idx]\n    group1.loc[index,'vtest_values'] = val\n    idx += 1\ngroup1 = group1.drop(['mean', 'var', 'globalMean', 'n', 'nCluster'], axis=1)\n\n# cluster 2\ngroup2_data = data_sample[km.labels_==1]\ngroup2_mean = group2_data.mean()\ngroup2_mean = group2_mean.to_frame()\ngroup2 = group2_mean\nsymbols = ['mean']\ngroup2.columns = symbols\n\ngroup2['var'] = globalVariance\ngroup2['globalMean'] = globalMean\ngroup2['n'] = len(group2_data.index)\ngroup2['nCluster'] = len(data_sample.index)\n\ntab = pd.Series(map(vTest, group2[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\ngroup2['vtest_values'] = pd.Series(map(vTest, group2[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\nidx=0\nfor val in tab:\n    index = rows[idx]\n    group2.loc[index,'vtest_values'] = val\n    idx += 1\ngroup2 = group2.drop(['mean', 'var', 'globalMean', 'n', 'nCluster'], axis=1)\n\n# cluster 3\ngroup3_data = data_sample[km.labels_==2]\ngroup3_mean = group3_data.mean()\ngroup3_mean = group3_mean.to_frame()\ngroup3 = group3_mean\nsymbols = ['mean']\ngroup3.columns = symbols\n\ngroup3['var'] = globalVariance\ngroup3['globalMean'] = globalMean\ngroup3['n'] = len(group3_data.index)\ngroup3['nCluster'] = len(data_sample.index)\n\ntab = pd.Series(map(vTest, group3[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\ngroup3['vtest_values'] = pd.Series(map(vTest, group3[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\nidx=0\nfor val in tab:\n    index = rows[idx]\n    group3.loc[index,'vtest_values'] = val\n    idx += 1\ngroup3 = group3.drop(['mean', 'var', 'globalMean', 'n', 'nCluster'], axis=1)\n\n# cluster 4\ngroup4_data = data_sample[km.labels_==3]\ngroup4_mean = group4_data.mean()\ngroup4_mean = group4_mean.to_frame()\ngroup4 = group4_mean\nsymbols = ['mean']\ngroup4.columns = symbols\n\ngroup4['var'] = globalVariance\ngroup4['globalMean'] = globalMean\ngroup4['n'] = len(group4_data.index)\ngroup4['nCluster'] = len(data_sample.index)\n\ntab = pd.Series(map(vTest, group4[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\ngroup4['vtest_values'] = pd.Series(map(vTest, group4[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\nidx=0\nfor val in tab:\n    index = rows[idx]\n    group4.loc[index,'vtest_values'] = val\n    idx += 1\ngroup4 = group4.drop(['mean', 'var', 'globalMean', 'n', 'nCluster'], axis=1)\n\n# cluster 5\ngroup5_data = data_sample[km.labels_==4]\ngroup5_mean = group5_data.mean()\ngroup5_mean = group5_mean.to_frame()\ngroup5 = group5_mean\nsymbols = ['mean']\ngroup5.columns = symbols\n\ngroup5['var'] = globalVariance\ngroup5['globalMean'] = globalMean\ngroup5['n'] = len(group5_data.index)\ngroup5['nCluster'] = len(data_sample.index)\n\ntab = pd.Series(map(vTest, group5[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\ngroup5['vtest_values'] = pd.Series(map(vTest, group5[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\nidx=0\nfor val in tab:\n    index = rows[idx]\n    group5.loc[index,'vtest_values'] = val\n    idx += 1\ngroup5 = group5.drop(['mean', 'var', 'globalMean', 'n', 'nCluster'], axis=1)\n\n# cluster 6\ngroup6_data = data_sample[km.labels_==5]\ngroup6_mean = group6_data.mean()\ngroup6_mean = group6_mean.to_frame()\ngroup6 = group6_mean\nsymbols = ['mean']\ngroup6.columns = symbols\n\ngroup6['var'] = globalVariance\ngroup6['globalMean'] = globalMean\ngroup6['n'] = len(group6_data.index)\ngroup6['nCluster'] = len(data_sample.index)\n\ntab = pd.Series(map(vTest, group6[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\ngroup6['vtest_values'] = pd.Series(map(vTest, group6[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\nidx=0\nfor val in tab:\n    index = rows[idx]\n    group6.loc[index,'vtest_values'] = val\n    idx += 1\ngroup6 = group6.drop(['mean', 'var', 'globalMean', 'n', 'nCluster'], axis=1)\n\n# cluster 7\ngroup7_data = data_sample[km.labels_==6]\ngroup7_mean = group7_data.mean()\ngroup7_mean = group7_mean.to_frame()\ngroup7 = group7_mean\nsymbols = ['mean']\ngroup7.columns = symbols\n\ngroup7['var'] = globalVariance\ngroup7['globalMean'] = globalMean\ngroup7['n'] = len(group7_data.index)\ngroup7['nCluster'] = len(data_sample.index)\n\ntab = pd.Series(map(vTest, group7[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\ngroup7['vtest_values'] = pd.Series(map(vTest, group7[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\nidx=0\nfor val in tab:\n    index = rows[idx]\n    group7.loc[index,'vtest_values'] = val\n    idx += 1\ngroup7 = group7.drop(['mean', 'var', 'globalMean', 'n', 'nCluster'], axis=1)\n\n# cluster 8\ngroup8_data = data_sample[km.labels_==7]\ngroup8_mean = group8_data.mean()\ngroup8_mean = group8_mean.to_frame()\ngroup8 = group8_mean\nsymbols = ['mean']\ngroup8.columns = symbols\n\ngroup8['var'] = globalVariance\ngroup8['globalMean'] = globalMean\ngroup8['n'] = len(group8_data.index)\ngroup8['nCluster'] = len(data_sample.index)\n\ntab = pd.Series(map(vTest, group8[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\ngroup8['vtest_values'] = pd.Series(map(vTest, group8[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\nidx=0\nfor val in tab:\n    index = rows[idx]\n    group8.loc[index,'vtest_values'] = val\n    idx += 1\ngroup8 = group8.drop(['mean', 'var', 'globalMean', 'n', 'nCluster'], axis=1)\n\n# cluster 9\ngroup9_data = data_sample[km.labels_==8]\ngroup9_mean = group9_data.mean()\ngroup9_mean = group9_mean.to_frame()\ngroup9 = group9_mean\nsymbols = ['mean']\ngroup9.columns = symbols\n\ngroup9['var'] = globalVariance\ngroup9['globalMean'] = globalMean\ngroup9['n'] = len(group9_data.index)\ngroup9['nCluster'] = len(data_sample.index)\n\ntab = pd.Series(map(vTest, group9[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\ngroup9['vtest_values'] = pd.Series(map(vTest, group9[['mean', 'var', 'globalMean', 'n', 'nCluster']].to_numpy()))\nidx=0\nfor val in tab:\n    index = rows[idx]\n    group9.loc[index,'vtest_values'] = val\n    idx += 1\ngroup9 = group9.drop(['mean', 'var', 'globalMean', 'n', 'nCluster'], axis=1)\n\n\ncategories = ['Aces_A_1','Aces_L5_1','BreakPointsConvertedPCT_A_1','BreakPointsConvertedPCT_L5_1','BreakPointsTotal_A_1','BreakPointsTotal_L5_1',\n              'ReceivingPointsWonPCT_A_1','ReceivingPointsWonPCT_L5_1','Serve1stPCT_A_1','Serve1stWonPCT_A_1','Serve2ndWonPCT_L5_1','Serve2ndWonPCT_A_1','Age_CUR_1',\n              'TotalPointsWon_A_1','TotalPointsWon_L5_1']\n\n# construction du radar chart\n\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatterpolar(\n      r=group1.vtest_values.to_numpy(),\n      theta=categories,\n      fill='toself',\n      name='Cluster 1'\n))\nfig.add_trace(go.Scatterpolar(\n      r=group2.vtest_values.to_numpy(),\n      theta=categories,\n      fill='toself',\n      name='Cluster 2'\n))\nfig.add_trace(go.Scatterpolar(\n      r=group3.vtest_values.to_numpy(),\n      theta=categories,\n      fill='toself',\n      name='Cluster 3'\n))\nfig.add_trace(go.Scatterpolar(\n      r=group4.vtest_values.to_numpy(),\n      theta=categories,\n      fill='toself',\n      name='Cluster 4'\n))\nfig.add_trace(go.Scatterpolar(\n      r=group5.vtest_values.to_numpy(),\n      theta=categories,\n      fill='toself',\n      name='Cluster 5'\n))\nfig.add_trace(go.Scatterpolar(\n      r=group6.vtest_values.to_numpy(),\n      theta=categories,\n      fill='toself',\n      name='Cluster 6'\n))\nfig.add_trace(go.Scatterpolar(\n      r=group7.vtest_values.to_numpy(),\n      theta=categories,\n      fill='toself',\n      name='Cluster 7'\n))\nfig.add_trace(go.Scatterpolar(\n      r=group8.vtest_values.to_numpy(),\n      theta=categories,\n      fill='toself',\n      name='Cluster 8'\n))\nfig.add_trace(go.Scatterpolar(\n      r=group9.vtest_values.to_numpy(),\n      theta=categories,\n      fill='toself',\n      name='Cluster 9'\n))\n\nfig.update_layout(\n  polar=dict(\n    radialaxis=dict(\n      visible=True,\n      range=[-4, 7]\n    )),\n  showlegend=False\n)\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"(PS : Nous n'avons pas eu le temps de fixer les numéros et couleurs du cluster entre chaque exécution)\n\n\nLa visualisation des différentes valeurs du V-Test en fonction des clusters nous permet de constater certaines tendances. En effet, on peut voir qu'un cluster se démarque sur les variables concernant les aces, c'est le groupe des joueurs qui font le plus d'aces et ces joueurs sont plus jeunes que la moyenne. Les joueurs les plus performants sont les joueurs du cluster qui se démarque sur le nombre de points gagnés, ce sont ceux qui en moyenne gagnent plus de points que les autres. Cependant ils convertissent moins les balles de break que la moyenne. Enfin on remarque un autre cluster qui obtient des résultats supérieurs à la moyenne sur les points importants (balles de break, balles reçues), ce sont des joueurs plus âgés en moyenne. Cela nous donne beaucoup d'informations concernant notre question principale.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 6. Arbre de décision\n\n### Arbre de classification\n\nNous nous interessons désormais à la variable 'G_Res'. Cette variable catégorique indique si le match a été gagné ou non par le joueur.\nNous proposons de l'analyser à l'aide d'un arbre de décision. Nous souhaitons dresser l'arbre de décision en se basant sur l'age du joueur, le nombre de jours écoulés depuis son dernier match, s'il joue le jour de son anniversaire et enfin le type de surface.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, export_graphviz # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn.preprocessing import OneHotEncoder #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n\n# suppression des NaN, non supporté par sklearn\ndfmerged = dfmerged[dfmerged['discretizedAges'].notna()]\ndfmerged = dfmerged[dfmerged['DaysFromLast_CUR_1'].notna()]\ndfmerged = dfmerged[dfmerged['GRes_1'].notna()]\ndfmerged = dfmerged[dfmerged['Surface'].notna()]\ndfmerged = dfmerged[dfmerged['TotalPointsWon_1'].notna()]\n\ndfmerged.Surface = pd.Series(map(int, dfmerged[['Surface']].to_numpy()))\n\n# normalisation de certaines valeurs\ndef func(arg):\n    return (arg == 1.0)\ndfTree = dfmerged.assign(IsBirthDay=pd.Series(map(func, dfmerged[['IsBirthDay_CUR_1']].to_numpy())))\ndfTree = dfTree.assign(DaysFromLast=pd.Series(map(int, dfmerged[['DaysFromLast_CUR_1']].to_numpy())))\ndfTree = dfTree.assign(GRes=pd.Series(map(func, dfmerged.GRes_1.to_numpy())))\ndfTree = dfTree.assign(TotalPointsWon=pd.Series(map(int, dfmerged.TotalPointsWon_1.to_numpy())))\n\n# categorisation de la variable surface\nnew_cols = pd.get_dummies(dfmerged.Surface.to_numpy(), prefix=\"Surface\")\ndfTree['Surface_1'] = new_cols[['Surface_1']]\ndfTree['Surface_2'] = new_cols[['Surface_2']]\ndfTree['Surface_3'] = new_cols[['Surface_3']]\ndfTree['Surface_4'] = new_cols[['Surface_4']]\ndfTree['Surface_5'] = new_cols[['Surface_5']]\n\n# Création des jeux de données\nfeature_cols = ['discretizedAges', 'DaysFromLast', 'IsBirthDay', 'TotalPointsWon']\nX = dfTree[feature_cols] # Features\ny = dfTree.GRes # Target variable\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# Génération de l'arbre\nclf = DecisionTreeClassifier(max_depth = 3, criterion=\"gini\", splitter=\"best\")\nclf = clf.fit(X_train,y_train) # entrainement\ny_pred = clf.predict(X_test) # Test de prédiction\n\nprint(\"Accuracy : \", metrics.accuracy_score(y_test, y_pred))\n\n# visualisation\nexport_graphviz(clf,\n                 out_file='tree.dot',\n                 max_depth = 3,\n                 rounded = True,\n                 impurity=False,\n                 class_names=['loose', 'win'],\n                 feature_names = X_test.columns.values,\n                 filled=True)\n\n# Convert to png\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in python\nplt.figure(figsize = (14, 16))\nplt.imshow(plt.imread('tree.png'))\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"L'algorithme a choisi DaysFromLast comme la première règle de décision. Cela nous assure que c'est le parametre (à lui seul) qui nous offre le plus d'informations sur la victoire d'un joueur. Le deuxieme paramètre qui attire notre attention est celui faisant référence aux nombre de points gagnés durant la partie.\n\nLa génération de l'arbre permet d'obtenir des précisions au alentours de 95%. Performances permises grace aux choix des meilleurs noeuds permis par l'attribut \"best\" comme critere de qualité des divisions. L'utilisation du splitter \"random\" propose des arbres aux performances réduites. Nous observons des valeurs de précisions de l'arbre au alentours de 57%. L'ajout de profondeur à l'arbre ne permet pas d'améliorer les résultats de maniere significative. Les noeuds situtés en profondeur 1 de l'arbre propose déja une répartion tres asymétrique des valeurs. La classification offerte par le premier branchement permet a lui seul de très bon résultats.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Arbre de régression\nIntéressons nous désormais à la variable âge. Dans quelle mesure pouvons-nous déterminer l'âge d'un joueur ? Pour répondre à cette question, nous appliquerons un algorithme d'arbre de régression. Nous avons choisi de déterminer l'âge du joueue en fonction de son résultat (victoire/défaite), de son nombre de jours de repos et des points marqués : sur l'ensemble du match, lors de ses premiers services et lors de ses seconds services.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor, export_graphviz # Import Decision Tree Regressor\n\n# suppression des NaN, non supporté par sklearn\ndfmerged = dfmerged[dfmerged['DaysFromLast_CUR_1'].notna()]\ndfmerged = dfmerged[dfmerged['Serve1stWon_1'].notna()]\ndfmerged = dfmerged[dfmerged['Serve2ndWon_1'].notna()]\ndfmerged = dfmerged[dfmerged['TotalPointsWon_1'].notna()]\n\n# normalisation des valeurs\ndfTree = dfmerged.assign(DaysFromLast=pd.Series(map(int, dfmerged[['DaysFromLast_CUR_1']].to_numpy())))\ndfTree = dfTree.assign(Serve1stWon=pd.Series(map(int, dfmerged[['Serve1stWon_1']].to_numpy())))\ndfTree = dfTree.assign(Serve2ndWon=pd.Series(map(int, dfmerged[['Serve2ndWon_1']].to_numpy())))\ndfTree = dfTree.assign(TotalPointsWon=pd.Series(map(int, dfmerged[['TotalPointsWon_1']].to_numpy())))\ndef func(arg):\n    return (arg == 1.0)\ndfTree = dfTree.assign(GRes=pd.Series(map(func, dfmerged.GRes_1.to_numpy())))\n\n# categorisation de la variable surface\nnew_cols = pd.get_dummies(dfmerged.Surface.to_numpy(), prefix=\"Surface\")\ndfTree['Surface_1'] = new_cols[['Surface_1']]\ndfTree['Surface_2'] = new_cols[['Surface_2']]\ndfTree['Surface_3'] = new_cols[['Surface_3']]\ndfTree['Surface_4'] = new_cols[['Surface_4']]\ndfTree['Surface_5'] = new_cols[['Surface_5']]\n\n# création des jeux de données\nfeature_cols = ['GRes_1', 'DaysFromLast_CUR_1', 'Serve1stWon_1', 'Serve2ndWon_1', 'TotalPointsWon_1', 'Surface_1', 'Surface_2', 'Surface_3', 'Surface_4', 'Surface_5']\nX = dfTree[feature_cols] # Features\ny = dfTree.Age_CUR_1 # Target variable\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\nbest_depth = 0\nbest_depth_score = 0\nfor i in range(1,10):\n    clf = DecisionTreeRegressor(max_depth=i)\n    clf = clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    print(\"score for max depth \", i, ' : ', score)\n    if score > best_depth_score:\n        best_depth = i\n        best_depth_score = score\n\nclf = DecisionTreeRegressor(max_depth=best_depth)\nclf = clf.fit(X_train, y_train)\n\n# visualisation\nexport_graphviz(clf,\n                 out_file='tree.dot',\n                 max_depth = 3,\n                 rounded = True,\n                 impurity=False,\n                 feature_names = X_test.columns.values,\n                 filled=True)\n\n# conversion en png\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'regressionTree.png', '-Gdpi=600'])\n\n# affichage\nplt.figure(figsize = (22, 20))\nplt.imshow(plt.imread('regressionTree.png'))\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Le coefficient de détermination R² de l'arbre généré ocile entre 0.008 et 0.024 (en fonction de la profondeur acceptée). Cet indicateur nous montre que l'arbre généré correspond mal aux données. Nous remarquons que le meilleur coefficient est obtenue avec une profondeur de 8. Il semblerait que passé ce cap, l'algorithme réalise du sur-apprentissage.\n\nNous pouvons donc en conclure qu'il est très difficile de déterminer l'âge d'un joueur a partir de ses performances et jours de repos. Toutefois, le fait que la première règle décisionnelle considère le nombre de point marqué lors du premier service nous montre qu'il sagit du meilleur indicateur pour déterminer l'âge d'un joueur. Cette information est malgré à considérer avec beaucoup de précaution compte tenu des résultats. De plus, la surface n'apparait dans aucune règle de décision. Nous pouvons donc considéré que les surfaces pratiquées par les joueurs ne dépendent pas de leur âge.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 7. Outlier detection\nDans un premier temps, nous décidons d'appliquer sur un jeu de données réduit un \"DBSCAN clustering\". Cette méthode permet de déterminer les outliers, c'est à dire les valeurs abérantes. Pour ce faire, la méthode s'appuie sur l'éloignement des points les uns envers les autres. Nous appliquons ici une distance euclidienne.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pca2D2 = PCA(2)\npca2D2.fit(data_scaled) \nX = pca2D2.fit_transform(data_scaled)\n\nfrom sklearn.cluster import DBSCAN\noutlier_detection = DBSCAN(\n    eps = 0.03,\n    metric=\"euclidean\",\n    min_samples = 3,\n    n_jobs = -1)\nclusters = outlier_detection.fit_predict(X)\n\n# les outliers sont les données ne faisant pas partit du premier cluster\nnb_outliers = clusters.size - clusters.tolist().count(0)\noutliers_fraction = nb_outliers / clusters.size\n\nprint(clusters)\n# visualisation des clusters\nplt.scatter(X[:, 0], X[:, 1],\n           c=clusters,\n           cmap=cm.get_cmap('Accent'))\nplt.xlabel('F1')\nplt.ylabel('F2')\nplt.title('Visualisation des outliers')\nplt.colorbar();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous avons pu, à l'aide de cette méthode déderminer le nombre de \"outlier\". Cette information peut désormais être utilisée afin de déterminer des règles de décisions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\n\nfrom sklearn import svm\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.ensemble import IsolationForest\n\n# Parametres\nn_samples = clusters.size\nclusters_separation = [0, 1, 2]\nrng = np.random.RandomState(42)\n\n# define two outlier detection tools to be compared\nclassifiers = {\n    \"One-Class SVM\": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,\n                                     kernel=\"rbf\", gamma=0.1),\n    \"Robust covariance\": EllipticEnvelope(contamination=outliers_fraction),\n    \"Isolation Forest\": IsolationForest(max_samples=n_samples,\n                                        contamination=outliers_fraction,\n                                        random_state=rng)}\n\n# Compare given classifiers under given settings\nxx, yy = np.meshgrid(np.linspace(-0.25, 0.25, 500), np.linspace(-0.25, 0.25, 500))\nn_inliers = int((1. - outliers_fraction) * n_samples)\nn_outliers = int(outliers_fraction * n_samples)\nground_truth = np.ones(n_samples, dtype=int)\nground_truth[-n_outliers:] = -1\n\n# Fit the model\nplt.figure(figsize=(10.8, 3.6))\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    # fit the data and tag outliers\n    clf.fit(X)\n    scores_pred = clf.decision_function(X)\n    threshold = stats.scoreatpercentile(scores_pred,\n                                        100 * outliers_fraction)\n    y_pred = clf.predict(X)\n    n_errors = (y_pred != ground_truth).sum()\n    # plot the levels lines and the points\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    subplot = plt.subplot(1, 3, i + 1)\n    subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n                     cmap=plt.cm.Blues_r)\n    a = subplot.contour(xx, yy, Z, levels=[threshold],\n                        linewidths=2, colors='red')\n    subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n                     colors='orange')\n    b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white')\n    c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black')\n    subplot.axis('tight')\n    subplot.legend(\n        [a.collections[0], b, c],\n        ['learned decision function', 'true inliers', 'true outliers'],\n        prop=matplotlib.font_manager.FontProperties(size=11),\n        loc='lower right')\n    subplot.set_title(\"%d. %s (errors: %d)\" % (i + 1, clf_name, n_errors))\n    subplot.set_xlim((-0.25, 0.25))\n    subplot.set_ylim((-0.25, 0.25))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Les 3 algorithmes nous montrent des résultats proche. Ils comportent chacun entre 10 et 15 erreurs. Malgré tout, les données rendent compliquées cette tache, car, comme l'on peut le voir sur les nuages de points générés, les données sont aglomérées. Il est donc naturellement difficile pour ces algorithmes de repérer les données absurdes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Bilan\n## Réponses à nos questions\n- Quel est l'âge optimal pour gagner au tennis ? \n\nIl semblerait qu'en moyenne les joueurs jeunes gagnent beaucoup plus de points que les joueurs agés. En effet, selon les différentes analyses du projet, les joueurs les moins agés sont ceux qui gagnent le plus de points. Cependant ce sont aussi eux qui convertissent moins de balles de break que la moyenne. De plus, l'analyse exploratoire sur l'âge et les victoire nous donnait des moyennes d'âges **équivalentes** que ce soit pour les victoires ou les défaites ce qu'il faut retenir.\n\nAinsi on peut déduire que les joueurs plus jeunes gagnent plus de points en moyenne **parce qu'ils** ne convertissent pas leurs balles de break. A l'inverse, les joueurs plus âgés sont au rendez-vous sur les points importants et gagnent moins de points en moyenne ce qui ne les empêchent pas de gagner tout de même.\n\nEn conclusion, il semblerait qu'il n'y ai pas d'âge optimal au tennis. On observe plutôt une confrontation entre le physique et l'expérience qui nous donne des résultats globaux équilibrés quelque soit l'âge. Pour optimiser ses chances de gagner il faut donc acquérir le mental et le savoir d'un joueur d'expérience en possédant des capacités phyisiques notables. C'est ainsi que les grands joueurs de l'ATP se distinguent. Certains joueurs en fin de carrière joue sur leur expérience tout en ayant gardé un physique d'exception pour rester au plus haut niveau (Federer par exemple).\n\n- Jouer le jour de son anniversaire modifie-t-il significativement les chances de victoire ?\n- Est-ce que la fréquence de jeu joue un rôle primordial dans les chances de victoire ?\n\nNous avons abandonné ces deux questions au moment de l'analyse exploratoire et nous n'avons pas sû trouver des tendances nous permettant d'y répondre par la suite.\n\n\n## Analyse du travail réalisé\n### Difficultés rencontrées\nGlobalement, nous avons rencontré beaucoup de difficultés pendant ce projet. En effet, le fait de partir de données brutes non sélectionnées pour nous dans le cadre d'un TP ou d'un exercice est quelque chose de compliqué. Nous avons pris beaucoup de temps à choisir les données, les sélectionner et à nous poser des questions dessus. Nous avons eu du mal à identifier des questions pertinentes et nous n'avons finalement pu répondre qu'à l'une d'entre elles. Nous avons également sous-estimé le temps alloué à ce projet, toutefois nous pensons avoir fourni une analyse poussée des données dont nous avions la charge.\n\n### Compétences acquises\nCe projet nous a permis d'utiliser de manière approfondie matplotlib, pandas et scikit learn. Nous pensons désormais pouvoir mettre en oeuvre une analyse statistique complète sur des données brutes même si nous avons eu du mal à donner du sens à nos analyses. Ce projet nous a également permis d'en apprendre davantage sur l'extraction de connaissances dans les données. Ce projet nous a aussi permis de découvrir le travail d'analyse de données dans un contexte plus concret qu'à notre habitude. Nous avons vraiment le sentiment d'avoir acquis de l'expérience que nous pourrons réutiliser à l'avenir. Enfin Kaggle est également une belle découverte.\n\n## Temps de travail\nNous avons passé sur la globalité du projet 25h de travail chacun ce qui donne environ 50h de travail au total. Nous avons toujours travaillé sur les mêmes horaires et en écran partagé c'est pourquoi nous estimons avoir fourni exactement la même quantité de travail.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}