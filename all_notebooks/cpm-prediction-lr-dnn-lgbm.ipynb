{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, PoissonRegressor\nfrom lightgbm.sklearn import LGBMRegressor\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/real-time-advertisers-auction/Dataset.csv', parse_dates=['date'])\n\ndef weird_division(n, d):\n    return n / d if d else 0\n\ndf['target'] = df.apply(lambda x: weird_division(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , axis=1)\n\ndf = df.drop(columns=[\n    'total_revenue', \n    'revenue_share_percent'\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['dayofweek'] = df['date'].dt.dayofweek","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VAL_DATE = '2019-06-20'\nTEST_DATE = '2019-06-22'\n\ntest_mask = df['date'] >= TEST_DATE\ntrain_mask = df['date'] < VAL_DATE\nval_mask = (df['date'] >= VAL_DATE) & (df['date'] < TEST_DATE)\n\n\nX, y = df.drop(columns=['target', 'date']), df['target']\n\nX_train, y_train = X[train_mask], y[train_mask]\nX_val, y_val = X[val_mask], y[val_mask]\nX_test, y_test = X[test_mask], y[test_mask]\n\n\ntest_max = y_test.quantile(0.95)\ntest_mask_filter = (y_test <= test_max) & (y_test >= 0)\nX_test, y_test = X_test[test_mask_filter], y_test[test_mask_filter]\n\ntrain_max = y_train.quantile(0.95)\ntrain_mask_filter = (y_train <= train_max) & (y_train >= 0)\nX_train, y_train = X_train[train_mask_filter], y_train[train_mask_filter]\n\nval_max = y_val.quantile(0.95)\nval_mask_filter = (y_val <= val_max) & (y_val >= 0)\nX_val, y_val = X_val[val_mask_filter], y_val[val_mask_filter]\n\nX_train_val = pd.concat([X_train, X_val])\ny_train_val = pd.concat([y_train, y_val])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(y_train, bins=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. LinearRegression (baseline)"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = [\n    'measurable_impressions', \n    'total_impressions',\n    'viewable_impressions',\n    \n]\n\ncategorical_features = [\n    'site_id', \n    'ad_type_id', \n    'geo_id',\n    'device_category_id',\n    'advertiser_id',\n    'os_id',\n    'monetization_channel_id',\n    'ad_unit_id',\n    'order_id',\n    'line_item_type_id',\n    'integration_type_id',\n    'dayofweek',\n]\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numeric_transformer, numeric_features),\n    ('cat', categorical_transformer, categorical_features)\n])\n\nlr = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', LinearRegression()),\n])\n\nlr.fit(X_train, y_train)\nmean_squared_error(lr.predict(X_test), y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. DNN (not the best choice for tabular data... but why not)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_norm = preprocessor.fit_transform(X_train)\nX_val_norm = preprocessor.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TabularDataset(Dataset):\n    \n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X.todense()).float()\n        self.y = torch.from_numpy(y.values).float().unsqueeze(-1)\n        \n    def __getitem__(self, index):\n        return self.X[index], self.y[index]\n        \n    def __len__ (self):\n        return len(self.y)\n    \ntrain_dataset = TabularDataset(X_train_norm, y_train)\nval_dataset = TabularDataset(X_val_norm, y_val)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\nval_dataloader = DataLoader(val_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, 1)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n\n        x = self.fc4(x)\n        x = F.relu(x) # оставим и здесь relu, ибо таргет >= 0\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_dim = X_train_norm.shape[1]\nmodel = Net(input_dim, 512)\nopt = optim.Adam(model.parameters(), lr=3e-4)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for epoch in range(10):\n    \n    epoch_train_loss = 0\n    epoch_val_loss = 0\n    \n    model.train()\n    for X_train_batch, y_train_batch in train_dataloader:\n        y_pred = model(X_train_batch)\n        loss = F.mse_loss(y_pred, y_train_batch)\n        \n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        \n        epoch_train_loss += loss.item()    \n    \n    with torch.no_grad():\n        model.eval()\n        for X_val_batch, y_val_batch in val_dataloader:\n            y_pred = model(X_val_batch)\n            loss = F.mse_loss(y_pred, y_val_batch)\n            epoch_val_loss += loss.item()\n            \n        \n    epoch_train_loss = epoch_train_loss / len(train_dataloader)\n    epoch_val_loss = epoch_val_loss / len(val_dataloader)\n    print(f'train: {epoch_train_loss:.2f}, val: {epoch_val_loss:.2f}')\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_tensor = torch.from_numpy(preprocessor.transform(X_test).todense()).float()\nwith torch.no_grad():\n    preds = model(X_test_tensor).squeeze(-1).numpy()\n    \nmean_squared_error(y_test, preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"(y_train == 0).sum() / len(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'learning_rate': 0.1, \n    'n_estimators': 250, \n    'reg_lambda': 0.1, \n    'num_leaves': 63,\n    'objective': 'tweedie', # потому что распределение таргета скошенное и много нулей\n    'tweedie_variance_power': 1.5,\n}","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"lgbm = LGBMRegressor(**params)\nlgbm.fit(\n    X_train, \n    y_train, \n    eval_set=(X_val, y_val), \n    eval_metric=['mse'], \n    verbose=20, \n    categorical_feature=categorical_features\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(lgbm.predict(X_test), y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Обучим с теми же параметрами, но добавим данные, которые использовали для валидации"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm_all_train = LGBMRegressor(**params)\nlgbm_all_train.fit(X_train_val, y_train_val, categorical_feature=categorical_features)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"mean_squared_error(lgbm_all_train.predict(X_test), y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Best MSE: 2603.4"},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}