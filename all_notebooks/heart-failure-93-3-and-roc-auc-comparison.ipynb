{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Basic dataset analysis"},{"metadata":{},"cell_type":"markdown","source":"Python Imports:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Import:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Basic analysis of the dataframe:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like we have **299** records (rows) in the dataset."},{"metadata":{},"cell_type":"markdown","source":"Check for missing data (however we can observe we shouldn't have any missing data from above):"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,3))\n\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NO missing data** and data quality is good!"},{"metadata":{},"cell_type":"markdown","source":"Number of columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have **13 columns**, and all of them are **numerical (quantitative)**, no column is categorical (if there were any we could have just used LabelEncoder or OneHotEncoding with DummyVariables)!"},{"metadata":{},"cell_type":"markdown","source":"Number of death occurances:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\ndf['DEATH_EVENT'].value_counts().plot(kind='pie', autopct='%1.1f', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['DEATH_EVENT'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have **96 death events** and 203 not death events. **32.1% of patients died!**"},{"metadata":{},"cell_type":"markdown","source":"# 2. Exploratory Data Analysis (EDA) + Visualizations"},{"metadata":{},"cell_type":"markdown","source":"Explore the correlations in this dataset, as all columns are numerical:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),cmap='coolwarm', annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can start to see that the **age, ejection_fraction, serum_creatinine, serum_sodium and time** columns are quite well correlated to the DEATH_EVENT label. These seem to be the **most important features** in the df. We could only keep these when we build the model."},{"metadata":{},"cell_type":"markdown","source":"**AGE**:"},{"metadata":{},"cell_type":"markdown","source":"Age distribution of the 2 sexes:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\n\ng = sns.FacetGrid(df, hue=\"sex\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"age\", bins=30, alpha=0.5)\n\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar normal distribution for M/F."},{"metadata":{},"cell_type":"markdown","source":"Age distribution of survived/not survived:"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, hue=\"DEATH_EVENT\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"age\", bins=30, alpha=0.5)\n\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like older people tend to be correlated to DEATH_EVENT (makes sense)."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"DEATH_EVENT\", y=\"age\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the mean age is higher for death_event. Note some outliers for the age of DEATH_EVENT=0, probably very old people that did not die from heart disease."},{"metadata":{},"cell_type":"markdown","source":"**ANAEMIA:**\n\nWhat percentage of people with anaemia died?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\ndf['anaemia'].value_counts().plot(kind='pie', autopct='%1.1f', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**creatinine_phosphokinase distribution hued by DEATH_EVENT**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, hue=\"DEATH_EVENT\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"creatinine_phosphokinase\", bins=50, alpha=0.5)\n\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DIABETES:**\n\nWhat percentage of people with diabetes died?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\ndf['diabetes'].value_counts().plot(kind='pie', autopct='%1.1f', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ejection_fraction distribution hued by DEATH_EVENT**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, hue=\"DEATH_EVENT\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"ejection_fraction\", bins=10, alpha=0.5)\n\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"DEATH_EVENT\", y=\"ejection_fraction\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that lower ejection fraction increases chances of DEATH_EVENT."},{"metadata":{},"cell_type":"markdown","source":"**HIGH BLOOD PRESSURE:**\n\nWhat percentage of people with high-blood pressure died?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\ndf['high_blood_pressure'].value_counts().plot(kind='pie', autopct='%1.1f', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**platelets distribution hued by DEATH_EVENT**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, hue=\"DEATH_EVENT\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"platelets\", bins=30, alpha=0.5)\n\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**serum_creatinine distribution hued by DEATH_EVENT**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, hue=\"DEATH_EVENT\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"serum_creatinine\", bins=30, alpha=0.5)\n\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"DEATH_EVENT\", y=\"serum_creatinine\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks to be higher for people that died of heart disease."},{"metadata":{},"cell_type":"markdown","source":"**serum_sodium distribution hued by DEATH_EVENT**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(df, hue=\"DEATH_EVENT\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"serum_sodium\", bins=30, alpha=0.5)\n\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"DEATH_EVENT\", y=\"serum_sodium\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks to be lower for people that died."},{"metadata":{},"cell_type":"markdown","source":"**GENDER:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\ndf['sex'].value_counts().plot(kind='pie', autopct='%1.1f', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SMOKING:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\ndf['smoking'].value_counts().plot(kind='pie', autopct='%1.1f', shadow=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**serum_sodium vs ejection_fraction:** (these look to be correlated from the corr heatmap above)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x='serum_sodium',y='ejection_fraction', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just a slight correlation here, the line is not too steep."},{"metadata":{},"cell_type":"markdown","source":"# 3. Models and Performance: "},{"metadata":{},"cell_type":"markdown","source":"Choose what features are included in the model (for now we will include all, but we could have chosen the ones given by corr heatmap):"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.drop(['DEATH_EVENT'], axis=1)\ny=df['DEATH_EVENT']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predefine performance metric functions for Classification Problems**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_validation_report(y_true, y_pred):\n    print(\"Classification Report\")\n    print(classification_report(y_true, y_pred))\n    acc_sc = accuracy_score(y_true, y_pred)\n    print(\"Accuracy : \"+ str(acc_sc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred):\n    mtx = confusion_matrix(y_true, y_pred)\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n                cmap=\"Blues\", cbar=False)\n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression(max_iter=10000)\nlr.fit(X_train,y_train)\np1=lr.predict(X_test)\ns1=accuracy_score(y_test,p1)\nprint(\"Linear Regression Success Rate :\", s1*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_validation_report(y_test,p1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test,p1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**FEATURE IMPORTANCE IN LOG REGRESSION:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = abs(lr.coef_[0])\ncoeffecients = pd.DataFrame(importance, X_train.columns)\ncoeffecients.columns = ['Coeffecient']\nplt.figure(figsize=(15,4))\nplt.bar(X_train.columns,importance)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected, the columns we predicted have the highest feature importance! The rest of the columns could be discarded and the models re-build using only the highlighted columns."},{"metadata":{},"cell_type":"markdown","source":"## 3.2 Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc=RandomForestClassifier()\nrfc.fit(X_train,y_train)\np2=rfc.predict(X_test)\ns2=accuracy_score(y_test,p2)\nprint(\"Random Forrest Accuracy :\", s2*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test,p2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.3 SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvm=SVC()\nsvm.fit(X_train,y_train)\np3=svm.predict(X_test)\ns3=accuracy_score(y_test,p3)\nprint(\"SVM Accuracy :\", s3*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test,p3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.4 KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train,y_train)\np4=knn.predict(X_test)\ns4=accuracy_score(y_test,p4)\nprint(\"KNN Accuracy :\", s4*100,'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's optimize for K:"},{"metadata":{"trusted":true},"cell_type":"code","source":"error_rate = []\nscores = []\n\nfor i in range(1,40): # check all values of K between 1 and 40\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    score=accuracy_score(y_test,pred_i)\n    scores.append(score)\n    error_rate.append(np.mean(pred_i != y_test)) # ERROR RATE DEF and add it to the list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),scores,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Accuracy Score vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Accuracy Score')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"K=35 seems a good value that minimises errors and maximises accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier(n_neighbors=35)\nknn.fit(X_train,y_train)\np4=knn.predict(X_test)\ns4=accuracy_score(y_test,p4)\nprint(\"KNN Accuracy:\", s4*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test,p4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.5 Gaussian Naive-Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\np5 =nb.predict(X_test)\ns5=accuracy_score(y_test,p5)\nprint(\"Naive-Bayes Accuracy:\", s5*100,'%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(y_test,p5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test,p5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Very good F1 score, meaning good precision-recall balance!!"},{"metadata":{},"cell_type":"markdown","source":"## Summarize results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': [\"LOGISTIC REGRESSION\",\"RANDOM FOREST\",\"SUPPORT VECTOR MACHINE\",\"KNN\",\"NAIVE-BAYES\"],\n    'Accuracy Score': [s1*100,s2*100,s3*100,s4*100,s5*100]})\nmodels.sort_values(by='Accuracy Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like **Naive-Bayes wins in accuracy**.\n\nWe can further compare each model's F1 scores for balance between precision and recall."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f1_score(y_test,p1))\nprint(f1_score(y_test,p2))\nprint(f1_score(y_test,p3))\nprint(f1_score(y_test,p4))\nprint(f1_score(y_test,p5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**NB** wins in F1 score as well, KNN and SVM are to be completely disregarded."},{"metadata":{},"cell_type":"markdown","source":"We can also compare the ROC curves and AUC scores for each model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve,roc_auc_score, auc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr1,tpr1, thr1=roc_curve(y_test,p1)\nfpr2,tpr2, thr2=roc_curve(y_test,p2)\nfpr3,tpr3, thr3=roc_curve(y_test,p3)\nfpr4,tpr4, thr4=roc_curve(y_test,p4)\nfpr5,tpr5, thr5=roc_curve(y_test,p5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(fpr1,tpr1, linestyle='--', label='LR')\nplt.plot(fpr2,tpr2, linestyle='--', label='RF')\nplt.plot(fpr3,tpr3, linestyle='--', label='SVM')\nplt.plot(fpr4,tpr4, linestyle='--', label='KNN')\nplt.plot(fpr5,tpr5, linestyle='--', label='NB')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the largest area under curve is the **Naive-Bayes**, as prediced. This means a good balance between Type1 and Type2 errors."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(roc_auc_score(y_test,p1))\nprint(roc_auc_score(y_test,p2))\nprint(roc_auc_score(y_test,p3))\nprint(roc_auc_score(y_test,p4))\nprint(roc_auc_score(y_test,p5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the **NB classifier has the largest AUC score.**"},{"metadata":{},"cell_type":"markdown","source":"# 4. Conclusion and model choice"},{"metadata":{},"cell_type":"markdown","source":"**Final model choice: Gaussian Naive-Bayes** presents the largest accuracy,f1 and auc scores, as well as only 4 miss-labelings!"},{"metadata":{},"cell_type":"markdown","source":"**Summary:** We have started with an initial data analysis, seeing if there was any missing data and ensuring the integrity and quality of data. We also obesrved each column (their data types) and for the categorical columns, we could have used LabelEncoding or Dummy variables (not the case in this dataset). Then, some visualizations based on each feature were created and a correlation heatmap was used to determine some preliminary important features, that were confirmed in the model-building section afterwards (these features could have been used for the models instead of choosing all of them like I have). The data was split into train-test and 5 Classifier Models (Logistic Regression, Random Forest, KNN, SVC and Gaussian Naive-Bayes) were build upon the train data and tested upon the test data. The comparison metrics were accuracy score, f1 score, as well as the ROC-AUC curve scores, and the best model was clearly the Gaussian Naive-Bayes, with around 93% accuracy and the least number of Type 1 + Type 2 errors. Feature importance was extracted from the Logistic Regression, however it is best to use regularization algorithms like Lasso/Ridge/ElasticNet to extract feature importance. To note that in medical diagnosis, it is desired to minimise False Negatives! Moreover, the KNN Classifier was optimized for the best K value."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}