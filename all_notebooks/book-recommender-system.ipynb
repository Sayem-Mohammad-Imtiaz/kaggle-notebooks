{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt # visualization\n!pip install seaborn as sns -q # visualization with seaborn v0.11.1\nimport seaborn as sns # visualization\nimport missingno as msno # missing values pattern visualization\n\nimport warnings # supress warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\nimport math\n\n\nplt.style.use('bmh')\n\n# set pandas display option\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)\n\n# Load the data \nBooks_df = pd.read_csv('../input/book-recommendation-dataset/Books.csv')\nRatings_df = pd.read_csv('../input/book-recommendation-dataset/Ratings.csv')\nUsers_df = pd.read_csv('../input/book-recommendation-dataset/Users.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display the dataset\nRatings_df.head().style.set_caption('Sample of Ratings data')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summarize the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dimension of dataset\nprint(f'''\\t  Book_df shape is {Books_df.shape}\n          Ratings_df shape is {Ratings_df.shape}\n          Users_df shape is {Users_df.shape}''')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_zero_values_table(df):\n    mis_val=df.isnull().sum()\n    mis_val_percent=round(df.isnull().mean().mul(100),2)\n    mz_table=pd.concat([mis_val,mis_val_percent],axis=1)\n    mz_table=mz_table.rename(\n    columns={df.index.name:'col_name',0:'Missing Values',1:'% of Total Values'})\n    mz_table['Data_type']=df.dtypes\n    mz_table=mz_table.sort_values('% of Total Values',ascending=False)\n    print(f\"Your selected dataframe has \"+str(df.shape[1])+\" columns and \"+str(df.shape[0])+\" Rows.\\n\"\n         \"There are \"+str(mz_table[mz_table.iloc[:,1] != 0].shape[0])+\n          \" columns that have missing values.\")\n    return mz_table.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_zero_values_table(Users_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_zero_values_table(Ratings_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_zero_values_table(Books_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check outlier data in **Age** and **Book-Rating** column  "},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsns.boxplot(y='Book-Rating', data=Ratings_df,ax=ax[0])\nax[0].set_title('Find outlier data in Rating Book column')\nsns.boxplot(y='Age', data=Users_df,ax=ax[1])\nax[1].set_title('Find outlier data in Age column')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sorted(Users_df.Age.unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age : 244 :))"},{"metadata":{},"cell_type":"markdown","source":"Ok we have Outlier data in Age    \nso must be fixed it   "},{"metadata":{},"cell_type":"markdown","source":"OK let's find our unique value in Location column "},{"metadata":{"trusted":true},"cell_type":"code","source":"Users_df.Location.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(Users_df.Location.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"57339 unique Value it's really hard to understand  \nso use regex and create column country"},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df['Book-Author'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Say us Miss [Agatha Christie](https://www.biography.com/writer/agatha-christie) is top in Books data frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(Books_df['Year-Of-Publication'].unique().tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Year of publication **2037** !!  \n'**Gallimard**' , '**DK Publishing Inc**' , type of sum year is **string**  "},{"metadata":{"trusted":true},"cell_type":"code","source":"1.0 - (np.count_nonzero(Ratings_df)/float(Ratings_df.size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"15 percent sparse"},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(Ratings_df['Book-Rating'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0 is an invalid number in the rated books  \nand rating value must be 1 to 10"},{"metadata":{"trusted":true},"cell_type":"code","source":"Ratings_df.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"usersCount=Users_df.shape[0]\nbooksCount=Books_df.shape[0]\nprint(f'Users : {usersCount}')\nprint(f'Books : {booksCount}')\nprint(f'Total : {usersCount*booksCount}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Users rated **1149780** books, but there are **271360** books        \nso users did not rate all books      \nand users participation that rated make two question   \n1. Are the books they rated part of the book's data frame ?  \n2. Are the users they rated part of the user's data frame ?  \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings_new = Ratings_df[Ratings_df.ISBN.isin(Books_df.ISBN)]\nratings_new = ratings_new[Ratings_df['User-ID'].isin(Users_df['User-ID'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Users or books aren't in dataset\")\nprint(f'Total : {Ratings_df.shape[0] - ratings_new.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparsity = round(1.0 - len(ratings_new)/float(usersCount*booksCount),6)\nsparsity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age column has 39 percent null data   \nand age column has outlier data   \nand I can don't use Cosine Similarity   \nso let's do it together  \nif any things it's not correct I'm really become happy to tell me "},{"metadata":{},"cell_type":"markdown","source":"# Visualization and Modeling"},{"metadata":{},"cell_type":"markdown","source":"**Steps**\n1. rename columns names :))\n2. Create country column to analyze better \n3. Fill Na value in  Country column \n4. Some data in Country Column has Misspellings \n5. Create rating_Avg and number_of_rating to analyze better\n6. users more in which countries\n7. Age column has outlier data \n8. Fill Na value in Age column \n9. Fill Na value in Book data frame's Author column \n10. Fill Na value in Book data frame's Publisher column \n11. Book data frame's Year of Publication column has two string value and some integer value  type is string\n12. Book data frame's Year of Publication has outlier data \n13. Fill Na value in Book data frame's Year of Publication \n14. join three data frames together\n15. Delete user and book columns they rated but aren't in the dataset\n16. Rating_book value must be 1 to 10\n17. drop three unhelpful columns 'Image-URL-S', 'Image-URL-M', 'Image-URL-L' "},{"metadata":{"trusted":true},"cell_type":"code","source":"Ratings_df.rename(columns={'User-ID':'user_id','Book-Rating':'book_rating'},inplace=True)\nUsers_df.rename(columns={'User-ID':'user_id'},inplace=True)\nBooks_df.rename(columns={'Book-Title':'Book_Title','Book-Author':'Book_Author',\n                         'Year-Of-Publication':'Year_Of_Publication'},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Country Column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Users_df['Country']='Iran'\nfor i in Users_df:\n    Users_df['Country']=Users_df.Location.str.extract(r'\\,+\\s?(\\w*\\s?\\w*)\\\"*$')   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(Users_df.Country.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Users_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"368 of users Country column is Nan so must be fill it "},{"metadata":{"trusted":true},"cell_type":"code","source":"Users_df.loc[Users_df.Country.isnull(),'Country']='other'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So I don't have any idea Location column has 57339 unique value    \nfor this I use Regex and create country column   \nbut we have [195 Countries in the World !!](https://www.worldometers.info/geography/how-many-countries-are-there-in-the-world/)  \nBut it's better than 57339 unique Location value :))  \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(Users_df.Country,Ratings_df.book_rating).T.style.background_gradient()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some data has Misspellings "},{"metadata":{"trusted":true},"cell_type":"code","source":"Users_df['Country'].replace(['','alachua','america','austria','autralia','cananda','geermany','italia','united kindgonm','united sates','united staes','united state','united states','us'],\n                           ['other','usa','usa','australia','australia','canada','germany','italy','united kingdom','usa','usa','usa','usa','usa'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create Column 'count rate'   \nuser participation in rated   \nand even users rated the books zero   "},{"metadata":{},"cell_type":"markdown","source":"Rating Average and"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create column Count_All_Rate\nRatings_df['Count_All_Rate']=Ratings_df.groupby('ISBN')['user_id'].transform('count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Country and Users**"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=sns.light_palette('green',as_cmap=True)\npopular=Users_df.Country.value_counts().to_frame()[:10]\npopular.rename(columns={'Country':'Count_Users_Country'},inplace=True)\npopular.style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the below chart there is one row has named 'other' it's mean    \nlocation is Nan, or regex it's not able to read"},{"metadata":{},"cell_type":"markdown","source":"**Age Columns**"},{"metadata":{},"cell_type":"markdown","source":"In the plot and in the unique value   \nwe understand we have outlier data   \nso for outlier data I convert it to Nan value  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# outlier data became NaN\nUsers_df.loc[(Users_df.Age > 100 ) | (Users_df.Age < 5),'Age']=np.nan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Users_df.Age.plot.hist(bins=20,edgecolor='black',color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(Users_df.Age.skew(axis=0,skipna=True),3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age has **positive Skewness** (right tail)      \nso we I have one idea to fill Na value from **Median**   \nfor this we don't like to fill Na value **just for one range of age** for handle it I use **country column** to fill Na "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Series of users data live in which country \ncountryUsers = Users_df.Country.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country=countryUsers[countryUsers>=5].index.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Range of Age users in country register in this library and had participation\nRangeOfAge = Users_df.loc[Users_df.Country.isin(country)][['Country','Age']].groupby('Country').agg(np.mean).to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor k,v in RangeOfAge['Age'].items():\n    Users_df.loc[(Users_df.Age.isnull())&(Users_df.Country== k),'Age'] = v\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Users_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"POF again we have 330 null Value   \nfor fill in it   \nAge has **positive Skewness** (right tail)        \nso we I have one idea to fill Na value from **Median**     "},{"metadata":{"trusted":true},"cell_type":"code","source":"medianAge = int(Users_df.Age.median())\nUsers_df.loc[Users_df.Age.isnull(),'Age']=medianAge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Users_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Book Author** column has **Nan** value"},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df[Books_df.Book_Author.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df.loc[(Books_df.ISBN=='9627982032'),'Book_Author']='other'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Publisher column has Nan value**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df[Books_df.Publisher.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df.loc[(Books_df.ISBN=='193169656X'),'Publisher']='other'\nBooks_df.loc[(Books_df.ISBN=='1931696993'),'Publisher']='other'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Year of Publication**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df[Books_df.Year_Of_Publication=='Gallimard']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df[Books_df.Year_Of_Publication=='DK Publishing Inc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df.loc[Books_df.ISBN=='2070426769','Year_Of_Publication']=2003\nBooks_df.loc[Books_df.ISBN=='2070426769','Book_Author']='Gallimard'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df.loc[Books_df.ISBN=='0789466953','Year_Of_Publication']=2000\nBooks_df.loc[Books_df.ISBN=='0789466953','Book_Author']='DK Publishing Inc'\nBooks_df.loc[Books_df.ISBN=='078946697X','Year_Of_Publication']=2000\nBooks_df.loc[Books_df.ISBN=='078946697X','Book_Author']='DK Publishing Inc'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df.Year_Of_Publication=Books_df.Year_Of_Publication.astype(np.int32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sorted(Books_df.Year_Of_Publication.unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Years of publication after 2021 and 0 it's not normal   \nso must be converted to Nan value"},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df.loc[(Books_df.Year_Of_Publication>=2021)|(Books_df.Year_Of_Publication==0),'Year_Of_Publication']=np.NAN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"author=Books_df[Books_df.Year_Of_Publication.isnull()].Book_Author.unique().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RangeYearOfPublication = Books_df.loc[Books_df.Book_Author.isin(author)][['Book_Author','Year_Of_Publication']].groupby('Book_Author').agg(np.mean).round(0).to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meanYear=round(Books_df.Year_Of_Publication.mean())\nauthorNanYear={}\nauthorYear={}\nfor k,v in RangeYearOfPublication['Year_Of_Publication'].items():\n    if math.isnan(v) != True:\n        authorYear[k]=v\n    else:\n        authorNanYear[k] = meanYear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(authorNanYear.keys())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1355 authors don't have a year of publication and the average of them is Nan   \nand I forced filling Nan value with mean of all year of publication authors"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(authorYear.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for k,v in authorYear.items():\n#     Books_df.loc[(Books_df.Year_Of_Publication.isnull())&(Books_df.Book_Author== k),'Year_Of_Publication'] = v","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1959 authors don't have year of publication of them books    \nand they return value   \nbut it's take long time to fill Nan value   \nI would like to find a fast way :))  \nbut now I don't know   \nif you know please tell me in the comment  "},{"metadata":{},"cell_type":"markdown","source":"This method it's not helpful     \nI must find another way      "},{"metadata":{"trusted":true},"cell_type":"code","source":"Books_df.loc[Books_df.Year_Of_Publication.isnull(),'Year_Of_Publication'] = round(Books_df.Year_Of_Publication.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I don't like this method, but I force to use this solution"},{"metadata":{},"cell_type":"markdown","source":"**new Ratings_book dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings_new = Ratings_df[Ratings_df.ISBN.isin(Books_df.ISBN)]\nratings_new = ratings_new[ratings_new.user_id.isin(Users_df.user_id)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separate 1 to 10 and 0 rated value"},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings_0 = ratings_new[ratings_new.book_rating ==0]\nratings_1to10 = ratings_new[ratings_new.book_rating !=0]\n# Create column Rating average \nratings_1to10['rating_Avg']=ratings_1to10.groupby('ISBN')['book_rating'].transform('mean')\n# Create column Rating sum\nratings_1to10['rating_sum']=ratings_1to10.groupby('ISBN')['book_rating'].transform('sum')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings_0.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings_1to10.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings_1to10.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=Users_df.copy()\ndataset=pd.merge(dataset,ratings_1to10,on='user_id')\ndataset=pd.merge(dataset,Books_df,on='ISBN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def skew_test(df):\n    col = df.skew(axis = 0, skipna = True)\n    val = df.skew(axis = 0, skipna = True) \n    sk_table = pd.concat([col, val], axis = 1)\n    sk_table = sk_table.rename(\n    columns = {0 : 'skewness'})\n    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n        \"There are \" + str(sk_table.shape[0]) +\n          \" columns that have skewed values - Non Gaussian distribution.\")\n    return sk_table.drop([1], axis = 1).sort_values('skewness',ascending = False).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skk = skew_test(dataset)\nskk.style.background_gradient(cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18,8))\nsns.countplot(data=ratings_1to10,x='book_rating',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dataset.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't need 3 columns : 'Image-URL-S', 'Image-URL-M', 'Image-URL-L'"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=dataset[['user_id', 'Location', 'Age', 'Country', 'ISBN', 'book_rating', 'rating_Avg','rating_sum', 'Count_All_Rate', 'Book_Title', 'Book_Author', 'Year_Of_Publication', 'Publisher']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_zero_values_table(dataset)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok everything's ok  "},{"metadata":{},"cell_type":"markdown","source":"# Simple Popularity based Recommendation System"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=sns.light_palette('red',as_cmap=True)\n# count all rate means include users rated 0 to book\npopular=dataset.groupby(['Book_Title','Count_All_Rate','rating_Avg','rating_sum']).size().reset_index().sort_values(['rating_sum','rating_Avg',0],\n                                                                                                            ascending=[False,False,True])[:20]\npopular.rename(columns={0:'Count_Rate'},inplace=True)\npopular.style.background_gradient(cmap=cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 20 most popular books in dataset  \nand they bought and rated it"},{"metadata":{},"cell_type":"markdown","source":"What !!  \nWhy it's recommended 'Wild Animus' book   \navg rate is low, but sum rate is high    \nthis is one problem of that    \nDo you know how can I fix this bug ??  \nIf you know say in the comment box  "},{"metadata":{},"cell_type":"markdown","source":"# Collaborative Filtering "},{"metadata":{},"cell_type":"markdown","source":"I don't have great knowledge, but I try to create best :))"},{"metadata":{},"cell_type":"markdown","source":"The First step is to find  persons who are similar to user  \nso must be calculated distance   \nand distance can calculate by those methods    \n1. Manhattan distance \n2. Euclidean distance\n3. Minkowski distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def manhattan(rating1,rating2):\n    \"Computes the Manhattan distance. Both rating1 and rating2 are dictionaries\"\n    user1=dict(zip(dataset.loc[dataset.user_id==rating1].Book_Title,dataset.loc[dataset.user_id==rating1].book_rating))\n    user2=dict(zip(dataset.loc[dataset.user_id==rating2].Book_Title,dataset.loc[dataset.user_id==rating2].book_rating))\n    distance = 0\n    for key in user1:\n        if key in user2:\n            distance += abs(user1[key] - user2[key])\n    return distance\nprint(f'Manhattan distance between user number 8 and 11676 : {manhattan(8,11676)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def euclidean(rating1,rating2):\n    \"Computes the Euclidean distance. Both rating1 and rating2 are dictionaries\"\n    user1=dict(zip(dataset.loc[dataset.user_id==rating1].Book_Title,dataset.loc[dataset.user_id==rating1].book_rating))\n    user2=dict(zip(dataset.loc[dataset.user_id==rating2].Book_Title,dataset.loc[dataset.user_id==rating2].book_rating))\n    distance = 0\n    for key in user1:\n        if key in user2:\n            distance += math.pow(abs(user1[key]-user2[key]),2)\n    return math.sqrt(distance)\nprint(f'Euclidean distance between user number 8 and 11676 : {euclidean(8,11676)}')  \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def minkowski(rating1,rating2,r):\n    \"\"\"Computes the Minkowski distance. Both rating1 and rating2 are dictionaries\"\"\"\n    user1=dict(zip(dataset.loc[dataset.user_id==rating1].Book_Title,dataset.loc[dataset.user_id==rating1].book_rating))\n    user2=dict(zip(dataset.loc[dataset.user_id==rating2].Book_Title,dataset.loc[dataset.user_id==rating2].book_rating))\n    distance = 0\n    for key in user1:\n        if key in user2:\n            distance += math.pow(abs(user1[key]-user2[key]),r)\n    return math.pow(distance,1/r)\nprint(f'Minkowski distance between user number 8 and 11676 : {minkowski(8,11676,2)}') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset has a lot of users had rated lower than ten books   \nand  users don't paid attention to some books  \nso I will drop it  "},{"metadata":{"trusted":true},"cell_type":"code","source":"counts1 = ratings_1to10['user_id'].value_counts()\nratings_1to10 = ratings_1to10[ratings_1to10['user_id'].isin(counts1[counts1 >= 100].index)]\ncounts = ratings_1to10['book_rating'].value_counts()\nratings_1to10 = ratings_1to10[ratings_1to10['book_rating'].isin(counts[counts >= 100].index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.user_id.unique().tolist()[500]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def computeNearestNeighbor(username):\n    \"\"\"Creates a sorted list of users based on their distance \n    to username \"\"\"\n    #users = list(dataset.user_id.unique())\n    users=dataset.user_id.unique().tolist()[:500]\n    distances = []\n    for user in users:\n        if user != username:\n            distance = manhattan(user,username)\n            distances.append((distance,user))\n    # sort based on distance -- closest first\n    distances.sort()\n    return distances\ncomputeNearestNeighbor(192762)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recommend(username):\n    \"\"\"Give list of recommendations\"\"\"\n    # first find nearest neighbor\n    nearest=computeNearestNeighbor(username)[0][1]\n    recommendations=[]\n    # now find bands neighbor rated that user didn't\n    neighborRatings = dataset.loc[dataset.user_id==nearest].Book_Title.tolist()\n    userRatings = dataset.loc[dataset.user_id==username].Book_Title.tolist()\n    for artist in neighborRatings:\n        if not artist in userRatings:\n            recommendations.append((artist,int(dataset[(dataset.Book_Title==artist) & (dataset.user_id==nearest)].book_rating)))\n    return sorted(recommendations,key=lambda artistTuple : artistTuple[1],reverse=True)\nprint(recommend(192762))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It shows us Manhattan distance between user 192762 with 500 other users distance to suggest book   \nand it's not helpful for high count users   \nso must find another solution   "},{"metadata":{},"cell_type":"markdown","source":"<hr>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}