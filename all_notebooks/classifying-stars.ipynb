{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # for graphics\nimport seaborn as sns # too for graphics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading the dataset","metadata":{}},{"cell_type":"code","source":"# this is for show the dataset route\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the dataset path\npath = '../input/star-type-classification/Stars.csv'\n# read the dataset\ndataset = pd.read_csv(path)\n\n# lets rename a few columns\nnew_names = {'L': 'Relative_Luminosity', 'R': 'Relative_Radius', 'A_M': 'Absolute_Magnitude'}\ndataset = dataset.rename(columns=new_names)\n# see the first dataset's rows\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Classifying Stars - Context\nAs far as I know about classifying stars there's many classes we can use depending on the star qualities. This dataset as I see requires to classify stars with categories that appear to be 'by size'. I found a [video](https://www.youtube.com/watch?v=Y5VU3Mp6abI) which explains that categories. `The target is the type`.\n\nThe video talks about a ***H-R Diagram*** which could shows the 6 clusters of stars that we want to classify. This plot is `composed by the temperature in X and the luminosity in Y`.","metadata":{}},{"cell_type":"code","source":"# This is only for give an image of the data\nY = np.log10(dataset['Relative_Luminosity'].values)\nX = np.log10(dataset['Temperature'].values)\nclusters = dataset['Type'].values\n\ncdict = {0: 'skyblue', 1: 'indigo', 2: 'orange', 3: 'yellow', 4: 'limegreen', 5: 'lightcoral'}\n\n\nfig, ax = plt.subplots()\nplt.title('H-R Diagram')\nfor g in np.unique(clusters):\n    ix = np.where(clusters == g)\n    ax.scatter(X[ix], Y[ix], c=cdict[g], label=g)\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data procesing","metadata":{}},{"cell_type":"markdown","source":"## Processing Color - One Hot Encoding","metadata":{}},{"cell_type":"code","source":"#dataset['Color'].value_counts().plot(kind='bar')\ndataset['Color'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there few are repeated colors that are not counted by the same column\n# so it's needed to replace some names for 'repair' the names and then\n# do not have too much columns, that's going to be important\nfixed_names = {\n    'Red' : 'Red', \n    'Blue' : 'Blue', \n    'Blue-white' : 'Blue-white', \n    'Blue White' : 'Blue-white', \n    'yellow-white' : 'White-Yellow', \n    'White' : 'White',\n    'Blue white' : 'Blue-white', \n    'white' : 'White', \n    'Yellowish White' : 'White-Yellow', \n    'yellowish' : 'Yellow', \n    'Orange' : 'Orange',\n    'Whitish' : 'White', \n    'Yellowish' : 'Yellow', \n    'Blue-White' : 'Blue-white', \n    'Pale yellow orange' : 'Orange',\n    'Orange-Red' : 'Orange', \n    'White-Yellow' : 'White-Yellow' \n}\n\n# this line renames values of Color depending on the dict\ndataset['Color'] = dataset['Color'].map(fixed_names).astype('category')\n# then we visualize the values of color\nfigure = plt.figure(figsize=(20,8))\nsns.barplot(x=\"Color\", y=\"Type\", data=dataset)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the type to categorical for get ids\ndataset['Color'] = dataset['Color'].astype('category')\n# add the categorical numbers in a new column\ndataset['Color_cats'] = dataset['Color'].cat.codes\n\n# get the categorical numbers of each Color\nids = list(dataset['Color_cats'].value_counts().index)\n# get the colors, both are in the same order\ncolors = list(dataset['Color'].value_counts().index)\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a df with the ids and the colors\ndf = pd.DataFrame(list(zip(ids, colors)),columns=['Ids', 'Color'])\n# and then use the oneHotEncoder in a new dataframe\nencoding = pd.get_dummies(dataset['Color'], prefix='Color')\n# finally there's to concat the encoding df to dataset\ndataset = pd.concat([dataset, encoding], axis=1, join=\"inner\")\n# and drop the Color_cats column\ndataset.drop(['Color_cats', 'Color'], axis=1, inplace=True)\nprint(dataset.shape)\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Processing Spectral Class - One Hot Encoding","metadata":{}},{"cell_type":"code","source":"figure = plt.figure(figsize=(20,8))\nsns.barplot(x=\"Spectral_Class\", y=\"Type\", data=dataset)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for this it's okey to repeat the previous process\ncol = 'Spectral_Class'\n\n# convert the type to categorical for get ids\ndataset[col] = dataset[col].astype('category')\n# add the categorical numbers in a new column\ndataset[f'{col}_cats'] = dataset[col].cat.codes\n\n# get the categorical numbers of each Color\nids = list(dataset[f'{col}_cats'].value_counts().index)\n# get the colors, both are in the same order\ncolors = list(dataset[col].value_counts().index)\n\n# create a df with the ids and the colors\ndf = pd.DataFrame(list(zip(ids, colors)),columns=['Ids', 'Color'])\n# and then use the oneHotEncoder in a new dataframe\nencoding = pd.get_dummies(dataset[col], prefix=col)\n# finally there's to concat the encoding df to dataset\ndataset = pd.concat([dataset, encoding], axis=1, join=\"inner\")\n# and drop the Color_cats column\ndataset.drop([f'{col}_cats', col], axis=1, inplace=True)\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Processing Temperature, Luminosity, Radius and Magnitude - Standard Scaler","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler # the scaler\n\ncols = ['Temperature','Absolute_Magnitude','Relative_Luminosity','Relative_Radius' ]\n\n# instance the scaler\ntemp_scaler = StandardScaler()\n# fit the scaler with the data\ntemp_scaler.fit(dataset[cols])\n# transform the data\nscaled_vals = temp_scaler.transform(dataset[cols])\n# and save the changes\ndataset['Temperature'] = scaled_vals[:,0]\ndataset['Absolute_Magnitude'] = scaled_vals[:,1]\ndataset['Relative_Luminosity'] = scaled_vals[:,2]\ndataset['Relative_Radius'] = scaled_vals[:,3]\ndataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The target: Type","metadata":{}},{"cell_type":"code","source":"# select the target\ntarget = dataset['Type'].values\n# delete the target from the dataset\ndataset.drop(['Type'], axis=1, inplace=True)\ntarget","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split the dataset on train and test","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(dataset, target, test_size=.2, random_state=2021)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Models\nTheres many models that can be used for this practise. Since there's no many data use a neural network may not be the best option. These models that are included are from sklearn, and are used for classify. These are the models:\n\n* [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n* [KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n* [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n* [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n* [GradientBoostingClassifier](https://scikit-learn.org/0.15/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n* [LGBMClassifier]()\n* [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html?highlight=kmeans#sklearn.cluster.KMeans)","metadata":{}},{"cell_type":"code","source":"# many models to prove how is them accuracy\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# and import the cross validation score to eval the models\nfrom sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# instance and fit all the models with the data\n\n# the solver=\"liblinear\" is better for small datasets\nlj = LogisticRegression(solver=\"liblinear\").fit(X_train, Y_train)\nknn = KNeighborsClassifier().fit(X_train, Y_train)\ndtc = DecisionTreeClassifier(random_state=0).fit(X_train, Y_train)\nrfc = RandomForestClassifier(random_state=0,verbose=False).fit(X_train, Y_train)\ngbmc = GradientBoostingClassifier(verbose=False).fit(X_train, Y_train)\n\n# save the models in a list to visualize the accuracy of each\nmodels = [lj, knn, dtc, rfc, gbmc]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iterate the models to evaluate them and see the accuracy\nfor model in models:\n    # select the name\n    name = model.__class__.__name__\n    # evaluate with cross validation and get the mean\n    score = cross_val_score(model,X_test, Y_test,cv=5,verbose=False).mean()\n    # calculate the error metric value with neg_mean_squared_error and same the mean value\n    error = -cross_val_score(model,X_test, Y_test,cv=5,scoring=\"neg_mean_squared_error\",verbose=False).mean()\n    # show the results\n    print(\"-> \" + name + \": \")\n    print('Accuracy:', score)\n    # aply the sqrt as it is squared error\n    print('Error:', np.sqrt(error))\n    print(\"*\" * 20)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize the evaluations","metadata":{}},{"cell_type":"code","source":"# we will try to eval the models with a largest cv\n\n# set the results in a dataframe\nresults = pd.DataFrame(columns=[\"model\",\"score\"])\n\n# iterate the models\nfor model in models:\n    # select the name\n    name = model.__class__.__name__\n    # evaluate the model \n    score = cross_val_score(model, X_test, Y_test, cv=5, verbose=False).mean()\n    # set the result on a dataframe\n    result = pd.DataFrame([[name,score]], columns=[\"model\",\"score\"])\n    # and append the result on the results dataframe\n    results = results.append(result)\n\n# then plot the results\nfigure = plt.figure(figsize=(20,8))   \nsns.barplot(x=\"score\",y=\"model\",data=results)\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Model\")\n# set the limit on 100 that is the score limit\nplt.xlim(0,1)\nplt.title(\"Model Accuracy Score\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do the same but for the error values\n\n# set the results in a dataframe\nresults = pd.DataFrame(columns=[\"model\",\"error\"])\n\n# iterate the models\nfor model in models:\n    # select the name\n    name = model.__class__.__name__\n    # evaluate the model \n    error = -1 * cross_val_score(model, X_test, Y_test, cv=5, scoring=\"neg_mean_squared_error\", verbose=False).mean()\n    # set the result on a dataframe\n    result = pd.DataFrame([[name, np.sqrt(error)]], columns=[\"model\",\"error\"])\n    # and append the result on the results dataframe\n    results = results.append(result)\n\n# then plot the results\nfigure = plt.figure(figsize=(20,8))   \nsns.barplot(x=\"error\",y=\"model\",data=results)\nplt.xlabel(\"Error\")\nplt.ylabel(\"Model\")\n# set the limit on 100 that is the score limit\nplt.xlim(0,1)\nplt.title(\"Model Error Score\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\n\nAs we can see the **DecisionTreeClassifier** looks like the best option as it has 100% of accuracy and 0 error.\nHowever we can see that **RandomForestClassifier** and **GradientBoostingClassifier**, both have a good performance in more than 90% and an not so high error value.","metadata":{}},{"cell_type":"markdown","source":"# Answer to a comment:\nSo from here, how might one go about using this model if given some measurements on an unclassified star?\n\nI will try to set the measurements in a **DataFrame** and then use the trained **OneHotEncoder and Standard Scaler** to normalize the measurements and then use that normalized data to predict with a model. The next cells develop the process.","metadata":{}},{"cell_type":"markdown","source":"## 1 - Set the new measurements in a DataFrame\nFor example the first star from the dataset.","metadata":{}},{"cell_type":"code","source":"# I will repeat the reading of the dataset\n# but with other name, data\ndata = pd.read_csv(path)\n\n# lets rename a few columns\nnew_names = {'L': 'Relative_Luminosity', 'R': 'Relative_Radius', 'A_M': 'Absolute_Magnitude'}\ndata = data.rename(columns=new_names)\n\n# FIRST: have the new measurements, for example:\n# select the first star in the dataset\n\n# get the values and the column names\nvals = data.drop(['Type'], axis=1).loc[0].values\ncols = data.drop(['Type'], axis=1).columns\n\n# set the data in a DataFrame\ndata = pd.DataFrame(data=[vals], columns=cols)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2 - Apply the Scaler and Encoder to the data\nThese must be the same, or have the same parameters.","metadata":{}},{"cell_type":"code","source":"# First, the scaler, temp_scaler\nnum_cols = ['Temperature','Absolute_Magnitude','Relative_Luminosity','Relative_Radius']\n\n# apply the transformation with the scaler\nscal_vals = temp_scaler.transform(data[num_cols])\n# set the scaled values\ndata['Temperature'] = scal_vals[:,0]\ndata['Absolute_Magnitude'] = scal_vals[:,1]\ndata['Relative_Luminosity'] = scal_vals[:,2]\ndata['Relative_Radius'] = scal_vals[:,3]\ndata\n# expected: -0.779382\t-0.598624\t-0.459210\t1.11674","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Second, the one hot encodind\n# I will use an extra function to apply one hot encoding\n# and I will use it with Color and Spectral_Class\n\n# target col will be 'Color' in data\n# cols will be the color columns in dataset\ndef set_encoding(value, target_col, cols):\n    res = pd.DataFrame()\n    # col with value 1\n    col_name = target_col + '_' + value\n    # iterate all the columns\n    for col in cols:\n        # look for the special column with value 1\n        if col == col_name:\n            res[col] = [1]\n        # add the column to the dataset with value 0\n        else:\n            res[col] = [0]\n    return res\n\n\n# collect the required parameters\ncolor_cols = [col for col in dataset.columns if col[:5] == 'Color']\ncolor_val = data['Color'].values[0]\n\nspectral_cols = [col for col in dataset.columns if col[:8] == 'Spectral']\nspectral_val = data['Spectral_Class'].values[0]\n\n\n# then use the function to Color and Spectral Class\ncolor_encoding = set_encoding(color_val, 'Color', color_cols)\nspectral_encoding = set_encoding(spectral_val, 'Spectral_Class', spectral_cols)\ncolor_encoding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat the results to the measurements dataframe\n# and delete the Color and Spectral_Class cols\ndata = pd.concat([data, color_encoding], axis=1)\ndata = data.drop(['Color'], axis=1)\n\ndata = pd.concat([data, spectral_encoding], axis=1)\ndata = data.drop(['Spectral_Class'], axis=1)\ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can compare data with the frist row of dataset\nfirst_row = dataset.loc[0]\ndata.values == first_row.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3 - Finally use a model for predict the measurements","metadata":{}},{"cell_type":"code","source":"# I will use the DecisionTreeClassifier\npred = dtc.predict(data)\nresult = target[0]\nprint('Prediction:', pred)\nprint('Answer:', result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion to comment\nThats what I will try to do to answer the question. Also if it's needed to do the prediction process in a single script .py, it will probably be the best option to compact this final process to a single function. The information needed to do that will be:\n\n* The model and scaler parameterss, in the Scikit-Learn documentation tells how to extract the params and then you can save it with numpy files\n* The column names from the original dataset are going to be needed in this case as I did not use OneHotEncoding module, which Scikit-Learn has [one](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n\nThat parameters are going to be needed if you want to run the model an use it to predict in a single script. Here there are documentation about how to save parameters:\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\nhttps://scikit-learn.org/stable/modules/model_persistence.html","metadata":{}}]}