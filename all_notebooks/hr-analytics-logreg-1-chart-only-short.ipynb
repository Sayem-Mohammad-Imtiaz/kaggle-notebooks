{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble, model_selection, metrics \nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/hr-analytics-case-study/general_data.csv\",sep=\",\")\nemp_surv = pd.read_csv(\"../input/hr-analytics-case-study/employee_survey_data.csv\",sep=\",\")\nman_surv = pd.read_csv(\"../input/hr-analytics-case-study/manager_survey_data.csv\",sep=\",\")\nin_time = pd.read_csv(\"../input/hr-analytics-case-study/in_time.csv\",sep=\",\")\nout_time = pd.read_csv(\"../input/hr-analytics-case-study/out_time.csv\",sep=\",\")\n\n\ny = data.Attrition\ny = np.where(y.values == 'Yes', 1, 0)\n\n# Drop constant features and EmpID - it's in index.\ndata.drop(['Attrition','EmployeeID', 'StandardHours', 'EmployeeCount', 'Over18' ], axis = 1, inplace = True)\n\n# Add surveys results to the main dataframe\nsurveys = pd.concat([man_surv, emp_surv], axis=1).drop('EmployeeID',axis=1 )\ndata = pd.concat([data, surveys], axis=1)\n\n# In and Out time data\nout_time.drop(['Unnamed: 0'], axis = 1,inplace =True )\nin_time.drop(['Unnamed: 0'], axis = 1,inplace =True )\nout_time.fillna(0,inplace =True)\nin_time.fillna(0,inplace =True)\nin_time = in_time.astype('datetime64[ns]')\nout_time = out_time.astype('datetime64[ns]') \n\n# Calculate time at the office\ntime_diff = out_time - in_time\n\n# Average office hours \nmeans_all = pd.DataFrame(round((time_diff.mean(axis = 1)/  np.timedelta64(60, 'm')),2))\nmeans_all.columns = ['Average_office_hours']\ndata = pd.concat([data, means_all], axis=1)\n\n# Split between categorical and numeric columns\ncolumns = data.columns\nnumeric_columns  = data._get_numeric_data().columns\ncategorical_columns = list(set(columns) - set(numeric_columns))\n\n# Transform numeric columns using log transformation\ndata[numeric_columns] = (data[numeric_columns] + 1).transform(np.log)\n\n\n# Change categorical features to dummies and scale data. \n# I dont use LabelEncoder since it's better to keep different feature values in different cells\n# otherwise the model could misunderstand data as 1 < 2 < 3 (Singe vs Married vs Divorced)\nX = pd.get_dummies(data = data, columns = categorical_columns)\nX_col = X.columns\n# Fill Nan\nmy_imputer = SimpleImputer(strategy = 'median')\nX = my_imputer.fit_transform(X)\n\n# Split dataset\nX_train,X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale data.\nScaler_S = StandardScaler()\n\nX_train = Scaler_S.fit_transform(X_train)\nX_test = Scaler_S.transform(X_test)\n\n\nclf = LogisticRegression().fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\nprint('Accuracy score of Logistic Regression', accuracy_score(y_test,y_pred), '\\n')\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_importance = pd.concat([pd.Series(X_col), pd.Series(clf.coef_[0])], axis=1)\nfeatures_importance.columns = ['Feature', 'Importance']\nmost_importnant_features = pd.concat([features_importance.nlargest(5, 'Importance'),\n                                      features_importance.nsmallest(5, 'Importance').sort_values(by='Importance', ascending=False)])\nplt.figure(figsize=(10,6))\nsns.set_style(\"whitegrid\")\nax = sns.barplot(y=\"Feature\", x=\"Importance\", data=most_importnant_features, palette= 'muted')\nax.set_title('Top and bottom 5 factors impacting attrition\\n', fontsize=15)\nax.set(xlabel='<---Negative correlation------------------------------Positive correlation--->', ylabel='')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Resume: Attrition risk is higher for single, harworking employees, starting their careers and receiving no promotion :) especially if they respond below average during surveys.  **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Job Satisfaction for all employees = ',round(surveys.JobSatisfaction.mean(),2))\nprint('Job Satisfaction for Attrition[0] = ',round(surveys.JobSatisfaction[y==0].mean(),2))\nprint('Job Satisfaction for Attrition[1] = ',round(surveys.JobSatisfaction[y==1].mean(),2))\nprint()\nprint('Environment Satisfaction for all employees = ',round(surveys.EnvironmentSatisfaction.mean(),2))\nprint('Environment Satisfaction for Attrition[0] = ',round(surveys.EnvironmentSatisfaction[y==0].mean(),2))\nprint('Environment Satisfaction for Attrition[1] = ',round(surveys.EnvironmentSatisfaction[y==1].mean(),2))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}