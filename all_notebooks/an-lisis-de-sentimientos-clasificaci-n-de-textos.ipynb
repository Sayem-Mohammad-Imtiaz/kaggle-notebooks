{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Análisis de sentimientos - Clasificación de críticas de películas filmaffinity\n\n* Notebook introductorio sobre clasificación de textos, aplicando algoritmos de aprendizaje sencillos.\n\n\n* Este notebook tiene como objetivo mostrar todo el proceso de clasificación de textos (análisis de sentimientos) sobre críticas de películas.\n\n\n* El proceso realizado es el siguiente:\n\n1. Carga de datos\n2. Definición del target en función de la nota de la crítica {Negativo, Neutro, Positivo}\n3. Normalización de textos\n4. Particionado de datos en entrenamiento y test\n5. Creacción del modelos de bolsa de palabras y su apliación a los textos\n6. Creacción de modelos de clasificación con algoritmos de aprendizaje sencillos de clasificación\n7. Evaluación de los modelos\n","metadata":{}},{"cell_type":"markdown","source":"## 1.- Carga de datos","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_table('../input/criticas-peliculas-filmaffinity-en-espaniol/reviews_filmaffinity.csv', sep='\\|\\|', header=0, engine='python')\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:57:48.320869Z","iopub.execute_input":"2021-07-12T10:57:48.321272Z","iopub.status.idle":"2021-07-12T10:57:48.72734Z","shell.execute_reply.started":"2021-07-12T10:57:48.321191Z","shell.execute_reply":"2021-07-12T10:57:48.726384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.- Distribución de votos (Positivo {>6} - Neutro {4-6} - Negativo {4>})\n\n1.- Creamos una nueva columna con la polaridad del voto: Positivo \\[10, 6), Neutro [6, 4], Negativo (4, 0\\].\n\n2.- Distribución numérica de los votos de las críticas.\n\n3.- Distribución categórica de la polaridad de los votos.\n\n4.- Distribución de los votos por película.","metadata":{}},{"cell_type":"code","source":"# 1.- Nueva columna con la polaridad de los votos\ndf['polaridad'] = df['review_rate'].apply(lambda  x: 'positivo' if x > 6\n                                          else ('negativo' if x < 4\n                                                else 'neutro'))\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:57:48.728702Z","iopub.execute_input":"2021-07-12T10:57:48.728988Z","iopub.status.idle":"2021-07-12T10:57:48.752419Z","shell.execute_reply.started":"2021-07-12T10:57:48.728962Z","shell.execute_reply":"2021-07-12T10:57:48.751556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2.-  Distribución numérica de los votos de las críticas.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nsns.catplot(x='review_rate', kind='count', color='b', data=df)\nplt.title('Distribución Votos')\nplt.xlabel('Notas')\nplt.ylabel('Nº Votos')","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:57:48.754308Z","iopub.execute_input":"2021-07-12T10:57:48.754598Z","iopub.status.idle":"2021-07-12T10:57:49.887719Z","shell.execute_reply.started":"2021-07-12T10:57:48.754569Z","shell.execute_reply":"2021-07-12T10:57:49.886795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3.-  Distribución de votos por polaridad\n\nsns.catplot(x='polaridad', kind='count', data=df,  order=['negativo', 'neutro', 'positivo'])\nplt.title('Distribución Polaridad')\nplt.xlabel('Polaridad')\nplt.ylabel('Nº Votos')","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:57:49.889583Z","iopub.execute_input":"2021-07-12T10:57:49.889852Z","iopub.status.idle":"2021-07-12T10:57:50.115653Z","shell.execute_reply.started":"2021-07-12T10:57:49.889827Z","shell.execute_reply":"2021-07-12T10:57:50.114751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4.- Distribución de los votos por película.\n\nsns.catplot(x=\"review_rate\", col=\"film_name\", data=df, kind='count', col_wrap=3)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:57:50.116802Z","iopub.execute_input":"2021-07-12T10:57:50.117065Z","iopub.status.idle":"2021-07-12T10:58:00.469605Z","shell.execute_reply.started":"2021-07-12T10:57:50.117039Z","shell.execute_reply":"2021-07-12T10:58:00.468497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.- Normalización de textos \n\n* Vamos a definir como texto a clasificar título y la crítica, ya que el título aporta significado al texto.\n\n\n* En este punto realizaremos los siguientes pasos:\n\n1. Concatenación de título y crítica\n1. Pasamos a array de numpy el texto y el target (polaridad)\n1. Importamos el modelo de spacy en español\n1. Normalización de los textos: La normalización es una tarea que tiene como objetivo poner todo el texto en igualdad de condiciones; como por ejemplo:\n        \n    * Pasar todo el texto a minúsculas (o mayúsculas)\n    * Eliminar signos de puntuación (puntos, comas, comillas, etc)\n    * Quitar las stop-words: palábras que no aportan significado a los textos\n    * Convertir números a su equivalente a palabras\n    * Transformar la palabra a su lema\n    * Pasar emoticonos a textos\n    * etc.","metadata":{}},{"cell_type":"code","source":"# 1.- Concatenación de título y crítica\n\ndf['texto'] = df['review_title'] + ' ' + df['review_text']\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:58:00.47117Z","iopub.execute_input":"2021-07-12T10:58:00.471573Z","iopub.status.idle":"2021-07-12T10:58:00.520212Z","shell.execute_reply.started":"2021-07-12T10:58:00.471538Z","shell.execute_reply":"2021-07-12T10:58:00.519331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2.- Pasamos a array de numpy el texto (X) y el target-polaridad (y)\n\nX = df['texto'].values\ny =  df['polaridad'].values\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:58:00.521403Z","iopub.execute_input":"2021-07-12T10:58:00.521702Z","iopub.status.idle":"2021-07-12T10:58:00.526794Z","shell.execute_reply.started":"2021-07-12T10:58:00.521664Z","shell.execute_reply":"2021-07-12T10:58:00.525861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3.- Importamos el modelo de spacy en español\n\nimport spacy\n\n# Este comando se ejecuta en consola\n!python -m spacy download es","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:58:00.529748Z","iopub.execute_input":"2021-07-12T10:58:00.530181Z","iopub.status.idle":"2021-07-12T10:58:15.666332Z","shell.execute_reply.started":"2021-07-12T10:58:00.530122Z","shell.execute_reply":"2021-07-12T10:58:15.665268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**NOTA:** *Si ejecutais este notebook en local, es posible que os de un error a la hora de importar el modelo de NLP en español. Si da ese error debéis de cambiar el nombre del modelo de 'es' a 'es_core_news_sm' o como se indique en el proceso de importación del modelo de Spacy.*","metadata":{}},{"cell_type":"code","source":"import re\n\nfrom tqdm import tqdm\n\n# Importamos el modelo en español de spacy\nnlp = spacy.load('es')\n\n\ndef normalize(corpus):\n    \"\"\"Función que dada una lista de textos, devuelve esa misma lista de textos\n       con los textos normalizados, realizando las siguientes tareas:\n       1.- Pasamos la palabra a minúsculas\n       2.- Elimina signos de puntuación\n       3.- Elimina las palabras con menos de 3 caracteres (palabras que seguramente no aporten significado)\n       4.- Elimina las palabras con más de 11 caracteres (palabras \"raras\" que seguramente no aporten significado)\n       5.- Elimina las stop words (palabras que no aportan significado como preposiciones, determinantes, etc.)\n       6.- Elimina los saltos de línea (en caso de haberlos)\n       7.- Eliminamos todas las palabras que no sean Nombres, adjetivos, verbos o advervios\n    \"\"\"\n    for index, doc in enumerate(tqdm(corpus)):\n        doc = nlp(doc.lower())\n        corpus[index] = \" \".join([word.lemma_ for word in doc if (not word.is_punct)\n                                  and (len(word.text) > 3) \n                                  and (len(word.text) < 11) \n                                  and (not word.is_stop)\n                                  and re.sub('\\s+', ' ', word.text)\n                                  and (word.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV'])])\n        \n        \n    return corpus\n\n# Normalización\nX_norm = normalize(X)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T10:58:15.668094Z","iopub.execute_input":"2021-07-12T10:58:15.668363Z","iopub.status.idle":"2021-07-12T11:03:34.840523Z","shell.execute_reply.started":"2021-07-12T10:58:15.668335Z","shell.execute_reply":"2021-07-12T11:03:34.839793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.- Bolsa de Palabras (BoW) - Particionado de datos","metadata":{}},{"cell_type":"code","source":"# 1.- Particionamos los textos en entrenamiento y test (80% entrenamiento, 20% test)\n\nfrom sklearn.model_selection import train_test_split  \n\nX_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2, random_state=0)\n\nprint('Textos de entrenamiento: {}'.format(len(X_train)))\nprint('Textos de test: {}'.format(len(X_test)))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T11:03:34.8415Z","iopub.execute_input":"2021-07-12T11:03:34.841742Z","iopub.status.idle":"2021-07-12T11:03:35.017184Z","shell.execute_reply.started":"2021-07-12T11:03:34.841716Z","shell.execute_reply":"2021-07-12T11:03:35.016333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Mostramos la distribución del target de los datos de entrenamiento y test para ver si siguen una distribución similar.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nkeys_train, counts_train = np.unique(y_train, return_counts=True)\nkeys_test, counts_test = np.unique(y_test, return_counts=True)\nperct_train = counts_train / np.sum(counts_train)\nperct_test = counts_test / np.sum(counts_test)\n\nplt.figure(figsize=(15, 15))\nplt.subplot(1, 2, 1)\nplt.pie(perct_train, labels=keys_train, autopct='%1.1f%%')\nplt.title('Distribución Target Train')\nplt.subplot(1, 2, 2)\nplt.pie(perct_test, labels=keys_test, autopct='%1.1f%%')\nplt.title('Distribución Target Test')\nplt.plot()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T11:03:35.018758Z","iopub.execute_input":"2021-07-12T11:03:35.019221Z","iopub.status.idle":"2021-07-12T11:03:35.247307Z","shell.execute_reply.started":"2021-07-12T11:03:35.019175Z","shell.execute_reply":"2021-07-12T11:03:35.246347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creamos una bolsa de palabras de frecuencias con los textos de entrenamiento.\n\n* Creamos un modelo de bolsa de palabras con las 2000 palabras más frecuentes de los textos de entrenamiento que aparezcan por lo menos en 3 documentos distintos.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\nbow = CountVectorizer(max_features=2000, min_df=3)\n\n# Creamos el modelo de bolsa de palabras con los textos de entrenamiento y aplicamos el modelo\nX_bow_train = bow.fit_transform(X_train)\n\n# A modo de ejemplo mostramos las 20 primeras palabras de la bolsa de palabras\nbow.get_feature_names()[0:21]","metadata":{"execution":{"iopub.status.busy":"2021-07-12T11:03:35.248704Z","iopub.execute_input":"2021-07-12T11:03:35.249072Z","iopub.status.idle":"2021-07-12T11:03:35.869829Z","shell.execute_reply.started":"2021-07-12T11:03:35.249034Z","shell.execute_reply":"2021-07-12T11:03:35.868958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Con el modelo de bolsa de palabras creado con los textos de entrenamiento, aplicamos el modelo a los textos de test.","metadata":{}},{"cell_type":"code","source":"X_bow_test = bow.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T11:03:35.871066Z","iopub.execute_input":"2021-07-12T11:03:35.871456Z","iopub.status.idle":"2021-07-12T11:03:36.028362Z","shell.execute_reply.started":"2021-07-12T11:03:35.871426Z","shell.execute_reply":"2021-07-12T11:03:36.027576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.- Creacción de modelos (clasificación)\n\nUtilizamos los siguientes algoritmos (o metaalgoritmos) de aprendizaje de clasificación para crear modelos predictivos capaces de clasificar una crítica de pelicula en alguna de las siguientes clases: {Negativa, Neutra, Positiva}\n\n* Multinomial Naive Bayes\n* Bernoulli Naive Bayes\n* Regresion Logistica\n* Support Vector Machine\n* Random Forest (ensemble)","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nmnb = MultinomialNB()\nbnb = BernoulliNB()\nlr = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000)\nsvm_lin = SVC(kernel='linear')\nsvm_rbf = SVC(kernel='rbf')\nrf_20 = RandomForestClassifier(n_estimators=500, bootstrap=True, criterion='gini', max_depth=20, random_state=0)\nrf_50 = RandomForestClassifier(n_estimators=500, bootstrap=True, criterion='gini', max_depth=50, random_state=0)\n\nclasificadores = {'Multinomial NB': mnb,\n                  'Bernoulli NB': bnb,\n                  'Regresion Logistica': lr,\n                  'SVM lineal': svm_lin,\n                  'SVM Kernel rbf': svm_rbf,\n                  'Random Forest d_20': rf_20,\n                  'Random Forest d_50': rf_50}\n\n\n# Ajustamos los modelos y calculamos el accuracy para los datos de entrenamiento\nfor k, v in clasificadores.items():\n    print ('CREANDO MODELO: {clas}'.format(clas=k))\n    v.fit(X_bow_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T11:03:36.029368Z","iopub.execute_input":"2021-07-12T11:03:36.029738Z","iopub.status.idle":"2021-07-12T11:05:07.024315Z","shell.execute_reply.started":"2021-07-12T11:03:36.02971Z","shell.execute_reply":"2021-07-12T11:05:07.023223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.- Evaluación de los modelos\n\n* Para cada uno de los modelos vamos a calcular las siguientes métricas de evaluación:\n\n1. Accuracy\n1. Precision\n1. Recall\n1. F1","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n\ndef evaluation(model, name, X_train, y_train, X_test, y_test):\n    \"\"\"\n    Función de devuelve en un diccionario las métricas de evaluación de \n    Accuracy, Precision, Recall y F1 para los conjuntos de datos de entrenamiento y test\n        model: modelo a evaluar\n        name: nombre del modelo\n        X_train: Variables de entrada del conjunto de datos de entrenamiento\n        y_train: Variable de salida del conjunto de datos de entrenamiento\n        X_test: Variables de entrada del conjunto de datos de test\n        y_test: Variable de salida del conjunto de datos de test\n        return: diccionario con el nombre del modelo y el valor de las métricas\n    \"\"\"\n    model_dict = {}\n    model_dict['name'] = name\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    model_dict['accuracy_train'] = accuracy_score(y_true=y_train, y_pred=y_pred_train)\n    model_dict['accuracy_tests'] = accuracy_score(y_true=y_test, y_pred=y_pred_test)\n    model_dict['precision_train'] = precision_score(y_true=y_train, y_pred=y_pred_train, average='weighted')\n    model_dict['precision_tests'] = precision_score(y_true=y_test, y_pred=y_pred_test, average='weighted')\n    model_dict['recall_train'] = recall_score(y_true=y_train, y_pred=y_pred_train, average='weighted')\n    model_dict['recall_tests'] = recall_score(y_true=y_test, y_pred=y_pred_test, average='weighted')\n    model_dict['f1_train'] = f1_score(y_true=y_train, y_pred=y_pred_train, average='weighted')\n    model_dict['f1_tests'] = f1_score(y_true=y_test, y_pred=y_pred_test, average='weighted')\n    \n    return model_dict\n\n\n# Calculamos las métricas de los modelos por separado\nevaluacion = list()\nfor key, model in clasificadores.items():\n    evaluacion.append(evaluation(model=model, name=key, \n                                 X_train=X_bow_train, y_train=y_train,\n                                 X_test=X_bow_test, y_test=y_test))\n\n# Pasamos los resultados a un DataFrame para visualizarlos mejor\ndf = pd.DataFrame.from_dict(evaluacion)\ndf.set_index(\"name\", inplace=True)\ndf","metadata":{"execution":{"iopub.status.busy":"2021-07-12T11:05:07.025742Z","iopub.execute_input":"2021-07-12T11:05:07.026079Z","iopub.status.idle":"2021-07-12T11:05:47.084934Z","shell.execute_reply.started":"2021-07-12T11:05:07.026049Z","shell.execute_reply":"2021-07-12T11:05:47.084058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Representamos las métricas para los diferentes modelos en un gráfico de barras:","metadata":{}},{"cell_type":"code","source":"# Métricas a pintar\nMETRICS = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n\n# Transformamos el dataframe para pintar las gráficas con seaborn\ndf_plot = df.reset_index().melt(id_vars='name').rename(columns=str.title)\n\nplt.figure(figsize=(25, 12))\npos = 1\nfor metric in METRICS:\n    # Filtramos la métrica a pintar\n    df_aux = df_plot[df_plot['Variable'].str.contains(metric)]\n    \n    # Pintamos la gráfica en su posición 2x2\n    plt.subplot(2, 2, pos)\n    sns.barplot(x='Name', y='Value', hue='Variable', data=df_aux)\n    plt.title(metric.upper())\n    plt.grid()\n    plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n    plt.xticks(rotation=20)\n    pos += 1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T11:05:47.086343Z","iopub.execute_input":"2021-07-12T11:05:47.086736Z","iopub.status.idle":"2021-07-12T11:05:47.937496Z","shell.execute_reply.started":"2021-07-12T11:05:47.086695Z","shell.execute_reply":"2021-07-12T11:05:47.936529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Dibujamos las matrices de confusión de cada uno de los modelos creados para los textos de entrenamiento y test","metadata":{}},{"cell_type":"code","source":"import itertools\n\nfrom sklearn.metrics import confusion_matrix\n\npolaridad = ['positivo', 'neutro', 'negativo']\n\n# Obtenemos las Matrices de confusión\nmsc = list()\nfor k, v in clasificadores.items():\n    print ('Obteniendo Matriz de Confusión de: {model}'.format(model=k))\n    model = {}\n    model['name'] = k\n    y_pred_train = v.predict(X_bow_train)\n    y_pred_test = v.predict(X_bow_test)\n    model['confusion_matrix_train'] = confusion_matrix(y_true=y_train, y_pred=y_pred_train, labels=polaridad)\n    model['confusion_matrix_test'] = confusion_matrix(y_true=y_test, y_pred=y_pred_test, labels=polaridad)\n    msc.append(model)\n\n    \n# Definimos el heatmap de la matriz de confusión\ndef plot_confusion_matrix(cm, classes, title, cmap=plt.cm.Greens):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], 'd'), horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n\n# Pintamos las matrices de confusión\nplt.figure(figsize=(20, 35))\npos = 0\nfor mc in msc:\n    pos += 1\n    plt.subplot(len(msc), 2, pos)\n    plot_confusion_matrix(mc['confusion_matrix_train'], classes=polaridad, \n                          title='{}\\nMatriz de Confusión Textos Entrenamiento'.format(mc['name']))\n    pos += 1\n    plt.subplot(len(msc), 2, pos)\n    plot_confusion_matrix(mc['confusion_matrix_test'], classes=polaridad, \n                          title='{}\\nMatriz de Confusión Textos Tests'.format(mc['name'] ))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T11:05:47.939041Z","iopub.execute_input":"2021-07-12T11:05:47.939459Z","iopub.status.idle":"2021-07-12T11:06:31.332687Z","shell.execute_reply.started":"2021-07-12T11:05:47.939407Z","shell.execute_reply":"2021-07-12T11:06:31.332022Z"},"trusted":true},"execution_count":null,"outputs":[]}]}