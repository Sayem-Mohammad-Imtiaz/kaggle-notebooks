{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Linear Regression Using Gradient Descent"},{"metadata":{},"cell_type":"markdown","source":"**NIM** : 2301914434\n\n**Name**: Ananto Joyoadikusumo\n\n**Notebook Link** : https://www.kaggle.com/anantoj/linear-regression-asg"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Generate Random Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1)\nX = np.array(sorted(list(range(5))*20)) + np.random.normal(size=100, scale=0.5)\ny = np.array(sorted(list(range(5))*20)) + np.random.normal(size=100, scale=0.25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot Our Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.scatter(X, y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DIY Linear Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LinearRegressor:\n    def __init__(self, learning_rate=0.01, iterations=1000):\n        self.learning_rate, self.iterations = learning_rate, iterations\n    \n    def fit(self, X, y, show_iter=False):\n        c = 0 # intercept\n        m = 0 # slope or gradient\n        n = X.shape[0]\n        for i in range(self.iterations):\n            \n            c_gradient = -2 * np.sum(y - (m*X + c)) / n\n            m_gradient = -2 * np.sum(X * (y - (m*X + c))) / n\n            \n            c = c - (self.learning_rate * c_gradient)\n            m = m - (self.learning_rate * m_gradient)\n            self.m, self.c = m, c\n            \n            if show_iter == True:\n                if i < 50 and i % 10 == 0:\n                    plt.figure(figsize=(5,5))\n                    plt.gca().set_title(\"Iteration \" + str(i))\n                    plt.scatter(X, y)\n                    plt.plot(X, self.predict(X), color='red')\n                    plt.show()\n            \n        \n    def predict(self, X):\n        return self.m*X + self.c","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create and Fit Our Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_0 = LinearRegressor()\nmodel_0.fit(X, y, show_iter = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Result"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nplt.gca().set_title(\"Gradient Descent Linear Regressor\")\nplt.scatter(X, y)\n\nplt.plot(X, model_0.predict(X), color='red')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Real Implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nX = X.reshape(-1,1)\ny = y.reshape(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nmodel_1 = LinearRegression()\nmodel_1.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_1.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model_1.score(X_test, y_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}