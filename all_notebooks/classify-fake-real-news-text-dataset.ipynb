{"cells":[{"metadata":{},"cell_type":"markdown","source":"# In this kernel I’ll test fake & real news dataset – I’ll use different classification methods ."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import EarlyStopping\n\n\nimport nltk\nimport nltk as nlp\nimport string\nimport re\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"combine fake/real to dataset"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"real = pd.read_csv(\"../input/fake-and-real-news-dataset/True.csv\")\nfake = pd.read_csv(\"../input/fake-and-real-news-dataset/Fake.csv\")\nfake['target'] = 'fake'\nreal['target'] = 'real'\ntxt_news = pd.concat([fake, real]).reset_index(drop = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_news.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# check if we have null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_news.isnull().all()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of examples : ' + str(txt_news.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target=txt_news.target\n#txt_news=txt_news.drop(columns=['date', 'subject','target'])\ntxt_news['total']=txt_news['text']+' '+txt_news['subject']+txt_news['title']\n\ntarget_binary=[]\nfor false_true in target :\n    if false_true==\"fake\":\n        target_binary.append(0)\n    else :\n        target_binary.append(1)\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tfidf\n#transformer = TfidfTransformer(smooth_idf=False)\n#count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n#counts = count_vectorizer.fit_transform(txt_news['total'].values)\n#tfidf = transformer.fit_transform(counts)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# split train/test"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(txt_news['total'], target, test_size=0.25, random_state=42) #txt_news['text']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([('CountV', CountVectorizer()),\n                 ('TfidfT', TfidfTransformer()),\n                 ('clf', KNeighborsClassifier(n_neighbors = 30,weights = 'distance'))])#algorithm = 'brute'\n\nmodel = pipeline.fit(x_train, y_train)\npred = model.predict(x_test)\nprint(\"accuracy KNeighborsClassifier: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"\npipeline = Pipeline([('CountV', CountVectorizer()),\n                 ('TfidfT', TfidfTransformer()),\n                 ('clf', LinearSVC(C=12, random_state=7))])#loss=\"hinge\"\n\nmodel = pipeline.fit(x_train, y_train)\npred = model.predict(x_test)\nprint(\"accuracy SVM: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ADABoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\npipeline = Pipeline([('CountV', CountVectorizer()),\n                 ('TfidfT', TfidfTransformer()),\n                 ('clf', AdaBoostClassifier())])\n\nmodel = pipeline.fit(x_train, y_train)\npred = model.predict(x_test)\nprint(\"accuracy AdaBoostClassifier: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MultinomialNB"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([('CountV', CountVectorizer()),\n                 ('TfidfT', TfidfTransformer()),\n                 ('clf', MultinomialNB(alpha=0.5))])\n\nmodel = pipeline.fit(x_train, y_train)\npred = model.predict(x_test)\nprint(\"accuracy MultinomialNB: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([('CountV', CountVectorizer()),\n                 ('TfidfT', TfidfTransformer()),\n                 ('clf', XGBClassifier(\n                                                   learning_rate = 0.015,\n                                                   n_estimators = 18,\n                                                   max_depth = 7,\n                                                   random_state=42))])#loss = 'deviance',\n\nmodel = pipeline.fit(x_train, y_train)\npred = model.predict(x_test)\nprint(\"accuracy XGBClassifier: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(objective='multiclass', random_state=5)\n\npipe = Pipeline([('CountV', CountVectorizer()),\n                 ('TfidfT', TfidfTransformer()),\n                 ('clf', LGBMClassifier(\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 18,\n                                                   max_depth = 7,\n                                                   random_state=42))])#loss = 'deviance',\n\nmodel = pipe.fit(x_train, y_train)\npred = model.predict(x_test)\nprint(\"accuracy LGBMClassifier: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM with Grid search"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom pprint import pprint\nfrom time import time\n\npipeline = Pipeline([('CountV', CountVectorizer()),\n                 ('TfidfT', TfidfTransformer()),\n                 ('clf', XGBClassifier()),])\n\nparameters = {}\nparameters['clf__learning_rate'] = [0.01]\nparameters['clf__n_estimators'] = [15,20]\nparameters['clf__max_depth'] = [7,9]\nparameters['clf__random_state'] = [42]\n\n\n\n    \n    \nCV = GridSearchCV(pipe, parameters, n_jobs= 1) # scoring = 'mean_absolute_error'\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in pipeline.steps])\nprint(\"parameters:\")\npprint(parameters)\nt0 = time()\n\nmodel = CV.fit(x_train, y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint()\n\nprint(\"Best score: %0.3f\" % CV.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = CV.best_estimator_.get_params()\nfor param_name in sorted(parameters.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n\n\nprediction = CV.predict(x_test)\nprint(\"accuracy LGBMClassifier with GridSearchCV: {}%\".format(round(accuracy_score(y_test, prediction)*100,3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline = Pipeline([('CountV', CountVectorizer()),\n                 ('TfidfT', TfidfTransformer()),\n                 ('clf', RandomForestClassifier(criterion= \"entropy\"))])\n\nmodel = pipe.fit(x_train, y_train)\npred = model.predict(x_test)\nprint(\"accuracy RandomForestClassifier: {}%\".format(round(accuracy_score(y_test, pred)*100,3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = txt_news.total\nY = txt_news.target\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get longest txt length"},{"metadata":{"trusted":true},"cell_type":"code","source":"len_size=0\nfor f in X:\n    if len_size<len(f) :\n        len_size=len(f)\n        \nprint(str(len_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.Series({c: txt_news[c].map(lambda x: len(str(x))).max() for c in txt_news}).sort_values(ascending =False))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\nmax_words = 500\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\ndef LSTM_MODEL():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model\n\n\nmodel_lstm = LSTM_MODEL()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.utils import plot_model \nplot_model(model_lstm, to_file='lstm_png1.png')\nmodel_lstm.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lstm.fit(sequences_matrix,Y_train,batch_size=256,epochs=10,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\naccr = model_lstm.evaluate(test_sequences_matrix,Y_test)\nprint('Accuracy LSTM: {:0.2f}'.format(accr[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers.recurrent import LSTM, GRU\n\ndef GRU_model():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = GRU(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model\n\nmodel_gru = GRU_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(model_gru, to_file='gru_png1.png')\nmodel_gru.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_gru.fit(sequences_matrix,Y_train,batch_size=256,epochs=10,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accr = model_gru.evaluate(test_sequences_matrix,Y_test)\nprint('Accuracy GRU: {:0.2f}'.format(accr[1]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}