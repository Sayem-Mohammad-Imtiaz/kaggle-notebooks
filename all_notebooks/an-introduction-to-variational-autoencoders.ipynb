{"cells":[{"metadata":{},"cell_type":"markdown","source":"# An Introduction to Variational Autoencoders\n\nVariational autoencoders (VAEs) are one of the most interesting applications of deep learning. Although they are not used for many practical applications, they still provide an insight into the power of deep generative modeling.  \n  \n<img src=\"https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-06-at-3.17.13-PM.png\" width=700>  \n  \n* The basic idea behind an autoencoder is to generate a low-dimensional (latent) representation of a high-dimensional input.\n* We achieve this by asking the model to simply recreate the input it is given. However, we impose an *information bottleneck* upon the model, so that it is forced to lose a massive amount of information from the original input in the process.\n* The model is therefore encouraged to encode and retain as much useful information as it passes through the bottleneck. This results in the development of two submodels: an **encoder** and a **decoder**.  \n* The **encoder** is the part of the model *before the bottleneck*: devoted to encoding the input into a information-rich low-dimensional form. We call this low-dimensional form a **latent representation**.\n* The **decoder** is the part of the model *after the bottleneck*: devoted to recreating the original image from the latent representation.  \n  \nA *variational* autoencoder works in the same way, but instead of directly learning the latent representation, we learn parameters μ (mean) and σ (standard deviation) for a probability distribution from which we sample the latent representation.  \n  \n<img src=\"https://i.stack.imgur.com/49HNA.png\" width=700>  \n  \nThis notebook aims to provide an intuitive understanding of variational autoencoders by demonstrating their capabilities in a visual manner.  \n  \n#### **NOTE:** *In order to use the widgets in this notebook, you must make a copy and run the code yourself.*  \n  \n*Images courtesy of www.jeremyjordan.me*"},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"# For working with and visualizing the data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# For training the VAE\nimport tensorflow as tf\n\n# For creating interactive widgets\nimport ipywidgets as widgets\nfrom IPython.display import display","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data from a .csv file\npixel_data = pd.read_csv('../input/age-gender-and-ethnicity-face-data-csv/age_gender.csv')['pixels']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffle the data\npixel_data = pixel_data.sample(frac=1.0, random_state=1)\n# Convert the data into a NumPy array\npixel_data = pixel_data.apply(lambda x: np.array(x.split(\" \"), dtype=np.int))\npixel_data = np.stack(np.array(pixel_data), axis=0)\n# Rescale pixel values to be between 0 and 1\npixel_data = pixel_data * (1./255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The data is now a NumPy array of 23705 images (each represented as a 1-D vector of 2304 pixels)\n# (2304 is 48^2, so we are working with 48x48x1 images)\npixel_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the VAE  \n  \nWe need to create a custom Sampling layer to sample the latent variables from a normal distribution with mean and variance given by the encoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Sampling(tf.keras.layers.Layer):\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return epsilon * tf.exp(z_log_var * 0.5) + z_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vae(num_pixels, num_latent_vars=3):\n    \n    # Encoder\n    encoder_inputs = tf.keras.Input(shape=(num_pixels,))\n    x = tf.keras.layers.Dense(512, activation='relu')(encoder_inputs)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = tf.keras.layers.Dense(32, activation='relu')(x)\n    z_mean = tf.keras.layers.Dense(num_latent_vars)(x)\n    z_log_var = tf.keras.layers.Dense(num_latent_vars)(z_mean)\n    z = Sampling()([z_mean, z_log_var])\n    \n    encoder = tf.keras.Model(inputs=encoder_inputs, outputs=z)\n    \n    # Decoder\n    decoder_inputs = tf.keras.Input(shape=(num_latent_vars,))\n    x = tf.keras.layers.Dense(32, activation='relu')(decoder_inputs)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = tf.keras.layers.Dense(512, activation='relu')(x)\n    reconstruction = tf.keras.layers.Dense(num_pixels, activation='linear')(x)\n    \n    decoder = tf.keras.Model(inputs=decoder_inputs, outputs=reconstruction)\n    \n    # Full model\n    model_inputs = encoder.input\n    model_outputs = decoder(encoder.output)\n    \n    model = tf.keras.Model(inputs=model_inputs, outputs=model_outputs)\n    \n    # Compile model for training\n    model.compile(\n        optimizer='adam',\n        loss='mse'\n    )\n    \n    # Return all three models\n    return encoder, decoder, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"face_encoder, face_decoder, face_model = build_vae(num_pixels=2304, num_latent_vars=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(face_encoder.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(face_decoder.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the VAE  \n  \nWe will use *pixel_data* as both the input to the model and the target to compare the output to."},{"metadata":{"trusted":true},"cell_type":"code","source":"history = face_model.fit(\n    pixel_data,\n    pixel_data,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Reconstruction  \n  \nLet's see how the model does at reconstructing an image that it has already seen."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ni = 6\n\nsample = np.array(pixel_data)[i].copy()\nsample = sample.reshape(48, 48, 1)\n\nreconstruction = face_model.predict(pixel_data)[i].copy()\nreconstruction = reconstruction.reshape(48, 48, 1)\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.imshow(sample, cmap='gray')\nplt.axis('off')\nplt.title(\"Original Image\")\n\nplt.subplot(1, 2, 2)\nplt.imshow(reconstruction, cmap='gray')\nplt.axis('off')\nplt.title(\"Reconstructed Image\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Specify our own latent variable values  \n  \nNow let's see how we can use our own values to generate never-before-seen images."},{"metadata":{"trusted":true},"cell_type":"code","source":"# A function to allow us to specify our own latent variable values and plot the constructed image\ndef generate_face_image(latent1, latent2, latent3):\n    latent_vars = np.array([[latent1, latent2, latent3]])\n    reconstruction = np.array(face_decoder(latent_vars))\n    reconstruction = reconstruction.reshape(48, 48, 1)\n    plt.figure()\n    plt.imshow(reconstruction, cmap='gray')\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's get the min and max for each slider on the interactive widget\nlatent1_min = np.min(face_encoder(pixel_data).numpy()[:, 0])\nlatent1_max = np.max(face_encoder(pixel_data).numpy()[:, 0])\n\nlatent2_min = np.min(face_encoder(pixel_data).numpy()[:, 1])\nlatent2_max = np.max(face_encoder(pixel_data).numpy()[:, 1])\n\nlatent3_min = np.min(face_encoder(pixel_data).numpy()[:, 2])\nlatent3_max = np.max(face_encoder(pixel_data).numpy()[:, 2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using ipywidgets, we can create a cool interactive widget for visualizing the results\n# NOTE: You must edit the notebook to be able to use the widget\nface_image_generator = widgets.interact(\n    generate_face_image,\n    latent1=(latent1_min, latent1_max),\n    latent2=(latent2_min, latent2_max),\n    latent3=(latent3_min, latent3_max),\n)\n\ndisplay(face_image_generator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# YouTube Tutorial Included!  \n  \n***\n  \nThis notebook was made to accompany a YouTube video that I made for my channel.  \n  \nIf you want an in-depth explanation of the steps taken, you can check out the video here:  \nhttps://youtu.be/ZfxNcO6BqDo"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}