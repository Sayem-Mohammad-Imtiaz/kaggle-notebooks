{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Pima Indians Diabetes\n\nOur objective is to predict whether a patient is suffering or not form diabetes. \n\nOur input values include \"number of pregnancies the patient has had, their BMI, insulin level, age\" and others.\nOur ouput value will be a boolean, i.e 1 if he has diabetes and 0 if not."},{"metadata":{},"cell_type":"markdown","source":"### Loading of packages and dataset path"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Pre-Processing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#we load the dataset\ndf = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 768 rows in our dataset with 9 columns (8 predictor variables and 1 outcome variable)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### N/A Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't have any N/A value"},{"metadata":{},"cell_type":"markdown","source":"## Machine Learning"},{"metadata":{},"cell_type":"markdown","source":"### X/Y definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Outcome'], axis = 1)\ny = df.Outcome","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train/Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Testing different models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nclassifiers = [\n    SVC(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GradientBoostingClassifier(),\n    KNeighborsClassifier(),\n    LogisticRegression()\n    ]\n\n\nfor clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \nprint(\"=\"*30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Baselines\n\nTo conclude on the accuracy of our models, we need to compare them with different baselines.\n\nThese baselines will be given by the Dummy Classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.dummy import DummyClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dc_strat = DummyClassifier(strategy = \"stratified\")\ndc_strat.fit(X_train, y_train)\n\ndc_freq = DummyClassifier(strategy = \"most_frequent\")\ndc_freq.fit(X_train, y_train)\n\nprint(\"DC Stratified accuracy : \", dc_strat.score(X_test, y_test))\nprint(\"DC Most Frequent accuracy : \", dc_freq.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parameters tuning : SVC\n\nWe want to improve the accuracy we have with our classifiers. To do so, we can tune their parameters.\n\nLet's pick our SVC model. We will tune its paramters thanks to the Cross-Validation tool of Grid Search."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the parameters by cross-validation\nparameters = {'kernel': ['rbf'],  'C': [1, 10, 100, 1000]}\n\n\nclf = GridSearchCV(SVC(), parameters)\nclf.fit(X_train, y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_params_)\nprint()\nprint(\"Best score:\")\nprint()\nprint(clf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Problème score GridSearchCV\n\n<span style=\"color:red\"> La précision du modèle avec hyperparamètres optimisés par GridSearch CV est moins bonne que la précision du modèle à vide... cf question posée en cours, je ne vois pas d'où peut venir le problème. Dans la documentation du SVC, la valeur par défaut de C est 1, et la valeur de kernel est 'rbf', que j'ai bien inclus dans la liste des hyperparamètres à tester."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}