{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# 样本文件路径\nSAMPLE_FILE_PATH = \"../input/hahahaha/icml_face_data.csv\"\n\n# 分类数量\nNUM_CLASSES = 7\n\n# 训练、校验、测试数据集HDF5文件的输出路径\nTRAIN_HDF5 = \"./train.hdf5\"\nVAL_HDF5 = \"./val.hdf5\"\nTEST_HDF5 = \"./test.hdf5\"\n\n# 每批次样本数量\nBATCH_SIZE = 128\n\n# 项目输出文件保存目录\nOUTPUT_PATH = \"./\"\n\n# 数据集样本RGB平均值存位置及文件名称\nDATASET_MEAN_FILE = OUTPUT_PATH + \"/rgb_mean.json\"\n\n# 模型保存位置及文件名称\nMODEL_FILE = OUTPUT_PATH + \"/model.h5\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 导入必要包\nfrom tensorflow.keras.callbacks import Callback\nimport os\n\n\n\nclass EpochCheckpoint(Callback):\n    def __init__(self, output_path, every=5, start_at=0):\n        super(Callback, self).__init__()\n        self.output_path = output_path\n        self.every = every\n        self.start_epoch = start_at\n\n    def on_epoch_end(self, epoch, logs={}):\n        if (self.start_epoch + 1) % self.every == 0:\n            p = os.path.sep.join([self.output_path,\n                                  \"epoch_{}.hdf5\".format(self.start_epoch + 1)])\n            self.model.save(p, overwrite=True)\n        self.start_epoch += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport h5py\nfrom tensorflow.python.keras.utils import np_utils\n\n\nclass HDF5DatasetGenerator:\n    def __init__(self, db_file, batch_size, preprocessors=None,\n                 aug=None, binarize=True, classes=2):\n        self.batchSize = batch_size\n        self.preprocessors = preprocessors\n        self.aug = aug\n        self.binarize = binarize\n        self.classes = classes\n        self.db = h5py.File(db_file)\n        self.numImages = self.db[\"labels\"].shape[0]\n\n    def generator(self, passes=np.inf):\n        epochs = 0\n        while epochs < passes:\n            for i in np.arange(0, self.numImages, self.batchSize):\n                images = self.db[\"images\"][i:i + self.batchSize]\n                labels = self.db[\"labels\"][i:i + self.batchSize]\n\n                if self.binarize:\n                    labels = np_utils.to_categorical(labels, self.classes)\n\n                if self.preprocessors is not None:\n                    processed_images = []\n                    for image in images:\n                        for p in self.preprocessors:\n                            image = p.preprocess(image)\n                        processed_images.append(image)\n                    images = np.array(processed_images)\n\n                if self.aug is not None:\n                    (images, labels) = next(self.aug.flow(images, labels,\n                                                          batch_size=self.batchSize))\n                yield images, labels\n            epochs += 1\n\n    def close(self):\n        self.db.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport h5py\n\n\nclass HDF5DatasetWriter:\n    def __init__(self, dims, output_path, data_key=\"images\", buf_size=1000):\n        if os.path.exists(output_path):\n            raise ValueError('你提供的输出文件{}已经存在，请先手工输出'.format(output_path))\n        self.db = h5py.File(output_path, 'w')\n        self.data = self.db.create_dataset(data_key, dims, dtype=\"float\")\n        self.labels = self.db.create_dataset(\"labels\", (dims[0],), dtype=\"int\")\n\n        self.buf_size = buf_size\n        self.buffer = {\"data\": [], \"labels\": []}\n        self.idx = 0\n\n    def add(self, raw, label):\n        self.buffer[\"data\"].extend(raw)\n        self.buffer[\"labels\"].extend(label)\n        if len(self.buffer[\"data\"]) >= self.buf_size:\n            self.flush()\n\n    def flush(self):\n        i = self.idx + len(self.buffer[\"data\"])\n        self.data[self.idx:i] = self.buffer[\"data\"]\n        self.labels[self.idx:i] = self.buffer[\"labels\"]\n        self.idx = i\n        self.buffer = {\"data\": [], \"labels\": []}\n\n    def store_class_labels(self, class_labels):\n        dt = h5py.special_dtype(vlen=str)\n        label_dim = (len(class_labels),)\n        label_set = self.db.create_dataset(\"label_names\", label_dim, dtype=dt)\n        label_set[:] = class_labels\n\n    def close(self):\n        if len(self.buffer[\"data\"]) > 0:\n            self.flush()\n        self.db.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import img_to_array\n\n\nclass ImageToArrayPreprocessor:\n    def __init__(self, data_format=None):\n        self.data_format = data_format\n\n    def preprocess(self, image):\n        return img_to_array(image, data_format=self.data_format)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import BaseLogger\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\nimport os\n\n\n\nclass TrainingMonitor(BaseLogger):\n    def __init__(self, fig_path, json_path=None, start_at=0):\n        super(TrainingMonitor, self).__init__()\n        self.history = {}\n        self.fig_path = fig_path\n        self.json_path = json_path\n        self.start_at = start_at\n\n    def on_train_begin(self, logs={}):\n        if self.json_path is not None:\n            if os.path.exists(self.json_path):\n                self.history = json.loads(open(self.json_path).read())\n                if self.start_at > 0:\n                    for k in self.history.keys():\n                        self.history[k] = self.history[k][:self.start_at]\n\n    def on_epoch_end(self, epoch, logs={}):\n        for (k, v) in logs.items():\n            log = self.history.get(k, [])\n            log.append(v)\n            self.history[k] = log\n\n        if self.json_path is not None:\n            f = open(self.json_path, \"w\")\n            f.write(json.dumps(self.history))\n            f.close()\n\n        if len(self.history[\"loss\"])>1:\n            N=np.arange(0,len(self.history[\"loss\"]))\n            plt.style.use(\"ggplot\")\n            plt.figure()\n            plt.plot(N,self.history[\"loss\"],label=\"train_loss\")\n            plt.plot(N,self.history[\"val_loss\"],label=\"val_loss\")\n            plt.plot(N,self.history[\"accuracy\"],label=\"train_acc\")\n            plt.plot(N,self.history[\"val_accuracy\"],label=\"val_acc\")\n            epochs=len(self.history[\"loss\"])\n            plt.title(\"Training Loss & Accuracy [Epoch{}]\".format(epochs))\n            plt.xlabel(\"Epoch #\")\n            plt.ylabel(\"Loss/Accuracy\")\n            plt.legend()\n            plt.savefig(self.fig_path)\n            plt.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 导包\nimport numpy as np\n#from .HDF5DatasetWriter import HDF5DatasetWriter\n#from config import setting\n\nprint(\"[信息]加载csv格式数据集文件...\")\nfile = open(SAMPLE_FILE_PATH)\n\nfile.__next__()\n\n(train_images, train_labels) = ([], [])\n(val_images, val_labels) = ([], [])\n(test_images, test_labels) = ([], [])\n\ncount_by_label_train = {}\ncount_by_label_val = {}\ncount_by_label_test = {}\n\nfor row in file:\n    (label, usage, image) = row.strip().split(\",\")\n    label = int(label)\n    image = np.array(image.split(\" \"), dtype=\"uint8\")\n    image = image.reshape((48, 48))\n    if usage == \"Training\":\n        train_images.append(image)\n        train_labels.append(label)\n        count = count_by_label_train.get(label, 0)\n        count_by_label_train[label] = count + 1\n\n    elif usage == 'PublicTest':\n        val_images.append(image)\n        val_labels.append(label)\n        count = count_by_label_val.get(label, 0)\n        count_by_label_val[label] = count + 1\n    elif usage == \"PrivateTest\":\n        test_images.append(image)\n        test_labels.append(label)\n        count = count_by_label_test.get(label, 0)\n        count_by_label_test[label] = count + 1\nfile.close()\n\nprint(\"[信息]训练样本数量：{}\".format(len(train_images)))\nprint(\"[信息]校验样本数量：{}\".format(len(val_images)))\nprint(\"[信息]测试样本数量：{}\".format(len(test_images)))\n\nprint(count_by_label_train)\nprint(\"[信息]校验样本分布：\")\nprint(count_by_label_val)\nprint(\"[信息]测试样本分布：\")\nprint(count_by_label_test)\n\ndatasets = [(train_images, train_labels,TRAIN_HDF5),\n            (val_images, val_labels, VAL_HDF5),\n            (test_images, test_labels, TEST_HDF5)]\n\nfor (images, labels, outputPath) in datasets:\n    print(\"[信息构建]{}...\".format(outputPath))\n    writer = HDF5DatasetWriter((len(images), 48, 48), outputPath)\n    for (image, label) in zip(images, labels):\n        writer.add([image], [label])\n\n    writer.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras import backend\n\n\n# 定义VGG11类\nclass VGG11:\n    @staticmethod\n    def build(width, height, channel, classes, reg=0.0002):\n        model = Sequential(name=\"VGG11\")\n        shape = (height, width, channel)\n        channel_dimension = -1\n\n        if backend.image_data_format() == \"channels_first\":\n            shape = (channel, height, width)\n            channel_dimension = 1\n\n\n        model.add(Conv2D(64, (3, 3), input_shape=shape, padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n        model.add(Conv2D(128, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Conv2D(256, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n        model.add(Conv2D(512, (3, 3), padding=\"same\"))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n        model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\", strides=(1, 1)))\n\n        model.add(Flatten())\n        model.add(Dense(256, kernel_regularizer=l2(reg)))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n\n        model.add(Dense(128, kernel_regularizer=l2(reg)))\n        model.add(BatchNormalization(axis=channel_dimension))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.5))\n\n        model.add(Dense(classes, kernel_regularizer=l1(reg)))\n        model.add(Activation(\"softmax\"))\n\n        return model\n\n\nif __name__ == \"__main__\":\n    my_model = VGG11.build(width=48, height=48, channel=1, classes=7, reg=0.0002)\n    print(my_model.summary())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib\n\n\n'''from config import setting\nfrom utils.ImageToArrayPreprocessor import ImageToArrayPreprocessor\nfrom utils.EpochCheckpoint import EpochCheckpoint\nfrom utils.TrainingMonitor import TrainingMonitor\nfrom utils.HDF5DatasetGenerator import HDF5DatasetGenerator\nfrom mini_vgg_11 import VGG11'''\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\nimport tensorflow.keras.backend as K\nimport argparse\nimport os\n\n\nmatplotlib.use(\"Agg\")\n\ntrain_aug = ImageDataGenerator(rotation_range=10,\n                   zoom_range = 0.1,\n                   rescale=1 / 255.0,\n                   fill_mode=\"nearest\")\nval_aug = ImageDataGenerator(rescale=1/255.0)\n\niap = ImageToArrayPreprocessor()\n\ntrain_gen = HDF5DatasetGenerator(TRAIN_HDF5,\n                                 BATCH_SIZE,\n                                 aug=train_aug,\n                                 preprocessors=[iap],\n                                 classes=NUM_CLASSES)\nval_gen = HDF5DatasetGenerator(VAL_HDF5,\n                                 BATCH_SIZE,\n                                 aug=val_aug,\n                                 preprocessors = [iap],\n                                 classes=NUM_CLASSES)\n\nopt = Adam(lr = 1e-3)\nmodel = VGG11.build(width=48,height=48,channel=1,classes=NUM_CLASSES)\nmodel.compile(loss=\"categorical_crossentropy\",optimizer=opt,metrics=[\"accuracy\"])\nfig_path = os.path.sep.join([OUTPUT_PATH, \"{}.png\".format(os.getpid())])\ncallbacks = [TrainingMonitor(fig_path=fig_path)]\nmodel.fit_generator(train_gen.generator(),\n                    steps_per_epoch=train_gen.numImages//BATCH_SIZE,\n                    validation_data=val_gen.generator(),\n                    validation_steps=val_gen.numImages // BATCH_SIZE,\n                    epochs=50,\n                    max_queue_size=BATCH_SIZE*2,\n                    callbacks=callbacks,\n                    verbose=1)\nprint(\"[信息] 保存模型...\")\nmodel.save(MODEL_FILE,overwrite=True)\ntrain_gen.close()\nval_gen.close()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import load_model\n\n# 初始化图像预处理器\ntestAug = ImageDataGenerator(rescale=1 / 255.0)\niap = ImageToArrayPreprocessor()\n\n# 初始化测试数据集生成器\ntestGen = HDF5DatasetGenerator(TEST_HDF5,\n                               BATCH_SIZE,\n                               aug=testAug,\n                               preprocessors=[iap],\n                               classes=NUM_CLASSES)\n\n# 加载前面训练好的网络\nprint(\"[信息]加载网络模型\")\nmodel = load_model(MODEL_FILE)\n\n# 评估网络模型\n(loss, acc) = model.evaluate_generator(testGen.generator(),\n                                       steps=testGen.numImages // BATCH_SIZE,\n                                       max_queue_size=BATCH_SIZE * 2)\nprint(\"[信息]测试集准确率：{:.2f}%\".format(acc * 100))\n\n# 关闭数据集HDF5\ntestGen.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}