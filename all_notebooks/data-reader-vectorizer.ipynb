{"cells":[{"metadata":{"_uuid":"67d586b7-ebcc-4545-9740-c0aaa8cba698","_cell_guid":"838c6e16-0b94-4be7-9680-8c6adf54dd0e","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport time\nimport gc\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b857b33a-bf47-4c5d-baff-761cd4ae3eb2","_cell_guid":"944759ff-be63-4d52-8786-70fbf9d5d493","trusted":true},"cell_type":"code","source":"def read_SRA_data(file_path1, file_path2):\n    df1 = pd.read_csv(file_path1)\n    df2 = pd.read_csv(file_path2)\n    df1.rename(columns= {'student-ans':'premise', 'ref-ans':'hypothesis', 'student-ans-score':'target'}, inplace=True)\n    df2.rename(columns= {'student-ans':'premise', 'ref-ans':'hypothesis', 'student-ans-score':'target'}, inplace=True)\n    df1 = df1[['premise', 'hypothesis', 'target']]\n    df2 = df2[['premise', 'hypothesis', 'target']]\n    print('\\nMerging df1 and df2...\\n')\n    main_df = pd.concat([df1,df2], axis=0, ignore_index=True)\n    main_df['target'] = main_df['target'].astype('category')\n    main_df = main_df.sample(axis=0, frac=1).reset_index(drop=True)\n    print('Resulting data shape:',main_df.shape)\n    print('Total number of responses: ',len(main_df))\n    print('Categories to predict: ',main_df['target'].unique())\n    return main_df\n\ndef read_SNLI_data(file_path):\n    data = pd.read_csv(file_path)\n    data = data[['sentence1','sentence2','gold_label']]\n    data.rename(columns= {'sentence1':'premise', 'sentence2':'hypothesis', 'gold_label':'target'}, inplace=True)\n    data = data.loc[data['target'] != '-' ]\n    data.dropna(axis=0, inplace=True)\n    data['target'].replace({'entailment':'correct', 'neutral':'incorrect', 'contradiction':'contradictory'}, inplace=True)\n    data['target'] = data['target'].astype('category')\n    data = data.sample(axis=0, frac=1).reset_index(drop=True)\n    print('Resulting data shape:',data.shape)\n    print('Total number of responses: ',len(data))\n    print('Categories to predict: ',data['target'].unique())\n    return data\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55c1f17d-78c9-40c5-8050-dcd6f8f5f33b","_cell_guid":"6d7a9053-73d5-4275-b1e2-03bd32066bf5","trusted":true},"cell_type":"code","source":"import pickle\n\ndef save_pickle(df,file_path):\n    with open(file_path, 'wb') as f:\n        pickle.dump(df,f)\n        \ndef load_pickle(file_path):\n    with open(file_path,'rb') as f:\n        return pickle.load(f)\n    \ndef generate_glove_dict(word_embedding_path):\n    glove_wordmap = {}\n    with open(word_embedding_path, \"r\") as glove:\n        for line in glove:\n            name, vector = tuple(line.split(\" \", 1))\n            glove_wordmap[name] = np.fromstring(vector, sep=\" \")\n    return glove_wordmap","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"545f5ab4-7078-48db-b209-7bc499596992","_cell_guid":"23093150-6397-4e31-ab95-7e0c5c7e44bb","trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\n\nclass WordEmbedding:\n    \n    def __init__(self, word_embedding_path, unknown_strategy='default'):\n        self.stopwords = set(stopwords.words('english'))\n        self.embed_model = self.get_glove_dict(word_embedding_path)\n        self.embed_dim = len(self.embed_model['car'])\n        print(\"Dimension of a word vector: {}\".format(len(self.embed_model['car'])))\n        \n        if unknown_strategy == 'default':\n            unknown = np.zeros(self.embed_dim)\n            unknown[1] = 1\n            self.embed_model['<UNK>'] = unknown\n        elif unknown_strategy == 'random':\n            np.random.seed(7)\n            self.embed_model['<UNK>'] = np.random.uniform(-0.01, 0.01, 300).astype(\"float32\")\n        elif unknown_strategy == 'zeros':\n            self.embed_model['<UNK>'] = np.zeros(self.embed_dim)\n        else:\n            self.embed_model['<UNK>'] = None\n            \n    def get_glove_dict(self,word_embedding_path):\n        return load_pickle(word_embedding_path)\n    \n    def get_vector(self, word):\n        if word in self.embed_model.keys():\n            return self.embed_model[word]\n        elif word.lower() in self.embed_model.keys():\n            return self.embed_model[word.lower()]\n        else:\n            return self.embed_model['<UNK>']\n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"799fdfb5-0478-48f4-b175-dc4d653578a7","_cell_guid":"21c33c85-8b5c-4a72-a3a9-e8b568853cd5","trusted":true},"cell_type":"code","source":"import string\nfrom sklearn import preprocessing\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TreebankWordTokenizer\n\nclass Prepare:\n    \n    def __init__(self, df):\n        self.stopwords = stopwords.words('english') \n        self.df = df\n        self.word_tokenizer = TreebankWordTokenizer()\n        self.tokenizer = Tokenizer()\n        self.le = preprocessing.LabelEncoder()\n        self.premise = self.valid_tokens(df['premise'].values)\n        self.hypothesis = self.valid_tokens(df['hypothesis'].values)\n        self.tokenizer.fit_on_texts(self.premise + self.hypothesis)\n        self.vocab_size = len(self.tokenizer.word_index)+1\n        self.vocab = self.tokenizer.word_index.items()\n        self.max_len = 100\n        self.premise, self.hypothesis, self.target = self.prep_df()\n        \n    def valid_tokens(self,texts):\n        valid = []\n        for text in texts:\n            valid_text = [token for token in self.word_tokenizer.tokenize(text) if token.lower() not in self.stopwords ]\n            valid.append(valid_text)\n        return valid\n                \n    def prep_target(self, score):\n        self.le.fit(score.unique())\n        score = self.le.transform(score)\n        return to_categorical(score)\n        \n    def prep_df(self):\n        premise = self.tokenizer.texts_to_sequences(self.premise) \n        hypothesis = self.tokenizer.texts_to_sequences(self.hypothesis)\n        #max1 = len(max(premise, key=len))\n        #max2 = len(max(hypothesis, key=len))\n        #self.max_len = max(max1, max2)\n        premise = pad_sequences(premise, maxlen = self.max_len, padding='post')\n        hypothesis = pad_sequences(hypothesis, maxlen = self.max_len, padding='post')\n        target = self.prep_target(self.df['target'])\n        return premise,hypothesis,target    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.compat.v1.keras import layers\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model, model_from_json, load_model\nfrom tensorflow.keras.optimizers import Adam, Adadelta\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.utils import plot_model\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.initializers import Constant\n\nclass SiameseModel:\n    \n    def __init__(self, word_embedding, data, use_cudnn_lstm=False, plot_model_architecture=True):\n        self.hidden_units = 300\n        self.embed_model = word_embedding\n        self.input_dim = word_embedding.embed_dim\n        self.vocab_size = data.vocab_size\n        self.left = data.premise\n        self.right = data.hypothesis\n        self.target = data.target\n        self.max_len = data.max_len\n        self.dense_units = 300\n        self.drop = 0.2\n        self.name = '{}_glove{}_lstm{}_dense{}'.format(str(int(time.time())),\n                                                        self.input_dim,self.hidden_units,self.dense_units)\n        \n        \n        embedding_matrix = np.zeros((self.vocab_size, self.input_dim))\n        for word, i in data.vocab:\n            embedding_vector = self.embed_model.get_vector(word)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n\n        embed = layers.Embedding(input_dim=self.vocab_size, output_dim=self.input_dim, \n                                 embeddings_initializer=Constant(embedding_matrix), \n                                 input_length=self.max_len, mask_zero=True, trainable=False)\n        \n        translate = layers.TimeDistributed(layers.Dense(self.hidden_units, activation='relu'))\n\n        if use_cudnn_lstm:\n            lstm = layers.CuDNNLSTM(self.hidden_units, input_shape=(None, self.input_dim),unit_forget_bias=True,\n                                   kernel_initializer='he_normal',\n                                   kernel_regularizer='l2', name='lstm_layer')\n        else:\n            lstm = layers.LSTM(self.hidden_units, input_shape=(None, self.input_dim), unit_forget_bias=True,\n                               activation = 'relu',\n                               kernel_initializer='he_normal',\n                               kernel_regularizer='l2', name='lstm_layer', return_sequences=False)\n            \n        left_input = Input(shape=(self.max_len), name='input_1')\n        right_input = Input(shape=(self.max_len), name='input_2')        \n\n        embed_left = embed(left_input)\n        embed_right = embed(right_input)\n        print('embed:',embed_right.shape)\n        \n        prem = translate(embed_left)\n        hypo = translate(embed_right)\n        print('translate:',prem.shape)\n\n        prem = lstm(prem)\n        hypo = lstm(hypo)\n        print('lstm:',hypo.shape)\n        prem = layers.BatchNormalization()(prem)\n        hypo = layers.BatchNormalization()(hypo)\n        \n        #l1_norm = lambda x: 1 - K.abs(x[0]-x[1])\n        #merged = layers.Lambda(function=l1_norm, output_shape=lambda x: x[0],name='L1_distance')([prem, hypo])\n        merged = layers.concatenate([prem, hypo])\n        merged = layers.Dropout(self.drop)(merged)\n        print('merged:', merged.shape)\n        \n        for i in range(3):\n            merged = layers.Dense(2*self.dense_units, activation='relu')(merged)\n            merged = layers.Dropout(self.drop)(merged)\n            merged = layers.BatchNormalization()(merged)\n            print('dense:',merged.shape)\n            \n        output = layers.Dense(3, activation='softmax', name='output_layer')(merged)\n        print('output:',output.shape)\n        \n        self.model = Model(inputs=[left_input, right_input], outputs=output)\n\n        self.compile()\n                \n        if plot_model_architecture:\n            plot_model(self.model, show_shapes=True, to_file=self.name+'.png')\n        \n    def compile(self):\n        optimizer = Adam(lr=0.001)\n        self.model.compile(loss='categorical_crossentropy',\n                          optimizer=optimizer, metrics=['accuracy'])\n        \n    def fit(self, validation_split=0.3, epochs=5, batch_size=128, patience=2):\n        left_data = self.left\n        right_data = self.right\n        target = self.target\n        early_stopping = EarlyStopping(patience=patience, monitor='val_loss')\n        learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=1,  \n                                            factor=0.5,\n                                            min_lr=0.00001)\n        callbacks = [early_stopping, learning_rate_reduction]\n        \n        self.history = self.model.fit([left_data, right_data], target, \n                                 validation_split=validation_split,\n                                 epochs=epochs, batch_size=batch_size, callbacks=callbacks)        \n        \n    def predict(self, left_data, right_data):\n        return self.model.predict([left_data, right_data])\n    \n    def evaluate(self, left_data, right_data, target, batch_size=128):\n        return self.model.evaluate([left_data, right_data], target, batch_size=batch_size)\n    \n    def save_pretrained_weights(self, path='./model/pretrained_weights.h5'):\n        self.model.save_weights(path)\n        print('Save pretrained weights at location: ', path)\n        \n    def load_pretrained_weights(self, path='./model/pretrained_weights.h5'):\n        self.model.load_weights(path, by_name=True, skip_mismatch=True)\n        print('Loaded pretrained weights')\n        self.compile()\n        \n    def save(self, model_folder=None):\n        print('Saving model in SavedModel format ...')    \n        if model_folder==None or not os.path.isdir(model_folder):\n            model_folder = self.name\n        os.mkdir(model_folder)\n        self.model.save(model_folder)\n        print('Saved model to disk')\n        \n    def load_activation_model(self):\n        self.activation_model = Model(inputs=self.model.input[0], \n                                      outputs=self.model.get_layer('lstm_layer').output)\n        \n    def load(self, model_folder='./model/'):\n        #use for encoder decoder alontg with load_activation\n        # load json and create model\n        json_file = open(model_folder + 'model.json', 'r')\n        loaded_model_json = json_file.read()\n        json_file.close()\n        loaded_model = model_from_json(loaded_model_json)\n        # load weights into new model\n        loaded_model.load_weights(model_folder + 'model.h5')\n        print('Loaded model from disk')\n        \n        self.model = loaded_model\n        # loaded model should be compiled\n        self.compile()\n        self.load_activation_model()\n        \n    def visualize_metrics(self):\n        epochs = len(self.history.history['accuracy'])\n        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n        t = f.suptitle('Performance', fontsize=12)\n        f.subplots_adjust(top=0.85, wspace=0.3)\n        epoch_list = list(range(1,epochs+1))\n        ax1.plot(epoch_list, self.history.history['accuracy'], label='Train Accuracy')\n        ax1.plot(epoch_list, self.history.history['val_accuracy'], label='Validation Accuracy')\n        ax1.set_xticks(np.arange(0, epochs+1, 5))\n        ax1.set_ylabel('Accuracy Value')\n        ax1.set_xlabel('Epoch')\n        ax1.set_title('Accuracy')\n        l1 = ax1.legend(loc=\"best\")\n        \n        ax2.plot(epoch_list, self.history.history['loss'], label='Train Loss')\n        ax2.plot(epoch_list, self.history.history['val_loss'], label='Validation Loss')\n        ax2.set_xticks(np.arange(0, epochs+1, 5))\n        ax2.set_ylabel('Loss Value')\n        ax2.set_xlabel('Epoch')\n        ax2.set_title('Loss')\n        l2 = ax2.legend(loc=\"best\")\n        plt.show()\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_embedding_path = '/kaggle/input/word-embedding/glove_vector_dict_300d.pickle'\nsnli_path = '/kaggle/input/word-embedding/snli_train_5l_df.pickle'\nsra_path = '/kaggle/input/word-embedding/student_response_8k_df.pickle'\ndf = load_pickle(snli_path)\nmain_df = load_pickle(sra_path)\n\nword_embedding = WordEmbedding(word_embedding_path)\nprep = Prepare(df)\nprep_main = Prepare(main_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.history\nepochs = len(history.history['accuracy'])\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))\nt = f.suptitle('Performance - Training on SRA', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\nepoch_list = list(range(1,epochs+1))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, epochs+1, 1))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, epochs+1, 1))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")\nplt.grid(True)\nplt.savefig(str(round(time.time()))+'_Training_SRA.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Pretraining model on SNLI data...')\nt1 = time.time()\nsiamese = SiameseModel(word_embedding, prep)\nprint(siamese.model.summary())\nsiamese.fit(epochs=10,batch_size=512)\nt2 = time.time()\nprint('\\tTook %f seconds'%(t2-t1))\nsiamese.save_pretrained_weights(path='snli_pretrained_weights.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_weights = '/kaggle/input/word-embedding/snli_pretrained_weights.h5'\nprint('Training model on SRA data ...')\nmodel = SiameseModel(word_embedding, prep_main)\nmodel.load_pretrained_weights(path=pretrained_weights)\nt1 = time.time()\nmodel.fit(epochs=10, patience=3, batch_size=32)\nt2 = time.time()\nprint('\\tTooke %f seconds'%(t2 - t1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.model.save_weights('SRA_trained_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1a = '/kaggle/input/test-data/3waySciEnts_test_unseen_ans.csv'\ntest1b = '/kaggle/input/test-data/3wayBeetle_test_unseen_ans.csv'\ntest2a = '/kaggle/input/test-data/3waySciEnts_test_unseen_domain.csv'\ntest3a = '/kaggle/input/test-data/3wayBeetle_test_unseen_ques.csv'\ntest3b = '/kaggle/input/test-data/3waySciEnts_test_unseen_ques.csv'\n\ntest1 = read_SRA_data(test1a, test1b)\ntest3 = read_SRA_data(test3a, test3b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1 = Prepare(test1)\ntest3 = Prepare(test3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test1.premise, test1.hypothesis, test1.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test3.premise, test3.hypothesis, test3.target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = Prepare(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(p.premise, p.hypothesis)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}