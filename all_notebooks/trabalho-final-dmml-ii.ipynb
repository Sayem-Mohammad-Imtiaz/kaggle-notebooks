{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport seaborn as sns# data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# IESB\n## Pós Graduação em Ciência de Dados\n\n### **Disciplina** - Data Mining e Machine Learning II\n### **Projeto de Conclusão da Disciplina **\n### **Aluno:** Nilson Romero Michiles Júnior\n### **Turma:** Asa Norte\n\n \n#### Descrição do Problema e Objetivo\n\nOs Bancos possuem uma necessidade de manter seu percentual de inadimplência baixo, sendo mais oneroso ainda tentar recuperar créditos de habitação(Home Equity), tendo em vista as proteções que a legislação concede aos inadimplentes dessa modalidade. Visto isso, urge a necessidade dos Bancos atuarem preventivamente para identificar possíveis maus pagadores e evitar possíveis adversidades futuras. \n\nAssim, esse modelo de dados apresenta preditor para identificar esses maus pagadores, por meio de um conjunto de dados de Home Equity com   cerca de seis mil empréstimos concedidos no passado. Esses dados contêm várias informações sobre a situação do cliente no momento do empréstimo e também contêm uma coluna 'RUIM', que indica se o cliente deixou de pagar o empréstimo posteriormente. Podemos usar esse conjunto de dados juntamente com a variável/etiqueta \"RUIM\" para treinar modelos de aprendizado de máquina, o que nos ajudaria a prever a probabilidade de alguém deixar o empréstimo no futuro com base na situação atual. \n\nAssim, ao final deste notebook será proposto o melhor modelo a ser usado para prever a etiqueta supramencionada baseada no padrão da situação apresentada nos dados. Esse problema pode ser classificado como problema de classificação binária, pois o modelo preverá se uma pessoa seria o padrão.\n\n\n#### Metodologia\n\nA base de dados \"Home Equity\" possui dados pessoas e informações de empréstimo de 5.960 empréstimos recentes. Para cada empréstimo existem 12 variáveis registradas. A variável alvo (BAD) indica quando o cliente não pagou o empréstimo (valor 1), e quando ele honrou o compromisso (valor 0).\n\nSerão utilizados os modelos Random Forest Classifier, XGBosst e XGBoost com auxílio do GridSearchCV para otimização do modelo"},{"metadata":{},"cell_type":"markdown","source":"### Dicionário de Dados\nO dicionário de dados das colunas disponíveis no Dataset estão elencadas abaixo:\n\n**BAD:** 1 = client defaulted on loan 0 = loan repaid\n\n**LOAN**: Amount of the loan request\n\n**MORTDUE**: Amount due on existing mortgage\n\n**VALUE**: Value of current property\n\n**REASON**: DebtCon = debt consolidation ; \nHomeImp = home improvement\n\n**JOB**: Six occupational categories\n\n**YOJ**: Years at present job\n\n**DEROG**: Number of major derogatory reports\n\n**DELINQ**: Number of delinquent credit lines\n\n**CLAGE**: Age of oldest trade line in months\n\n**NINQ**: Number of recent credit lines\n\n**CLNO**: Number of credit lines\n\n**DEBTINC**: Debt-to-income ratio"},{"metadata":{},"cell_type":"markdown","source":"### Importando o Dataset "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hmeq-data/hmeq.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploração do Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape, df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observa-se que Dataset possui 5960 linhase  13 colunas, sendo apenas as colunas 'JOB' e 'REASON' com valores não númericos"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Estatísticas Descritivas\ndf.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Para a análise inicial, há a biblioteca Pandas Profiling que gera um report com análise de todas os campos e suas estatísticas. Por meio deste relatório é possível ter uma noção das distribuiçoes dos dados e correlações"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Utilizando o pandas profiling para auxiliar a EDA\nimport pandas_profiling as pp\npp.ProfileReport(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# avaliação das variáveis numéricas por meio de histogramas\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndf.hist(figsize=(20,10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Na análise inicial, se observa que a variável Target('BAD'), possui um número pequeno de 'maus pagadores'(=1), o que indica que é uma base desbalanceada, sendo necessários ajustes ou métricas específicas para essa distribuição"},{"metadata":{},"cell_type":"markdown","source":"## Tratamento dos Dados\n\nPara o tratamento, serão avaliadas a existência de missing values ou valores null."},{"metadata":{"trusted":true},"cell_type":"code","source":"MissingValues =df.isnull().sum().rename_axis('Colunas').reset_index(name='Missing Values')\nMissingValues","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pelo quadro acima, se observa que a base de dados não foi tratada, havendo uma quantidade alta de missings. Para a análise foram excluídos as linhas que possuiam algum valor com NA, restando 3364 linhas."},{"metadata":{"trusted":true},"cell_type":"code","source":"# retirando os na\ndf2 = df.copy()\ndf2.dropna(axis=0,how='any',inplace= True)\ndf2.info(), df2.isna().any() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Análise Descritiva Exploratória (EDA)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\ndf2.hist(figsize=(25,14),bins=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A avaliação dos histogramas mostra inicialmente que :\n- A variável BAD (Target) possui poucos valores 1 para treinamento do modelo\n- A maior parte dos valores totais de financiamento (LOAN) possuem uma distribuição próxima da normalidade, e os valores a receber(MORTDUE), na média, são maiores que os totais emprestados. Observa-se o terror dos juros bancários\n- Os valores das propriedades possuem distribuição próxima dos valores dos financiamentos\n- O DEROG, algo equivalente à um aviso de negativação do serviço de proteção ao consumidor, é baixo, contudo possui uma correlação próxima de moderada(p=0,25) com os maus pagadores.\n- O DELINQ, linhas de crédito com inadimplência, também possui correlação próxima à moderada(p=0,27) com maus pagadores\n- O número de linhas de crédito possui correlação, mas a intensidade é menor(p=0,13), com maus pagadores\n- Por fim, a base possui um indicador (Débitos/Renda) que possui uma correlação próxima a moderada (p=0,23), sendo um bom indicador."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlação das variáveis numéricas\nplt.figure(figsize= (15, 15))\n\nsns.heatmap(df2.corr(), square=True, annot=True, linewidth=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfWithBin = df.copy()\nbins=[0,3,15] \ngroup=['Low','High'] \ndfWithBin['DELINQ_bin']=pd.cut(dfWithBin['DELINQ'],bins,labels=group)\nLOAN_bin=pd.crosstab(dfWithBin['DELINQ_bin'],dfWithBin['BAD'])\nLOAN_bin.div(LOAN_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True,title='Cruzamento de Linhas de Crédito Inadimplentes e Maus pagadores')\nplt.xlabel('DELINQ')\nP= plt.ylabel('%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A título de demonstração da correlação entre os valores, o gráfico acima explicita que um número alto de linhas de crédito inadimplentes tem relação positiva com maus pagadores."},{"metadata":{},"cell_type":"markdown","source":"Se observa que, os devedores(BAD)  em média fizeram empréstimos de 19.260,00, contudo sua dívida em média está em $ 73.864,00."},{"metadata":{"trusted":true},"cell_type":"code","source":"#avaliacao dos default loans\n\ndf2[df2['BAD']==1].drop('BAD', axis=1).describe().style.format(\"{:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avaliando as variáveis categóricas em relacao ao pefil do pagador\n\nJOB=pd.crosstab(df['JOB'],df['BAD'])\nJOB.div(JOB.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='Tipos de Empregos e Clientes', figsize=(4,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pela análise, se observa que o grupo que trabalha com vendas e empreendedores possuem um número maior de maus pagadores"},{"metadata":{"trusted":true},"cell_type":"code","source":"REASON=pd.crosstab(df['REASON'],df['BAD'])\nREASON.div(REASON.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='Tipos de Empregos e Razões', figsize=(4,4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A variável razão do débito apresentou valores próximos para as duas categorias, o que não dá muita informação ao modelo"},{"metadata":{},"cell_type":"markdown","source":"### Featuring Engineering"},{"metadata":{},"cell_type":"markdown","source":"Para melhor ajuste ao modelo, foram dummizadas as tabelas com type Object"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gerando Dummies para modelos que utilizam apenas variaveis numéricas\n\ndf2 = pd.get_dummies(df2, columns=['REASON', 'JOB'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalizando os dados para facilitar possível visualizacoes\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndf3 = pd.DataFrame(sc.fit_transform(df2), columns=df2.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Geração Amostras de Treino e Teste"},{"metadata":{},"cell_type":"markdown","source":"Nota: Neste trabalho foi realizada a modelagem utilzando uma amostra para validação inclusive, contudo, devida a baixa quantidade de registros, foi utilizado apenas treino e teste "},{"metadata":{"trusted":true},"cell_type":"code","source":"# importando a biblioteca\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Etapa 1- Primeiro Separando em Treino e Teste, parâmetro test_size = 0.25 (default)\ntreino, teste = train_test_split(df2, random_state=42)\n\n#Etapa 2 -  Separando o Treino em treino e validacao, para refinar o modelo\n#treino, validacao = train_test_split(treino, random_state=42)\n\ntreino.shape, teste.shape # validacao.shape, ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"teste.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verificando se as amostras possuem similaridade, avaliando se há discrepância alta considerando a média e desvio padrão de cada uma. Pela análise verifica-se que a amostra gerada \n# possuem estatísticas próximas, portanto atendem ao requisito.\ntreino.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecionando as colunas que usaremos para treinar o modelo\nnao_usadas = ['BAD']\n\n# Lista das colunas que serão usadas\nusadas = [c for c in treino.columns if c not in nao_usadas]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Métricas de Avaliação\n\nPara a avaliação do modelo serão utilizadas duas métricas,sendo a Accuracy(Acurácia) e o F1 Score, melhor detalhados abaixo:\n\n<img src=\"https://miro.medium.com/max/1000/1*t1vf-ofJrJqtmam0KSn3EQ.png\" width=\"250px\"/>\n\nA **Accuracy** mede a performance do modelo como um todo, contudo não é uma métrica interessante em situações de bases muito desbalanceadas.\n\nA **Precision** é importante quando os Falsos Positivos são considerados mais prejudiciais que os Falsos Negativos. Sendo uma métrica interessante para o modelo em análise caso o apetite à risco do Banco seja baixo, logo se para o Banco acertar na predição dos maus pagadores seja mais importante que acabar deixando de emprestar para algum bom pagador que o modelo etiquetou errado.\n\nO **Recall**, ao contrário,  pode ser usada em situações em que os Falsos Negativos são mais prejudiciais que os Falso Positivos. Nesse sentido, o foco seria ter mais produtos financiamentos aprovados, assim, o Banco sofre mais deixando de vender para os bons pagadores do que aceitando um mau pagador etiquetado como bom.\n\nO **F1 Score** é uma média harmônica entre de Precision e Recall, portanto, quando tem-se um F1-Score baixo, é um indicativo de que ou a precisão ou o recall está baixo.\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avaliando desempenho do modelo\n#importando métrica\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nresults = pd.DataFrame(columns=['Modelo', 'Accuracy', 'F1score'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelo RandomForest Classifier"},{"metadata":{},"cell_type":"markdown","source":"As Florestas aleatórias ou florestas de decisão aleatória são modelos ensemble das DecisionTree, que utilizam um método de aprendizado conjunto para classificação,regressão e outras tarefas que operam construindo uma infinidade de árvores de decisão no momento do treinamento e gerando a classe que é o modo das classes ou a previsão média das árvores individuais."},{"metadata":{"trusted":true},"cell_type":"code","source":"# importanto o modelo\nfrom sklearn.ensemble import RandomForestClassifier\n\n#instanciando o modelo\nrf = RandomForestClassifier(n_estimators=200,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# treinando o modelo\nrf.fit(treino[usadas], treino['BAD'])\n\n#Prevendo os dados de validacao\n\n# gerando predicoes do modelo com os dados de teste\npred_teste = rf.predict(teste[usadas])\n\n#Medindo a acuracia nos dados de teste\nresults.loc[0]= ['RandonForest sem ajuste', accuracy_score(teste['BAD'],pred_teste), f1_score(teste['BAD'],pred_teste)]\n\naccuracy_score(teste['BAD'],pred_teste), f1_score(teste['BAD'],pred_teste)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Avaliando a importancia de cada coluna (cada variável de entrada)\npd.Series(rf.feature_importances_, index=usadas).sort_values().plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importando a bilbioteca para plotar o gráfico de Matriz de Confusão\nimport scikitplot as skplt\n\n# Matriz de Confusão - Dados de Validação\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_teste)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nPela análise da Matriz de Confusão, considerando que é uma base bastante desbalanceada, como se observa no gráfico abaixo. Assim, a análise das métricas de especifidade e esforço pode realçar os falsos positivos"},{"metadata":{},"cell_type":"markdown","source":"### Utilizando o RandonForest Classifier com ajuste nos parâmetros\n\nForam ajustados os parâmetros de aumentando o número de estimadores para 900, quando o default é 100, e informando que o número de folhas aceitavel para as ramificações das árvores de decisão como 2.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setando parametros\nrf2 = RandomForestClassifier(max_depth=None, random_state=42, n_jobs=-1, n_estimators=900,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2,  class_weight='balanced')\n# treinando o modelo RF2\nrf2.fit(treino[usadas], treino['BAD'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#relizando a predicao do RF2 com base teste\npred_teste2 = rf2.predict(teste[usadas])\n\n#métrica para RF2 validacao\nresults.loc[1]= ['RandonForest COM ajuste', accuracy_score(teste['BAD'],pred_teste2), f1_score(teste['BAD'],pred_teste2)]\n\naccuracy_score(teste['BAD'],pred_teste2), f1_score(teste['BAD'],pred_teste2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Matriz de Confusão - Dados de Validação\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_teste2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Com os ajustes de parâmetros, houve uma melhora pequena no F1 Score."},{"metadata":{},"cell_type":"markdown","source":"## Modelo XGBoost\n\nO XGBoost é uma implementação de árvores de decisão aprimoradas por gradiente, projetadas para velocidade e desempenho.Sua sigla significa eXtreme Gradient Boosting, e sua vantagem é devida a uma implementação de máquinas de aumento de gradiente."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importar o modelo\nfrom xgboost import XGBClassifier\n\n# Instanciar o modelo\nxgb = XGBClassifier(n_jobs=-1, random_state=42)\n\n# treinando o modelo\nxgb.fit(treino[usadas],treino['BAD']) \n\n# Fazendo predições\n#pred_xgb_validacao = xgb.predict(validacao[usadas])\n\n# Metrícas XGB validacao\n#accuracy_score(validacao['BAD'],pred_xgb_validacao), balanced_accuracy_score(validacao['BAD'],pred_xgb_validacao), f1_score(validacao['BAD'],pred_xgb_validacao)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fazendo predições\npred_xgb_teste = xgb.predict(teste[usadas])\n\n# Metrícas XGB teste\nresults.loc[2]= ['XGBoost', accuracy_score(teste['BAD'],pred_xgb_teste), f1_score(teste['BAD'],pred_xgb_teste)]\n\naccuracy_score(teste['BAD'],pred_xgb_teste), f1_score(teste['BAD'],pred_xgb_teste)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Matriz de Confusão - Dados de Validação\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_xgb_teste)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"O XGBoost apresentou uma melhora um pouco maior no modelo, contudo não foi significante."},{"metadata":{},"cell_type":"markdown","source":"## Modelo XGBoost com GridSearchCV\n\nO GridSearchCV é um módulo do Scikit Learn que é amplamente usado para automatizar grande parte do processo de tuning. O objetivo primário do GridSearchCV é a criação de combinações de parâmetros para posteriormente avaliá-las."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importação bibliotecas\n# Importação GridSearchCV.\nfrom sklearn.model_selection import GridSearchCV\n\n# Uso do constructor do XGBoost para criar um classifier.\nxgb2 = XGBClassifier(n_jobs=-1) # Sem nada dentro, pois vamos \"variar\" os parâmetros.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Para o balaceamento do gridSearchCV foram realizadas três rodadas, a partir dos best score de cada época. \nparametros = {'n_estimators':[100,500, 900, 1100],\n              'learning_rate':[0.02,0.08,0.09,1.5]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importando o Make Scorer\nfrom sklearn.metrics import make_scorer\n\n# Importando os módulos de cálculo de métricas\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando um dicionário com as métricas que desejo calcular.\nmeus_scores = {'accuracy' :make_scorer(accuracy_score),\n               'recall'   :make_scorer(recall_score),\n               'precision':make_scorer(precision_score),\n               'f1'       :make_scorer(f1_score)}\n\n# Exemplo para o uso scoring igual ao meus_scores.\ngrid = GridSearchCV(estimator = xgb2,\n                      param_grid = parametros,\n                      cv = 10,\n                      scoring = meus_scores,   # É o meus_scores\n                      refit = 'f1')            # Observe que foi configurado para f1\n\n# Imprime o melhor score(f1) e melhor parâmetro \ngrid.fit(treino[usadas],treino['BAD'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.best_score_, grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  Caso queira dar uma olhada nos outros scores\npd.DataFrame(grid.cv_results_).sort_values('rank_test_f1')[:3].T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando um objeto que os melhores parametros.\nxgb_gs = grid.best_estimator_\n\n# Visualizar o objeto para conferir os parametros.\nxgb_gs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#primeira epoca \n# Fazendo predições teste\npred_xgb_gs_teste = xgb_gs.predict(teste[usadas])\n\n# Metrícas XGB teste\nresults.loc[3]= ['XGBoost com GridSearchCV',accuracy_score(teste['BAD'],pred_xgb_gs_teste), f1_score(teste['BAD'],pred_xgb_gs_teste)]\n\naccuracy_score(teste['BAD'],pred_xgb_gs_teste), f1_score(teste['BAD'],pred_xgb_gs_teste)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Matriz de Confusão - Dados de Validação\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_xgb_gs_teste)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelo Random Forest Classifier e GridSearchCV \n\nPor fim, a título de avaliação, foi testada a Random Forest com tuning pelo GridSearchCV para verifiar possível melhora no modelo."},{"metadata":{"trusted":true},"cell_type":"code","source":"#instanciando o modelo\nrf2=RandomForestClassifier(n_jobs=-1)\n\n#setando parametros para o gridSearchCV\nparam_dict = { 'n_estimators':[100,400,800,1000],\n               'criterion': ['gini','entropy']\n              }\n\ngrid2 = GridSearchCV(rf2, param_dict, cv=10)\n\n#treinando modelo\ngrid2.fit(treino[usadas], treino['BAD'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Resultados\ngrid2.best_params_ , grid2.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Criando um objeto que os melhores parametros.\nrf2_gs2 = grid2.best_estimator_\n\n# Visualizar o objeto para conferir os parametros.\nrf2_gs2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicao teste\npred_rf2_gs2_teste = rf2_gs2.predict(teste[usadas])\n\n# metricas predicao teste\nresults.loc[4]= ['RandomForest com GridSearchCV', accuracy_score(teste['BAD'],pred_rf2_gs2_teste),f1_score(teste['BAD'],pred_rf2_gs2_teste)]\n\naccuracy_score(teste['BAD'],pred_rf2_gs2_teste), f1_score(teste['BAD'],pred_rf2_gs2_teste)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Matriz de Confusão - Dados de Validação\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_rf2_gs2_teste)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusão\n\n**Pela Análise dos Modelos, verifica-se que o que apresentou o melhor resultado foi XGBoost, com as métricas de acurária 0.958 e F1 Score 0.6846)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}