{"cells":[{"metadata":{},"cell_type":"markdown","source":"<b>Data Description:</b>\n    The file Bank.xls contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan\ncampaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted\nthe personal loan that was offered to them in the earlier campaign.\n\n<b>Domain:</b>\n    Banking\n\n<b>Context:</b>\n    This case is about a bank (Thera Bank) whose management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as\ndepositors). A campaign that the bank ran last year for liability customers showed a\nhealthy conversion rate of over 9% success. This has encouraged the retail marketing\ndepartment to devise campaigns with better target marketing to increase the success\nratio with minimal budget.\n\n<b>Learning Outcomes:</b>\n\n   -  Exploratory Data Analysis\n   -  Data Cleaning\n   -  Data Visualization\n   -  Preparing the data to train a model \n   -  Training and making predictions using a classification model\n   -  Model evaluation\n    \n<b>Objective:</b>\n    The classification goal is to predict the likelihood of a liability customer buying personal loans which means we have to build a model which will be used to predict which customer will most likely to accept the offer for personal loan, based on the specific relationship with the bank across various features given in the dataset. Here I will be using the Supervised Learning methods to predict which model is best for this problem amongst Logistic Regresssion, K-Nearest Neighbors(KNN) and Naive Bayes Algorigthm.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Import the necessary libraries :"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\n\nfrom scipy import stats\n\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n# To model the Gaussian Navie Bayes classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import accuracy_score # Performance measure â€“ Accuracy\n\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment :</b> Here I have used numpy, pandas, matplotlib, seaborn, scipy for EDA and Data Visualization. Also used sklearn for data spliting, model building and for confusion matrix. "},{"metadata":{},"cell_type":"markdown","source":"## ::--------------------------- Exploratory Data Analysis ---------------------------- ::"},{"metadata":{},"cell_type":"markdown","source":"#### Read the data as a data frame :- "},{"metadata":{"trusted":true},"cell_type":"code","source":"df  = pd.read_csv('/kaggle/input/bank-personal-loan-modellingthera-bank/Bank_Personal_Loan_Modelling.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment:</b> Here I have read the Personal Loan  dataset using read_csv() function of pandas. df is a dataframe. I have used head() funtion to display first 5 records of the dataset."},{"metadata":{},"cell_type":"markdown","source":"<b>Target Column rearrange:- </b> As our Target Column(Personal Loan) is in middle of dataframe so for more convinient I have drop the personal loan column from the original place and appended at last of dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"personal_loan = df['Personal Loan']\ndf.drop(['Personal Loan'], axis=1, inplace = True)\ndf['Personal Loan'] = personal_loan\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment:</b> Now the traget column is appended at the end of the dataframe."},{"metadata":{},"cell_type":"markdown","source":"<b> Features(attributes) Understanding from the above dataframe :- </b> \n\n- The ID variable can be ignored as it will not any effect on our model. As we know customer Id is just to maitain the record in serial order. There is no relationship with Id and Loan.\n- Target Variable is <b>Personal Loan</b> which describe whether the person has taken loan or not. This is the variable which we need to predict.\n\nNonimal Varibles :\n- ID - Customer ID\n- ZIP Code - Home Address ZIP code of the customer. This variable can also be ignored becasue we can not judge the customers based on thier area or location.\n\nOrdinal Categorical variables :\n- Family - Number of famlily member of the customer\n- Education - Education level of the customer. In our dataset it ranges from 1 to 3 which are Under Graduate, Graduate and Post Graduate respectivly.\n\nInterval Variables :\n- Age        - Age of the customer\n- Experience - Years of experience of customer has\n- Income     - Annula Income of the customer which is in dollars\n- CCAvg      - Avg. spending on credit cards per month which in dollars.\n- Mortgage    - Value of House Mortgage\n\nBinary Categorical Variable :\n- CD Account - Does the customer have CD Account with bank or not?\n- Security Account - Does the customer have Security Account with bank or not?\n- Online  - Does the customer have Online banking facility with bank or not?\n- Credit Card - Does the customer have a credit card issued by Universal Bank or not?\n- Personal Loan - This our target variable which we have to predict. This indicates that the customer has token loan or not?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Shape of the data :- "},{"metadata":{"trusted":true},"cell_type":"code","source":"rows_count, columns_count = df.shape\nprint('Total Number of rows :', rows_count)\nprint('Total Number of columns :', columns_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment:</b> Shape of the dataframe is (5000, 14).\nThere are 5000 rows and 14 columns in the dataset. "},{"metadata":{},"cell_type":"markdown","source":"###  Data type of each attribute :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>comment : </b> We can also display the data types of dataframe using df.info() function which gives even more useful info."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment :</b> Here we can see that all the variables are numerical. But the columns 'CD Account', 'Online', 'Family', 'Education' , 'CreditCard' and 'Securities Account' are categorical variable which should be in 'category' type. "},{"metadata":{},"cell_type":"markdown","source":"### Checking the presence of missing values :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment:</b> Here total missing values count from each column is 0 and we can see there is no missing value in the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().values.any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment:</b> This gives if we have any missing values at all. False : No missing value, True: Missing value "},{"metadata":{},"cell_type":"markdown","source":"#### Missing value Visualization :- "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.isna(), yticklabels=False, cbar=False, cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment: </b> From the above heatmap graph we can see that there is no missing value in dataset."},{"metadata":{},"cell_type":"markdown","source":"### Checking the unique data :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transposing index and columns:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b> \n- <b>Important : </b>Here we can see that minumum value of Experience column is <b>-3.0</b> which could be a mistake because Experience can not be negative. So I will be fixing it in data cleaning and error fixing part.\n- Binary varibales 'Personal Loan', 'Credit Card', 'Online', 'CD Account', 'Security Account' has clean data.\n- Ordinary Cat variables 'Familty' and 'Education' are also clean.\n- Target variable also looks fine. "},{"metadata":{"trusted":true},"cell_type":"code","source":"### Five point summary of  attributes and label :-","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_transpose = df.describe().T\ndf_transpose[['min', '25%', '50%', '75%', 'max']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Pair plot that includes all the columns of the data frame :-\n\n<b>Note: </b> I am not using ID column in tha pair plot as it is not relevent with our analysis. Id column is jut for record index."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df.iloc[:,1:]) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation : </b> From the above pair plot we can infer the association among the attributes and target column as follows:\n- 'Age' column is normally distributed. Here can see that the mean and midean is almost same which we have also seen in the transpose matrics. Most of the customers age is between 25 to 65 years.\n- 'Experience' is also mormally distributed. Here also mean is amost equal to midean. 'Experience' and 'Age' are stong possitive associasion.\n- 'Income' is positively skewed and it will also have the outlier\n- We dont see any relationship with the ZIP Code and other variables.\n- Family and Education has low association with the 'Personal Loan'. \n- The disribution of CCAvg is also a possotively skewed variable. Majority of the customers average monthly spending is between 1k to 9k.\n- 'Mortgage' is also positively skewed. Majority of the individuals have a mortgage of less than 40K. "},{"metadata":{},"cell_type":"markdown","source":" ### Error Fixing(Data Cleaning) :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the negative values\ndf[df['Experience'] < 0]['Experience'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total records of negative experience\ndf[df['Experience'] < 0]['Experience'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment:</b> There are 52 records with negative Experience are present in the dataset.\n\n\n\n#### Checking the association of Experience with other quantitive variables :"},{"metadata":{"trusted":true},"cell_type":"code","source":"quantitiveVar = ['Age', 'Income', 'Income', 'CCAvg', 'Mortgage']\nexpGrid = sns.PairGrid(df, y_vars = 'Experience', x_vars = quantitiveVar)\nexpGrid.map(sns.regplot)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b> By looking above plots we can see that 'Age' has very strong and possitive association with 'Experience'. I am also considering 'Education' to fix the negative experience error. Becuase as we know experience relats to the education level. \n\n<b>Decission: </b> We can replace each negative 'Experience' value with the median of possitive 'Experience' associated with the particular 'Age' and 'Education' value.\n\n<b>Steps to be followed in the following code: </b> \n- Get the record of experience whose value is greater than 0\n- Get the record of experience whose value is lesser than 0\n- Get the list of Cutomer ID whose experience value is negative\n- Get the list of 'Age' values where it finds negative values in 'Experience' column\n- Get the list of 'Education' values where it finds negaitve values in 'Experience' column\n- Next it filters the records matching the above conditions from the dataframe which has data with possive experience and takes the median and store in exp. There could be chance that there will be no possive experience which matches the above condtion. In such case it matches the above condiiton from the dataframe which has the record with negative experience and gets the median and store in exp.\n- Next it will replace the negative experience with the median.\n- After execution of below code we will check the negative experience again."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Possitive_Experience = df[df['Experience'] > 0]\ndf_Negative_Experience =  df[df['Experience'] < 0]\ndf_Negative_Experience_List = df_Negative_Experience['ID'].tolist()\n\nfor id in df_Negative_Experience_List:\n    age_values = df.loc[np.where(df['ID']==id)][\"Age\"].tolist()[0]\n    education_values = df.loc[np.where(df['ID']==id)][\"Education\"].tolist()[0]\n    possitive_Experience_Filtered = df_Possitive_Experience[(df_Possitive_Experience['Age'] == age_values) & (df_Possitive_Experience['Education'] == education_values)]\n    if possitive_Experience_Filtered.empty :\n        negative_Experience_Filtered = df_Negative_Experience[(df_Negative_Experience['Age'] == age_values) & (df_Negative_Experience['Education'] == education_values)]\n        exp = round(negative_Experience_Filtered['Experience'].median())\n    else:\n        exp = round(possitive_Experience_Filtered['Experience'].median())\n    df.loc[df.loc[np.where(df['ID']==id)].index, 'Experience'] = abs(exp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total records of negative experience\ndf[df['Experience'] < 0]['Experience'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Comment: </b> Now we can see that negative values count is 0 means there is no negative value anymore in the dataframe.\n- Describing the 'Experience' column to check the count, mean, standard deviation and five point summary."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Experience.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment: </b> Now we can see the min is 0.0 which was -3.0 before error fixing.   "},{"metadata":{},"cell_type":"markdown","source":"## ::----------------------------------- Data Visualization --------------------------------::"},{"metadata":{},"cell_type":"markdown","source":"###  Data distribution in each attribute :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['ID'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b> The above graph is unform distribution. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Age'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b> Age column is normaly distrubuted."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Experience'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b>  Experience is also normaly distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Income'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Observation: </b>The above distributionis is right skewed distribution because the tail goes to the right."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['ZIP Code'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b> The is uniformaly distrubuted. Data points are more with family size 1 and 2."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['CCAvg'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Comment: </b> The above distributionis is right skewed distribution because the tail goes to the right. Most of the customers monthly avg. spending on credit cards is between 1k to 2.5K. There are very few customers whose monthly avg. spending on credit card is more than 8k. \n    "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Education'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b> Undergrad level customers are more than the Graduate and Advanced/Professional customers.\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Mortgage'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b>The above distributionis is right skewed distribution because the tail goes to the right. \nMost of the customers do not have mortgage. There are more customers whose mortgage amount is between $80000 to $150000 . Very few customers whos mortgage amount is more than $600000."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Online'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b> This is a Bernoulli Distrubution. Number of customers who have Online accout is geater than the number of customers who do not have online account"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['CreditCard'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b> It is a Bernoulli Distrubution. Number of customers without Credit Card is almost double than the number of customers with Credit Card."},{"metadata":{},"cell_type":"markdown","source":"###  Target column distribution and Data Visualization with Personal Loan Column :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_counts = pd.DataFrame(df[\"Personal Loan\"].value_counts()).reset_index()\nloan_counts.columns =[\"Labels\",\"Personal Loan\"]\nloan_counts","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment:</b> Out of 5000 data points, 4520 are labeled as 0 and 480 as 1. Percentage of customers who took loan is significantlly greater than customers who did not take loan. I have also show the percentage using in pie chart below.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1, ax1 = plt.subplots()\nexplode = (0, 0.15)\nax1.pie(loan_counts[\"Personal Loan\"], explode=explode, labels=loan_counts[\"Labels\"], autopct='%1.1f%%',\n        shadow=True, startangle=70)\nax1.axis('equal')  \nplt.title(\"Personal Loan Percentage\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Important :</b> From the above we can see that the data is having a huge bias(alomot 1:10) towards the category of people not accepting the personal loan. Hence we can build an opinion that our model will tends to perform better towards predicting which customers will not accept the personal loan. However, our goal is to identityfy the customer who can accept the personal loan based on the given features. "},{"metadata":{},"cell_type":"markdown","source":"#### Influence of important features on Personal Loan. From the above pair plot we can see some features which has relationship with the target column i.e personal loan :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Family', y='Income', hue='Personal Loan', data = df, kind='swarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b> Customers who have family size 3 or greater with higher income between 100k to 200k are more likely to take loan."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Education', y='Income', hue='Personal Loan', data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>observation: </b> From above we can say that customers with undergraduate level of education and family greater than 3 are good customers who took loan. Customer who took loan have same income range irrespective of education level. Education of Graduate and above have more chance to take loan."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"Education\", y='Mortgage', hue=\"Personal Loan\", data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation :</b> From the above box plot we can see that customers whose education level is 1 and did not take loan has higher mortgage than customers who take loan of same education level. Customers whose education level is 2and 3 and did not take loan has lesser mortgage than customers who take loan of same education level. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x=\"Securities Account\", data=df,hue=\"Personal Loan\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation :</b> Customers who has securies account are more likly to take loan. Majority of customers who does not have loan do not have securities account. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Family',data=df,hue='Personal Loan')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation :</b> Family size does not have any impact in personal loan. But it seems families with size of 3 and 4 are more likely to take loan."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='CD Account',data=df,hue='Personal Loan')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation: </b> Customers who does not have CD account , does not have loan as well. This seems to be majority. But almost all customers who has CD account has loan as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"CreditCard\", y='CCAvg', hue=\"Personal Loan\", data=df)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation: </b> Customers who have credit card and monthly spending is higher are more likly to take loan."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='Age', y='Experience', hue='Personal Loan', data = df, height=8.27, aspect=11/5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation: </b>From the above we clearly see that  Age and Experience have very storng association. As Age increases Experience also increases. We can try building our model by droping Experience column. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nsns.distplot(df[df[\"Personal Loan\"] == 0]['CCAvg'], color = 'r',label='Personal Loan=0')\nsns.distplot(df[df[\"Personal Loan\"] == 1]['CCAvg'], color = 'b',label='Personal Loan=1')\nplt.legend()\nplt.title(\"CCAvg Distribution\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation: </b> Customers who have taken personal loan have higher credit card average than those who did not take loan. So high credit card average seems to be good predictor of whether or not a customer will take a personal loan."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Credit card spending of Non-Loan customers: ',df[df['Personal Loan'] == 0]['CCAvg'].median()*1000)\nprint('Credit card spending of Loan customers    : ', df[df['Personal Loan'] == 1]['CCAvg'].median()*1000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment:</b> The graph show persons who have personal loan have a higher credit card average. Average credit card spending with a median of 3800 dollar indicates a higher probability of personal loan. Lower credit card spending with a median of 1400 dollars is less likely to take a loan."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nsns.distplot(df[df[\"Personal Loan\"] == 0]['Income'], color = 'r',label='Personal Loan=0')\nsns.distplot(df[df[\"Personal Loan\"] == 1]['Income'], color = 'b',label='Personal Loan=1')\nplt.legend()\nplt.title(\"Income Distribution\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation: </b> Customers who have taken personal loan have income than those who did not take. So high income seems to be good predictor of whether or not a customer will take a personal loan."},{"metadata":{},"cell_type":"markdown","source":"### Outliers Detection :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.boxplot(return_type='axes', figsize=(20,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comment:</b> From the above boxplot we can see there are outliers on few colomns. Mortgage has more number of outlier. Income, CCAvg have also outliers. We will try to fix the outlier by scaling the attributes."},{"metadata":{},"cell_type":"markdown","source":"### Correlation using Heatmap :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,7))\nplt.title('Correlation of Attributes', y=1.05, size=19)\nsns.heatmap(df.corr(), cmap='plasma',annot=True, fmt='.2f')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation:</b> From the above heatmap we can see that :\n- Age and Experience are highly correlated and the correlation is almost 1.\n- 'Income' and 'CCAvg' is moderately correlated.\n- Personal Loan has maximum correlation with 'Income', 'CCAvg', 'CD Account', 'Mortgage', and 'Education'.\n- We can see in above heat map there is association of 'CD Account' with 'Credit Card', 'Securities Account', 'Online', 'CCAvg' and 'Income'.\n- 'Mortgage' has moderate correlation with 'Income' which is about 12%.\n- 'Income' influences 'CCAvg', 'Personal Loan', 'CD Account' and 'Mortgage'.\n"},{"metadata":{},"cell_type":"markdown","source":"# ::------------------------- Model Building ---------------------------------::"},{"metadata":{},"cell_type":"markdown","source":"## Dimensionality Reduction : -\n- As we have seen above 'ID' and 'ZIP Code' are not relevent for our model building so we will drop it.\n- 'Age' and 'Experience' are highly correlated so we will build our model <b>with 'Experience'</b> and <b>without 'Experience'</b> after that we will compare the accurace which will lead us to the conclution that with 'Experience' or without 'Experience' which model is better for prediction. "},{"metadata":{},"cell_type":"markdown","source":"#### Dropping 'ID' and 'ZIP Code' :"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['ID','ZIP Code'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating two dataframes with 'Experience' and without 'Experience' repectively :"},{"metadata":{"trusted":true},"cell_type":"code","source":"loan_with_experience = df\nloan_without_experience = df.drop(['Experience'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Columns With Experience : ', loan_with_experience.columns)\nprint('Columns Without Experience : ', loan_without_experience.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Seperating Target Variable from Independent Variables from Expr and Wihtout Expr dataframe :\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# From Exprenece Dataframe:\nX_Expr = loan_with_experience.drop('Personal Loan', axis=1)\nY_Expr = loan_with_experience[['Personal Loan']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From Exprenece Dataframe:\nX_Without_Expr = loan_without_experience.drop('Personal Loan', axis=1)\nY_Without_Expr = loan_without_experience[['Personal Loan']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spliting the data into training and test set in the ratio of 70:30 respectively :-"},{"metadata":{"trusted":true},"cell_type":"code","source":"# From Experience Dataframe:\nX_Expr_train, X_Expr_test, y_Expr_train, y_Expr_test = train_test_split(X_Expr, Y_Expr, test_size=0.30, random_state=1)\nprint('x train data {}'.format(X_Expr_train.shape))\nprint('y train data {}'.format(y_Expr_train.shape))\nprint('x test data  {}'.format(X_Expr_test.shape))\nprint('y test data  {}'.format(y_Expr_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From Without Experience Dataframe:\nX_train, X_test, y_train, y_test = train_test_split(X_Without_Expr, Y_Without_Expr, test_size=0.30, random_state=1)\nprint('x train data {}'.format(X_train.shape))\nprint('y train data {}'.format(y_train.shape))\nprint('x test data  {}'.format(X_test.shape))\nprint('y test data  {}'.format(y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ::---------------------------- Logistic Regression ---------------------------------------::"},{"metadata":{},"cell_type":"markdown","source":"### With Experience Column:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_Exp_train, X_Exp_test, y_Exp_train, y_Exp_test\nlogreg_expr_model = LogisticRegression()\nlogreg_expr_model.fit(X_Expr_train, y_Expr_train)\nprint(logreg_expr_model , '\\n')\n\n# Predicting for test set\nlogreg_expr_y_predicted = logreg_expr_model.predict(X_Expr_test)\nlogreg_expr_score = logreg_expr_model.score(X_Expr_test, y_Expr_test)\nlogreg_expr_accuracy = accuracy_score(y_Expr_test, logreg_expr_y_predicted)\n\nlogestic_confusion_matrix_expr = metrics.confusion_matrix(y_Expr_test, logreg_expr_y_predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Without Experience Column:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train, X_test, y_train, y_test\nlogreg_model = LogisticRegression()\nlogreg_model.fit(X_train, y_train)\n\n# Predicting for test set\nlogreg_y_predicted = logreg_model.predict(X_test)\nlogreg_score = logreg_model.score(X_test, y_test)\nlogreg_accuracy = accuracy_score(y_test, logreg_y_predicted)\nlogestic_confusion_matrix = metrics.confusion_matrix(y_test, logreg_y_predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comparison :</b> Below is the comparison b/w Logistic Regression Model Accuracy and Confussion Matrix with 'Experience' and W/O 'Experience'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy\nprint('Logistic Regression Model Accuracy Score W/O Experience  : %f'  % logreg_accuracy)\nprint('Logistic Regression Model Accuracy Score With Experience : %f'  % logreg_expr_accuracy)\n\n# Confusion Matrix\nprint('\\nLogistic Regression Confusion Matrix W/O Experience: \\n', logestic_confusion_matrix)\nprint('\\nTrue Possitive    = ', logestic_confusion_matrix[1][1])\nprint('True Negative     = ',   logestic_confusion_matrix[0][0])\nprint('False Possive     = ',   logestic_confusion_matrix[0][1])\nprint('False Negative    = ',   logestic_confusion_matrix[1][0])\nprint('\\nLogistic Regression Confusion Matrix With Experience: \\n', logestic_confusion_matrix_expr)\nprint('\\nTrue Possitive    = ', logestic_confusion_matrix_expr[1][1])\nprint('True Negative     = ',   logestic_confusion_matrix_expr[0][0])\nprint('False Possive     = ',   logestic_confusion_matrix_expr[0][1])\nprint('False Negative    = ',   logestic_confusion_matrix_expr[1][0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation: </b> \n- From the above accuracy results we see that accuracy is higher with 'Experience' (94.60 %) than without 'Experience' (94.26%). \n- Also from the above confusion metrices we can see that the prediction of customers who dont accept loan and the customers who accept loan is better with 'Experience'.\n- <b>Type 1 (False Possitive)</b> and <b>Type 2(False Negative)</b> errors is less with experience.\n- Hence we can imporove the accuracy by scalling the attributes.\n- We will not consider dafaframe without 'Experience' for further iteration."},{"metadata":{},"cell_type":"markdown","source":"### Improvement of  the model -------- Iteration 2 For Logistic Regression with Experience--------"},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_Expr_train, X_Expr_test, y_Expr_train, y_Expr_test\nX_train_scaled = preprocessing.scale(X_Expr_train)\nX_test_scaled = preprocessing.scale(X_Expr_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_logreg_model = LogisticRegression()\nscaled_logreg_model.fit(X_train_scaled, y_Expr_train)\n\n# Predicting for test set\nscaled_logreg_y_predicted = scaled_logreg_model.predict(X_test_scaled)\nscaled_logreg_model_score = scaled_logreg_model.score(X_test_scaled, y_Expr_test)\nscaled_logreg_accuracy = accuracy_score(y_Expr_test, scaled_logreg_y_predicted)\n\nscaled_logreg_confusion_matrix = metrics.confusion_matrix(y_Expr_test, scaled_logreg_y_predicted)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('----------------------Final Analysis of Logistic Regression----------------------------\\n')\nprint('After Scalling Logistic Regression Model Accuracy Score with Experience: %f'  % scaled_logreg_accuracy)\nprint('\\nAfter Scalling Logistic Regression Confusion Matrix With Experience: \\n', scaled_logreg_confusion_matrix)\nprint('\\nTrue Possitive    = ', scaled_logreg_confusion_matrix[1][1])\nprint('True Negative     = ',   scaled_logreg_confusion_matrix[0][0])\nprint('False Possive     = ',   scaled_logreg_confusion_matrix[0][1])\nprint('False Negative    = ',   scaled_logreg_confusion_matrix[1][0])\nprint('\\nK-NN classification Report : \\n',metrics.classification_report(y_Expr_test, scaled_logreg_y_predicted))\nconf_table = scaled_logreg_confusion_matrix\na = (conf_table[0,0] + conf_table[1,1]) / (conf_table[0,0] + conf_table[0,1] + conf_table[1,0] + conf_table[1,1])\np = conf_table[1,1] / (conf_table[1,1] + conf_table[0,1])\nr = conf_table[1,1] / (conf_table[1,1] + conf_table[1,0])\nf = (2 * p * r) / (p + r)\nprint(\"Accuracy of accepting Loan  : \",round(a,2))\nprint(\"precision of accepting Loan : \",round(p,2))\nprint(\"recall of accepting Loan    : \",round(r,2))\nprint(\"F1 score of accepting Loan  : \",round(f,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ::------------------------------ K-NN --------------------------------------------::\n\n### Steps to be followed:-\n- We will follow the same procedures as we have followed in Logistic Regression.\n- We will try the model with 'Experience' and without 'Experience'.\n- will run the KNN with number of odd neighbours ranges from 1 to 20 and will find the optimal number of neighbours using the Mis classification error.\n- After finding <b>best K</b>, will build the model using 'Experience' and without 'Experience'. "},{"metadata":{},"cell_type":"markdown","source":"#### Below Code is to find the best K(Neighbors)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating number list from range 1 to 20 of K for KNN\n\nnumberList = list(range(1,20))\nneighbors = list(filter(lambda x: x % 2 != 0 , numberList)) #subsetting just the odd ones\n\n#Declearing a empty list that will hold the accuracy scores\nac_scores = []\n#performing accuracy metrics for value from 1,3,5....19\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    #predict the response\n    knn.fit(X_train, y_train.values.ravel())               \n    y_pred = knn.predict(X_test)\n    #evaluate accuracy\n    scores = accuracy_score(y_test, y_pred)\n    #insert scores to the list\n    ac_scores.append(scores)                \n\nMSE = [1 - x for x in ac_scores] # changing to misclassification error\n\n\n# determining best k\noptimal_k = neighbors[MSE.index(min(MSE))]\n\nprint('Odd Neighbors : \\n', neighbors)\nprint('\\nAccuracy Score : \\n', ac_scores)\nprint('\\nMisclassification error :\\n', MSE)\nprint(\"\\nThe optimal number of neighbor is k=\",optimal_k)\n\n# plot misclassification error vs k\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Decission: </b> \n- From above result we can see that the misclassification error is minimum at <b>k = 3</b>. \n- Hence I am considering 3 is the optimal k. When k=3 the model accuracy is 0.909.\n- From above graph of misclassification error vs k (with k value on X-axis) we can also see that error is very low when K is 3."},{"metadata":{},"cell_type":"markdown","source":"### Model building using  'Without Experience' dataframe:-\n- We have already created the dataframe and splited the data logistic regression, will be using the same.\n- Splited data from 'Without Experience' dataframe: X_train, X_test, y_train, y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiating learning model (optimal_k = 3)\nknn_model = KNeighborsClassifier(n_neighbors=optimal_k , weights = 'uniform', metric='euclidean')\nknn_model.fit(X_train, y_train)\nknn_y_predicted = knn_model.predict(X_test)\nknn_score = knn_model.score(X_test, y_test)\nknn_accuracy = accuracy_score(y_test, knn_y_predicted)\nknn_confusion_matrix = metrics.confusion_matrix(y_test, knn_y_predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model building using  'With Experience' dataframe:-\n- We have already created the dataframe and splited the data logistic regression, will be using the same.\n- Splited data from 'With Experience' dataframe: X_Expr_train, X_Expr_test, y_Expr_train, y_Expr_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiating learning model (optimal_k = 3)\nknn_model_expr = KNeighborsClassifier(n_neighbors=optimal_k , weights = 'uniform', metric='euclidean')\nknn_model_expr.fit(X_Expr_train, y_Expr_train)\nknn_expr_y_predicted = knn_model_expr.predict(X_Expr_test)\nknn_expr_score = knn_model_expr.score(X_Expr_test, y_Expr_test)\nknn_expr_accuracy = accuracy_score(y_Expr_test, knn_expr_y_predicted)\nknn_confusion_matrix_expr = metrics.confusion_matrix(y_Expr_test, knn_expr_y_predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Comparison :</b> Below is the comparison b/w K-NN Model Accuracy and Confussion Matrix with 'Experience' and W/O 'Experience'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparison \nprint('K-NN Model Accuracy Score W/O Experience  : %f'  % knn_accuracy)\nprint('K-NN Model Accuracy Score With Experience : %f'  % knn_expr_accuracy)\n\n# Confusion Matrix\nprint('\\nK-NN Confusion Matrix W/O Experience: \\n', knn_confusion_matrix)\nprint('\\nTrue Possitive    = ', knn_confusion_matrix[1][1])\nprint('True Negative     = ',   knn_confusion_matrix[0][0])\nprint('False Possive     = ',   knn_confusion_matrix[0][1])\nprint('False Negative    = ',   knn_confusion_matrix[1][0])\nprint('\\nK-NN Confusion Matrix With Experience: \\n', knn_confusion_matrix_expr)\nprint('\\nTrue Possitive    = ', knn_confusion_matrix_expr[1][1])\nprint('True Negative     = ',   knn_confusion_matrix_expr[0][0])\nprint('False Possive     = ',   knn_confusion_matrix_expr[0][1])\nprint('False Negative    = ',   knn_confusion_matrix_expr[1][0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation: </b> \n- From the above accuracy results we see that accuracy is higher at without 'Experience' (90.93%) than with 'Experience' (90.20%). \n- Also from the above confusion metrices we can see that the prediction of customers who dont accept loan and the customers who accept loan is better at without 'Experience'.\n- <b>Type 1 (False Possitive)</b> and <b>Type 2(False Negative)</b> errors is less at without Experience.\n- Hence we can imporove the accuracy by scalling the attributes.\n- We will not consider dafaframe 'With Experience' for further iteration."},{"metadata":{},"cell_type":"markdown","source":"### Improvement of  the model -------- Iteration 2 For K-NN without Experience dataset--------"},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train, X_test, y_train, y_test\nX_train_scaled = preprocessing.scale(X_train)\nX_test_scaled = preprocessing.scale(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_knn_model = KNeighborsClassifier(n_neighbors=optimal_k , weights = 'uniform', metric='euclidean')\nscaled_knn_model.fit(X_train_scaled, y_train)\nscaled_knn_y_predict = scaled_knn_model.predict(X_test_scaled)\nscaled_knn_score = scaled_knn_model.score(X_test_scaled, y_test)\nscaled_knn_accuracy = accuracy_score(y_test, scaled_knn_y_predict)\nscaled_knn_confusion_matrix = metrics.confusion_matrix(y_test, scaled_knn_y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('----------------------Final Analysis of K-NN----------------------------\\n')\nprint('After Scalling K-NN Model Accuracy Score without Experience: %f'  % scaled_knn_accuracy)\nprint('\\nAfter Scalling K-NN Confusion Matrix Without Experience: \\n', scaled_knn_confusion_matrix)\nprint('\\nTrue Possitive    = ', scaled_knn_confusion_matrix[1][1])\nprint('True Negative     = ',   scaled_knn_confusion_matrix[0][0])\nprint('False Possive     = ',   scaled_knn_confusion_matrix[0][1])\nprint('False Negative    = ',   scaled_knn_confusion_matrix[1][0])\nprint('\\nK-NN classification Report : \\n',metrics.classification_report(y_test, scaled_knn_y_predict))\nknn_conf_table = scaled_knn_confusion_matrix\na = (knn_conf_table[0,0] + knn_conf_table[1,1]) / (knn_conf_table[0,0] + knn_conf_table[0,1] + knn_conf_table[1,0] + knn_conf_table[1,1])\np = knn_conf_table[1,1] / (knn_conf_table[1,1] + knn_conf_table[0,1])\nr = knn_conf_table[1,1] / (knn_conf_table[1,1] + knn_conf_table[1,0])\nf = (2 * p * r) / (p + r)\nprint(\"\\nAccuracy of accepting Loan  : \",round(a,2))\nprint(\"precision of accepting Loan : \",round(p,2))\nprint(\"recall of accepting Loan    : \",round(r,2))\nprint(\"F1 score of accepting Loan  : \",round(f,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ::-------------------------- NaÃ¯ve Bayes -------------------------------------::\n\n\n### Steps to be followed:-\n- We will follow the same procedures as we have done in above models.\n- We will try the model with 'Experience' and without 'Experience'."},{"metadata":{},"cell_type":"markdown","source":"### Model building using  'Without Experience' dataframe:-\n- Splited data from 'Without Experience' dataframe: X_train, X_test, y_train, y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb_model = GaussianNB()\ngnb_model.fit(X_train, y_train)\ngnb_y_predicted = gnb_model.predict(X_test)\ngnb_score = gnb_model.score(X_test, y_test)\ngnb_accuracy = accuracy_score(y_test, gnb_y_predicted)\ngnb_confusion_matrix = metrics.confusion_matrix(y_test, gnb_y_predicted)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model building using  'With Experience' dataframe:-\n- Splited data from 'With Experience' dataframe: X_Expr_train, X_Expr_test, y_Expr_train, y_Expr_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"gnb_expr_model = GaussianNB()\ngnb_expr_model.fit(X_Expr_train, y_Expr_train)\ngnb_expr_y_predicted = gnb_expr_model.predict(X_Expr_test)\ngnb_expr_score = gnb_expr_model.score(X_Expr_test, y_Expr_test)\ngnb_expr_accuracy = accuracy_score(y_Expr_test, gnb_expr_y_predicted)\ngnb_expr_confusion_matrix = metrics.confusion_matrix(y_Expr_test, gnb_expr_y_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparison \nprint('NaÃ¯ve Bayes Model Accuracy Score W/O Experience  : %f'  % gnb_accuracy)\nprint('NaÃ¯ve Bayes Model Accuracy Score With Experience : %f'  % gnb_expr_accuracy)\n\n# Confusion Matrix\nprint('\\nNaÃ¯ve Bayes Confusion Matrix W/O Experience: \\n', gnb_confusion_matrix)\nprint('\\nTrue Possitive    = ', gnb_confusion_matrix[1][1])\nprint('True Negative     = ',   gnb_confusion_matrix[0][0])\nprint('False Possive     = ',   gnb_confusion_matrix[0][1])\nprint('False Negative    = ',   gnb_confusion_matrix[1][0])\nprint('\\nNaÃ¯ve Bayes Confusion Matrix With Experience: \\n', gnb_expr_confusion_matrix)\nprint('\\nTrue Possitive    = ', gnb_expr_confusion_matrix[1][1])\nprint('True Negative     = ',   gnb_expr_confusion_matrix[0][0])\nprint('False Possive     = ',   gnb_expr_confusion_matrix[0][1])\nprint('False Negative    = ',   gnb_expr_confusion_matrix[1][0])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Observation: </b> \n- From the above accuracy results we see that accuracy is alomost at without 'Experience' and with 'Experience'. \n- Also from the above confusion metrices we can see that the prediction of customers who dont accept loan and the customers who accept loan is almost same at without 'Experience'.\n- <b>Type I (False Possitive)</b> and <b>Type II(False Negative)</b> errors is same.\n- Hence we can imporove the accuracy by scalling the attributes.\n- We can consider any dafaframe 'With Experience' or 'Without Experience' for further iteration."},{"metadata":{},"cell_type":"markdown","source":"### Improvement of  the model -------- Iteration 2 For NaÃ¯ve Bayes without Experience dataset--------\n- We have already scaled the attribute in K-NN model building, will be using the same\n- X_train_scaled and X_test_scaled"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_gnb_model = GaussianNB()\nscaled_gnb_model.fit(X_train_scaled, y_train)\nscaled_gnb_y_predict = scaled_gnb_model.predict(X_test_scaled)\nscaled_gnb_score = scaled_gnb_model.score(X_test_scaled, y_test)\nscaled_gnb_accuracy = accuracy_score(y_test, scaled_gnb_y_predict)\nscaled_gnb_connfusion_matrix = metrics.confusion_matrix(y_test, scaled_gnb_y_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint('----------------------Final Analysis of NaÃ¯ve Bayes----------------------------\\n')\nprint('After Scalling NaÃ¯ve Bayes Model Accuracy Score: %f'  % scaled_gnb_accuracy)\nprint('\\nAfter Scalling NaÃ¯ve Bayes Confusion Matrix: \\n', scaled_gnb_connfusion_matrix)\nprint('\\nTrue Possitive    = ', scaled_gnb_connfusion_matrix[1][1])\nprint('True Negative     = ',   scaled_gnb_connfusion_matrix[0][0])\nprint('False Possive     = ',   scaled_gnb_connfusion_matrix[0][1])\nprint('False Negative    = ',   scaled_gnb_connfusion_matrix[1][0])\nprint('\\n Gaussian Naive Bayes classification Report : \\n',metrics.classification_report(y_test, gnb_y_predicted))\ngnb_conf_table = scaled_gnb_connfusion_matrix\na = (gnb_conf_table[0,0] + gnb_conf_table[1,1]) / (gnb_conf_table[0,0] + gnb_conf_table[0,1] + gnb_conf_table[1,0] + knn_conf_table[1,1])\np = gnb_conf_table[1,1] / (gnb_conf_table[1,1] + gnb_conf_table[0,1])\nr = gnb_conf_table[1,1] / (gnb_conf_table[1,1] + gnb_conf_table[1,0])\nf = (2 * p * r) / (p + r)\nprint(\"\\nAccuracy of accepting Loan   : \",round(a,2))\nprint(\"precision of accepting Loan  : \",round(p,2))\nprint(\"recall of accepting Loan     : \",round(r,2))\nprint(\"F1 score of accepting Loan   : \",round(f,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# =========== COMPARISON OF ABOVE THREE MODELS =========="},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Overall Model Accuracy After scaling:\\n')\nprint ('Logistic Regression : {0:.0f}%'. format(scaled_logreg_accuracy * 100))\nprint ('K-Nearest Neighbors : {0:.0f}%'. format(scaled_knn_accuracy * 100))\nprint ('Naive Bayes         : {0:.0f}%'. format(scaled_gnb_accuracy * 100))\n\nprint('\\nOverall Model Confusion matrix After scaling:\\n')\nprint('\\nLogistic Regression: \\n', scaled_logreg_confusion_matrix)\nprint('\\n     True Possitive    = ', scaled_logreg_confusion_matrix[1][1])\nprint('     True Negative     = ',   scaled_logreg_confusion_matrix[0][0])\nprint('     False Possive     = ',   scaled_logreg_confusion_matrix[0][1])\nprint('     False Negative    = ',   scaled_logreg_confusion_matrix[1][0])\n\nprint('\\nK-Nearest Neighbors: \\n', scaled_knn_confusion_matrix)\nprint('\\n    True Possitive    = ', scaled_knn_confusion_matrix[1][1])\nprint('    True Negative     = ',   scaled_knn_confusion_matrix[0][0])\nprint('    False Possive     = ',   scaled_knn_confusion_matrix[0][1])\nprint('    False Negative    = ',   scaled_knn_confusion_matrix[1][0])\n\nprint('\\nNaive Bayes: \\n', scaled_gnb_connfusion_matrix)\nprint('\\n    True Possitive    = ', scaled_gnb_connfusion_matrix[1][1])\nprint('    True Negative     = ',   scaled_gnb_connfusion_matrix[0][0])\nprint('    False Possive     = ',   scaled_gnb_connfusion_matrix[0][1])\nprint('    False Negative    = ',   scaled_gnb_connfusion_matrix[1][0])\n\n\nprint('\\n\\nReceiver Operating Characteristic (ROC) curve to evalute the classifier output quality.  If area of curve is closer to 1 which means better the model and if area of curve is closer to 0 which means poor the model.')\n\nknn_fpr, knn_tpr, knn_threshold = metrics.roc_curve(y_test, scaled_knn_y_predict)\nknn_roc_auc = metrics.roc_auc_score(y_test, scaled_knn_y_predict)\nfig1_graph = plt.figure(figsize=(15,4))\nfig1_graph.add_subplot(1,3,1)\nplt.plot(knn_fpr, knn_tpr, label='KNN Model (area = %0.2f)' % knn_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic (ROC)')\nplt.legend(loc=\"lower right\")\n\n\nlogistic_fpr, logistic_tpr, logistic_threshold = metrics.roc_curve(y_Expr_test, scaled_logreg_y_predicted)\nlogistic_roc_auc = metrics.roc_auc_score(y_Expr_test, scaled_logreg_y_predicted)\nfig1_graph.add_subplot(1,3,2)\nplt.plot(logistic_fpr, logistic_tpr, label='Logistic Model (area = %0.2f)' % logistic_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic (ROC)')\nplt.legend(loc=\"lower right\")\n\nnb_fpr, nb_tpr, nb_threshold = metrics.roc_curve(y_test, scaled_gnb_y_predict)\nnb_roc_auc = metrics.roc_auc_score(y_test, scaled_gnb_y_predict)\nfig1_graph.add_subplot(1,3,3)\nplt.plot(nb_fpr, nb_tpr, label='Naive-Bayes Model (area = %0.2f)' % nb_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic (ROC)')\nplt.legend(loc=\"lower right\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Final Conclusion to Predict the best Model ::-"},{"metadata":{},"cell_type":"markdown","source":"- From the above, we can clearly see that k-Nearest Neighbors Alogorithm with scaled data gives us best accuracy of 96%. \n- Also the <b>Type I(False Posssitive)</b> and <b>Type II(False Negative)</b> errors are least in K-Nearest model. \n- The area in ROC curve for K-NN is 0.82 which is close to 1 which stats that K-NN is the best model in comparesion of Logistic Model and Naive-Bayes Model whose ROC area is 0.79 and 0.73 repectively.\n- Hence among the above three algorithm applied on the underline dataset, K-NN would be the best choice to predict the customers who will accept the personal loan."},{"metadata":{},"cell_type":"markdown","source":"## ---------------------------------------END----------------------------------------------"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}