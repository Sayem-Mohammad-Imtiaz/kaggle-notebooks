{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Text classification\n\nThis notebook covers 3 different approaches to a text classification problem (fake news dataset).\n\n1. TF-IDF features with various classifiers from *scikit-learn*.\n2. GloVe with LSTM in Keras.\n3. BERT in PyTorch using the *transformers* library.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install transformers==3.0.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from abc import ABC\nimport random\nimport multiprocessing\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\n\ntorch.manual_seed(0)\nnp.random.seed(0)\nrandom.seed(0)\n\n\ndef read_fake_news_raw_data(train_test_ratio=0.9, train_valid_ratio=0.8, max_samples=None):\n    raw_data_path = '../input/real-and-fake-news-dataset/news.csv'\n    df_raw = pd.read_csv(raw_data_path)\n\n    if max_samples:\n        df_raw = df_raw.head(max_samples)\n\n    # Prepare columns\n    df_raw['label'] = (df_raw['label'] == 'FAKE').astype('int')\n    df_raw['titletext'] = df_raw['title'] + \". \" + df_raw['text']\n\n    df_train, df_test = train_test_split(df_raw, train_size=train_test_ratio, random_state=1)\n    df_valid = pd.DataFrame()\n    if train_valid_ratio < 1:\n        df_train, df_valid = train_test_split(df_train, train_size=train_valid_ratio, random_state=1)\n        \n    print('train, valid, test:', df_train.shape, df_valid.shape, df_test.shape)\n    return df_train, df_valid, df_test\n\n\ndef report(y_pred, y_true):\n    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n    print(confusion_matrix(y_true, y_pred, labels=[1,0]))\n    \n\ndef run_experiments(models, max_samples=None, train=True):\n    df_train, df_valid, df_test = read_fake_news_raw_data(max_samples=max_samples)\n    for model in models:\n        print(model)\n        if train:\n            model.train(df_train, df_valid)\n        y_preds = model.predict(df_test)\n        report(y_preds, df_test['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TextDataset(torch.utils.data.Dataset):\n\n    def __init__(self, df, pretrained_model_name='bert-base-uncased', max_len=512):\n        tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n        self.inputs = []\n        self.labels = torch.from_numpy(np.asarray(df['label'], dtype=np.int32)).long()\n        for titletext in df['titletext']:\n            split = titletext.split(maxsplit=512)\n            text = ' '.join(split[:512])\n            inputs = tokenizer(titletext, max_length=max_len, pad_to_max_length=True, return_tensors='pt')\n            self.inputs.append(inputs)\n        \n    def __getitem__(self, idx):\n        item = self.inputs[idx]\n        return item['input_ids'][0], item['attention_mask'][0], item['token_type_ids'][0], self.labels[idx]\n        \n    def __len__(self):\n        return len(self.inputs)\n\n\nclass BertModel:\n    \n    def __init__(self, pretrained_model_name='bert-base-uncased', freeze_bert=False, max_len=512):\n        self.num_epochs = 5\n        self.freeze_bert = freeze_bert\n        self.max_len = max_len\n        self.pretrained_model_name = pretrained_model_name\n\n    def train(self, df_train, df_valid):\n        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        self._model = BertForSequenceClassification.from_pretrained(self.pretrained_model_name).to(self.device)\n        self._model.bert.requires_grad = not self.freeze_bert\n        \n        train_loader = self._create_data_loader(df_train)\n        valid_loader = self._create_data_loader(df_valid)\n        \n        optimizer = optim.Adam(self._model.parameters(), lr=2e-5)\n        best_valid_loss = float(\"Inf\")\n\n        for epoch in range(self.num_epochs):\n            self._model.train()\n            train_loss = 0.0\n            valid_loss = 0.0\n\n            for batch in train_loader:\n                loss, _ = self.model_forward(batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                train_loss += loss.item()\n\n            self._model.eval()\n            with torch.no_grad():                    \n                for batch in valid_loader:\n                    loss, _ = self.model_forward(batch)\n                    valid_loss += loss.item()\n\n            train_loss = train_loss / len(train_loader)\n            valid_loss = valid_loss / len(valid_loader)\n\n            print((f'Epoch [{epoch+1}/{self.num_epochs}] '\n                   f'Train Loss: {train_loss:.4f} Valid Loss: {valid_loss:.4f}'))\n\n            if valid_loss < best_valid_loss:\n                best_valid_loss = valid_loss\n                torch.save(self._model.state_dict(), 'checkpoint.pt')\n            else:\n                # restore the best weights and quit\n                self._model.load_state_dict(torch.load('checkpoint.pt', map_location=self.device))\n                break\n        \n    def predict(self, df_test):\n        y_probs = self.predict_probs(df_test)\n        return torch.argmax(y_probs, 1).tolist()\n    \n    def predict_probs(self, df_test):\n        test_loader = self._create_data_loader(df_test)\n        outputs = []\n        # the model doesn't output probabilities, because Softmax is hidden in the loss function for better performance\n        softmax = nn.Softmax(dim=1)\n\n        self._model.eval()\n        with torch.no_grad():\n            for batch in test_loader:\n                _, output = self.model_forward(batch)\n                output = softmax(output)\n                outputs.append(output)\n        return torch.cat(outputs, dim=0).cpu()\n    \n    def model_forward(self, batch):\n        input_ids, attention_mask, token_type_ids, labels = batch\n        \n        input_ids = input_ids.to(self.device)\n        attention_mask = attention_mask.to(self.device)\n        token_type_ids =token_type_ids.to(self.device)\n        labels = labels.to(self.device)\n        \n        loss, text_fea = self._model(input_ids, attention_mask, token_type_ids, labels=labels)[:2]\n        return loss, text_fea\n\n    def _create_data_loader(self, df):\n        dataset = TextDataset(df, self.pretrained_model_name, max_len=self.max_len)\n        return DataLoader(dataset, batch_size=8, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SciKitClassifierModel(ABC):\n    \n    def train(self, df_train, df_valid):\n        df_train = pd.concat([df_train, df_valid])\n        self.vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=100000)\n        self.vectorizer.fit_transform(df_train['titletext'])\n        X_train = self.vectorize(df_train)\n        y_train = df_train['label']\n        self.model.fit(X_train, y_train)\n        \n    def predict(self, df_test):\n        X_test = self.vectorize(df_test)\n        return self.model.predict(X_test)\n        \n    def predict_probs(self, df_test):\n        X_test = self.vectorize(df_test)\n        return self.model.predict_proba(X_test)\n    \n    def vectorize(self, df):\n        return self.vectorizer.transform(df['titletext'])\n\n\nclass PACModel(SciKitClassifierModel):\n\n    def __init__(self):\n        self.model = PassiveAggressiveClassifier(max_iter=50)\n        \n    def predict_probs(self, df_test):\n        X_test = self.vectorize(df_test)\n        return self.model._predict_proba_lr(X_test)\n    \n\nclass SVMModel(SciKitClassifierModel):\n    \n    def __init__(self):\n        self.model = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n        \n\nclass GradientBoostingModel(SciKitClassifierModel):\n    \n    def __init__(self):\n        self.model = GradientBoostingClassifier()\n        \n\nclass Ensemble:\n    \n    def __init__(self, models):\n        self.models = models\n    \n    def train(self, df_train, df_valid):\n        for model in self.models:\n            model.train(df_train, df_valid)\n        \n    def predict(self, df_test):\n        y_probs = self.predict_probs(df_test)\n        return np.argmax(y_probs, 1)\n        \n    def predict_probs(self, df_test):\n        predictions = np.stack([np.array(model.predict_probs(df_test)) for model in self.models])\n        return np.mean(predictions, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GloveModel:\n    \n    def __init__(self, max_len=1024):\n        self.max_len = max_len\n        self.tokenizer = text.Tokenizer(num_words=None)\n    \n    def train(self, df_train, df_valid):\n        self.tokenizer.fit_on_texts(list(df_train['titletext']) + list(df_valid['titletext']))\n        \n        X_train = self._vectorize(df_train)\n        X_valid = self._vectorize(df_valid)\n        y_train = np_utils.to_categorical(df_train['label'])\n        y_valid = np_utils.to_categorical(df_valid['label'])\n        \n        self.model = self._init_model(y_train.shape[1])\n        earlystop = EarlyStopping(monitor='val_loss', restore_best_weights=True)\n        # note that keras automatically uses GPU if available\n        self.model.fit(X_train, y=y_train, batch_size=256, epochs=10, verbose=1, \n                       validation_data=(X_valid, y_valid), callbacks=[earlystop])\n    \n    def predict(self, df_test):\n        y_probs = self.predict_probs(df_test)\n        return np.argmax(y_probs, 1)\n    \n    def predict_probs(self, df_test):\n        X_test = self._vectorize(df_test)\n        return self.model.predict(X_test)\n    \n    def _vectorize(self, df):\n        seq = self.tokenizer.texts_to_sequences(df['titletext'])\n        return sequence.pad_sequences(seq, maxlen=self.max_len)\n        \n    def _init_embedding(self):\n        # in kaggle notebooks, just hit \"Add data\" in upper right and search for glove.840B.300d.pkl\n        embeddings_index = np.load('../input/pickled-glove840b300d-for-10sec-loading/glove.840B.300d.pkl', allow_pickle=True)\n\n        word_index = self.tokenizer.word_index\n        embedding_matrix = np.zeros((len(word_index) + 1, 300))\n        for word, i in word_index.items():\n            embedding_vector = embeddings_index.get(word)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n        return embedding_matrix\n    \n    def _init_model(self, output_shape):\n        embedding_matrix = self._init_embedding()\n        model = Sequential()\n        model.add(Embedding(len(self.tokenizer.word_index) + 1, 300, weights=[embedding_matrix], \n                            input_length=self.max_len, trainable=True))\n        model.add(Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3)))\n\n        model.add(Dense(512, activation='relu'))\n        model.add(Dropout(0.5))\n\n        model.add(Dense(output_shape))\n        model.add(Activation('softmax'))\n        model.compile(loss='categorical_crossentropy', optimizer='adam')\n        return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# classic models\nclassic_models = [\n    PACModel(),\n    GradientBoostingModel(), \n    SVMModel(),\n]\nrun_experiments(classic_models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GloVe models\n# there are some issues with cleaning up GPU memory after Keras uses it, so let's run this in a subprocess\ndef run_glove_models():\n    glove_models = [\n        GloveModel(max_len=512),\n        GloveModel(),\n    ]\n    run_experiments(glove_models)\np = multiprocessing.Process(target=run_glove_models)\np.start()\np.join()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BERT models\nbert_models = [\n    BertModel(max_len=128),\n    BertModel(freeze_bert=True),\n    BertModel('bert-base-cased'),\n    BertModel(),\n]\nrun_experiments(bert_models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensambles\nensemble_models = [\n    Ensemble(classic_models[:2] + bert_models[-1:]),\n    Ensemble(classic_models[:2] + bert_models[-2:]),\n]\nrun_experiments(ensemble_models, train=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next possible steps for ensembles would be:\n1. Weighted average of the models. Estimate the weights by looking into individual models performance on validation set.\n2. Model stacking - train a model given individual models outputs and true outputs.\n\n#### Notes about the results\n- If the accuracy difference doesn't look big, think about the relative error improvement.\n- This is a pretty simple task. The new approaches shine in harder tasks like GLUE or Squad, where BERT is superhuman and the difference from the other approaches is much larger.\n- The TF-IDF methods are still relevant for many tasks, perform pretty well, the code is super simple and much faster than the new approaches.\n\n\n#### Acknowlegements\n- Illustrated \n[Attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/),\n[Transformer](http://jalammar.github.io/illustrated-transformer/),\n[BERT](http://jalammar.github.io/illustrated-bert/),\n[GPT-2](http://jalammar.github.io/illustrated-gpt2/)\n- [BERT Text Classification Using Pytorch](https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b)\n- [Approaching (Almost) Any NLP Problem on Kaggle](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle)\n- [Fake News Classification](https://www.kaggle.com/harrycheng5/fake-news-classification)\n- [Pytorch Bert -Plain](https://www.kaggle.com/phoenix9032/pytorch-bert-plain)\n- [Stacked Regressions](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}