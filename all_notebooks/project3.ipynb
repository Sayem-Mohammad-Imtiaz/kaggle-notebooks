{"cells":[{"metadata":{"_uuid":"cf12a60869da47ad3436f88e25247266aa9f5014"},"cell_type":"markdown","source":"# Text Analysis\n**Dataset : Women's E-Commerce Clothing Review**"},{"metadata":{"_uuid":"365ef7be95a32fb90cc55c3be004cf79b8b930d1"},"cell_type":"markdown","source":"**1. Preprocessing**\n* Tokenization\n* Removing Stop Words\n* Lemmetizing (Stemming produces some words that are not present in the actual dictionary)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom scipy.spatial.distance import cosine\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_curve, auc\n%matplotlib inline\nimport nltk\nfrom nltk.tokenize import sent_tokenize, PunktSentenceTokenizer, word_tokenize\nimport os\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\n\ncorpus = pd.read_csv('../input/Womens Clothing E-Commerce Reviews.csv')\ncorpus = corpus[['Review Text', 'Clothing ID']]\n# We have selected Clothing ID 1080 for oyr analysis\ncorpus = corpus.loc[corpus['Clothing ID'] == 1080 ]\ncorpus = corpus['Review Text']\n#  Dropping the entries with empty reviews\ncorpus.dropna(inplace = True)\n# Tokenizing the corpus\nsent_tokenized_corpus = []\n# To store the reviews\nreviewsList = []\n# Firstly we are tokenizing reviews using sentence tokenizer\nfor review in corpus :\n    reviewsList.append(review)\n    sent_tokenized_corpus.append(sent_tokenize(review))\n    \nword_tokenized_reviews = []\nwords = []\n# Now we are using word tokenizer to tokenize the review into words\nfor review in sent_tokenized_corpus :\n    for sent in review :\n        words += (word_tokenize(sent))\n    word_tokenized_reviews.append(words)\n    words = []\n#print(word_tokenized_reviews)\n\nlemmatizer = WordNetLemmatizer()\n\nreview_str = \"\"\nstop_words = set(stopwords.words('english'))\n\nfinal_corpus = []\nfor review in word_tokenized_reviews:\n    for words in review :\n        if words not in stop_words:\n            review_str += (\" \"+(lemmatizer.lemmatize(words.lower())))\n    final_corpus.append(review_str)\n    review_str =\"\"\n\n# Tokenizing the corpus after removing stop words and lemmetizing\nvectorizer = CountVectorizer(min_df=0, stop_words=stop_words)\n\ndocs_tf = vectorizer.fit_transform(final_corpus)\nvocabulary_terms = vectorizer.get_feature_names()\n\n#selecting the keywords\nkeywords = [\"love\", \"pretty\", \"incredible\", \"adorable\", \"stunner\" ]\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe03c43bbd3a4488653f69196813c3a729625b38"},"cell_type":"markdown","source":"**2. Making TF-IDF matrix**"},{"metadata":{"trusted":true,"_uuid":"e70da574b7d9bee5b705481690d35a33217f9eeb","scrolled":true},"cell_type":"code","source":"docs_query_tf = vectorizer.transform(final_corpus + [' '.join(keywords)]) \n\ntransformer = TfidfTransformer(smooth_idf = False)\ntfidf = transformer.fit_transform(docs_query_tf.toarray())\n\n# D x V document-term matrix \ntfidf_matrix = tfidf.toarray()[:-1] \n\n# 1 x V query-term vector \nquery_tfidf = tfidf.toarray()[-1] ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47c4b3a9fba02e1e4deb0ea7e1bc4896c4d37ada"},"cell_type":"markdown","source":"**3. Information Retrieval using IF-IDF  **\n* Here we are using cosine distance to see the correlation between our keywords and reviews."},{"metadata":{"trusted":true,"_uuid":"3155fc006ef313b30b5b6c53f24eb98bb60f2fd1","scrolled":false},"cell_type":"code","source":"query_doc_tfidf_cos_dist = [cosine(query_tfidf, doc_tfidf) for doc_tfidf in tfidf_matrix]\nquery_doc_tfidf_sort_index = np.argsort(np.array(query_doc_tfidf_cos_dist))\n\nfor rank, sort_index in enumerate(query_doc_tfidf_sort_index):\n    if rank == 5 :\n        break\n    print(\"The rank is\", rank)\n    print(\"The cosine distance is\", query_doc_tfidf_cos_dist[sort_index])\n    print(\"Review\")\n    print(reviewsList[sort_index])\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fdd5aa17854dff9488f85348cb7f3180c904005"},"cell_type":"markdown","source":"**4. LSA using TF matrix**"},{"metadata":{"trusted":true,"_uuid":"1d65acb595173d28c1eb1fdff64a0f5d481df186"},"cell_type":"code","source":"tf_matrix = docs_tf.toarray() # D x V matrix \nA = tf_matrix.T \n\nU, s, V = np.linalg.svd(A, full_matrices=1, compute_uv=1)\nK = 2 # number of components\n\nA_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), V[:K, :])) # D x V matrix \n\ndocs_rep = np.dot(np.diag(s[:K]), V[:K, :]).T # D x K matrix \nterms_rep = np.dot(U[:,:K], np.diag(s[:K])) # V x K matrix \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9154f486d470be039ad8ef8cd95f4b63bf32ebdb"},"cell_type":"code","source":"key_word_indices = [vocabulary_terms.index(key_word) for key_word in keywords] # vocabulary indices \n\nkey_words_rep = terms_rep[key_word_indices,:]     \nquery_rep = np.sum(key_words_rep, axis = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b67f9457faff8728fcc73f0d236f5ef006ca829a"},"cell_type":"markdown","source":"**5. Information Retrieval using in LSA**"},{"metadata":{"trusted":true,"_uuid":"03aaa7bc6c81c865aa202c68bf9f7fac13455aa3","scrolled":false},"cell_type":"code","source":"query_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\nquery_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n\nfor rank, sort_index in enumerate(query_doc_sort_index):\n    if rank == 5 :\n        break\n    print(\"The rank is\", rank)\n    print(\"The cosine distance is\", query_doc_tfidf_cos_dist[sort_index])\n    print(\"Review\")\n    print(reviewsList[sort_index])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4fe70fa16015fa18a644a25fe198db5ddff3612"},"cell_type":"markdown","source":"**6. Plotting**"},{"metadata":{"trusted":true,"_uuid":"5e81fd16b86c5066051059ccbac9f1253da4419b"},"cell_type":"code","source":"# plot documents in the new space  \n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.scatter(docs_rep[:,0], docs_rep[:,1], c=query_doc_cos_dist) # all documents \nplt.scatter(query_rep[0],query_rep[1],   marker='+', c='red') # the query \nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}