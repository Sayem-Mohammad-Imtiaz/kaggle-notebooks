{"cells":[{"metadata":{},"cell_type":"markdown","source":"### *In this notebook intended for beginners, a simple implementation of ElasticNet for predicting airfoil self-noise is demonstrated. This is a good dataset for practicing regression modelling because of its simplicity. In the latter part, simple ANN is also implemented to try to see how it performs vs the ElasticNet.*"},{"metadata":{},"cell_type":"markdown","source":" ### **Data & Libraries Import**\n #### Notice that a seed value is set because we want to be able to recreate the ANN results."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"seed_value = 770\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\nimport random\nrandom.seed(seed_value)\nimport numpy as np\nnp.random.seed(seed_value)\nimport tensorflow as tf\ntf.random.set_seed(seed_value)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n%matplotlib inline\nimport sklearn.metrics as metrics\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/nasa-airfoil-self-noise/NASA_airfoil_self_noise.csv',sep = \",\", header = 0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can already see the data's simplicity. The goal is to predict the sound levels based on all the other variables."},{"metadata":{},"cell_type":"markdown","source":"### Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_palette(\"GnBu_d\")\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (7.0, 5.0)\nsns.pairplot(df,plot_kws={\"s\": 75}, height = 1.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (7.0, 5.0)\nplt.title(\"Correlation Plot\")\nsns.heatmap(df.corr(),cmap = 'viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### It is noticeable that AngleAttack and SuctionSide variables are correlated. We'll see later if their correlation is significant enough to affect the regression modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x='AngleAttack',y='SuctionSide',data=df,\n              joint_kws={\"s\": 200}, kind = \"scatter\" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining X&y"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Sound'], axis = 1)\ny = df['Sound']\nprint(\"Dependent Variables\")\ndisplay(X.head())\nprint(\"Independent Variable\")\ndisplay(y.to_frame().head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Collinearity Verification using Variance Inflation Factor"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nX_numeric = X._get_numeric_data()\nX_numeric = add_constant(X_numeric)\nVIF_frame = pd.Series([variance_inflation_factor(X_numeric.values, i) \n               for i in range(X_numeric.shape[1])], \n              index=X_numeric.columns).to_frame()\n\nVIF_frame.drop('const', axis = 0, inplace = True) \nVIF_frame.rename(columns={VIF_frame.columns[0]: 'VIF'},inplace = True)\nVIF_frame[~VIF_frame.isin([np.nan, np.inf, -np.inf]).any(1)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We can say that there is no VIF value that is high enough that can affect the regression modelling. Let's go ahead and move on to the next step"},{"metadata":{},"cell_type":"markdown","source":"### Splitting & Scaling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.25, random_state=823)\nX_train_numeric = X_train._get_numeric_data()\nX_test_numeric = X_test._get_numeric_data()\nscaler = StandardScaler()\nX_train_numeric_scaled = pd.DataFrame(scaler.fit_transform(X_train_numeric), \n                                      index=X_train.index,\n                                      columns=X_train_numeric.columns)\nX_test_numeric_scaled = pd.DataFrame(scaler.transform(X_test_numeric), \n                                     index = X_test.index, \n                                     columns=X_test_numeric.columns)\nX_train.update(X_train_numeric_scaled)\nX_test.update(X_test_numeric_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Notes on Evaluation"},{"metadata":{},"cell_type":"markdown","source":"Evaluation metrics for regression problems:\n\n**Mean Absolute Error** (MAE) - mean of the absolute value of the errors:\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n**Mean Squared Error** (MSE)  - mean of the squared errors:\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n**Root Mean Squared Error** (RMSE) - square root of the mean of the squared errors:\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$"},{"metadata":{},"cell_type":"markdown","source":"### ElasticNetCV - Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = [1e-3, 1e-2, 1e-1, 1]\ncv = 10\nencv = ElasticNetCV(alphas = alpha, cv = cv, random_state = 1234)\nencv.fit(X_train,y_train)\ncoeff_df = pd.DataFrame(encv.coef_,X.columns,columns=['Coefficient'])\nintercept = pd.DataFrame(encv.intercept_,['Intercept'],['Coefficient'])\ncoeffs = pd.concat([coeff_df,intercept])\ncoeffs.round(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ElasticNetCV - Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_test_en = encv.predict(X_test)\npreds_train = encv.predict(X_train)\nprint(\"Sample Test Predictions: \" + str(preds_test_en[0:5]))\nprint(\"Sample Train Predictions: \" + str(preds_train[0:5]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d1 = {'Test' : [metrics.mean_absolute_error(y_test, preds_test_en),\n                metrics.mean_squared_error(y_test, preds_test_en),\n                np.sqrt(metrics.mean_squared_error(y_test, preds_test_en))],\n     'Train' : [metrics.mean_absolute_error(y_train, preds_train),\n                metrics.mean_squared_error(y_train, preds_train),\n               np.sqrt(metrics.mean_squared_error(y_train, preds_train))]}\nm = pd.DataFrame(d1,['MAE','MSE','RMSE'])\nm.style.format(\"{:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our predictions\nplt.scatter(y_test,preds_test_en)\n# Perfect predictions\nplt.plot(y_test,y_test,'lime')\nplt.xlabel(\"Actual\", fontsize = 18)\nplt.ylabel(\"Predicted\", fontsize = 18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_test_en = pd.Series(preds_test_en.flatten().tolist())\nplt.rcParams['figure.figsize'] = (7.0, 5.0)\nplt.title(\"Error Distribution Plot\")\nsns.distplot(y_test-preds_test_en, bins = 20);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ANN - Training"},{"metadata":{},"cell_type":"markdown","source":"#### Prior to creating the ANN, we split the train set into train and val. This is necessary because we need a validation set during training of the ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, \n                                                    test_size = 0.30, random_state=823)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We construct an ANN that has a very simple architecture. It has an input layer of 5 nodes representing the 5 features then we have 2 hidden layers with 3 neurons each and then of course an output layer with a single neuron.\n\nNote that it is possible to get significantly better results with better architectures"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Initialization\nNN_model = Sequential()\n\n# Input Layer\nNN_model.add(Dense(5,activation='relu'))\n\n#Hidden Layer/s\nNN_model.add(Dense(3,activation='relu'))\nNN_model.add(Dense(3,activation='relu'))\n\n# Output Layer:\nNN_model.add(Dense(1))\n\nearly_stop = EarlyStopping(monitor = 'val_loss', \n                           mode ='min', \n                           verbose = 1, \n                           patience = 20)\n\nNN_model.compile(optimizer = 'adam', loss = 'mse')\n\nNN_model.fit(x = X_train2.values, \n             y = y_train2.values,\n             validation_data = (X_val.values, y_val.values),\n             epochs = 1000,\n             callbacks=[early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NN_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_test_nn = NN_model.predict(X_test)\n# Our predictions\nplt.scatter(y_test,preds_test_nn)\n# Perfect predictions\nplt.plot(y_test,y_test,'lime')\nplt.xlabel(\"Actual\", fontsize = 18)\nplt.ylabel(\"Predicted\", fontsize = 18)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_train = NN_model.predict(X_train)\nd1 = {'Test' : [metrics.mean_absolute_error(y_test, preds_test_nn),\n                metrics.mean_squared_error(y_test, preds_test_nn),\n                np.sqrt(metrics.mean_squared_error(y_test, preds_test_nn))],\n     'Train' : [metrics.mean_absolute_error(y_train, preds_train),\n                metrics.mean_squared_error(y_train, preds_train),\n               np.sqrt(metrics.mean_squared_error(y_train, preds_train))]}\nm = pd.DataFrame(d1,['MAE','MSE','RMSE'])\nm.style.format(\"{:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_test_en = preds_test_en.values.flatten().tolist()\npreds_test_nn = preds_test_nn.flatten().tolist()\nsns.distplot(y_test-preds_test_nn, bins = 20,color = 'green', hist = False, label = \"ANN\")\nsns.distplot(y_test-preds_test_en, bins = 20,color = 'yellow', hist = False, label = \"ElasticNetCV\")\nplt.legend(prop={'size': 12})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### For this dataset, you can notice that the ANN actually did a slightly better job in predicting. The MSE for ANN is only around 18 compared to the the 23ish MSEs of the ElasticNetCV model. This can be visualized through the actualvspredicted scatterplots of the 2 models (the actualvspredicted scatterplot of ANN is more densed towards the perfect line, indicating better fit) and the histogram (The yellow line represents the ElasticNetCV model and the green one is the ANN. We can notice that the ANN model (the green line) had more errors which lie closer to zero, indicating that it did a better job predicting)"},{"metadata":{},"cell_type":"markdown","source":"#### That's it! If you find this notebook helpful, please upvote :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}