{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **MURA DATASET visualization and preprocessing**\n\n\nthis note book for loading MURA dataset v1.1 and doing some visualization\nand prepare for learning phase with some preprocessing like data augmentation and datagenerator setting\n\n\n### contents\n...\n...\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"change the folowing cell to code if you faced a problem with keras-preprocessing library \nthis library update to be able to use flow_from_dataframe api ","execution_count":null},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"markdown","source":"!pip install git+https://github.com/keras-team/keras-preprocessing.git\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # showing and rendering figures\n# io related\n#from skimage.io import imread\n\n\nimport os\nfrom glob import glob\n# not needed in Kaggle, but required in Jupyter\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#csv files path\npath = '../input/mura-v11/MURA-v1.1'\n#csv files names\ntrain_image_paths_csv = \"train_image_paths.csv\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reading Training data paths \n\nHere we organize all of the images into one DataFrame with adding descriptive colomns so we can easily filter, view and analyze what is available\n\nusing read_csv to read the csv file containing the paths\n\nadding new colomns from the image name ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data table colomns\n* **image_path** : the absolute path to image \n* **label**      : positive/negative the case classification\n* **category**   : the part of body in the image(XR_WRIST,XR_SHOULDER,XR_HAND,XR_FINGER,XR_ELBOW,XR_FOREARM,XR_HUMERUS)\n* **patientId**  :the study subfolder name withch refer to specific patient","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images_paths = pd.read_csv(os.path.join(path,train_image_paths_csv),dtype=str,header=None)\ntrain_images_paths.columns = ['image_path']\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#(doesnt work) train_images_paths['label'] = df[1 if 'positive' in train_images_paths['image_path'] else 0]\n#(dosent work) train_images_paths['label'] =  np.where('positive' in train_images_paths['image_path'], 1, 0)\ntrain_images_paths['label'] = train_images_paths['image_path'].map(lambda x:'positive' if 'positive' in x else 'negative')\n    #xsplit('/')[4].split('_')[-1])\n# (suggest)train_images_paths['category'] =np.resize [train_images_paths['label'].split('/')[3]]\n#(suggest)try doing it using map or apply\ntrain_images_paths['category']  = train_images_paths['image_path'].apply(lambda x: x.split('/')[2])  \ntrain_images_paths['patientId']  = train_images_paths['image_path'].apply(lambda x: x.split('/')[3].replace('patient',''))\ntrain_images_paths.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_number_of_training_images = np.shape(train_images_paths)[0]\nprint(\"total number of images:\",total_number_of_training_images )\nprint (\"\\n\\nnumber of null values\", train_images_paths.isnull().sum())\nprint(\"\\n\\nnumber of training images:\",np.shape(train_images_paths['image_path'])[0])\n\n#df.groupby('a').count()\n#values counts table \ncategories_counts = pd.DataFrame(train_images_paths['category'].value_counts())\nprint ('\\n\\ncategories:\\n',categories_counts )\nprint('\\n\\nnumber of patients:',train_images_paths['patientId'].nunique())\nprint('\\n\\nnumber of labels:',train_images_paths['label'].nunique())\nprint ('\\n\\npositive casses:',len(train_images_paths[train_images_paths['label']=='positive']))\nprint ('\\n\\nnegative casses:',len(train_images_paths[train_images_paths['label']=='negative']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Validation data \nrepeat prevois steps for validation data \n\nvalidation data has another CSV file containing paths","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_image_paths_csv = \"valid_image_paths.csv\"\nvalid_data_paths = pd.read_csv(os.path.join(path,valid_image_paths_csv),dtype=str,header=None)\nvalid_data_paths.columns = ['image_path']\nprint (valid_data_paths.head(5))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_data_paths['label'] = valid_data_paths['image_path'].map(lambda x:'positive' if 'positive' in x else 'negative')\nvalid_data_paths['category']  = valid_data_paths['image_path'].apply(lambda x: x.split('/')[2])  \nvalid_data_paths['dir'] =  valid_data_paths['image_path'].apply(lambda x: x.split('/')[1])\nvalid_data_paths['patientId']  = valid_data_paths['image_path'].apply(lambda x: x.split('/')[3].replace('patient',''))\nvalid_data_paths.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"data_shape:\",np.shape(valid_data_paths))\nprint (\"\\n\\nnumber of null values\", valid_data_paths.isnull().sum())\nprint(\"\\n\\nnumber of training images:\",np.shape(valid_data_paths['image_path']))\n\nvalidaton_categories_counts = pd.DataFrame(valid_data_paths['category'].value_counts())\nprint ('\\n\\ncategories:\\n',validaton_categories_counts)\nprint('\\n\\nnumber of patients:',valid_data_paths['patientId'].nunique())\nprint('\\n\\nnumber of labels:',valid_data_paths['label'].nunique())\nprint ('\\n\\npositive casses:',len(valid_data_paths[valid_data_paths['label']=='positive']))\nprint ('\\n\\nnegative casses:',len(valid_data_paths[valid_data_paths['label']=='negative']))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0b5f85eedfd3ddeee3b7faaaa617e8cfa4e4d7c","_cell_guid":"9d1bff1d-a494-4c11-86d4-2aabc185abc2"},"cell_type":"markdown","source":"## Build the data generator\n\n> ImageDataGenerator Generate batches of tensor image data with real-time data augmentation.\nThe data will be looped over (in batches).\n\n### augmentation\n- rotation\n- width shift\n- height shift\n- zoom range\n- horizontal flip\n- vertical flip ","execution_count":null},{"metadata":{"_uuid":"c29a0c3ec6519a003dd2fe1922187e6a98e674ef","_cell_guid":"63d5876d-d81b-4cff-80b1-d5048a8ce817","trusted":true},"cell_type":"code","source":"# from keras.preprocessing.image import ImageDataGenerator\nfrom keras_preprocessing.image import ImageDataGenerator\n\nidg_train_settings = dict(samplewise_center = True,\n                         samplewise_std_normalization = True,\n                          rotation_range = 5, \n                          width_shift_range = 0.1, \n                         height_shift_range = 0.1,\n                         zoom_range = 0.1, \n                         horizontal_flip = True,\n                         vertical_flip = True)\nidg_train = ImageDataGenerator(**idg_train_settings)\n\nidg_valid_settings = dict(samplewise_center = True,\n                         samplewise_std_normalization = True,\n                          rotation_range = 0, \n                          width_shift_range = 0., \n                         height_shift_range = 0.,\n                         zoom_range = 0.0, \n                         horizontal_flip = False,\n                         vertical_flip = False)\nidg_valid = ImageDataGenerator(**idg_valid_settings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"choose category","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# put in train_df_of_cat all train_images_paths if you want to train on all dataset\n# we will choose category which is not too much images or too many \n# XR_ELBOW or XR_FINGER both have some moderate nomber of examples \n# for train only on XR_ELBOW category we use this mask\ncategory = 'XR_WRIST'\ntrain_mask = train_images_paths['category'] == category\nvalid_mask = valid_data_paths['category']==category\ntrain_df_of_cat = train_images_paths\nvalid_df_of_cat = valid_data_paths\ntrain_df_of_cat['label'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### prepare data for training \n> flow_from_dataframe Takes the dataframe and the path to a directory + generates batches.\nThe generated batches contain augmented/normalized data.\n\n","execution_count":null},{"metadata":{"_uuid":"2f5770d1f8c8a29008b71ef3379d4ed5a06c6ca8","_cell_guid":"258c6ecb-119b-4d54-858b-ff6e9dbde89e","trusted":true},"cell_type":"code","source":"import math\nimages_path_dir = \"../input/mura-v11\"\nout_dir = \"./\"\n\ntrain_generator = idg_train.flow_from_dataframe(\n    dataframe = train_df_of_cat,\n    directory = images_path_dir,\n    x_col = 'image_path',\n    y_col = 'label',\n    batch_size = 32,\n    shuffle = True,\n    class_mode = \"categorical\",\n    target_size = (128, 128),\n    save_to_dir = out_dir,\n    save_format = \"png\",\n    color_mode = 'grayscale')\n\nvalid_generator = idg_valid.flow_from_dataframe(\n    dataframe = valid_df_of_cat,\n    directory = images_path_dir,\n    x_col = 'image_path',\n    y_col = 'label',\n    batch_size = 32,\n    shuffle = True,\n    class_mode = \"categorical\",\n    target_size = (128, 128),\n    save_to_dir = out_dir,\n    save_format = \"png\",\n    color_mode = 'grayscale')\n\nSTEP_SIZE_TRAIN=math.ceil(train_generator.n / train_generator.batch_size)\nSTEP_SIZE_VALID=math.ceil(valid_generator.n / valid_generator.batch_size)\n\na, b = next(train_generator)\ni,l = next(valid_generator)\nprint(\"training input images patch shape  : \", a.shape)\nprint(\"training input labels patch shape  : \",b.shape)\nprint(\"training labels:\",train_generator.class_indices)\nprint (\"________________________________________\")\nprint(\"validation input images patch shape: \", a.shape)\nprint(\"validation input labels patch shape: \",b.shape)\nprint(\"validation labels:\",valid_generator.class_indices)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12bc23f900027dbebd400ac3970155839289187b","_cell_guid":"68e5554b-d56e-4567-a0ee-992f9d419baa"},"cell_type":"markdown","source":"# Build a Model\nWe use an untrained MobileNet to process the images","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shape =a.shape[1:]\nclasses = b.shape[1] #binary classification normal vs upnormal \nloss_func = 'categorical_crossentropy'\noptimizer = 'adam'\nepochs = 30\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b691c2b0665f19eaa4fe52bd5235addb8058dae1","_cell_guid":"0e5f82e2-6125-4260-a363-3f7c5b681db8","trusted":true},"cell_type":"code","source":"from keras.applications import MobileNet\nmodel = MobileNet(classes=classes,dropout=0.01, weights = None, input_shape=input_shape)\nmodel.compile(optimizer=optimizer, loss=loss_func, metrics = ['acc'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bafd9e1f576e3f9c5aecfc2d8c9323adb5ca98e","_cell_guid":"f87714bc-4d90-48c7-97a5-ac80a942a39e","scrolled":false,"trusted":true},"cell_type":"code","source":"print('Layers: {}, parameters: {}'.format(len(model.layers), model.count_params()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a303496021cd54f5e99c0b2a3bd6e53bd533013","_cell_guid":"bbd6c9ad-1f3c-41dd-adbd-ec58f93d5fe7","trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfile_path=category+\"_MobileNet_weights.best.hdf5\"\ncheckpoint = ModelCheckpoint(os.path.join(out_dir,file_path),\n                                monitor='val_loss',\n                                verbose=1,\n                                save_best_only=True,\n                                mode='min')\n#                              monitor='val_loss', \n#                              verbose=1,\n#                              save_best_only=True,\n#                              mode='auto')\nearly = EarlyStopping(monitor=\"val_acc\",\n                      mode=\"max\",\n                      patience=4)\ncallbacks_list = [checkpoint,early] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%env JOBLIB_TEMP_FOLDER=/tmp","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ecebd77f6f853473fa797e1bff2e06ee28fb012","_cell_guid":"06f8f7c9-052a-4321-9c0c-7a15d8913694","trusted":true,"_kg_hide-input":true},"cell_type":"markdown","source":"history = model.fit_generator(train_generator, \n                    steps_per_epoch = STEP_SIZE_TRAIN, \n                    validation_data = valid_generator,\n                    validation_steps =STEP_SIZE_VALID ,\n                    epochs=epochs,\n                   callbacks = callbacks_list)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nscore = model.evaluate_generator(generator=valid_generator,\nsteps=STEP_SIZE_VALID)\nprint(\"Accuracy:\",score[1])\nprint(\"Loss    :\",score[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(file_path)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"237840ee188c12133f4cc02b1bf69078da4c2259","_cell_guid":"da1e27aa-67c0-4edd-b1fd-aeab841c7acd"},"cell_type":"markdown","source":"\n\\0\n44# Run the validation data\nHere we run the validation data with minimal augmentation to see how well it performs and calculate a few stats","execution_count":null},{"metadata":{"_uuid":"0e01cf42088ff54a28ff8601da5d1f3b90674e17","_cell_guid":"c7013ac7-abe2-4086-a68d-fe8721d48536","trusted":true},"cell_type":"code","source":"from keras.models import load_model\nout_dir = \"./\"\nmodel.load_weights(os.path.join(out_dir,file_path)) # load the best model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_loss_values = history.history['loss']\nvalidation_loss_values = history.history['val_loss']\n\nepochs = range(1, len(training_loss_values)+1)\n\nplt.plot(epochs, training_loss_values, label='Training Loss')\nplt.plot(validation_loss_values, label = 'Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#add test if available\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nvalid_generator.reset()\npred=model.predict_generator(valid_generator,steps=STEP_SIZE_validate,verbose=1)\nprint(pred)\ny_true = valid_generator.classes\ny_pred =list(np.argmax([pred > 0.5],axis=-1).flatten()) \n\nprint(\"kk\",np.shape(y_true))\nprint(\"gg\",np.shape(y_pred))\nprint(train_generator.class_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_true)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28c832f58bdbeb515562d3a179c82a68c59964f6","_cell_guid":"b6f20581-e7d2-4d16-8856-5083da1b1e67","trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_true,y_pred ))\n# plt.matshow(confusion_matrix(y_true,y_pred ))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b8c9a042cbb9bfff8022d98340abaf3168a5f4d","_cell_guid":"06226ae1-c4c8-4203-92f7-aba5d4eee125"},"cell_type":"markdown","source":"# Make an ROC\nHere we create a simple ROC curve to evaluate the model","execution_count":null},{"metadata":{"_uuid":"be0dcccaba11859b4fd003d353feb6dc5bc5fe94","_cell_guid":"815f3422-7008-45d6-a2b5-4332097cb1be","trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, thresholds = roc_curve(y_true,y_pred )\nfig, ax1 = plt.subplots(1,1)\nax1.plot(fpr, tpr, 'r.', label = 'MobileNet (AUC:%2.2f)' % roc_auc_score(y_pred, y_true))\nax1.plot(fpr, fpr, 'b-', label = 'Random Guessing')\nax1.legend()\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dddf5a6661d2dc6f872744a929fb1196d4a78ab8","_cell_guid":"902a7ea3-4431-459a-9e20-958af56ce394","trusted":true},"cell_type":"code","source":"# clean up the virtual directories\nimport shutil\nfor c_dir in glob('v_*'):\n    shutil.rmtree(c_dir)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d98ccb2d2404df284eec9f7f2e221753fa09e69","_cell_guid":"9147cb9b-c35c-44b3-9479-9b4cc52c27e1","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}