{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nimport os\nimport warnings\nfrom sklearn.preprocessing import StandardScaler\nwarnings.filterwarnings(\"ignore\")\nos.listdir(\"../input\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The image loading part**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def toArray(k):\n    return np.array(list(k.getdata())).reshape(k.size[1], k.size[0], 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error as mae \nfrom skimage.measure import compare_psnr as psnr\n\ntrain_data = []\nfor img_path in os.listdir(\"../input/images\"):\n    train_data += [Image.open('../input/images/'+img_path)]\nfor img_path in os.listdir(\"../input/general100\"):\n    train_data += [Image.open('../input/general100/'+img_path)]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for img_path in os.listdir(\"../input/image-classification/images/images/art and culture\")[:110]:\n    train_data += [Image.open('../input/image-classification/images/images/art and culture/'+img_path)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'll generate some training samples:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def imageListToNiceSamples(images, downscale_factor = 2, img_size = 40, n_convolutions = 4): \n    X = []\n    Y = []\n    for image in tqdm(images):\n        cutoff = n_convolutions\n        size = np.array(image.size)\n        samples_from_image = size//img_size\n        newimage = image.resize(size//downscale_factor, resample = Image.BICUBIC).resize(size, resample = Image.BICUBIC)\n        try:\n            image_array = toArray(image)\n            newimage_array = toArray(newimage)\n            if(image_array.shape[2]==1):\n                continue\n            X_temp = []\n            Y_temp = []\n          #  print(size, image.size, samples_from_image)\n            for j in range(samples_from_image[0]):\n                for i in range(samples_from_image[1]):\n                    x = newimage_array[i*img_size:(i+1)*img_size,j*img_size:(j+1)*img_size,:]/130-0.99\n                    y = image_array[i*img_size:(i+1)*img_size,j*img_size:(j+1)*img_size,:]/130-0.99 # this for preserving image size (as now images are large padding boundaries won't change much)\n\n                    X_temp+=[x.reshape(1,img_size,img_size,3)]\n                    Y_temp+=[y.reshape(1,img_size,img_size,3)]\n                    \n            #X_temp_2 = np.concatenate([np.array(X_temp)[:,:,:,:,0],np.array(X_temp)[:,:,:,:,1],np.array(X_temp)[:,:,:,:,2]], axis=0) # Channel separation doesn't work just for mobilenet because it uses depthise separable convolutions\n            #Y_temp_2 = np.concatenate([np.array(Y_temp)[:,:,:,:,0],np.array(Y_temp)[:,:,:,:,1],np.array(Y_temp)[:,:,:,:,2]], axis=0)\n            X+=[np.concatenate(X_temp, axis=0)] \n            del X_temp\n            \n            Y+=[np.concatenate(Y_temp, axis=0)]\n            del Y_temp\n        except:\n            continue # There may be black and white images in the data or else, and I don't need to care for them.\n            \n    return(np.concatenate(X, axis=0), np.concatenate(Y, axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size = 224\nn_convolutions = 3\ndownscale_factor = 2\nX, y = imageListToNiceSamples(train_data, img_size = image_size, downscale_factor = downscale_factor, n_convolutions=n_convolutions)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data = []\nfor img_path in os.listdir(\"../input/image-classification/images/images/art and culture\")[200:230]:\n    val_data += [Image.open('../input/image-classification/images/images/art and culture/'+img_path)]\nX_val, y_val = imageListToNiceSamples(val_data, img_size = image_size, downscale_factor = downscale_factor, n_convolutions=n_convolutions)\n# Here I gave up as using ShuffleSplit made memory blow out.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential, Model\nfrom keras.layers import Conv2D, Dense, Activation, Dropout, Lambda, MaxPooling2D, BatchNormalization, Reshape, Flatten, Input, Concatenate, Add, Conv2DTranspose\nfrom keras.optimizers import Nadam\nfrom keras.callbacks import EarlyStopping\nfrom keras.applications.mobilenet_v2 import MobileNetV2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getModel(lr = 0.002, dropout_rate = .2, input_dropout = .2, conv_layer_size = 64, image_size =224, downscale_factor =4, n_conv = 2): # encapsulation to facilitate skopt usage\n    opt = Nadam(lr)\n\n    inpt = Input((image_size,image_size, 3))\n    \n    mn2 = MobileNetV2(include_top=False, input_tensor = inpt)\n    #mn2.summary()\n    x = mn2.get_layer(\"Conv1_pad\")(inpt)\n    x = mn2.get_layer(\"Conv1\")(x)\n    x = mn2.get_layer(\"bn_Conv1\")(x)\n    x = mn2.get_layer(\"Conv1_relu\")(x)\n    x = mn2.get_layer(\"expanded_conv_depthwise\")(x)\n    x = mn2.get_layer(\"expanded_conv_depthwise_BN\")(x)\n    x = mn2.get_layer(\"expanded_conv_depthwise_relu\")(x)\n    x = mn2.get_layer(\"expanded_conv_project\")(x)\n    x = mn2.get_layer(\"expanded_conv_project_BN\")(x)\n    x = mn2.get_layer(\"block_1_expand\")(x)\n    x = mn2.get_layer(\"block_1_expand_BN\")(x)\n    x = mn2.get_layer(\"block_1_expand_relu\")(x)\n    x = mn2.get_layer(\"block_1_pad\")(x)\n    x = mn2.get_layer(\"block_1_depthwise\")(x)\n    x = mn2.get_layer(\"block_1_depthwise_BN\")(x)\n    x = mn2.get_layer(\"block_1_depthwise_relu\")(x)\n    x = mn2.get_layer(\"block_1_project\")(x)\n    y = mn2.get_layer(\"block_1_project_BN\")(x)\n    y = mn2.get_layer(\"block_2_expand\")(y)\n    y = mn2.get_layer(\"block_2_expand_BN\")(y)\n    y = mn2.get_layer(\"block_2_expand_relu\")(y)\n    y = mn2.get_layer(\"block_2_depthwise\")(y)\n    y = mn2.get_layer(\"block_2_depthwise_BN\")(y)\n    y = mn2.get_layer(\"block_2_depthwise_relu\")(y)\n    y = mn2.get_layer(\"block_2_project\")(y)\n    y = mn2.get_layer(\"block_2_project_BN\")(y)\n    z = mn2.get_layer(\"block_2_add\")([y,x])\n    \n    aux_model = Model(inpt,z)\n    for layer in aux_model.layers: #-6 overfit\n        layer.trainable = False\n\n    conv2dT = Conv2DTranspose(conv_layer_size,4, strides = 4)(z)\n    conv2dT = BatchNormalization()(conv2dT)\n    conv2dT = Activation('relu')(conv2dT)\n    concat = Concatenate(axis = 3)([inpt, conv2dT])\n    mid = concat\n    for i in range(n_conv):\n        mid = Conv2D(conv_layer_size,3, padding='same')(mid)\n        mid = BatchNormalization()(mid)\n        mid = Activation('tanh')(mid)\n    output = Activation('tanh')(Conv2D(3,3, padding='same')(mid))\n    \n    model = Model(inpt,output)\n    model.compile(loss = 'mae', optimizer = opt) \n    return(model)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"benchmark = psnr(y_val, X_val)\nprint(benchmark)\nss = StandardScaler()\ntarget = ss.fit_transform(y.reshape(-1,150528)-X.reshape(-1,150528)).reshape(-1,224,224,3)/2\ntarget_val = ss.transform(y_val.reshape(-1,150528)-X_val.reshape(-1,150528)).reshape(-1,224,224,3)/2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"psnrs=[]\nfor i in range(10):\n    model = getModel(dropout_rate = .35, input_dropout = 0.0, image_size = image_size, conv_layer_size = 16, n_conv=3, downscale_factor = downscale_factor)\n    #model.summary() #to give overview of number of params\n    stop = EarlyStopping(patience=3, restore_best_weights = True) #patience derived empirically\n    model.fit(X[:125*(i+1)], y[:125*(i+1)], validation_data = [X_val,y_val], batch_size = 8, epochs = 20, callbacks = [stop], verbose = False)\n    target_pred = model.predict(X_val)\n    y_pred = X_val + ss.inverse_transform(target_pred.reshape(-1,150528)*2).reshape(-1,224,224,3)*2\n    psnrs+=[psnr(y_val, y_pred)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(psnrs)\nplt.title('model psnr')\nplt.ylabel('psnr')\nplt.xlabel('samples used/50')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}