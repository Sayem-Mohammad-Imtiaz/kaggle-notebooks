{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#general imports\n\nfrom collections import Counter\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd\nfrom PIL import Image\nimport re\nimport seaborn as sns\nimport string\nfrom wordcloud import WordCloud\n\n#SKL imports\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\n\n#NLTK imports\nimport nltk\nfrom nltk.collocations import *\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n#nltk.download('stopwords')\n#nltk.download('wordnet')\n#nltk.download('names')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore data"},{"metadata":{"trusted":true},"cell_type":"code","source":"job_data = pd.read_csv('../input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv', sep=',')\npercent_missing_values = job_data.isnull().sum() * 100 / len(job_data)\nmissing_values_df = pd.DataFrame({'percent_missing': percent_missing_values})\nmissing_values_df.sort_values('percent_missing', inplace=True)\nmissing_values_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So at first glance, there are quite a few variables with a large percentage of NaNs. With missing values in the text columns making any valid NLP is hard. For that reason, I will only look at description, requirements and company profile and the variables without missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"job_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of unique functions in this dataset {len(job_data.function.unique())}')\nprint(f'Number of unique locations in this dataset {len(job_data.location.unique())}')\nprint(f'Number of unique departments in this dataset {len(job_data.department.unique())}')\nprint(f'Number of unique employment types in this dataset {len(job_data.employment_type.unique())}')\nprint(f'Number of unique required education in this dataset {len(job_data.required_education.unique())}')\nprint(f'Number of unique industries in this dataset {len(job_data.industry.unique())}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean text columns and create bigram columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(text):\n    if text is not None:\n        text = text.lower()\n        regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n        no_punct = regex.sub(\" \", str(text))\n        #no_2_letters = re.sub(r'\\b\\w{1,2}\\b', '', no_punct)\n        no_special_characters = re.sub('[^A-Za-z0-9]+', ' ', no_punct)\n        lemmatizer = WordNetLemmatizer()\n        word_list = nltk.word_tokenize(no_punct)\n        lemmatized = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n\n        return lemmatized\n    else:\n        return None\n\n#clean description\njob_data['nan_count'] = job_data.isnull().sum(axis=1)\njob_description = job_data[['job_id','description','fraudulent']]\njob_description = job_description.dropna(how ='any', subset=['description'])#drop NaNs, but modifying the clean_text function is a second possibility\njob_description['clean_description'] = job_description.description.apply(lambda x: clean_text(x))\njob_description['bigrams'] = job_description.clean_description.apply(lambda row: list(nltk.ngrams(row.split(' '),2)))\n\n#-----------------------------------------------------------------------------------------------------------------------------------------------\n#clean company profile\njob_cp = job_data[['job_id','company_profile','fraudulent']]\njob_cp = job_cp.dropna(how ='any', subset=['company_profile'])\njob_cp['clean_company_profile'] = job_cp.company_profile.apply(lambda x: clean_text(x))\njob_cp['bigrams_cp'] = job_cp.clean_company_profile.apply(lambda row: list(nltk.ngrams(row.split(' '),2)))\n\n#-----------------------------------------------------------------------------------------------------------------------------------------------\njob_req = job_data[['job_id','requirements','fraudulent']]\njob_req = job_req.dropna(how ='any', subset=['requirements'])\njob_req['clean_requirements'] = job_req.requirements.apply(lambda x: clean_text(x))\njob_req['bigrams_r'] = job_req.clean_requirements.apply(lambda row: list(nltk.ngrams(row.split(' '),2)))\n\n#-----------------------------------------------------------------------------------------------------------------------------------------------\njob_description = pd.merge(job_description, job_cp, on='job_id', how='outer')\njob_description = job_description.drop(['fraudulent_y'], axis=1)\njob_description = job_description.rename({'fraudulent_x':'fraudulent'}, axis=1)\njob_description['fraudulent'] = job_description['fraudulent'].astype(int)\njob_description = pd.merge(job_description, job_req, on='job_id', how='outer')\njob_description = job_description.drop(['fraudulent_y'], axis=1)\njob_description = job_description.rename({'fraudulent_x':'fraudulent'}, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"job_description.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Look at single grams"},{"metadata":{"trusted":true},"cell_type":"code","source":"#look at the frequency distribution of single words\nstop_words = stopwords.words('english')\nword_count_all = Counter()\nword_count_fraud = Counter()\n\njob_description['clean_description'].str.split().apply(word_count_all.update)\njob_description[job_description['fraudulent']==1]['clean_description'].str.split().apply(word_count_fraud.update)\n\n\nfor word in stop_words:#delete all words that are in the stopword list\n    del word_count_all[word]\n    del word_count_fraud[word]\n\n \n\nf, axarr = plt.subplots(1,2, figsize=(20,20))\n\nwc_all = WordCloud(max_words=30,background_color=\"white\").generate_from_frequencies(word_count_all)\nplt.title(\"Wordcloud all\")\naxarr[0].imshow(wc_all)\n\nwc_fraud = WordCloud(max_words=30,background_color=\"white\").generate_from_frequencies(word_count_fraud)\nplt.title(\"Wordcloud fraud \")\naxarr[1].imshow(wc_fraud)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well the single words do not seem to show anything out of the ordinary, at least not in the description. So I will have a look at the bigrams and hopefully find some indicators for fraudulent job postings. "},{"metadata":{},"cell_type":"markdown","source":"# Bigrams in the description field and company profile field"},{"metadata":{"trusted":true},"cell_type":"code","source":"bigram_list_all = list([a for b in job_description.bigrams.tolist() for a in b])\nbigram_dict_all = {}\nbigram_dict_all = Counter((bigram_list_all))\n\nbigram_list_non_fraud = list([a for b in job_description[job_description['fraudulent']==0].bigrams.tolist() for a in b])\nbigram_dict_non_fraud = {}\nbigram_dict_non_fraud = Counter((bigram_list_non_fraud))\n\nbigram_list_fraud = list([a for b in job_description[job_description['fraudulent']==1].bigrams.tolist() for a in b])\nbigram_dict_fraud = {}\nbigram_dict_fraud = Counter((bigram_list_fraud))\n\nbigram_list_non_fraud_cp = list([a for b in job_cp[job_cp['fraudulent']==0].bigrams_cp.tolist() for a in b])\nbigram_dict_non_fraud_cp = {}\nbigram_dict_non_fraud_cp = Counter((bigram_list_non_fraud_cp))\n\nbigram_list_fraud_cp = list([a for b in job_cp[job_cp['fraudulent']==1].bigrams_cp.tolist() for a in b])\nbigram_dict_fraud_cp = {}\nbigram_dict_fraud_cp = Counter((bigram_list_fraud_cp))\n\nbigram_list_non_fraud_r = list([a for b in job_req[job_req['fraudulent']==0].bigrams_r.tolist() for a in b])\nbigram_dict_non_fraud_r = {}\nbigram_dict_non_fraud_r = Counter((bigram_list_non_fraud_r))\n\nbigram_list_fraud_r = list([a for b in job_req[job_req['fraudulent']==1].bigrams_r.tolist() for a in b])\nbigram_dict_fraud_r = {}\nbigram_dict_fraud_r = Counter((bigram_list_fraud_r))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#print bigrams in description of non fraudulent postings\ncounter = 0\nfor element in sorted(bigram_dict_non_fraud.items(), key=lambda x: x[1], reverse=True):\n    if counter < 25:\n        if (element[0][0] not in stop_words) & (element[0][1] not in stop_words):\n            print(element)\n            counter +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print bigrams in description of fraudulent postings\ncounter = 0\nfor element in sorted(bigram_dict_fraud.items(), key=lambda x: x[1], reverse=True):\n    if counter < 25:\n        if (element[0][0] not in stop_words) & (element[0][1] not in stop_words):\n            print(element)\n            counter +=1\n            \n#bigram_dict_non_fraud[('data', 'entry')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So by checking the bigrams that most frequently occur, I could discover some bigrams that are only used in fraudulent text. The bigram 'Aker Solution' only appeared in fraudulent job postings. This is actually the firm name. A second bigram that pops up is the bigram ' 6 ultra' or the follow up 'ultra luxury', which is used by a supposedly american cruise company. The third bigram, that I found, was 'application onlyclick' which is an error since only and click should have been written as two separate words. But this type of error, that two words are written together is very common, probably happened during the webcrawl and writing it to a csv. The description of this job is very suspicious and revealed another firm that only post fraudulent jobs. With this bigram another bigram is very related, namely 'data entry', but this bigram is also found in non fraudulent job postings, but relatively seldom. We might find something that has more discriminating power."},{"metadata":{"trusted":true},"cell_type":"code","source":"#print bigrams in company profile of non fraudulent postings\ncounter = 0\nfor element in sorted(bigram_dict_non_fraud_cp.items(), key=lambda x: x[1], reverse=True):\n    if counter < 25:\n        if (element[0][0] not in stop_words) & (element[0][1] not in stop_words):\n            print(element)\n            counter +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print bigrams in company profile of fraudulent postings\ncounter = 0\nfor element in sorted(bigram_dict_fraud_cp.items(), key=lambda x: x[1], reverse=True):\n    if counter < 25:\n        if (element[0][0] not in stop_words) & (element[0][1] not in stop_words):\n            print(element)\n            counter +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Here again a few very useful bigrams could be detected. The first bigram, which is used in 56 fake job postings and 0 real job postings, is 'signing bonus'. Apparently using this in the company profile is not serious or at least not used in real job postings in this dataset. Also, it sounds like a way to attract people to apply. Furthermore, the bigrams 'represented candidate' and 'solid geopgraphical' are only used in fake job postings.  "},{"metadata":{},"cell_type":"markdown","source":"So in the requirements there are also a few bigrams spotted, which are quite strong discriminators. 'amp personal' is a weird bigram, but it happens mostly in fraudulent job postings. This definitly has something to do with html parsing, but it could still be seen as 'pattern'. Also 'data entry' is found here, which is a skill that is not often mentioned in non fraudulent job postings. Another skill, that is probably related to data entry is typing skill. Just something that is not written in a real job posting. So below we will 'translate' these bigram features into machine learning interpretable features. I excluded the code for printing the bigrams, since, as we will see below, the bigrams in the description and mostly in the company profile already have the strongest relation to the target.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check how many fraudulent job postings can be found only using these bigram rules:\nlen(job_description[(job_description['clean_company_profile'].str.contains('signing bonus')) |\\\n                    (job_description['clean_company_profile'].str.contains('represented candidate'))|\\\n                    (job_description['clean_company_profile'].str.contains('solid geographical'))|\n                    (job_description['clean_description'].str.contains('aker solution'))|\\\n                    (job_description['clean_description'].str.contains('good english'))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Actually not that bad, by only introducing 5 rules or one-hot encoded variables, any classifier would at least find 181 fraudulent job postings. So the baseline model (which in this case is rulebased) here actually reaches an accuracy of: (17014(number of real job postings) + 181 (rules based identifiable fake job postings))/17880 = 96.2%. But, as mentioned before in other kernels, accuracy is an inappropriate measure for how good a model is, when the data set is highly unbalanced. We will look at precision and recall when comparing algorithm. Especially the recall in fraudulent job postings should be the main target, without offering to much of precision.   "},{"metadata":{},"cell_type":"markdown","source":"# Prepare dataframe for ML"},{"metadata":{"trusted":true},"cell_type":"code","source":"def contains_bigram(description):\n    if 'aker solution' in description.lower():\n        return 1\n    if 'good english' in description.lower():\n        return 1\n#    if '#EMAIL' in description.lower():\n#        return 1\n    else:\n        return 0\n    \ndef contains_data_entry(description):\n    if 'data entry' in description.lower():\n        return 1\n    else:\n        return 0\n\ndef cp_contains_bigram(description):\n    if 'signing bonus' in description.lower():\n        return 1\n    if 'represented candidate' in description.lower():\n        return 1\n    if 'solid geographical' in description.lower():\n        return 1\n    else:\n        return 0\n\ndef bigram_r(requirements):\n    if 'typing skill ability' in requirements.lower():#yes, cheated here. I found this trigram while looking at the bigram\n        return 1\n    if 'data entry' in requirements.lower():\n        return 1\n    if 'qualification amp personal' in requirements.lower():\n        return 1\n    else:\n        return 0    \n    \ndef cap_word_count(description):\n    count = len(re.findall(r'([A-Z]{4,})', description))\n    if count > 2:\n        return 1\n    else:\n        return 0\n\ndef descr_length(lenght):\n    if lenght <=150:\n        return 1\n    else:\n        return 0\n\ndef nan_counter(count):\n    if count >=5:\n        return 1\n    else:\n        return 0\n\ndef bigram_occurence(bigrams):\n    output = defaultdict(int)\n    for element1, element2 in bigrams:\n        if (element1 not in stop_words) & (element2 not in stop_words):\n            output[element1,element2]+= 1\n    output = sorted(output.items(), reverse=True, key=lambda item: item[1])\n    \n    return output\n\ndef non_stopword_percentage(bigram_occ, bigrams):\n    sum_bigram_occ=0\n    for elem in bigram_occ:\n        sum_bigram_occ= sum_bigram_occ + elem[1]\n    num_bigrams = len(bigrams)\n    if num_bigrams > 0:\n        return(round(sum_bigram_occ/num_bigrams*100,0))\n    else:\n        return 0\n\ndef repeated_bigrams(bigram_occ):\n    repeated = 0\n    for element in bigram_occ:\n        if element[1] > 1:\n            repeated +=1\n    return repeated       \n    \ndef rep_classes(repetition):\n    if repetition <=3:\n        return 1\n    if repetition == 4:\n        return 2\n    if repetition > 4:\n        return 3\n    \n    \njob_data['nan_5'] = job_data.nan_count.apply(lambda x: nan_counter(x))\ndescription_var = job_data[['job_id', 'description']]\ndescription_var = description_var.dropna(how ='any', subset=['description'])\ndescription_var['char_length'] = description_var.description.str.len()\ndescription_var['length_150'] = description_var.char_length.apply(lambda x: descr_length(x))\ndescription_var['contains_bigram'] = description_var.description.apply(lambda x: contains_bigram(x))\ndescription_var['contains_data_entry'] = description_var.description.apply(lambda x: contains_data_entry(x))\ndescription_var['cap_word_count'] = description_var.description.apply(lambda x: cap_word_count(x))\ndescription_var['bigrams'] = description_var.description.apply(lambda row: list(nltk.ngrams(row.split(' '),2)))\ndescription_var['bigram_occ'] = description_var.bigrams.apply(lambda x: bigram_occurence(x))\ndescription_var['non_stopword_bigrams%'] = description_var.apply(lambda x: non_stopword_percentage(x.bigram_occ, x.bigrams), axis=1)\ndescription_var['repetition'] = description_var.bigram_occ.apply(lambda x: repeated_bigrams(x))\ndescription_var['rep_class'] = description_var.repetition.apply(lambda x: rep_classes(x))\n\n#--------------------------------------------------------------------------------------------------------------------\ncp_var = job_data[['job_id', 'company_profile']]\ncp_var = cp_var.dropna(how ='any', subset=['company_profile'])\ncp_var['char_length_cp'] = cp_var.company_profile.str.len()\ncp_var['contains_bigram_cp'] = cp_var.company_profile.apply(lambda x: cp_contains_bigram(x))\ncp_var['bigrams_cp'] = cp_var.company_profile.apply(lambda row: list(nltk.ngrams(row.split(' '),2)))\ncp_var['bigram_occ_cp'] = cp_var.bigrams_cp.apply(lambda x: bigram_occurence(x))\ncp_var['non_stopword_bigrams_cp%'] = cp_var.apply(lambda x: non_stopword_percentage(x.bigram_occ_cp, x.bigrams_cp), axis=1)\ncp_var['repetition_cp'] = cp_var.bigram_occ_cp.apply(lambda x: repeated_bigrams(x))\ncp_var['rep_class_cp'] = cp_var.repetition_cp.apply(lambda x: rep_classes(x))\n#--------------------------------------------------------------------------------------------------------------------\n\nr_var = job_data[['job_id', 'requirements']]\nr_var = r_var.dropna(how ='any', subset=['requirements'])\nr_var['r_length'] = r_var.requirements.str.len()\nr_var['contains_bigram_r'] = r_var.requirements.apply(lambda x: bigram_r(x))\nr_var['bigrams_r'] = r_var.requirements.apply(lambda row: list(nltk.ngrams(row.split(' '),2)))\nr_var['bigram_occ_r'] = r_var.bigrams_r.apply(lambda x: bigram_occurence(x))\nr_var['non_stopword_bigrams_r%'] = r_var.apply(lambda x: non_stopword_percentage(x.bigram_occ_r, x.bigrams_r), axis=1)\nr_var['repetition_r'] = r_var.bigram_occ_r.apply(lambda x: repeated_bigrams(x))\n\n#--------------------------------------------------------------------------------------------------------------------\nresult = pd.merge(job_data, description_var, on='job_id', how='outer')\nresult = pd.merge(result, cp_var, on='job_id', how='outer')\nresult = pd.merge(result, r_var, on='job_id', how='outer')\nresult = result.fillna({'contains_bigram':2, 'contains_bigram_cp':2, 'contains_data_entry':2, 'length_150':2, 'cap_word_count':2, 'contains_bigram_r':2, 'rep_class':4,'rep_class_cp':4}) # code NaNs as a 2 for the 'binary' variables and for categorical variables take 4\nresult[['contains_bigram','contains_bigram_cp','contains_data_entry','length_150','cap_word_count','contains_bigram_r']] = result[['contains_bigram', 'contains_bigram_cp','contains_data_entry','length_150','cap_word_count', 'contains_bigram_r']].astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above cell I tried to translate some ideas on possible features into ML algorithm interpretable features. The first four functions check whether the before identified bigrams are in the description, company profile or requirements text. Another feature, that I thought could be interesting, is the number of capitalised words <span style=\"color:blue\">(cap_word_count)</span>. Since the word URL is contained in almost all descriptions, I only looked for words containing at least 4 characters. Since fraudulent job postings should attract as many people as possible, maybe they use more capitalisation to do so. I looked at the histograms and did not really find a pattern, but maybe somebody finds an appropriate binning strategy. Furthermore I checked the length of the description <span style=\"color:blue\">(descr_length)</span> and the number of columns per row with NaN <span style=\"color:blue\">(nan_counter)</span>. I couldn't really find a pattern, so I made a boolean based on the histograms. These variables did a quite poore job in the prediction. But maybe somebody get's another creative idea, that is why I left them in this notebook. In the function <span style=\"color:blue\">(bigram_occurence)</span> I looked at the number of bigrams which does not contain a stopword. This column is then used in the function <span style=\"color:blue\">(non_stopword_percentage)</span>, how high the percentage of non-stop-word-bigrams is to all bigrams in the text. The histogram showed an almost identical pattern for fraudulent and non fraudulent job postings. The last functions (<span style=\"color:blue\">(repeated_bigrams)</span> &<span style=\"color:blue\">(rep_classes)</span> check how often bigrams are repeated, since I thougth, fraudulent job postings may be less creative and use the same phrase/words more often. This last variable repetition is actually something interesting, since it would be a more global pattern, by which I mean, that the contain_bigram functions are really good for this dataset, but on datasets in different languages have absolutely no chance. The repetition variable could also work in different languages. I did found a pattern, but it is not so strong. Nevertheless, I think it is worth using the variable and maybe somebody finds a better binning strategy. "},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression and Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check influence categorical variable on target variable\nx = result.rep_class_cp.values.reshape(-1, 1)\nclf = LogisticRegression(random_state=0).fit(x, result.fraudulent)\nclf.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pick variables that could be used\nresult = result[['telecommuting','has_company_logo','fraudulent','contains_bigram','contains_data_entry','contains_bigram_cp','cap_word_count','contains_bigram_r','rep_class_cp','rep_class']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#do a simple logistic regression, just to see how it performs\nX_train, X_test, y_train, y_test = train_test_split\\\n                                    (result[['contains_bigram','contains_data_entry','contains_bigram_cp','contains_bigram_r','rep_class_cp','rep_class']],\\\n                                     result['fraudulent'],\\\n                                     test_size=0.25,random_state=109)\n\nclf = LogisticRegression(random_state=0).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred, digits=3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the performance of a simple logistic regression is not that bad, with a recall of .327 on fraudulent job postings. (Rulebased = 181/866 = .209). But it is far from great. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use the grid search to find the best parameters for the rfc\nrfc=RandomForestClassifier(random_state=24)\n\nparam_grid = { \n    'n_estimators': [50,100,150, 200],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [3,4,5,6,7,8],\n    'criterion' :['gini']\n    }\n\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(X_train, y_train)\n\nCV_rfc.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_jobs=2, random_state=0,max_depth = 4, criterion='gini', n_estimators=150).fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred, digits=3))\nprint(list(zip(X_train.columns, clf.feature_importances_)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"So the random forest does a better job, but in terms of recall, it is only slightly better than the logistic regression. I am well aware, that there are many more parameters to tune, but my foremost goal was to think about possible variables and look how they perform in simple ML algorithms. From my perspective the most informative column is actually the company profile column, since I found the best dicriminating variables by using this column. This is a surprise for me, since I expected to find the best information in the discription column. "},{"metadata":{},"cell_type":"markdown","source":"Thanks for reading this notebook, if you find any error, have any questions or any other feedback, please let me know!"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}