{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center>JobChangeClassifier</center>\n<img src=\"https://www.personalcareermanagement.com/wp-content/uploads/2014/02/change-300x213.jpg\" width=\"300\" height=\"200\" align=\"center\"/>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Task Details\nThis dataset is designed to understand the factors that lead to a person to work for a different company(leaving current job), by model(s) that uses the current credentials/demographics/experience to predict the probability of a candidate to look for a new job or will work for the company.\n\nThe whole data divided to train and test. Sample submission has been provided correspond to enrollee_ id of test set (enrolle_ id | target)"},{"metadata":{},"cell_type":"markdown","source":"## Notes \n\nThe dataset is imbalanced.\n\nMost features are categorical (Nominal, Ordinal, Binary), some with high cardinality.\n\nMissing imputation can be a part of your pipeline as well.\n\n## Features\n\n**enrollee_id** : Unique ID for candidate\n\n**city**: City code\n\n**city_development_index** : Developement index of the city (scaled)\n\n**gender**: Gender of candidate\n\n**relevent_experience**: Relevant experience of candidate\n\n**enrolled_university**: Type of University course enrolled if any\n\n**education_level**: Education level of candidate\n\n**major_discipline** :Education major discipline of candidate\n\n**experience**: Candidate total experience in years\n\n**company_size**: No of employees in current employer's company\n\n**company_type** : Type of current employer\n\n**lastnewjob**: Difference in years between previous job and current job\n\n**training_hours**: training hours completed\n\n**target**: 0 – Not looking for job change, 1 – Looking for a job change"},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Imports\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport shap\n%matplotlib inline\n\nfrom pprint import pprint\nfrom IPython.display import display \nfrom sklearn import model_selection\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read in Training Data (aug_train.csv)"},{"metadata":{},"cell_type":"markdown","source":"## Initial Glance at Training Data"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"#%% Read aug_train.csv\naug_train = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\n#%% Initial Glance at Data\ndisplay(aug_train.info(verbose = True,null_counts=True))\nprint(aug_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"aug_train has 1**9,158 observations** with **13 features** and **1 target variable**. The dataset has **missing data** and must be handled properly. "},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"## enrollee_id \nenrolle_id is an meaningless feature that is a unique value for each employee. "},{"metadata":{"trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"code","source":"#%% count total number of unique values in enrollee_id column\nprint('Number of Unique Values: ' + str(aug_train['enrollee_id'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## city\ncity has 123 unique values and is a categorical variable. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of Unique Values: ' + str(aug_train['city'].nunique()))\nprint('Number of NaN Values: ' + str(sum(aug_train['city'].isnull())))\n# top 10 cities \nprint((aug_train['city'].value_counts()[0:10]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## city_development_index\nContinous Variable  \n[How to Calculate the CDI](https://en.wikipedia.org/wiki/City_development_index#:~:text=The%20City%20Development%20Index%20was,level%20of%20development%20in%20cities.&text=It%20was%20invented%20by%20Dr,analysis%20of%20city%20indicators%20data.)\n\n| Index | Formula   |\n|------|------|\n|   Infrastructure  | 25 x Water connections + 25 x Sewerage + 25 x Electricity + 25 x Telephone|\n|   Waste  | Wastewater treated x 50 + Formal solid waste disposal x 50|\n|   Health  | (Life expectancy - 25) x 50/60 + (32 - Child mortality) x 50/31.92|\n|   Education  | Literacy x 25 + Combined enrolment x 25 |\n|Product\t|(log City Product - 4.61) x 100/5.99|\n|   City Development  | (Infrastructure index + Waste index + Education index + Health index + City Product index)/5|"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Missing Values: \", aug_train['city_development_index'].isna().sum())\ndisplay(aug_train['city_development_index'].describe())\nboxplot = aug_train.boxplot(column ='city_development_index')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## gender\nCatagorical Variable: Male, Female, Other, or NaN"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Missing Values: \", aug_train['gender'].isna().sum())\nfig = px.pie(aug_train['gender'].value_counts(), values='gender', names = aug_train['gender'].value_counts().index,title = 'gender',template='ggplot2')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## relevent_experience\nBinary Variable with no missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Missing Values: \", aug_train['relevent_experience'].isna().sum())\nfig = px.pie(aug_train['relevent_experience'].value_counts(), values='relevent_experience', \n             names = aug_train['relevent_experience'].value_counts().index,title = 'relevent_experience',template='ggplot2')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## education_level\nCatagorical Variable indicating education level of worker, has 460 missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Missing Values: \", aug_train['education_level'].isna().sum())\nfig = px.pie(aug_train['education_level'].value_counts(), values='education_level', \n             names = aug_train['education_level'].value_counts().index,title = 'education_level',template='ggplot2')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## major_discipline\nCatagorical Variable indicating major discipline of worker, has 2813 missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Missing Values: \", aug_train['major_discipline'].isna().sum())\nfig = px.pie(aug_train['major_discipline'].value_counts(), values='major_discipline', \n             names = aug_train['major_discipline'].value_counts().index,title = 'major_discipline',template='ggplot2')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## experience\nOrdinal Variable, can replace <1 with 0 and >20 with 21 "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Missing Values: \", aug_train['experience'].isna().sum())\nfig = px.pie(aug_train['experience'].value_counts(), values='experience', \n             names = aug_train['experience'].value_counts().index,title = 'experience',template='ggplot2')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## company_size\nOrdinal Catagorical variable has 5938 missing variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Missing Values: \", aug_train['company_size'].isna().sum())\nfig = px.pie(aug_train['company_size'].value_counts(), values='company_size', \n             names = aug_train['company_size'].value_counts().index,title = 'company_size',template='ggplot2')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## company_type\nCatagorical Variable "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Missing Values: \", aug_train['company_type'].isna().sum())\nfig = px.pie(aug_train['company_type'].value_counts(), values='company_type', \n             names = aug_train['company_type'].value_counts().index,title = 'company_type',template='ggplot2')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## last_new_job\nCatagorical Variable "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Missing Values: \", aug_train['last_new_job'].isna().sum())\nfig = px.pie(aug_train['last_new_job'].value_counts(), values='last_new_job', \n             names = aug_train['last_new_job'].value_counts().index,title = 'last_new_job',template='ggplot2')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## training_hours\nContinous variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Missing Values: \", aug_train['training_hours'].isna().sum())\ndisplay(aug_train['training_hours'].describe())\naug_train.boxplot(column ='training_hours')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## target"},{"metadata":{},"cell_type":"markdown","source":"target is the variable we are trying to predict and calculate the probablities for.   \n\n**0 – Not looking for job change, 1 – Looking for a job change.**\n\nIt is better to have a high recall to better target employees who are looking for a job change.  \nThis is an unbalanced classification problem as seen in the pie chart below."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Missing Values: \", aug_train['target'].isna().sum())\nfig = px.pie(aug_train['target'].value_counts(), values='target', \n             names = aug_train['target'].value_counts().index,title = 'target',template='ggplot2')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read in Testing Data (aug_test.csv)"},{"metadata":{},"cell_type":"markdown","source":"## Initial Glance at Testing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Read aug_test.csv\naug_test = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_test.csv')\n#%% Initial Glance at Data\ndisplay(aug_test.info(verbose = True,null_counts=True))\nprint(aug_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prepare Data for LightGBM"},{"metadata":{},"cell_type":"markdown","source":"Extract only the features from aug_train and aug_test and rowbind them. We then will perform label encoding so that the LightGBM can be used."},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Prepare Data for LightGBM\n\n# Seperate aug_train into target and features \ny = aug_train['target']\nX_aug_train = aug_train.drop('target',axis = 'columns')\n# save the index for X_aug_train \nX_aug_train_index = X_aug_train.index.to_list()\n\n# row bind aug_train features with aug_test features \n# this makes it easier to apply label encoding onto the entire dataset \nX_aug_total = X_aug_train.append(aug_test,ignore_index = True)\ndisplay(X_aug_total.info(verbose = True,null_counts=True))\n\n# save the index for X_aug_test \nX_aug_test_index = np.setdiff1d(X_aug_total.index.to_list() ,X_aug_train_index) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MultiColumnLabelEncoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% MultiColumnLabelEncoder\n# Code snipet found on Stack Exchange \n# https://stackoverflow.com/questions/24458645/label-encoding-across-multiple-columns-in-scikit-learn\n# from sklearn.preprocessing import LabelEncoder\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                # convert float NaN --> string NaN\n                output[col] = output[col].fillna('NaN')\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\n# store the catagorical features names as a list      \ncat_features = X_aug_total.select_dtypes(['object']).columns.to_list()\n\n# use MultiColumnLabelEncoder to apply LabelEncoding on cat_features \n# uses NaN as a value , no imputation will be used for missing data\nX_aug_total_transform = MultiColumnLabelEncoder(columns = cat_features).fit_transform(X_aug_total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Before and After LabelEncoding\ndisplay(X_aug_total)\ndisplay(X_aug_total_transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split X_aug_total_transform \n\nSplit X_aug_total_transform back into X_aug_train_transform and X_aug_test_transform by using the index we saved before."},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Split X_aug_total_transform \nX_aug_train_transform = X_aug_total_transform.iloc[X_aug_train_index, :]\nX_aug_test_transform = X_aug_total_transform.iloc[X_aug_test_index, :].reset_index(drop = True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Before and After LabelEncoding for aug_train \ndisplay(X_aug_train)\ndisplay(X_aug_train_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Before and After LabelEncoding for aug_test \ndisplay(aug_test)\ndisplay(X_aug_test_transform)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train-Test Stratified Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%  train-test stratified split using a 80-20 split\n# drop enrollee_id for aug_train as it is a useless feature \ntrain_x, valid_x, train_y, valid_y = train_test_split(X_aug_train_transform.drop('enrollee_id',axis = 'columns'), y, test_size=0.2, shuffle=True, stratify=y, random_state=1301)\n\n# Create the LightGBM data containers\n# Make sure that cat_features are used\ntrain_data=lgb.Dataset(train_x,label=train_y, categorical_feature = cat_features)\nvalid_data=lgb.Dataset(valid_x,label=valid_y, categorical_feature = cat_features)\n\n#Select Hyper-Parameters\nparams = {'objective':'binary',\n          'metric' : 'auc',\n          'boosting_type' : 'gbdt',\n          'colsample_bytree' : 0.9234,\n          'num_leaves' : 13,\n          'max_depth' : -1,\n          'n_estimators' : 200,\n          'min_child_samples': 399, \n          'min_child_weight': 0.1,\n          'reg_alpha': 2,\n          'reg_lambda': 5,\n          'subsample': 0.855,\n          'verbose' : -1,\n          'num_threads' : 4\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Run LightGBM on train data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Train model on selected parameters and number of iterations\nlgbm = lgb.train(params,\n                 train_data,\n                 2500,\n                 valid_sets=valid_data,\n                 early_stopping_rounds= 30,\n                 verbose_eval= 10\n                 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Overall AUC\ny_hat = lgbm.predict(X_aug_train_transform.drop('enrollee_id',axis = 'columns'))\nscore = roc_auc_score(y, y_hat)\nprint(\"Overall AUC: {:.3f}\" .format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ROC Curve for training/validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% ROC Curve for training/validation data\n# https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\ny_probas = lgbm.predict(valid_x) \nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(valid_y, y_probas)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic for training data')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Feature Importance \nlgb.plot_importance(lgbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Feature Importance using shap package \nlgbm.params['objective'] = 'binary'\nshap_values = shap.TreeExplainer(lgbm).shap_values(valid_x)\nshap.summary_plot(shap_values, valid_x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From both feature importance, we can see that **city** contributes a lot if a employee is looking to change jobs or not. The next feature that is also important is **company_size**. The shap package is prefer when finding feature importance as it preservces consistency and accuracy. You can read more about the shap package in the links provided below \n\n[https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d](https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d)  \n[https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27)"},{"metadata":{},"cell_type":"markdown","source":"# Predictions for aug_test.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"#%% Predictions for aug_test.csv\npredict = lgbm.predict(X_aug_test_transform.drop('enrollee_id',axis = 'columns')) \nsubmission = pd.DataFrame({'enrollee_id':X_aug_test_transform['enrollee_id'],'target':predict})\ndisplay(submission)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit Predictions "},{"metadata":{"trusted":true},"cell_type":"code","source":"##% Submit Predictions \nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions / Challenges"},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n* LightGBM is a great ML algorithim that handles catagorical features and missing values \n* This is a great dataset to work on and lots of knowledge can be gain from withing with this dataset \n* Researching and reading other Kaggle notebooks is essential for becoming a better data scientist\n\n**Challenges**\n* LightGBM has many parameters and other methods that can be utilize to better tune the parameters, this is my first time using LightGBM so mistakes might have occured \n* Working with catagorical features is difficult, especialy when using One-Hot Encoding, this leads to a messy dataframe and longer computational. This is why I opt for Label Encoding and LightGBM \n\n**Closing Remarks**  \nPlease comment and like the notebook if it of use to you! Have a wonderful year! \n\n1-8-2020\nJoseph Chan "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}