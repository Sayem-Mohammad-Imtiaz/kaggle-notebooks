{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=+4 color=\"Black\"><center><b>EDA & Viz for Covid-19 tweets</b></center></font>\n<font size=-1 color=\"Black\"><center><b>* preprocessing & modeling to follow</b></right></font>"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+2\" color=\"Red\"><b>Please Upvote if you like the work</b></font>\n\n### It gives motivation to a working professional (like me) to contribute more."},{"metadata":{},"cell_type":"markdown","source":"# **About this notebook:**\n\n\n\n![](https://media1.tenor.com/images/ed7cffc243c6a6ffe63058e79d1ea0ac/tenor.gif?itemid=16735375)\n\n\nThis notebook aims at presenting EDA & Viz on Covid-19 tweets. The emphasis has been on the text (tweets) and its in-depth analysis for pre-processing. The modeling will be done in a separate notebook.\nThis notebook is prepared on the Covid-19 tweets which are tagged manually from Highly Negative to Highly Positive - i.e. five classes. In this EDA we will change them to 3 classes (Positive, Negative & Neutal)."},{"metadata":{},"cell_type":"markdown","source":"Contents:\n\n* [1. Data](#1)\n* [2. Class Distribution](#2)\n* [3. Number of characters](#3)\n* [4. Number of words in a tweet](#4)\n* [5. Avg. word length in a tweet](#5)\n* [6. Common Stop-words](#6)\n* [7. Punctuations](#7)\n* [8. Common words](#8)\n* [9. Hashtagss](#9)\n* [10. Mentions](#10)\n* [11. Basic pre-processing](#11)\n* [12. Wordclouds](#12)\n* [13. N-grams](#13)\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install textstat","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"collapsed":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"#Loading libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('ggplot')\n\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#for displaying 500 results in pandas dataframe\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n\nimport re\nimport gensim\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict,Counter\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nimport string\nnltk.download('stopwords')\n\n\nstop=set(stopwords.words('english'))\nplt.style.use('seaborn')\n\n\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport textstat\nfrom textblob import TextBlob \nfrom tqdm import tqdm\nfrom statistics import *\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's get started..........\n![](https://habrastorage.org/webt/t6/sr/jr/t6srjrmjjmm6qn8gpld9emy4txu.gif)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>1. Data</b></font><br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv\",encoding='latin1')\ntest=pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv\",encoding='latin1')\n\n\n\ndf=pd.concat([train,test])\ndf['OriginalTweet']=df['OriginalTweet'].astype(str)\ndf['Sentiment']=df['Sentiment'].astype(str)\n\ntrain['OriginalTweet']=train['OriginalTweet'].astype(str)\ntrain['Sentiment']=train['Sentiment'].astype(str)\n\ntest['OriginalTweet']=test['OriginalTweet'].astype(str)\ntest['Sentiment']=test['Sentiment'].astype(str)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print('Training Set Shape = {}'.format(train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(train.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(test.memory_usage().sum() / 1024**2))\nprint(\"\\n\")\nprint(train.head())\nprint(\"\\n\")\nprint(train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Drop duplicates\ntrain.drop_duplicates()\nprint(\" Shape of dataframe after dropping duplicates: \", df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Null values\n\nnull= df.isnull().sum().sort_values(ascending=False)\ntotal =df.shape[0]\npercent_missing= (df.isnull().sum()/total).sort_values(ascending=False)\n\nmissing_data= pd.concat([null, percent_missing], axis=1, keys=['Total missing', 'Percent missing'])\n\nmissing_data.reset_index(inplace=True)\nmissing_data= missing_data.rename(columns= { \"index\": \" column name\"})\n \nprint (\"Null Values in each column:\\n\", missing_data)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Missing data as white lines \nimport missingno as msno\nmsno.matrix(df,color=(0.3,0.36,0.44))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Null values in location"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total tweets in this data: {}'.format(df.shape[0]))\nprint('Total Unique Users in this data: {}'.format(df['UserName'].nunique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"print(df.Sentiment.unique())\nprint(df.Sentiment.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_kg_hide-input":true},"cell_type":"code","source":"# We will copy the text in another column so that the original text is also there for comparison\n\ndf['text'] = df.OriginalTweet\ndf[\"text\"] = df[\"text\"].astype(str)\n\ntrain['text'] = train.OriginalTweet\ntrain[\"text\"] = train[\"text\"].astype(str)\n\ntest['text'] = test.OriginalTweet\ntest[\"text\"] = test[\"text\"].astype(str)\n\n# Data has 5 classes, let's convert them to 3\n\ndef classes_def(x):\n    if x ==  \"Extremely Positive\":\n        return \"positive\"\n    elif x == \"Extremely Negative\":\n        return \"negative\"\n    elif x == \"Negative\":\n        return \"negative\"\n    elif x ==  \"Positive\":\n        return \"positive\"\n    else:\n        return \"neutral\"\n    \ndf['sentiment']=df['Sentiment'].apply(lambda x:classes_def(x))\ntrain['sentiment']=train['Sentiment'].apply(lambda x:classes_def(x))\ntest['sentiment']=test['Sentiment'].apply(lambda x:classes_def(x))\ntarget=df['sentiment']\n\ndf.sentiment.value_counts(normalize= True)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA, Metafeatures & Viz"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>2. Class Distribution</b></font><br>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_df = df.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\nclass_df.style.background_gradient(cmap='winter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"percent_class=class_df.text\nlabels= class_df.sentiment\n\ncolors = ['#17C37B','#F92969','#FACA0C']\n\nmy_pie,_,_ = plt.pie(percent_class,radius = 1.2,labels=labels,colors=colors,autopct=\"%.1f%%\")\n\nplt.setp(my_pie, width=0.6, edgecolor='white') \n\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### There is uneven distribution of classes with positive taking the largest of pie followed by negative.\n## The colors used above will represent the classes ahead."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig=make_subplots(1,2,subplot_titles=('Train set','Test set'))\nx=train.sentiment.value_counts()\nfig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['#17C37B','#F92969','#FACA0C'],name='train'),row=1,col=1)\nx=test.sentiment.value_counts()\nfig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['#17C37B','#F92969','#FACA0C'],name='test'),row=1,col=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Same pattern of uneven distribution in both train and test data"},{"metadata":{},"cell_type":"markdown","source":"\n\n<a id=\"3\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3. Number of characters</b></font><br>\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\n\ntweet_len=train[train['sentiment']==\"positive\"]['text'].str.len()\nax1.hist(tweet_len,color='#17C37B')\nax1.set_title('Positive Sentiments')\n\ntweet_len=train[train['sentiment']==\"negative\"]['text'].str.len()\nax2.hist(tweet_len,color='#F92969')\nax2.set_title('Negative Sentiments')\n\ntweet_len=train[train['sentiment']==\"neutral\"]['text'].str.len()\nax3.hist(tweet_len,color='#FACA0C')\nax3.set_title('Neutral Sentiments')\n\nfig.suptitle('Characters in tweets')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def length(text):    \n    '''a function which returns the length of text'''\n    return len(text)\ndf['length'] = df['text'].apply(length)\n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(df[df['sentiment'] == \"neutral\"]['length'], alpha = 0.5, bins=bins, label='neutral')\nplt.hist(df[df['sentiment'] == \"positive\"]['length'], alpha = 0.7, bins=bins, label='positive')\nplt.hist(df[df['sentiment'] == \"negative\"]['length'], alpha = 0.8, bins=bins, label='negative')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n<a id=\"4\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4. Number of words in a tweet</b></font><br>\n\n\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\n\ntweet_len=train[train['sentiment']==\"positive\"]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='#17C37B')\nax1.set_title('Positive Sentiments')\n\n\ntweet_len=train[train['sentiment']==\"negative\"]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='#F92969')\nax2.set_title('Negative Sentiments')\n\ntweet_len=train[train['sentiment']==\"neutral\"]['text'].str.split().map(lambda x: len(x))\nax3.hist(tweet_len,color='#FACA0C')\nax3.set_title('Neutral Sentiments')\n\nfig.suptitle('Words in a tweet')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n<a id=\"5\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5. Average word length in a tweet</b></font><br>\n\n\n\n\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig,(ax1,ax2, ax3)=plt.subplots(1,3,figsize=(15,5))\n\nword=train[train['sentiment']==\"positive\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='#17C37B')\nax1.set_title('Positive')\n\n\nword=train[train['sentiment']==\"negative\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='#F92969')\nax2.set_title('Negative')\n\nword=train[train['sentiment']==\"neutral\"]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax3,color='#FACA0C')\nax3.set_title('Neutral')\n\n\nfig.suptitle('Average word length in each tweet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n<a id=\"6\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>6. Common Stopwords in the tweets</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train[train['sentiment']==target ]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"np.array(stop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"comment_words = '' \nstopwords = set(STOPWORDS) \n  \n\nfor val in stop: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = \"white\") \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corpus=create_corpus(\"positive\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1     \n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \nx,y=zip(*top)\nplt.bar(x,y, color='#17C37B')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corpus=create_corpus(\"negative\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n          \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \nx,y=zip(*top)\nplt.bar(x,y, color='#F92969')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"corpus=create_corpus(\"neutral\")\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n               \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \nx,y=zip(*top)\nplt.bar(x,y, color='#FACA0C')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### A lot stopwords are present. Require preprocessing"},{"metadata":{},"cell_type":"markdown","source":"\n\n<a id=\"7\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>7. Punctuations</b></font><br>\n\n \n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(\"positive\")\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='#17C37B')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(\"negative\")\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n                \nx,y=zip(*dic.items())\nplt.bar(x,y, color='#F92969')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(\"neutral\")\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='#FACA0C')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### More cleaning to be done"},{"metadata":{},"cell_type":"markdown","source":"\n\n<a id=\"8\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>8. Common Words</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"counter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Common words feature punctuations, we need extensive data cleaning"},{"metadata":{},"cell_type":"markdown","source":"\n\n<a id=\"9\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>9. Hashtags</b></font><br>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_hash(text):\n    line=re.findall(r'(?<=#)\\w+',text)\n    return \" \".join(line)\ndf['hash']=df['text'].apply(lambda x:find_hash(x))\ntemp=df['hash'].value_counts()[:][1:11]\ntemp= temp.to_frame().reset_index().rename(columns={'index':'Hashtag','hash':'count'})\nsns.barplot(x=\"Hashtag\",y=\"count\", data = temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from matplotlib import cm\nfrom math import log10\n\nlabels = df['hash'].value_counts()[:][2:11].index.tolist()\ndata = df['hash'].value_counts()[:][2:11]\n\ndf['hash'].value_counts()[:][1:11].index.tolist()\n#number of data points\nn = len(data)\n#find max value for full ring\nk = 10 ** int(log10(max(data)))\nm = k * (1 + max(data) // k)\n\n#radius of donut chart\nr = 1.5\n#calculate width of each ring\nw = r / n \n\n#create colors along a chosen colormap\ncolors = [cm.terrain(i / n) for i in range(n)]\n\n#create figure, axis\nfig, ax = plt.subplots()\nax.axis(\"equal\")\n\n#create rings of donut chart\nfor i in range(n):\n    #hide labels in segments with textprops: alpha = 0 - transparent, alpha = 1 - visible\n    innerring, _ = ax.pie([m - data[i], data[i]], radius = r - i * w, startangle = 90, labels = [\"\", labels[i]], labeldistance = 1 - 1 / (1.5 * (n - i)), textprops = {\"alpha\": 0}, colors = [\"white\", colors[i]])\n    plt.setp(innerring, width = w, edgecolor = \"white\")\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### While lower casing is required, we can see that hashtags contain keywords related to coronavirus "},{"metadata":{},"cell_type":"markdown","source":"\n\n<a id=\"10\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>10. Mentions</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef mentions(text):\n    line=re.findall(r'(?<=@)\\w+',text)\n    return \" \".join(line)\ndf['mentions']=df['text'].apply(lambda x:mentions(x))\n\ntemp=df['mentions'].value_counts()[:][1:11]\ntemp =temp.to_frame().reset_index().rename(columns={'index':'Mentions','mentions':'count'})\n\nsns.barplot(x=\"Mentions\",y=\"count\", data = temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"b = df['mentions'].value_counts()[:][1:11].index.tolist()\na = df['mentions'].value_counts()[:][1:11].tolist()\nrow = pd.DataFrame({'scenario' : []})\nrow[\"scenario\"] = b\nrow[\"Percentage\"] = a\nfig = px.treemap(row, path= [\"scenario\"], values=\"Percentage\",title='Tree of Mentions')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n<a id=\"11\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>11. Pre-processing</b></font><br>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove Urls and HTML links\ndef remove_urls(text):\n    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_remove.sub(r'', text)\ndf['text_new']=df['text'].apply(lambda x:remove_urls(x))\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ndf['text']=df['text_new'].apply(lambda x:remove_html(x))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Lower casing\ndef lower(text):\n    low_text= text.lower()\n    return low_text\ndf['text_new']=df['text'].apply(lambda x:lower(x))\n\n\n# Number removal\ndef remove_num(text):\n    remove= re.sub(r'\\d+', '', text)\n    return remove\ndf['text']=df['text_new'].apply(lambda x:remove_num(x))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Remove stopwords & Punctuations\nfrom nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\n\ndef punct_remove(text):\n    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n    return punct\ndf['text_new']=df['text'].apply(lambda x:punct_remove(x))\n\n\n\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\ndf['text']=df['text_new'].apply(lambda x:remove_stopwords(x))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove mentions and hashtags\ndef remove_mention(x):\n    text=re.sub(r'@\\w+','',x)\n    return text\ndf['text_new']=df['text'].apply(lambda x:remove_mention(x))\ndef remove_hash(x):\n    text=re.sub(r'#\\w+','',x)\n    return text\ndf['text']=df['text_new'].apply(lambda x:remove_hash(x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Remove extra white space left while removing stuff\ndef remove_space(text):\n    space_remove = re.sub(r\"\\s+\",\" \",text).strip()\n    return space_remove\ndf['text_new']=df['text'].apply(lambda x:remove_space(x))\n\ndf = df.drop(columns=['text_new'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n<a id=\"12\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>12. Wordclouds</b></font><br>\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\n\ndf_pos = df[df[\"sentiment\"]==\"positive\"]\ndf_neg = df[df[\"sentiment\"]==\"negative\"]\ndf_neu = df[df[\"sentiment\"]==\"neutral\"]\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n\nfor val in df_pos.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n   \n\nwordcloud1 = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                colormap=\"Greens\",\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive Sentiment',fontsize=35);\n\ncomment_words = ''\n\nfor val in df_neg.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n\n\n\n\nwordcloud2 = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                colormap=\"Reds\",\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words)  \nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative Sentiment',fontsize=35);\n\n\n\ncomment_words = ''\nfor val in df_neu.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n\nwordcloud3 = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                colormap=\"Greys\",\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutal Sentiment',fontsize=35);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"del df_pos\ndel df_neg\ndel df_neu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"\n\n<a id=\"13\"></a>\n    \n<font size=\"+2\" color=\"indigo\"><b>13. N-grams</b></font><br>\n"},{"metadata":{},"cell_type":"markdown","source":"## Unigrams"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Define functions\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\nN = 30\n\n\n\npositive= train[\"sentiment\"]== \"positive\"\nnegative= train[\"sentiment\"]== \"negative\"\nneutral= train[\"sentiment\"]== \"neutral\"\n\npositive_unigrams = defaultdict(int)\nneutral_unigrams = defaultdict(int)\nnegative_unigrams = defaultdict(int)\n\n# Unigrams\nfor tweet in train[positive]['text']:\n    for word in generate_ngrams(tweet):\n        positive_unigrams[word] += 1\n        \nfor tweet in train[negative]['text']:\n    for word in generate_ngrams(tweet):\n        negative_unigrams[word] += 1\n        \nfor tweet in train[neutral]['text']:\n    for word in generate_ngrams(tweet):\n        neutral_unigrams[word] += 1        \n        \ndf_positive_unigrams = pd.DataFrame(sorted(positive_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_negative_unigrams = pd.DataFrame(sorted(negative_unigrams.items(), key=lambda x: x[1])[::-1])\ndf_neutral_unigrams = pd.DataFrame(sorted(neutral_unigrams.items(), key=lambda x: x[1])[::-1])\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(27, 30), dpi=150)\nplt.tight_layout()\n\nsns.barplot(y=df_positive_unigrams[0].values[:N], x=df_positive_unigrams[1].values[:N], ax=axes[0], color='#17C37B')\nsns.barplot(y=df_negative_unigrams[0].values[:N], x=df_negative_unigrams[1].values[:N], ax=axes[1], color='#F92969')\nsns.barplot(y=df_neutral_unigrams[0].values[:N], x=df_neutral_unigrams[1].values[:N], ax=axes[2], color='#FACA0C')\n\n\nfor i in range(3):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common unigrams in Postive Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common unigrams in Negative Tweets', fontsize=15)\naxes[2].set_title(f'Top {N} most common unigrams in Neutral Tweets', fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bi grams"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Bigrams\npositive_bigrams = defaultdict(int)\nneutral_bigrams = defaultdict(int)\nnegative_bigrams = defaultdict(int)\n\nfor tweet in train[positive]['text']:\n    for word in generate_ngrams(tweet, n_gram=2):\n        positive_bigrams[word] += 1\n        \nfor tweet in train[negative]['text']:\n    for word in generate_ngrams(tweet, n_gram=2):\n        negative_bigrams[word] += 1\n        \nfor tweet in train[neutral]['text']:\n    for word in generate_ngrams(tweet, n_gram=2):\n        neutral_bigrams[word] += 1        \n        \ndf_positive_bigrams = pd.DataFrame(sorted(positive_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_negative_bigrams = pd.DataFrame(sorted(negative_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_neutral_bigrams = pd.DataFrame(sorted(neutral_bigrams.items(), key=lambda x: x[1])[::-1])\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(27, 30), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_positive_bigrams[0].values[:N], x=df_positive_bigrams[1].values[:N], ax=axes[0], color='#17C37B')\nsns.barplot(y=df_negative_bigrams[0].values[:N], x=df_negative_bigrams[1].values[:N], ax=axes[1], color='#F92969')\nsns.barplot(y=df_neutral_bigrams[0].values[:N], x=df_neutral_bigrams[1].values[:N], ax=axes[2], color='#FACA0C')\n\n\nfor i in range(3):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common bigrams in Postive Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common bigrams in Negative Tweets', fontsize=15)\naxes[2].set_title(f'Top {N} most common bigrams in Neutral Tweets', fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Tri-grams"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Trigrams\npositive_trigrams = defaultdict(int)\nneutral_trigrams = defaultdict(int)\nnegative_trigrams = defaultdict(int)\n\nfor tweet in train[positive]['text']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        positive_trigrams[word] += 1\n        \nfor tweet in train[negative]['text']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        negative_trigrams[word] += 1\n        \nfor tweet in train[neutral]['text']:\n    for word in generate_ngrams(tweet, n_gram=3):\n        neutral_trigrams[word] += 1        \n        \ndf_positive_trigrams = pd.DataFrame(sorted(positive_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_negative_trigrams = pd.DataFrame(sorted(negative_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_neutral_trigrams = pd.DataFrame(sorted(neutral_trigrams.items(), key=lambda x: x[1])[::-1])\n\n\nfig, axes = plt.subplots(ncols=3, figsize=(27, 30), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=df_positive_trigrams[0].values[:N], x=df_positive_trigrams[1].values[:N], ax=axes[0], color='#17C37B')\nsns.barplot(y=df_negative_trigrams[0].values[:N], x=df_negative_trigrams[1].values[:N], ax=axes[1], color='#F92969')\nsns.barplot(y=df_neutral_trigrams[0].values[:N], x=df_neutral_trigrams[1].values[:N], ax=axes[2], color='#FACA0C')\n\n\n\n\nfor i in range(3):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common trigrams in Postive Tweets', fontsize=15)\naxes[1].set_title(f'Top {N} most common trigrams in Negative Tweets', fontsize=15)\naxes[2].set_title(f'Top {N} most common trigrams in Neutral Tweets', fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"Green\"><b>Future Work:</b></font>\n\n<font size=\"+1\" color=\"Black\"><b>The modeling kernel notebook will soon be published here (link).\nIf you like my work then please leave an upvote, I am working professional (in data science and analytics) and take my time out to prepare these. Soon I will be preparing tutorials and notebooks on Market Mix Modeling which is seeing a rapid rise in the industry. \nYou can also leave the suggestions in the comment box.\nHappy learning. :)</b></font>\n"},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"Green\"><b>Please Upvote if you liked the work</b></font>"},{"metadata":{},"cell_type":"markdown","source":"\n![#Precious](https://i.imgur.com/5YSC6pg.gif)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}