{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install --upgrade scikit-learn","metadata":{"id":"pEzkjCM1J6hX","outputId":"0751a073-26e2-435b-962e-533728bc44ac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas\ntrain = pandas.read_csv('https://github.com/Ashuto7h/ML-works/blob/main/HE%20%20challenges/reduce%20market%20waste%20-%20HE%20april%202021/train.csv?raw=true', na_values='?')\nvalid = pandas.read_csv('https://github.com/Ashuto7h/ML-works/blob/main/HE%20%20challenges/reduce%20market%20waste%20-%20HE%20april%202021/test.csv?raw=true',na_values='?')\ntrain","metadata":{"id":"ar7NMt6MAU3i","outputId":"4261e7aa-95ee-44bc-f4bb-f26d21110ccf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Profiling Dataset","metadata":{"id":"HBTjjHw9BLGB"}},{"cell_type":"code","source":"# !pip install --upgrade pandas_profiling ","metadata":{"id":"Nve-uDTxBZ3Q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from pandas_profiling import ProfileReport\n# train.profile_report()","metadata":{"id":"wKaxkQslBKU3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis and Preprocessing","metadata":{"id":"EqNQ1mNKBhEs"}},{"cell_type":"markdown","source":"### 23. Success_probability","metadata":{"id":"9Y1WwhoGKhxM"}},{"cell_type":"code","source":"import matplotlib.pyplot as pyplot\nimport seaborn\ntrain.loc[(train.Success_probability > 100) | (train.Success_probability < 0), 'Success_probability'] = train['Success_probability'].median()\nfig = pyplot.figure(figsize = (25,5))\npyplot.subplot(121)\nseaborn.histplot(data =train,x = train['Success_probability'])","metadata":{"id":"0JZlLKoRKpbw","outputId":"a48958ce-0f1d-4bc6-f97c-5ad3a662bec9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Deal_title ","metadata":{"id":"5wkWcBbxCfut"}},{"cell_type":"code","source":"deal_title = valid['Deal_title']\ntrain.drop(columns = 'Deal_title', inplace= True)\nvalid.drop(columns = 'Deal_title', inplace= True)","metadata":{"id":"Nm4xgyBtCe2O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Lead_name\n### 8. Contact_no\n### 13. POC_name\n### 15. Lead_POC_email","metadata":{"id":"53jux1lWC-pI"}},{"cell_type":"code","source":"cols = ['Lead_name', 'Contact_no','POC_name','Lead_POC_email']\ntrain = train.drop(columns = cols)\nvalid= valid.drop(columns = cols)","metadata":{"id":"dAZhS-lmFCU7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Industry\nnot affect much","metadata":{"id":"wrH99R6IFM6j"}},{"cell_type":"code","source":"print(train['Industry'].value_counts())\ntrain['Industry'].fillna(value = 'Banks', inplace =True)\nvalid['Industry'].fillna(value = 'Banks', inplace =True)","metadata":{"id":"miMcguupFURT","outputId":"d9e5e10c-da3b-4171-f11a-302c57d6dd19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Deal_value\n### 5. Weighted Amount","metadata":{"id":"rW6tVDqEVJhX"}},{"cell_type":"code","source":"import numpy\ncols = ['Deal_value', 'Weighted_amount']\nfor col in cols:\n    train[col] = train[col].str.replace(r'\\D', '')\n    valid[col] = valid[col].str.replace(r'\\D', '')\n    train[col].fillna(value = -1, inplace = True)\n    valid[col].fillna(value = -1, inplace = True)\n    train[col] = train[col].astype(int)\n    valid[col] = valid[col].astype(int)\n","metadata":{"id":"W6BaIZXiJF2I","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pyplot.figure(figsize = (25,10))\ni = 1\nfor col in cols:\n    pyplot.subplot(int(str(1)+ str(len(cols)) + str(i)))\n    seaborn.boxplot(data=train, y = col)\n    i += 1","metadata":{"id":"T0h-P0UpMDr8","outputId":"9dd957b6-f7ad-4c27-9154-d839107e37d9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train['Deal_value'].isna().sum())\ntrain['Deal_value'].replace(-1, train['Deal_value'].mean(),inplace = True)\nvalid['Deal_value'].replace(-1, train['Deal_value'].mean(),inplace = True)\ntrain['Weighted_amount'].replace(-1, train['Weighted_amount'].median(),inplace = True)\nvalid['Weighted_amount'].replace(-1, train['Weighted_amount'].median(),inplace = True)\n","metadata":{"id":"TpQI7jI2LtKu","outputId":"79db6baf-9ad7-4192-98fd-a485866a1ae3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Date_of_creation","metadata":{"id":"-iVCtwmmS84O"}},{"cell_type":"code","source":"train['Date_of_creation'] = pandas.to_datetime(train['Date_of_creation'], format = '%Y-%m-%d',errors = 'coerce')\nvalid['Date_of_creation'] = pandas.to_datetime(valid['Date_of_creation'], format = '%Y-%m-%d',errors = 'coerce')\n\ntrain['creation_year'] = train['Date_of_creation'].dt.year\ntrain['creation_week'] = train['Date_of_creation'].dt.isocalendar().week\ntrain['creation_day'] = train['Date_of_creation'].dt.day\n\nvalid['creation_year'] = valid['Date_of_creation'].dt.year\nvalid['creation_week'] = valid['Date_of_creation'].dt.isocalendar().week\nvalid['creation_day'] = valid['Date_of_creation'].dt.day\n\ntrain.drop(columns=['Date_of_creation'],inplace = True)\nvalid.drop(columns=['Date_of_creation'],inplace = True)","metadata":{"id":"Fdj0zfyPTBbz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 11. Geography","metadata":{"id":"t_xsxq9uryrz"}},{"cell_type":"code","source":"train['Geography'].fillna(value = 'USA', inplace =True)\nvalid['Geography'].fillna(value = 'USA', inplace =True)","metadata":{"id":"NyCRo1F-r2An","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 12. Location","metadata":{"id":"9Q7S6uGEaVcO"}},{"cell_type":"code","source":"vc = train['Location'].value_counts()\ntrain['Location'].fillna(value = vc.index[0], inplace = True)\nvalid['Location'].fillna(value = vc.index[0], inplace = True)\n","metadata":{"id":"Pli4IWG3Zz7e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 19. Last_lead_update\naffects","metadata":{"id":"zupFZzJydY8B"}},{"cell_type":"code","source":"vc = train['Last_lead_update'].value_counts()\ntrain['Last_lead_update'].fillna(value = vc.index[0], inplace =True)\nvalid['Last_lead_update'].fillna(value = vc.index[0], inplace =True)","metadata":{"id":"nVtGarDXdYNI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 21.  Resource","metadata":{"id":"INu6Sv2UdX5P"}},{"cell_type":"code","source":"train['Resource'].fillna(value = 'No', inplace = True)\nvalid['Resource'].fillna(value = 'No', inplace = True)","metadata":{"id":"Xfffx--Fe3-w","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{"id":"rm7sKxikf4hq"}},{"cell_type":"code","source":"train.describe()","metadata":{"id":"MvoSC80hf34B","outputId":"99afb03a-d013-4562-f59a-37a8f5593878","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pyplot.figure(figsize=(7,7))\nseaborn.heatmap(train.corr(), annot = True, fmt = '.2f',square = True,vmax=1, vmin = -1,linewidths=0.5, cmap='Dark2')","metadata":{"id":"AVpRB1Pxhcqw","outputId":"21e65d7f-376f-4279-c872-a57045c4f892","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['Deal_value','Weighted_amount','Internal_rating','Success_probability','creation_year','creation_week','creation_day']\ni = 1\npyplot.figure(figsize=(20,11))\nfor col in cols:\n    pyplot.subplot(int(str(33)+str(i)))\n    seaborn.histplot(data = train, x = col)\n    i += 1\n","metadata":{"id":"i4CiCAFWj20D","outputId":"9700c01a-9b87-405d-9ad8-d48249f95911","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = ['Deal_value','Weighted_amount','Internal_rating','Success_probability','creation_year','creation_week','creation_day']\ni = 1\npyplot.figure(figsize=(14,11))\nfor col in cols:\n    pyplot.subplot(int(str(33)+str(i)))\n    seaborn.violinplot(data = train, y = train[col].astype(float))\n    i += 1\n","metadata":{"id":"GYJwOEZBnEwL","outputId":"73c738d3-8f84-4819-e7b3-c32f3714a472","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling - Standardscaling","metadata":{"id":"IX8_ljw2A_1l"}},{"cell_type":"code","source":"cols = ['Deal_value','Weighted_amount','Internal_rating','creation_year','creation_week','creation_day']\n\nfrom sklearn.preprocessing import StandardScaler\ntrain[cols] = StandardScaler().fit_transform(train[cols].values)\nvalid[cols] = StandardScaler().fit_transform(valid[cols].values)\n\ntrain","metadata":{"id":"D5y-3VYEA-9f","outputId":"6b35738d-3507-4c89-bbc7-b3d1ca2656b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Imputing Categorical variables","metadata":{"id":"17VOvJ9qofyc"}},{"cell_type":"code","source":"y = train['Success_probability']\ntrain.drop(columns='Success_probability', inplace= True)","metadata":{"id":"EoIG_-NRwQLE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(train.shape,valid.shape)\ncombo = pandas.concat([train,valid])\nprint(combo.shape)\ncombo = pandas.get_dummies(combo, drop_first=True)\n# ohe = OneHotEncoder(drop='first') \n# ohe.fit_transform(combo)","metadata":{"id":"d5S-bik2oi9l","outputId":"befb94fb-2a3f-45ee-a88e-d262f794661a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combo","metadata":{"id":"ty_0ZZ0ZuYkZ","outputId":"1dc8223a-e002-47fe-f28c-3911eeb86ddf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = combo.iloc[:7007]\nvalid = combo.iloc[7007:]\ntrain.shape, valid.shape","metadata":{"id":"gGlRqHvvpkyC","outputId":"ffc7b5d9-ecc8-4bbe-db67-e879f647b163","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA","metadata":{"id":"l9psvqodIb6M"}},{"cell_type":"code","source":"train.shape,valid.shape","metadata":{"id":"3WaJIN1NKvZz","outputId":"fe94bbc9-b6d7-49e6-f175-1081b32e40b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=0.95, random_state = 42)\npca.fit(train)\npca_train = pca.transform(train)\npca_valid = pca.transform(valid)\ntrain = pandas.DataFrame(data = pca_train)\nvalid = pandas.DataFrame(data = pca_valid)","metadata":{"id":"d-uFfpDJIbPw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Recursive Feature elimination","metadata":{"id":"SG6yu3OWZ1pr"}},{"cell_type":"code","source":"from sklearn.feature_selection import RFECV\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nestimator = DecisionTreeRegressor(criterion='friedman_mse', max_features='auto',\n                      max_leaf_nodes=5, min_samples_leaf=28,\n                      min_samples_split=5,\n                      min_weight_fraction_leaf=0.2777777777777778,\n                      random_state=42)\nselector = RFECV(estimator, cv=5)\nselector = selector.fit(train, y)\nselector.support_","metadata":{"id":"6GlICQzKZ1Ce","outputId":"1502e830-7069-4b7f-8f54-6104939eb726","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = train.columns[selector.support_ == False]\ncols\ntrain.drop(columns = cols, inplace=True)\nvalid.drop(columns = cols, inplace=True)","metadata":{"id":"TmO6brXEfe1d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# column = pandas.read_csv('rfe.csv')\n# column.columns = ['0','1']\n# column = column['0'].values\n# column","metadata":{"id":"Lp_ZHbCi0eR0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train[column]\n# valid = valid[column]\n# train","metadata":{"id":"qyR4OQyr126x","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Test Split","metadata":{"id":"mDgbP1jyJm6c"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train, y, test_size=0.25, random_state=42)","metadata":{"id":"xjIc37yGJlv8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns.to_series().to_csv('rfe.csv')","metadata":{"id":"4pdT7bdkqndo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Models","metadata":{"id":"mVC7gFGiwt63"}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.linear_model import MultiTaskLasso, ElasticNet, MultiTaskElasticNet\nfrom sklearn.linear_model import Lars, LassoLars, OrthogonalMatchingPursuit\nfrom sklearn.linear_model import BayesianRidge, ARDRegression\nfrom sklearn.linear_model import TweedieRegressor, PoissonRegressor,GammaRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVR\n\nfrom sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error, max_error, r2_score\nfrom sklearn.metrics import mean_absolute_percentage_error","metadata":{"id":"mhYwNdvOvoiA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef pcr(n_components=1,whiten = True, svd_solver = 'auto' ):\n    return make_pipeline(StandardScaler(), PCA(n_components=n_components,\n                                               whiten = whiten,\n                                               svd_solver = svd_solver), LinearRegression())\nmodels = [      \n    LinearRegression(),\n    Ridge(random_state = 42),\n    Lasso(random_state = 42),\n    ElasticNet(random_state = 42),\n    Lars(random_state = 42),\n    LassoLars(random_state = 42),\n    OrthogonalMatchingPursuit(),\n    BayesianRidge(),\n    ARDRegression(),\n    TweedieRegressor(),\n    SGDRegressor(random_state = 42),\n    PoissonRegressor(max_iter = 1000),\n    GammaRegressor(),\n    DecisionTreeRegressor(random_state = 42),\n    # RandomForestRegressor(random_state = 42),\n    KNeighborsRegressor(),\n    SVR(),\n    GradientBoostingRegressor(),\n    pcr(),\n    PLSRegression()\n]\nparams = [\n          ['linear', {'normalize' : [True,False]}],\n          ['ridge', {'alpha': numpy.logspace(0.01,100,100),\n                     'normalize' : [True, False],\n                     'solver' : ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}],\n          ['lasso', {'alpha' : numpy.logspace(0.01, 100, 100),\n                     'normalize' : [True, False],\n                     'precompute' : ['auto', True, False],\n                     'positive' : [True, False],\n                     'selection' : ['cyclic', 'random']}],\n          ['elasticnet', {'alpha' : numpy.logspace(0.01,100,100),\n                          'l1_ratio' : numpy.linspace(0.01, 1, 50),\n                          'normalize': [True, False],\n                          'selection' : ['cyclic', 'random'],\n                          'precompute' : [True, False],\n                          'positive' : [True,False]}],\n          ['lars', {'normalize': [True, False],\n                    'precompute' : [True, False],\n                    'eps' : numpy.linspace(0.01, 2, 100),\n                    'jitter' : numpy.linspace(0.1, 100, 100)}],\n          ['lassolars', {'alpha' : numpy.logspace(0.01,100,100),\n                         'normalize': [True, False],\n                         'precompute' : [True, False],\n                         'positive' :[True, False],\n                         'jitter' : numpy.linspace(0.1, 100, 100)}],\n          ['omp', {'normalize': [True, False],\n                   'precompute' : [True, False]}],\n          ['bayesian_ridge', {'alpha_1' : numpy.logspace(0.01, 100,100),\n                              'alpha_2' : numpy.logspace(0.01, 100,100),\n                              'lambda_1' : numpy.linspace(0.001, 1, 100),\n                              'lambda_2' : numpy.linspace(0.001, 1, 100),\n                              'lambda_init' : numpy.linspace(0.1, 1, 10),\n                              'normalize' : [True, False]}],\n          ['ard',  {'alpha_1' : numpy.logspace(0.01, 100,100),\n                    'alpha_2' : numpy.logspace(0.01, 100,100),\n                    'lambda_1' : numpy.linspace(0.001, 1, 100),\n                    'lambda_2' : numpy.linspace(0.001, 1, 100),\n                    'threshold_lambda' : numpy.linspace(0.1, 1, 10),\n                    'normalize' : [True, False]}],\n          ['tweedie', {'power' : [0,1,2,3],\n                       'alpha' : numpy.logspace(0.01,100,100),\n                       'link' : ['auto', 'identity', 'log']}],\n          ['SGD', {'loss' : ['squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n                   'penalty' : ['l2', 'l1', 'elasticnet'],\n                   'alpha' : numpy.logspace(0.01,100,100),\n                   'l1_ratio' : numpy.linspace(0.001, 1, 100),\n                   'shuffle' : [True,False],\n                   'epsilon' : numpy.linspace(0.01, 1,30),\n                   'learning_rate' : ['constant', 'invscaling', 'optimal', 'adaptive'],\n                   'power_t' : numpy.linspace(0.1, 2, 50),\n                   'validation_fraction' : numpy.linspace(0.1, 0.9,10),\n                   'average' : [True,False]}],\n          ['poisson', {'alpha' : numpy.logspace(0.01,100,100)}],\n          ['gamma', {'alpha' : numpy.logspace(0.01,100,100)}],\n          ['dtr', {'criterion' : ['mse', 'friedman_mse', 'mae', 'poisson'],\n                   'splitter' : ['best', 'random'],\n                   'min_samples_split' : range(2, 30),\n                   'min_samples_leaf' : range(1, 30),\n                   'min_weight_fraction_leaf': numpy.linspace(0,0.5, 10),\n                   'max_features' : ['auto', 'sqrt', 'log2'],\n                   'max_leaf_nodes' : range(1,30)}],\n        #   ['rfr', {'n_estimators' : range(2, 100, 50),\n        #            'criterion' : ['mse', 'mae'],\n        #            'max_depth' : [None, 10,20,30,40,50,60,70,80,90,100],\n        #            'min_samples_split' : range(2, 30),\n        #            'min_samples_leaf' : range(1, 30),\n        #            'min_weight_fraction_leaf': numpy.linspace(0,0.5, 10),\n        #            'max_features' : ['auto', 'sqrt', 'log2'],\n        #            'max_leaf_nodes' : range(1,30),\n        #            'bootstrap' : [True, False],\n        #            'oob_score' : [True,False],\n        #            'max_samples' : numpy.linspace(0.1,1, 10),\n        #            'ccp_alpha' : numpy.linspace(0.01, 100, 100)}],\n          ['knn' , {'n_neighbors' : range(2, 30),\n                    'weights' : ['uniform', 'distance'],\n                    'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n                    'p' : [1,2],\n                    'metric' : ['euclidean','manhattan','chebyshev','minkowski','seuclidean','mahalanobis']}],\n          ['svr', {}],\n          ['gbr', {}],\n        #   ['svr', {'kernel' : ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n        #            'degree' : [1,2,3,4,5,6,7,8,9],\n        #            'gamma' : ['scale', 'auto'],\n        #            'C' : numpy.logspace(1, 100, 100),\n        #            'epsilon' : numpy.linspace(0.01, 1, 20),\n        #            'shrinking' : [True,False]}],\n        #   ['gbr', {'loss' : ['ls', 'lad','huber', 'quantile'],\n        #            'learning_rate': numpy.linspace(0.01, 1, 50),\n        #            'n_estimators': range(20, 120, 5),\n        #            'subsample' : numpy.linspace(0.1, 1.0, 20),\n        #            'criterion' : ['friedman_mse', 'mse', 'mae'],\n        #            'max_depth' : [None, 10,20,30,40,50,60,70,80,90,100],\n        #            'min_samples_split' : range(2, 30),\n        #            'min_samples_leaf' : range(1, 30),\n        #            'min_weight_fraction_leaf': numpy.linspace(0,0.5, 10),\n        #            'ccp_alpha' : numpy.linspace(0.1, 10, 100)}],\n          ['pcr', {'pca__n_components' : numpy.linspace(0,1,50),\n                   'pca__whiten' : [True, False],\n                   'pca__svd_solver' : ['auto', 'full', 'arpack', 'randomized']}],\n        #   ['pls', {'n_components' : range(1,1505),\n        #            'scale' : [True, False]}],\n          ['pls', {}]         \n]","metadata":{"id":"VE8FsnTPwznp","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_score = {'mean_sq_err' : [],\n         'max_err': [],\n         'r2' : [],\n         'percent_err' : []}    \n\ntest_score = {'mean_sq_err' : [],\n         'max_err': [],\n         'r2' : [],\n         'percent_err' : []}    \n\nimport traceback\nfor i in range(len(models)):\n    try:\n        print(params[i][0])\n        clf = RandomizedSearchCV(models[i],\n                            param_distributions= params[i][1],\n                            cv = 5,\n                            scoring = \"neg_mean_squared_error\",\n                            n_jobs = 5, verbose = 100)\n\n        clf.fit(x_train, y_train)\n        y_pred = clf.predict(x_test)\n        y_train_pred = clf.predict(x_train)\n        train_score['mean_sq_err'].append(clf.best_score_)\n        train_score['max_err'].append(max_error(y_train,y_train_pred))\n        train_score['r2'].append(r2_score(y_train,y_train_pred))\n        train_score['percent_err'].append(mean_absolute_percentage_error(y_train,y_train_pred))\n\n        test_score['mean_sq_err'].append(mean_squared_error(y_test,y_pred))\n        test_score['max_err'].append(max_error(y_test,y_pred))\n        test_score['r2'].append(r2_score(y_test,y_pred))\n        test_score['percent_err'].append(mean_absolute_percentage_error(y_test,y_pred))\n\n        print()\n        print('best_estimator :', clf.best_estimator_)\n        print('train report :')\n        for key in train_score:\n            print(f'  {key} : {train_score[key][i]}')\n\n        print('test report :')\n        for key in test_score:\n            print(f'  {key} : {test_score[key][i]}')\n    except Exception as e:\n        print('\\n\\n ', e)\n    print(\"---------------------------------------------------------------------------\")","metadata":{"id":"v9E3CoQcIT4E","outputId":"1986824b-1ea7-4ff4-a30c-9a6efd36eb84","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_score = pandas.DataFrame(train_score, index = [i[0] for i in params[:]])\ntest_score = pandas.DataFrame(test_score, index = [i[0] for i in params[:]])\nnames = [i[0] for i in params]\npyplot.figure(figsize = (25,10))\npyplot.subplot(121)\nseaborn.barplot( y = train_score['mean_sq_err'][1:], x = train_score.index[1:])\npyplot.subplot(122)\nseaborn.barplot( y = test_score['mean_sq_err'][1:], x = test_score.index[1:])\npyplot.show()\n# print(format(train_score['mean_sq_err']))","metadata":{"id":"6XQxuxGuL0PX","outputId":"b5649564-3cf2-4356-903d-1a72dc6047a5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclf = RandomizedSearchCV(LassoLars(),\n                    param_distributions = {'alpha' : numpy.logspace(0.01,100,100),\n                         'normalize': [True, False],\n                         'precompute' : [True, False],\n                         'positive' :[True, False],\n                         'jitter' : numpy.linspace(0.1, 100, 100)},\n                    cv = 5,\n                    scoring = \"neg_mean_squared_error\",\n                    n_jobs = 5, verbose = 100)\n\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\ny_train_pred = clf.predict(x_train)\n\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = numpy.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n                  .format(results['mean_test_score'][candidate],\n                          results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n\nreport(clf.cv_results_)","metadata":{"id":"PFCOmwLhFnsG","outputId":"4ebf57f8-dbd2-42d1-95ab-3ad88098311b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.columns","metadata":{"id":"QTs9nszNWQdM","outputId":"bd83b9ac-ec29-48f2-a4dd-5f5a40ab7d0c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# omp best regressor\nparams = {'precompute': True, 'positive': True, 'normalize': False, 'jitter': 7.163636363636363, 'alpha': 2.187761623949534e+33}\nclf = LassoLars(**params)\nclf.fit(x_train,y_train)\ny_pred = clf.predict(valid)\ny_pred.shape, valid.shape","metadata":{"id":"ybv8O7JADUm0","outputId":"143340d7-6842-4552-9174-339e4accb9a7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred","metadata":{"id":"yZgJlJ5QOnXS","outputId":"995d8ba7-0e79-4133-e5f9-e57fa6a1439c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = pandas.DataFrame(y_pred, columns = ['Success_probability'])\ndeal_title = pandas.DataFrame(deal_title, columns = ['Deal_title'])\n# # deal_title.reset_index(drop=True, inplace = True)\n# # y_pred.reset_index(drop=True, inplace = True)\nsubmit = pandas.concat([deal_title, y_pred], axis = 1 ,ignore_index=True)\n# # submit.to_csv('submit.csv')\nsubmit\n# deal_title['Success_probability'] = y_pred\n# deal_title","metadata":{"id":"fhSw88c4FJaD","outputId":"006aab76-5464-4e04-9b33-4ebee389fea2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit.columns = ['Deal_title', 'Success_probability']\nsubmit.to_csv('submit.csv')","metadata":{"id":"1rwC5u4aKFbU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Neural Network Model","metadata":{"id":"JIoL3tiDsQU6"}},{"cell_type":"code","source":"train.columns","metadata":{"id":"qfQyXncegS86","outputId":"ea4f03b8-b06b-451a-9d51-463066fb3b61","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gridsearch cross validation in neural network model\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.metrics import MeanSquaredError\nx_train.astype(int)\nx_test.astype(int)\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = numpy.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n                  .format(results['mean_test_score'][candidate],\n                          results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n\ndef nn_model(activation = 'relu', neurons = 32, optimizer = 'Adam',dropout = 0.1, init_mode = 'uniform'):\n    model = Sequential()\n    model.add(Dense(neurons, input_dim = 163, kernel_initializer = init_mode, activation= activation))\n    model.add(Dense((neurons*2)//3, kernel_initializer = init_mode,activation= activation))\n    model.add(Dense((neurons*4)//9,kernel_initializer = init_mode,  activation = activation))\n    model.add(Dropout(dropout))\n    model.add(Dense(1, kernel_initializer = init_mode, activation='linear'))\n    model.compile(loss='mean_squared_error', optimizer= optimizer, metrics=[MeanSquaredError()])\n    return model\n\n# Defining grid parameters\nactivation = ['softmax', 'softplus', 'softsign', 'relu', 'selu', 'elu', 'tanh','sigmoid', 'linear']\nneurons = range(1, 1600, 150)\ndropout = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\ninit_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\noptimizer = ['SGD', 'Adam', 'Adamax','RMSprop','Adagrad','Adadelta','Nadam','Ftrl']\nbatch_size = range(10,101,10)\nparam_grid = dict(activation = activation, neurons = neurons, optimizer = optimizer, dropout = dropout, init_mode = init_mode, batch_size = batch_size)\n\nkr = KerasRegressor(build_fn= nn_model, epochs= 5, batch_size = 40, verbose = 1)\n\nmodel = RandomizedSearchCV(estimator= kr,cv = 3, param_distributions = param_grid, n_jobs=5, verbose = 100)\nmodel.fit(x_train,y_train)\n\nreport(model.cv_results_)","metadata":{"id":"zyum4Y8ysUJg","outputId":"c880355f-da83-4280-e680-01ac73ac82ee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gridsearch cross validation in neural network model\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.metrics import MeanSquaredError\n\ndef nn_model(activation = 'softplus', neurons = 163, optimizer = 'Adamax', dropout = 0.4, init_mode = 'normal'):\n    model = Sequential()\n    model.add(Dense(neurons, input_dim =163, kernel_initializer = init_mode, activation= activation))\n    model.add(Dense((neurons*2)//3, kernel_initializer = init_mode, activation = activation))\n    model.add(Dense((neurons*4)//9, kernel_initializer= init_mode,  activation = activation))\n    model.add(Dropout(dropout))\n    model.add(Dense(1, kernel_initializer = init_mode, activation='linear'))\n    model.compile(loss='mean_squared_error', optimizer= optimizer, metrics=[MeanSquaredError()])\n    return model\n\nkr = KerasRegressor(build_fn= nn_model, epochs=89, verbose=1, batch_size = 50)\nkr.fit(x_train,y_train)\ny_train_pred = kr.predict(x_train)\ny_pred = kr.predict(x_test)\n\nprint('train_score : ')\nprint('  mean_sq_err : ', mean_squared_error(y_train,y_train_pred))\nprint('  max_err : ', max_error(y_train,y_train_pred))\nprint('  r2 : ',r2_score(y_train,y_train_pred))\n# print('  percent_err : ', mean_absolute_percentage_error(y_train,y_train_pred))\n\nprint('test_score : ') \nprint('  mean_sq_err : ', mean_squared_error(y_test,y_pred))\nprint('  max_err : ', max_error(y_test,y_pred))\nprint('  r2 : ',r2_score(y_test,y_pred))\n# print('  percent_err : ', mean_absolute_percentage_error(y_test,y_pred))\n","metadata":{"id":"s_sciXaz8IsS","outputId":"b3172e12-d97a-4b99-e862-bacb94787fb8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid.astype(int)\ny_valid = kr.predict(valid)\ndeal_title = pandas.DataFrame(deal_title)\nsubmit = deal_title.copy()\nsubmit['1'] = y_valid\nsubmit.columns = ['Deal_title', 'Success_probability']\nsubmit.to_csv('submit.csv')","metadata":{"id":"Uglw2ESs9dhv","outputId":"f34eaaf8-b77e-466c-c37e-8e6cb63c126b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit","metadata":{"id":"ogPKLOTI-bd3","outputId":"251c407f-1002-43d3-ae28-5b291c26afc1","trusted":true},"execution_count":null,"outputs":[]}]}