{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report\n\nfrom lightgbm import LGBMClassifier\n\nimport tensorflow as tf\nimport tensorflow_hub as hub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/disaster-tweets-cleaned/df.csv')\ntest_df = pd.read_csv('../input/disaster-tweets-cleaned/test_df.csv')\nprint(df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_sent_len(dataf, col, title):\n    dataf['len_' + col] = dataf[col].apply(lambda txt: len(txt.split()))\n    plt.hist(dataf['len_' + col], bins = 100)\n    plt.title('Train sentences length')\n    plt.show()\n    return dataf\n\ncol = 'ctext'\ndf = plot_sent_len(df, col, 'sentence lengths')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_txts, val_txts, y_train, y_val = train_test_split(\n    df[col].values, df['target'].values,\n    shuffle = True, test_size = 0.15,\n    stratify = df['target'].values,\n)\ntest_txts = test_df[col].values\ny_test = test_df['target'].values\nprint('Train size:', train_txts.shape)\nprint('Validation size:', val_txts.shape)\nprint('Test size:', test_txts.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_ext = hub.KerasLayer(module_url, trainable = False)\nprint (\"module %s loaded\" % module_url)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = feature_ext(train_txts)\nval_X = feature_ext(val_txts)\ntest_X = feature_ext(test_txts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_embedding(X, y):\n    colors = ['green', 'red']\n    labels = ['NoDisastor', 'Disastor']\n    proj = PCA(2)\n    proj_X = proj.fit_transform(X)\n    for y_id in np.unique(y):\n        plt.scatter(\n            x = proj_X[y == y_id, 0], \n            y = proj_X[y == y_id, 1],\n            s = 4,\n            label = labels[y_id],\n            c = colors[y_id], \n            alpha = 0.4\n        )\n        plt.xlabel('X1')\n        plt.ylabel('X2')\n        plt.legend()\n\ndef plot_embeddings(train_X, y_train, val_X, y_val, test_X ,y_test):\n    fig = plt.figure(figsize = (12, 4))\n    plt.subplot(1, 3 ,1 )\n    plot_embedding(train_X, y_train)\n    plt.title(f'Train dataset')\n    \n    plt.subplot(1, 3 ,2)\n    plot_embedding(val_X, y_val)\n    plt.title(f'Val dataset')\n    \n    plt.subplot(1, 3 ,3)\n    plot_embedding(test_X, y_test)\n    plt.title(f'Test dataset')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_embeddings(train_X, y_train, val_X, y_val, test_X ,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    x_in = tf.keras.layers.Input(shape = (), dtype = tf.string)\n    feature_ext = hub.KerasLayer(module_url, trainable = False, name = 'feat_ext')\n    x_features = feature_ext(x_in)\n    x = tf.keras.layers.Dropout(0.1)(x_features)\n    x_out = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n    return tf.keras.Model(x_in, x_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(0.005)\nloss_objective = tf.keras.losses.BinaryCrossentropy()\nmodel = create_model()\nmodel.compile(loss = loss_objective, optimizer = optimizer, metrics = ['acc'])\nmodel.fit(\n    x = train_txts, y = y_train,\n    validation_data = (val_txts, y_val),\n    epochs = 5,\n    batch_size = 32\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_hat = model.predict(test_txts)\ny_model_hat = np.array([1 if x[0] >0.5 else 0 for x in y_test_hat])\nprint(classification_report(y_test, y_model_hat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_gbm_cls(X_tr, y_tr, X_val, y_val, X_test, y_test):\n    gbm_cls = LGBMClassifier(\n        objective = 'binary',\n    )\n    gbm_cls.fit(\n        X_tr, y_tr,\n        eval_set = (X_val, y_val),\n        early_stopping_rounds = 20,\n        verbose = 0,\n    )\n    print('Train')\n    print(classification_report(y_train, gbm_cls.predict(X_tr)))\n    print('Validation')\n    print(classification_report(y_val, gbm_cls.predict(X_val)))\n    print('Test')\n    gbm_y_hat = gbm_cls.predict(X_test)\n    print(classification_report(y_test, gbm_y_hat))\n    return gbm_cls, gbm_y_hat\n\ngbm_cls, gbm_y_hat = train_gbm_cls(train_X, y_train, val_X, y_val, test_X ,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}