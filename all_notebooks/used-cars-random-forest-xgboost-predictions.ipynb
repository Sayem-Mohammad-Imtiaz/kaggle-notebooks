{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from shutil import copyfile\ncopyfile(src = \"../input/dataproc/data_proc.py\", dst = \"../working/data_proc.py\")\nfrom data_proc import *\n\nfrom sklearn.model_selection import train_test_split\nfrom fastai.tabular.all import *\nfrom fastai.callback import *\nimport fastai ; print(fastai.__version__)\n\n## Loading ML tools\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.tree import DecisionTreeRegressor #Decision Tree regressor\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.metrics import mean_absolute_error\n\n#pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = Path('../input/craigslist-carstrucks-data')\n\nimport datatable as dt\ndata_tbl = dt.fread(PATH/\"vehicles.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_raw=data_tbl.to_pandas()\ndf_raw.head().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove of unused columns\ndrop_columns = ['C0','id','url','region_url','image_url','VIN','description','lat','long','posting_date']\ndf_cars_raw = df_raw.drop(columns = drop_columns)\n\n# add age as float\ndf_cars_raw['age'] = (2020 - df_cars_raw['year']).astype(float)\ndf_cars_raw.tail().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# there are still some numerical columns with missing values. We will use Fastai for preprocessing\ndf_cars_raw.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FASTAI tabular model"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cars_no_price = df_cars_raw.query(\"price<100 or price>100000\", engine='python').reindex()\ndf_cars = df_cars_raw.query(\"price>100 and price<100000\", engine='python').reindex()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cars.query(\"price<100\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# When doing Regression with these large numbers, there is often used the log of target\n#df_cars.loc[df_cars['price'] < 100, 'price'] = 100 # update the ~0 price to 100\n\n# df_cars.loc[df_cars['price'] > 100000, 'price'] = 100000 # update the super high price to 100000\ndf_cars[\"price_log\"] = np.log(df_cars['price'])\ndf_cars.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cars.hist('price',figsize=(12,7),bins=100,alpha=0.75)\nplt.title('price distribution')\nplt.ylabel('Number of objects')\nplt.xlabel(\"total_price\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cars.hist('price_log',figsize=(12,7),bins=100,alpha=0.75)\nplt.title('price log distribution')\nplt.ylabel('Number of objects')\nplt.xlabel(\"total_price_log\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Memory adjustment, drop variables what are not used."},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop unused variables from memory\nimport sys\ndef sizeof_fmt(num, suffix='B'):\n    \n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n                         key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del df_raw\ndel data_tbl\ndel df_cars_raw\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reserve 10% test \nprint(\"Total lenght car_df:\", len(df_cars))\n\ndf_training, df_test = train_test_split(df_cars, test_size=0.1)\n#split  20% valid \ndf_train, df_valid = train_test_split(df_training, test_size=0.25)\nprint(\"Split lenght: Train:\", len(df_train),\" Valid:\",len(df_valid),\" Test:\", len(df_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocesing"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_train= df_train.drop(['price'], axis=1)\ntrain_cats(df_train)\ntrain_df,train_target,nas = proc_df(df_train,'price_log')\n\ndf_valid= df_valid.drop(['price'], axis=1)\ntrain_cats(df_valid)\nvalid_df,valid_target,nas= proc_df(df_valid,'price_log')\n\n#save column names for later\nculumn_names = train_df.columns[0:]\nculumn_names","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Normalization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#before normalization\ntrain_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\n\ntrain_df = pd.DataFrame(min_max_scaler.fit_transform(train_df.values))\nvalid_df = pd.DataFrame(min_max_scaler.transform(valid_df.values))\ntrain_target = pd.DataFrame(min_max_scaler.fit_transform(train_target[:,None]))\nvalid_target = pd.DataFrame(min_max_scaler.transform(valid_target[:,None]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Restore column names\ntrain_df.columns = culumn_names\nvalid_df.columns = culumn_names\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data after normalization\ntrain_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#format targets to series\ntrain_target=train_target.values.ravel()\nvalid_target=valid_target.values.ravel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baseline models\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"regr = linear_model.LinearRegression()\nregr.fit(train_df, train_target)\nprint_score(regr,train_df, valid_df,train_target, valid_target)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestRegressor( n_jobs=-1, max_depth=30)\n%time random_forest.fit(train_df, train_target)\n\nprint('Basic RandomForest model stats:')\nprint_score(random_forest,train_df, valid_df,train_target, valid_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.stack([t.predict(valid_df) for t in random_forest.estimators_])\npreds[:,0], np.mean(preds[:,0]), valid_target[0]\nplt.plot([metrics.r2_score(valid_target, np.mean(preds[:i+1], axis=0)) for i in range(40)]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = rf_feat_importance(random_forest,train_df); fi[:40]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tuning RandomForestRegressor hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = [10,20,25,30,40,50]\nmin_samples_split = [2, 5, 10]\nmax_leaf_nodes = [500,1000, 1500,2000]\nmin_samples_leaf = [1, 2, 4]\nn_estimators = [10,20,40,60,80]\nmax_features = ['sqrt','log',0.5,1,'auto']\n\nhyperparameters = {'max_depth': max_depth,\n                   'min_samples_split': min_samples_split,\n                   'max_leaf_nodes': max_leaf_nodes,\n                   'min_samples_leaf': min_samples_leaf,\n                   'n_estimators': n_estimators,\n                   'max_features': max_features\n              }\nprint('Hyperparameters:')\npprint(hyperparameters )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestRegressor(n_jobs=-1)\nparams_search = RandomizedSearchCV(estimator=random_forest,\n                                   param_distributions=hyperparameters ,\n                                   cv = 2, n_iter = 10, random_state=158,\n                                   scoring = 'neg_mean_absolute_error')\nparams_search.fit(train_df, train_target)\nprint(\" RandomizedSearchCV results \" )\nprint(\"\\n The best estimator across ALL searched params:\\n\", params_search.best_estimator_)\nprint(\"\\n The best score across ALL searched params:\\n\", params_search.best_score_)\nprint(\"\\n The best parameters across ALL searched params:\\n\", params_search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Runing random forest models with best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestRegressor(max_depth=20, max_features=0.5, max_leaf_nodes=2000,\n                      min_samples_split=5, n_estimators=10, n_jobs=-1)\n%time random_forest.fit(train_df, train_target)\n\nprint('Basic RandomForest model stats:')\nprint_score(random_forest,train_df, valid_df,train_target, valid_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestRegressor(max_depth=50, max_features=0.5, max_leaf_nodes=1500,min_samples_leaf=4,\n                      min_samples_split=5, n_estimators=60, n_jobs=-1)\n%time random_forest.fit(train_df, train_target)\n\nprint('Basic RandomForest model stats:')\nprint_score(random_forest,train_df, valid_df,train_target, valid_target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost "},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor,XGBClassifier\n\nxgb_model = XGBRegressor(n_estimators=500,n_jobs=-1,learning_rate=0.1)\nxgb_model.fit(train_df, train_target, \n             early_stopping_rounds=5, \n             eval_set=[(valid_df, valid_target)],\n             verbose=False)\n\n\n\nscore_board = print_score(xgb_model,train_df, valid_df,train_target, valid_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBRegressor(n_jobs=-1,)\n\nparam_grid = {\n        'max_depth': [5, 10, 15, 20,30],\n        'learning_rate': [0.05, 0.1, 0.2, 0,3],\n        'n_estimators': [100,500,1000,1500,2000]}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rs_clf = RandomizedSearchCV(xgb_model, param_grid, n_iter=20,\n                            n_jobs=-1, verbose=2, cv=2,\n                            \n                            scoring='neg_log_loss', refit=False, random_state=42)\nprint(\"Randomized search..\")\nsearch_time_start = time.time()\nrs_clf.fit(train_df, train_target,early_stopping_rounds=5,\n           eval_set=[(valid_df, valid_target)],\n             verbose=False,eval_metric= 'mlogloss', )\nprint(\"Randomized search time:\", time.time() - search_time_start)\n\nbest_params = rs_clf.best_params_\n\nprint(\"Best params: \")\nfor param_name in sorted(best_params.keys()):\n    print('%s: %r' % (param_name, best_params[param_name]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = XGBRegressor(n_estimators=1500,n_jobs=-1,learning_rate=0.05,max_depth= 20)\nxgb_model.fit(train_df, train_target, \n             early_stopping_rounds=10, \n             eval_set=[(valid_df, valid_target)],\n             verbose=False)\n\n\nscore_board = print_score(xgb_model,train_df, valid_df,train_target, valid_target)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocesing\n\ndf_test = df_test.reset_index()\n\ndf_test=df_test.drop(columns='index')\n\n#make var for predictions\ndf_test_to_model=df_test.drop(columns='price')\n\ntrain_cats(df_test_to_model)\ndf_test_to_model,test_target,nas= proc_df(df_test_to_model,'price_log')\n\n#normalize\ndf_test_to_model=pd.DataFrame(min_max_scaler.transform(df_test_to_model.values))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\ny_pred = random_forest.predict(df_test_to_model)\n#inverse normalization\ny_pred_inverse= pd.DataFrame(min_max_scaler.inverse_transform(y_pred[:,None]))\n\n\ndf_test['predicted_price'] =np.exp(y_pred_inverse).round()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[['price','predicted_price','region','year','manufacturer','model','condition','cylinders','fuel','title_status', 'transmission','drive', 'size', 'type', 'state']].head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test dataset what was without price"},{"metadata":{"trusted":true},"cell_type":"code","source":"#preprocesing\n\ndf_cars_no_price = df_cars_no_price.reset_index()\n\ndf_cars_no_price=df_cars_no_price.drop(columns='index')\ndf_cars_no_price[\"price_log\"] = np.log1p(df_cars_no_price['price'])\n\n#make var for predictions\ndf_cars_no_price_to_model=df_cars_no_price.drop(columns='price')\n\ntrain_cats(df_cars_no_price_to_model)\ndf_cars_no_price_to_model,test_target,nas= proc_df(df_cars_no_price_to_model,'price_log')\n\n#normalize\ndf_cars_no_price_to_model=pd.DataFrame(min_max_scaler.transform(df_cars_no_price_to_model.values))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict\ny_pred = random_forest.predict(df_cars_no_price_to_model)\n#inverse normalization\ny_pred_inverse= pd.DataFrame(min_max_scaler.inverse_transform(y_pred[:,None]))\n\n\ndf_cars_no_price['predicted_price'] =np.expm1(y_pred_inverse).round()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the teory what the price was not right for condition was confirmed, the price is predicted greater than 0\ndf_cars_no_price[['price','predicted_price','region','year','manufacturer','model','condition','cylinders','fuel',\n                  'title_status', 'transmission','drive', 'size', 'type', 'state']].tail(20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}