{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Statsmodels is a Python module that provides classes and functions for the estimation of statistical models (such as Ordinary, Weighted, and Generalized Least Squares), as well as conducting statistical tests and data exploration.  I will be using this module to analyze the historical data set of Szeged's weather from 2006-2016 from https://www.kaggle.com/budincsevity/szeged-weather, which is a data set consisting of 96,453 observations and 12 potential weather variables","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/szeged-weather/weatherHistory.csv')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To prepare my data for analysis, I determined that the time, summary, loud cover, and daily summaries were irrelevant to how I wanted to analyze my data. First of all, the time was in a format that Python did not recognize and I did not want to spend additional time cropping the date and formatting it into a interpretative integer so that we could add it to our model.  Secondly, the summary and daily summary consisted of strings that do not convert to dummy variables well as they had multiple categories of partly cloudy, mostly cloudy, overcast, foggy, clear, etc.  Finally, loud cover consisted of a column entirely made up of 0's, therefore irrelevant to our data analysis.","metadata":{}},{"cell_type":"markdown","source":"Next, Precip type had three values: rain, snow, or null. I converted this parameter into two dummy variable such that the combination of 1 and 0 indicated rain, 0 and 0 indicated snow, 0 and 1 indicated null (or clear weather). My final data set looked like:","metadata":{}},{"cell_type":"code","source":"#dropping Date, Summary, loud cover, and daily summary\ndf.drop(['Formatted Date','Summary','Loud Cover','Daily Summary'], axis=1,inplace=True)\n\n#alighing Apparent Temp to \"y\" and adding dummy variables for precip type\nfirst_column=df.pop('Apparent Temperature (C)')\ndf.insert(0,'Apparent Temperature (C)',first_column)\ndf=pd.get_dummies(df, columns=['Precip Type'],dummy_na=True)\n\n#Precip Type_nan is when it is neither rainy or snowy\ndf.drop('Precip Type_snow', axis=1,inplace=True)\ndf.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating X-Matrix\nX=df.copy()\nX=X.iloc[:,1:]\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#creating y-vector\ny=df.copy()\ny=y.iloc[:,0]\ny","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#inserting our intercept variable (note: SKLearn does this automatically, but not statsmodels)\nones=np.full((96453,1),1)\nX.insert(0,'intercept',ones)\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using Python's SKLearn module, I then split my data set into two equal halves: a train set and test set. My train set I would be using to estimate my parameters, and my test set would be to determine how well my model worked on existing data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.5)\nX_train=X_train.reset_index(drop=True)\ny_train=y_train.reset_index(drop=True)\nX_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"My first model will include all remaining variables: Temperature, Humidity, Wind Speed, Wind Bearing, Visibility, Pressure, and dummy variables for rain, snow, and null.","metadata":{}},{"cell_type":"code","source":"import statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom scipy import stats\nfrom matplotlib import pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=sm.OLS(y_train,X_train)\nresults=model.fit()\nparams=results.params\nparams=pd.DataFrame(params)\nparams","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residuals = results.resid\nfig, ax = plt.subplots(figsize=(10,8))\nfig=sm.qqplot(residuals,color='k', ax=ax)\nax.set_title('Normal Q-Q')\nax.set_ylabel('Standardized Residuals')\nax.set_xlabel('Theoretical Quantiles')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From our probability plot we see flattening at the extremes, which is a pattern typical of samples from a distribution with heavier tails than normal. This potentially indicates that our true error (difference between our observed value and true unobserved value) is not normally distributed.","metadata":{}},{"cell_type":"code","source":"fitted = results.fittedvalues\nfig, ax = plt.subplots(figsize=(10,8))\nax.scatter(fitted, residuals, edgecolors = 'k', facecolors = 'none')\nax.set_ylabel('Residuals')\nax.set_xlabel('Fitted Values')\nax.set_title('Residuals vs. Fitted')\nax.plot([min(fitted),max(fitted)],[0,0],color = 'k',linestyle = ':', alpha = .3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A curved plot (such as a plot exhibiting a u-like shape or inverted u-like shape) indicates nonlinearity.  This could mean that other regressor variables are needed in model or transformations on the regressor and/or the response variable could be helpful. However, before we go on to transforming our model, I would like to note that the condition number in our summary table is large (1.47x$10^{04}$) indicating that there is strong multicollinearity or other numerical problems in our data. Multicollinearity means that one of the predictors is an exact linear combination of some of the others.  So at this point we could either transform our model or reduce the amount of parameters in our model to remove our multicollinearity using a form of backward elimination.\n\nBackward elimination begins with a model that includes all candidate regressors.  Then the partial F statistic (or equivalently, a t statistic) is computed for each regressor as if it were the last variable to enter the model.  The smallest of these partial F (or t) statistics is compared with a preselected value, and if the smallest partial F (or t) is less than our presselected value, that regressor is removed from the model.","metadata":{}},{"cell_type":"code","source":"plt.rc(\"figure\", figsize=(20,10))\nplt.rc(\"font\", size=14)\nfig = sm.graphics.plot_partregress_grid(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After obtaining the least-squares fit, three questions come into mind:\n*     How well does this equation fit the data?\n*     Is the model likely to be useful as a predictor?\n*     Are any of the basic assumptions (such as constant variance and uncorrelated errors) violated, and if so, how serious is this?\n\nFrom our summary data, our $R^2$ and $R^2_{Adj}$ indicate that our equation fits the data extremely well. To determine how likely the model will be useful as a predictor we need to calculate our $R^2_{prediction}$.  The PRESS statistic can be used to compute an $R^2$-like statistic for prediction.  This statistic gives some indication of the predictive capability of the regression model:\n\\begin{equation*}\n    \\begin{split}\n        PRESS&=\\sum_{i=1}^n\\left(\\frac{e_i}{1-h_{ii}}\\right)^2\\\\\n        R^2_{prediction}&=1-\\frac{PRESS}{SS_T}\n    \\end{split}\n\\end{equation*}\nFor the weather data model, we find\n\\begin{equation*}\n    \\begin{split}\n        R^2_{prediction}&=1-\\frac{55158.4212}{5544530.877}\\\\\n        &=0.9901\n    \\end{split}\n\\end{equation*}\nTherefore, we could expect this model to \"explain\" about 99.01\\% of the variability in predicting new observations.","metadata":{}},{"cell_type":"code","source":"n=48226\nresults.mse_total*n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import OLSInfluence\n\n#This number may vary from the data in markdown as the train test split is different\n#calculating PRESS\ninfl=results.get_influence()\ndiag=infl.hat_matrix_diag\nPRESS=np.full(diag.shape,0,dtype=float)\nfor i in range(len(diag)):\n    denom=(1-diag[i])\n    PRESS[i]=np.divide(residuals[i],denom)**2\nPRESS.sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculating R^2 prediction\nn=48226\nRpred=1-(PRESS.sum()/(results.mse_total*n))\nRpred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see if we can make our model better. I removed the dummy variables and the pressure as their t-statistics were smaller than compared to the other parameters","metadata":{}},{"cell_type":"code","source":"X_train2=X_train.copy()\nX_train2.drop(['Precip Type_nan','Precip Type_rain','Pressure (millibars)'], axis=1,inplace=True)\nX_train2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2=sm.OLS(y_train,X_train2)\nresults2=model2.fit()\nresults2.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once again we see strong multicollinearity, however I wanted to see how my PRESS statistic and $R^2_{prediction}$ values changed.\n\\begin{equation*}\n    \\begin{split}\n        PRESS&=55807.794\\\\\n        R^2_{prediction}&=1-\\frac{55807.794}{5544530.877}\\\\\n        &=0.9899 \\quad (\\Delta 0.0001171)\\textrm{ from Model 1}\n    \\end{split}\n\\end{equation*}","metadata":{}},{"cell_type":"code","source":"residuals2 = results2.resid\ninfl2=results2.get_influence()\ndiag2=infl2.hat_matrix_diag\nPRESS2=np.full(diag2.shape,0,dtype=float)\nfor i in range(len(diag2)):\n    denom=(1-diag2[i])\n    PRESS2[i]=np.divide(residuals2[i],denom)**2\nPRESS2.sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n=48226\nRpred2=1-(PRESS2.sum()/(results2.mse_total*n))\nRpred2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets remove a few more parameters","metadata":{}},{"cell_type":"code","source":"X_train3=X_train2.copy()\nX_train3.drop(['Visibility (km)','Wind Bearing (degrees)'], axis=1,inplace=True)\nX_train3.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3=sm.OLS(y_train,X_train3)\nresults3=model3.fit()\nresults3.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"residuals3 = results3.resid\nfig, ax = plt.subplots(figsize=(10,8))\nfig=sm.qqplot(residuals3,color='k', ax=ax)\nax.set_title('Normal Q-Q')\nax.set_ylabel('Standardized Residuals')\nax.set_xlabel('Theoretical Quantiles')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fitted3 = results3.fittedvalues\nfig, ax = plt.subplots(figsize=(10,8))\nax.scatter(fitted3, residuals3, edgecolors = 'k', facecolors = 'none')\nax.set_ylabel('Residuals')\nax.set_xlabel('Fitted Values')\nax.set_title('Residuals vs. Fitted')\nax.plot([min(fitted3),max(fitted3)],[0,0],color = 'k',linestyle = ':', alpha = .3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infl3=results3.get_influence()\ndiag3=infl3.hat_matrix_diag\nPRESS3=np.full(diag3.shape,0,dtype=float)\nfor i in range(len(diag3)):\n    denom=(1-diag3[i])\n    PRESS3[i]=np.divide(residuals3[i],denom)**2\nPRESS3.sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n=48226\nRpred3=1-(PRESS3.sum()/(results3.mse_total*n))\nRpred3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### PRESS Comparison","metadata":{}},{"cell_type":"code","source":"print(PRESS.sum(), PRESS2.sum(),PRESS3.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### R^2 prediction comparison","metadata":{}},{"cell_type":"code","source":"print(Rpred,Rpred2,Rpred3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### $MS_{Res}$ comparison","metadata":{}},{"cell_type":"code","source":"ssr=results.ssr\nssr2=results2.ssr\nssr3=results3.ssr\nmsres=ssr/(n-9)\nmsres2=ssr2/(n-6)\nmsres3=ssr3/(n-4)\nprint(msres,msres2,msres3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When determining the best model, we would like to choose a model that maximizes $R^2$ and $R^2_{prediction}$, but also minimizes our $MS_{Res}$ and PRESS. Taking into consideration that our first two models had high multicollinearity, we see that choosing model 3 is an acceptable choice as our $R^2$, $R^2_{prediction}$, $MS_{Res}$ and PRESS values are not that different than our first model (which has the best values). Therefore our final model would be model 3 with the following parameters","metadata":{}},{"cell_type":"code","source":"results3.params","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How well does it predict new values?","metadata":{}},{"cell_type":"code","source":"resMatrix=results3.params.to_numpy().reshape((4,1))\nX=X_test.to_numpy()\nx=np.full(X[:,:4].shape,0,dtype=float)\nfor i in range(len(resMatrix)):\n    for j in range(len(X_test)):\n        a=np.dot(resMatrix[i],X[j,i])\n        x[j][i]=a[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yhat=x.sum(axis=1).reshape((48227,1))\ny=y_test.to_numpy().reshape((48227,1))\nresid=np.subtract(y,yhat)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,8))\nax.scatter(yhat, resid, edgecolors = 'k', facecolors = 'none')\nax.set_ylabel('Residuals')\nax.set_xlabel('Fitted Values')\nax.set_title('Residuals vs. Fitted')\nax.plot([min(yhat),max(yhat)],[0,0],color = 'k',linestyle = ':', alpha = .3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#calculating SS_Res\nnp.matmul(resid.T,resid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see our $SS_{Res}$ isn't as different as the values we determined from the previous models, so I would conclude we have a pretty good model.\n\nLet me know what you think\n\n-Sonja","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}