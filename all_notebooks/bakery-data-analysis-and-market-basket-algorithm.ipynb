{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# We will go with 3 parts\n# part 1: data analysis\n# part 2: Pareto analysis\n# part 3: market basket analysis (this is my first time for this algorithm, please give your feedback, cheers)\n\n# Importing all the necessary librabries\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#read data CSV file\nraw_data=pd.read_csv('../input/BreadBasket_DMS.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#PART 1- DATA ANALYSIS ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12ae33307c0e02b615581b508293aa3efcc7f3b9"},"cell_type":"code","source":"#removing all rows where item value is NONE\nraw_data=raw_data[raw_data['Item']!='NONE']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82c90282b89a6ea9bbcbf26ce1480d147ea65cc5"},"cell_type":"code","source":"#groupby item\nno_of_item_transc= raw_data.groupby(['Item'])['Transaction'].count() \nno_of_item_transc=no_of_item_transc.reset_index()\nno_of_item_transc=no_of_item_transc.sort_values(['Transaction'],ascending=False)\ntop_item_transc=no_of_item_transc.head(25)\n\n#plot graph for highest sold top 25 items\nfig,axis=plt.subplots(figsize=(10,6))\naxis=sns.barplot(data=top_item_transc,x='Item',y='Transaction')\naxis.set_xticklabels(top_item_transc['Item'],rotation=70,color='b')\naxis.set_xlabel('Name Of The Items',color='red',fontsize=16)\naxis.set_ylabel('Total # of Transcations of Items', fontsize=16,color='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a08718d5ec33c308b30874bbf56b5aabc84e029"},"cell_type":"code","source":"#groupby month and year\nmonth_year=raw_data.copy()  \nmonth_year['Date']=pd.to_datetime(month_year['Date'])\nmonth_year['Month'],month_year['Year']=month_year['Date'].dt.month,month_year['Date'].dt.year\n\ngrp_month_year=month_year.groupby(['Month','Year'])['Transaction'].count().reset_index()\ngrp_month_year['Period'] = grp_month_year.Month.astype(str).str.cat(grp_month_year.Year.astype(str), sep=',')\n\n#plot graph for each month of the year 2016,2017\nfig,axis=plt.subplots(figsize=(8,4))\naxis=sns.barplot(data=grp_month_year,x='Period',y='Transaction')\naxis.set_xlabel('Month & Year',fontsize=14,color='r')\naxis.set_ylabel('# Of TRANSACTIONS',fontsize=14,color='r')\naxis.set_xticklabels(grp_month_year['Period'],color='b',rotation=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b013f359584717edd541a8858ee69fcc177f1d1"},"cell_type":"code","source":"#part of the day groupby\npart_of_day=raw_data.copy()\npart_of_day['timestamp'] = part_of_day.Date.astype(str).str.cat(part_of_day.Time.astype(str), sep=' ')\npart_of_day['timestamp']=pd.to_datetime(part_of_day['timestamp'])\n\npart_of_day['hour'] = part_of_day['timestamp'].dt.round('H').dt.hour\npart_of_day.drop(['Time','Date'],axis=1,inplace=True)\n\n#peak coffee hours\ncofy_hours=part_of_day[part_of_day['Item']=='Coffee']\ncofy_hours=cofy_hours.groupby('hour')['Item'].count()\ncofy_hours=cofy_hours.reset_index()\n\nfig,ax=plt.subplots(figsize=(10,6))\nax=sns.barplot(data=cofy_hours,x='hour',y='Item')\nax.set_xlabel('Hours Of The Day',fontsize=16,color='r')\nax.set_ylabel('No of Times Coffee is Sold',fontsize=16,color='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3916633cc38df3d60c3d81427c9f6256b1bcdb8"},"cell_type":"code","source":"#peak Bread hours\nbread_hours=part_of_day[part_of_day['Item']=='Bread']\nbread_hours=bread_hours.groupby('hour')['Item'].count()\nbread_hours=bread_hours.reset_index()\n\nfig,ax=plt.subplots(figsize=(10,6))\nax=sns.barplot(data=bread_hours,x='hour',y='Item')\nax.set_xlabel('Hours Of The Day',fontsize=16,color='r')\nax.set_ylabel('No of Times Bread is Sold',fontsize=16,color='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d75a3e7229478db6b6784e12d559059d92d8b7b"},"cell_type":"code","source":"#peak Cake hours\ncake_hours=part_of_day.loc[(part_of_day['Item']=='Cake') | (part_of_day['Item']=='Pastry')]\ncake_hours=cake_hours.groupby('hour')['Item'].count()\ncake_hours=cake_hours.reset_index()\n\nfig,ax=plt.subplots(figsize=(10,6))\nax=sns.barplot(data=cake_hours,x='hour',y='Item')\nax.set_xlabel('Hours Of The Day',fontsize=16,color='r')\nax.set_ylabel('No of Times Cake is Sold',fontsize=16,color='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"959c328426ab2caa73dde6706fb50210b079a26c"},"cell_type":"code","source":"#peak tea hours\ntea_hours=part_of_day.loc[(part_of_day['Item']=='Tea')]\ntea_hours=tea_hours.groupby('hour')['Item'].count()\ntea_hours=tea_hours.reset_index()\n\nfig,ax=plt.subplots(figsize=(9,6))\nax=sns.barplot(data=tea_hours,x='hour',y='Item')\nax.set_xlabel('Hours Of The Day',fontsize=16,color='r')\nax.set_ylabel('No of Times Tea is Sold',fontsize=16,color='r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"372e4bd19d0e465813a240a92dce0dbf4fb97a28"},"cell_type":"code","source":"#PART 2 Pareto analysis","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e3e2a6d81f9c9e5d0c3f8b66199eb6f864d5938"},"cell_type":"code","source":"pareto_data=raw_data.copy()\ntotal_transc=len(pareto_data)\n\neighty_percent_transc=(total_transc/100)*80\ntotal_unique_items=pareto_data.Item.unique()\ntwenty_percent_top_items=(len(total_unique_items)/100)*20\ntotal_transc_top_20_Items=no_of_item_transc.head(int(np.round(twenty_percent_top_items)))\ntotal_transc_top_20_Items=total_transc_top_20_Items.Transaction.sum()\n\nprint('80% of Total Transcation is :', eighty_percent_transc,'\\n' )\nprint('Total Transcation Of 20% Top Items is :', total_transc_top_20_Items,'\\n' )\nprint ('So, we can say this almost complies to 80-20% pareto rule')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b37fdbf6151a1d37de22c405bb6d92a529a3273"},"cell_type":"code","source":"#PART 3, MACHINE LEARNING\n#Market Basket Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0c88939e45b62adbe5606edd5cae4ffb95a7faa"},"cell_type":"code","source":"\"\"\"\nTheory of Apriori Algorithm\nThere are three major components of Apriori algorithm:\n\nSupport\nConfidence\nLift\nWe will explain these three concepts with the help of an example.\n\nSuppose we have a record of 1 thousand customer transactions, and we want to find the Support, Confidence, and Lift for two items e.g. burgers and ketchup.\nOut of one thousand transactions, 100 contain ketchup while 150 contain a burger. Out of 150 transactions where a burger is purchased, 50 transactions contain ketchup as well.\nUsing this data, we want to find the support, confidence, and lift.\n\nSupport\nSupport refers to the default popularity of an item and can be calculated by finding number of transactions containing a particular item divided by total number of transactions.\nSuppose we want to find support for item B. This can be calculated as:\n\nSupport(B) = (Transactions containing (B))/(Total Transactions)  \nFor instance if out of 1000 transactions, 100 transactions contain Ketchup then the support for item Ketchup can be calculated as:\n\nSupport(Ketchup) = (Transactions containingKetchup)/(Total Transactions)\nSupport(Ketchup) = 100/1000  \n                 = 10%\n                 \nConfidence\nConfidence refers to the likelihood that an item B is also bought if item A is bought. It can be calculated by finding the number of transactions where A and B are bought together,\ndivided by total number of transactions where A is bought. Mathematically, it can be represented as:\n\nConfidence(A→B) = (Transactions containing both (A and B))/(Transactions containing A)  \nComing back to our problem, we had 50 transactions where Burger and Ketchup were bought together. While in 150 transactions, burgers are bought.\nThen we can find likelihood of buying ketchup when a burger is bought can be represented as confidence of Burger -> Ketchup and can be mathematically written as:\n\nConfidence(Burger→Ketchup) = (Transactions containing both (Burger and Ketchup))/(Transactions containing A)\nConfidence(Burger→Ketchup) = 50/150  \n                           = 33.3%\nYou may notice that this is similar to what you'd see in the Naive Bayes Algorithm, however, the two algorithms are meant for different types of problems.\n\nLift\nLift(A -> B) refers to the increase in the ratio of sale of B when A is sold. Lift(A –> B) can be calculated by dividing Confidence(A -> B) divided by Support(B).\nMathematically it can be represented as:\n\nLift(A→B) = (Confidence (A→B))/(Support (B))  \nComing back to our Burger and Ketchup problem, the Lift(Burger -> Ketchup) can be calculated as:\n\nLift(Burger→Ketchup) = (Confidence (Burger→Ketchup))/(Support (Ketchup))\nLift(Burger→Ketchup) = 33.3/10  \n                     = 3.33\n\nLift basically tells us that the likelihood of buying a Burger and Ketchup together is 3.33 times more than the likelihood of just buying the ketchup.\nA Lift of 1 means there is no association between products A and B. Lift of greater than 1 means products A and B are more likely to be bought together.\nFinally, Lift of less than 1 refers to the case where two products are unlikely to be bought together.\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"951dc403dc8dddc51ec70b9c4fd76eb703836e4e"},"cell_type":"code","source":"#from apyori import apriori \nfrom mlxtend.frequent_patterns import association_rules\nfrom mlxtend.frequent_patterns import apriori\n\n#preparing the data for applying apriori\n# filling all the null vaues with zero and later unstacking them to fed to apriori to find CONFIDENCE, SUPPORT and LIFT\napr_data=raw_data.copy()\n\ngrp_apr_data = apr_data.groupby(['Transaction', 'Item'])['Item'].count().unstack().reset_index().fillna(0).set_index('Transaction')\ndef encode_units(x):\n    if x <= 0:\n        return 0\n    if x >= 1:\n        return 1\nhot_encoded_df = grp_apr_data.applymap(encode_units)\n\n# applying support with min value of 5%\nfrequent_itemsets =apriori(hot_encoded_df, min_support=0.05,use_colnames=True)\nprint('Items with min 5% support are:','\\n', frequent_itemsets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6210e44355cdf8f1f1120c200befa48fd360910"},"cell_type":"code","source":"# applying lift to items which have min value of support of 5% \nlift_items = association_rules(frequent_itemsets, metric='lift', min_threshold=0.5)\nprint('most sold pair of items are:','\\n',lift_items[['antecedents','consequents','consequent support','confidence','lift']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d93971f77b92c70204514f66b227e1fb484c747"},"cell_type":"code","source":"\"\"\"\nPlease give your feedback and it encourages me\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1726990f7ef809877e99a5289b848f165369271"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}