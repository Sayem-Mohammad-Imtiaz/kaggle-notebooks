{"cells":[{"metadata":{"_uuid":"bac5df46-fda6-43c1-9e9d-a19c7fcdd579","_cell_guid":"70bab2a0-fdc7-4a27-9c88-38fe78d09f10","trusted":true},"cell_type":"code","source":"import pandas as pd  # To read data\nimport numpy as np # To calculate data\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Less is more. This is my approach to everything, and data science projects are no exception. I like clean, simple yet meaningful insights, which are straight to the point. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1.0 Importing the Data\n\n### Load the csv file from directory to the dataframe\n \n- Use the .head() function to catch a snapshot of the top end of the dataframe","execution_count":null},{"metadata":{"_uuid":"2ed42f4d-5660-41fe-8b9c-aea088e487d5","_cell_guid":"08b22774-a6a3-4169-90c8-0fe2f00550b5","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/house-prices-dataset/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/house-prices-dataset/test.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1eb5a440-c3a5-4ca4-845b-3e161b387c3f","_cell_guid":"9c1dbe5d-f670-4fb9-ae3c-78cb594c0e5e","trusted":true},"cell_type":"markdown","source":"- #### Display the dataframe statistical summary for better overview of thats low or high. Using the function - .describe()\n","execution_count":null},{"metadata":{"_uuid":"152e90bd-1973-4c4f-bddc-fdf29e1cf261","_cell_guid":"e1e6bdef-ebd0-4ba4-b5fc-a85e865c3cbd","trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Lets check the data types for each column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.0 Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Lets take a quick look at a correlation heatmap/matrix and determine the most relevant data to work with, instead of just going through the entire 81 column list, looking which columns to drop.","execution_count":null},{"metadata":{"_uuid":"8bcaccee-e42b-48b2-a126-8fa60c403937","_cell_guid":"546a9033-e26f-45a8-a0aa-810db33d2535","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom matplotlib import pyplot as plt\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 7,7 \nimport seaborn as sns\nimport numpy as np\nsns.set(color_codes=True, font_scale=1.2)\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nimport heatmapk\nfrom heatmapk import heatmap, corrplot\nplt.figure(figsize=(10, 10))\ncorrplot(df.corr(), size_scale=300);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can determine from this heatmap, the biggest variables that the Sale Prices is corelated to, which are OverallQual, GrLivArea, GarageCars, GarageArea, TotalBsmtSF and 1stFlrSF. Atleast from what i cant make out real quick. \n- Lets confirm the exact correlation coeficient between these variables and our \"SalePrice\" value. Lets narrow down the exact variables via their coeficient.","execution_count":null},{"metadata":{"_uuid":"0914ed51-41db-4b05-91f9-5588008fcb51","_cell_guid":"dda5d0c6-10ee-426c-b5cf-271ec01156df","trusted":true},"cell_type":"code","source":"corr_matrix=df.corr()\ncorr_matrix['SalePrice'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Lets select a new dataframe just with these main above 60%ish correlation having percentile variables, so it will be easier to process.","execution_count":null},{"metadata":{"_uuid":"041373bd-f1c4-4a68-abc4-446f0af33e61","_cell_guid":"888141cb-8cbe-4e6d-851f-49d7df67bb4c","trusted":true},"cell_type":"code","source":"df1 = df[['OverallQual', 'GrLivArea', 'GarageCars', \n          'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'SalePrice']].copy()\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Much easier on the eyes.\n- Now lets find out if and how many missing values we are dealing with, within this new data frame AKA our features list. \n- We will do that by applying the - .isnull() method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Good stuff. No need to drop or insert any means within any of our columns since there are no missing values. Lets move on.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3.0 Explolatory Data Anlysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Lets put up some histograms to observe any crazy skewsness or abnormalities from our variables within an eyes view.","execution_count":null},{"metadata":{"_uuid":"d7bd6426-5411-4e9b-bc72-26c80505b866","_cell_guid":"03a63780-9fd5-41c2-a2d1-8f482b3bb378","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\ndf1.hist(bins=50, figsize=(20,15))\nplt.savefig(\"attribute_histogram_plots\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- All seems to be doing fine so far. Lets use seaborn regression functions to plot our Linear Regression of each attribute in comparison to our Target value - Sale Price","execution_count":null},{"metadata":{"_uuid":"0b919ccf-d3c5-468b-9e40-e9e6bbe9b565","_cell_guid":"c9b98c7f-04b9-4dfb-8c64-df9bacd72aec","trusted":true},"cell_type":"code","source":"\nsns.regplot(x='1stFlrSF', y='SalePrice', data=df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[['1stFlrSF', 'SalePrice']].corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4abd275-4d8f-43b5-abda-86c45aa239d4","_cell_guid":"3c85a0c0-2c0d-4d51-8d92-4cd6f0050b61","trusted":true},"cell_type":"code","source":"sns.regplot(x='GarageArea', y='SalePrice', data=df1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[['GarageArea','SalePrice']].corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"496f5556-aa38-4571-8d50-fc4c1f9859fe","_cell_guid":"49805f56-73d9-4432-9bad-d95b44cc6459","trusted":true},"cell_type":"code","source":"sns.regplot(x='GarageCars', y='SalePrice', data=df1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[['GarageCars', 'SalePrice']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x='GrLivArea', y='SalePrice', data=df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[['GrLivArea', 'SalePrice']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x='OverallQual', y='SalePrice', data=df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[['OverallQual', 'SalePrice']].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x='TotalBsmtSF', y='SalePrice', data=df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1[['TotalBsmtSF', 'SalePrice']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets calculate the P-value. It is the probability value between two variables is statistically significant. For example:\n- if we choose significance level of 0.05, that means that we are 95% confident that the correlation between the variables is significant.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"By convention, when the:\n\n- p-value is  < 0.001: we say there is strong evidence that the correlation is significant.\n- the p-value is < 0.05: there is moderate evidence that the correlation is significant.\n- the p-value is  < 0.1: there is weak evidence that the correlation is significant.\n- the p-value is  > 0.1: there is no evidence that the correlation is significant.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ### Lets calculate the Pearson Correlation Coefficient and P-value of OverallQual and SalePrice:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value=stats.pearsonr(df1['OverallQual'], df1['SalePrice'])\nprint('The Pearson Correlation Coefficient is ', pearson_coef, 'with a P-value of P =', p_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion:\n\n- Since the p-value is < 0.001, the correlation between OverallQual and SalePrice is statistically _SIGNIFICANT_, and the coefficient of ~ 0.79 shows that the relationship is _QUITE STRONG_.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- ### Lets calculate the Pearson Correlation Coefficient and P-value of GrLivAea and SalePrice:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value=stats.pearsonr(df1['GrLivArea'], df1['SalePrice'])\nprint('The Pearson Correlation Coefficient is ', pearson_coef, 'with a P-value of P=', p_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion:\n\n- Since the p-value is < 0.001, the correlation between GrLivArea and SalePrice is statistically _SIGNIFICANT_, and the coefficient of ~ 0.70 shows that the relationship is _MODERATELY STRONG_.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- ### Lets calculate the Pearson Correlation Coefficient and P-value of GarageCars and SalePrice:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value=stats.pearsonr(df1['GarageCars'], df1['SalePrice'])\nprint('The Pearson Correlation Coefficient is ', pearson_coef, 'with a P-value of P=', p_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion:\n\n- Since the p-value is < 0.001, the correlation between GarageCars and SalePrice is statistically _SIGNIFICANT_, and the coefficient of ~ 0.64 shows that the relationship is _MODERATELY STRONG_.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- ### Lets calculate the Pearson Correlation Coefficient and P-value of GarageArea and SalePrice:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value=stats.pearsonr(df1['GarageArea'], df1['SalePrice'])\nprint('The Pearson Correlation Coefficient is ', pearson_coef, 'with a P-value of P=', p_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion:\n\n- Since the p-value is < 0.001, the correlation between GarageArea and SalePrice is statistically _SIGNIFICANT_, and the coefficient of ~ 0.62 shows that the relationship is _MODERATELY STRONG_.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- ### Lets calculate the Pearson Correlation Coefficient and P-value of TotalBsmtSF and SalePrice:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value=stats.pearsonr(df1['TotalBsmtSF'], df1['SalePrice'])\nprint('The Pearson Correlation Coefficient is', pearson_coef, ' with a P-value of P=', p_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion:\n\n- Since the p-value is < 0.001, the correlation between TotalBsmtSF and SalePrice is statistically _SIGNIFICANT_, and the coefficient of ~ 0.61 shows that the relationship is _MODERATE_.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- ### Lets calculate the Pearson Correlation Coefficient and P-value of 1StFlrSF and SalePrice:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value=stats.pearsonr(df1['1stFlrSF'], df1['SalePrice'])\nprint('The Pearson Correlation Coefficient is', pearson_coef, 'with a P-value of P=', p_value)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion:\n\n- Since the p-value is < 0.001, the correlation between 1stFLrSF and SalePrice is statistically _SIGNIFICANT_, and the coefficient of ~ 0.60 shows that the relationship is _MODERATE_.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"   - ### I  summary, we came to a conclusion what our data looks like and which variables are important to take into account when predicting the house price. We have narrowed it down to the following 6 variables:\n - OverallQual\n - GrLivArea\n - GarageCars\n - GarageArea\n - TotalBsmtSF\n - 1stFlrSF\n\n\nOut of all of them, variable  _'OverallQual'_ has the Strongest positive and Significant relationship to our target - 'SalePrice'.\n\n- ### As we will be moving into building our machine learning models to automate our analysis, feeding the model with variables we selected in conclusion, which meaningfully affect our target variable will improve our model's prediction performance.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4.0 Model Development","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### In this part of the project, we will develop several models that will predict the price of the house using the variables or features above. This is just an estimate obviously, but should give us an objective idea of how much the house should cost.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Setup\n\n- Import libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Linear Regression and Multiple Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- In our first example of Data Model we will be using is _Simple Linear Regression_\n\nSimple Linear Regression is a method to help us understand the relationship between two variables:\n- The predictor/independent variable(X) - OverallQual\n- The response/dependent variable (that we want to predict), (Y) - SalePrice","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Creating train and test dataset\n\n- Lets split the dataset into train and test sets, 80% of the entire data for training and the 20% for testing. LEts create a mask to select random rows using np.random.rand() function:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"msk = np.random.rand(len(df)) < 0.8\ntrain =df1[msk]\ntest =df1[~msk]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Train data distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(train.OverallQual, train.SalePrice, color='green')\nplt.xlabel('Overall Quality')\nplt.ylabel('Sales Price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\n\nregr = linear_model.LinearRegression()\ntrain_x = np.asanyarray(train[['OverallQual']])\ntrain_y = np.asanyarray(train[['SalePrice']])\nregr.fit (train_x, train_y)\n\n# The coefficients\nprint('Coefficients: ', regr.coef_)\nprint('Intercept: ', regr.intercept_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot outputs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(train.OverallQual,  train.SalePrice, color ='green')\nplt.plot(train_x, regr.coef_[0][0] * train_x + regr.intercept_[0], '-r')\nplt.xlabel('Overall Quality')\nplt.ylabel('Sales Price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\n\ntest_x = np.asanyarray(test[['OverallQual']])\ntest_y = np.asanyarray(test[['SalePrice']])\ntest_y_hat = regr.predict(test_x)\n\nprint('Mean Absolute Error: %.2f' % np.mean(np.absolute(test_y_hat-test_y)))\nprint('Residual sum of squares (MSE): %.2f' % np.mean((test_y_hat - test_y) **2))\nprint('R2-score: of %.2f' % r2_score(test_y_hat, test_y))     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### - Lets fit the model with the feature 'OverallQual'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df1[['OverallQual']]\nY=df1['SalePrice']\nlm=LinearRegression()\nlm\nlm.fit(X, Y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Output a prediction:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Yhat=lm.predict(X)\nYhat[0:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- What is the value of intercept (a)?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.intercept_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- What is the value of the Slope (b)?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What is the final estimated linear model we get?\n\n- Yhat = a +bX\n\nAnd putting in our actual values we get:\n\nPrice = -96206 + 45435 * OverallQual","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Convert the output to a readable Data Frame:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Yhat_df1 = pd.DataFrame(Yhat)\nYhat_df1.columns = ['SalePrice']\nYhat_df1.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Add 'ID' Column:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Id = df_test[['Id']]\nYhat = pd.concat([Id,Yhat_df1],axis=1)\nYhat.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multiple Linear Regression","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- #### From the previous analysis, we know that we have 6 good predictor variables. Lets use them to create a multiple linear regression model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Lets fit linear regression model to predict 'SalePrice' using list of the main features we analyzed earlier.\n - Lets convert this list of features to a 'list' right away.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = df1[['OverallQual', 'GrLivArea', 'GarageCars', \n          'GarageArea', 'TotalBsmtSF', '1stFlrSF']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Fit the linear model using the above listed variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.fit(Z, df1['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Value of INTERCEPT (a)?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.intercept_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Values of the coefficients (b1,b2,b3,b4,b5,b6)?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lm.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- What's the stimated linear model we get?\n\nLinear function structure as follows:\n\n    Yhat = a +b1X1 + b2X2 + b3X3 + b4X4 + b5X5 + b6X6\n    \n- Price = -102650 + 2.39970394e+04*OverallQual +4.31228864e+01*GrLiveArea + 1.45151932e+04*GarageCars +1.56639341e+01*GarageArea + 2.43907676e+01* TotalBsmtSF + 1.11859135e+01* 1stFlrSF\n       \n       ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Fit the model with the features data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_output = lm.predict(Z)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Convert the output to a readable Data Frame:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_output_df1 = pd.DataFrame(y_output)\ny_output_df1.columns = ['SalePrice']\ny_output_df1.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Add 'ID' Column:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Id = df_test[['Id']]\nY_output = pd.concat([Id,y_output_df1],axis=1)\nY_output.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now lets calculate the R^2 using the features compared to the 'SalePrice'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x2 = Z\ny2 = df1['SalePrice']\nlm.fit(x2, y2)\nlm.score(x2,y2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Create a list of tuples: first element in the tuple contains the name of the estimator:\n- 'scale', 'polynomial', 'model'\n\n#### The second element contains the model constructor\n- StandardScaler()\n- PolynomialFeatures(include_bias=False)\n- LinearRegression()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Input=[('scale', StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)),\n      ('model', LinearRegression())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Use the list to create a pipeline object, predit the'SalePrice', fit the object using features in the features list, then fit the model and calculare the R^2","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## To be fair, all of this seems like a lot of work for simple prediction of house prices, and there should be easier more efficient way to calculate this. Lets check out XGboost.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nimport numpy as np\n\n\ntest = pd.read_csv(\"/kaggle/input/house-prices-dataset/test.csv\")\ndf.head()\n#Opening our file with the training data in\ntrain = pd.read_csv(\"/kaggle/input/house-prices-dataset/train.csv\")\n\n#We are trying to predict the sale price column\ntarget = train.SalePrice\n\n#Get rid of the answer and anything thats not an object\ntrain = train.drop(['SalePrice'],axis=1).select_dtypes(exclude=['object'])\n\n#Split the data into test and validation\ntrain_X, test_X, train_y, test_y = train_test_split(train,target,test_size=0.25)\n\n#Impute all the NaNs\nmy_imputer = SimpleImputer()\ntrain_X = my_imputer.fit_transform(train_X)\ntest_X = my_imputer.fit_transform(test_X)\n\n#Simplist XGBRegressor\n#my_model = XGBRegressor()\n#my_model.fit(train_X, train_y)\n\nmy_model = XGBRegressor(n_estimators=300, learning_rate=0.08)\nmy_model.fit(train_X, train_y, early_stopping_rounds=4, \n             eval_set=[(test_X, test_y)], verbose=False)\n\n\n#Make predictions\npredictions = my_model.predict(test_X)\n\nprint(\"Mean absolute error = \" + str(mean_absolute_error(predictions,test_y)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Similary evaluate a model and make predictions just as we would do in scikit-learn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions\n\npredictions = my_model.predict(test_X)\n\nfrom sklearn.metrics import mean_absolute_error\nprint('Mean Absolute Error : ' + str(mean_absolute_error(predictions, test_y)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### After all the hard work, in the beginning, since it is my first ever Machine Learning excercise, i came across XGBoost. This should be prerequisite for every beginner. This not only shortened the time, was efficient, easier on the eyes, but also powerful and more accurate predictions. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}