{"cells":[{"metadata":{},"cell_type":"markdown","source":"code from sklearn \"Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\" example"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport datetime\ndate_depart=datetime.datetime.now()\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport glob\nimport random\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\nimport nltk\nfrom nltk import ngrams\nimport joblib\nfrom dask.distributed import Client\n\nimport spacy\nimport sklearn\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,TfidfTransformer\nfrom sklearn.decomposition import  PCA,NMF, LatentDirichletAllocation\nfrom sklearn.decomposition import  IncrementalPCA\n!python -m spacy download fr_core_news_md\n!python -m spacy download fr_core_news_sm\nimport fr_core_news_md\nnlp_fr = fr_core_news_md.load()\nimport  cloudpickle\nimport joblib\nfrom dask.distributed import Client","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    client = Client('127.0.0.1:8786',timeout=5,set_as_default=True,processes=False)\nexcept OSError as e:\n#     print(e)\n    client =Client(set_as_default=True,n_workers=2,threads_per_worker=4,dashboard_address=None,processes=False)\nclient","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import requests\nimport spacy.lang.fr\nfrom stop_words import get_stop_words\nstopwords_fr_set=set(nltk.corpus.stopwords.words('french'))\nstopwords_fr_set.update(get_stop_words('fr'))\nstopwords_fr_set.update(spacy.lang.fr.stop_words.STOP_WORDS)\nstopwords_fr_set.update([\"c'est\",\"j'ai\",\"n'est\",\"n'ait\",\"ca\",\"ça\",\"sais\",\"jamais\",\"chose\",\"ex\",\"'quelqu'\",'quelqu',\"br\"])\nstopwords_fr_set.update((str(i) for i in range(30)))\nstopwords_fr_set.update([\"faut\", \"arrêter\", \"faisons\", \"faite\", \"faits\",'oui' ,\"www\",\"https\",\"http\",\"ect\"])\nstopwords_fr_set.update(requests.get(\"https://raw.githubusercontent.com/stopwords-iso/stopwords-fr/master/stopwords-fr.json\").json())\nstopwords_fr_set.update(str(i) for i in range(100))\nstopwords_fr_set.update(str(i) for i in range(1980,2025))\nstopwords_fr_set=list(stopwords_fr_set)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_samples = 512*1024\nvectorsamples=int(1e6)\nn_features = 3000\nn_components = 300\nn_top_words = 15\nngram_range=(1,3)\nmax_df=0.25\nmin_df=20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df,\n                                   max_features=n_features,\n                                   stop_words=stopwords_fr_set,\n                                   ngram_range=ngram_range,\n                                   dtype=np.float32\n                                   )\ntf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df,\n                                max_features=n_features,\n                                stop_words=stopwords_fr_set,\n                                ngram_range=ngram_range,\n                                  dtype=np.uint16\n                               )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#load texts"},{"metadata":{"trusted":true},"cell_type":"code","source":"texts=[]\ntexts_byfiles=dict()\nfilelist=glob.glob(\"../input/**/*.csv*\", recursive=True)\nrandom.shuffle(filelist)\nfor f in filelist:\n    print (f)\n    df=pd.read_csv(f,low_memory=False)\n    dftext=[]\n    texts_byfiles[\"f\"]=dftext\n    for n,s in df.items():\n        for e in s:\n            if isinstance(e,str):\n                if len(e.split())>2 :\n                    dftext.append(e)\n    texts+=dftext\n\ntexts=list(set(texts))\nrandom.shuffle(texts)\ntextes_base=texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"texts=random.sample(texts,n_samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#tfidf"},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vectorizer.fit(random.sample(textes_base,vectorsamples))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntfidf = tfidf_vectorizer.transform(texts)\ntfidf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rr = dict(zip(tfidf_vectorizer.get_feature_names(),  tfidf_vectorizer.idf_))\n\ntoken_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\ndel rr\ntoken_weight.columns=('token','weight')\ntoken_weight = token_weight.sort_values(by='weight', ascending=False)\ntoken_weight.reset_index(drop=True,inplace=True) \n\n\nsns.barplot(x='token', y='weight', data=token_weight.iloc[:60], )            \nplt.title(\"Inverse Document Frequency(idf) per token\")\nfig=plt.gcf()\nfig.set_size_inches(25,15)\nax=fig.axes[0]\nplt.yscale(\"log\")\nax.tick_params(axis='x',labelrotation=90 )\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,12))\ntoken_weight.plot()\n# plt.yscale(\"log\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_weight","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#mmf"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nwith joblib.parallel_backend('dask'):\n    nmf = NMF(n_components=n_components,\n              alpha=.1, l1_ratio=.5,\n               tol=0.005,\n              init=\"nndsvd\",\n              max_iter =20,\n              shuffle =True,\n              verbose=True)\nnmf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_nmf=nmf.fit_transform(tfidf)\ntfidf_nmf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()\ndef get_top_words_list(model, feature_names, n_top_words):\n    topwords=[]\n    for topic_idx, topic in enumerate(model.components_):\n       \n        topwords.append([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n    return topwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\nprint_top_words(nmf, tfidf_feature_names, n_top_words)\nnmf_top_words_list=get_top_words_list(nmf, tfidf_feature_names, n_top_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_text=np.random.choice(n_samples,5)\n\nfor t,topics in zip([texts[i] for i in ind_text ],\n                    \n        tfidf_nmf[ind_text]):\n    print(t)\n    top_topics=np.argsort(topics)[-3:]\n    for n in top_topics:\n        print(f\"topic {n}: {', '.join(nmf_top_words_list[n][:6])}\")\n    print()\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#pca_nmf"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwith joblib.parallel_backend('dask'):\n    pca_nmf = IncrementalPCA(n_components=3, batch_size=200)\n\n\n    tfidf_nmf_pca=pca_nmf.fit_transform(tfidf_nmf)\n\n\nsns.scatterplot(x=\"pca1\",y=\"pca2\",hue=\"pca3\" ,data=pd.DataFrame(tfidf_nmf_pca[np.random.choice(len(tfidf_nmf_pca),9000)]\n                                                                ,columns=[\"pca1\",\"pca2\",\"pca3\"]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"norms=np.linalg.norm(pca_nmf.components_[0:2, :],axis=0)\npca2_n=norms>np.quantile(norms,0.03)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_nmf.components_[0:2, pca2_n].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnp.array(nmf_top_words_list)[pca2_n][:,:4]\n[str(l[:4]) for l in np.array(nmf_top_words_list)[pca2_n]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def myplot(score,coeff,labels=None):\n    plt.figure(figsize=(25,50))\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0/(xs.max() - xs.min())\n    scaley = 1.0/(ys.max() - ys.min())\n    plt.scatter(xs * scalex,ys * scaley, c = ys)\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)\n        if labels is None:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'g', ha = 'center', va = 'center')\n        else:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')\n    plt.xlim(-1,1)\n    plt.ylim(-1,1)\n    plt.xlabel(\"PC{}\".format(1))\n    plt.ylabel(\"PC{}\".format(2))\n    plt.grid()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#Call the function. Use only the 2 PCs.\nmyplot(tfidf_nmf_pca[:,0:2],np.transpose(pca_nmf.components_[0:2, pca2_n]),labels=[str(l[:4]) for l in np.array(nmf_top_words_list)[pca2_n]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#CountVectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf_vectorizer.fit(random.sample(textes_base,vectorsamples))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntf = tf_vectorizer.transform(texts)\ntf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.max())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#lda"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlda = LatentDirichletAllocation(n_components=n_components,\n                                max_iter=20,\n                                learning_method='online',\n                                learning_offset=50.,\n                              \n                                verbose =1,\n                                n_jobs =-1\n                               )\n\nwith joblib.parallel_backend('dask'):\n    lda.fit(tf)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"partial_batch=32*1024\nwith joblib.parallel_backend('dask'):\n    for i in range(0,len(textes_base),partial_batch):\n\n        batchmat=tf_vectorizer.transform(textes_base[i:i+partial_batch])\n        lda.partial_fit(batchmat)\n        if (datetime.datetime.now()-date_depart)>datetime.timedelta(hours=7,minutes=20):\n            break\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with joblib.parallel_backend('dask'):\n    tfidf_lda=lda.transform(tf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"\\nTopics in LDA model:\")\ntf_feature_names = tf_vectorizer.get_feature_names()\nlda_top_words_list=get_top_words_list(lda, tf_feature_names, n_top_words)\nprint_top_words(lda, tf_feature_names, n_top_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_best_topics=np.argsort(tfidf_lda.mean(axis=0))[-10:]\nfor n in range(6):\n    print(f\"topic {lda_best_topics[-n]}: {', '.join(lda_top_words_list[lda_best_topics[-n]][:15])}\")\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"topic_texts=np.argsort(tfidf_lda,axis=0)[:300]\nfor t in range(tfidf_lda.shape[1]):\n    for i in range(6):\n        nmax=topic_texts[-i,t]\n        print(tfidf_lda[nmax,t])\n        print(texts[nmax][:300])    \n    print(f\"topic {t}: {', '.join(lda_top_words_list[t][:15])}\")\n    print(\"***\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ind_text=np.random.choice(len(textes_base),10)\n\nfor i in ind_text:\n    t=textes_base[i]\n    vect=tf_vectorizer.transform([t])\n#     .toarray().flatten()\n    topics=lda.transform(vect)\n    \n                    \n        \n    \n    tf_vectorizer.transform(textes_base[i:i+partial_batch])\n    print(t+\"\\n\")\n    top_topics=np.argsort(topics.flatten())[-3:]\n    for n in top_topics:\n        print(f\"topic {n}: {', '.join(lda_top_words_list[n][:6])}\")\n    print(\"\\n\\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nipca = IncrementalPCA(n_components=3, batch_size=200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwith joblib.parallel_backend('dask'):\n    for i in range(0,len(textes_base),partial_batch):\n        batchmat=tf_vectorizer.transform(textes_base[i:i+partial_batch])\n        batchmat_lda=lda.transform(batchmat)\n        ipca.partial_fit(batchmat_lda)\n        print(i,end=\"\\r\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntfidf_lda_ipca=ipca.transform(tfidf_lda)\nsns.scatterplot(x=\"pca1\",y=\"pca2\",hue=\"pca3\" ,data=pd.DataFrame(tfidf_lda_ipca,columns=[\"pca1\",\"pca2\",\"pca3\"]))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}