{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# libraries we will use\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve,confusion_matrix,f1_score,precision_recall_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score,StratifiedShuffleSplit\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Reading data and taking a quick look at it"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndata_copy = data.copy()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outcome_values = data['Outcome'].value_counts()\noutcome_values.sort_index()\nplt.pie(outcome_values.values, labels=['Negative', 'Positive'], autopct='%1.1f%%')\nplt.title('Diabete result')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that there are enough examples of both classes and we will easily split data to train-test splits"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like there are no missing values and there are no categorical variables , that makes it easier to work with datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some columns have minimum value of 0, which means they can be missing values, even though there are no NaN value in dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (17,14))\ncorr = data.corr()\nsns.heatmap(corr, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From correlation matrix, we can see that there are no highly correlated features with outcome, but we can note glucose whose corelation with outcome is 0.47. There is also moderate correlation between pregnancies and age, which is understandable. It can also be seen that BloodPressure and SkinThicknes aren't correlated with Outcome.\nLet's look at the distribution of features:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(3,3,figsize = (18,16))\n\nfor i,feature in enumerate(data.columns):\n    sns.histplot(data[feature], ax=axes[i//3, i%3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are several columns that can't be 0. If your blood pressure or glucose level is 0, then you are dead. BMI also can't be 0 as it is weight/(height\\*height). Skin thickness can't be 0 too. Although insulin can be 0, it isn't likely that about half of women in this dataset to have 0 insulin. Let's compare insulin, glucose level and outcome:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.scatterplot(data=data, x = 'Glucose', y='Insulin', hue = 'Outcome')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am no medical expert, but I know that 0 insulin leads to type-I diabete and in this scatterplot we see that there are several patients with insulin level of 0, but they are not diagnosed with diabete, which means there is missing data. To be honest, I can't be sure that all those 0 level insulins are missing, nevertheless I will fill them. In the scatterplot we can also see that Outcome classes doesn't have the same distribution at least in Glucose-Insulin relation, so when we are imputing values, we have to be careful of class. Also let's look at how many zeros are in columns."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"missing_cols = ['Glucose', 'Insulin', 'SkinThickness', 'BloodPressure', 'BMI']\nmissing_counts = {}\ntotal_rows = data.shape[0]\nfor col in missing_cols:\n    count = (data[col] == 0).sum()\n    missing_counts[col] = count\n\nplt.figure(figsize=(13,10))\nax = sns.barplot(x=list(missing_counts.keys()),y=list(missing_counts.values()))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()/2,\n            height + 3,\n            '{:1.2f}%'.format(height*100/total_rows),\n            ha=\"center\") \nplt.title('Distribution of missing values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's mark 0 values as NaN to make things easier"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in missing_cols:\n    data.loc[data[col] == 0.0, [col]] = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(2,3,figsize = (15,10))\nfor i, col in enumerate(missing_cols):\n    sns.boxplot(y = data[col], x=[\"\"]*(data.shape[0]),hue = data['Outcome'], ax = ax[i//3][i%3])\nax[1][2].set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we clearly see that, we can't just impute median value to missing columns. We should take Outcome into account"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor col in missing_cols:\n    \n    positive_median = data[data['Outcome'] == 1][col].median()\n    negative_median = data[data['Outcome'] == 0][col].median()\n    \n    data.loc[(data['Outcome']==0)&(data[col].isna()),col] = negative_median\n    data.loc[(data['Outcome']==1)&(data[col].isna()),col] = positive_median","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at age, pregnancy and BMI with regard to outcome. Even though they are continuous variables, it doesn't make sense to use them as it is."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(3,1,figsize=(15,15))\n\nsns.histplot(x=data['BMI'],hue=data['Outcome'],multiple='stack',ax=ax[0])\nsns.countplot(x=data['Age'],hue=data['Outcome'],ax=ax[1])\nsns.countplot(x=data['Pregnancies'],hue=data['Outcome'],ax=ax[2])\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Older people's chances of having diabete is slightly higher than younger people's, but it's not that clear from barplot (look at ages 37-38-39). Let's divide these features to ranges and treat them as categoric variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['CategoricalAge'] = pd.qcut(data['Age'], q = 5)\ndata['CategoricalBMI'] = pd.qcut(data['BMI'], q=5)\ndata['CategoricalPregnancies'] = pd.qcut(data['Pregnancies'], q=5)\n\n\nfig, ax = plt.subplots(3,1,figsize=(15,15))\n\nsns.countplot(x=data['CategoricalBMI'],hue=data['Outcome'],ax=ax[0])\nsns.countplot(x=data['CategoricalAge'],hue=data['Outcome'],ax=ax[1])\nsns.countplot(x=data['CategoricalPregnancies'],hue=data['Outcome'],ax=ax[2])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It now looks much better, let's one-hot encode these categorical columns and drop previous features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data)\ndata.drop(['Age', 'Pregnancies', 'BMI'], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_variables = ['Glucose','BloodPressure','SkinThickness','Insulin','DiabetesPedigreeFunction']\nplt.figure(figsize = (17,14))\nsns.pairplot(data[continuous_variables+['Outcome']],hue='Outcome')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are outlier in data and we will deal with them after checking accuracies"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata[continuous_variables] = pd.DataFrame(scaler.fit_transform(data[continuous_variables]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are not same amount of classes, we should use StratifiedShuffleSplit to keep ratio in test/train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_x = data.drop(['Outcome'], axis = 1)\ndata_y = data['Outcome']\n\n\nsss = StratifiedShuffleSplit(test_size=0.3,n_splits=1,random_state=4321)\ntrain_val_index, test_index = next(sss.split(data_x, data_y))\nX_train, X_test = data_x.iloc[train_val_index, :], data_x.iloc[test_index]\ny_train, y_test = data_y[train_val_index], data_y[test_index]\n\nX_train.reset_index(drop = True, inplace=True)\ny_train.reset_index(drop=True,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = {}\nmodels = [LogisticRegression(max_iter=10000),KNeighborsClassifier(),RandomForestClassifier(random_state=42),GradientBoostingClassifier(random_state=42)]\nfor model in models:\n    cv_scores = cross_val_score(model, X_train,y_train)\n    estimator = model.__class__.__name__\n    scores[estimator] = np.mean(cv_scores)*100\nax = sns.barplot(y=list(scores.keys()),x=list(scores.values()),orient='h')\nfor p in ax.patches:\n    width = p.get_width()\n    ax.text( width/2,\n            p.get_y()+0.5,\n            '{:1.2f}%'.format(width))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RandomForestClassifier look okay, let's fine tune it"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'n_estimators':np.arange(100,1001,100),\n         'max_depth':np.arange(2,41,2)}\n    \nrfc_cv = GridSearchCV(RandomForestClassifier(),param_grid=params, cv = 5, verbose = 2,n_jobs=5,scoring = 'f1').fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc_model = rfc_cv.best_estimator_\nrfc_cv.best_score_,rfc_cv.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scoring(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    rfc_mat = confusion_matrix(y_test, y_pred)\n    print('Recall: ', recall_score(y_test, y_pred))\n    print('Precision: ', precision_score(y_test, y_pred))\n    print('Roc-auc score: ',roc_auc_score(y_test,y_pred))\n    print('F1 score: ', f1_score(y_test, y_pred))\n    sns.heatmap(rfc_mat,annot=True,fmt='1')\n    plt.xlabel('True classes')\n    plt.ylabel('Predictions')\n    plt.show()\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scoring(rfc_model, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is sensitive data and we must increase recall to predict as much diabetic patients as possible, let's look at precision and recall and see if we can choose ."},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_proba = rfc_model.predict_proba(X_test)\nprecision, recall, thresholds = precision_recall_curve(y_test, predict_proba[:,1])\nthresholds=np.concatenate([thresholds, [1.0]])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax =   plt.subplots(1,2,figsize = (15,5))\nax[0].grid()\nax[0].plot(recall, precision)\nax[0].set_xlabel('Recall')\nax[0].set_ylabel('Precision')\nax[0].plot([0.9, 0.9], [min(precision),1],'g--')\nax[1].grid()\nax[1].plot(thresholds, precision,'r',label = 'Precision')\nax[1].plot(thresholds, recall, label = 'Recall')\nax[1].set_xlabel('Thresholds')\nax[1].legend()\nax[1].plot([min(thresholds), 1], [0.9,0.9],'g--')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like we can get about 0.85 precision with 0.9 recall, We can find the threshold where recall is greater than 0.9, but also precision isn't too low."},{"metadata":{"trusted":true},"cell_type":"code","source":"ind = np.argmin(recall >= 0.90)-1\nthreshold = thresholds[ind]\nprint('Threshold is {}'.format(threshold))\ny_pred = (predict_proba[:,1]>threshold).astype(np.int32)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot = True,fmt = '1')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('AUC:',roc_auc_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can get 90% without handling outliers, so I will leave them as they are. This is my first time sharing notebook here and I have mistakes of course. Please do tell me what can I improve. If you like my work, consider upvoting "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}