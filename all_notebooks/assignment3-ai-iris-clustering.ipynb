{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install SimpSOM","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport SimpSOM as sps\nfrom imageio import imwrite\nimport matplotlib.pyplot as plot\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nfrom matplotlib.cbook import get_sample_data\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom sklearn.preprocessing import StandardScaler\nfrom PIL import Image, ImageChops\ndata=pd.read_csv('../input/iris-flower-dataset/IRIS.csv')\n#first we check the data \ndata.head();\ndata.describe();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we plot the data into some bar graphs that show the correlation between the different attributes of the irises \n#1-->a barplot showing the ris data\nplot.figure(figsize=(12,6))\nplot.title('Barplot for Iris Data', fontsize=10)\nsb.barplot(data=data)\n#2--> a barplot showing correltaions between the different attributes of the iris data \nlabel = ['SEPAL LENGTH', 'SEPAL WIDTH', 'PETAL LENGTH', 'PETAL WIDTH']\ncolumn = data.columns[:-1].tolist()\nplot.figure(figsize=(12,6))\nplot.title('Barplot for {}'.format(label[0]), fontsize=10)\nsb.barplot(x=column[0], y='species', data=data, color='blue')\nplot.xlabel(label[0], fontsize=12)\nplot.ylabel('SPECIES', fontsize=12)\n#3-->\nplot.figure(figsize=(12,6))\nplot.title('Barplot for {}'.format(label[1]), fontsize=10)\nsb.barplot(x=column[1], y='species', data=data, color='yellow')\nplot.xlabel(label[1], fontsize=12)\nplot.ylabel('SPECIES', fontsize=12)\n#4-->\nplot.figure(figsize=(12,6))\nplot.title('Barplot for {}'.format(label[2]), fontsize=10)\nsb.barplot(y=column[2], x='species', data=data, color='red')\nplot.xlabel(label[2], fontsize=12)\nplot.ylabel('SPECIES', fontsize=12)\n#5-->\nplot.figure(figsize=(12,6))\nplot.title('Barplot for {}'.format(label[3]), fontsize=10)\nsb.barplot(y=column[3], x='species', data=data, color='lightblue')\nplot.xlabel(label[3], fontsize=12)\nplot.ylabel('SPECIES', fontsize=12)\nplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#printing the count plot for the species of the flowers \n#according to the data \nplot.figure(figsize=(10,5))\nplot.title('Countplot for the Data', fontsize=15)\nsb.countplot(x=data.species);\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now we need to do the histogram for the data, which shows a correlation between the attributes values and the count of each value \nplot.figure(figsize=(12,6))\nplot.title('Histplot for the IRIS Data', fontsize=15)\nsb.histplot(data)\nplot.show","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we visualize the data as a pie chart in this section, the amount list has the count of each iris \nlabels = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\nsizes = [50,50,50]\ncolors = ['lightskyblue', 'lightcoral', 'orange'] \nexplode = (0, 0, 0)\n\nplot.pie(sizes,              # data\n       explode=explode,    # offset parameters \n        labels=labels,      # slice labels\n        colors=colors,      # array of colours\n        autopct='%1.1f%%',  # print the values inside the wedges\n        shadow=True,        # enable shadow\n        startangle=70       # starting angle\n        )\nplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#in order to see the differences between each iris, we use a scatter plot to cluster the similar ones according to their attributes\nplot.figure(figsize=(10,5))\nplot.title('Scatterplot for the Iris data', fontsize=15)\nsb.scatterplot(data=data, s=100)\nplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we use a correlation matrix in the form of a heatmap to show the relations between each column \nplot.figure(figsize=(8,6))\nplot.title('Heatmap of the Correlations between Iris Data', fontsize=10)\nsb.heatmap(data=data[column].corr(), annot=True, cmap='YlGnBu')\nplot.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#after visualizing the data, we want to cluster the irises using SOM, this notebook uses the Unsupervised Learning with SOM kaggle notebook \n\n#first we start by seeding\n\nnp.random.seed(0)\n# we get the data and we prepare a training set from it, we take 70 from the 150\niris=data\niris = iris.sample(n=70, random_state=0,replace='true')\niris.info()\n#then we need to normalize the data, which is one of the most important stages of an SOM \n#we need to use numerical data for the categories that we have \nfrom sklearn.preprocessing import OrdinalEncoder\nordinal_encoder=OrdinalEncoder()\niris_encoded=ordinal_encoder.fit_transform(iris[['species']])\n# after we use numerical data for the categorization, we normalize the data \niris_norm = StandardScaler().fit_transform(iris_encoded)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we build a network to cluster the data\nnet = sps.somNet(20, 20, iris_norm, PBC=True, PCI=True)\nnet.train(0.1, 10000)\nnet.diff_graph(show=True,printout=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some functions are defined here to make it easier to cluster and visualize the data \n#taken from the SOM project mentioned above\ndef autocrop(fileName):\n    im = Image.open(fileName)\n    im=im.crop((0,100,2900,im.size[1]))\n    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))\n    diff = ImageChops.difference(im, bg)\n    diff = ImageChops.add(diff, diff, 2.0, -100)\n    bbox = diff.getbbox()\n    if bbox:\n        return im.crop(bbox)\n\ndef posMap(x,y):\n     if y%2==0:\n        return [y, x*2/np.sqrt(3)*3/4]\n     else:\n        return [y+0.5, x*2/np.sqrt(3)*3/4]\n    \ndef posCount(x,y):\n     return y*40+x\n\ndef posCountR(x):\n     return [np.int(x%40),np.int(x/40)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we print a couple of example nodes here to see the progress \nlistNodes=[[20,0],[23,11],[1,6],[13,37],[7,33],[18,31]]\nlistCount=[posCount(20,0), posCount(23,11), posCount(1,6), posCount(13,37), posCount(7,33), posCount(18,31)]\n\ni=0\nfor node in net.nodeList:\n    if i in listCount:\n        print('Node\\'s position: {:d} {:d}'.format(posCountR(i)[1], posCountR(i)[0]) )\n        plot.imshow(np.asarray(node.weights).reshape(28,28))\n        plot.axis('off')\n        plot.show()\n    i+=1\n    \nprojData=net.project(trainSt[:500])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we take a cropped version of the map \ncropped = autocrop('nodesDifference.png')\ncropped.save('cropped.png')\n#then we use a scatter plot to visualize the data \n#And here we prepare the plotly graph. \ntrace0 = go.Scatter(\n    x = [x for x,y in projData],\n    y = [y for x,y in projData],\n#    name = labels,\n    hoveron = [str(n) for n in labels],\n    text = [str(n) for n in labels],\n    mode = 'markers',\n    marker = dict(\n        size = 8,\n        color = labels,\n        colorscale ='Jet',\n        showscale = False,\n        opacity = 1\n    ),\n    showlegend = False\n\n)\ndata = [trace0]\n\nlayout = go.Layout(\n    images= [dict(\n                  source= \"cropped.png\",\n                  xref= \"x\",\n                  yref= \"y\",\n                  x= -0.5,\n                  y= 39.5*2/np.sqrt(3)*3/4,\n                  sizex= 40.5,\n                  sizey= 40*2/np.sqrt(3)*3/4,\n                  sizing= \"stretch\",\n                  opacity= 0.5,\n                  layer= \"below\")],\n    width = 800,\n    height = 800,\n    hovermode= 'closest',\n    xaxis= dict(\n        range=[-1,41],\n        zeroline=False,\n        showgrid=False,\n        ticks='',\n        showticklabels=False\n    ),\n    yaxis=dict(\n        range=[-1,41],\n        zeroline=False,\n        showgrid=False,\n        ticks='',\n        showticklabels=False\n    ),\n    showlegend= True\n)\n\n\nfig = dict(data=data, layout=layout)\npy.iplot(fig, filename='styled-scatter')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}