{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97cb4f1c7b3f6ac937dc3dad8148d257c5df0f27"},"cell_type":"code","source":"#Initialization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_selection import mutual_info_regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install graphviz\n!apt-get -qq install -y graphviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\nimport graphviz \n\ndef plot_decision_tree(model, columns):\n    dot_data = tree.export_graphviz(model, out_file=None, \n                             feature_names=columns,  \n                             class_names=['Malignant','Benign'],  \n                             filled=False, rounded=True,  \n                             special_characters=False)  \n    graph = graphviz.Source(dot_data)  \n    return graph \n  \n    \ndef plot_importances_features(model, columns):\n    indices = np.argsort(model.feature_importances_)[::-1]\n    feat_imp = pd.DataFrame({'Feature':columns.values[indices],\n                        'Feature ranking':model.feature_importances_[indices]})\n    plt.rcParams['figure.figsize']=(8,12)\n    sns.set_style('whitegrid')\n    ax = sns.barplot(x='Feature ranking', y='Feature', data=feat_imp)\n    ax.set(xlabel='Feature ranking')\n    plt.show()\n    \ndef getCenter(D,clusters): # distance matrix and clusters\n    err = 0.0\n    centers = []\n    contr = []\n    for i in range(len(set(clusters))):\n        id_pts = [index for index,value in enumerate(clusters) if value == i+1] #ids cluster i-th\n        sub_ms = D[id_pts,:][:,id_pts] #sub distance matrix\n        err = err + np.sum(D[np.argmin(np.mean(sub_ms, axis=0)), :])\n        beta = 1\n        index = np.exp(-beta * sub_ms / sub_ms.std()).sum(axis=1).argmax()\n        centers.append(id_pts[index])\n        contr.append(float(\"{0:.2f}\".format((len(sub_ms) * 100) / len(D))))\n    return contr,err,centers \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6c2901f1e2e7cfb24315d17f20f7e6bfcd83855"},"cell_type":"markdown","source":"**Explore the Data and Browse through its columns**"},{"metadata":{"trusted":true,"_uuid":"3c6d6029b16a3316841d1386d3aa4ce303ff6633"},"cell_type":"code","source":"df = pd.read_csv('../input/data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0ffa48d638efd78a2d91bb7043e5bcb0dca49f2"},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df5147ed345d0b5bfa92cb763cf21b3279418156"},"cell_type":"markdown","source":"\nNote that the column 'diagnosis' is the actual result and will be used for comparision of the clustering result at the later time. Hence, it is the target column or 'Y'"},{"metadata":{"trusted":true,"_uuid":"1a5f273a73e562985752f1e60ba615858d203af0"},"cell_type":"code","source":"# Datatype of columns\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4adba3353461edbde66ee050d3e9d01459a513b"},"cell_type":"markdown","source":"* 33 columns and 569 rows in total\n* 'Unnamed: 32' has null data and we will remove it from 'df'\n* 'id' is not required in the data processing, hence it's removed "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(labels=['Unnamed: 32','id'],axis=1,inplace=True)\nprint(\"Some error occured\" if 'Unnamed: 32' in df.columns else f\"Successfully removed 'Unnamed: 32'\\nCOLUMNS: {df.columns}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffcfe537b8106b60e0dd3aeed8b6ab16c9029f7b"},"cell_type":"code","source":"df.describe() ## Numerical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a126b736bd26268c32e7584c8dde0ab186dc102"},"cell_type":"code","source":"df.describe(include=['O']) # Objects","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5905f0f19e8e007b86e5dfe97a7635fcf763b5d7"},"cell_type":"code","source":"# Check for duplication exclude (id)\ndf.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f414cb3fb9ff8e91e684cb41f23adf33be6ae68"},"cell_type":"code","source":"# Few initialization\nsns.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['diagnosis'],label=\"Count\")    \nB, M = df['diagnosis'].value_counts()\n\nprint('Number of Benign\\t:\\t ',B)\nprint('Number of Malignant\\t:\\t ',M)\nprint('Percentage Benign\\t:\\t % 2.2f %%' % (B/(B+M)*100))\nprint('Percentage Malignant\\t:\\t % 2.2f %%' % (M/(B+M)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We study the correlation dividing the features into three groups: (The mean, standard error and \"worst\" or largest )"},{"metadata":{"trusted":true,"_uuid":"a68f49a73eb156f8a6952ee18965a709da078b11"},"cell_type":"code","source":"# Seperation of feature and target columns\n# Mapping Benign to 0 and Malignant to 1 and storing it in a different dataframe\ny = pd.DataFrame()\ny['diagnosis'] = df['diagnosis'].map({'M':0,'B':1})\ndf.drop('diagnosis',axis=1,inplace=True)\n# This is done for ease of use when comparing with the results obtained with different methods for clustering","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abd046258438bd8fd497d80b451efa26828d8bee"},"cell_type":"markdown","source":"### Scaling\nWe need to scale the data to apply the algo as Measurement units weight one better than other and affect final result of clustering algorithm. For example the parameters may be 'Height (m)' or 'Weight (pounds)' of humans. The model would understand only numerics and does not care about units, and especially for clustering, scaling data is sensitive to the unsupervised result obtained."},{"metadata":{"trusted":true,"_uuid":"f28bfa403b53099b0734244f838e85e0d42c8a58"},"cell_type":"code","source":"# Using the scale() in sklearn.preprocessing module\ncolumns = ['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\ndf_scaled = pd.DataFrame(preprocessing.scale(df))\nX = df_scaled.copy()\ndf_scaled.columns = columns\ndf_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_scaled.shape)\n# Now the data contains 30 columns and 569 rows, we divide it into three categories of features\ndf_columns = df_scaled.columns\n\ncolumns_mean = df_columns[0:10]\ncolumns_se = df_columns[10:20]\ncolumns_worst = df_columns[20:30]\n\nprint(\"columns_mean : \",columns_mean )\nprint(\"columns_se : \",columns_se )\nprint(\"columns_worst : \", columns_worst)\n# Getting the features out of the dataframe\nfeatures_mean = df_scaled[columns_mean]\nfeatures_se = df_scaled[columns_se]\nfeatures_worst = df_scaled[columns_worst]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot( pd.concat([features_mean,y], axis=1),  hue='diagnosis', diag_kind=\"kde\",diag_kws=dict(shade=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(22,5))\nplt.subplot(1, 3, 1)\nsns.heatmap(features_mean.corr(), cbar = False,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},cmap= 'coolwarm')\nplt.subplot(1, 3, 2)\nsns.heatmap(features_worst.corr(), cbar = False,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},cmap= 'coolwarm')\nplt.subplot(1, 3, 3)\nsns.heatmap(features_se.corr(), cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 8},cmap= 'coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**radius, area and perimeter** (mean, the wrost, and the error standard) are closely correlated to each other, the same for the characteristics of **compactness, concave points and concavity**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# SWARMPLOT\nplt.figure(figsize=(23,8))\ndata = pd.melt(pd.concat([df_scaled,y],axis=1),id_vars=\"diagnosis\",var_name=\"features\", value_name='value')\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90) \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature engineering\nSelecting the most discriminating the features that are most discriminating (see swarmplot) among the most correlated features (see correlation plot). the following considerations were therefore made,\n\n* from the correlated group [compactness_se, concavity_se concave points_se], we select **concavity_se**.\n* from the correlated group [compactness_worst, concavity_worst and concave points_worst], we select **concave points_worst**)\n* from the correlated group [concavity_mean, compactness_mean and concave points_mean] , we select **concave points_mean**)\n* from the correlated group [area_worst, perimeter_worst, radius_worst ] , we select **radius_worst**)\n* from the correlated group [perimeter_mean, area_mean, radius_mean ] , we select **area_mean**)\n* from the correlated group [area_se, perimeter_se, radius_se] , we select **area_se**)"},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_features = ['concavity_se',\n'concave points_worst',\n'concave points_mean',\n'radius_worst',\n'area_mean',\n'area_se',\n'texture_mean','texture_se','texture_worst',\n'smoothness_mean','smoothness_se','smoothness_worst',\n'symmetry_mean','symmetry_se','symmetry_worst',\n'fractal_dimension_mean','fractal_dimension_se','fractal_dimension_worst']\ndf_selected_features = df_scaled[selected_features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize= [10,6])\npd.Series(mutual_info_regression(df_selected_features, y), index= df_selected_features.columns).sort_values(ascending=True).plot(kind=\"barh\")\nplt.title(\"Feature importances\", fontsize= 20)\nplt.yticks(fontsize= 12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The most important features of the reduced dataset are (total 7 columns) : \n* 'radius_worst'\n* 'concave points_mean'\n* 'concave points_worst'\n* 'area_mean'\n* 'area_se'\n* 'concavity_se'\n* 'texture_worst'\n\nWe then select the above features for further processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reduced_features = df_scaled[['radius_worst', 'concave points_mean', 'concave points_worst', 'area_mean', 'area_se', 'concavity_se', 'texture_worst']]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7fc5f49d70cf7e41cd6431fbf89e5b36961d7c7"},"cell_type":"markdown","source":"# Clustering\nClustering (grouping a list of observations into various buckets) can be based on various factors. We try to cluster according to two clustering techniques : \n* Hierarchical based clusters (Distance) \n* k-means clusters (Centroid based)"},{"metadata":{},"cell_type":"markdown","source":"## Heirarchical Clustering\nStandard implementation is a bottom up approach where each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy where the hierarchy is defined by the distance between the clusters. "},{"metadata":{},"cell_type":"markdown","source":"### Linkage functions\nSelecting the best linkage"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.cluster.hierarchy import dendrogram, linkage\nD = df_reduced_features.values\n\n#ward = Similarity of two clusters is based on the increase in squared error when two clusters are merged\nmethods = ['single','complete','average','weighted','median','ward']\n\n\nplt.figure(figsize=(25, 8))\nfor i in range(len(methods)):\n    plt.subplot(231+i)\n    Z = linkage(D, method=methods[i]) #Perform hierarchical/agglomerative clustering. \n    de = dendrogram(\n      Z,\n      leaf_rotation=90.,\n      leaf_font_size=11.,\n      distance_sort='descending',\n      truncate_mode = 'lastp',\n      p=50\n      \n    )\n    plt.title(methods[i])\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As evidenced by the plot, among the various linkage functions selected, the **ward** method was the most suitable, as it allowed to create clusters and well separated clusters.\n- Ward suggests 2 clusters by default (different color)"},{"metadata":{"trusted":true,"_uuid":"e52491759db511fe06014256cdc878c089e85865"},"cell_type":"code","source":"sns.set_style('whitegrid') \nD = df_reduced_features.values\n\nZ = linkage(D, method='ward', metric='euclidean') #Perform hierarchical/agglomerative clustering. \n# ward = Similarity of two clusters is based on the increase in squared error when two clusters are merged\n\nplt.figure(figsize=(15, 7))\ndendrogram(\n    Z,\n    leaf_rotation=90.,\n    leaf_font_size=11.,\n    show_contracted=True,\n    distance_sort='descending',\n    truncate_mode = 'lastp', # truncated output for better rep\n    p=50\n)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21b1afa2c7b48c270595a8f104c55e2e66ac2e38"},"cell_type":"code","source":"from sklearn.decomposition import PCA # Principal Component Analysis Module\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.cluster.hierarchy import fcluster  # simple clustering\n\npca_2d = PCA(n_components=2)\nX = pca_2d.fit_transform(D)\nY = pdist(D, 'euclidean')\nY = squareform(Y)\nY.shape\nk=2\nclusters = fcluster(Z, k, criterion='maxclust')\n# print(clusters)\ncontr, err,centers = getCenter(Y,clusters)\nprint('centroid: ',centers,'\\t %items ',contr)\n# print(X)\nplt.figure(figsize=(10, 8))\nplt.scatter(X[:,0], X[:,1], c=clusters, cmap='prism')  # plot points with cluster dependent colors\nplt.scatter([X[centers[0],0], X[centers[1],0]],# all x\n            [X[centers[0],1], X[centers[1],1]],# all y\n            c='black', \n            cmap='prism',\n            marker='x',\n            s=50,\n            label=\"centroid\")  # plot centroids\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34e2331b4956071d5a741510ef5afb6dec3b95db"},"cell_type":"markdown","source":"**Truncating Dentogram**"},{"metadata":{"trusted":true,"_uuid":"d6581151d2ec0f500170f861fc9ffe8cc2db0b09"},"cell_type":"code","source":"df_y_features = pd.concat([df_reduced_features,y], axis=1)\nmean_pca_M = pca_2d.transform(df_reduced_features[ df_y_features['diagnosis']==0].mean().values.reshape(1,-1))\nmean_pca_B = pca_2d.transform(df_reduced_features[ df_y_features['diagnosis']==1].mean().values.reshape(1,-1))\n\nplt.figure(figsize=(10, 8))\nplt.scatter(X[:,0], X[:,1], c=clusters, cmap='prism')  # plot points with cluster dependent colors\nplt.scatter([mean_pca_M[:,0], mean_pca_B[:,0]],[mean_pca_M[:,1], mean_pca_B[:,1]], c='blue', cmap='prism',marker='+',s=50,\n            label=\"centroid diagnosis class (b,m)\")  # plot points with cluster dependent colors\nplt.scatter([X[centers[0],0], X[centers[1],0]],[X[centers[0],1], X[centers[1],1]], c='black', cmap='prism',marker='x',s=50,\n            label=\"centroid cluster\")  # plot points with cluster dependent colors\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is interesting to note how the centroids of the clusters fall very close to the average values of the two classes of tumors (benign and malignant). Therefore It is possible to note that, If we did not have a labeled dataset (with well-defined classes B and M) we would still be able to determine (with good probability) the class of belonging of the dataset elements, through an unsupervised clustering process ."},{"metadata":{},"cell_type":"markdown","source":"Usually the distance cutoff is set at 70% max distance for ward [refer docs](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.cluster.hierarchy.dendrogram.html)"},{"metadata":{"_uuid":"00ae9376bcfc16c2d3213a99ce604e39eb9edf98"},"cell_type":"markdown","source":"### K-Means Clustering\n\nk-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster."},{"metadata":{},"cell_type":"markdown","source":"For K means clustering, the main question here also is how many clusters. For which we use the Elbow method. It tries to find the clustering step where the acceleration of distance growth is the biggest (the \"strongest elbow\" of the blue line graph below, which is the highest value of the green graph below)"},{"metadata":{"trusted":true},"cell_type":"code","source":"D = df_reduced_features.values\nfrom sklearn.cluster import KMeans\nwcss = []\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n    kmeans.fit(D)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 10), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The steepest change occurs in when number of clusters is 2, followed by 3. Hence the number of clusters according to the elbow plot is 2 or 3."},{"metadata":{},"cell_type":"markdown","source":"However, this analysis is often not enough and we must try different methods like silhouette score to get a more concrete answer. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually.\nSilhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster."},{"metadata":{"trusted":true,"_uuid":"871013f84bff1636eec73fa2416a50760a2c34fa"},"cell_type":"code","source":"from sklearn.metrics import silhouette_score\n\nsse = []\nfor k in range(2, 8):\n    kmeans = KMeans(n_clusters=k).fit(X)\n    sse.append([k, silhouette_score(X, kmeans.labels_)])\n\nplt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1])\nplt.title('Silhouette Analysis')\nplt.xlabel('Number of clusters')\nplt.ylabel('silhouette_score')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"520a04d14aad396ac5ec4d2540b05d9884436fec"},"cell_type":"markdown","source":"Typically the case is that we select the number of clusters with the maximum silhouette_score. In our case this is '2' which matches to the previous observations. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\nX = df_reduced_features.values\n\nrange_n_clusters = [2, 3, 4]\n\npca_2d = PCA(n_components=2)\npca_2d_r = pca_2d.fit_transform(X)\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, ax2 = plt.subplots(1, 1)\n    fig.set_size_inches(18, 5)\n\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10, max_iter=9000)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(pca_2d_r[:, 0], pca_2d_r[:, 1], marker='.', s=90, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n    \n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    centers = pca_2d.transform(centers)\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=250, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=100, edgecolor='k')\n\n    ax2.set_title(f\"Silhouette analysis for KMeans clustering on sample data with n_clusters = {n_clusters}\")\n    ax2.set_xlabel(\"PC1\")\n    ax2.set_ylabel(\"PC2\")\n\n    plt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}