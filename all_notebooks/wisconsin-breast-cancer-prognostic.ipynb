{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Breast Cancer Wisconsin Prognostic\n## Context\nData is from UCI Machine Learning Repository\nhttp://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wpbc.names ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Content\n\n1. Importing Libraries.\n2. Exploration of Data.\n3. Normalization of Data.\n4. Modelling of Data.\n5. Comparing Model Performance\n6. Fitting Data to Final Model\n7. Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Objective\nThe main goal here is to fit a model to be able to predict whether breast cancer is at the malignant or benign stage based on 30 features and to which variable contributes the most.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1. Importing libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder # for creating dummy variables\nfrom sklearn.preprocessing import MinMaxScaler # for normalising data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Importing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading csv file into dataframe\ndf = pd.read_csv(\"../input/uci-wisconsin-breast-cancer/BreastCancer.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explanation of variables\n\n- ID number\n- Diagnosis (M = malignant, B = benign)\n\nThe mean, standard error and “worst” (mean of the three largest values) of ten features were computed for each image, resulting in 30 features. Below is a list of the ten real-valued features computed for each cell nucleus:\n- radius (mean of distances from center to points on the perimeter)\n- texture (standard deviation of gray-scale values)\n- perimeter\n- area\n- smoothness (local variation in radius lengths)\n- compactness (perimeter^2 / area - 1.0)\n- concavity (severity of concave portions of the contour)\n- concave points (number of concave portions of the contour)\n- symmetry\n- fractal dimension","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Data cleaning","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Dropping id column\ndf1 = df.drop(columns=\"id\")\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Total missing values for each feature\ndf1.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A summary of the statistical details of the features show that the means of the features varies widely and therefore we will have to normalise the data before modelling.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# summary of the DataFrame\ndf1.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#  some basic statistical details for all features\ndf1.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All variables except **diagnosis** are numeric variables.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Check the number of malignant(M) and benign(B) cases\nsns.countplot(x=\"diagnosis\", data=df1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Iniatial visualisation to showed that patients who with malignant prognostics had higher radius, area, perimeter and smoothness mean as compared to those with benign prognostics.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(5,5)})\nplt.subplot(2, 2, 1)\nsns.boxplot(x='diagnosis', y='radius_mean', data=df1)\nplt.ylabel('Radius Mean')\nplt.xlabel('Diagnosis')\nplt.title('Dianosis vs Radius Mean')\nplt.subplot(2, 2, 2)\nsns.boxplot(x='diagnosis', y='perimeter_mean', data=df1)\nplt.ylabel('Perimeter Mean')\nplt.xlabel('Diagnosis')\nplt.title('Dianosis vs Perimeter Mean')\nplt.subplot(2, 2, 3)\nsns.boxplot(x='diagnosis', y='area_mean', data=df1)\nplt.ylabel('Area Mean')\nplt.xlabel('Diagnosis')\nplt.title('Dianosis vs Area Mean')\nplt.subplot(2, 2, 4)\nsns.boxplot(x='diagnosis', y='smoothness_mean', data=df1)\nplt.ylabel('Smoothness Mean')\nplt.xlabel('Diagnosis')\nplt.title('Dianosis vs Smoothness Mean')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['radius_mean', 'perimeter_mean','smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'texture_mean', 'symmetry_mean','diagnosis']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's examine how features determine prognostics\nsns.pairplot(df1[labels], hue='diagnosis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = round(df1.corr(), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(15,15)})\nsns.heatmap(corr_matrix, cmap='BuPu', annot_kws={'size': 8}, cbar = True, annot=True)\nplt.title('Variable Correlation Plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Normalising data","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# dividing the data into X and Y\nX=df1.iloc[:,1:31]\nX.head(2)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"Y=df1.iloc[:,0:1]\nY.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LabelEncoder is used to convert the categorical response into dummy variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"le = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# converting diagnosis to dummy variables\nY['diagnosis_new'] = le.fit_transform(Y.diagnosis)\nY.head()\nY_new=Y.iloc[:,1:2]\nY_new.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to the wide difference between the the means of the features, we will have to normalise the features for learning algorithm that computes the distance between the data points lke KNN. This includes all curve based algorithms. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"scaler.fit(X)\nX1 = scaler.transform(X)\nX_new=pd.DataFrame(X1, columns=X.columns)\nX_new.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features are normalised now as shown in the statistics details below.","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_new.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Importing libraries for fitting data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_predict, cross_val_score # This is for cross-validation\nfrom sklearn.metrics import accuracy_score, recall_score, confusion_matrix, balanced_accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Instantiating Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# search for an optimal value of K for KNN\n\n# range of k we want to try\nk_range = range(1, 31)\n# empty list to store scores\nk_scores = []\n\n# 1. we will loop through reasonable values of k\nfor k in k_range:\n    # 2. run KNeighborsClassifier with k neighbours\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # 3. obtain cross_val_score for KNeighborsClassifier with k neighbours\n    scores = cross_val_score(knn, X_new, Y_new, cv=10,  n_jobs=10)\n    # 4. append mean of scores for k neighbors to k_scores list\n    k_scores.append(scores.mean())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\nsns.set(rc={'figure.figsize':(5,5)})\nplt.plot(k_range, k_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-validated accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding the best k\nk_df = pd.DataFrame(k_scores, index=k_range)\nbest_kest = int(k_df.idxmax())\nbest_kest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=best_kest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC(random_state=100, C=1.0,\n    kernel='linear',\n    probability=True,\n    ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit = LogisticRegression(penalty='l2',\n    tol=0.0001,\n    random_state=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"etc = ExtraTreesClassifier(criterion='entropy',\n    min_samples_split=3,\n    min_samples_leaf=1,\n    n_jobs=10,\n    random_state=100,\n    verbose=2\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bagging = BaggingClassifier(n_estimators=1000,\n    n_jobs=10,\n    random_state=100,\n    verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=10, random_state=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Fitting Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting models that does not require scaling\nmodels_1 = [[\"DecisionTreeClassifier\",etc],\n         [\"BaggingClassifier\",bagging],\n         [\"GaussianNB\",nb],\n         [\"RandomForestClassifier\",rf]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_accuracy = []\nfor i in models_1:\n    y_predict = cross_val_predict(i[1], X, Y_new, cv=10, n_jobs=10)\n    ACC = round(accuracy_score(Y_new, y_predict), 2) \n    recall = round(recall_score(Y_new, y_predict, average='weighted'), 2) \n    B_ACC = round(balanced_accuracy_score(Y_new, y_predict), 2)\n    Specificiti = round(2 * B_ACC - recall, 2)\n    m_accuracy.append([i[0],ACC,recall,B_ACC,Specificiti]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting models that require scaling\nmodels_2 = [[\"LogisticRegression\",logit],\n         [\"SupportVector Machine\",svm],\n         [\"KNeighborsClassifier\",knn]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in models_2:\n    y_predict = cross_val_predict(i[1], X_new, Y_new, cv=10, n_jobs=10)\n    ACC = round(accuracy_score(Y_new, y_predict), 2) \n    recall = round(recall_score(Y_new, y_predict, average='weighted'), 2) \n    B_ACC = round(balanced_accuracy_score(Y_new, y_predict), 2)\n    Specificiti = round(2 * B_ACC - recall, 2)\n    m_accuracy.append([i[0],ACC,recall,B_ACC,Specificiti]) \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. Comapring model performance","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"performace_table = pd.DataFrame(m_accuracy)\nperformace_table.columns = ['Model','Accuracy', 'Recall','Bal. Accuracy','Specificity']\nperformace_table.style.bar(subset=[\"Accuracy\",], color='#0d8ca6')\\\n                 .bar(subset=[\"Recall\"], color='#50cce6')\\\n                 .bar(subset=[\"Bal. Accuracy\"], color='#17990e')\\\n.bar(subset=[\"Specificity\"], color='#6ed667')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nplt.barh(performace_table.Model, performace_table.Accuracy, color='#f5ec42', edgecolor='black')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# list of feature importance in desecending order\nrf.fit(X, Y_new)\nimportance = pd.DataFrame(rf.feature_importances_, index=X_new.columns, columns=['FeatureImportance'])\nimportance.sort_values(by='FeatureImportance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1 Fitting final model","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Now, try to train again with the full data\nsvm.fit(X_new,Y_new)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2 Saving final Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Python pickle module is used for serializing and de-serializing a Python object structure\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the model\nf1=open('breat_cancer_svm_model','wb') # wb => write binary\npickle.dump(svm, f1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# better close (or flush) a file when done.\nf1.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}