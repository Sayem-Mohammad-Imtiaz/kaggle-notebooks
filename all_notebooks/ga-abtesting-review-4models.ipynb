{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load packages for modeling","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\ndef rmse(y_true, y_pred):\n    return round(np.sqrt(mean_squared_error(y_true, y_pred)), 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/ga-dataset-clean/train_clean.csv', converters={'fullVisitorId': str})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [col for col in df.columns if df[col].nunique() > 1]\ndf = df[columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use datetime to split the dataset into training and testing dataset and set validation data","metadata":{}},{"cell_type":"code","source":"import datetime\ndf[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = df[df['date']<=datetime.date(2017,5,31)]\n\ntest_df = df[df['date']>datetime.date(2017,5,31)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = train_df['totals.transactionRevenue'].fillna(0).astype(float)\ntarget = target.apply(lambda x: np.log1p(x))\ndel train_df['totals.transactionRevenue']\n\ntarget_test = test_df['totals.transactionRevenue'].fillna(0).astype(float)\ntarget_test = target_test.apply(lambda x: np.log1p(x))\ndel test_df['totals.transactionRevenue']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = [col for col in train_df.columns if train_df[col].nunique() > 1]\ntrain_df = train_df[columns]\ntest_df = test_df[columns]\n##Before performing label encoding, we merge the test and train sets to insure we have consistent labels in the two sets:\ntrn_len = train_df.shape[0]\nmerged_df = pd.concat([train_df, test_df])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in merged_df.columns:\n    if col in ['fullVisitorId', 'month', 'day', 'weekday', 'visithour']: continue\n    if merged_df[col].dtypes == object or merged_df[col].dtypes == bool:\n        merged_df[col], indexer = pd.factorize(merged_df[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerics = [col for col in merged_df.columns if 'totals.' in col]\nnumerics += ['visitNumber', 'mean_hits_per_day', 'fullVisitorId']\ncategorical_feats =  [col for col in merged_df.columns if col not in numerics]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in categorical_feats:\n    merged_df[col] = merged_df[col].astype(int)\nmerged_df['fullVisitorId'] = merged_df['fullVisitorId'].astype(float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = merged_df[:trn_len]\ntest_df = merged_df[trn_len:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.005,\n        \"bagging_fraction\" : 0.6,\n        \"feature_fraction\" : 0.6,\n        \"bagging_frequency\" : 6,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 1\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn_cols = [col for col in train_df.columns if col not in ['fullVisitorId']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# crossâ€”validation\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)  \n# set dataframe\noof = np.zeros(len(train_df))\nstart = time.time()\nfeatures = list(train_df[trn_cols].columns)\nfeature_importance_df = pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    # train data\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][trn_cols], label=target.iloc[trn_idx], categorical_feature=categorical_feats) \n    # alidation data\n    val_data = lgb.Dataset(train_df.iloc[val_idx][trn_cols], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n\n    \nevals_result = {} ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"## 1. LightGB","metadata":{}},{"cell_type":"code","source":"num_round = 10000\nlgbmodel = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data],evals_result=evals_result,verbose_eval=100,early_stopping_rounds = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results & evaluation","metadata":{}},{"cell_type":"code","source":"# prediction\noof[val_idx] = lgbmodel.predict(train_df.iloc[val_idx][trn_cols], num_iteration=lgbmodel.best_iteration)\noof[trn_idx] = lgbmodel.predict(train_df.iloc[trn_idx][trn_cols], num_iteration=lgbmodel.best_iteration)\n\nprint(f\"LGB : RMSE val: {rmse(target.iloc[val_idx], oof[val_idx] )}  - RMSE train: {rmse(target.iloc[trn_idx], oof[trn_idx])}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_test  = lgbmodel.predict(test_df[trn_cols], num_iteration=lgbmodel.best_iteration) \nprint(f\"LGB : RMSE test: {rmse(target_test, oof_test )}  \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = lgb.plot_metric(evals_result, metric='rmse')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## feature importance    \nfold_importance_df = pd.DataFrame()\nfold_importance_df[\"feature\"] = features\nfold_importance_df[\"importance\"] = lgbmodel.feature_importance()\nfold_importance_df[\"fold\"] = fold_ + 1\nfeature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:1000].index\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,10))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\n\nexplainer = shap.TreeExplainer(lgbmodel)\n\nshap_values = explainer.shap_values(train_df[trn_cols])\n\nshap.summary_plot(shap_values, train_df[trn_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shap\n\nexplainer = shap.TreeExplainer(lgbmodel)\n\nshap_values = explainer.shap_values(test_df[trn_cols])\n\nshap.summary_plot(shap_values, test_df[trn_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"####  metric MAE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"mae\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.005,\n        \"bagging_fraction\" : 0.6,\n        \"feature_fraction\" : 0.6,\n        \"bagging_frequency\" : 6,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 1\n    }\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    ##train data\n    trn_data_mae = lgb.Dataset(train_df.iloc[trn_idx][trn_cols], label=target.iloc[trn_idx], categorical_feature=categorical_feats) \n    ##validation data\n    val_data_mae = lgb.Dataset(train_df.iloc[val_idx][trn_cols], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n\n\nnum_round = 10000\nlgbmodel_mae = lgb.train(params, trn_data_mae, num_round, valid_sets = [trn_data_mae, val_data_mae],verbose_eval=100,early_stopping_rounds = 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prediction\noof_mae = np.zeros(len(train_df))\noof_mae[val_idx] = lgbmodel_mae.predict(train_df.iloc[val_idx][trn_cols], num_iteration=lgbmodel_mae.best_iteration)\noof_mae[trn_idx] = lgbmodel_mae.predict(train_df.iloc[trn_idx][trn_cols], num_iteration=lgbmodel_mae.best_iteration)\n\nprint(f\"LGB_MAE : RMSE val: {rmse(target.iloc[val_idx], oof_mae[val_idx] )}  - RMSE train: {rmse(target.iloc[trn_idx], oof_mae[trn_idx])}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_mae_test = lgbmodel_mae.predict(test_df[trn_cols], num_iteration=lgbmodel.best_iteration) \nprint(f\"LGB_MAE : RMSE test: {rmse(target_test, oof_mae_test )}  \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reference:https://www.kaggle.com/fabiendaniel/lgbm-starter?scriptVersionId=5983064","metadata":{}},{"cell_type":"markdown","source":"##  2. XGBoosting","metadata":{}},{"cell_type":"markdown","source":"https://yunyaniu.blog.csdn.net/article/details/103938851\n\nhttps://blog.csdn.net/qq_26684561/article/details/102574708\n\nhttps://www.bbsmax.com/A/kvJ33rxwJg/\n\nhttps://www.jianshu.com/p/5504c1f9e562\n\nhttps://zhuanlan.zhihu.com/p/64799119","metadata":{}},{"cell_type":"code","source":"params = {'objective': 'reg:linear',\n              'eval_metric': 'rmse',\n              'eta': 0.001,\n              'max_depth': 10,\n              'subsample': 0.6,\n              'colsample_bytree': 0.6,\n              'alpha':0.001,\n              'random_state': 1,\n              'silent': True}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_trn_data = xgb.DMatrix(train_df.iloc[trn_idx][trn_cols], target.iloc[trn_idx])\nxgb_val_data = xgb.DMatrix(train_df.iloc[val_idx][trn_cols], target.iloc[val_idx])\nxgb_test = xgb.DMatrix(test_df[trn_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgbmodel = xgb.train(params, xgb_trn_data, \n                      num_boost_round=2000, \n                      evals= [(xgb_trn_data, 'train'), (xgb_val_data, 'valid')],\n                      early_stopping_rounds=100, \n                      verbose_eval=100\n                     )\n\ny_pred_train = xgbmodel.predict(xgb_trn_data, ntree_limit=xgbmodel.best_ntree_limit)\ny_pred_val = xgbmodel.predict(xgb_val_data, ntree_limit=xgbmodel.best_ntree_limit)\n\n\nprint(f\"XGB : RMSE val: {rmse(target.iloc[val_idx], y_pred_val)}  - RMSE train: {rmse(target.iloc[trn_idx], y_pred_train)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results & evaluation","metadata":{}},{"cell_type":"code","source":"y_xgb_test = xgbmodel.predict(xgb_test, ntree_limit=xgbmodel.best_ntree_limit)\nprint(f\"XGB : RMSE test: {rmse(target_test, y_xgb_test )}  \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom xgboost import plot_importance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.plot_importance(xgbmodel,max_num_features=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.plot_importance(xgbmodel,max_num_features=10,importance_type='gain')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"explainer = shap.TreeExplainer(xgbmodel)\n\nshap_values = explainer.shap_values(test_df[trn_cols])\n\nshap.summary_plot(shap_values, test_df[trn_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Catboost","metadata":{}},{"cell_type":"code","source":"catmodel = CatBoostRegressor(iterations=1000,\n                             learning_rate=0.05,\n                             depth=10,\n                             eval_metric='RMSE',\n                             random_seed = 1,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 50,\n                             od_wait=20)\ncatmodel.fit(train_df.iloc[trn_idx][trn_cols], target.iloc[trn_idx],\n              eval_set=(train_df.iloc[val_idx][trn_cols], target.iloc[val_idx]),\n              use_best_model=True,\n              verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results & evaluation","metadata":{}},{"cell_type":"code","source":"y_pred_train = catmodel.predict(train_df.iloc[trn_idx][trn_cols])\ny_pred_val = catmodel.predict(train_df.iloc[val_idx][trn_cols])\ny_cat_test = catmodel.predict(test_df[trn_cols])\n\nprint(f\"CatB: RMSE val: {rmse(target.iloc[val_idx], y_pred_val)}  - RMSE train: {rmse(target.iloc[trn_idx], y_pred_train)}\")\nprint(f\"CatB : RMSE test: {rmse(target_test, y_cat_test )}  \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reference:https://www.kaggle.com/julian3833/2-quick-study-lgbm-xgb-and-catboost-lb-1-66","metadata":{}},{"cell_type":"markdown","source":"## 4. Random forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_RF = train_df[trn_cols]\ny_train_RF = target\nX_test_RF  = test_df[trn_cols]\ny_test_RF  = target_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results & evaluation","metadata":{}},{"cell_type":"code","source":"RF=RandomForestRegressor()\nRF.fit(X_train_RF,y_train_RF)\nRF_scores = cross_val_score(RF,X_train_RF,y_train_RF,cv=5,scoring='neg_mean_squared_error')\nprint(RF_scores.mean())\ny_RF_test = RF.predict(X_test_RF)\nprint(\"randomforest RMSE : \", np.sqrt(metrics.mean_squared_error(y_test_RF,y_RF_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature importance\nfor name, score in zip(df.columns, RF.feature_importances_):\n    print(f\" \",name, \" = \" ,score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}