{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Importing libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:11:31.040283Z","iopub.execute_input":"2021-09-01T02:11:31.040662Z","iopub.status.idle":"2021-09-01T02:11:31.045777Z","shell.execute_reply.started":"2021-09-01T02:11:31.040631Z","shell.execute_reply":"2021-09-01T02:11:31.044779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading data**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/breastcancerdataset/BRCA.csv')\ndata = data.dropna()\ndata = data.drop([data.columns[i] for i in range(8,15)], axis=1)\ndata = data.drop('Patient_ID', axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:11:32.971698Z","iopub.execute_input":"2021-09-01T02:11:32.972236Z","iopub.status.idle":"2021-09-01T02:11:32.995305Z","shell.execute_reply.started":"2021-09-01T02:11:32.972202Z","shell.execute_reply":"2021-09-01T02:11:32.99452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data preprocessing**","metadata":{}},{"cell_type":"code","source":"le = LabelEncoder()\n# Transform categorical data\ndata['Patient_Status'] = le.fit_transform(data['Patient_Status']) \ndata['Gender'] = le.fit_transform(data['Gender'].astype(str))\nX,Y = data.iloc[:,:-1], data.iloc[:,-1] # Extract features and labels\nX_dummies = X.copy() # Copy of the features with \"dummied\" categorical data for one kind of feature\nX_dummies = pd.get_dummies(X_dummies, prefix=['Tumour_Stage']) # Get dummies for a categorical data\nX['Tumour_Stage'] = le.fit_transform(X['Tumour_Stage'].astype(str)) # Transform categorical data\n# Split into training and testing sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.151, random_state=42)\nX_dummies_train, X_dummies_test, Y_dummies_train, Y_dummies_test = train_test_split(X_dummies, Y, test_size=0.151, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:11:35.755907Z","iopub.execute_input":"2021-09-01T02:11:35.756396Z","iopub.status.idle":"2021-09-01T02:11:35.775347Z","shell.execute_reply.started":"2021-09-01T02:11:35.756365Z","shell.execute_reply":"2021-09-01T02:11:35.774512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Training the model**","metadata":{}},{"cell_type":"code","source":"## Train models\nlog_reg = LogisticRegression(max_iter = 7777)\nlog_reg.fit(X_train, Y_train)\ntree = DecisionTreeClassifier()\ntree.fit(X_train, Y_train)\n\n## Train models with dummies\nlog_reg_dummies = LogisticRegression(max_iter = 7777)\nlog_reg_dummies.fit(X_dummies_train, Y_dummies_train)\ntree_dummies = DecisionTreeClassifier()\ntree_dummies.fit(X_dummies_train, Y_dummies_train)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:11:38.711685Z","iopub.execute_input":"2021-09-01T02:11:38.712039Z","iopub.status.idle":"2021-09-01T02:11:38.783206Z","shell.execute_reply.started":"2021-09-01T02:11:38.71201Z","shell.execute_reply":"2021-09-01T02:11:38.781897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Evaluate the models**","metadata":{}},{"cell_type":"code","source":"## Evalueate models\nlog_reg_acc = 100*log_reg.score(X_test, Y_test)\ntree_acc = 100*tree.score(X_test, Y_test)\nlog_reg_acc_dummies = 100*log_reg_dummies.score(X_dummies_test, Y_dummies_test)\ntree_acc_dummies = 100*tree_dummies.score(X_dummies_test, Y_dummies_test)\n\nprint(\"Logistic Regression: {:.4f}%\".format(log_reg_acc))\nprint(\"Decision Tree Classifier: {:.4f}%\".format(tree_acc))\nprint(\"Logistic Regression with dummies: {:.4f}%\".format(log_reg_acc_dummies))\nprint(\"Decision Tree Classifier with dummies: {:.4f}%\".format(tree_acc_dummies))","metadata":{"execution":{"iopub.status.busy":"2021-09-01T02:11:40.914973Z","iopub.execute_input":"2021-09-01T02:11:40.915548Z","iopub.status.idle":"2021-09-01T02:11:40.932523Z","shell.execute_reply.started":"2021-09-01T02:11:40.915501Z","shell.execute_reply":"2021-09-01T02:11:40.931616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Conclusion**\nAs far as my knowledge and experience in ML is spreading - this dataset is a candidate for regression classifications as I can see. I used only 2 kind of model algorithms to predict the desired label from the data and as much as my experience, as I said earlier, in ML, data science and statistics is spreading - I achieved 83% for the Logistic Regression model I used, among the others. Concerns, corrections, criticism, tips and tricks, leave them all in the comments.","metadata":{}}]}