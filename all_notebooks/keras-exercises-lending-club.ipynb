{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"<h1 align=\"center\"> Keras Exercise and Deep Learning project on Lending Club Dataset </h1> <br>\n\n**Before starting:  If you find this kernal helpful please upvote the project.**\n\n\n \n## Company Information:\nLending Club is a  peer to peer lending company based in , headquartered in San Francisco, California, in which investors provide funds for potential borrowers and investors earn a profit depending on the risk they take (the borrowers credit score). Lending Club provides the \"bridge\" between investors and borrowers. LendingClub is the world's largest peer-to-peer lending platform. For more basic information about the company please check out: <a src=\"https://en.wikipedia.org/wiki/Lending_Club\"> Lending Club Information </a>\n\n<img src=\"http://echeck.org/wp-content/uploads/2016/12/Showing-how-the-lending-club-works-and-makes-money-1.png\"><br><br>\n<img src=\"https://media.giphy.com/media/3orif2cYsDAMzFMvjW/giphy.gif\"><br>\n\n## Our Goal\n\nGiven historical data on loans can we build a model to predict wether or nor a borrower will pay back their loan? For example, in the future when we get a new potential customer we can assess whether or not they are likely to pay back the loan. \n\nThe \"loan_status\" column contains our label.\n\n\n## Outline: <br><br>\nI. Introduction <br>\na) [Data Overview](#data_overview)<br>\nb) [Starter Code](#starter_code)<br>\nc) [Loading data and other imports](#loading)<br>\n\nII. [Exploratory Data Analysis](#EDA)</b><br>\na) Exploring correlation between the continuous feature variables\nb) Changing the catagorical label of loan_status to numerical one\n\n\nIII. [Data Preprocessing](#data_prepro)<br>\na) Missing data<br>\nb) Categorical Variables and Dummy Variables<br>\nc) Normalizing the Data<br>\n\nIV. Creating the Model<br>\n\nV. <b>Evaluating Model Performance.</b><br>\n\n\n\n## References:\n1) <a src=\"https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/\"> Python for Data Science and Machine Learning Bootcamp </a> by  Jose Portilla <br> \n2) <a src=\"https://www.kaggle.com/janiobachmann/lending-club-risk-analysis-and-metrics\"> Lending Club || Risk Analysis and Metrics</a> by Janio Martinez <br>\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n## Data Overview \n<a id=\"data_overview\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"----\n-----\nThere are many LendingClub data sets on Kaggle. Here is the information on this particular data set. It is always a good practice to investigate data and trys to guess what is going to happen through it. So, read the data columns and descriptions and imagine how the system works, in this case how lending clubs company works , how it colects data and try to guess which data you think is important. At the end, you will see how you are far of your gut instinct! let's look at and think:\n\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LoanStatNew</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>loan_amnt</td>\n      <td>The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>term</td>\n      <td>The number of payments on the loan. Values are in months and can be either 36 or 60.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>int_rate</td>\n      <td>Interest Rate on the loan</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>installment</td>\n      <td>The monthly payment owed by the borrower if the loan originates.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>grade</td>\n      <td>LC assigned loan grade</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>sub_grade</td>\n      <td>LC assigned loan subgrade</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>emp_title</td>\n      <td>The job title supplied by the Borrower when applying for the loan.*</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>emp_length</td>\n      <td>Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>home_ownership</td>\n      <td>The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>annual_inc</td>\n      <td>The self-reported annual income provided by the borrower during registration.</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>verification_status</td>\n      <td>Indicates if income was verified by LC, not verified, or if the income source was verified</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>issue_d</td>\n      <td>The month which the loan was funded</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>loan_status</td>\n      <td>Current status of the loan</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>purpose</td>\n      <td>A category provided by the borrower for the loan request.</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>title</td>\n      <td>The loan title provided by the borrower</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>zip_code</td>\n      <td>The first 3 numbers of the zip code provided by the borrower in the loan application.</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>addr_state</td>\n      <td>The state provided by the borrower in the loan application</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>dti</td>\n      <td>A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>earliest_cr_line</td>\n      <td>The month the borrower's earliest reported credit line was opened</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>open_acc</td>\n      <td>The number of open credit lines in the borrower's credit file.</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>pub_rec</td>\n      <td>Number of derogatory public records</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>revol_bal</td>\n      <td>Total credit revolving balance</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>revol_util</td>\n      <td>Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>total_acc</td>\n      <td>The total number of credit lines currently in the borrower's credit file</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>initial_list_status</td>\n      <td>The initial listing status of the loan. Possible values are – W, F</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>application_type</td>\n      <td>Indicates whether the loan is an individual application or a joint application with two co-borrowers</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>mort_acc</td>\n      <td>Number of mortgage accounts.</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>pub_rec_bankruptcies</td>\n      <td>Number of public record bankruptcies</td>\n    </tr>\n  </tbody>\n</table>\n\n---\n----","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndata_info = pd.read_csv('../input/lendingclub-data-sets/lending_club_info.csv',index_col='LoanStatNew')\nprint(data_info.loc['revol_util']['Description'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feat_info(col_name):\n    print(data_info.loc[col_name]['Description'])\nfeat_info('mort_acc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"loading\"></a>\n## Loading the data and other imports\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# might be needed depending on your version of Jupyter\n%matplotlib inline\n\n#reading data\ndf = pd.read_csv('../input/lendingclub-data-sets/lending_club_loan_two.csv')\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"EDA\"></a>\n# Exploratory Data Analysis\n\nWhy we want to explore data? \nAnswer: To better understand which variables are important, view summary statistics, and visualize the data\n\nlet see the target value? Loan_status. \n\nIt is the ultimate goal of the lending club! If the customers fully paid their loan or not! Which information are really important and detrimine if a person would returns the loan or not. \n\nFirst let's see the Loan_status:\nIt is especially good for classification task to see how our target vlaues are balanced. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='loan_status', data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We would say this is unbalanced problem. From above plot we see that most of loans were paid off  by 80%. \nFurther comments comes to my mind: It is a little unbalnced dataset.\n\nThis is really common thing in calssification problems like fruad or spam. There is a lot of less instances of fraud or spam then are of ligimtate actions , such as ligimitate email, ligimatate credit card purchase and ligimitate paid off. \n\nIt means that we can expect very well accuracy while evaluating our ML model. But,in fact the precision and recall are going to be ture metrics. \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,4))\nsns.distplot(df['loan_amnt'], kde=False, bins=40)\nplt.xlim(0,45000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the plot you can see some spikes is happining between bins. It means we have some loans as a standard. In all, you can see the amount of the loan applied for by the borrower. means seems to around 10000 $ and you see the distribution.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Exploring correlation between the continuous feature variables\n\nWe can explore the correlation between numeric variables using .corr() method:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is a hard to read and visualise above correlation values. So let us try heatmap to visualise it:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(df.corr(),annot=True,cmap=\"YlGnBu\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You should have noticed almost perfect correlation with the \"installment\" feature. loan_amnt has 0.95 correlation with 'installment' and it is quiet intersting.So, let us explore this feature further. But way we want to explor this furthur?\n\nThe reason : We do not want to accedintly leaking of data from the features into our label. We do not want to see that a single label is a perfect predictor of a label. Because, it indicates that it is not really a feature but it is probably just some duplicated information very similar to the label. Let's go ahead and print that out:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_info('installment')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the keyword is \"if\"! ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_info('loan_amnt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It pretty much makes sense that the \"instalment\" and \"loan_amnt\" would have be extremely correlated. Because, they are essentialy correlated by some sort of internal formula that this company uses. We also can do a scatter plot to confirm this and view this high correlation.\n\nnote: It is abvious that if a loan amount is high, the monthly payment would be high.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x='installment',y='loan_amnt',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's creat a boxplot showing the relationship between the loan_status and the loan amount. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='loan_status',y='loan_amnt',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In gerneral it looks like both similar. Charged off is slightly higher, meaning that if our loan amount is highr we have a slight increase in liklihood of being charded off. Which again it intuitivly makes sense that it is hard to pay back larger loans than smaller loans. We can calculate the summary of statistics for the loan amount by the loan status:\n\nThis is a quantitative description of the above box plot:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.groupby('loan_status')['loan_amnt'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's explore the Grade and SubGrade columns that LendingClub attributes to the loans. What are the unique possible grades and subgrades?\n<br><br>\n**Note**: These columns are catagoricals and not numerical. So it is abvius that you can not investigate corr() method on it. Although, you can investigate its relation with output by simply use countplot by specifying output label. So, you can observe for each specific catagorical feature what how many belongs to different output class! ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['grade'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['sub_grade'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if we have any relation with grade and the label:\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='grade',data=df, hue='loan_status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we can see clear relationship , but it is hard to tell the order of grade! So we may need to reorder them. \nThe percentage of charged off loans seem increasing as the letter grade gets higher. Actually we can compare it by ratios. However, let's do it with subgrades\n\nDisplay a count plot per subgrade. You may need to resize for this plot and reorder the x axis. Feel free to edit the color palette. Explore both all loans made per subgrade as well being separated based on the loan_status. After creating this plot, go ahead and create a similar plot, but set hue=\"loan_status\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,4))\nsubgrade_order = sorted(df['sub_grade'].unique())\nsns.countplot(x='sub_grade',data=df,order=subgrade_order,palette='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nice, we can see all sub_grades counts. Now, let's see the relation of sub_grades with the output. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,4))\nsubgrade_order = sorted(df['sub_grade'].unique())\nsns.countplot(x='sub_grade',data=df,order=subgrade_order,palette='coolwarm',hue='loan_status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You see that first groups starting from A group are commenly pay back their loans. But this trend decreasing as we continue to the rest. Think about these groups. In description of sub_grade you can see that it is a grade assigned by lending club. Why and how lending clubs assign this feature or information on each person!? What is the forumla and the reason behind that? It looks like that as grades goes down the loan status get worse and people can not pay their loan back.Is it kind of prediction performed by lending club? These are all questions that we should answer? We also need to talk to expert from lending club to get more insight. \n\n<br><br>\nLet's continue our analysis.\nIt looks like F and G subgrades don't get paid back that often. Isloate those and recreate the countplot just for those subgrades.\n\nlet's zoom at that section of the plot:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f_and_g = df[(df['grade']=='G') | (df['grade']=='F')]\n\nplt.figure(figsize=(12,4))\nsubgrade_order = sorted(f_and_g['sub_grade'].unique())\nsns.countplot(x='sub_grade',data=f_and_g,order=subgrade_order,palette='coolwarm',hue='loan_status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, you can see the statuses for almost the worse subgrades. For example, for G5 the liklihood almost same for both paid of and charged off. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Changing the catagorical label of loan_status to numerical one:\n\n\nBy doing this we are able use it in our model. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['loan_repaid'] = df['loan_status'].map({'Fully Paid':1, 'Charged Off':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['loan_repaid','loan_status']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can see which one of our features has highest correlation with the label(it is numerical now and the name is loan_repaid):\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.corr()['loan_repaid'].sort_values().drop('loan_repaid').plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that int_rate has high negative correlation with loan repaid. It actually makes sene that with high interest rate it is acutally hard to repay the loan! ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"You can further explore the data base on your interset, expert knowlege , or google some info about the problem and gain some knowledge about this specific task and use it as expert domain knowledge! It is obt to you. But it is enough for now let go to the next section. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data_prepro\"></a>\n# Data Preprocessing\n**Section Goals: Remove or fill any missing data. Remove unnecessary or repetitive features. Convert categorical string features to dummy variables.**\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing Data\n**Let's explore this missing data columns. We use a variety of factors to decide whether or not they would be useful, to see if we should keep, discard, or fill in the missing data.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So what is the length of the dataframe?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's create a Series that displays the total count of missing values per column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You see some values are missing. In order to have better insight that what percengtage of the total Datafram is missing you can calculate the percentage:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"100 *df.isnull().sum()/len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"emp_title and emp_length have almost 5 percent missing data. But, let's see what are these features. Let's see even if we could drop those columns or not. We can print out their information using the feat_info() function which we have defined at the first of the notbook:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_info('emp_title')\nprint('\\n')\nfeat_info('emp_length')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In catagorical features , the important thing that we should always consider is uniqueness of its values. We actully want to convert catagorical featurs to numerical through dummy varaible. But, if there are too many unique sample from a feature, then it makes harder to convert it to dummy variable feature. Let's check it the problem here;","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['emp_title'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['emp_title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Realistically there are too many unique job titles to try to convert this to a dummy variable feature. In fact this columns is nominal data which is a varibale that has no numerical importance, such as occupation, person name etc. let's remove this column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('emp_title',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now explore emp_length feature column. It is kind of timeseries data. Time series data has a temporal value attached to it, so this would be something like a date or a time stamp that you can look for trends in time.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted(df['emp_length'].dropna().unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emp_length_order = [ '< 1 year',\n                      '1 year',\n                     '2 years',\n                     '3 years',\n                     '4 years',\n                     '5 years',\n                     '6 years',\n                     '7 years',\n                     '8 years',\n                     '9 years',\n                     '10+ years']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,4))\n\nsns.countplot(x='emp_length',data=df,order=emp_length_order)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can also plot out the countplot with a hue separating Fully Paid vs Charged Off. Because, the most important thing for us here is if a person paid back his loan or not?! ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,4))\nsns.countplot(x='emp_length',data=df,order=emp_length_order,hue='loan_status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CHALLENGE TASK: This still doesn't really inform us if there is a strong relationship between employment length and being charged off, what we want is the percentage of charge offs per category. Essentially informing us what percent of people per employment category didn't pay back their loan. There are a multitude of ways to create this Series. Once you've created it, see if visualize it with a [bar plot](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.DataFrame.plot.html). This may be tricky, refer to solutions if you get stuck on creating this Series.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"emp_co = df[df['loan_status']==\"Charged Off\"].groupby(\"emp_length\").count()['loan_status']\nemp_fp = df[df['loan_status']== \"Fully Paid\"].groupby(\"emp_length\").count()['loan_status']\nemp_len = emp_co/emp_fp\nemp_len.plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Charge off rates are extremely similar across all employment lengths. Go ahead and drop the emp_length column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('emp_length',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, let's investigate other columns with missing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have title value with some missing data. But if you explore more, you can find out that the purpose column is actully contains repeated information of title group.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['purpose'].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['title'].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You see we do not need title column actually. Let's drop it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('title',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We continue with the rest columns containing missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_info('mort_acc')\nprint('\\n')\ndf['mort_acc'].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"AS you see it is a numerical feature, and has 37795 missing data. Is this feature important to us. Do you think it has relation with the target output. Let's create a value_counts of the mort_acc column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['mort_acc'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are many ways we could deal with this missing data. We could attempt to build a simple model to fill it in, such as a linear model, we could just fill it in based on the mean of the other columns, or you could even bin the columns into categories and then set NaN as its own category. There is no 100% correct approach! Let's review the other columsn to see which most highly correlates to mort_acc","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Correlation with the mort_acc column\")\ndf.corr()['mort_acc'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks like the total_acc feature correlates with the mort_acc , this makes sense! Let's try this fillna() approach. We will group the dataframe by the total_acc and calculate the mean value for the mort_acc per total_acc entry. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Mean of mort_acc column per total_acc\")\ndf.groupby('total_acc').mean()['mort_acc']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's fill in the missing mort_acc values based on their total_acc value. If the mort_acc is missing, then we will fill in that missing value with the mean value corresponding to its total_acc value from the Series we created above. This involves using an .apply() method with two columns. Check out the link below for more info, or review the solutions video/notebook.\n\n[Helpful Link](https://stackoverflow.com/questions/13331698/how-to-apply-a-function-to-two-columns-of-pandas-dataframe) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total_acc_avg = df.groupby('total_acc').mean()['mort_acc']\ntotal_acc_avg[2.0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_mort_acc(total_acc,mort_acc):\n    '''\n    Accepts the total_acc and mort_acc values for the row.\n    Checks if the mort_acc is NaN , if so, it returns the avg mort_acc value\n    for the corresponding total_acc value for that row.\n    \n    total_acc_avg here should be a Series or dictionary containing the mapping of the\n    groupby averages of mort_acc per total_acc values.\n    '''\n    if np.isnan(mort_acc):\n        return total_acc_avg[total_acc]\n    else:\n        return mort_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"revol_util and the pub_rec_bankruptcies have missing data points, but they account for less than 0.5% of the total data. Go ahead and remove the rows that are missing those values in those columns with dropna().","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Variables and Dummy Variables\n\n**We're done working with the missing data! Now we just need to deal with the string values due to the categorical columns.** <br>\n**Note:** For more informaiton on Categorical feature encoding, you could check my other kernal on [Categorical feature encoding.](https://www.kaggle.com/hadiyad/categorical-feature-encoding-skill) \n\nNote:  A categorical variable is a variable that can take some limited number of values.for example,day of the week.It can be one of 1,2,3,4,5,6,7 only.\n\n**We can list all the columns that are currently non-numeric. For more info visit: [Helpful Link](https://stackoverflow.com/questions/22470690/get-list-of-pandas-dataframe-columns-based-on-data-type)**\n\n\n[Another very useful method call](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html)\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.select_dtypes(['object']).columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's now go through all the string features to see what we should do with them.**\n\n---\n\n### 1. **'term'** feature:\n\n Let's convert the \"term\" feature into either a 36 or 60 integer numeric data type using .apply() or .map().","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['term'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['term'] = df['term'].apply(lambda term: int(term[:3]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. **'grade'** feature:\n\n**We already know grade is part of sub_grade, so just drop the grade feature.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('grade',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You could convert the subgrade into dummy variables. Then concatenate these new columns to the original dataframe. Remember you always drop the original subgrade column and add drop_first=True to your get_dummies call.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subgrade_dummies = pd.get_dummies(df['sub_grade'],drop_first=True)\ndf = pd.concat([df.drop('sub_grade',axis=1),subgrade_dummies],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we check now df we will have more columns, because, more subgrade columns are added through one hot encoding:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. '**verification_status', 'application_type','initial_list_status','purpose' **' features:\n\nLet's convert also these columns: ['verification_status', 'application_type','initial_list_status','purpose'] into dummy variables and concatenate them with the original dataframe. The reason is these columns are really good candidate to be converted to dummy variables, because their values are few catagories. Remember to set drop_first=True and to drop the original columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies = pd.get_dummies(df[['verification_status', 'application_type','initial_list_status','purpose' ]],drop_first=True)\ndf = df.drop(['verification_status', 'application_type','initial_list_status','purpose'],axis=1)\ndf = pd.concat([df,dummies],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. **'home_ownership'** feature:\n\nFirst, review the value_counts for the home_ownership column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['home_ownership'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can convert these to dummy variables, but [replace](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html) NONE and ANY with OTHER, so that we end up with just 4 categories, MORTGAGE, RENT, OWN, OTHER. Then concatenate them with the original dataframe. Remember to set drop_first=True and to drop the original columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['home_ownership']=df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')\n\ndummies = pd.get_dummies(df['home_ownership'],drop_first=True)\ndf = df.drop('home_ownership',axis=1)\ndf = pd.concat([df,dummies],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. **'address'** feature:\nLet's feature engineer a zip code column from the address in the data set. Create a column called 'zip_code' that extracts the zip code from the address column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['zip_code'] = df['address'].apply(lambda address:address[-5:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Now let's make this zip_code column into dummy variables using pandas. Concatenate the result and drop the original zip_code column along with dropping the address column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies = pd.get_dummies(df['zip_code'],drop_first=True)\ndf = df.drop(['zip_code','address'],axis=1)\ndf = pd.concat([df,dummies],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6. **'issue_d '** feature:\n\n**This would be data leakage**, we wouldn't know beforehand whether or not a loan would be issued when using our model, so in theory we wouldn't have an issue_date, so we drop this feature. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('issue_d',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. **'earliest_cr_line'** feature:\n\nThis appears to be a historical time stamp feature. We can extract the year from this feature using a .apply function, then convert it to a numeric feature. We set this new data to a feature column called 'earliest_cr_year'.Then we can drop the earliest_cr_line feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda date:int(date[-4:]))\ndf = df.drop('earliest_cr_line',axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! we are done with all categorical and no onve left. let's check it again!:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.select_dtypes(['object']).columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We only have 'loan_status' left, and if you remember we converted this catagorical label to numerical label as 'loan_repaid'!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[['loan_status','loan_repaid']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So far so good, we have finished EDA and some feature engineering. We are ready to go to the next step. The training and modeling. We now use these clean, and well prepared data to build a machine learning model! Let's begin modlleing** But before that let's check all columns and features that we have now:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split\n\nLet's first split our train and test dataset as always we do before creating model. The reason is that we want to test the model after training.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we drop the load_status column we created earlier, since its a duplicate of the loan_repaid column. We'll use the loan_repaid column since its already in 0s and 1s.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop('loan_status',axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We set X and y variables to the .values of the features and label.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('loan_repaid',axis=1).values\ny = df['loan_repaid'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And let's perform a train/test split with test_size=0.2,","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Normalizing the Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We use a MinMaxScaler to normalize the feature data X_train and X_test. We don't want data leakge from the test set so we only fit on the X_train data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating the Model\n Run the cell below to import the necessary Keras functions.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nfrom tensorflow.keras.constraints import max_norm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, we can build a sequential model to will be trained on the data. You have unlimited options here, but here is what the solution uses: a model that goes 78 --> 39 --> 19--> 1 output neuron. OPTIONAL: Explore adding [Dropout layers](https://keras.io/layers/core/) [1](https://en.wikipedia.org/wiki/Dropout_(neural_networks) and [2](https://towardsdatascience.com/machine-learning-part-20-dropout-keras-layers-explained-8c9f6dc4c9ab)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\n\n# https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n\n\n# input layer\nmodel.add(Dense(78,  activation='relu'))\nmodel.add(Dropout(0.2))\n\n# hidden layer\nmodel.add(Dense(39, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# hidden layer\nmodel.add(Dense(19, activation='relu'))\nmodel.add(Dropout(0.2))\n\n# output layer\nmodel.add(Dense(units=1,activation='sigmoid'))\n\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, it is time to fit our model to the training data. We can fit the model to the training data for at least 25 epochs. Also add in the validation data for later plotting. Optional: add in a batch_size of 256.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=X_train, \n          y=y_train, \n          epochs=25,\n          batch_size=256,\n          validation_data=(X_test, y_test), \n          )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's also save our model to outuput.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('full_data_project_model.h5')  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating Model Performance\n\nYes , it is time to evaluate our model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = pd.DataFrame(model.history.history)\nlosses[['loss','val_loss']].plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Now, let's create predictions from the X_test set and display a classification report and confusion matrix for the X_test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\npredictions = model.predict_classes(X_test)\nprint(classification_report(y_test,predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion_matrix(y_test,predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results are kind of satisfactory. However, we can do further works to even get better results.\n<br><br>\nIt is time that we use our model to see if we want to offer a customer a loan or not?\n<br><br>\nGiven the customer below, would you offer this person a loan? We randomly select one of costumers and we want to see if the model works well or not!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nrandom.seed(101)\nrandom_ind = random.randint(0,len(df))\n\nnew_customer = df.drop('loan_repaid',axis=1).iloc[random_ind]\nnew_customer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict_classes(new_customer.values.reshape(1,78))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now check, did this person actually end up paying back their loan?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.iloc[random_ind]['loan_repaid']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Nice and great job!**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}