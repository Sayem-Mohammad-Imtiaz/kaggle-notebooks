{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"9f977914-6c42-a319-8eb5-e7d4c3abf231"},"source":"### Kaggle datasets: IMDB movie ratings analysis"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"558f56e7-8d7c-aca4-de20-8d473d414bbc"},"outputs":[],"source":"from __future__ import division\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# Disable pandas SettingWithCopyWarning\npd.options.mode.chained_assignment = None\n\nwork_dir = os.environ['WORK_DIR']\n\n# Load data (movie_metadata.csv downloaded from https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset)\ndf = pd.read_csv(os.path.join(work_dir, 'movie_metadata.csv'))\ndf.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e42799d0-d912-474f-ada2-a81a446b90e7"},"outputs":[],"source":"### SHARE OF MISSING DATA PER ATTRIBUTE ###\n\nmissing = df[df.isnull().any(axis=1)].count().to_frame()\nmissing.columns = ['missing']\ntotal = df.count().to_frame()\ntotal.columns = ['total']\n\nshare_of_missing = missing.join(total)\nshare_of_missing.share = share_of_missing.missing / share_of_missing.total\nshare_of_missing.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6dc2b40d-0610-33cc-72ae-2bdde38073f5"},"outputs":[],"source":"### CONTENT RATINGS TIME SERIES ###\n\n# Prepare subset of the data\n\nlabels = ['content_rating', 'title_year']\n# print share_of_missing.loc[labels]\n\nts = df[labels]\nts = ts[pd.notnull(ts.title_year) & pd.notnull(ts.content_rating)]\nts.title_year = ts.title_year.astype(int)\n\n\n# Get distinct content rating values\n\n# print ts.content_rating.unique()\n\nratings = ts[labels].groupby('title_year')['content_rating'].unique\n\n# print ratings.head(10)\n# print ratings.tail(10)\n\n\n# Map content ratings from different periods \n# https://en.wikipedia.org/wiki/Motion_Picture_Association_of_America_film_rating_system\n\nts = ts[ts.title_year >= 1968]\n\ndef map_ratings(row):\n    immaculate = ['G', 'TV-G']\n    decent = ['PG', 'GP', 'PG-13', 'M']\n    foul = ['R', 'NC-17', 'X']\n    \n    if row.content_rating in immaculate:\n        return 'immaculate'\n    if row.content_rating in decent:\n        return 'decent'\n    if row.content_rating in foul:\n        return 'foul'\n\nts['rating_map'] = ts.apply(map_ratings, axis=1)\n\n\n# Check how the share of immaculate, decent and foul movies changed over time\n\nts = ts.groupby(['title_year', 'rating_map']).count().reset_index()\nts['yearly_total'] = ts.groupby('title_year')['content_rating'].transform('sum')\nts['rating_share'] = ts.content_rating / ts.yearly_total\n\n\n# Plot the time series\n\nratings = ts['rating_map'].unique()\n\nfor rating in ts['rating_map'].unique():\n    sub_ts = ts[ts.rating_map == rating]\n    plt.plot(sub_ts['title_year'], sub_ts['rating_share'])\naxes = plt.gca()\naxes.set_ylim([ts.rating_share.min(), ts.rating_share.max()])\naxes.set_xlim([ts.title_year.min(), ts.title_year.max()])\nplt.legend(ratings)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"13d62aed-a28c-3563-c926-b27af890354e"},"outputs":[],"source":"# Comparing change in content rating share between 2000 and 2010 using Chi-square \nratings_2000 = ts[ts.title_year == 2000][['rating_map', 'rating_share']]\nratings_2000.rename(columns={'rating_share': 'rating_share_2000'}, inplace=True)\nratings_2010 = ts[ts.title_year == 2010][['rating_map', 'content_rating', 'yearly_total']]\nratings_2010.rename(columns={'content_rating': 'rating_count_2010', 'yearly_total': 'total_2010'}, inplace=True)\nchi_2_test = ratings_2000.merge(ratings_2010, on='rating_map')\n\n# print chi_2_test.head()\n\n# H0: the share of ratings didn't change between 2000 and 2010 \n#     foul = 0.482\n#     decent = 0.476\n#     immaculate = 0.043 \n#\n# HA: the share of ratings changed between 2000 and 2010\n#     foul <> 0.482 OR decent <> 0.476 OR immaculate <> 0.043\n\n# Computing Chi-square test statistic\nchi_2_test['expected_cell_count'] = chi_2_test['rating_share_2000'] * chi_2_test['total_2010']\n\n# Check if the sample size is large enough to use chi-square distribution\nsmall_subsets = chi_2_test[chi_2_test['expected_cell_count'] < 5]\nassert small_subsets.empty\n\ndef compute_partial_chi_2(row):\n    expected_count = row.total_2010 * row.rating_share_2000\n    return (row.rating_count_2010 - expected_count)**2 / expected_count \n\nchi_2_test['partial_chi_2'] = chi_2_test.apply(compute_partial_chi_2, axis=1)\ncomputed_chi_2 = chi_2_test['partial_chi_2'].sum()\n# print computed_chi_2\n\n\n# Check the boarder chi-square value from distribution with 2 degrees of freedom and 5% confidence level\n\ntabulated_chi_2 = 10.597\n\n# Because computed_chi_2 < tabulated_chi_2 we don't reject H0\n# There is no difference in share of ratings between 2000 and 2010 "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0666035b-8b93-0c11-c4d1-5169a4eed009"},"outputs":[],"source":"### GENDER VS YEARS IN BUSINESS LOGITISTIC REGRESSION ###\n\nimport sexmachine.detector as gender\nd = gender.Detector()\n\n# prepare subset of the data\nactors = pd.DataFrame({'actor_name': [], 'year': []})\nfor label in ['actor_1_name', 'actor_2_name', 'actor_3_name']:\n    labels = [label, 'title_year']\n    tmp = df[labels]\n    tmp = tmp[pd.notnull(tmp['title_year']) & pd.notnull(tmp[label])]\n    tmp = tmp.rename(columns={'title_year': 'year', label: 'actor_name'})\n    actors = pd.concat([actors, tmp])\nactors['gender'] = actors.actor_name.apply(lambda x: d.get_gender(x.split()[0]))\n\n# print actors.gender.unique()\n\ndef map_genders(row):\n    female = ['female', 'mostly_female']\n    male = ['male', 'mostly_male']\n    \n    if row.gender == 'andy':\n        return np.nan\n    if row.gender in female:\n        return 0\n    if row.gender in male:\n        return 1\n\nactors['gender'] = actors.apply(map_genders, axis=1)\nactors.dropna(inplace=True)\n\nstart_years = actors.groupby(['actor_name'])['year'].min().reset_index()\nstart_years = start_years.rename(columns={'year': 'start_year'})\nend_years = actors.groupby(['actor_name'])['year'].max().reset_index()\nend_years = end_years.rename(columns={'year': 'end_year'})\n\nactors = actors.merge(start_years)\nactors = actors.merge(end_years)\n\nactors['years_of_work'] = actors.end_year - actors.start_year\n\ndel actors['end_year']\ndel actors['start_year']\ndel actors['year']\nactors.drop_duplicates(inplace=True)\nactors.head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2a108109-8604-7123-2521-4c6f99ebc3a0"},"outputs":[],"source":"# years of work distribution by gender\n\nfemales = actors[actors.gender == 0]\nmales = actors[actors.gender == 1]\n\nplt.hist(females['years_of_work'], 20, alpha=0.5, label='Actresses', normed=True, cumulative=True)\nplt.hist(males['years_of_work'], 20, alpha=0.5, label='Actors', normed=True, cumulative=True)\nplt.legend(loc='upper right')\nplt.show()\n\nactors.plot(kind='scatter', x='years_of_work', y='gender')\nplt.show()\n\n# The are more actresses than actors with very short careers.\n# Women on average DO live longer, but the are more old active actors than old active actresses."}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}