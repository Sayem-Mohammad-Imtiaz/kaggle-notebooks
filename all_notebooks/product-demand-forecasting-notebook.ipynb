{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Demand Forecasting\n Dataset : Historical Product Demand from Kaggle\n \n The dataset contains historical product demand for a manufacturing company with footprints globally. The company providesthousands of products within dozens of product categories. There are four central warehouses to ship products within the region it is responsible for. Since the products are manufactured in different locations all over the world, it normally takes more than one month to ship products via ocean to different central warehouses. If forecasts for each product in different central with reasonable accuracy for the monthly demand for month after next can be achieved, it would be beneficial to the company in multiple ways."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"IMPORT LIBRARY\n\n**Import library for data visualizaton**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport sys \nimport matplotlib \nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DATA SET FROM KAGGLE\n\n**Import dataset from kaggle**\n\n**Check the data types and data set description**"},{"metadata":{"trusted":true},"cell_type":"code","source":"FilePath = r'/kaggle/input/productdemandforecasting/Historical Product Demand.csv'\ndf = pd.read_csv(FilePath)\n\nprint(\"Data Types\")\nprint(df.dtypes)\nprint(\"\\t\")\n\nprint(df.head())\nprint(\"\\t\")\n\nprint(df.describe())\nprint(\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DATA WRANGLING AND VISUALIZATION\n\nFrom describe function we know that there are:\n1. 2160 unique products\n2. 4 different warehouses\n3. 33 product categories\n\nFrom dtypes function we know that the data types from each column are in object format, we need to reformat the data types.\n1. Date column to datetime data types\n2. Order_Demand to integer data types\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The visualization of raw dataset\n# Groupby Warehouse\n\nProdWarehouse = df.groupby(['Warehouse']).count()['Product_Category'].reset_index(name='counts').sort_values(['counts'],ascending=False)\n\n\n%matplotlib inline\nplt.figure(figsize=(15,15))\nax = sns.barplot(y='counts',x='Warehouse',data=ProdWarehouse,palette=\"tab10\")\nax.set(ylabel='counts', xlabel='Warehouse')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Groupby Category\nProdCategory = df.groupby(['Product_Category']).size().reset_index(name='counts').sort_values(['counts'],ascending=False)\n \n%matplotlib inline\nplt.figure(figsize=(15,15))\nax = sns.barplot(              y='Product_Category',x='counts',data=ProdCategory,palette=\"tab10\")\nax.set(ylabel='Product_Category', xlabel='counts')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nf, axes = plt.subplots(2, 2, figsize=(20, 20),sharex=True)\nsns.despine(left=True)\n\n# Subplot 1: Warehouse J\nprod_WHouseJ=df.loc[df['Warehouse']=='Whse_J']\nprod_WHouseJ=prod_WHouseJ.groupby(['Product_Category']).sum()['Order_Demand'].reset_index(name='Order_Demand_WHouseJ').sort_values(['Order_Demand_WHouseJ'],ascending=False)\n\nsns.barplot(x='Order_Demand_WHouseJ',y='Product_Category',data=prod_WHouseJ,palette=\"tab10\",ax=axes[0,0])\naxes[0,0].set( xlabel='Order_Demand_WHouseJ',ylabel='Product_Category')\n\n\n# Subplot 2: Warehouse A\nprod_WHouseA=df.loc[df['Warehouse']=='Whse_A']\nprod_WHouseA=prod_WHouseA.groupby(['Product_Category']).sum()['Order_Demand'].reset_index(name='Order_Demand_WHouseA').sort_values(['Order_Demand_WHouseA'],ascending=False)\n\nsns.barplot(x='Order_Demand_WHouseA',y='Product_Category',data=prod_WHouseA,palette=\"tab10\",ax=axes[0,1])\naxes[0,1].set( xlabel='Order_Demand_WHouseA',ylabel='Product_Category')\n\n# Subplot 3: Warehouse S\nprod_WHouseS=df.loc[df['Warehouse']=='Whse_S']\nprod_WHouseS=prod_WHouseS.groupby(['Product_Category']).sum()['Order_Demand'].reset_index(name='Order_Demand_WHouseS').sort_values(['Order_Demand_WHouseS'],ascending=False)\n\nsns.barplot(x='Order_Demand_WHouseS',y='Product_Category',data=prod_WHouseS,palette=\"tab10\",ax=axes[1,0])\naxes[1,0].set( xlabel='Order_Demand_WHouseS',ylabel='Product_Category')\n\n# Subplot 4: WarehouseC\nprod_WHouseC=df.loc[df['Warehouse']=='Whse_C']\nprod_WHouseC=prod_WHouseC.groupby(['Product_Category']).sum()['Order_Demand'].reset_index(name='Order_Demand_WHouseC').sort_values(['Order_Demand_WHouseC'],ascending=False)\n\nsns.barplot(x='Order_Demand_WHouseC',y='Product_Category',data=prod_WHouseC,palette=\"tab10\",ax=axes[1,1])\naxes[1,1].set( xlabel='Order_Demand_WHouseC',ylabel='Product_Category')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Change data types\n\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Order_Demand'] = df['Order_Demand'].astype(str).map(lambda a: a.lstrip('(').rstrip(')')).astype(int)\n\n\n# Sort data by period\n\ndf_SortedDate = df.sort_values('Date').reset_index().drop('index',axis=1)\ndf_SortedDate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print(\"Data Types\")\nprint(df.dtypes)\nprint(\"\\t\")\n\nprint(df.head())\nprint(\"\\t\")\n\nprint('Number of missing values by column',[sum(df[i].isnull()) for i in df.columns])\nprint('All missing values are in Date column.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Display the product with no date information.\n\nbool_series = pd.isnull(df[\"Date\"])\ndf[bool_series]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop product with no Date information\ndf_Date=df.dropna()\n\nprint(\"Data Dimension\",df_Date.shape)\nprint(\"\\t\")\nprint(\"The number of products is\",len(df_Date['Product_Code'].value_counts().index))\nprint(\"Period range is from\",df_Date['Date'].min(),\"to\", df_Date['Date'].max())\nprint(\"Order Quantity is from\",df_Date['Order_Demand'].min(),\"to\", df_Date['Order_Demand'].max())\nprint(\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Remove rows that have the same products, dates, order demand, and warehouse\n# Order with same product code, dates, order demand, and warehouse information indicates that customer making same order by mistakes\n# Leave the last order in the record \n\ndf_NoDup = df_Date.drop_duplicates(subset = ['Product_Code','Date','Order_Demand','Warehouse'], keep = 'last')\ndf_NoDup ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# First order in 2011-01-08 and last order in 2017-01-09\n# Data in Jan, 2017 should be removed; otherwise, model interpretation will be mislead.\n# It is because the latest date in this dataset is 2017-01-09 while forecasting is for monthly horizon.\n# Remove data in Jan 2017\n\ndf_11_to_16 = df_NoDup.loc[df_NoDup['Date']<'2017-01']\ndf_11_to_16","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"df_monthly = df_11_to_16.rename(columns = {\"Date\": 'Period'})\ndf_monthly['Period']=df_monthly['Period'].dt.to_period('M')\ndf_monthly = df_monthly.groupby(['Product_Code','Period'])['Order_Demand'].sum().reset_index().sort_values('Period').reset_index().drop('index',axis=1)\ndf_monthly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check to see if periods in dataset is continuous\n# Create a duration with continuous periods\n\nperiod_range = pd.date_range('2011-01-01','2016-12-31', freq='MS').to_period('M')\nperiod_range = set(period_range)\ndata_period = set(df_monthly['Period'])\nperiod_range.difference(data_period)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# The missing periods are 5 months in 2011, including Feb, Mar, Apr, Jul, and Aug.\n# Drop the record before 2011-12\n\ndf_monthly= df_monthly.loc[df_monthly['Period'] > '2011-12']\ndf_monthly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Remove stopped products\n# After removing order in 2017-01 we consider the last order is in 2016-12\n# If the product didn't have history during 2016 we consider the product stopped\n\nlatest_datamonth = df_monthly.groupby('Product_Code')['Period'].max().reset_index()\nlatest_datamonth = latest_datamonth.loc[latest_datamonth['Period'] > '2015-12']\n\ndata_ready = df_monthly.loc[df_monthly['Product_Code'].isin(latest_datamonth['Product_Code'])]\ndata_ready","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Remove new products for statistical forecast method\n# A minimum of 24 months data is required to forecast trends and seasonality using statistical forecasting methods. \n\nfrom operator import attrgetter\nduration_data = data_ready.groupby('Product_Code').agg({'Period': ['min', 'max']}).reset_index()\nduration_data['Duration (Month)'] = (duration_data[('Period', 'max')] - duration_data[('Period', 'min')]).apply(attrgetter('n')) + 1\nduration_data = duration_data.loc[duration_data['Duration (Month)'] > 24 ]\ndata = data_ready.loc[data_ready['Product_Code'].isin(duration_data['Product_Code'])]\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# In cases when history is too short (let's say the history less than 24 months), \n# the products are likely to be new products and forecasting methods for new products are different.\n\ndata_NewProd = data_ready.loc[~data_ready['Product_Code'].isin(duration_data['Product_Code'])]\ndata_NewProd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# The dataset includes 2061 products\n# Coose several products for demonstration\n\ndata_sorted=data.groupby(['Product_Code']).sum().reset_index().sort_values(['Order_Demand'],ascending=False)\ndata_sorted.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_transpose = pd.pivot_table(data, values = 'Order_Demand', index = 'Period', columns = 'Product_Code',aggfunc=np.sum).reset_index().rename_axis(\"\", axis=\"columns\")\n\n#Fill in missing values with 0. Months with missing values are implied to have zero demands.\ndata_transpose = data_transpose.fillna(0)\ndata_transpose = data_transpose.set_index('Period')\ndata_transpose","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The dataset includes data for 2061 products, which takes Python too long to return model results.\n# Thus, I sampled only several products with highests demands for demonstration purpose\ndata_set = data_transpose[['Product_1359','Product_1248','Product_0083','Product_1341','Product_1241']]\ndata_set.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_set","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TIME SERIES VISUALIZATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot each product column before split the data to train and test set \nimport warnings\nfor i in data_set.columns[0:]:\n    ax = data_set[i].plot(figsize=(15, 5), color ='green')\n    fig = ax.get_figure()\n    plt.show(block=False)\n    plt.close(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DECOMPOSE THE DATA\n\nBy looking at the graph of sales data above, we can see a general increasing trend with no clear pattern of seasonal or cyclical changes. The next step is to decompose the data to view more of the complexity behind the linear visualization. A useful Python function called seasonal_decompose within the 'statsmodels' package can help us to decompose the data into four different components:\n\n* Observed\n* Trended\n* Seasonal\n* Residual\n\nhttps://www.bounteous.com/insights/2020/09/15/forecasting-time-series-model-using-python-part-one/"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n\n# graphs to show seasonal_decompose\n\n#def seasonal_decompose (i):\nfor i in data_set.columns[0:]: \n    y=data_set[i]\n    y.index=y.index.to_timestamp()\n    decomposition = sm.tsa.seasonal_decompose(y, model='additive',extrapolate_trend='freq')\n    fig = decomposition.plot()\n    fig.set_size_inches(14,7)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CHECK FOR STATIONARY\n\nNext, we need to check whether the dataset is stationary or not. A dataset is stationary if its statistical properties like mean, variance, and autocorrelation do not change over time.\n\nMost time series datasets related to business activity are not stationary since there are usually all sorts of non-stationary elements like trends and economic cycles. But, since most time series forecasting models use stationarity—and mathematical transformations related to it—to make predictions, we need to ‘stationarize’ the time series as part of the process of fitting a model.\n\nTwo common methods to check for stationarity are Visualization and the Augmented Dickey-Fuller (ADF) Test. \n\nhttps://www.bounteous.com/insights/2020/09/15/forecasting-time-series-model-using-python-part-one/\n\nhttps://www.kaggle.com/sumi25/understand-arima-and-tune-p-d-q"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries, window = 12, cutoff = 0.01):\n\n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window).mean()\n    rolstd = timeseries.rolling(window).std()\n\n    #Plot rolling statistics:\n    fig = plt.figure(figsize=(12, 8))\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show()\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20 )\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    pvalue = dftest[1]\n    if pvalue < cutoff:\n        print('p-value = %.4f. The series is likely stationary.' % pvalue)\n    else:\n        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)\n    \n    print(dfoutput)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the stationarity of five highest ordered products \n# Product_1359,Product_1248,Product_0083,Product_1341,Product_1241\n\nfor i in data_set.columns[0:]: \n    test_stationarity(data_set[i])\n    print (i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"MAKE THE DATA STATIONER\n\nFrom the stationary test before, we know that Product 0083 and Product 1341 is likely non stationary.\n\nTo proceed with our time series analysis, we need to stationarize the dataset. There are many approaches to stationarize data, but we’ll use de-trending, differencing, and then a combination of the two.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detrending\n\ndef detrending_func(y):\n    y_detrend =  (y - y.rolling(window=12).mean())/y.rolling(window=12).std()\n    return y_detrend","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"y=data_set['Product_0083']\na=detrending_func(y)\n#test_stationarity(a)\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Differencing\n\ndef differencing_func(y):\n    y_12lag =  y - y.shift(12)\n    return  y_12lag","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"y=data_set['Product_0083']\nb=differencing_func(y)\n#test_stationarity(b)\nb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detrending + Differencing\ndef diff_detrend_func(y) :\n    y_detrend =  (y - y.rolling(window=12).mean())/y.rolling(window=12).std()\n    y_12lag_detrend =  y_detrend - y_detrend.shift(12)\n    return y_12lag_detrend","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=data_set['Product_0083']\nc=diff_detrend_func(y)\ntest_stationarity(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Product 0083 is likely non-stationary\n# Stationarize this product\n\ny=data_set['Product_0083']\n\n# Reduce it's trend using transformation\n\ny_log=np.log(y)\nplt.plot(y_log)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is some noise in realizing the forward trend here. There are some methods to model these trends and then remove them from the series. Some of the common ones are:\n• Smoothing: using rolling/moving average\n• Aggression: by taking the mean for a certain time period (year/month)\n\nhttps://medium.com/@stallonejacob/time-series-forecast-a-basic-introduction-using-python-414fcb963000"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Smoothing: using rolling/moving average\n# In smoothing we usually take the past few instances (rolling estimates) We will discuss two methods under smoothing\n# Moving average and Exponentially weighted moving average.\n# The first method we will use Smoothing-Moving average \n# take x consecutive values and this depends on the frequency if it is 1 year we take 12 values\n\nmoving_avg = y_log.rolling(12).mean()\nplt.plot(y_log)\nplt.plot(moving_avg, color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"y_log_moving_avg_diff=y_log - moving_avg\ny_log_moving_avg_diff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"y_log_moving_avg_diff.dropna(inplace=True)\ny_log_moving_avg_diff","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"test_stationarity(y_log_moving_avg_diff)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Smoothing-Weighted Moving Average\n\nexpweighted_avg=y_log.ewm(halflife=12,ignore_na=False,min_periods=0,adjust=True).mean()\nplt.plot(y_log)\nplt.plot(expweighted_avg,color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Check it's stationarity\n\ny_log_ewma_diff=y_log - expweighted_avg\ntest_stationarity(y_log_ewma_diff)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Stationary test for product 1341\n\nIn the test result before stated that after doing smoothing-weighted average the product is still non-stationary. The test conducted with 1 % cut off (1 % critical value).\n\nThe test statistic is smaller than 10% critical value. So, we can say we are almost 90% confident that this is stationary.\n\n"},{"metadata":{},"cell_type":"markdown","source":"SEASONALITY ( ALONG WITH TREND )\n\nPreviously we saw just trend part of the time series, now we will see both trend and seasonality. \n\nMost Time series have trends along with seasonality. \n\nThere are two common methods to remove trend and seasonality, they are:\n\n• Differencing: by taking difference using time lag\n\n• Decomposition: model both trend and seasonality, then remove them"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Seasonality (along with trend)\n# Differencing\n\ny_log_diff=y_log - y_log.shift()\nplt.plot(y_log_diff)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Looks ok to me but let’s parse it using our stationary testing function\n\ny_log_diff.dropna(inplace=True)\ntest_stationarity(y_log_diff)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seasonality (along with trend)\n# Decomposition\n\nimport statsmodels.api as sm\n\n# graphs to show seasonal_decompose\n\ndef seasonal_decompose (x):\n    decomposition = sm.tsa.seasonal_decompose(x, model='additive',extrapolate_trend='freq')\n    fig = decomposition.plot()\n    fig.set_size_inches(14,7)\n    plt.show()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"seasonal_decompose (y_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndef seasonal_decom (a):\n    decomposition=seasonal_decompose(a)\n\n    trend=decomposition.trend\n    seasonal=decomposition.seasonal\n    residual=decomposition.resid\n\n    plt.subplot(411)\n    plt.plot(a, label='Original')\n    plt.legend(loc='best')\n    plt.subplot(412)\n    plt.plot(trend, label='Trend')\n    plt.legend(loc='best')\n    plt.subplot(413)\n    plt.plot(seasonal, label='Seasonality')\n    plt.legend(loc='best')\n    plt.subplot(414)\n    plt.plot(residual, label='Residuals')\n    plt.legend(residual='best')\n    plt.tight_layout()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"seasonal_decom (y_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"y_log_decompose=residual\ny_log_decompose.dropna(inplace=True)\ntest_stationarity(y_log_decompose)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\n# ACF dan PACF plots :\nfrom statsmodels.tsa.stattools import acf, pacf\n\nlag_acf=acf(y_log_diff, nlags=20)\nlag_pacf=pacf(y_log_diff, nlags=20, method='ols')\n\n# Plot ACF\nplt.subplot(121)\nplt.plot(lag_acf)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From five products we have choosen before, there are three product that have stationary time series\n\nProducts 1359, Products 1248, and Products 1241. We will make a forecasting model for theese three products."},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to split train and test set\n# history length of a product starts from the month of first order, not all products have the same history length\n# the function is to split train-test based on product's history length instead of dataset length\n\ndef train_test_split(data_set):\n    List = data_set.values.tolist()\n    i = List.index(next(filter(lambda x: x!=0, myList)))\n    data_set = data_set.iloc[i:,]\n    train_set = data_set[:int(0.8*(len(data)))]\n    test_set = data_set[int(0.8*len(data)):]\n    return train_set, test_set, data_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create data for forecasting\nstart = data.index.tolist()[-1] + 1\nfcastperiods = 12  # forecast periods is subject to change by forecast users\nfull_period = [start + x for x in range(0,fcastperiods)]                                                                   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rom statsmodels.tsa.arima_model import ARIMA\n# ACF dan PACF plots :\nfrom statsmodels.tsa.stattools import acf, pacf\n\nlag_acf=acf(y_log_diff, nlags=20)\nlag_pacf=pacf(y_log_diff, nlags=20, method='ols')\n\n# Plot ACF\nplt.subplot(121)\nplt.plot(lag_acf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# ARIMA example\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom random import random\n# contrived dataset\ndata_set = [x + random() for x in range(1, 100)]\n# fit model\nmodel = ARIMA(data_set, order=(1, 1, 1))\nmodel_fit = model.fit()\n# make prediction\nyhat = model_fit.predict(len(data_set), len(data_set), typ='levels')\nprint(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit an ARIMA model and plot residual errors\nfrom pandas import datetime\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom matplotlib import pyplot\n# load dataset\ndef parser(x):\n\treturn datetime.strptime('190'+x, '%Y-%m')\nseries = read_csv('shampoo-sales.csv', header=0, index_col=0, parse_dates=True, squeeze=True, date_parser=parser)\nseries.index = series.index.to_period('M')\n# fit model\nmodel = ARIMA(series, order=(5,1,0))\nmodel_fit = model.fit()\n# summary of fit model\nprint(model_fit.summary())\n# line plot of residuals\nresiduals = DataFrame(model_fit.resid)\nresiduals.plot()\npyplot.show()\n# density plot of residuals\nresiduals.plot(kind='kde')\npyplot.show()\n# summary stats of residuals\nprint(residuals.describe())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}