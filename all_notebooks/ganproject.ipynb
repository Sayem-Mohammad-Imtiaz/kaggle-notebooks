{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport os\nimport math\nfrom statistics import mean \n\nfrom tqdm.notebook import tqdm\n\n\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.utils import shuffle\n\nimport tensorflow as tf\nfrom keras import Input\nfrom keras.layers import Dense, Reshape, LeakyReLU, Conv2D, Conv2DTranspose, Flatten, Dropout, Concatenate, BatchNormalization\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam, RMSprop\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def read_data(image_dir, width, height, attribute_data, n_images):\n    images = []\n    attributes = []\n    for i in tqdm(range(n_images)):\n        image_id, current = attribute_data.iloc[i, 0], attribute_data.iloc[i, 1:]\n        current[current < 0] = 0\n        crop = (30, 55, 150, 175) #croping size for the image so that only the face at centre is obtained\n        image = np.array((Image.open(image_dir+image_id).crop(crop)).resize((width, height)))\n\n#\n        image = ((image - image.min())/(255 - image.min()))\n#         image = cv2.imread(image_dir+image_id)\n#         image = image[:, :, [2, 1, 0]]\n#         image = image[55:175, 30:150]\n#         image = cv2.resize(image, (width, height))\n        images.append(image)\n        attributes.append(current)\n\n#     for i in range(len(images)):\n#         images[i] = ((images[i] - images[i].min())/(255 - images[i].min()))\n#         #images[i] = images[i]*2-1  #uncomment this if activation is tanh for generator last layer\n    \n    images = np.array(images)\n    attributes = np.array(attributes)\n    return images, attributes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_images(images):\n    size = min(int(math.sqrt(len(images))), 5)\n    plt.figure(1, figsize=(10, 10))\n    for i in range(size**2):\n        plt.subplot(size, size, i+1)\n        plt.imshow(images[i])\n        plt.axis('off')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_real_samples(dataset, n_samples):\n    # split into images and labels\n    images, labels = dataset\n    # choose random instances\n    ix = np.random.randint(0, images.shape[0], n_samples)\n    # select images and labels\n    X, labels = np.array(images[ix],dtype='float32'), np.array(labels[ix], dtype='float32')\n    # generate class labels\n    y = np.ones((n_samples, 1))\n    return [X, labels], y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_latent_points(latent_dim, n_samples, n_attributes):\n    # generate points in the latent space\n    x_input = np.random.randn(latent_dim * n_samples)\n    # reshape into a batch of inputs for the network\n    z_input = x_input.reshape(n_samples, latent_dim)\n    # generate labels\n    labels = []\n    for _ in range(n_samples):\n        labels.append(np.random.randint(1, 2, n_attributes))\n    z_input, labels = np.array(z_input), np.array(labels, dtype=np.float32)\n    return [z_input, labels]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the generator to generate n fake examples, with class labels\ndef generate_fake_samples(generator, latent_dim, n_samples, n_attributes):\n\t# generate points in latent space\n\tz_input, labels_input = generate_latent_points(latent_dim, n_samples, n_attributes)\n\t# predict outputs\n\timages = generator.predict(z_input)\n\t# create class labels\n\ty = np.zeros((n_samples, 1))\n\treturn [images, labels_input], y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_generator(latent_dim = 100, channels = 3, n_attributes=2):\n    gen_input = Input(shape=(latent_dim, ))\n    #100,1\n    \n#     label_input = Input(shape=(n_attributes,))\n#     label_intermediate = Dense(4*4)(label_input)\n#     label_reshaped = Reshape((4,4,1))(label_intermediate)\n\n    x = Dense(512 * 4 * 4)(gen_input)\n    x = Reshape((4, 4, 512))(x)\n    \n#     merge = Concatenate()([x, label_reshaped])\n\n        \n    x = Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = BatchNormalization()(x)\n    #16 * 16 * 256\n\n    x = Conv2DTranspose(128, 4, strides=2, padding='same')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = BatchNormalization()(x)\n    #32*32*256\n\n    x = Conv2DTranspose(64, 4, strides=2, padding='same')(x)\n    x = LeakyReLU(alpha=0.2)(x)\n    x = BatchNormalization()(x)\n    #64*64*256\n\n    output = Conv2DTranspose(channels, 4, strides=2, padding='same', activation='sigmoid')(x)\n    #128*128*256\n\n#     x = Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n#     x = LeakyReLU()(x)\n    #256*256*256\n\n#     x = Conv2D(512, kernel_size=(3, 3), padding='same')(x)\n#     x = BatchNormalization()(x)\n#     x = LeakyReLU()(x)\n#     #256*256*512\n\n#     output = Conv2D(channels, 4, strides=2, padding='same', activation='sigmoid')(x)\n    #256*256*512\n\n#     output = Conv2D(channels, kernel_size=(3, 3), activation='relu', padding='same')(x)\n    #256*256*3\n\n    generator = Model(gen_input, output)\n\n    return generator\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_discriminator(height, width, channels, n_attributes=40):\n    disc_input = Input(shape=(height, width, channels))\n\n#     label_input = Input(shape=(n_attributes,))\n    \n#     n_nodes = height * width\n#     label_intermediate = Dense(n_nodes, activation='relu')(label_input)\n#     label_intermediate = Reshape((height,width,1))(label_intermediate)\n#     merge = Concatenate()([disc_input, label_intermediate])\n    \n    x = Conv2D(32, 4, strides=2, padding='same')(disc_input)\n   \n    x = Conv2D(64, 4, strides=2, padding='same')(x)\n    x = LeakyReLU(0.2)(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv2D(128, 4, strides=2, padding='same')(x)\n    x = LeakyReLU(0.2)(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv2D(256, 4, strides=2, padding='same')(x)\n    x = LeakyReLU(0.2)(x)\n    \n    x = Flatten()(x)\n    x = Dropout(0.4)(x)\n    \n    x = Dense(1, activation='sigmoid')(x)\n    discriminator = Model(disc_input, x)\n    \n    \n    \n    return discriminator\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_DIRECTORY = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'\nTOTAL_IMAGES = len(os.listdir(IMAGE_DIRECTORY))\nprint(TOTAL_IMAGES)\n\nWIDTH, HEIGHT = 128,128\nLATENT_DIM = 100\n\nattribute_data = pd.read_csv('../input/celeba-dataset/list_attr_celeba.csv')\nattribute_data = attribute_data[['image_id', 'Male', 'Young']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"noise_shape = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gen_input = Input(shape=(100, ))\n\nx = Dense(128 * 16 * 16)(gen_input)\nx = LeakyReLU()(x)\nx = Reshape((16, 16, 128))(x)\n\nx = Conv2D(256, 5, padding='same')(x)\nx = LeakyReLU()(x)\n\nx = Conv2DTranspose(256, 4, strides=2, padding='same')(x)\nx = LeakyReLU()(x)\n\nx = Conv2DTranspose(256, 4, strides=2, padding='same')(x)\nx = LeakyReLU()(x)\n\nx = Conv2DTranspose(256, 4, strides=2, padding='same')(x)\nx = LeakyReLU()(x)\n\nx = Conv2D(512, 5, padding='same')(x)\nx = LeakyReLU()(x)\nx = Conv2D(512, 5, padding='same')(x)\nx = LeakyReLU()(x)\nx = Conv2D(3, 7, activation='tanh', padding='same')(x)\n\ngenerator = Model(gen_input, x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"disc_input = Input(shape=(128, 128, 3))\n\nx = Conv2D(256, 3)(disc_input)\nx = LeakyReLU()(x)\n\nx = Conv2D(256, 4, strides=2)(x)\nx = LeakyReLU()(x)\n\nx = Conv2D(256, 4, strides=2)(x)\nx = LeakyReLU()(x)\n\nx = Conv2D(256, 4, strides=2)(x)\nx = LeakyReLU()(x)\n\nx = Conv2D(256, 4, strides=2)(x)\nx = LeakyReLU()(x)\n\nx = Flatten()(x)\nx = Dropout(0.4)(x)\n\nx = Dense(1, activation='sigmoid')(x)\ndiscriminator = Model(disc_input, x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" optimizer = RMSprop(\n        lr=.0001,\n        clipvalue=1.0,\n        decay=1e-8\n    )\ndiscriminator.compile(\n        optimizer=optimizer,\n        loss='binary_crossentropy'\n    )\ngan_input = Input(shape=(100, ))\ngan_output = discriminator(generator(gan_input))\ngan = Model(gan_input, gan_output)\n\noptimizer = RMSprop(lr=.0001, clipvalue=1.0, decay=1e-8)\ngan.compile(optimizer=optimizer, loss='binary_crossentropy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gan =Sequential([generator,discriminator])\n# discriminator.compile(optimizer='adam',loss='binary_crossentropy')\n# discriminator.trainable = False\n# gan.compile(optimizer='adam',loss='binary_crossentropy')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.keras.utils.plot_model(gan, show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images, attributes = read_data(IMAGE_DIRECTORY, WIDTH, HEIGHT, attribute_data, 10000)\nimages.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[first_generated, labels], y = generate_real_samples([images, attributes], 9)\nshow_images(first_generated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[first_generated2, labels2], y2 = generate_fake_samples(generator, LATENT_DIM, 9, n_attributes=2)\n\nfirst_generated2[0].max()\nshow_images(first_generated2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n    bat_per_epo = dataset[0].shape[0] // n_batch\n    half_batch = n_batch // 2\n    FILE_PATH = '%s/generated_%d.png'\n    RES_DIR = 'resources'\n    if not os.path.isdir(RES_DIR):\n        os.mkdir(RES_DIR)\n\n    print('STARTING TRAINING')\n    # manually enumerate epochs\n    CHANNELS = 3\n    discriminator_loss = []\n    gan_loss = []\n    CONTROL_SIZE_SQRT = 3\n    control_vectors = np.random.normal(size=(CONTROL_SIZE_SQRT**2, LATENT_DIM)) / 2\n    control_image = np.zeros((WIDTH * CONTROL_SIZE_SQRT, HEIGHT * CONTROL_SIZE_SQRT, CHANNELS))\n    control_generated = generator.predict(control_vectors)\n    for i in range(CONTROL_SIZE_SQRT ** 2):\n        x_off = i % CONTROL_SIZE_SQRT\n        y_off = i // CONTROL_SIZE_SQRT\n        control_image[x_off * WIDTH:(x_off + 1) * WIDTH, y_off * HEIGHT:(y_off + 1) * HEIGHT, :] = control_generated[i, :, :, :]  \n    plt.imshow(control_image)\n    plt.show()\n    for epoch in range(n_epochs):\n        temp1 = []\n        temp2 = []\n        print(bat_per_epo, dataset[0].shape[0], n_batch)\n        for j in range(bat_per_epo):\n            # get randomly selected 'real' samples\n            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n            # update discriminator model weights\n            d_model.trainable = True\n            y_real += .05 * np.random.random(y_real.shape)\n\n            d_loss1 = d_model.train_on_batch(X_real, y_real)\n            # generate 'fake' examples\n            [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch, 2)\n            y_fake += .05 * np.random.random(y_fake.shape)\n\n            # update discriminator model weights\n#             print(d_model.predict(X_fake).round(), y_fake)\n            d_loss2= d_model.train_on_batch(X_fake, y_fake)\n#             print(d_loss2, _)\n            d_model.trainable = False\n\n            # prepare points in latent space as input for the generator\n            [z_input, labels_input] = generate_latent_points(latent_dim, n_batch, 2)\n            # create inverted labels for the fake samples\n            y_gan = np.ones((n_batch, 1))\n            # update the generator via the discriminator's error\n            g_loss = gan_model.train_on_batch(z_input, y_gan)\n            \n            temp1.append(d_loss1 + d_loss2)\n            temp2.append(g_loss)\n            # summarize loss on this batch\n            print('>Epoch:%d, Batch:%d/%d, Real=%.3f, Fake=%.3f GAN=%.3f' % (epoch+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss), end='\\r')\n        discriminator_loss.append(mean(temp1))\n        gan_loss.append(mean(temp2))\n#         [temp, labels], y = generate_fake_samples(generator, LATENT_DIM, 9, n_attributes=2)\n#         show_images(temp)\n        control_image = np.zeros((WIDTH * CONTROL_SIZE_SQRT, HEIGHT * CONTROL_SIZE_SQRT, CHANNELS))\n        control_generated = generator.predict(control_vectors)\n        for i in range(CONTROL_SIZE_SQRT ** 2):\n            x_off = i % CONTROL_SIZE_SQRT\n            y_off = i // CONTROL_SIZE_SQRT\n            control_image[x_off * WIDTH:(x_off + 1) * WIDTH, y_off * HEIGHT:(y_off + 1) * HEIGHT, :] = control_generated[i, :, :, :]  \n        plt.imshow(control_image)\n        plt.show()\n        im = Image.fromarray(np.uint8(control_image * 255))\n        im.save(FILE_PATH % (RES_DIR, epoch))\n        [X_fake, labels], y_fake = generate_fake_samples(generator, 100, 1, 2)\n        print(discriminator.predict(X_fake))\n    # save the generator model\n    g_model.save('cgan_generator_base.h5')\n    return discriminator_loss, gan_loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator_loss, gan_loss = train(generator, discriminator, gan, [images, attributes], LATENT_DIM, 20, 32)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[temp, labels], y = generate_fake_samples(generator, LATENT_DIM, 9, n_attributes=2)\nshow_images(temp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discriminator.predict(first_generated)\nshow_images(first_generated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[X_fake, labels], y_fake = generate_fake_samples(generator, 100, 1, 2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(discriminator.predict([X_fake, labels]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(discriminator_loss)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(gan_loss)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import imageio\nimport shutil\n\nRES_DIR = 'resources'\n\nimages_to_gif = []\nfor filename in os.listdir(RES_DIR):\n    images_to_gif.append(imageio.imread(RES_DIR + '/' + filename))\nimageio.mimsave('trainnig_visual.gif', images_to_gif)\nshutil.rmtree(RES_DIR)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}