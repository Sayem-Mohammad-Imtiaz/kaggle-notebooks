{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score,f1_score,recall_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature dictionary"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = pd.read_excel(r'../input/employees-attrition-analysis/data_dictionary.xlsx')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(r'../input/employees-attrition-analysis/whole data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Null values are very few, we can drop them without affecting data set "},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Label encoding: to convert categorical values into continuous values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nencoder = preprocessing.LabelEncoder()\n\nfor i in df.columns:\n    if isinstance(df[i][0],str):\n        df[i] = encoder.fit_transform(df[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The data is imbalanced"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Attrition.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Attrition'], axis=1)\ny =df.Attrition","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=.2, random_state = 4589)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nScaler_X = StandardScaler()\nx_train = Scaler_X.fit_transform(x_train)\nx_test = Scaler_X.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(x_train, y_train)\nlr.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = lr.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_test,pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## As the data is imbalance, accuracy is might not be the best option for measuring performance.\n## Hence if we look at  F1 score and recall score, they are very low"},{"metadata":{},"cell_type":"markdown","source":"## Trying undersampling\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.reset_index(inplace=True)\nli = list(df[df.Attrition == 0].sample(n=2910).index)\ndf = df.drop(df.index[li])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=.2, random_state = 489)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nScaler_X = StandardScaler()\nx_train = Scaler_X.fit_transform(x_train)\nx_test = Scaler_X.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(x_train, y_train)\nlr.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lr.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_test,y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test,y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## After undersampling the F1 score and recall score dropped. So undersampling is not the best option."},{"metadata":{},"cell_type":"markdown","source":"## Using SMOTE: Synthetic Minority Oversampling Technique\n## SMOTE uses a nearest neighbors algorithm to generate new and synthetic data we can use for training our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Scaler_X = StandardScaler()\nscaled_X = Scaler_X.fit_transform(X)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.25, random_state=27)\n\nsm = SMOTE(random_state=27, sampling_strategy='auto')\nX_train, y_train = sm.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\nsmote_pred = lr.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, smote_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, smote_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_test, smote_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating intercept and coefficient table to see how features are related to target."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = X.columns.values\nsummary_table = pd.DataFrame(columns = ['Feature_names'], data = feature_names)\nsummary_table['coeff']= np.transpose(lr.coef_)\nsummary_table\n\nsummary_table.index = summary_table.index +1\nsummary_table.iloc[0]= ['Intercept', lr.intercept_[0]]\n\nsummary_table.sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can say that SMOTE is best for this imbalanced data set.\n### Now we need to select relevant features and see if we can increase the accuracy more or not!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#get correlations of each features in dataset\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(40,40))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.Over18.value_counts())\nprint(df.StandardHours.value_counts())\nprint(df.EmployeeCount.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping them as they are not relevant\ndf.drop(['StandardHours','EmployeeCount','EmployeeID','Over18'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['Attrition'], axis=1)\ny =df.Attrition","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating VIF\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nX_vif=add_constant(X)\n\npd.Series([variance_inflation_factor(X_vif.values, i) \n               for i in range(X_vif.shape[1])], \n              index=X_vif.columns)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get correlations of each features in dataset\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Columns we should drop:\n\n\n\n### 1) Job involvement: because its coefficient value is near 0 that means it does not have major effect on Attrition\n### 2) Age: as it has high correlation with many features such as total working years, years at company and it does not affect Attrition that much.\n### 3) Business travel the coefficient table shows that this feature has approximately zero effect on Attrition\n### 4) Performance Rating: as it is highly correlated to percent salary hike and has less significance, VIF is also high\n### 5) Years At company: it is correlated with years with current manager and age hence dropping it.\n### 6) Stock Option level: approximately zero effect on Attrition\n### 7) Distance from home: approximately zero effect on Attrition\n### 8) Education: approximately zero effect on Attrition\n### 9) Gender: approximately zero effect on Attrition\n### 10) Department: approximately zero effect on Attrition"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.drop(['JobInvolvement','Age','BusinessTravel','PerformanceRating','YearsAtCompany','DistanceFromHome', 'StockOptionLevel'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.drop(['Education','Gender','JobRole','Department'],inplace =True, axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Scaler_X = StandardScaler()\nscaled_X = Scaler_X.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.25, random_state=27)\n\nsm = SMOTE(random_state=27, sampling_strategy='auto')\nX_train, y_train = sm.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n\nsmote_pred = lr.predict(X_test)\n\n# Checking accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_score(y_test, smote_pred) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1_score(y_test, smote_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_test, smote_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## recall and F1 increased significantly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = X.columns.values\nsummary_table = pd.DataFrame(columns = ['Feature_names'], data = feature_names)\nsummary_table['coeff']= np.transpose(lr.coef_)\nsummary_table\n\nsummary_table.index = summary_table.index +1\nsummary_table.iloc[0]= ['Intercept', lr.intercept_[0]]\n\n\n\n\nsummary_table.sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Environment Satisfaction, Job satisfaction, Marital Status, Total working years, Years since last promotion, Years with current managers are some important features to take into consideration if company wants to reduce its attrition rate."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}