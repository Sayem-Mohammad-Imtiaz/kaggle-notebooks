{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Shared Bikes Sales analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Problem statement\n- A US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic.\n- BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19.\n- The company wants to know:\n    1. Which variables are significant in predicting the demand for shared bikes.\n    2. How well those variables describe the bike demands","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Solution","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing and understanding the available data\n- Import the data \n- Clean the data and create the derived data which can give more insights\n- Visualize the demand with different variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/boombikes/day.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dropping variables which are not required for evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# The below variables are not useful in the analysis\n# We have temp and atemp which are highly correlated. So, removing temp variable from the analysis\n\n# Also, we should not have casual and registered since they are directly used in the cnt calculation\n# cnt = casual + registered, so both these values will directly affect the calculation\nredundant_variables = ['instant', 'dteday', 'temp', 'casual', 'registered']\n\ndf = df.drop(redundant_variables, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now we can visualise the relationship of our target variable with other variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.pairplot(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Among all the continous variables, an interesting relationship of cnt is seen with atemp. Let's create a seperate regplot for atemp vs cnt","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x='atemp', y='cnt', data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph clearly shows that with the increase in temperature cnt increases too.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A lot of variables are categorical. It's a better idea to use boxplot for visualizing the pattern across the categories.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nplt.subplot(3,3,1)\nsns.boxplot(x='season', y='cnt', data=df)\nplt.subplot(3,3,2)\nsns.boxplot(x='yr', y='cnt', data=df)\nplt.subplot(3,3,3)\nsns.boxplot(x='mnth', y='cnt', data=df)\nplt.subplot(3,3,4)\nsns.boxplot(x='holiday', y='cnt', data=df)\nplt.subplot(3,3,5)\nsns.boxplot(x='weekday', y='cnt', data=df)\nplt.subplot(3,3,6)\nsns.boxplot(x='workingday', y='cnt', data=df)\nplt.subplot(3,3,7)\nsns.boxplot(x='weathersit', y='cnt', data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Influence of different variables to demand - \n1. Seasons - There is quite a lot of variation with season. Spring seems to have significantly lesser demand while summer and fall have higher demands.\n2. Year - 2019 clearly has a much higher demand. This clearly shows the business has a good prospect and it was witnessing positive trend in demand before the Covid scene\n3. Month - This is again a detailed view of seasons and we can clearly see the same pattern that months corresponding to summer and fall have higher demands.\n4. Holidays - The demand seems to be higher on non-holidays as we can see significant difference in the median value with respect to holidays.\n5. Weekday - Although we see difference in the range of values across the weekdays but the median seems to be close for all days. So, in first glance weekday does not seem to be affecting the demand to a great extent.\n6. Working day - Working days have little higher 25 percentile however the median are fairly close.\n7. Weathersit - There is very significant influence of weather on the demand. As we can see, the demand is much more during clear weather and reduces when it is misty or cloudy. It reduces even more during rains and there is almost no demand when it is snowy.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's also have a look at the correlations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot=True, cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The heatmap shows high positive correlation of `cnt` with `atemp` and `yr`.\n- `cnt` has significant negative correlation with `windspeed` and `weathersit`  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have to first create dummy columns for our analysis\n#season : season (1:spring, 2:summer, 3:fall, 4:winter)\nseason_dict = { \n    1: 'Spring',\n    2: 'Summer',\n    3: 'Fall',\n    4: 'Winter'\n}\n# Months\nmonth_dict = {\n    1: 'Jan',\n    2: 'Feb',\n    3: 'Mar',\n    4: 'Apr',\n    5: 'May',\n    6: 'Jun',\n    7: 'Jul',\n    8: 'Aug',\n    9: 'Sep',\n    10: 'Oct',\n    11: 'Nov',\n    12: 'Dec'\n}\n# weathersit : \n# 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n# 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n# 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n# 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\nweathersit_dict = {\n    1: 'Clear',\n    2: 'Mist',\n    3: 'Light_Snow',\n    4: 'Heavy_Rain'\n}\nweekday_dict = {\n    0: 'Sun',\n    1: 'Mon',\n    2: 'Tue',\n    3: 'Wed',\n    4: 'Thu',\n    5: 'Fri',\n    6: 'Sat',\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Coverting the categorical columns to strings since pd.get_dummies takes object as parameter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.weathersit = df.weathersit.apply(lambda x: weathersit_dict[x])\ndf.weathersit.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.mnth = df.mnth.apply(lambda x: month_dict[x])\ndf.season = df.season.apply(lambda x: season_dict[x])\ndf.weekday = df.weekday.apply(lambda x: weekday_dict[x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, that we have named columns, it's easiar to comprehend the variation of these categories with `cnt` which we had done earlier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,12))\nplt.subplot(4,1,1)\nsns.boxplot(x='season', y='cnt', data=df)\nplt.subplot(4,1,2)\nsns.boxplot(x='weathersit', y='cnt', data=df)\nplt.subplot(4,1,3)\nsns.boxplot(x='mnth', y='cnt', data=df)\nplt.show()\nsns.boxplot(x='weekday', y='cnt', data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"month_dummies = pd.get_dummies(df.mnth, drop_first = True)\nseason_dummies = pd.get_dummies(df.season, drop_first = True)\nweathersit_dummies = pd.get_dummies(df.weathersit, drop_first = True)\nweekday_dummies = pd.get_dummies(df.weekday, drop_first = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df, month_dummies, season_dummies, weathersit_dummies, weekday_dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now remove the original columns for which we have the dummies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['mnth', 'weekday', 'weathersit', 'season'], axis=1, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting the Data into Training and Testing Sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(df, train_size = 0.7, test_size = 0.3, random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Rescaling the Features\nWe need to rescale all the features that are non-binary and not dummies so that we perform analysis on the variable coefficents correctly. We will also exclude the target variable since we are interested in the predictor variables coeffecients.\n- Using MinMaxScaler here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_vars = ['atemp', 'hum', 'windspeed']\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dividing the train dataset into the target and predictor variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.pop('cnt')\nX_train = df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using RFE to filter narrow down to first 15 features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running RFE with the output number of the variable equal to 10\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             # running RFE\nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting top15 columns\ntop15_cols = X_train.columns[rfe.support_]\n\ntop15_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# following columns are dropped from analysis\nX_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting model summary","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[top15_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \nimport statsmodels.api as sm  \nX_train_rfe = sm.add_constant(X_train_rfe)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm = sm.OLS(y_train,X_train_rfe).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_rfe = X_train_rfe.drop(['const'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping hum since it's highly collinear VIF value is very high.\nX_train_new = X_train_rfe.drop(['hum'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rebuilding the model\nX_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train,X_train_lm).fit()\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"R-Squared value for the model is pretty good. Also, all the predictor variables are significant since the P-value is very less.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check for collinearity again in the new model\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`atemp` has a high VIF. We can drop it. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop(['atemp'], axis = 1)\n# Rebuilding the model\nX_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train,X_train_lm).fit()\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping July now.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop(['Jul'], axis = 1)\n# Rebuilding the model\nX_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train,X_train_lm).fit()\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping Winter","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop(['Winter'], axis = 1)\n# Rebuilding the model\nX_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train,X_train_lm).fit()\nprint(lm.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- All the variables are significant now.\n- Let's check the VIF now","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's check for collinearity again in the new model\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we have a fairly stable model now with very less multi-collinearity and a good Adjusted R-Squared value of 0.789","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Residual analysis on the training data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_cnt = lm.predict(X_train_lm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nsns.distplot((y_train - y_train_cnt), bins = 50)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[num_vars] = scaler.transform(df_test[num_vars])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dividing into X_test and y_test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = df_test.pop('cnt')\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using only the filtered columns present in X_train_new\nX_test_new = X_test[X_train_new.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions\ny_pred = lm.predict(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's compute the final r2_score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusions and recommendations to the bike company","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Statistics","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lm.summary())\n# Total variables considered - 11","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Recommendations:\n- Yr has a positive coefficient. So, the demand is increasing with each year. This gives confidence that the business model is has potential to have high demand in future after the Covid problems are over.\n- The demand is more on the working days and Saturdays. So, in order to boost demand on non-working days they can come up with some offer for those days (espcially Sunday).\n- Bad weather(snow and mist) and winters adversely affect the demand. In fact, demand is low uptil Spring. So, the company needs to come up with some strategy and offers during this period.\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}