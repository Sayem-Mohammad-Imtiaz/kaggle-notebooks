{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#https://towardsdatascience.com/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\nEMBED_DIM = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\",encoding = \"utf-8\")\ndf[\"sentiment\"] = (df[\"sentiment\"]==\"positive\").astype(\"int8\")\nx_train = df[\"review\"].iloc[:25000].values\ny_train = df[\"sentiment\"].iloc[:25000].values\nx_test = df[\"review\"].iloc[25000:].values\ny_test = df[\"sentiment\"].iloc[25000:].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer()\ntotal_reviews = x_train + x_test\ntokenizer.fit_on_texts(total_reviews)\nmax_length = max([len(review.split()) for review in total_reviews])\nx_train_tokens = tokenizer.texts_to_sequences(x_train)\nx_test_tokens = tokenizer.texts_to_sequences(x_test)\nvocab_size = len(tokenizer.word_index) + 1\nx_train_pad = pad_sequences(x_train_tokens,maxlen=max_length,padding=\"post\")\nx_test_pad = pad_sequences(x_test_tokens,maxlen=max_length,padding=\"post\")\nprint(\"max_len:%d,vocab_size:%d\"%(max_length,vocab_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(\"test:\",x_test[0])\n# print(\"train:\",x_train[0])\n# print(\"+:\",(x_test+x_train)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nprint(\"punctuation:\",string.punctuation)\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nprint(\"stopwords:\",set(stopwords.words(\"english\")))\nlines = df[\"review\"].values.tolist()\nstop_words = set(stopwords.words(\"english\"))\nreviews = list()\nfor line in lines:\n    tokens = word_tokenize(line)\n    tokens = [w.lower() for w in tokens]\n    table = str.maketrans(\"\",\"\",string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n    words = [w for w in stripped if w.isalpha()]\n    words = [w for w in words if w not in stop_words]\n    reviews.append(words)\nlen(reviews)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\nmodel = gensim.models.Word2Vec(sentences=reviews,size=EMBED_DIM,window=5,workers=4,min_count=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words=list(model.wv.vocab)\nprint(\"vocabulary size:\",len(words))\nmodel.wv.get_vector(words[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(reviews)\nseqs = tokenizer.texts_to_sequences(reviews)\nreview_pad = pad_sequences(seqs,maxlen=max_length,padding=\"post\")\nword_index = tokenizer.word_index\nsentiments = df[\"sentiment\"].values\n\nnum_words = len(word_index)+1\nembedding_matrix = np.zeros((num_words,EMBED_DIM))\nfor word,i in word_index.items():\n    vector = model.wv.get_vector(word)\n    if vector is not None:\n        embedding_matrix[i] = vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,GRU,Flatten,Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\nfrom keras.layers.convolutional import Conv1D,MaxPooling1D\n\nmodel_main = Sequential()\nmodel_main.add(Embedding(num_words,EMBED_DIM,input_length=max_length))#,embeddings_initializer=Constant(embedding_matrix),trainable=False))\nmodel_main.add(LSTM(32,dropout=0.2,recurrent_dropout=0.2))\nmodel_main.add(Dense(32,activation=\"relu\"))\nmodel_main.add(Dense(1,activation=\"sigmoid\"))\nmodel_main.compile(loss=\"binary_crossentropy\",optimizer = \"adam\",metrics=[\"acc\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_main.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_main.fit(review_pad,sentiments,batch_size=128,epochs=10,validation_split=0.25)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}