{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imports statements.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport multiprocessing\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom collections import defaultdict\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom IPython.display import display\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def display_all_details(dataframe):\n    print(('='*50)+'DATA'+('='*50))\n    print(('-'*50)+'SHAPE'+('-'*50))\n    print(dataframe.shape)\n    print(('-'*50)+'COLUMNS'+('-'*50))\n    print(dataframe.columns)\n    print(('-'*50)+'DESCRIBE'+('-'*50))\n    print(dataframe.describe())\n    print(('-'*50)+'INFO'+('-'*50))\n    print(dataframe.info())\n    print(('='*50)+'===='+('='*50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_tweets_data = pd.read_csv('../input/covid19-tweets/covid19_tweets.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_tweets_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_tweets_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_all_details(covid19_tweets_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_tweets_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_missing_values_info(df):\n    missing_values_count_df = df.isnull().sum()\n    print(('='*50)+'DATA WITH MISSING VALUES'+('='*50))\n    print(missing_values_count_df[missing_values_count_df>0])\n    print(('='*50)+'DATA WITHOUT MISSING VALUES'+('='*50))\n    print(missing_values_count_df[missing_values_count_df==0])\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display_missing_values_info(covid19_tweets_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there is no target class, it means we have to go with unsupervised techniques to classify the text into sentiments.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for tweets in covid19_tweets_data.text.head(20):\n    print(tweets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for tweet in covid19_tweets_data.text:\n    link = re.search(\"(?P<url>https?://[^\\s]+)\", myString).group(\"url\")\n    if link!=None:\n        covid19_tweets_data['links'] = link\n    else:\n        covid19_tweets_data['links'] = pd.NA\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_tweets_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_tweets_data.links.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text_column(row):\n    text = row['text'].lower()\n    text = re.sub(\"(?P<url>https?://[^\\s]+)\",'',text)\n    text = re.sub(r'[^(a-zA-Z\\s)]','',text)\n    text = re.sub(r'\\(','',text)\n    text = re.sub(r'\\)','',text)\n    text = text.replace('\\n',' ')\n    text = text.strip()\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_tweets_data['cleaned_text'] = covid19_tweets_data.apply(clean_text_column,axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for tweets in covid19_tweets_data.cleaned_text.head(20):\n    print(tweets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_tweets_data.cleaned_text.str.isspace().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_tweets_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_tweets_data.drop(covid19_tweets_data[covid19_tweets_data['cleaned_text'].str.isspace()==True].index,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_tweets_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create Word2Vec using cleaned text vocabulary.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sent = [row for row in covid19_tweets_data.cleaned_text]\nphrases = Phrases(sent, min_count=1, progress_per=50000)\nbigram = Phraser(phrases)\nsentences = bigram[sent]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_sentences = []\nfor tweet in sentences:\n    filtered_sentences.append(remove_stopwords(tweet))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_sentences_2 = []\nfor tweet in filtered_sentences:\n    filtered_sentences_2.append(re.sub(r'\\b\\w{1,2}\\b', '',tweet))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filtered_sentences_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model = Word2Vec(min_count=3,\n                     window=4,\n                     size=200,\n                     sample=1e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=multiprocessing.cpu_count()-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.build_vocab(filtered_sentences_2, progress_per=50000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.train(filtered_sentences_2, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.init_sims(replace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w2v_model.save(\"word2vec.model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now Word2Vec model is trained and saved","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectors = Word2Vec.load(\"./word2vec.model\").wv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(model.cluster_centers_[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectors.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_vectors.similar_by_vector(model.cluster_centers_[1], topn=10, restrict_vocab=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"positive_cluster_center = model.cluster_centers_[0]\nnegative_cluster_center = model.cluster_centers_[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = pd.DataFrame(word_vectors.vocab.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words.columns = ['words']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwords['vectors'] = words.words.apply(lambda x: word_vectors.wv[f'{x}'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words.vectors[0].dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwords['cluster'] = words.vectors.apply(lambda x: model.predict(np.array([x])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words.cluster","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwords.cluster = words.cluster.apply(lambda x: x[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words.cluster.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words['cluster_value'] = [1 if i==0 else -1 for i in words.cluster]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nwords['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words['sentiment_coeff'] = words.closeness_score * words.cluster_value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_map = pd.read_csv('./sentiment_dictionary.csv')\nsentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiment_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\ntfidf.fit(covid19_tweets_data.cleaned_text)\nfeatures = pd.Series(tfidf.get_feature_names())\ntransformed = tfidf.transform(covid19_tweets_data.cleaned_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'covid' in features.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_tfidf_dictionary(x, transformed_file, features):\n    '''\n    create dictionary for each input sentence x, where each word has assigned its tfidf score\n    \n    inspired  by function from this wonderful article: \n    https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n    \n    x - row of dataframe, containing sentences, and their indexes,\n    transformed_file - all sentences transformed with TfidfVectorizer\n    features - names of all words in corpus used in TfidfVectorizer\n\n    '''\n    vector_coo = transformed_file[x.name].tocoo()\n    vector_coo.col = features.iloc[vector_coo.col].values\n    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n    return dict_from_coo\n\ndef replace_tfidf_words(x, transformed_file, features):\n    '''\n    replacing each word with it's calculated tfidf dictionary with scores of each word\n    x - row of dataframe, containing sentences, and their indexes,\n    transformed_file - all sentences transformed with TfidfVectorizer\n    features - names of all words in corpus used in TfidfVectorizer\n    '''\n    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n    return list(map(lambda y:dictionary[f'{y}'], x.cleaned_text.split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replaced_tfidf_scores = covid19_tweets_data.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_sentiment_words(word, sentiment_dict):\n    '''\n    replacing each word with its associated sentiment score from sentiment dict\n    '''\n    try:\n        out = sentiment_dict[word]\n    except KeyError:\n        out = 0\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replaced_closeness_scores = covid19_tweets_data.cleaned_text.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"covid19_tweets_data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, covid19_tweets_data.cleaned_text]).T\nreplacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence']\nreplacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\nreplacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')\n#replacement_df['sentiment'] = [1 if i==1 else 0 for i in replacement_df.sentiment]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replacement_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"replacement_df.prediction.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}