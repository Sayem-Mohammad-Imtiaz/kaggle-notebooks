{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/covid19-case-surveillance-public-use-dataset/COVID-19_Case_Surveillance_Public_Use_Data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping the date columns and then the null values from dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['pos_spec_dt','onset_dt'],axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colums = ['current_status', 'sex', 'age_group', 'Race and ethnicity (combined)', 'hosp_yn','icu_yn', 'death_yn', 'medcond_yn']\nfor col in colums:\n    print(col)\n    print(df[colums].value_counts())\n    print(\"______________________\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unique values in data\ndf.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some Interesting Insights from Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,\n                      wspace=0.5, hspace=0.2)\nplt.subplot(141)\nplt.title('medcond_yn ',fontsize = 20)\ndf['medcond_yn'].value_counts().plot.pie(autopct=\"%1.1f%%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,\n                      wspace=0.5, hspace=0.2)\nplt.subplot(141)\nplt.title('death_yn',fontsize = 20)\ndf['death_yn'].value_counts().plot.pie(autopct=\"%1.1f%%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,\n                      wspace=0.5, hspace=0.2)\nplt.subplot(141)\nplt.title('hosp_yn',fontsize = 20)\ndf['hosp_yn'].value_counts().plot.pie(autopct=\"%1.1f%%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,\n                      wspace=0.5, hspace=0.2)\nplt.subplot(141)\nplt.title('icu_yn',fontsize = 20)\ndf['icu_yn'].value_counts().plot.pie(autopct=\"%1.1f%%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above pie charts we can see there are lot of missing data to conclude."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,wspace=0.5, hspace=0.2)\nplt.subplot(141)\nplt.title('Race and ethnicity (combined)',fontsize = 20)\ndf['Race and ethnicity (combined)'].value_counts().plot.pie(autopct=\"%1.1f%%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pie chart on the basis of Ethnicity/Race"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,10))\nplt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9,\n                      wspace=0.5, hspace=0.2)\nplt.subplot(141)\nplt.title('current_status',fontsize = 20)\ndf['current_status'].value_counts().plot.pie(autopct=\"%1.1f%%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.countplot(y=\"current_status\",hue ='sex',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Medical condition with respect to gender, while we have lot of missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.countplot(y=\"medcond_yn\",hue ='sex',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Death ratio of male and female are same, while missing rate is highest"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.countplot(y=\"death_yn\",hue ='sex',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age group from 20-29 has the highest count"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.countplot(y=\"age_group\",data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age Group with respect to their gender."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.countplot(y=\"age_group\",hue ='sex',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.catplot(x=\"age_group\", hue=\"sex\", col=\"current_status\", data=df, kind=\"count\", height=5, aspect=.7);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Start Date:\", df['cdc_report_dt'].min())\nprint(\"End Date:\", df['cdc_report_dt'].max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['age_group'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation For Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['sex'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting categorical feature to numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Converting a categorical feature\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=0 and male=1 and so on."},{"metadata":{"trusted":true},"cell_type":"code","source":"# mapp = {'Female':1,'Male':2,'Unknown':3,'Missing':4,'Other':5}\n# data['sex'] = data['sex'].apply(lambda x:mapp[x])\n\ndata['sex'] = data['sex'].map({'Female':0,'Male':1,'Unknown':2,'Missing':3,'Other':4})\nprint(data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"current_status\"] = lb_make.fit_transform(data[\"current_status\"])\ndata[\"hosp_yn\"] = lb_make.fit_transform(data[\"hosp_yn\"])\ndata[\"icu_yn\"] = lb_make.fit_transform(data[\"icu_yn\"])\ndata[\"death_yn\"] = lb_make.fit_transform(data[\"death_yn\"])\ndata[\"medcond_yn\"] = lb_make.fit_transform(data[\"medcond_yn\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X =  data[['current_status','hosp_yn','icu_yn','medcond_yn','sex']]\ny = data['death_yn']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building with Logistic Regression and Decision Tree:"},{"metadata":{},"cell_type":"markdown","source":"Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nacc_log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.\n\nThe model confidence score is the highest among models evaluated so far."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Model evaluation\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Logisitic Regression score the different, we choose to use Decision Tree, habit of overfitting to their training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Decision Tree'],\n    'Score': [ acc_log, acc_decision_tree]})\nsorted_model=models.sort_values(by='Score', ascending=False)\nsorted_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nfig = plt.bar(sorted_model['Model'], sorted_model['Score'],color='aqua')\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Can Try with below Models as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Logistic Regression\n\n# logreg = LogisticRegression()\n# logreg.fit(X_train, Y_train)\n# Y_pred = logreg.predict(X_test)\n# acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n# acc_log\n\n# # Support Vector Machines\n\n# svc = SVC()\n# svc.fit(X_train, Y_train)\n# Y_pred = svc.predict(X_test)\n# acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n# acc_svc\n\n\n# knn = KNeighborsClassifier(n_neighbors = 3)\n# knn.fit(X_train, Y_train)\n# Y_pred = knn.predict(X_test)\n# acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n# acc_knn\n\n# # Gaussian Naive Bayes\n\n# gaussian = GaussianNB()\n# gaussian.fit(X_train, Y_train)\n# Y_pred = gaussian.predict(X_test)\n# acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n# acc_gaussian\n\n\n# # Perceptron\n\n# perceptron = Perceptron()\n# perceptron.fit(X_train, Y_train)\n# Y_pred = perceptron.predict(X_test)\n# acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n# acc_perceptron\n\n\n\n# # Linear SVC\n\n# linear_svc = LinearSVC()\n# linear_svc.fit(X_train, Y_train)\n# Y_pred = linear_svc.predict(X_test)\n# acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n# acc_linear_svc\n\n\n# # Stochastic Gradient Descent\n\n# sgd = SGDClassifier()\n# sgd.fit(X_train, Y_train)\n# Y_pred = sgd.predict(X_test)\n# acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n# acc_sgd\n\n\n# # Decision Tree\n\n# decision_tree = DecisionTreeClassifier()\n# decision_tree.fit(X_train, Y_train)\n# Y_pred = decision_tree.predict(X_test)\n# acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n# acc_decision_tree\n\n\n# # Random Forest\n\n# random_forest = RandomForestClassifier(n_estimators=100)\n# random_forest.fit(X_train, Y_train)\n# Y_pred = random_forest.predict(X_test)\n# random_forest.score(X_train, Y_train)\n# acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n# acc_random_forest\n\n# models = pd.DataFrame({\n#     'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n#               'Random Forest', 'Naive Bayes', 'Perceptron', \n#               'Stochastic Gradient Decent', 'Linear SVC', \n#               'Decision Tree'],\n#     'Score': [acc_svc, acc_knn, acc_log, \n#               acc_random_forest, acc_gaussian, acc_perceptron, \n#               acc_sgd, acc_linear_svc, acc_decision_tree]})\n# sorted_model=models.sort_values(by='Score', ascending=False)\n# sorted_model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# submission = pd.DataFrame({\n#         \"PatientId\": test_df[\"patient\"],\n#         \"death_yn\": Y_pred\n#     })\n# submission.to_csv('submission2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Please upvote, if it helps :)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}