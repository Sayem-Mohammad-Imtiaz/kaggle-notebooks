{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Your own Loss Function\nIn this Notebook, we will look at how can we write our very own loss functions from scratch. I will be using keras framework for this purpose. You Guys can implement this notebook in other frameworks ðŸ˜‰ \n\nFirst we will look at how some standard loss functions are written, like binary cross entropy, categorical cross entropy etc, then how can we write a arbitary loss function based on some mathematical formula.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport keras \nimport keras.backend as K\nimport tensorflow as tf\nimport math","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Binary Cross Entropy","metadata":{}},{"cell_type":"markdown","source":"Binary Cross Entropy is defined as: y * log(p) + (1-y) * log(1-p)","metadata":{}},{"cell_type":"code","source":"def naive_bce(y_true,y_pred):\n    loss = (-1)* (y_true * K.log(y_pred) + (1-y_true)*K.log(1- y_pred))\n    return K.mean(loss)\n\n\n\ndef bce_numpy_equivalent(y_true,y_pred):     # Important Note :- numpy equivalent Functions are there for understanding only, they shouldn't be passed to the models!!!!!\n    y_true=np.asarray(y_true).flatten()\n    y_pred=np.asarray(y_pred).flatten()\n    loss=0\n    for i in range(len(y_true)):\n        loss+= y_true[i]*(math.log(y_pred[i]))+(1-y_true[i])*math.log(1-y_pred[i])\n    loss*=-1\n    return loss/len(y_true)\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now as y_pred approaches 1 or more, 1-y_pred becomes negative and hence k.log(1-y_pred) -> Nan. Similarly when y_pred is 0, k.log(y_pred)-> Nan","metadata":{}},{"cell_type":"code","source":"y_true=K.variable([[0, 1],[0, 0]])    # batch size = 2\ny_pred=K.variable([[0.43, 0.51],[0.32, 0.49]])\nnaive_bce(y_true,y_pred).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bce_numpy_equivalent(y_true,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our bce function works!!!!!","metadata":{}},{"cell_type":"code","source":"# Comparing with Keras BCE\nBCE=keras.losses.BinaryCrossentropy(from_logits=False)\nBCE(y_true,y_pred).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical Cross entropy","metadata":{}},{"cell_type":"markdown","source":"Categorical Cross entropy can be defined as âˆ’  (y[i] * log(y_pred[i])) for all i in 1 to n_class\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.python.ops import clip_ops\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.ops import math_ops","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cce(y_true,y_pred):\n    if len(y_true.shape)==2:     #batch_size=1\n        samples=y_true.shape[0]\n        batch=1\n    else:                        #batch_size>1  \n        samples=y_true.shape[1]\n        batch=y_true.shape[0]\n    loss=(-1)*(y_true*K.log(y_pred))\n    loss=(math_ops.reduce_sum(loss))/(samples*batch)\n    return loss\n\n\ndef cce_numpy_equivalent(y_true,y_pred):\n    if len(y_true.shape)==2:     #batch_size=1\n        samples=y_true.shape[0]\n        batch=1\n    else:                        #batch_size>1  \n        samples=y_true.shape[1]\n        batch=y_true.shape[0]\n    y_true=np.asarray(y_true).flatten()\n    y_pred=np.asarray(y_pred).flatten()\n    loss=0\n    for i in range(len(y_true)):\n        loss+= y_true[i]*(math.log(y_pred[i]))\n    loss*=-1\n    return loss/(samples*batch)\n    \n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true=K.variable([[[0, 1, 0], [0, 0, 1]],[[0, 1, 0], [0, 0, 1]]])    # batch size = 2\ny_pred=K.variable([[[0.05, 0.9499, 0.0001], [0.1, 0.8, 0.1]],[[0.05, 0.9499, 0.0001], [0.1, 0.8, 0.1]]])\ncce(y_true,y_pred).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cce_numpy_equivalent(y_true,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparing with Keras CCE\nCCE=keras.losses.CategoricalCrossentropy(from_logits=False)\nCCE(y_true,y_pred).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mean Squared Eroor","metadata":{}},{"cell_type":"code","source":"def mse(y_true,y_pred):\n    loss = K.mean(K.square(y_true - y_pred), axis=-1)\n    return K.mean(loss)\n\ndef mse_numpy_equivalent(y_true,y_pred):\n    y_true=np.asarray(y_true).flatten()\n    y_pred=np.asarray(y_pred).flatten()\n    loss=0\n    for i in range(len(y_true)):\n        loss+=(y_true[i]-y_pred[i])*(y_true[i]-y_pred[i])\n    return loss/len(y_true)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = K.variable([[0., 1.], [0., 0.]])\ny_pred =K.variable( [[1., 1.], [1., 0.]])\nmse(y_true,y_pred).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mse_numpy_equivalent(y_true,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparing with Keras MSE\nMSE = keras.losses.MeanSquaredError()\nMSE(y_true,y_pred).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mean Absolute Eroor","metadata":{}},{"cell_type":"code","source":"def mae(y_true,y_pred):\n    loss = K.mean(abs(y_true - y_pred), axis=-1)\n    return K.mean(loss)\n\ndef mae_numpy_equivalent(y_true,y_pred):\n    y_true=np.asarray(y_true).flatten()\n    y_pred=np.asarray(y_pred).flatten()\n    loss=0\n    for i in range(len(y_true)):\n        loss+=abs(y_true[i]-y_pred[i])\n    return loss/len(y_true)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = K.variable([[0., 1.], [0., 0.]])\ny_pred =K.variable( [[1., 1.], [1., 0.]])\nmae(y_true,y_pred).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mae_numpy_equivalent(y_true,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparing with Keras MAE\nMAE = keras.losses.MeanAbsoluteError()\nMAE(y_true,y_pred).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom Loss Functions","metadata":{}},{"cell_type":"markdown","source":"The code for the Custom Loss function depends first and foremost on its definition. So for writing a custom loss function its mathematical definition should be pretty damn clear.","metadata":{}},{"cell_type":"markdown","source":"Say we define Our Loss Function as mean ([ y^3 + p^3 ] - [ y^2 + p^2 ] -[ y + p ]) .\nWe can take any other mathematical definition too, that simply depends upon the task you have to perform.","metadata":{}},{"cell_type":"code","source":"def custom_loss(y_true, y_pred):\n    loss=K.mean((pow(y_true,3)+pow(y_pred,3))-(pow(y_true,2)+pow(y_pred,2))-(pow(y_true,1)+pow(y_pred,1)))\n    return K.mean(loss)\n\n\n\ndef custom_loss_numpy_equivalent(y_true, y_pred):\n    y_true=np.asarray(y_true).flatten()\n    y_pred=np.asarray(y_pred).flatten()\n    loss=0\n    for i in range(len(y_true)):\n        loss+=(pow(y_true[i],3)+pow(y_pred[i],3)) - (pow(y_true[i],2)+pow(y_pred[i],2)) -(pow(y_true[i],1)+pow(y_pred[i],1))\n    return loss/len(y_true)\n    \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = K.variable([[0., 1.], [0., 0.]])\ny_pred =K.variable( [[1., 1.], [1., 0.]])\ncustom_loss(y_true,y_pred).numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_loss_numpy_equivalent(y_true, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using Our loss Functions In a Deep Learning Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\ndata = pd.read_csv('../input/oranges-vs-grapefruit/citrus.csv')\ncol=data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target=[]\nfor i in range(len(data)):\n    if data['name'][i]=='orange':\n        target.append(0)\n    else :\n        target.append(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target=np.asarray(target).astype(np.float32)\nnp.unique(target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=data[col[1:]]\nX.head()\nX=np.asarray(X).astype(np.float32)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model():\n    model = Sequential()\n    model.add(Dense(30,input_dim=5,activation='relu'))\n    model.add(Dense(5,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss=naive_bce, optimizer='adam', metrics=['accuracy'])\n    return model\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=create_model()\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train,y_train,epochs=3,batch_size=128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see as y_pred is reaching 0 or 1, so the loss is becoming *nan* , to combat this we can use alternative implementation of bce ","metadata":{}},{"cell_type":"code","source":"def bce(y_true,y_pred):\n    loss = K.max(y_pred,0)-y_pred * y_true + K.log(1+K.exp((-1)*K.abs(y_pred)))\n    return K.mean(loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_modelv2():\n    model = Sequential()\n    model.add(Dense(30,input_dim=5,activation='relu'))\n    model.add(Dense(5,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss=bce, optimizer='adam', metrics=['accuracy'])\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelv2=create_modelv2()\nmodelv2.fit(X_train,y_train,epochs=3,batch_size=128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelv2.evaluate(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparing it with Standard Keras BCE Loss Function\nWe expect a lower acccuracy cause of the alternate implementation as it doesn't give same loss valuse as standard keras BCE loss function","metadata":{}},{"cell_type":"code","source":"def create_model2():\n    model = Sequential()\n    model.add(Dense(30,input_dim=5,activation='relu'))\n    model.add(Dense(5,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2=create_model2()\nmodel2.fit(X_train,y_train,epochs=3,batch_size=128)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.evaluate(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Regression Example","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.datasets import make_regression\nfrom matplotlib import pyplot\n\nX, y = make_regression(n_samples=10000, n_features=10,  n_targets=1, bias=0.1, noise=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model2():\n    model = Sequential()\n    model.add(Dense(30,input_dim=10,activation='relu'))\n    model.add(Dense(5,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss=mse, optimizer='adam', metrics=['mse'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2=create_model2()\nmodel2.fit(X_train,y_train,epochs=3,batch_size=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.evaluate(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparing it with Standard Keras BCE Loss Function","metadata":{}},{"cell_type":"code","source":"def create_model2():\n    model = Sequential()\n    model.add(Dense(30,input_dim=10,activation='relu'))\n    model.add(Dense(5,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=create_model2()\nmodel.fit(X_train,y_train,epochs=3,batch_size=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test,y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see we have got exactly the same result with our custom mse loss function and standard keras mse loss function","metadata":{}},{"cell_type":"markdown","source":"# IF You got to learn something or you enjoyed the notebook. Please Do upvote ðŸ˜…\n## Follow me for upcoming tutorials and please show some love to my previous notebooks.ðŸ˜…\n### Peace Out!!\n### Sourabh Yadav!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}