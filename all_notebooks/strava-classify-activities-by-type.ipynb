{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Motivation\n\nI am currently learning some data science with python. I am following this course in udemy:\n\n> [Python for Data Science and Machine Learning Bootcamp](https://www.udemy.com/course/python-for-data-science-and-machine-learning-bootcamp/)\n\n\nIn this notebook I want to practice some of the concepts and skills that I have learned so far:\n\n- Some general exploratory data analysis (EDA)\n- Principal Component Analysis\n- Logistic Regressin\n- Grid search cross validation (CV)\n\n## Some background\n\nI am an amateur cyclist and occasional runner who uses *STRAVA* every time I get on my bike or put my trainers on. In case you don't know it, [*STRAVA*](https://www.strava.com/) (which is Swedish for \"strive\") is an app that allows you to monitor and register your trainings and then share them with other users in a social network style. *STRAVA* is mostly used by cyclist and running folk.\n\nEvery time you register an activity it keeps track of data like distance, moving time, elevation gain, average speed, heart rate, and so on. Thanks to their API, it is relatively easy to obtain your activities data, provided you are a registered user of course. Explainining how to get all your activities data is out of the scope of this notebook (although I plan on uploading a notebook about it in the future), however, if you are curious about that, check the following links\n\n- [1 - Intro and accessing Strava API with Postman - Strava API for Beginners](https://www.youtube.com/watch?v=sgscChKfGyg)\n\n- [3 - Using Strava API with Python - Strava API for Beginners\n](https://www.youtube.com/watch?v=2FPNb1XECGs)\n\n- [Strava API v3 reference](https://developers.strava.com/docs/reference/)\n\n## The task\n\nI will provide a dataset of my activities and the objective will be to train an logistic regression model\nthat works only with a few principal components that is capable to predict if an activity was of type 'Ride' or type 'Run'. The task may not be very challenging, but I think is a good exercise for a beginner working in a field that is of my interest. Additionaly I want to perform a grid search CV to optimize the number of components and then use my model to perform predictions with other peoples strava datasets."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Start by importing the modules I will use"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sb\nfrom matplotlib import pyplot as plt\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the data\n\nthe data of my activities is on the csv file called ```activities_Miguel__Training-Validation.csv``` "},{"metadata":{"trusted":true},"cell_type":"code","source":"activities = pd.read_csv('../input/strava-ride-or-run-classification/activities_Miguel__Training-Validation.csv',index_col=0)\nprint(activities.info())\nactivities.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Meaning of the columns\n\nI grouped some of the columns in two groups: performance_features and social_features.\n\n#### Performance features\n\nDistances and heigths are in meters, while time is on seconds (it is called international system of units)\n\n- distance: distance of the activity.\n- moving_time: time where you are actually doing something (coffee stops don't count).\n- elapsed_time: total time STRAVA is registering.\n- total_elevation_gain: positive amount of climbing.\n- achievement_count: number of time you get a 3rd, 2nd or personal record in a segment.\n- pr_count: number of personal records.\n- max_speed: in meters/s. I will not use average_speed because it is just distance/moving_time.\n- elev_high: maximumn height reached.\n- elev_low: minimum\n\n\n#### social features\n- kudos_count: A kudo is the name in strava for the likes given by friends.\n- comment_count: number of comments.\n- athlete_count: if this was a group activity then it is the number of companions in the activity. If it is a solo activiy then it is 0.\n- total_photo_count: Total number of photos uploaded in an activity.\n\nThen there is the column that I will use for classification: **type**. **type** stands for the sport of the activity: Ride, Run, Walk, Hike, Surf,.... Since I am only interested in classifying Rides or Runs I will just keep those two labels\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"social_features = [\n     'kudos_count',\n     'comment_count',\n     'athlete_count',\n     'total_photo_count',\n]\n\nperformance_features = [\n     'distance',\n     'moving_time',\n     'elapsed_time',\n     'max_speed',\n     'total_elevation_gain',\n     'elev_high',\n     'elev_low',\n     'achievement_count',\n     'pr_count',\n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Some exploratory data analysis of my strava activities "},{"metadata":{},"cell_type":"markdown","source":"Keep only rides and run and since average_speed = distance / moving_time, I will also drop this column because it does not add more information"},{"metadata":{"trusted":true},"cell_type":"code","source":"activities.drop(\n    activities[\n        (activities['type']!='Run') &\n        (activities['type']!='Ride')\n    ].index,\n    inplace = True\n)\n\nactivities.drop('average_speed',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Start with some pair plots of the data set\n### Pair plot of performance data"},{"metadata":{"trusted":true},"cell_type":"code","source":"performance_pair_plot = sb.pairplot(\n    activities,\n    vars = performance_features,\n    hue = 'type',\n    diag_kind = 'hist',\n    palette = 'colorblind'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pair plot of social data"},{"metadata":{"trusted":true},"cell_type":"code","source":"performance_pair_plot = sb.pairplot(\n    activities,\n    vars = social_features,\n    hue = 'type',\n    diag_kind = 'hist',\n    palette = 'colorblind'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Principal Component Analysis\n\nI will start by defining a data frame for the performance data"},{"metadata":{"trusted":true},"cell_type":"code","source":"activities_performance = activities[performance_features]\nactivities_performance.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then I will scale the dataframe of performance to 0 mean and unit standard deviation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaled_performance = scaler.fit_transform(activities_performance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make a PCA decomposition fit and transform the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=9)\nperformance_pca = pca.fit_transform(scaled_performance)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see what the data looks like after being transformed to principal components"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6));\nsb.scatterplot(x = performance_pca[:,0],y = performance_pca[:,1],hue=activities['type'],cmap='plasma');\nplt.xlabel('First principal component');\nplt.ylabel('Second Principal Component');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The value of the first PC is a good indicator of the type of activity"},{"metadata":{},"cell_type":"markdown","source":"### Plot of the ratio of the components to the explained variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsb.barplot(\n    y = pca.explained_variance_ratio_,\n    x = [f'Component {n+1}' for n in range(pca.n_components) ]\n)\nplt.ylabel('Ratio of explained variance');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like we can describe the 70% variance with the first 3 components"},{"metadata":{},"cell_type":"markdown","source":"### Relation of the components to the original data\n\nTo interpret the principal components, it is useful to see how the different features contribute to each of them. This information is in the loadings of the principal components. I will make a dataframe with the loadings of the PCs"},{"metadata":{"trusted":true},"cell_type":"code","source":"components_dataframe = pd.DataFrame(\n    pca.components_,\n    columns = performance_features,\n    index = [f'Component {n+1}' for n in range(pca.n_components_) ]\n)\ncomponents_dataframe","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A heat map will help to see the contribution of each feature in the components. I make a heat map showing the absolute value of the loadings and a + or - sign, indicating if the component value increases or decreases withe the corresponding feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"ann = components_dataframe.applymap(\n    lambda x:{-1:'-',1:'+'}.get(np.sign(x).astype('int'),'0')\n)\n\nplt.figure(figsize=(15,10))\nsb.heatmap(\n    components_dataframe.applymap(np.abs),\n    annot = ann,\n    fmt = '',\n    cmap = 'rocket'\n);\nax = plt.gca()\nax.tick_params(axis='x', labelrotation=60)\nfor label in ax.xaxis.get_ticklabels():\n    label.set_horizontalalignment('right')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"distance, moving time and elevation gain are of great importance for the first component, whihc explains almost the 50% of the variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"sb.scatterplot(y = activities_performance['distance'],x = performance_pca[:,0],hue=activities['type']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic regression with principal components\n\n\nI will train a logistic classifier that classifies the activities according to their type: Run and Ride. However,\nI want a model that classifies the activities according to the values of some of the first principal components,\ninstead of using all the features as input directly. In order to do this the data needs to be scaled, then transformed to\nthe principal components and finally inserted in the logistic classifier.\n\nThe easiest way of doing this in sklearn is by using a pipeline, i.e., a concatenation of different transformers and an estimator at the end."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\n# Define a pipeline to search for the best PCA truncation\n# The concatenated steps in the pipeline are : scale->pca transform -> logistic classifier\npipe = Pipeline(steps=[('scaler',StandardScaler()),('pca', PCA()), ('logistic', LogisticRegression(max_iter=10000, tol=0.1))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## As usual, split my data in train and test sets. X will be the performance data and y the type of the activity"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    activities_performance,\n#     pd.get_dummies(activities['type'])['Ride'],\n    activities['type'],\n    test_size=0.25,\n    random_state=42\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of classes in the train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,[ax1,ax2] = plt.subplots(ncols=2,figsize = (10,5),sharey=True)\nsb.countplot(x = y_train,ax = ax1)\nax1.set_title('Training')\nsb.countplot(x = y_test,ax = ax2)\nax2.set_title('Test');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two classes are not very well balanced. I need to run more often."},{"metadata":{},"cell_type":"markdown","source":"## Fit the pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe.fit( X_train, y_train );","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make some predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_predictions = pipe.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate the performance of the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix, confusion_matrix, classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot of the confusion matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nplot_confusion_matrix(pipe,X_test,y_test,ax=plt.gca());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\n    classification_report(y_test,y_predictions)\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualize the predictions for two selected features and the 2 first principal components"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,[[ax1,ax2],[ax3,ax4]] = plt.subplots(ncols=2,nrows=2,figsize = (15,15),sharey='row',sharex='row')\n\n# Scatter plots for distance and elevation gain \n\n#Scatter plot for true classes\nsb.scatterplot(\n    x = X_test['distance'],\n    y = X_test['total_elevation_gain'],\n    hue = y_test,\n    ax = ax1\n)\nax1.set_title('True classes');\n\n#Scatter plot for predicted classes\nsb.scatterplot(\n    x = X_test['distance'],\n    y = X_test['total_elevation_gain'],\n    hue = y_predictions,\n    ax = ax2\n)\nax2.set_title('Predicted');\n\n# Scatter plots for 2 first PC\nX_test_transformed = pipe[:-1].transform(X_test) #I am applying the pipe line until the pc step\n\n#Scatter plot for true classes\nsb.scatterplot(\n    x = X_test_transformed[:,0],\n    y = X_test_transformed[:,1],\n    hue = y_test,\n    ax = ax3\n)\nax3.set_title('True classes');\nax3.set_xlabel('First PC')\nax3.set_ylabel('Second PC')\n\n#Scatter plot for predicted classes\nsb.scatterplot(\n    x = X_test_transformed[:,0],\n    y = X_test_transformed[:,1],\n    hue = y_predictions,\n    ax = ax4\n)\nax4.set_title('Predicted');\nax4.set_xlabel('First PC')\n\nfig.suptitle('Compare predictions with true classes');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen, the model performs reasonably well. Do not stop there and see if it can be improved by selecting the optimal number of components. I will do this with a gridsearch CV"},{"metadata":{},"cell_type":"markdown","source":"# Grid Search CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the grid estimator.\n\nA neat feature of pipelines is that they can crosvalidated for different hyperparameters in a really simple way. You just have to introduce the values of the parameters for different steps of the pipeline with the following convention\n\n```python\nparam_grid = {\n    '<step_name>_<parameter_name>':'<list of values>',\n    ...\n}\n```"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'pca__n_components':[1,2,3,4,5,6,7,8,9]}\ngrid = GridSearchCV(\n    pipe,\n    param_grid = param_grid,\n    verbose = 3,\n    scoring = 'f1_weighted'\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fit the grid estimator"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.fit(X_train,y_train);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results of the grid search\n\n### Best hyperparameters ( number of PCs)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\n    '\\n'.join(   f'{ind}: {val}' for ind,val in grid.best_params_.items() ) \n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Complete results in a dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame(grid.cv_results_)\nresults","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot of the score as a function of the number of components"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax0,ax1) = plt.subplots(nrows=2,sharex=True,figsize=(6,8))\n\n# Plot of the explained variance ratio of every component\nax0.plot(\n    np.arange(1, pipe['pca'].n_components_ + 1),\n    pipe['pca'].explained_variance_ratio_,\n    'k-o',\n    linewidth=2\n)\nax0.axvline(grid.best_params_['pca__n_components'],ls='--',c='k',label = 'Chosen number\\n of components')\n\nax0.legend(prop=dict(size=12))\nax0.set_ylabel('PCA explained variance ratio')\n\n# Plot of the score as a function of the number of components\nresults.plot(\n    x = 'param_pca__n_components',\n    y = 'mean_test_score',\n    yerr = 'std_test_score',\n    style='-o',\n    c = 'k',\n    capsize=4,\n    ax = ax1,\n    legend = False\n)\nax1.set_ylabel('classes weighted average of f1 score')\n\n\nax1.set_xlabel('n components');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The optimal number of principal components seem to be 4. 5, 6, and 9 give also nice results but 4 is simpler to evaluate."},{"metadata":{},"cell_type":"markdown","source":"## Test the optimal estimator with the test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,6))\nplot_confusion_matrix(grid,X_test,y_test,ax=plt.gca());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Test the model with external data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/strava-data/strava_full_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.columns","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}