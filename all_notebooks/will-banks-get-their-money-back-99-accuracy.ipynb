{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nWe are presented with the dataset about bank's customers which have borrowed money from the bank (it wasn't specified which bank exactly though). There are couple of things we may (and we will) explore, but the most important question we will try to answer in this notebook is: **Based on the dataset, can we predict whether a loan will be repaid**?"},{"metadata":{},"cell_type":"markdown","source":"# Import relevant libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nfrom scipy.stats import uniform\nfrom matplotlib import pyplot as plt\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler,LabelEncoder\nfrom sklearn.metrics import classification_report, roc_auc_score, recall_score, make_scorer, plot_confusion_matrix, confusion_matrix, accuracy_score,f1_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/credit-risk/original.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get the basic idea about the dataset"},{"metadata":{},"cell_type":"markdown","source":"Check what type of objects our dataset contains"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be noted that the feature space is very small."},{"metadata":{},"cell_type":"markdown","source":"Check nulls"},{"metadata":{"trusted":true},"cell_type":"code","source":"ser = df.isnull().sum()\nser[ser>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, only `age` has missing values. Since 3 missing values is fairly small number (given that we have 2k entries in the dataset), we will use imputation method. Before imputing though, let's check the basic stats relating to `age`"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the `age` column has **negative values**. There are two possible explanations:\n\n1. Data has been transformed\n\n2. Someone has made a mistake when entering the values\n\n\nLet's check which explanation is more likely"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = df\nfeature = 'age'\nsns.set_style('ticks')\nplt.figure(figsize=(10,7))\ndataframe[feature].hist(bins=50)\nplt.title(f\"Distribution of the feature `{feature}`\",fontsize=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['age'] < 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fistly we note that only 3 entries have negative age (out of 1997 non-null values). Furthermore, the non-negative values falls into the range which agrees with our common sense about the age (i.e (1) only adults (older than 18 or 20) people can borrow money from bank, and (2) it is unlikely that there will be a lot of people older than 90 (or even 100) that will borrow money from a bank)\n\nConsequently, it will be reasonable to assume that the negative values were entered by mistake. Hence we will remove the entries with negative values."},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_age = df[df['age'] < 0].index\ndf.drop(neg_age,axis=0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We impute `age` with mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['age'].fillna(df['age'].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It also should be noted that the values in the `age` are mostly floats. However, by \"age\" we normally mean how many years a person has already lived for (in other words, one's age is normally represented by the whole number, not a decimal). One way to explain why we observe decimals in the feature  `age` is: it might be the case that in our dataset the `age` is defined as:\n\n$$ \\frac{\\text{number of days a person has lived for}}{365}$$\n\n\nFor example, if the person was born in 1995/05/01 and today is 2020/11/07, then the person has lived for $9322$ days, which implies that the person's age is \n\n$$\\frac{9322}{365}=25.53972602739726$$\n\n\nThis definition of age at least explains the occurence of decimal numbers in the column `age` (although it may not be the actual definition used in the dataset; only the creator of the dataset can tell)"},{"metadata":{},"cell_type":"markdown","source":"After cleaning the feature, we proceed by answering the following question: Is `age` a good predictor for defaults?"},{"metadata":{},"cell_type":"markdown","source":"# Distribution of a target feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = df\nfeature1 = 'default'\nsns.countplot(dataframe[feature1])\nplt.title(f\"Distribution of the feature `{feature1}`\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the distribution is heavily unbalanced. "},{"metadata":{},"cell_type":"markdown","source":"# Is age a good predictor of a default scenario?"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = df\nfeature_1 = 'default'\nfeature_2 = 'age'\nplt.figure(figsize=(7,7))\nsns.boxplot(x=feature_1, y=feature_2, data=dataframe)\nplt.title(\"How does one's age affect risk of default?\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the answer is: Yes, `age` is a good predictor. As the box plots shows, **younger people are more likely to default**; the conditional distributions deviate significantly (one can use ANOVA to quantify the difference)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['income'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = df\nfeature = 'income'\nsns.set_style('ticks')\nplt.figure(figsize=(10,7))\ndataframe[feature].hist()\nplt.title(f\"Distribution of the feature `{feature}`\",fontsize=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that in our dataset, the income has (roughly) uniform distribution"},{"metadata":{},"cell_type":"markdown","source":"# Is income a good predictor of a default scenario?"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = df\nfeature_1 = 'default'\nfeature_2 = 'income'\nplt.figure(figsize=(7,7))\nsns.boxplot(x=feature_1, y=feature_2, data=dataframe)\nplt.title(\"How does one's income affect risk of default?\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = df\ncat_feat = 'default'\ncont_feat = 'income'\nplt.figure(figsize=(7,7))\nfor value in df[cat_feat].unique():\n    sns.distplot(df[df[cat_feat] == value][cont_feat], label=value)\nplt.legend()\nplt.title(f\"Distribution of `{cont_feat}` conditional on `{cat_feat}`\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Contrary to one's intuition, in our dataset the lower income doesn't not imply higher chance of default. As we see from the charts above, the conditional distributions are rouhgly the same, indicating that the `income` does not do a good job at differentiating bad loans"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['loan'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = df\nfeature = 'loan'\nsns.set_style('ticks')\nplt.figure(figsize=(10,7))\ndataframe[feature].hist()\nplt.title(f\"Distribution of {feature}\",fontsize=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unlike the distributions of the `age` and `income`, the distribution of `loan` is skewed to the right."},{"metadata":{},"cell_type":"markdown","source":"Are there loans where amount is less than 10?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['loan'] < 10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see the conditional distribution"},{"metadata":{},"cell_type":"markdown","source":"# Is loan amount a good predictor of a default scenario?"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = df\nfeature_1 = 'default'\nfeature_2 = 'loan'\nplt.figure(figsize=(7,7))\nsns.boxplot(x=feature_1, y=feature_2, data=dataframe)\nplt.title(\"How does loan amount affect risk of default?\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = df\ncat_feat = 'default'\ncont_feat = 'loan'\nplt.figure(figsize=(7,7))\nfor value in df[cat_feat].unique():\n    sns.distplot(df[df[cat_feat] == value][cont_feat], label=value)\nplt.legend()\nplt.title(f\"Distribution of `{cont_feat}` conditional on `{cat_feat}`\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, there is a clear separation: larger loan amount implies higher likelihood of default."},{"metadata":{},"cell_type":"markdown","source":"# Is there any relation between features  `income` and `loan`?"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataframe = df\nfeature1 = 'loan'\nfeature2 = 'income'\n\n\ng=sns.jointplot(x=dataframe[feature1], y=dataframe[feature2], kind=\"kde\")\ng.fig.set_figwidth(11)\ng.fig.set_figheight(13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Couple of observations can be made: \n1. For the high income customers, the amount borrowed has a very large spread (i.e it is quite possible that the person with the high income will borrow a very small amount of money)\n\n2. For the low income customers, the spread is very small (in other words, it is highly unlikely (even impossible) that the people with the small income will borrow large amounts of money)"},{"metadata":{},"cell_type":"markdown","source":"To see that the conclusion above hold (quantitatively), we can do following:\n\n1. Bin income (i.e discretize the feature `income`)\n\n2. For each bin (i.e income category), calculate a spread of loan amount (for example, using variance/standard deviation)\n\n\nLet's try"},{"metadata":{},"cell_type":"markdown","source":"# Does higher income imply larger loan amount spread?"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = df[['income','loan']].copy()\ndf1['Binned income'] = pd.cut(df1['income'],7)\ndf1 = df1.groupby('Binned income').std()\ndf1.reset_index(inplace=True)\ndf1['Binned income'] = df1['Binned income'].astype(str)\ndf1.sort_values(by=['loan'],ascending=True,inplace=False)\n\n\nfig = px.bar(df1,\n             x='Binned income',\n             y='loan',\n             title='Spread (stdev) of the loan amount for each income category')\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, income is positively correlated with the loan spread (i.e Some rich people borrow large sums, but some of them also borrow tiny amounts; but the people with low income (generally) borrow low amounts) "},{"metadata":{},"cell_type":"markdown","source":"# Is there a correlation between customer's income and an amount borrowed?"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df[['income','loan']].corr().values[0][1]\nprint(f'Correlation between `income` and `loan`: {round(corr,2)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, there is a correlation between income and loan, but it's pretty weak one (for the reason we've just elaborated on: loan amounts are spread out for high-income customers)"},{"metadata":{},"cell_type":"markdown","source":"# EDA: Conclusions. \nBased on our dataset, we can make following conclusions (should be noted though, these conclusions may not hold in general: our dataset is quite small, and the sample taken might not necessarily be random):\n1. Younger people have way higher likelihood of default.\n2. Income doesn't affect the likelihood of default.\n3. Larger loan amounts imply higher likelihood of a default.\n4. Amounts borrowed by rich people are way more spread out than those borrowed by low-income customers."},{"metadata":{},"cell_type":"markdown","source":"# Feature importance estimation: Random Forest"},{"metadata":{},"cell_type":"markdown","source":"We've hypothesized that `income` is the least useful feature for predicting default. One can use several ways to verify this, but I will use the most straightforward one: Random Forest feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['clientid','default'],axis=1)\ny = df['default']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')\n\nforest_clf = RandomForestClassifier(n_estimators=100)\nforest_clf.fit(X, y)\n\nimportances = forest_clf.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize=(7,7))\nplt.bar(range(len(indices)),importances[indices])\nplt.xticks(range(len(indices)), indices)\nplt.title(\"Feature importance (Random Forest)\")\nplt.xlabel('Index of a feature')\nplt.ylabel('Feature importance')\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lowest_importance = X[X.columns[indices[-1]]].name\nprint(f'RF esimations show that the feature with the lowest importance is: {lowest_importance}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, Random Forest estimations agree with the conclusion we reached after visualizing conditional distributions."},{"metadata":{},"cell_type":"markdown","source":"# Feature selection"},{"metadata":{},"cell_type":"markdown","source":"We see that the feature space is very small, so we will use all features available to us (except `clientid`, of course; this feature has no value)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=15)\n\n\nmmsc = MinMaxScaler()\nX_train = mmsc.fit_transform(X_train)\nX_test = mmsc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_clf = GaussianNB().fit(X_train,y_train)\nprint(classification_report(y_true=y_test, y_pred=nb_clf.predict(X_test)))\nplot_confusion_matrix(nb_clf, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"log_random_state = None\nlog_clf = LogisticRegression(random_state=log_random_state).fit(X_train, y_train)\nprint(classification_report(y_true=y_test, y_pred=log_clf.predict(X_test)))\nplot_confusion_matrix(log_clf, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Important note:\nWe see that the recall (given that the label `1` is positive) is pretty low for both logistic and NB. This is to be expected, mainly because the target feature is disbalanced (there are way less entries with label `1`)."},{"metadata":{},"cell_type":"markdown","source":"Let's try different models (but now we will select hyperparameters that will maximize recall for the label `1`)"},{"metadata":{},"cell_type":"markdown","source":"# KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_1 = make_scorer(recall_score,pos_label=1)\n\nMIN = 1 #Min number of neighbors\nMAX = 30 #Max number of neighbors\nknn_estimator = KNeighborsClassifier()\nknn_clf = GridSearchCV(knn_estimator,\n                       {'n_neighbors': range(MIN,MAX+1)}\n                       ,scoring=recall_1).fit(X_train, y_train)\nprint(f\"Best estimator: {knn_clf.best_estimator_}\")\nprint(classification_report(y_true=y_test, y_pred=knn_clf.predict(X_test)))\nplot_confusion_matrix(knn_clf, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = RandomForestClassifier(random_state=13)\nrf_clf = GridSearchCV(estimator,\n                      param_grid={'n_estimators':[10,20,50,100], 'criterion': ['entropy','gini']},\n                      scoring=recall_1).fit(X_train, y_train)\n\nprint(classification_report(y_true=y_test, y_pred=rf_clf.predict(X_test)))\nplot_confusion_matrix(rf_clf, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training: Conclusions\n\nWe see that the Random Forest does the best job (by 'best\" I mean that the accuracy and macro f1 score is the highest out of all models we used)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}