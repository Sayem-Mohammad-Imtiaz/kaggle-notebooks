{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# check what the data we have is\nimport os\nprint(os.listdir(\"../input\"))","execution_count":197,"outputs":[]},{"metadata":{"_uuid":"786d1ee89a0ecc35cc36497f0e979147ed37de54"},"cell_type":"markdown","source":"# Helpers\nI'm just writing a couple of helper functions here. They print the head / tail of pandas dataframes a little nicer. You can find the original code, which I copied, [here](https://gist.github.com/dmyersturnbull/035876942070ced4c565e4e96161be3e)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"32c5e03df74708cbde618e830f9d77994a07a703"},"cell_type":"code","source":"# from https://gist.github.com/dmyersturnbull/035876942070ced4c565e4e96161be3e\n\nfrom IPython.display import display, Markdown\nimport pandas as pd\n\ndef head(df: pd.DataFrame, n_rows:int=1) -> None:\n    \"\"\"Pretty-print the head of a Pandas table in a Jupyter notebook and show its dimensions.\"\"\"\n    display(Markdown(\"**whole table (below):** {} rows × {} columns\".format(len(df), len(df.columns))))\n    display(df.head(n_rows))\n    \ndef tail(df: pd.DataFrame, n_rows:int=1) -> None:\n    \"\"\"Pretty-print the tail of a Pandas table in a Jupyter notebook and show its dimensions.\"\"\"\n    display(Markdown(\"**whole table (below):** {} rows × {} columns\".format(len(df), len(df.columns))))\n    display(df.tail(n_rows))","execution_count":198,"outputs":[]},{"metadata":{"_uuid":"d976008da1b20ace3800efbc89bb32ae35b98712"},"cell_type":"markdown","source":"# Preprocessing\nLet's go ahead and read in the data. Then, after we've read it in, we're going to split it into features and labels. I do two things to the features data. First, I normalize all the data, and secondly, I run PCA on the data. When I was initially running random forest on the data, I found that only four of the features were really lending to the data (found by looking at `your_rfc.feature_importances_`). Therefore, I chose to reduce the dimensionality of the data to four principal components, all of which will lend more or less equally to the variability of the data (which we of course want to maximize)."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f7a4c7b61157f46daf56e17b93e2431ae82a1c12"},"cell_type":"code","source":"input = pd.read_csv('../input/winequality-red.csv')\n\n# get X and y slices, do preprocessing\nX = input.iloc[:, :10]\n\n# https://stackoverflow.com/questions/26414913/normalize-columns-of-pandas-data-frame\nfrom sklearn import preprocessing\n\nto_scale = X.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nscaled = min_max_scaler.fit_transform(to_scale)\nX = pd.DataFrame(scaled)\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=4)\npca.fit(X)\n\nX = pd.DataFrame(pca.transform(X), columns=['PCA%i' % i for i in range(4)], index=X.index)\n\ny = input.iloc[:, 11]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)","execution_count":199,"outputs":[]},{"metadata":{"_uuid":"1f217ce25cc5897c89dd300820e868914c4c1197"},"cell_type":"markdown","source":"# Predictions\nWe'll use check_stats to show some statistics, namely loss and accuracy, related to some models. I copied the models from [another Kaggle kernel](https://www.kaggle.com/pranavcoder/random-forests-and-keras). We're focusing on random forest, but we have a single decision tree as a reference, as well as a gradient boosting classifier, which I included so we can do some ensembling a bit later as well. Note that I use a heck-ton of estimators in the random forest classifier."},{"metadata":{"trusted":true,"_uuid":"0073c24b2c663c39b6300e54de97cdf69fcf024f"},"cell_type":"code","source":"def check_stats(model, model_name):\n    sum = 0\n    loss = 0\n    total = X_test.shape[0]\n\n    predictions = model.predict(X_test)\n\n    index = 0\n    for prediction in predictions:\n\n        actual = y_test.iloc[index]\n\n#         print('pred', prediction, 'actual: ', actual)\n\n        loss += abs(actual - prediction)\n        if prediction == actual:\n            sum += 1\n\n        index += 1\n\n    accuracy = sum / total\n    avg_loss = loss / total\n\n    print('MODEL STATS: ' + model_name)\n    print('loss: ', loss)\n    print('avg loss: ', avg_loss)\n    print('accuracy: ', round(accuracy * 100, 2), '%\\n')\n    \n# https://www.kaggle.com/pranavcoder/random-forests-and-keras\n\nfrom sklearn import ensemble, tree\nfrom imblearn.pipeline import make_pipeline\n\ncart = tree.DecisionTreeClassifier(criterion='entropy', max_depth=None)\nforest = ensemble.RandomForestClassifier(criterion='entropy', n_estimators=1000, max_features=None, max_depth=None)\ngboost = ensemble.GradientBoostingClassifier(max_depth=None)\n\ncart.fit(X_train, y_train)\nforest.fit(X_train, y_train)\ngboost.fit(X_train, y_train)\n\ncheck_stats(cart, 'Decision Tree')\ncheck_stats(forest, 'Random Forest')\ncheck_stats(gboost, 'GBoost')","execution_count":200,"outputs":[]},{"metadata":{"_uuid":"d4b1b64fd31994cb19b91767a80a0ca1f190027c"},"cell_type":"markdown","source":"# Voting Classifier\nNow, I had two other models than random forest so I could ensemble the results. Let's go ahead and do so, using sklearn's built-in voting classifier. I tried both soft and hard voting, and it didn't really make a difference. I think soft voting is fine in this situation, and it's the default. Notice the voting classifier is just like the other three models, which is nice since we can reuse our check_stats function."},{"metadata":{"trusted":true,"_uuid":"fd9f971991aeed836b2befbdf554c94fd2b9285f"},"cell_type":"code","source":"# let's try a voting classifier\nfrom sklearn.ensemble import VotingClassifier\n\ncart = tree.DecisionTreeClassifier(criterion='entropy', max_depth=None)\nforest = ensemble.RandomForestClassifier(criterion='entropy', n_estimators=1000, max_features=None, max_depth=None)\ngboost = ensemble.GradientBoostingClassifier(max_depth=None)\n\nvc = VotingClassifier(estimators=[('cart', cart), ('forest', forest), ('gboost', gboost)], voting='soft')\n\nvc = vc.fit(X_train, y_train)\n\ncheck_stats(vc, 'Voting Classifier')\n    ","execution_count":201,"outputs":[]},{"metadata":{"_uuid":"056a0c91d20e65be10a1ca65bbbb7ceed196e363"},"cell_type":"markdown","source":"# Voting Classifier Results \nSo voting among the classifiers doesn't yield better results. That's fine, we just wanted to play around with ensembling in this situation. There's other problems that we can attribute our current low accuracy to. One of these is the size of the dataset relative to the number of classes we're trying to predict. We have eight outputs, for the range of qualities: `[0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]`, and only 1600 samples. If we did something with two classes, like good wine / bad wine, we would probably get better results. But where's the fun in that? In the future I might try synthesizing additional data to see if I can get better results across all the classes - but that would of course be artificial, so the current results are fine."},{"metadata":{"_uuid":"1f88bf4878c9b0a8a86a856f0f223dfc6f1570c8"},"cell_type":"markdown","source":"# Cross Validation\nLet's go ahead and try one more thing, based off of [another kernel using this dataset](https://www.kaggle.com/vishalyo990/prediction-of-quality-of-wine) which does do good/bad classes. They used cross validation and found that their 'precision' increased by an appreciable 4%."},{"metadata":{"trusted":true,"_uuid":"ed35d64104e01905d55967847508cda1ab5b5df3"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n#Now lets try to do some evaluation for random forest model using cross validation.\nrfc_eval = cross_val_score(estimator = forest, X = X_train, y = y_train, cv = 8)\nrfc_eval.mean()\n","execution_count":202,"outputs":[]},{"metadata":{"_uuid":"1c8135421d7437fc9701fd57813fa6cfad876e6d"},"cell_type":"markdown","source":"# Cross Validation Results\nSo in our case, again likely due to the number of classes we have, cross validation didn't really help our random forest model. It in fact lowers the accuracy in general - as long as `rfc_eval.mean()` is equivalent to my accuracy score."},{"metadata":{"_uuid":"60ba4257fcc6a34bfd0083b8e0dd156ea3d82703"},"cell_type":"markdown","source":"### Thanks for taking a look at this kernel, it is of course just a very quick exploration into random forest classification and a bit of ensembling. Let me know if I've made any grave mistakes etc."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"af00aa829d6719a47a9d551f18af6d62dccd2bea"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}