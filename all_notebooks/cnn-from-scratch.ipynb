{"cells":[{"metadata":{"_uuid":"16ed2b99-e41b-47ef-b102-390aae351191","_cell_guid":"f52051bd-3b2c-40e2-b3b2-bf5bcdd1f482","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom numba import jit\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15c55914-34db-49a7-984e-789649d6f397","_cell_guid":"51cae1b0-f813-41c8-88bc-47798401974d","trusted":true},"cell_type":"code","source":"#df = pd.read_csv('../input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv')\ndf1 = pd.read_csv('../input/mnist-in-csv/mnist_train.csv')\n\n#df = pd.read_csv('../input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv')\ndf2 = pd.read_csv('../input/mnist-in-csv/mnist_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4c50aae-a8c7-461e-9ae9-eefcaf79eebd","_cell_guid":"2478a9b3-c091-49eb-a9b6-343a327d0b27","trusted":true},"cell_type":"code","source":"#@jit(nopython=True)      \ndef conv_forward(A_prev, W, b, hyperparam, stride):\n\n    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n    \n    (f, f, n_C_prev, n_C) = W.shape\n    \n    #stride = hyperparam['stride']\n    pad = hyperparam['pad']\n    #pad = 0\n    n_H = int(((n_H_prev + 2 * pad - f) / stride) + 1)\n    n_W = int(((n_W_prev + 2 * pad - f) / stride) + 1)\n    \n    Z = np.zeros((m, n_H, n_W, n_C))\n    \n    A_prev_pad = np.pad(A_prev, ((0,0), (pad,pad), (pad,pad), (0,0)), mode='constant', constant_values=(0,0))\n    for i in range(m):\n        a_prev_pad = A_prev_pad[i,:,:,:]\n        \n        for h in range(n_H):\n            v_start = h * stride\n            v_end = h * stride + f\n            \n            for w in range(n_W):\n                h_start = w * stride\n                h_end = w * stride + f\n                \n                for c in range(n_C):\n                    a_slice_prev = a_prev_pad[v_start:v_end, h_start:h_end,:]\n                    weights = W[:,:,:,c]\n                    biases = b[:,:,:,c]\n                    Z[i,h,w,c] = np.sum(a_slice_prev * weights) + float(biases)\n                    \n    assert(Z.shape == (m, n_H, n_W, n_C))\n    \n    cache = (A_prev, W, b, hyperparam, stride)\n    \n    return Z, cache\n\n#@jit(nopython=True)        \ndef pool_forward(A_prev, hyperparam, stride, mode='max'):\n    \n    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n    f = hyperparam['f']\n    #stride = hyperparam['stride']\n    n_H = int((n_H_prev - f) / stride + 1)\n    n_W = int((n_W_prev - f) / stride + 1)\n    n_C = n_C_prev\n    A = np.zeros((m, n_H, n_W, n_C))\n    \n    for i in range(m):\n        for h in range(n_H):\n            v_start = h * stride\n            v_end = h * stride + f\n        \n            for w in range(n_W):\n                h_start = w * stride\n                h_end = w * stride + f\n                \n                for c in range(n_C):\n                    a_prev_slice = A_prev[i, v_start:v_end, h_start:h_end, c]\n                    \n                    if mode == 'max':\n                        A[i, h, w, c] = np.amax(a_prev_slice)\n                    elif mode == 'average':\n                        A[i, h, w, c] = np.average(a_prev_slice)\n                        \n    \n    assert(A.shape == (m, n_H, n_W, n_C))\n    \n    cache = (A_prev, hyperparam, stride)\n    \n    return A, cache\n    \ndef activation_forward(Z, mode='relu'):\n    cache = Z\n    if mode == 'relu':\n        return np.maximum(Z, 0), cache\n    elif mode == 'tanh':\n        return np.tanh(Z), cache\n    \n    \ndef FC_forward(A, W, b):\n    Z = np.dot(W, A) + b\n\n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    cache = (A, W, b)\n    \n    return Z, cache","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebde628e-3f96-4c6d-a3ff-c80fcc8f76ea","_cell_guid":"bc110905-91f8-48cb-ba00-c431803a080e","trusted":true},"cell_type":"code","source":"#@jit(nopython=True)\ndef conv_backward(dZ, cache):\n    \n    (A_prev, W, b, hyperparam, stride) = cache\n    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n    (f, f, n_C_prev, n_C) = W.shape\n    #stride = hyperparam['stride']\n    pad = hyperparam['pad']\n    (m, n_H, n_W, n_C) = dZ.shape\n\n    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n    dW = np.zeros((f, f, n_C_prev, n_C))\n    db = np.zeros((1, 1, 1, n_C))\n\n    A_prev_pad = np.pad(A_prev, ((0,0), (pad,pad), (pad,pad), (0,0)), mode='constant', constant_values=(0,0))\n    dA_prev_pad = np.pad(dA_prev, ((0,0), (pad,pad), (pad,pad), (0,0)), mode='constant', constant_values=(0,0))    \n    \n    for i in range(m):\n        a_prev_pad = A_prev_pad[i,:,:,:]\n        da_prev_pad = dA_prev_pad[i,:,:,:]\n        \n        for h in range(n_H):\n            for w in range(n_W):\n                for c in range(n_C):\n                    v_start = h * stride\n                    v_end = h * stride + f\n                    h_start = w * stride\n                    h_end = w * stride + f\n\n                    a_slice = a_prev_pad[v_start:v_end, h_start:h_end, :]\n                    da_prev_pad[v_start:v_end, h_start:h_end, :] += W[:,:,:,c] * float(dZ[i,h,w,c])\n                    dW[:,:,:,c] += a_slice * float(dZ[i,h,w,c])\n                    db[:,:,:,c] += float(dZ[i,h,w,c])\n        if pad == 0:\n                dA_prev[i,:,:,:] = da_prev_pad\n        else:\n            dA_prev[i,:,:,:] = da_prev_pad[pad:-pad, pad:-pad,:]\n        \n    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n    \n    return dA_prev, dW, db\n\n#@jit(nopython=True)\ndef pool_backward(dA, cache, mode='max'):\n    \n    (A_prev, hyperparam, stride) = cache\n    #stride = hyperparam['stride']\n    f = hyperparam['f']\n    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n    m, n_H, n_W, n_C = dA.shape\n    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n    \n    for i in range(m):\n        a_prev = A_prev[i]\n        \n        for h in range(n_H):\n            for w in range(n_W):\n                for c in range(n_C):\n                    v_start = h * stride\n                    v_end = h * stride + f\n                    h_start = w * stride\n                    h_end = w * stride + f\n\n                    if mode == 'max':\n                        a_prev_slice = a_prev[v_start:v_end, h_start:h_end, c]\n                        \n                        mask = (a_prev_slice == np.max(a_prev_slice))\n\n                        dA_prev[i, v_start:v_end, h_start:h_end, c] += mask * dA[i, h, w, c]\n                        \n                    elif mode == 'average':\n\n                        dA_prev[i, v_start:v_end, h_start:h_end, c] += (dA[i,h,w,c] / (f * f)) * np.ones((f, f))\n\n    assert(dA_prev.shape == A_prev.shape)\n    \n    return dA_prev\n\ndef dRelu(x):\n    x[x<=0] = 0\n    x[x>0] = 1\n    return x\n\ndef dTanh(x):\n    return 1.0 - np.tanh(x)**2\n\ndef activation_backward(dA, cache, mode='relu'):\n    Z = cache\n    if mode == 'relu':\n        dZ = dA * dRelu(Z)\n    elif mode == 'tanh':\n        dZ = dA * dTanh(Z)\n    return dZ\n\ndef FC_backward(dZ, cache):\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    \n    dW = (1 / m) * np.dot(dZ, A_prev.T)\n    db = (1 / m) * np.sum(dZ, axis=0, keepdims=True)\n    dA_prev = np.dot(W.T, dZ)\n    \n    return dA_prev, dW, db","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51bd9e83-78a7-460b-b58f-5be64ccf836d","_cell_guid":"11341bf7-1fde-41ae-97ce-20f38975a44f","trusted":true},"cell_type":"code","source":"def nloss(Z, y):\n    \n    n = Z.shape[1]\n    \n    softmax = np.exp(Z) / (np.sum(np.exp(Z), axis=0, keepdims=True))\n    \n    if n == 2:\n        return - (1 / n) * np.sum(y * np.log(softmax) + (1 - y) * np.log(1 - softmax), keepdims=True), softmax\n    else:\n        return - (1 / n) * np.sum((y * np.log(softmax))), softmax","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f965d34a-c7b6-4f2f-be72-2d95942a9e02","_cell_guid":"6d82c2c6-47bf-4012-9a1e-7f13a21cc51e","trusted":true},"cell_type":"code","source":"def initialize_parameters():\n    C_W1 = np.random.randn(5, 5, 1, 8) * (1 / np.sqrt(28 * 28))\n    C_b1 = np.zeros((1, 1, 1, 8))\n    \n    C_W2 = np.random.randn(5, 5, 8, 16) * (1 / np.sqrt(12 * 12 * 16))\n    C_b2 = np.zeros((1, 1, 1, 16))\n    \n    FC_W1 = np.random.randn(120, 256) * (1 / np.sqrt(256))\n    FC_b1 = np.zeros((120, 1))\n    \n    FC_W2 = np.random.randn(10, 120) * (1 / np.sqrt(120))\n    FC_b2 = np.zeros((10, 1))\n    \n    hyperparam = {\n        'stride': 1,\n        'pad': 0,\n        'f': 2\n    }\n    \n    parameters = {\n        'C_W1': C_W1,\n        'C_b1': C_b1,\n        'C_W2': C_W2,\n        'C_b2': C_b2,\n        'FC_W1': FC_W1,\n        'FC_b1': FC_b1,\n        'FC_W2': FC_W2,\n        'FC_b2': FC_b2\n    }\n    return parameters, hyperparam","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"314f82cd-1508-44f7-912d-0ec87443ba33","_cell_guid":"5af71822-678f-4851-b56b-cf9a7dc4d7b4","trusted":true},"cell_type":"code","source":"def forward_step(train_set_X, Y, params, hyperparam):\n    model_cache ={}\n    Z, cache_0 = conv_forward(train_set_X, params['C_W1'], params['C_b1'], hyperparam, 1)\n    model_cache['0'] = cache_0\n\n    A, cache_1 = activation_forward(Z, mode='tanh')\n    model_cache['1'] = cache_1\n\n    A, cache_2 = pool_forward(A, hyperparam, 2, mode='max')\n    model_cache['2'] = cache_2\n\n    Z, cache_3 = conv_forward(A, params['C_W2'], params['C_b2'], hyperparam, 1)\n    model_cache['3'] = cache_3\n\n    A, cache_4 = activation_forward(Z, mode='tanh')\n    model_cache['4'] = cache_4\n\n    A, cache_5 = pool_forward(A, hyperparam, 2, mode='max')\n    model_cache['5'] = cache_5\n    \n    A = A.reshape(-1, 256).T\n    \n    Z, cache_6 = FC_forward(A, params['FC_W1'], params['FC_b1'])\n    model_cache['6'] = cache_6\n\n    A, cache_7 = activation_forward(Z, mode='tanh')\n    model_cache['7'] = cache_7\n\n    Z, cache_8 = FC_forward(A, params['FC_W2'], params['FC_b2'])\n    model_cache['8'] = cache_8\n\n    loss, softmax = nloss(Z, Y)\n    \n    return loss, softmax, model_cache","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88ca3663-0a9b-426c-9956-d3106ee7f2c6","_cell_guid":"222feac6-0cdd-4cab-92dc-baa77ae39180","trusted":true},"cell_type":"code","source":"def backprop_step(softmax, Y, model_cache, params, alpha, adam_terms, t):\n    B1 = 0.9\n    B2 = 0.999\n    epsilon = 10 ** -8\n    #collect gradiants\n    dZ = softmax - Y\n    \n    dA_prev, FC_dW2, FC_db2 = FC_backward(dZ, model_cache['8'])\n\n    dZ = activation_backward(dA_prev, model_cache['7'], mode='tanh')\n\n    dA_prev, FC_dW1, FC_db1 = FC_backward(dZ, model_cache['6'])\n    \n    dA_prev = dA_prev.reshape(-1, 4, 4, 16)\n    \n    dA_prev = pool_backward(dA_prev, model_cache['5'], mode='max')\n\n    dZ = activation_backward(dA_prev, model_cache['4'], mode='tanh')\n    \n    dA_prev, C_dW2, C_db2 = conv_backward(dZ, model_cache['3'])\n\n    dA_prev = pool_backward(dA_prev, model_cache['2'], mode='max')\n\n    dZ = activation_backward(dA_prev, model_cache['1'], mode='tanh')\n\n    dA_prev, C_dW1, C_db1 = conv_backward(dZ, model_cache['0'])\n    \n\n    #calculating terms for adam optimization\n    adam_terms['VC_dW1'] = B1 * adam_terms['VC_dW1'] + (1 - B1) * C_dW1\n    adam_terms['VC_db1'] = B1 * adam_terms['VC_db1'] + (1 - B1) * C_db1\n    adam_terms['SC_dW1'] = B2 * adam_terms['SC_dW1'] + (1 - B2) * (C_dW1 ** 2)\n    adam_terms['SC_db1'] = B2 * adam_terms['SC_db1'] + (1 - B2) * (C_db1 ** 2)\n    adam_terms['VC_dW2'] = B1 * adam_terms['VC_dW2'] + (1 - B1) * C_dW2\n    adam_terms['VC_db2'] = B1 * adam_terms['VC_db2'] + (1 - B1) * C_db2\n    adam_terms['SC_dW2'] = B2 * adam_terms['SC_dW2'] + (1 - B2) * (C_dW2 ** 2)\n    adam_terms['SC_db2'] = B2 * adam_terms['SC_db2'] + (1 - B2) * (C_db2 ** 2)\n    \n    adam_terms['VFC_dW1'] = B1 * adam_terms['VFC_dW1'] + (1 - B1) * FC_dW1\n    adam_terms['VFC_db1'] = B1 * adam_terms['VFC_db1'] + (1 - B1) * FC_db1\n    adam_terms['SFC_dW1'] = B2 * adam_terms['SFC_dW1'] + (1 - B2) * (FC_dW1 ** 2)\n    adam_terms['SFC_db1'] = B2 * adam_terms['SFC_db1'] + (1 - B2) * (FC_db1 ** 2)\n    adam_terms['VFC_dW2'] = B1 * adam_terms['VFC_dW2'] + (1 - B1) * FC_dW2\n    adam_terms['VFC_db2'] = B1 * adam_terms['VFC_db2'] + (1 - B1) * FC_db2\n    adam_terms['SFC_dW2'] = B2 * adam_terms['SFC_dW2'] + (1 - B2) * (FC_dW2 ** 2)\n    adam_terms['SFC_db2'] = B2 * adam_terms['SFC_db2'] + (1 - B2) * (FC_db2 ** 2)\n    \n    #update parameters\n    params['C_W1'] = params['C_W1'] - alpha * ((adam_terms['VC_dW1'] / (1 - (B1 ** t))) / np.sqrt((adam_terms['SC_dW1'] / (1 - (B2 ** t))) + epsilon))\n    params['C_b1'] = params['C_b1'] - alpha * ((adam_terms['VC_db1'] / (1 - (B1 ** t))) / np.sqrt((adam_terms['SC_db1'] / (1 - (B2 ** t))) + epsilon))\n    params['C_W2'] = params['C_W2'] - alpha * ((adam_terms['VC_dW2'] / (1 - (B1 ** t))) / np.sqrt((adam_terms['SC_dW2'] / (1 - (B2 ** t))) + epsilon))\n    params['C_b2'] = params['C_b2'] - alpha * ((adam_terms['VC_db2'] / (1 - (B1 ** t))) / np.sqrt((adam_terms['SC_db2'] / (1 - (B2 ** t))) + epsilon))\n    params['FC_W1'] = params['FC_W1'] - alpha * ((adam_terms['VFC_dW1'] / (1 - (B1 ** t))) / np.sqrt((adam_terms['SFC_dW1'] / (1 - (B2 ** t))) + epsilon))\n    params['FC_b1'] = params['FC_b1'] - alpha * ((adam_terms['VFC_db1'] / (1 - (B1 ** t))) / np.sqrt((adam_terms['SFC_db1'] / (1 - (B2 ** t))) + epsilon))\n    params['FC_W2'] = params['FC_W2'] - alpha * ((adam_terms['VFC_dW2'] / (1 - (B1 ** t))) / np.sqrt((adam_terms['SFC_dW2'] / (1 - (B2 ** t))) + epsilon))\n    params['FC_b2'] = params['FC_b2'] - alpha * ((adam_terms['VFC_db2'] / (1 - (B1 ** t))) / np.sqrt((adam_terms['SFC_db2'] / (1 - (B2 ** t))) + epsilon))\n    \n    return params, adam_terms","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b57b2f72-c52e-4dd1-aaf6-ef47a80bf31a","_cell_guid":"d76d4954-6219-4ba5-bb12-740571ebcab8","trusted":true},"cell_type":"code","source":"df1 = sklearn.utils.shuffle(df1)\nY_train = df1.loc[:,'label'].to_numpy()\nX_train = df1.loc[:,'1x1':].to_numpy().reshape(-1, 28, 28, 1)\n\n#print(Y_train[3])\n#image = X_train[3,:,:,0]\n#fig = plt.figure\n#plt.imshow(image, cmap='gray')\n#plt.show()\n\ndf2 = sklearn.utils.shuffle(df2)\nY_test = df2.loc[:,'label'].to_numpy()\nX_test = df2.loc[:,'1x1':].to_numpy().reshape(-1, 28, 28, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"544b51ef-bc25-4d7d-a8df-5f8cfb06ff68","_cell_guid":"33a44ef7-f0e8-4784-95bb-deacec9baf4a","trusted":true},"cell_type":"code","source":"params, hyperparam = initialize_parameters()\n#train_set_X = X_train[:50,:,:,:]/255\n#train_set_Y = Y_train[:50]\nminibatch_size = 16\nalpha = 0.001\nadam_terms = {\n    'VC_dW1': np.zeros_like(params['C_W1']), 'VC_db1':  np.zeros_like(params['C_b1']), 'VC_dW2':  np.zeros_like(params['C_W2']), 'VC_db2':  np.zeros_like(params['C_b2']),\n    'VFC_dW1':  np.zeros_like(params['FC_W1']), 'VFC_db1':  np.zeros_like(params['FC_b1']), 'VFC_dW2':  np.zeros_like(params['FC_W2']), 'VFC_db2':  np.zeros_like(params['FC_b2']),\n    'SC_dW1':  np.zeros_like(params['C_W1']), 'SC_db1':  np.zeros_like(params['C_b1']), 'SC_dW2':  np.zeros_like(params['C_W2']), 'SC_db2':  np.zeros_like(params['C_b2']),\n    'SFC_dW1':  np.zeros_like(params['FC_W1']), 'SFC_db1':  np.zeros_like(params['FC_b1']), 'SFC_dW2':  np.zeros_like(params['FC_W2']), 'SFC_db2':  np.zeros_like(params['FC_b2'])\n}\n\n\n#for y in range(5):\nY_set = np.zeros((10, Y_train.shape[0]))\ncount = 0\ndf1 = sklearn.utils.shuffle(df1)\nY_train = df1.loc[:,'label'].to_numpy()\nX_train = df1.loc[:,'1x1':].to_numpy().reshape(-1, 28, 28, 1)\nfor x in Y_train:\n    Y_set[int(x), count] = 1\n    count += 1\n\nfor x in range(int(len(Y_train)/minibatch_size)):\n    X = X_train[x * minibatch_size:(x+1)*minibatch_size,:,:,:] / 255\n    Y = Y_set[:, x * minibatch_size:(x+1)*minibatch_size]\n\n    loss, softmax, model_cache = forward_step(X, Y, params, hyperparam)\n\n    params, adam_terms = backprop_step(softmax, Y, model_cache, params, alpha, adam_terms, x + 1)\n    if x % 200 == 0:\n        print ('Iteration loss:', x, loss)\n    #print('Epoch', y, ' loss:', loss)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18f0cbea-ea2d-45b2-8e81-208931b4d5c5","_cell_guid":"74f35e07-4b7f-4a1a-989d-5f9c9ed5e179","trusted":true},"cell_type":"code","source":"correct = 0\nminibatch_size = 16\nfor x in range(int(len(Y_test)/minibatch_size)):\n    X = X_test[x * minibatch_size:(x+1)*minibatch_size,:,:,:] \n    Y = Y_test[x * minibatch_size:(x+1)*minibatch_size]\n    \n    loss, softmax, _ = forward_step(X, Y, params, hyperparam)\n    \n    predictions = np.argmax(softmax, axis=0)\n    correct += np.sum(predictions == Y)\n    accuracy = correct / ((x+1) * minibatch_size)\n    print(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8420596-32ac-41b8-80ae-20f40802a082","_cell_guid":"a47cbf65-488a-46ce-aaeb-4cbbeb408e1c","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}