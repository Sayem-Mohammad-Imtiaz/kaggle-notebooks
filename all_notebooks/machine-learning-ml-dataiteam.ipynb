{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this kernel, I want to criticize usage of different machine learnings algorithms as KNN,Logistic Regression, Naive Bayes and SVM for the same dataset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# linear regression\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv',sep = ',')\ndf_new = df.head(20)\n\ncompareScore = []\n\nx = df_new.age.values.reshape(-1,1)\ny = df_new.trestbps.values.reshape(-1,1)\n\nplt.scatter(x,y)\nplt.xlabel('age')\nplt.ylabel('trestbps')\nplt.show()\n\nfrom sklearn.linear_model import LinearRegression\n\nlinear_reg = LinearRegression()\nlinear_reg.fit(x,y)\n\nb0 = linear_reg.intercept_\nb1 = linear_reg.coef_\nprint(\"b0 is \",b0)\nprint(\"b1 is \",b1)\n\narray = np.array([0,100,200,300,400,500,600,700,800,900,1000]).reshape(-1,1)\ny_head = linear_reg.predict(array)\ny_head\narray\nplt.plot(array,y_head, color='red')\nplt.xlabel('age')\nplt.ylabel('trestbps')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# multiple linear regression\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv',sep=',')\ndf_new = df.head(10)\n\nx = df_new.iloc[:,[3,4]].values\ny = df_new.age.values.reshape(-1,1)\n\nfrom sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nlinear_reg.fit(x,y)\n\nb0 = linear_reg.intercept_\nb1 = linear_reg.coef_\n\nprint(\"b0 is \",b0)\nprint(\"b1 is \",b1)\n\nx = np.array([[10,20],[100,200]])\ny_head = linear_reg.predict(x)\nplt.plot(x,y_head)\nplt.xlabel('trestbps and chol')\nplt.ylabel('age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# polynomial linear regression\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv',sep=',')\ndf_new = df.head(10)\n\nx = df_new.cp.values.reshape(-1,1)\ny = df_new.chol.values.reshape(-1,1)\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\npolynomial_reg = PolynomialFeatures(degree=4)\nx_new = polynomial_reg.fit_transform(x)\n\nlinear_model = LinearRegression()\nlinear_model.fit(x_new,y)\n\ny_new = linear_model.predict(x_new)\nplt.plot(x_new,y_new,color='red')\nplt.xlabel('cp')\nplt.ylabel('chol')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# random forest\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv')\ndf = df.head(10)\n\nx = df.age.values.reshape(-1,1)\ny = df.trestbps.values.reshape(-1,1)\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators = 100,random_state=42)\nrf.fit(x,y)\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head = rf.predict(x_)\n\nplt.scatter(x,y,color='red')\nplt.plot(x_,y_head,color='green')\nplt.xlabel('age')\nplt.ylabel('trestbps')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv')\ndf_new = df.head(10)\n\nx = df_new.age.values.reshape(-1,1)\ny = df_new.trestbps.values.reshape(-1,1)\n\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head = tree_reg.predict(x_)\n\n\nplt.scatter(x,y,color='red')\nplt.plot(x_,y_head,color='green')\nplt.xlabel('age')\nplt.ylabel('trestbps')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#logistic regression\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plot\n\ndata = pd.read_csv('../input/heart-disease-uci/heart.csv')\ndata.drop([\"cp\",\"sex\",\"fbs\",\"restecg\"],axis=1,inplace = True)\ny = data.age.values\nprint(\"Mean of ages: \",np.mean(y))\ndata.age = [1 if each > np.mean(y) else 0 for each in data.age]\nx_data = data.drop(\"age\",axis = 1)\n\nx = (x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data)).values\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection  import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy: \",format(lr.score(x_test.T,y_test.T)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# logistic regression \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv('../input/heart-disease-uci/heart.csv')\ndata.drop([\"sex\",\"cp\",\"fbs\",\"restecg\"],axis=1,inplace=True)\ny = data.age.values\nprint(\"Mean of ages: \",np.mean(y))\ndata.age = [1 if each < np.mean(y) else 0 for each in data.age.values]\ny = data.age.values\nx = data.drop(\"age\",axis = 1)\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=42)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\n\nprint(\"Test accuracy: \",format(lr.score(x_test,y_test)))\n\nlrScore = lr.score(x_test, y_test) * 100\ncompareScore.append(lrScore)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# knn(K-Nearest Neighbors)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv')\ndf.drop(['sex','cp','fbs','restecg','exang','oldpeak','slope','ca','thal','thalach'],axis = 1,inplace = True)\ntarget_1 = df[df.target == 1]\ntarget_0 = df[df.target == 0]\nplt.scatter(target_1.trestbps,target_1.chol,color=\"red\",label=\"target_1\")\nplt.scatter(target_0.trestbps,target_0.chol,color=\"green\",label=\"target_0\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.target.values\nx_data = df.drop(['target','age'],axis=1)\n\n#normalization\nx = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=24)\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train,y_train)\nprint('{} nn score: {}'.format(3,knn.score(x_train,y_train)))\nknnScore = knn.score(x_test, y_test) * 100\ncompareScore.append(knnScore)\n\n# confusion matrix\ny_pred = knn.predict(x_test)\ny_true = y_test\n                     \nfrom sklearn.metrics import confusion_matrix  \n\ncm = confusion_matrix(y_true,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machine(SVM)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv')\na = df['chol']\ndf.chol = [1 if each > np.mean(a) else 0 for each in df.chol]\n\nchol_1 = df[df.chol == 1]\nchol_0 = df[df.chol == 0]\n\nsns.countplot(x = \"chol\", data = df)\ndf.loc[:,'chol'].value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nplt.scatter(chol_1.age,chol_1.trestbps,label='over_chol',color=\"red\")\nplt.scatter(chol_0.age,chol_0.trestbps,label='lowel_chol',color=\"green\")\nplt.xlabel('age')\nplt.ylabel('trestbps')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.chol.values.reshape(-1,1)\nx_data = df.drop('chol',axis=1)\n#x_data = df.iloc[:,[0,1,2,3,5,6]]\nx_data\n# normalization\nx = ((x_data - np.min(x_data)) / (np.max(x_data)- np.min(x_data))).values\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=42)\n\nfrom sklearn.svm import SVC\n\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n\nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))\n\nsvmScore = svm.score(x_test, y_test) * 100\ncompareScore.append(svmScore)\n# confusion matrix\ny_pred = svm.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_true\")\nplt.ylabel(\"y_pred\")\nplt.show()\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# naive bayes\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('../input/heart-disease-uci/heart.csv')\ndf.age = [1 if each > np.mean(df.age) else 0 for each in df.age]\n\nage_old = df[df.age == 1]\nage_young = df[df.age == 0]\nplt.scatter(age_old.trestbps,age_old.chol,label = \"older\",color=\"red\")\nplt.scatter(age_young.trestbps,age_young.chol,label=\"younger\",color=\"green\")\nplt.xlabel('trestbps')\nplt.ylabel('chol')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df.age.values.reshape(-1,1)\nx_data = df.drop('age',axis=1)\n\nx = ((x_data - np.min(x_data)) / (np.max(x_data)-np.min(x_data))).values\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=24)\n\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB() \nnb.fit(x_train,y_train)\nprint(\"naive_bayes accuracy: \",nb.score(x_test,y_test))\n\nnbScore = nb.score(x_test, y_test) * 100\ncompareScore.append(nbScore)\n# confusion matrix\ny_pred = nb.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_true\")\nplt.ylabel(\"y_head\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nalgoList = [\"LogisticRegression\", \"KNN\", \"SVM\", \"NaiveBayes\"]\ncomparison = {\"Models\" : algoList, \"Accuracy\" : compareScore}\ndfComparison = pd.DataFrame(comparison)\n\nnewIndex = (dfComparison.Accuracy.sort_values(ascending = False)).index.values\nsorted_dfComparison = dfComparison.reindex(newIndex)\n\n\ndata = [go.Bar(\n               x = sorted_dfComparison.Models,\n               y = sorted_dfComparison.Accuracy,\n               name = \"Scores of Models\",\n               marker = dict(color = \"rgba(116,173,209,0.8)\",\n                             line=dict(color='rgb(0,0,0)',width=1.0)))]\n\nlayout = go.Layout(xaxis= dict(title= 'Models',ticklen= 5,zeroline= False))\n\nfig = go.Figure(data = data, layout = layout)\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Conclusion:\n\nAs we can seen final graph, usage of Logistic Regression gives us best accuracy. \nAlso, we can criticize the data distribution for the other approaches. \nWith the result of SVM, we can say it is hard to find a vector to disperse variables seperately. \nAlso, the result NaiveBayes tell us that it is not easy to find suitable circles which includes variables in similarity range.\nAnd finally, it is hard to find suitable K value in the usage of KNN."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}