{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## OLID (Offensive Language Identification Dataset)\n### Predicting the Type and Target of Offensive Posts in Social Media","metadata":{}},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport string\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB\nimport seaborn as sns\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:42:30.655803Z","iopub.execute_input":"2021-06-20T08:42:30.656428Z","iopub.status.idle":"2021-06-20T08:42:31.846068Z","shell.execute_reply.started":"2021-06-20T08:42:30.656337Z","shell.execute_reply":"2021-06-20T08:42:31.845156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load the data","metadata":{}},{"cell_type":"code","source":"train_data=pd.read_csv('data/olid-training-v1.0.tsv', delimiter='\\t', encoding='utf-8')\n\ntrain_tweets = train_data[['tweet']] #Extract tweets\ntrain_task_a_labels= train_data[['subtask_a']] #Extract subtsak_a labels\ntrain_task_b_labels= train_data[['subtask_b']] #Extract subtsak_b labels\ntrain_task_c_labels= train_data[['subtask_c']] #Extract subtsak_c labels\n\ntrain_task_a_labels.columns.values[0] = 'class_a' #Rename class attribute\ntrain_task_b_labels.columns.values[0] = 'class_b' #Rename class attribute\ntrain_task_c_labels.columns.values[0] = 'class_c' #Rename class attribute\n\n#print(train_data)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T08:34:48.431733Z","iopub.execute_input":"2021-06-20T08:34:48.432258Z","iopub.status.idle":"2021-06-20T08:34:48.483044Z","shell.execute_reply.started":"2021-06-20T08:34:48.432176Z","shell.execute_reply":"2021-06-20T08:34:48.481881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstopwords = set(stopwords.words(\"english\"))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data preprocessing","metadata":{}},{"cell_type":"code","source":"#Function to clean tweets in a data frame's tweet column\ndef clean_tweets(df):\n    \n   \n    #Stopwords\n    tweet_colm = df.iloc[:,0]\n    data = tweet_colm[3]\n    x = [word for word in data.split() if word.lower() not in stopwords]\n    data_1 = \" \".join(x)\n    data_1 = [data_1]\n    tweet_colm[3] = data_1[0]\n    df.loc[:, 'tweet'] = tweet_colm\n    \n    punctuations = string.punctuation\n    \n    df.loc[:, 'tweet'] = df.tweet.str.replace('@USER', '') #Remove mentions (@USER)\n    df.loc[:, 'tweet'] = df.tweet.str.replace('URL', '') #Remove URLs\n    df.loc[:, 'tweet'] = df.tweet.str.replace('&amp', 'and') #Replace ampersand (&) with and\n    df.loc[:, 'tweet'] = df.tweet.str.replace('&lt','') #Remove &lt\n    df.loc[:, 'tweet'] = df.tweet.str.replace('&gt','') #Remove &gt\n    df.loc[:, 'tweet'] = df.tweet.str.replace('\\d+','') #Remove numbers\n    df.loc[:, 'tweet'] = df.tweet.str.lower() #Lowercase\n    #data =  df.loc[:, 'tweet']\n    #tweet = str(data)\n    \n    \n    \n    \n\n    #Remove punctuations\n    for punctuation in punctuations:\n        df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n\n    df.loc[:, 'tweet'] = df.astype(str).apply(\n        lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii')\n    )\n    #Remove emojis\n    df.loc[:, 'tweet'] = df.tweet.str.strip() #Trim leading and trailing whitespaces","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_tweets(train_tweets)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_task_a_data = train_tweets.join(train_task_a_labels)\n\ntrain_task_b_data = train_tweets.join(train_task_b_labels)\ntrain_task_b_data = train_task_b_data.dropna() #Drop records with missing values\n\ntrain_task_c_data = train_tweets.join(train_task_c_labels)\ntrain_task_c_data = train_task_c_data.dropna() #Drop records with missing values\n\n#Apply quotes to cleaned tweets\ntrain_task_a_data.update(train_task_a_data[['tweet']].applymap('\\'{}\\''.format))\ntrain_task_b_data.update(train_task_b_data[['tweet']].applymap('\\'{}\\''.format))\ntrain_task_c_data.update(train_task_c_data[['tweet']].applymap('\\'{}\\''.format))\n\n# train_task_a_data.to_csv('olid_training_a.csv', index=None)\n# train_task_b_data.to_csv('olid_training_b.csv', index=None)\n# train_task_c_data.to_csv('olid_training_c.csv', index=None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_task_a_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparing the test sets","metadata":{}},{"cell_type":"code","source":"#Read tweets from test sets\ntest_tweet_a=pd.read_csv('data/testset-levela.tsv', delimiter='\\t', encoding='utf-8')\ntest_tweet_b=pd.read_csv('data/testset-levelb.tsv', delimiter='\\t', encoding='utf-8')\ntest_tweet_c=pd.read_csv('data/testset-levelc.tsv', delimiter='\\t', encoding='utf-8')\n\n#Read tweet labels\ntest_label_a=pd.read_csv('data/labels-levela.csv', encoding='utf-8', \n                         index_col=False, names=['id', 'class_a'])\ntest_label_b=pd.read_csv('data/labels-levelb.csv', encoding='utf-8', \n                         index_col=False, names=['id', 'class_b'])\ntest_label_c=pd.read_csv('data/labels-levelc.csv', encoding='utf-8', \n                         index_col=False, names=['id', 'class_c'])\n\n#Merge tweets with labels by id\ntest_tweet_a = test_tweet_a.merge(test_label_a, on='id')\ntest_tweet_b = test_tweet_b.merge(test_label_b, on='id')\ntest_tweet_c = test_tweet_c.merge(test_label_c, on='id')\n\n#Drop id column\ntest_tweet_a = test_tweet_a.drop(columns='id')\ntest_tweet_b = test_tweet_b.drop(columns='id')\ntest_tweet_c = test_tweet_c.drop(columns='id')\n\n#Clean tweets in test sets\nclean_tweets(test_tweet_a)\nclean_tweets(test_tweet_b)\nclean_tweets(test_tweet_c)\n\n#Apply quotes to cleaned tweets\ntest_tweet_a.update(test_tweet_a[['tweet']].applymap('\\'{}\\''.format))\ntest_tweet_b.update(test_tweet_b[['tweet']].applymap('\\'{}\\''.format))\ntest_tweet_c.update(test_tweet_c[['tweet']].applymap('\\'{}\\''.format))\n\n\n#Export to csv file\n# test_tweet_a.to_csv('olid_test_a.csv', index=None,header=True)\n# test_tweet_b.to_csv('olid_test_b.csv', index=None, header=True)\n# test_tweet_c.to_csv('olid_test_c.csv', index=None, header=True)\n\ntest_tweet_a.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick look at the topic numbers on the total dataset\n\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(6,4))\nprint(\"train data\",train_task_a_data.groupby('class_a').class_a.count())\ntrain_task_a_data.groupby('class_a').class_a.count().plot.bar(ylim=0)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick look at the topic numbers on the total dataset\n\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(6,4))\nprint(\"test data\",test_tweet_a.groupby('class_a').class_a.count())\ntest_tweet_a.groupby('class_a').class_a.count().plot.bar(ylim=0)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1st Model - MultinomialNB Naive Bayes","metadata":{}},{"cell_type":"code","source":"# Model\n\n#creating labelEncoder\nencoder = LabelEncoder()\n# Converting string labels into numbers.\ntrain_task_a_data[\"class_a_code\"] = encoder.fit_transform(train_task_a_data[\"class_a\"])\ntest_tweet_a[\"class_a_code\"] = encoder.fit_transform(test_tweet_a[\"class_a\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create tuple pair for class and class code\ntrain_task_a_data['class-tuple'] = train_task_a_data[['class_a', 'class_a_code']].apply(tuple, axis=1)\nclass_a = train_task_a_data['class-tuple'].unique()\nclass_a","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the Dataset as train and test set\n\nX_train = train_task_a_data['tweet']\ny_train = train_task_a_data['class_a_code']\n\nX_test = test_tweet_a['tweet']\ny_test = test_tweet_a['class_a_code']\n\n\nprint(\"Shape of X_train is {} and shape of y_train is {}\".format(X_train.shape, y_train.shape))\nprint(\"Shape of X_test is {} and shape of y_test is {}\".format(X_test.shape, y_test.shape))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MultinomialNB Model\nmodel= MultinomialNB()\npipeline_Mnv = Pipeline([('vectorizer', TfidfVectorizer(sublinear_tf=True, min_df=5, \n                                                        norm='l2', encoding='latin-1', \n                                                        ngram_range=(1, 2), stop_words='english')),\n                         ('classifier', model)])\n\npipeline_Mnv.fit(X_train, y_train)\ny_pred = pipeline_Mnv.predict(X_test)\naccuracy_Mnv = accuracy_score(y_test, y_pred)\nprint(\"model accuracy:\",accuracy_Mnv)\nprint(\"\\n\")\nprint(metrics.c lassification_report(y_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2nd Model - LSTM Text Classification","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_task_b_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_task_b_data.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(train_task_b_data.class_b)\nplt.xlabel('Label')\nplt.title('Number of TIN(Targetet Insults) and UNT(Untargeted) tweets')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_task_b_data.tweet\nY_train = train_task_b_data.class_b\n\nX_test = test_tweet_b.tweet\nY_test = test_tweet_b.class_b\n\n\nle = LabelEncoder()\nY_train = le.fit_transform(Y_train)\nY_train = Y_train.reshape(-1,1)\n\nY_test = le.fit_transform(Y_test)\nY_test = Y_test.reshape(-1,1)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tweet_b","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing import sequence\nfrom keras.optimizers import RMSprop\nfrom keras.models import Model\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accr = model.evaluate(test_sequences_matrix,Y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}