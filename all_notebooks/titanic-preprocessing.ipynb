{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Libraries importing and configuration"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/titanic/train.csv')\ntest_data = pd.read_csv('../input/titanic/test.csv')\n\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Removing irrelevant features"},{"metadata":{},"cell_type":"markdown","source":"For the classification models predictions some features will not give useful information, so we'll be removing the features 'PassengerId' and 'Name' that arent relevant information to know if a passenger survived or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_data['Name']\ndel train_data['PassengerId']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling missing data I"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the feature Embarked has a small number of missing data, we can remove the rows containing them."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data[train_data[\"Embarked\"].notna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Textual data encoding"},{"metadata":{},"cell_type":"markdown","source":"Most of the available machine learning algorithms can't handle textual data, so we'll be transforming them into numercical data.\n\nFirst we'll see how many unique values each of the textual features has to see the right encoding for each of them. If we aren't careful with the encoding, the model can suffer from the curse of dimensionality or it'll learn the features in the wrong way."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in train_data.columns:\n    print(col+\": \",len(pd.unique(train_data[col])), \" (\"+str(train_data[col].dtype)+\")\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen, some of the features would lead to high dimensional data if we applied the one hot encoding. So, we'll apply the Ordinal encoding for the ticket and cabin features."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Cabin'][train_data['Cabin'].isna()] = 'NaN'\nord_enc = OrdinalEncoder()\nord_enc = ord_enc.fit(train_data[['Ticket', 'Cabin']])\ntrain_data[['Ticket', 'Cabin']] = ord_enc.transform(train_data[['Ticket', 'Cabin']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the embarked and sex features we'll be using one hot encoding because the problem with the ordinal encoding is that the model could learn a order relationship between the values, and as we know, there isn't this kind of relation on these features values."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.get_dummies(train_data, columns=['Sex', 'Embarked'])\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling missing data II"},{"metadata":{},"cell_type":"markdown","source":"For the rest, we'll be using the kNN imputer to fill the missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_imputer = KNNImputer(n_neighbors=5)\ntrain_data = pd.DataFrame(knn_imputer.fit_transform(train_data), columns=train_data.columns)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking for outliers"},{"metadata":{},"cell_type":"markdown","source":"One way to check if a attribute has outliers is to check the statistical summary of the data. If the feature has a high discrepance between the mean and the median, its likely that it has outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be seen, only the fare feature has a significant difference between the mean and median, so we'll be removing all rows where the fare is higher than 2.4 standard deviations. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data[(np.abs(stats.zscore(train_data['Fare'])) < 2.4)]\ntrain_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature scaling\n\nFor a lot of machine learning algorithms is important for the data to have the same scale, so we'll be applying the MinMax encoding on the features with high variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"variance = np.var(train_data)\nprint(variance)\nhighvar_cols = [col for col in train_data.columns if variance[col] > 2]\nprint(highvar_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_scaled = train_data.copy()\nminmax_scal = MinMaxScaler(feature_range=(0.0,1.0))\nminmax_scal = minmax_scal.fit(train_data_scaled[highvar_cols])\ntrain_data_scaled[highvar_cols] = minmax_scal.transform(train_data_scaled[highvar_cols])\ntrain_data_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation analysis\n\nAs we can see above, there are no features with high correlation."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train_data_scaled)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_scaled.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(train_data_scaled.corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_titanic(data):\n    from sklearn.impute import KNNImputer\n    from sklearn.preprocessing import OrdinalEncoder\n    from sklearn.preprocessing import MinMaxScaler\n    from scipy import stats\n    \n    del data['Name']\n    del data['PassengerId']\n    \n    data = data[data[\"Embarked\"].notna()]\n    data['Cabin'][data['Cabin'].isna()] = 'NaN'\n    \n    ord_enc = OrdinalEncoder()\n    ord_enc = ord_enc.fit(data[['Ticket', 'Cabin']])\n    data[['Ticket', 'Cabin']] = ord_enc.transform(data[['Ticket', 'Cabin']])\n    \n    data = pd.get_dummies(data, columns=['Sex', 'Embarked'])\n    \n    knn_imputer = KNNImputer(n_neighbors=5)\n    data = pd.DataFrame(knn_imputer.fit_transform(data), columns=data.columns)\n    \n    data = data[(np.abs(stats.zscore(data['Fare'])) < 2.4)]\n    \n    variance = np.var(data)\n    highvar_cols = [col for col in data.columns if variance[col] > 2]\n\n    minmax_scal = MinMaxScaler(feature_range=(0.0,1.0))\n    minmax_scal = minmax_scal.fit(data[highvar_cols])\n    data[highvar_cols] = minmax_scal.transform(data[highvar_cols])\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"teste = pd.read_csv('../input/titanic/train.csv')\nteste = preprocess_titanic(teste)\nteste.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_scaled.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}