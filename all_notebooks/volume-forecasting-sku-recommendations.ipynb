{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Volume Forecasting : SKU future volume analysis and prediction"},{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction"},{"metadata":{},"cell_type":"markdown","source":"Country Beeristan, a high potential market, accounts for nearly 10% of Stallion & Co.’s global beer sales. Stallion & Co. has a large portfolio of products distributed to retailers through wholesalers (agencies). There are thousands of unique wholesaler-SKU/products combinations. In order to plan its production and distribution as well as help wholesalers with their planning, it is important for Stallion & Co. to have an accurate estimate of demand at SKU level for each wholesaler.<br>\n\nCurrently demand is estimated by sales executives, who generally have a “feel” for the market and predict the net effect of forces of supply, demand and other external factors based on past experience. The more experienced a sales exec is in a particular market, the better a job he does at estimating. Joshua, the new Head of S&OP for Stallion & Co. just took an analytics course and realized he can do the forecasts in a much more effective way. He approaches you, the best data scientist at Stallion, to transform the exercise of demand forecasting.<br>"},{"metadata":{},"cell_type":"markdown","source":"### Loading the needs library "},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_columns', 500)\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly import tools\n\nfrom math import sqrt\nfrom numpy import concatenate\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n\n# multivariate output stacked lstm example\nfrom numpy import array\nfrom numpy import hstack\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Collection of Data"},{"metadata":{},"cell_type":"markdown","source":"###  2.1 Load Data"},{"metadata":{},"cell_type":"markdown","source":"### Data Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"demo = pd.read_csv('../input/train_OwBvO8W/demographics.csv')\nprint(demo.shape)\ndemo.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"event = pd.read_csv('../input/train_OwBvO8W/event_calendar.csv')\nprint(event.shape)\nevent['YearMonth']  = pd.to_datetime(event['YearMonth'],format='%Y%m')\nevent.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"historical = pd.read_csv('../input/train_OwBvO8W/historical_volume.csv')\nprint(historical.shape)\nhistorical['YearMonth'] = pd.to_datetime(historical['YearMonth'],format='%Y%m')\nhistorical.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"soda = pd.read_csv('../input/train_OwBvO8W/industry_soda_sales.csv')\nprint(soda.shape)\nsoda['YearMonth'] = pd.to_datetime(soda['YearMonth'],format='%Y%m')\nsoda.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"industry = pd.read_csv('../input/train_OwBvO8W/industry_volume.csv')\nprint(industry.shape)\nindustry['YearMonth'] = pd.to_datetime(industry['YearMonth'],format='%Y%m')\nindustry.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"price = pd.read_csv('../input/train_OwBvO8W/price_sales_promotion.csv')\nprint(price.shape)\nprice['YearMonth'] = pd.to_datetime(price['YearMonth'],format='%Y%m')\nprice.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather = pd.read_csv('../input/train_OwBvO8W/weather.csv')\nprint(weather.shape)\nweather['YearMonth'] = pd.to_datetime(weather['YearMonth'],format='%Y%m')\nweather.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Merge all features that depends on SKU"},{"metadata":{"trusted":true},"cell_type":"code","source":"sku = historical.merge(price,on=['Agency','SKU','YearMonth'],how='left')\nsku = sku.merge(soda,on=['YearMonth'],how='left')\nsku = sku.merge(industry,on='YearMonth',how='left')\nsku = sku.merge(event,on=['YearMonth'],how='left')\nprint(sku.shape)\nsku.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sku.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Merge all features that depends on Agency"},{"metadata":{"trusted":true},"cell_type":"code","source":"agency = weather.merge(demo,on=['Agency'],how='left')\nprint(agency.shape)\nagency.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"agency.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4 Merge all dataset and preparing data training"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = sku.merge(agency,on=['YearMonth','Agency'],how='left')\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df, columns= ['SKU'], dummy_na= False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = df.drop(columns=['Price','Sales','Promotions'])\ntrain_df.set_index('YearMonth',inplace=True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.5 Preparing Testing Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/test_8uviCCm/volume_forecast.csv')\nprint(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_date = len(train_df.index.unique())\ntes4 = pd.date_range(start='1/1/2013', end='31/12/2017',freq='M')\ntes3 = list(tes4)*len(test)\ntes1 = list(test.Agency)*len(tes4)\ntes2 = list(test.SKU)*len(tes4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.DataFrame({'Agency':tes1,'SKU':tes2,'Volume':np.nan})\ntest_df.sort_values(['Agency','SKU'],inplace=True,ascending=False)\ntest_df.reset_index(inplace=True,drop=True)\ntest_df.loc[:,'YearMonth'] = tes3\ntest_df['YearMonth'] = test_df['YearMonth'].dt.floor('d') - pd.offsets.MonthBegin(1)\nprint(test_df.shape)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"test_df = test_df.merge(weather,on=['YearMonth','Agency'],how='left')\ntest_df = test_df.merge(demo,on='Agency',how='left')\ntest_df = test_df.merge(industry,on='YearMonth',how='left')\ntest_df = test_df.merge(soda,on=['YearMonth'],how='left')\ntest_df = test_df.merge(event,on=['YearMonth'],how='left')\ntest_df = pd.get_dummies(test_df, columns= ['SKU'], dummy_na= False)\ntest_df.set_index('YearMonth',inplace=True)\ntest_df = test_df[train_df.columns]\nprint(test_df.shape)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.6 Preparing SKU Recomendations Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"tes = ['Agency_06','Agency_14']*len(price.SKU.unique())*len(tes4)\ntes.sort()\ntes5 = list(price.SKU.unique())*2*len(tes4)\ndf_agen = pd.DataFrame({'Agency':tes,'SKU':tes5,'YearMonth':np.NaN,'Volume':np.NaN})\ndf_agen.sort_values(['Agency','SKU'],inplace=True)\ndf_agen.loc[:,'YearMonth'] = list(tes4)*2*25\ndf_agen.loc[:,'YearMonth'] = df_agen.loc[:,'YearMonth'].dt.floor('d') - pd.offsets.MonthBegin(1)\nprint(df_agen.shape)\ndf_agen.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_agen = df_agen.merge(weather,on=['YearMonth','Agency'],how='left')\ndf_agen = df_agen.merge(demo,on='Agency',how='left')\ndf_agen = df_agen.merge(industry,on='YearMonth',how='left')\ndf_agen = df_agen.merge(soda,on=['YearMonth'],how='left')\ndf_agen = df_agen.merge(event,on=['YearMonth'],how='left')\ndf_agen = pd.get_dummies(df_agen, columns= ['SKU'], dummy_na= False)\ndf_agen.set_index('YearMonth',inplace=True)\ndf_agen = df_agen[train_df.columns]\nprint(df_agen.shape)\ndf_agen.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Preparation and Data Distribution"},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Check Null and Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(test_df)\nmissing_values.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no missing values in all dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train_df[train_df.columns[:18]].corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(corr,vmin=-1,cmap='coolwarm', annot=True, fmt = \".2f\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(columns=['FIFA U-17 World Cup','Football Gold Cup'],inplace=True)\ntest_df.drop(columns=['FIFA U-17 World Cup','Football Gold Cup'],inplace=True)\ndf_agen.drop(columns=['FIFA U-17 World Cup','Football Gold Cup'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Standardize Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_call = train_df.columns[2:]\nX = train_df[x_call]\ny = train_df['Volume']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std_call = ['Soda_Volume','Industry_Volume','Avg_Max_Temp','Avg_Population_2017','Avg_Yearly_Household_Income_2017']\nscaller = StandardScaler()\nstd = pd.DataFrame(scaller.fit_transform(X[std_call]),columns=std_call)\nstd_test = pd.DataFrame(scaller.transform(test_df[std_call]),columns=std_call)\nstd_agen = pd.DataFrame(scaller.transform(df_agen[std_call]),columns=std_call)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_std = X.copy()\nX_std.loc[:,std_call] = std.values\ntest_df_std = test_df.copy() \ndf_agen_std = df_agen.copy()\ntest_df_std.loc[:,std_call] = std_test.values \ndf_agen_std.loc[:,std_call] = std_agen.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_std.shape)\nX_std.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Splitting Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.30, random_state = 217,shuffle=True)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Build Model to predict Volume Industry"},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Choosing  Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spot all methods want to be used\nmodels = []\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('LR', LinearRegression()))\nmodels.append(('GB', GradientBoostingRegressor()))\nmodels.append(('LG', LGBMRegressor()))\nmodels.append(('KN', KNeighborsRegressor()))\nmodels.append(('XG', XGBRegressor(objective='reg:squarederror')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame({'Score':['fit_time', 'score_time', 'test_R_Square', 'test_MSE', 'test_MAE']})\nfor name, model in models:\n    # Spot all scorers want to be used\n    scorer = {'R_Square' : 'r2',\n              'MSE'  : 'neg_mean_squared_error',\n              'MAE' : 'neg_mean_absolute_error'}\n        \n    # Cross Validation Model\n    kfold = KFold(n_splits=5, random_state=217,shuffle=True)\n    cv_results = cross_validate(model,X_train, y_train,cv=kfold,scoring=scorer)\n    cv_results['test_R_Square'] = cv_results['test_R_Square']*100\n    cv_results['test_MSE'] = np.log(np.sqrt(np.abs(cv_results['test_MSE'])))*10\n    cv_results['test_MAE'] = np.log(np.abs(cv_results['test_MAE']))*10\n    results[name] = pd.DataFrame(cv_results).mean().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name = ['RandomForest', 'LinearRegression', 'GradientBoosting', 'KNeighbors', 'LGBM', 'XGBRegressor']\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=model_name,\n    y=results.iloc[2,1:],\n    name='R Square',\n    marker_color='indianred'\n))\nfig.add_trace(go.Bar(\n    x=model_name,\n    y=results.iloc[3,1:],\n    name='logRMSE*10',\n    marker_color='lightsalmon'\n))\nfig.add_trace(go.Bar(\n    x=model_name,\n    y=results.iloc[4,1:],\n    name='logMAE*10',\n    marker_color='mediumslateblue'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.layout.update(barmode='group', xaxis_tickangle=-45)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_rf = RandomForestRegressor()\n\nmodel_rf.fit(X_train, y_train)\n\npredictions = model_rf.predict(X_test)\nprint(\"R Square: %.3f\" % r2_score(y_test, predictions))\nprint(\"RMSE: %f\" % np.sqrt(mean_squared_error(y_test, predictions)))\nprint(\"MAE: %f\" % mean_absolute_error(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_name  = ['Random Forest']\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=model_name,\n    y=[r2_score(y_test, predictions)*100],\n    name='R Square',\n    marker_color='indianred'\n))\nfig.add_trace(go.Bar(\n    x=model_name,\n    y=np.log([np.sqrt(mean_squared_error(y_test, predictions))])*10,\n    name='logRMSE*10',\n    marker_color='lightsalmon'\n))\nfig.add_trace(go.Bar(\n    x=model_name,\n    y=np.log([mean_absolute_error(y_test, predictions)])*10,\n    name='logMAE*10',\n    marker_color='mediumslateblue'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.layout.update(barmode='group', xaxis_tickangle=-45)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lg = LGBMRegressor()\n\nmodel_lg.fit(X_train, y_train)\n\npredictions = model_lg.predict(X_test)\nprint(\"R Square: %.3f\" % r2_score(y_test, predictions))\nprint(\"RMSE: %f\" % np.sqrt(mean_squared_error(y_test, predictions)))\nprint(\"MAE: %f\" % mean_absolute_error(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By the score above, we decide Random Forest Regressor to build the final model."},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Training Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_fix = RandomForestRegressor()\nmodel_fix.fit(X_std, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(df,n):\n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (12, 12))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:n]))), \n            df['importance_normalized'].head(n), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:n]))))\n    ax.set_yticklabels(df['feature'].head(n))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract feature importances\nfeature_importance = model_fix.feature_importances_\nfeature_importances = pd.DataFrame({'feature': x_call, 'importance': feature_importance})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = plot_feature_importances(feature_importances,30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.4. Predicting The Volume Industry"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_df.shape)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test = model_fix.predict(test_df_std[x_call])\ntest_df.loc[:,'Volume'] = pred_test\nprint(test_df.shape)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split(x):\n    x = x.split('_')\n    return x[1]+'_'+x[2]\n\ntest_df.loc[:,'SKU'] = test_df[test_df.columns[17:]].idxmax(axis=1).apply(split).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.drop(columns=test_df.iloc[:,17:-1].columns,inplace=True)\ntest_df.reset_index(inplace=True)\nprint(test_df.shape)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pivot = pd.pivot_table(test_df, values='Volume', index='YearMonth', columns=['Agency','SKU'])\nprint(pivot.shape)\npivot.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Forecasting Volume Industry"},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Build Model using LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = np.array(pivot)\n# choose a number of time steps\nn_steps = 12\n# convert into input/output\nX, y = split_sequences(dataset, n_steps)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and validating\ntrain_X, train_y = X[:-12, :], y[:-12,:]\nval_X, val_y = X[-12:-3, :], y[-12:-3,:]\ntest_X, test_y = X[-3:, :], y[-3:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel = Sequential()\nmodel.add(LSTM(1024, activation='relu', return_sequences=True, input_shape=(n_steps, n_features),recurrent_dropout=0.2))\nmodel.add(LSTM(512, activation='relu',return_sequences=True,recurrent_dropout=0.2))\nmodel.add(LSTM(256, activation='relu',return_sequences=True,recurrent_dropout=0.1))\nmodel.add(LSTM(128, activation='relu',recurrent_dropout=0.1))\nmodel.add(Dense(n_features))\nmodel.compile(optimizer='adam', loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# fit model\nhistory = model.fit(train_X, train_y, epochs=712, batch_size=32, verbose=0, shuffle=False,validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n                x=list(range(1,722)),\n                y=history.history['loss'],\n                name=\"Train\",\n                line_color='deepskyblue',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=list(range(1,722)),\n                y=history.history['val_loss'],\n                name=\"Test\",\n                line_color='dimgray',\n                opacity=0.8))\n\n# Use date string to set xaxis range\nfig.layout.update(title_text=\"Forecasting Score\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Testing Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\ntrain_yhat = model.predict(train_X, verbose=0)\nprint(\"RMSE of training data : %.3f\" % np.sqrt(mean_squared_error(train_y,train_yhat)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\nval_yhat = model.predict(val_X, verbose=0)\nprint(\"RMSE of training data : %.3f\" % np.sqrt(mean_squared_error(val_y,val_yhat)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\ntest_yhat = model.predict(test_X, verbose=0)\nprint(\"RMSE of testing data : %.3f\" % np.sqrt(mean_squared_error(test_y,test_yhat)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Forecasting of Volume Industry for one month a head"},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\nx_input = X[len(X)-1,:]\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pivot.loc['2018-01-01',:] = yhat\npivot.index = pd.to_datetime(pivot.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vol_for = pd.read_csv('../input/test_8uviCCm/volume_forecast.csv')\nprint(vol_for.shape)\nvol_for.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def volume(row):\n    agen = row.Agency\n    cost = row.SKU\n    return pivot[agen,cost]['2018-01-01']\n\nvol_for['Volume'] = vol_for.apply(volume,axis=1)\nprint(vol_for.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vol_for.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vol_for.tail(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. SKU Recommendations for Agency_06 & Agency_14"},{"metadata":{},"cell_type":"markdown","source":"## 6.1 Predict Volume Industry"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_agen.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model_fix.predict(df_agen_std[x_call])\ndf_agen.loc[:,'Volume'] = pred\nprint(df_agen.shape)\ndf_agen.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_agen.loc[:,'SKU'] = df_agen[df_agen.columns[17:]].idxmax(axis=1).apply(split).values\ndf_agen.drop(columns=df_agen.iloc[:,17:-1].columns,inplace=True)\ndf_agen.reset_index(inplace=True)\nprint(df_agen.shape)\ndf_agen.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pivot_agen = pd.pivot_table(df_agen.reset_index(), values='Volume', index='YearMonth', columns=['Agency','SKU'])\nprint(pivot_agen.shape)\npivot_agen.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.2 Forecast Volume Industry one month a head"},{"metadata":{},"cell_type":"markdown","source":"### 6.2.1 Training Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = np.array(pivot_agen)\n# choose a number of time steps\nn_steps = 12\n# convert into input/output\nX, y = split_sequences(dataset, n_steps)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split into train and validating\ntrain_X, train_y = X[:-12, :], y[:-12,:]\nval_X, val_y = X[-12:-3, :], y[-12:-3,:]\ntest_X, test_y = X[-3:, :], y[-3:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel = Sequential()\n# define model\nmodel = Sequential()\nmodel.add(LSTM(1024, activation='relu', return_sequences=True, input_shape=(n_steps, n_features),recurrent_dropout=0.2))\nmodel.add(LSTM(512, activation='relu',return_sequences=True,recurrent_dropout=0.2))\nmodel.add(LSTM(256, activation='relu',return_sequences=True,recurrent_dropout=0.1))\nmodel.add(LSTM(128, activation='relu',recurrent_dropout=0.1))\nmodel.add(Dense(n_features))\nmodel.compile(optimizer='adam', loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# fit model\nhistory = model.fit(train_X, train_y, epochs=712, batch_size=32, verbose=0, shuffle=False,validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n                x=list(range(1,722)),\n                y=history.history['loss'],\n                name=\"Train\",\n                line_color='deepskyblue',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=list(range(1,722)),\n                y=history.history['val_loss'],\n                name=\"Test\",\n                line_color='dimgray',\n                opacity=0.8))\n\n# Use date string to set xaxis range\nfig.layout.update(title_text=\"Forecasting Score\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2.2 Testing Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\ntrain_yhat = model.predict(train_X, verbose=0)\nprint(\"RMSE of training data : %.3f\" % np.sqrt(mean_squared_error(train_y,train_yhat)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\nval_yhat = model.predict(val_X, verbose=0)\nprint(\"RMSE of training data : %.3f\" % np.sqrt(mean_squared_error(val_y,val_yhat)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\ntest_yhat = model.predict(test_X, verbose=0)\nprint(\"RMSE of testing data : %.3f\" % np.sqrt(mean_squared_error(test_y,test_yhat)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\nx_input = X[len(X)-1,:]\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pivot_agen.loc['2018-01-01',:] = yhat\npivot_agen.index = pd.to_datetime(pivot_agen.index)\npivot_agen.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6.3 SKU Recommendations for Agency_06 and Agency_14 "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_01'],\n                name=\"Agency_06 with SKU_01\",\n                line_color='deepskyblue',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_02'],\n                name=\"Agency_06 with SKU_02\",\n                line_color='darkgoldenrod',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_03'],\n                name=\"Agency_06 with SKU_03\",\n                line_color='dimgray',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_04'],\n                name=\"Agency_06 with SKU_04\",\n                line_color='aquamarine',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_05'],\n                name=\"Agency_06 with SKU_03\",\n                line_color='lightpink',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_14','SKU_01'],\n                name=\"Agency_14 with SKU_01\",\n                line_color='cornflowerblue',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_14','SKU_02'],\n                name=\"Agency_14 with SKU_02\",\n                line_color='lawngreen',\n                opacity=0.8))\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_14','SKU_04'],\n                name=\"Agency_14 with SKU_04\",\n                line_color='lightsalmon',\n                opacity=0.8))\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_14','SKU_05'],\n                name=\"Agency_14 with SKU_05\",\n                line_color='indianred',\n                opacity=0.8))\n\n# Use date string to set xaxis range\nfig.layout.update(title_text=\"Top Four Recommendation SKU for Agency_06 & Agency_14\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_02'],\n                name=\"Agency_06 with SKU_02\",\n                line_color='darkgoldenrod',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_01'],\n                name=\"Agency_06 with SKU_01\",\n                line_color='dimgray',\n                opacity=0.8))\n\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_14','SKU_01'],\n                name=\"Agency_14 with SKU_01\",\n                line_color='cornflowerblue',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_14','SKU_02'],\n                name=\"Agency_14 with SKU_02\",\n                line_color='lawngreen',\n                opacity=0.8))\n\n# Use date string to set xaxis range\nfig.layout.update(title_text=\"Recommendation SKU for Agency_06 & Agency_14\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sku_recom = pd.read_csv('../input/test_8uviCCm/sku_recommendation.csv')\ntes = pivot_agen.loc['2018-01-01',:].reset_index()\ntes.columns = ['Agency','SKU','Volume']\ntes_1 = tes[tes.Agency=='Agency_06']\ntes_2 = tes[tes.Agency=='Agency_14']\ntes_3 = list(tes_1.loc[tes_1['Volume'].nlargest(2).index,'SKU']) + list(tes_2.loc[tes_2['Volume'].nlargest(2).index,'SKU'])\nsku_recom.SKU = tes_3\nprint(sku_recom)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, both Agency, Agency_06 and AGency_14, are recommended to handle the same SKU. Those are SKU_01 and SKU_02."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}