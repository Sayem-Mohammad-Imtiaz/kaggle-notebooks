{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set up","metadata":{}},{"cell_type":"code","source":"!pip install bert-for-tf2","metadata":{"execution":{"iopub.status.busy":"2021-08-12T02:42:16.425277Z","iopub.execute_input":"2021-08-12T02:42:16.425726Z","iopub.status.idle":"2021-08-12T02:42:31.518391Z","shell.execute_reply.started":"2021-08-12T02:42:16.42564Z","shell.execute_reply":"2021-08-12T02:42:31.517252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport json\nimport re\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, roc_auc_score\n\nimport tensorflow as tf\nimport transformers\nimport bert \nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\nfrom bert.tokenization.bert_tokenization import FullTokenizer \nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model,Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:21:50.178893Z","iopub.execute_input":"2021-08-12T03:21:50.179317Z","iopub.status.idle":"2021-08-12T03:21:50.188703Z","shell.execute_reply.started":"2021-08-12T03:21:50.179282Z","shell.execute_reply":"2021-08-12T03:21:50.187527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data and Cleaning","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/imdb-dataset/imdb_train.csv', usecols = ['review','sentiment'])\ndf_val = pd.read_csv('../input/imdb-dataset/imdb_val.csv', usecols = ['review','sentiment'])\ndf_test = pd.read_csv('../input/imdb-dataset/imdb_test.csv', usecols = ['review','sentiment'])","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:21:54.123565Z","iopub.execute_input":"2021-08-12T03:21:54.123925Z","iopub.status.idle":"2021-08-12T03:21:54.831646Z","shell.execute_reply.started":"2021-08-12T03:21:54.123891Z","shell.execute_reply":"2021-08-12T03:21:54.830642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.info())\ndf_train","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:22:04.695367Z","iopub.execute_input":"2021-08-12T03:22:04.695754Z","iopub.status.idle":"2021-08-12T03:22:04.735034Z","shell.execute_reply.started":"2021-08-12T03:22:04.695723Z","shell.execute_reply":"2021-08-12T03:22:04.733988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_val.info())\ndf_val","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:22:08.837498Z","iopub.execute_input":"2021-08-12T03:22:08.837852Z","iopub.status.idle":"2021-08-12T03:22:08.868322Z","shell.execute_reply.started":"2021-08-12T03:22:08.83782Z","shell.execute_reply":"2021-08-12T03:22:08.867171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test.info())\ndf_test","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:22:14.067522Z","iopub.execute_input":"2021-08-12T03:22:14.067879Z","iopub.status.idle":"2021-08-12T03:22:14.104816Z","shell.execute_reply.started":"2021-08-12T03:22:14.067846Z","shell.execute_reply":"2021-08-12T03:22:14.10368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Refer Cleaning Data \n[https://www.kaggle.com/colearninglounge/nlp-data-preprocessing-and-cleaning](https://www.kaggle.com/colearninglounge/nlp-data-preprocessing-and-cleaning)","metadata":{}},{"cell_type":"code","source":"#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https://\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\ndef remove_abb(data):\n    data = re.sub(r\"he's\", \"he is\", data)\n    data = re.sub(r\"there's\", \"there is\", data)\n    data = re.sub(r\"We're\", \"We are\", data)\n    data = re.sub(r\"That's\", \"That is\", data)\n    data = re.sub(r\"won't\", \"will not\", data)\n    data = re.sub(r\"they're\", \"they are\", data)\n    data = re.sub(r\"Can't\", \"Cannot\", data)\n    data = re.sub(r\"wasn't\", \"was not\", data)\n    data = re.sub(r\"don\\x89Ûªt\", \"do not\", data)\n    data= re.sub(r\"aren't\", \"are not\", data)\n    data = re.sub(r\"isn't\", \"is not\", data)\n    data = re.sub(r\"What's\", \"What is\", data)\n    data = re.sub(r\"haven't\", \"have not\", data)\n    data = re.sub(r\"hasn't\", \"has not\", data)\n    data = re.sub(r\"There's\", \"There is\", data)\n    data = re.sub(r\"He's\", \"He is\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"You're\", \"You are\", data)\n    data = re.sub(r\"I'M\", \"I am\", data)\n    data = re.sub(r\"shouldn't\", \"should not\", data)\n    data = re.sub(r\"wouldn't\", \"would not\", data)\n    data = re.sub(r\"i'm\", \"I am\", data)\n    data = re.sub(r\"I\\x89Ûªm\", \"I am\", data)\n    data = re.sub(r\"I'm\", \"I am\", data)\n    data = re.sub(r\"Isn't\", \"is not\", data)\n    data = re.sub(r\"Here's\", \"Here is\", data)\n    data = re.sub(r\"you've\", \"you have\", data)\n    data = re.sub(r\"you\\x89Ûªve\", \"you have\", data)\n    data = re.sub(r\"we're\", \"we are\", data)\n    data = re.sub(r\"what's\", \"what is\", data)\n    data = re.sub(r\"couldn't\", \"could not\", data)\n    data = re.sub(r\"we've\", \"we have\", data)\n    data = re.sub(r\"it\\x89Ûªs\", \"it is\", data)\n    data = re.sub(r\"doesn\\x89Ûªt\", \"does not\", data)\n    data = re.sub(r\"It\\x89Ûªs\", \"It is\", data)\n    data = re.sub(r\"Here\\x89Ûªs\", \"Here is\", data)\n    data = re.sub(r\"who's\", \"who is\", data)\n    data = re.sub(r\"I\\x89Ûªve\", \"I have\", data)\n    data = re.sub(r\"y'all\", \"you all\", data)\n    data = re.sub(r\"can\\x89Ûªt\", \"cannot\", data)\n    data = re.sub(r\"would've\", \"would have\", data)\n    data = re.sub(r\"it'll\", \"it will\", data)\n    data = re.sub(r\"we'll\", \"we will\", data)\n    data = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", data)\n    data = re.sub(r\"We've\", \"We have\", data)\n    data = re.sub(r\"he'll\", \"he will\", data)\n    data = re.sub(r\"Y'all\", \"You all\", data)\n    data = re.sub(r\"Weren't\", \"Were not\", data)\n    data = re.sub(r\"Didn't\", \"Did not\", data)\n    data = re.sub(r\"they'll\", \"they will\", data)\n    data = re.sub(r\"they'd\", \"they would\", data)\n    data = re.sub(r\"DON'T\", \"DO NOT\", data)\n    data = re.sub(r\"That\\x89Ûªs\", \"That is\", data)\n    data = re.sub(r\"they've\", \"they have\", data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"should've\", \"should have\", data)\n    data = re.sub(r\"You\\x89Ûªre\", \"You are\", data)\n    data = re.sub(r\"where's\", \"where is\", data)\n    data = re.sub(r\"Don\\x89Ûªt\", \"Do not\", data)\n    data = re.sub(r\"we'd\", \"we would\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"weren't\", \"were not\", data)\n    data = re.sub(r\"They're\", \"They are\", data)\n    data = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", data)\n    data = re.sub(r\"you\\x89Ûªll\", \"you will\", data)\n    data = re.sub(r\"I\\x89Ûªd\", \"I would\", data)\n    data = re.sub(r\"let's\", \"let us\", data)\n    data = re.sub(r\"it's\", \"it is\", data)\n    data = re.sub(r\"can't\", \"cannot\", data)\n    data = re.sub(r\"don't\", \"do not\", data)\n    data = re.sub(r\"you're\", \"you are\", data)\n    data = re.sub(r\"i've\", \"I have\", data)\n    data = re.sub(r\"that's\", \"that is\", data)\n    data = re.sub(r\"i'll\", \"I will\", data)\n    data = re.sub(r\"doesn't\", \"does not\",data)\n    data = re.sub(r\"i'd\", \"I would\", data)\n    data = re.sub(r\"didn't\", \"did not\", data)\n    data = re.sub(r\"ain't\", \"am not\", data)\n    data = re.sub(r\"you'll\", \"you will\", data)\n    data = re.sub(r\"I've\", \"I have\", data)\n    data = re.sub(r\"Don't\", \"do not\", data)\n    data = re.sub(r\"I'll\", \"I will\", data)\n    data = re.sub(r\"I'd\", \"I would\", data)\n    data = re.sub(r\"Let's\", \"Let us\", data)\n    data = re.sub(r\"you'd\", \"You would\", data)\n    data = re.sub(r\"It's\", \"It is\", data)\n    data = re.sub(r\"Ain't\", \"am not\", data)\n    data = re.sub(r\"Haven't\", \"Have not\", data)\n    data = re.sub(r\"Could've\", \"Could have\", data)\n    data = re.sub(r\"youve\", \"you have\", data)  \n    data = re.sub(r\"donå«t\", \"do not\", data)  \n    return data\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:22:18.273086Z","iopub.execute_input":"2021-08-12T03:22:18.273464Z","iopub.status.idle":"2021-08-12T03:22:18.313093Z","shell.execute_reply.started":"2021-08-12T03:22:18.273423Z","shell.execute_reply":"2021-08-12T03:22:18.309839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Cleaning\ndf_train['review'] = df_train['review'].apply(lambda z: remove_punctuations(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_html(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_url(z))\ndf_train['review'] = df_train['review'].apply(lambda z: remove_emoji(z))\n\ndf_val['review'] = df_val['review'].apply(lambda z: remove_punctuations(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_html(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_url(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_emoji(z))\n\ndf_test['review'] = df_test['review'].apply(lambda z: remove_punctuations(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_html(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_url(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_emoji(z))\n\ndf_train['review'] = df_train['review'].apply(lambda z: remove_abb(z))\ndf_val['review'] = df_val['review'].apply(lambda z: remove_abb(z))\ndf_test['review'] = df_test['review'].apply(lambda z: remove_abb(z))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:22:23.155499Z","iopub.execute_input":"2021-08-12T03:22:23.155865Z","iopub.status.idle":"2021-08-12T03:22:44.318409Z","shell.execute_reply.started":"2021-08-12T03:22:23.155831Z","shell.execute_reply":"2021-08-12T03:22:44.317316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape)\nprint(df_train.head(5))\nprint(df_val.shape)\nprint(df_val.head(5))\nprint(df_test.shape)\nprint(df_test.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:22:48.836949Z","iopub.execute_input":"2021-08-12T03:22:48.837359Z","iopub.status.idle":"2021-08-12T03:22:48.850301Z","shell.execute_reply.started":"2021-08-12T03:22:48.837325Z","shell.execute_reply":"2021-08-12T03:22:48.848946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Encodeing \nclass IntentDetectionData:\n    DATA_COLUMN,  LABEL_COLUMN  = \"review\",\"sentiment\"\n\n    def __init__(self, train, val, test, tokenizer: FullTokenizer, classes, max_seq_len):\n        self.tokenizer = tokenizer\n        self.max_seq_len = 0\n        self.classes = classes\n\n        ((self.train_x, self.train_y), (self.val_x, self.val_y), (self.test_x, self.test_y)) = map(self._prepare, [train, val, test])\n\n        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n        self.train_x, self.val_x, self.test_x = map(self._pad, [self.train_x, self.val_x, self.test_x])\n\n    def _prepare(self, df):\n        x, y = [], []\n    \n        for non, row in tqdm(df.iterrows()):\n            text, label =\\\n                row[IntentDetectionData.DATA_COLUMN], row[IntentDetectionData.LABEL_COLUMN]\n\n            tokens = self.tokenizer.tokenize(text)\n            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"] ## Tokens beigning and ending specified by separation of tokens.\n\n            token_ids = self.tokenizer.convert_tokens_to_ids(tokens) ## Convert Tokens to IDs\n\n            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n\n            x.append(token_ids)\n            y.append(self.classes.index(label))\n\n        return np.array(x), np.array(y)\n\n    def _pad(self, ids):\n        x = []\n        for input_ids in ids:\n            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)] ## -2 as ignoring tokens provided by bert\n            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids)) ## padding by zeros\n            x.append(np.array(input_ids))\n        \n        return np.array(x)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:30:30.224053Z","iopub.execute_input":"2021-08-12T03:30:30.224443Z","iopub.status.idle":"2021-08-12T03:30:30.240964Z","shell.execute_reply.started":"2021-08-12T03:30:30.224408Z","shell.execute_reply":"2021-08-12T03:30:30.239493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"markdown","source":"### BiLSTM","metadata":{}},{"cell_type":"code","source":"def BiLSTM_V0(bert_output):\n    net = Bidirectional(LSTM(units=32, return_sequences=True,))(bert_output)\n    net = GlobalAveragePooling1D()(net)\n    net = Dense(20, activation='relu')(net)\n    net = Dropout(rate=0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:22:52.273238Z","iopub.execute_input":"2021-08-12T03:22:52.273602Z","iopub.status.idle":"2021-08-12T03:22:52.280946Z","shell.execute_reply.started":"2021-08-12T03:22:52.27357Z","shell.execute_reply":"2021-08-12T03:22:52.279849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN","metadata":{}},{"cell_type":"code","source":"def CNN_V0(bert_output):\n    net = Conv1D(128, 7, activation='relu',padding='same')(bert_output)\n    net = MaxPooling1D()(net)\n    net = Conv1D(256, 5, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(512, 3, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Flatten()(net)\n    net = Dense(128, activation='relu')(net)\n    net = Dropout(0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:22:56.497805Z","iopub.execute_input":"2021-08-12T03:22:56.498205Z","iopub.status.idle":"2021-08-12T03:22:56.505856Z","shell.execute_reply.started":"2021-08-12T03:22:56.498167Z","shell.execute_reply":"2021-08-12T03:22:56.504573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN + LSTM","metadata":{}},{"cell_type":"code","source":"def CNN_LSTM_V0(bert_output):\n    net = Dropout(0.3)(bert_output)\n    net = Conv1D(200, 5, activation='relu')(net)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = LSTM(100)(net)\n    net = Dropout(0.3)(net)\n    net = Dense(16,activation='relu')(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net)\n    return outputs\n\ndef CNN_LSTM_V1(bert_output):\n\n    # channel 1\n    net = Conv1D(filters=128, kernel_size=3*32, activation='relu')(bert_output)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    a = LSTM(128)(net)\n\n    # channel 2\n    net = Conv1D(filters=128, kernel_size=5*32, activation='relu')(bert_output)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    b = LSTM(128)(net)\n\n    # channel 3\n    net = Conv1D(filters=128, kernel_size=7*32, activation='relu')(bert_output)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    c = LSTM(128)(net)\n\n    # channel 4\n    net = Conv1D(filters=128, kernel_size=9*32, activation='relu')(bert_output)\n    net = MaxPooling1D(pool_size=2)(net)\n    net = Dropout(0.5)(net)\n    net = BatchNormalization()(net)\n    d = LSTM(128)(net)\n\n    merged = concatenate([a,b,c,d])\n    dense = Dense(100, activation='relu')(merged)\n    drop = Dropout(0.2)(dense)\n    outputs = Dense(1, activation='sigmoid')(merged)\n    return outputs\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:23:00.097697Z","iopub.execute_input":"2021-08-12T03:23:00.098099Z","iopub.status.idle":"2021-08-12T03:23:00.112185Z","shell.execute_reply.started":"2021-08-12T03:23:00.098062Z","shell.execute_reply":"2021-08-12T03:23:00.110857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BiLSTM + CNN ","metadata":{}},{"cell_type":"code","source":"def BiLSTM_CNN_V0(bert_output):\n    net = Bidirectional(LSTM(128, return_sequences=True))(bert_output)\n    net = Conv1D(128, 7, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(256, 5, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Conv1D(512, 3, activation='relu',padding='same')(net)\n    net = MaxPooling1D()(net)\n    net = Flatten()(net)\n    net = Dense(128, activation='relu')(net)\n    net = Dropout(0.5)(net)\n    outputs = Dense(1, activation='sigmoid', name='classifier')(net) \n    return outputs","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:23:04.436519Z","iopub.execute_input":"2021-08-12T03:23:04.43691Z","iopub.status.idle":"2021-08-12T03:23:04.444914Z","shell.execute_reply.started":"2021-08-12T03:23:04.436877Z","shell.execute_reply":"2021-08-12T03:23:04.443599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Choose Model","metadata":{}},{"cell_type":"code","source":"def create_model(model_name, model_ver, max_seq_len, bert_checkpnt_file):\n\n    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n        bc = StockBertConfig.from_json_string(reader.read()) ## Reading bert config\n        bert_params = map_stock_config_to_params(bc) ## Mapping parameters \n        bert_params.adapter_size = None # Adapter size helps tune Bert model faster\n        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n        \n    ## Creat dictionary\n    choose_model = {'LSTM':{},\n                    'CNN':{0: CNN_V0},\n                    'BiLSTM':{0: BiLSTM_V0},\n                    'CNN+LSTM':{0: CNN_LSTM_V0, 1: CNN_LSTM_V1},\n                    'BiLSTM+CNN':{0: BiLSTM_CNN_V0},}\n    \n    ## Specifying input\n    input_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n    bert_output = bert(input_ids)\n        \n    outputs = choose_model[model_name][model_ver](bert_output)\n\n    model = keras.Model(input_ids, outputs)\n    model.build(input_shape=(None, max_seq_len))\n    load_stock_weights(bert, bert_checkpnt_file) ##Loading the weights from bert chckpoint file\n        \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:23:07.907845Z","iopub.execute_input":"2021-08-12T03:23:07.908242Z","iopub.status.idle":"2021-08-12T03:23:07.916975Z","shell.execute_reply.started":"2021-08-12T03:23:07.908209Z","shell.execute_reply":"2021-08-12T03:23:07.915753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BERT model","metadata":{}},{"cell_type":"code","source":"# Load BERT model\n# https://github.com/google-research/bert/blob/master/README.md\n\nbert_model_name = \"uncased_L-12_H-768_A-12\"\n# uncased_L-4_H-512_A-8\n# uncased_L-12_H-768_A-12\n\n!wget  https://storage.googleapis.com/bert_models/2020_02_20/{bert_model_name}.zip\n!unzip {bert_model_name}.zip","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:26:20.039664Z","iopub.execute_input":"2021-08-12T03:26:20.040183Z","iopub.status.idle":"2021-08-12T03:28:25.600572Z","shell.execute_reply.started":"2021-08-12T03:26:20.040135Z","shell.execute_reply":"2021-08-12T03:28:25.599486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nbert_model_path = \"./\"\nbert_checkpnt_file = os.path.join(bert_model_path, \"bert_model.ckpt\")\nbert_config_file = os.path.join(bert_model_path, \"bert_config.json\")\nbert_vocab_file = os.path.join(bert_model_path, \"vocab.txt\")\nprint(bert_checkpnt_file)\nprint(bert_config_file)\nprint(bert_vocab_file)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:28:30.313355Z","iopub.execute_input":"2021-08-12T03:28:30.313762Z","iopub.status.idle":"2021-08-12T03:28:30.321085Z","shell.execute_reply.started":"2021-08-12T03:28:30.31373Z","shell.execute_reply":"2021-08-12T03:28:30.319571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data pretrain","metadata":{}},{"cell_type":"code","source":"# Tokenize\ntokenizer = FullTokenizer(vocab_file=bert_vocab_file)\nclasses = [0, 1]\nmax_seq_len = 384\ndata = IntentDetectionData(df_train, df_val, df_test, tokenizer, classes, max_seq_len)\nprint(data.max_seq_len)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:30:39.584981Z","iopub.execute_input":"2021-08-12T03:30:39.585389Z","iopub.status.idle":"2021-08-12T03:36:08.450326Z","shell.execute_reply.started":"2021-08-12T03:30:39.585352Z","shell.execute_reply":"2021-08-12T03:36:08.449271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Metrics\ndef get_metrics(y_test, y_pred_proba):\n    print('ACCURACY_SCORE: ', round(accuracy_score(y_test, y_pred_proba >= 0.5), 4))\n    print('F1_SCORE: ', round(f1_score(y_test, y_pred_proba >= 0.5, average = \"macro\"), 4))\n    print('ROC_AUC_SCORE: ', round(roc_auc_score(y_test, y_pred_proba), 4))\n    print('CONFUSION_MATRIX:\\n', confusion_matrix(y_test, y_pred_proba >= 0.5),'\\n')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:37:40.774909Z","iopub.execute_input":"2021-08-12T03:37:40.775288Z","iopub.status.idle":"2021-08-12T03:37:40.782742Z","shell.execute_reply.started":"2021-08-12T03:37:40.775255Z","shell.execute_reply":"2021-08-12T03:37:40.781318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build and Train","metadata":{}},{"cell_type":"markdown","source":"#### BERT + BiLSTM + CNN","metadata":{}},{"cell_type":"code","source":"model_name = \"BiLSTM+CNN\"\nmodel_ver = 0\nLR = 2e-5\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\noptimizer = Adam(learning_rate=LR)\nmetrics = tf.metrics.BinaryAccuracy()\n\nmodel = create_model(model_name, model_ver, max_seq_len, bert_checkpnt_file)\nmodel.compile(loss=loss, optimizer=optimizer, metrics=metrics)\nmodel.summary()\n\n# Plot architecture model\ntf.keras.utils.plot_model(model, show_shapes=True, dpi=96) #to_file='model.jpeg'\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:37:44.936497Z","iopub.execute_input":"2021-08-12T03:37:44.936857Z","iopub.status.idle":"2021-08-12T03:37:58.8711Z","shell.execute_reply.started":"2021-08-12T03:37:44.936825Z","shell.execute_reply":"2021-08-12T03:37:58.86543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model\nmodel_ckpt_path = f\"[{bert_model_name}]{model_name}_V{model_ver}_{max_seq_len}.hdf5\"\ncheckpoint = ModelCheckpoint(model_ckpt_path, monitor='val_binary_accuracy', mode='max',verbose=1, save_best_only=True, save_weights_only=True)\ncallbacks_list = [checkpoint]\n\n# Training\nprint(f\"Training model with {bert_model_name}_{model_name}_V{model_ver}_{max_seq_len}\\n\")\ntrain_history = model.fit(data.train_x, data.train_y, validation_data=(data.val_x,data.val_y), epochs=3, batch_size=16, verbose=1, callbacks=callbacks_list)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T03:38:55.565174Z","iopub.execute_input":"2021-08-12T03:38:55.565573Z","iopub.status.idle":"2021-08-12T05:56:16.353229Z","shell.execute_reply.started":"2021-08-12T03:38:55.565538Z","shell.execute_reply":"2021-08-12T05:56:16.352143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot accuracy and loss\nhistory_dict = train_history.history\nprint(history_dict.keys())\n\nacc = history_dict['binary_accuracy']\nval_acc = history_dict['val_binary_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\nfig = plt.figure(figsize=(10, 6))\nfig.tight_layout()\n\nplt.subplot(2, 1, 1)\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save architecture model\nconfig = model.to_json()\nmodel_config_path = f\"[{bert_model_name}]{model_name}_V{model_ver}_{max_seq_len}.json\"\nwith open(model_config_path, \"w\") as outfile:\n    json.dump(config, outfile)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T05:59:23.365287Z","iopub.execute_input":"2021-08-12T05:59:23.365707Z","iopub.status.idle":"2021-08-12T05:59:23.378863Z","shell.execute_reply.started":"2021-08-12T05:59:23.36567Z","shell.execute_reply":"2021-08-12T05:59:23.377758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights(model_ckpt_path)\ny_pred_proba = model.predict(data.test_x)\nget_metrics(data.test_y, y_pred_proba)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T06:00:02.512971Z","iopub.execute_input":"2021-08-12T06:00:02.513376Z","iopub.status.idle":"2021-08-12T06:02:58.227203Z","shell.execute_reply.started":"2021-08-12T06:00:02.513342Z","shell.execute_reply":"2021-08-12T06:02:58.226156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}