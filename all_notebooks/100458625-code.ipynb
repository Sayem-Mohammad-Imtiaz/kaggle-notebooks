{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# importing the packages\nimport numpy as np #for multi-dimensional arrays, matrices and high level mathematical functions\nimport matplotlib.pyplot as plt #for object-oriented API. Acts like an extention to numpy\nimport pandas as pd #for data manipulation and analysis\nfrom sklearn.cluster import KMeans #for unsupervised A.I. algorithms\nfrom yellowbrick.cluster import KElbowVisualizer #for visualising the elbow when deciding the number of clusters for KMeans\nimport seaborn as sns; sns.set()  # for plot styling\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Doing the EDA**\n","metadata":{}},{"cell_type":"markdown","source":"In order to have a good cluster result, we  need to understand data, find patterns and interpret the plots. This can help us both with unsupervised clustering and shaping up a marketing strategy that can be implemented in our smar sale system.","metadata":{}},{"cell_type":"code","source":"#Read data from the CSV file\ncustomers=pd.read_csv(\"../input/customer-segmentation/Cust_Segmentation.csv\")\ncustomers.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check for missing data and value types \ncustomers.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#look for patterns in the dataset. Where would it be better to use unsupervised machine learning algorithms\nsns.pairplot(customers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe some patterns in specific plots. These are the fisrt clues on where the algorithms could be implemented. We can also try to find patterns by separating the dataset entries based on different criteria. We move now into a three dimensions plot where not only the X and Y axis give us details about the data but the color too. The dataset has 8 features which means it can be represented in maximul 8 dimensions. But we are just going to analyse scatter plots which cannot ve visualised in 8 dimensions. ","metadata":{}},{"cell_type":"code","source":"#what if we cluster the data based on wether the customers are defaulted or not? Can we see patterns?\nsns.pairplot(hue=\"Defaulted\", data=customers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In some graphs we can observe already some form of clustering where black dots are not blended with white dots. This means we can already see how supervised clustering shapes up based on this criteria.\nWe will now separate the entries based on the level of education. Let's see how the level of education can change the way we look at the graphs.","metadata":{}},{"cell_type":"code","source":"#what about the level of education?\nsns.pairplot(hue=\"Edu\", data=customers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unlike the separation based on wether the customers are defaulted or not, the separation based on education level doesn't have so many graphs that are so well separated. Education levels are mixed in most of the graphs. However it can be observed some sort of pattern if we look at the \"Years Employed\" and \"Income\" graph. Higher education levels are at higher income levels than lower education levels. Even in this situation, the more years employed does a customer have, the more likely is to have a greater income. We can observe the minimum income raises with the number of years employed.\nWe will look now at a corelation matrix to see which graph have values that corelate more ","metadata":{}},{"cell_type":"code","source":"#corelation matrix helps us better visualise which graphs have stronger corelation between values\ncustomers.corr().style.background_gradient(cmap=\"coolwarm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In depth view of plots**\n","metadata":{}},{"cell_type":"markdown","source":"We can observe the intersection between \"Years Employed\" and \"Income\". As a result we will take a deeper look at the graph representing the before-mentioned features. ","metadata":{}},{"cell_type":"code","source":"#create plot to ilustrate the coleration between years employed and income. Color diferentiation based on the level of education\nplt.figure(figsize = (10,10))\nsns.scatterplot(x=\"Years Employed\", y=\"Income\", hue = 'Edu',palette=\"blend:#55cf59,#000000\", data=customers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the plot that we've created above, we can observe that people with less experience in the filed they are working in can have similar income levels with people that have more years of experience but lower level of education. This kind of information can be already used for a marketing strategy. Because this graph is one of a very high coeficient of corelation we should look further in it to find other patterns.","metadata":{}},{"cell_type":"code","source":"# create the same plot but differentiate entries by color based on the customers' age\nplt.figure(figsize = (10,10))\nsns.scatterplot(x=\"Years Employed\", y=\"Income\", hue = 'Age', data=customers)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is no surprise to see that older people are more likely to ave more years employed in the domain they are working in but the interesting fact about this plot is that older people with less years of experience have better incomes than younger people with the same number of years employed. This is a strong sign that further analiysis is required on a plot ilustrating the income in relation to age.","metadata":{}},{"cell_type":"code","source":"#create plot to ilustrate the income in relation to age. Color diferentiation based on Years employed\nplt.figure(figsize = (10,10))\nsns.scatterplot(x=\"Age\", y=\"Income\", hue = 'Years Employed', palette=\"blend:#fc8d8d,#000000\", data=customers)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This might be the one of the clearest graphs. We can see a very clear separation between young customers with little experience and older customers with more experience. This can be considered supervised clustering because we have such a strong coraltion and so good separation.\nWe can also observe that the big majority of customers are young and with less years of experience. This might mean the business is probably a STUDENT RELATED bussines since most of the students are young and have few years  of experience with low income.","metadata":{}},{"cell_type":"markdown","source":"If we look at the corelation matrix again, we can observe the biggest corelation ratio is between the \"Card debt\" and \"Other debt\" features. so let's plot this graph and do color distintion based on second best corelation ratio in the corelation matrx, namely, the \"Income\"","metadata":{}},{"cell_type":"code","source":"#create graph of \"card debt\" in relation to \"Other debt\" and create color dinstinction based on icome\nplt.figure(figsize = (10,10))\nsns.scatterplot(x=\"Card Debt\", y=\"Other Debt\", palette = \"blend:#d9d6ff,#000000\",hue = 'Income', data=customers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The scatter plot above shows us that most of the customers have little to no debts at all. But the same people who don't have any debts are the ones with the least amounts of money. This detail enhances the idea of a bussines related to students.","metadata":{}},{"cell_type":"markdown","source":"Yet, in the graph below we can see the younger people with lower incomes are more likely to be defaulted than older people with more income. A trend line has been aded to better visualise the difference in the effect of the bigger income over the defaulted feature.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10,10))\nsns.lmplot(x=\"Age\", y=\"Income\", hue=\"Defaulted\", data=customers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Scaling the graph**","metadata":{}},{"cell_type":"markdown","source":"Before implementing the unsupervised algorithms we want to make sure we get acurate results. In order to improve the acuracy of the results, we want to scale the values of both axis of a plot to be between 0 and 1 so the error of the algorithms is minimal.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler #for scaling  the values in the graph\nscaler=MinMaxScaler()\ncustomers[['Income']] = scaler.fit_transform(customers[['Income']])\ncustomers[['Years Employed']] = scaler.fit_transform(customers[['Years Employed']])\n\nplt.figure(figsize = (10,10))\nsns.scatterplot(x=\"Years Employed\", y=\"Income\", hue = 'Edu',palette=\"blend:#55cf59,#000000\", data=customers)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check the right number of clusters to implement KMeans**","metadata":{}},{"cell_type":"markdown","source":"To implement the KMeans algorithm we need to find out which is the right number of clusters we want to work on. Therefore, we create a grapf that ilustrates the distortion score for each cluster number between 1 and 12 and we look at the last number which will not increase this distortion significantly.","metadata":{}},{"cell_type":"code","source":"model = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,12))\nplt.figure(figsize = (10,10))\nvisualizer.fit(customers[['Years Employed', 'Income']])  # Fit the data to the visualizer\nvisualizer.show() #plot the visualiser","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Show the clustered array based on specific graph **","metadata":{}},{"cell_type":"markdown","source":"Now we have the number of clusters and we can implement the Kmeans algorithm.  ","metadata":{}},{"cell_type":"code","source":"km = KMeans(n_clusters = 3) #set the number of clusters\ny_predicted = km.fit_predict(customers[['Years Employed', 'Income']]) #give the algorithm the graph we want to apply the calculations on\nprint(y_predicted) #print the array of clusters","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the graph\ncustomers['cluster'] = y_predicted\ncustomers1=customers[customers.cluster==0]\ncustomers2=customers[customers.cluster==1]\ncustomers3=customers[customers.cluster==2]\nplt.figure(figsize = (10,10))\nsns.scatterplot(x=\"Years Employed\", y=\"Income\", color = 'red', data=customers1)\nsns.scatterplot(x=\"Years Employed\", y=\"Income\", color = 'green', data=customers2)\nsns.scatterplot(x=\"Years Employed\", y=\"Income\", color = 'blue', data=customers3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the algorithm worked preey well, The clustering is very well defined and the entries mix very little which is a good sign of a quality clustering. Now we can repeat this proces for multipla graphs to see what the result are in different situations.","metadata":{}},{"cell_type":"code","source":"\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,12))\nplt.figure(figsize = (10,10))\nvisualizer.fit(customers[['Age', 'Income']])        # Fit the data to the visualizer\nvisualizer.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"km = KMeans(n_clusters = 3)\ny_predicted = km.fit_predict(customers[['Age', 'Income']])\ncustomers['cluster'] = y_predicted\ncustomers1=customers[customers.cluster==0]\ncustomers2=customers[customers.cluster==1]\ncustomers3=customers[customers.cluster==2]\nplt.figure(figsize = (10,10))\nsns.scatterplot(x=\"Age\", y=\"Income\", color = 'red', data=customers1)\nsns.scatterplot(x=\"Age\", y=\"Income\", color = 'green', data=customers2)\nsns.scatterplot(x=\"Age\", y=\"Income\", color = 'blue', data=customers3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,12))\nplt.figure(figsize = (10,10))\nvisualizer.fit(customers[['Card Debt', 'Other Debt']])        # Fit the data to the visualizer\nvisualizer.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"km = KMeans(n_clusters = 3)\ny_predicted = km.fit_predict(customers[['Card Debt', 'Other Debt']])\ncustomers['cluster'] = y_predicted\ncustomers1=customers[customers.cluster==0]\ncustomers2=customers[customers.cluster==1]\ncustomers3=customers[customers.cluster==2]\nplt.figure(figsize = (10,10))\nsns.scatterplot(x=\"Card Debt\", y=\"Other Debt\", color = 'red', data=customers1)\nsns.scatterplot(x=\"Card Debt\", y=\"Other Debt\", color = 'green', data=customers2)\nsns.scatterplot(x=\"Card Debt\", y=\"Other Debt\", color = 'blue', data=customers3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we look at the \"Card Debt\" and \"Other debt\" clustering we can observe it went really close to the clustering we achieved from supervised cluustering on the same graph. This is another sign that the algorithm is precise enough","metadata":{}},{"cell_type":"code","source":"model = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,12))\nplt.figure(figsize = (10,10))\nvisualizer.fit(customers[['DebtIncomeRatio', 'Card Debt']])        # Fit the data to the visualizer\nvisualizer.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"km = KMeans(n_clusters = 3)\ny_predicted = km.fit_predict(customers[['DebtIncomeRatio', 'Card Debt']])\ncustomers['cluster'] = y_predicted\ncustomers1=customers[customers.cluster==0]\ncustomers2=customers[customers.cluster==1]\ncustomers3=customers[customers.cluster==2]\nplt.figure(figsize = (10,10))\nsns.scatterplot(x=\"DebtIncomeRatio\", y=\"Card Debt\", color = 'red', data=customers1)\nsns.scatterplot(x=\"DebtIncomeRatio\", y=\"Card Debt\", color = 'green', data=customers2)\nsns.scatterplot(x=\"DebtIncomeRatio\", y=\"Card Debt\", color = 'blue', data=customers3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"km = KMeans(n_clusters = 3)\ny_predicted = km.fit_predict(customers[['Card Debt','DebtIncomeRatio']])\ncustomers['cluster'] = y_predicted\ncustomers1=customers[customers.cluster==0]\ncustomers2=customers[customers.cluster==1]\ncustomers3=customers[customers.cluster==2]\nplt.figure(figsize = (10,10))\nsns.scatterplot(x=\"Card Debt\", y=\"DebtIncomeRatio\", color = 'red', data=customers1)\nsns.scatterplot(x=\"Card Debt\", y=\"DebtIncomeRatio\", color = 'green', data=customers2)\nsns.scatterplot(x=\"Card Debt\", y=\"DebtIncomeRatio\", color = 'blue', data=customers3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hierarchical clustering**","metadata":{}},{"cell_type":"markdown","source":"Now we need to compare the results of different unsupervised clustering algorithms to uderstand which one is better to be used for a business decision.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering  # for hiearchical clustering\nhclusters = AgglomerativeClustering().fit(customers[['Card Debt','DebtIncomeRatio']])\nhclusters.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers['H_clusters'] = hclusters.labels_\ncustomers1=customers[customers.H_clusters==0]\ncustomers2=customers[customers.H_clusters==1]\ncustomers3=customers[customers.H_clusters==2]\nplt.figure(figsize = (10,10))\nsns.scatterplot(x=\"Years Employed\", y=\"Income\", color = 'red', data=customers1)\nsns.scatterplot(x=\"Years Employed\", y=\"Income\", color = 'green', data=customers2)\nsns.scatterplot(x=\"Years Employed\", y=\"Income\", color = 'blue', data=customers3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that the clusteing we achieved when using hierarchical clustering is very messy, values are mixed together and we have no clear information to extract from this scatterplot.","metadata":{}},{"cell_type":"code","source":"customers['H_clusters'] = hclusters.labels_\ncustomers1=customers[customers.H_clusters==0]\ncustomers2=customers[customers.H_clusters==1]\ncustomers3=customers[customers.H_clusters==2]\nplt.figure(figsize = (10,10))\nsns.scatterplot(x=\"Card Debt\", y=\"Other Debt\", color = 'red', data=customers1)\nsns.scatterplot(x=\"Card Debt\", y=\"Other Debt\", color = 'green', data=customers2)\nsns.scatterplot(x=\"Card Debt\", y=\"Other Debt\", color = 'blue', data=customers3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On the Card Debt-Other debt graph we can see a similar clustering with KMeans but with just 2 clusters. the values are still mixed together but the information is better separated than other graphs.","metadata":{}},{"cell_type":"markdown","source":"**DBSCAN Clustering**","metadata":{}},{"cell_type":"markdown","source":"For better evaluation of the results, we are also comparing the results from DBSCAN clustering algorithm with the other two.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\ndbclusters = DBSCAN(eps=0.3, min_samples=10).fit(customers[['DebtIncomeRatio', 'Card Debt']]) \nprint(dbclusters.labels_) #print the full aray of clusters\nprint(dbclusters.labels_.min()) #show the minimum and the maximum value of the array so we know how many clusters we have.\nprint(dbclusters.labels_.max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers['DBSCAN_clusters'] = dbclusters.labels_\ncustomers0=customers[customers.DBSCAN_clusters==-1]\ncustomers1=customers[customers.DBSCAN_clusters==0]\ncustomers2=customers[customers.DBSCAN_clusters==1]\ncustomers3=customers[customers.DBSCAN_clusters==2]\ncustomers4=customers[customers.DBSCAN_clusters==3]\ncustomers5=customers[customers.DBSCAN_clusters==4]\nplt.figure(figsize = (10,10))\nsns.scatterplot(x='DebtIncomeRatio', y=\"Card Debt\", color = 'brown', data=customers0)\nsns.scatterplot(x='DebtIncomeRatio', y=\"Card Debt\", color = 'red', data=customers1)\nsns.scatterplot(x='DebtIncomeRatio', y=\"Card Debt\", color = 'green', data=customers2)\nsns.scatterplot(x='DebtIncomeRatio', y=\"Card Debt\", color = 'blue', data=customers3)\nsns.scatterplot(x='DebtIncomeRatio', y=\"Card Debt\", color = 'orange', data=customers4)\nsns.scatterplot(x='DebtIncomeRatio', y=\"Card Debt\", color = 'black', data=customers5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The clustering result done with DBSCAN is made out of 6 clusters which might be useful for a bussines decision if the purpose is to create multipla marketing strategies. Now let's see hoe DBSCAN is working with other graphs","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import DBSCAN\ndbclusters = DBSCAN(eps=0.3, min_samples=10).fit(customers[['Card Debt', 'Other Debt']]) \ndbclusters.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers['DBSCAN_clusters'] = dbclusters.labels_\ncustomers1=customers[customers.DBSCAN_clusters==-1]\ncustomers2=customers[customers.DBSCAN_clusters==0]\nplt.figure(figsize = (10,10))\nsns.scatterplot(x='Card Debt', y=\"Other Debt\", color = 'red', data=customers1)\nsns.scatterplot(x='Card Debt', y=\"Other Debt\", color = 'green', data=customers2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we compare the result from the Hierachical clustering with DBSCAN, they look similar but DBSCAN is definetely separing the clusters in a more precise way. howeve, if we need more clusters, the KMeans is the choice.","metadata":{}},{"cell_type":"code","source":"dbclusters = DBSCAN(eps=0.3, min_samples=10).fit(customers[['Years Employed', 'Income']]) \ndbclusters.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers['DBSCAN_clusters'] = dbclusters.labels_\ncustomers1=customers[customers.DBSCAN_clusters==-1]\ncustomers2=customers[customers.DBSCAN_clusters==0]\nplt.figure(figsize = (10,10))\nsns.scatterplot(x='Years Employed', y=\"Income\", color = 'red', data=customers1)\nsns.scatterplot(x='Years Employed', y=\"Income\", color = 'green', data=customers2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see DBSCAN made 2 clusters for this graph and one of the clusters contains just one element which can be considered an outlier. Let's get rid of this to see how DBSCAN will change after that.","metadata":{}},{"cell_type":"code","source":"customers.drop(labels = 532, inplace=True)\ncustomers[customers['Income'] == customers['Income'].max()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndbclusters = DBSCAN(eps=0.3, min_samples=10).fit(customers[['Years Employed', 'Income']]) \ndbclusters.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customers['DBSCAN_clusters'] = dbclusters.labels_\n#customers1=customers[customers.DBSCAN_clusters==-1]\ncustomers2=customers[customers.DBSCAN_clusters==0]\nplt.figure(figsize = (10,10))\nsns.scatterplot(x='Years Employed', y=\"Income\", color = 'red', data=customers2)\n#sns.scatterplot(x='Years Employed', y=\"Income\", color = 'green', data=customers2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once we take out the outlier we can see DBSCAN is doing no clustering at all. As a result, the best clustering algorithms for our smart sale system could be KMeans but this really depends on what the bussiness is planing to achieve.","metadata":{}}]}