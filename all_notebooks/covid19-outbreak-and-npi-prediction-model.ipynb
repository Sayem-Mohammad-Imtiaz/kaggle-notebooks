{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Building a model that can assist in scaling up the implementation of NPIs based on kalman filter and machine learning algorithms.\n\nAs we know the **Covid19** is frightening all of us and has affected more than 185 countries, there is an urgent need in ways to control its further spread and help people saving lives. There is a lot of research work that is being done currently in the field of effectiveness of different mitigation measures for **Covid19**. In this notebook we describe an effective way that can be used to deploy npis in regions based on various parameters. We hope this work would help us fighting the covid19 pandemic in an effective way."},{"metadata":{},"cell_type":"markdown","source":"This map shows the present status of the coronavirus infections across the world."},{"metadata":{"trusted":true},"cell_type":"code","source":"from ipywidgets import Image\nf = open(\"../input/covidimages/c1.jpg\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Goal/Tasks\n\n1) Our major goal is to provide guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.\n\n2) Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.\n\n3) Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.\n\n4) The last point would be building a guideline model that will help control, any such pandemic and not let things go too far."},{"metadata":{},"cell_type":"markdown","source":"# What all did we do?\n\n1) First step: Prediction of the outbreak, forecasting in terms of number of cases, number of deaths etc.\n\n2) Second step: Prediction of the best-suited npi for the particular situation based on the previous history of implementations.\n"},{"metadata":{},"cell_type":"markdown","source":"* **Methods used for the first step:** We used regression techniques accompanied by a tool called Kalman filter which is found effective to give accurate predictions for such outbreaks.\n\n* Kalman filter: In statistics and control theory, Kalman filtering, also known as linear quadratic estimation (LQE), is an algorithm that uses a series of measurements observed over time, containing statistical noise and other inaccuracies, and produces estimates of unknown variables that tend to be more accurate than those based on a single measurement alone, by estimating a joint probability distribution over the variables for each timeframe. The filter is named after Rudolf E. Kálmán, one of the primary developers of its theory.\n\n* Source: https://en.wikipedia.org/wiki/Kalman_filter\n\n* **Methods used for the second step** We have techniques like SMOTE(synthetic minority oversampling technique) to tackle our data-imbalance problem, various machine learning techniques like Random-Forest, KNearNeighboursClassifier, Decision trees etc. We have shown the performance of some of the best machine-learning techniques on our dataset in step two."},{"metadata":{},"cell_type":"markdown","source":"# Internet must be turned on for this notebook to work."},{"metadata":{},"cell_type":"markdown","source":"**Importing the necessary packages and libraries that are necessary for step-one.**"},{"metadata":{"id":"KvnEBAYTm4DR","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport cv2\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* All the time-series data that is required for our analysis and prediction is taken from: https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset\n\n* Although similar data is being maintained by this github repository: https://github.com/CSSEGISandData/COVID-19\n\n* We will be going ahead with the data that is maintained by the above-mentioned **repository**.\n\n* **Disclaimer**: Turn your internet on for this."},{"metadata":{},"cell_type":"markdown","source":"* Using the urls, we import the required data and store it different dataframes.\n\n* We are using the **Pandas** library for doing this task.\n\n* You can read the **official-documentation** for any additional information: https://pandas.pydata.org/"},{"metadata":{"id":"e8gHkwW5nHK1","trusted":true},"cell_type":"code","source":"\nurl = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\nconfirmed = pd.read_csv(url, error_bad_lines=False)\nurl = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\ndeath = pd.read_csv(url, error_bad_lines=False)\nurl = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'\nrecover = pd.read_csv(url, error_bad_lines=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Replacing some of the column names."},{"metadata":{"id":"v_FOeJqInHUE","trusted":true},"cell_type":"code","source":"# replacing the region names\nconfirmed['Country/Region']= confirmed['Country/Region'].str.replace(\"Mainland China\", \"China\")\nconfirmed['Country/Region']= confirmed['Country/Region'].str.replace(\"US\", \"Unites States\")\ndeath['Country/Region']= death['Country/Region'].str.replace(\"Mainland China\", \"China\")\ndeath['Country/Region']= death['Country/Region'].str.replace(\"US\", \"Unites States\")\nrecover['Country/Region']= recover['Country/Region'].str.replace(\"Mainland China\", \"China\")\nrecover['Country/Region']= recover['Country/Region'].str.replace(\"US\", \"Unites States\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In the next step, we are going to merge the population-values of different countries with all the above created dataframes.\n\n* The population-values are being taken from this dataset on kaggle: https://www.kaggle.com/fernandol/countries-of-the-world"},{"metadata":{"id":"2pbyHeX7nHOb","trusted":true},"cell_type":"code","source":"population=pd.read_csv('../input/populationv/population.csv', sep=',', encoding='latin1') \nconfirmed=pd.merge(confirmed, population,how='left' ,on=['Province/State','Country/Region'])\ndeath=pd.merge(death, population,how='left' ,on=['Province/State','Country/Region'])\nrecover=pd.merge(recover, population,how='left' ,on=['Province/State','Country/Region'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We make sure this is working by looking at the first 5 rows of the dataframe, by calling the pandas.dataframe.head() function.\n\n* You can find more information regarding this function here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"u2oMrBHsoI3a","outputId":"8da1aefb-96d2-4b57-8882-18558fc54c59","trusted":true},"cell_type":"code","source":"#now we merge the regions\nconfirmed['region']=confirmed['Country/Region'].map(str)+'_'+confirmed['Province/State'].map(str)\ndeath['region']=death['Country/Region'].map(str)+'_'+death['Province/State'].map(str)\nrecover['region']=recover['Country/Region'].map(str)+'_'+recover['Province/State'].map(str)\nconfirmed=confirmed[confirmed.iloc[:,confirmed.shape[1]-3]>1000]# regions having more than 1000 active cases till date.\nconfirmed.iloc[:5,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we extract the data in the form of time-series with the help the below displayed- create_ab function which accepts the dataframe as its parameter and returns a dataframe.\n\n* Along with it we drop certain columns such as Lat(i.e the latitude), as we dont have any use of them at this step."},{"metadata":{"id":"ndpHxVEroM_1","trusted":true},"cell_type":"code","source":"#a function to just extract the time series information excluding other columns.\ndef create_ab(df):\n  ab=df\n  ab=ab.drop(['Province/State', 'Country/Region','Lat', 'Long',' Population '], axis=1)\n  ab.set_index('region')\n  ab=ab.T\n  ab.columns=ab.loc['region']\n  ab=ab.drop('region')\n  ab=ab.fillna(0)\n  ab=ab.reindex(sorted(ab.columns), axis=1)\n  return (ab)","execution_count":null,"outputs":[]},{"metadata":{"id":"ucRFO8xYoOmV","trusted":true},"cell_type":"code","source":"#creating time series data of confirmed, deaths, recovered cases.\nts=create_ab(confirmed)\nts_d=create_ab(death)\nts_rec=create_ab(recover)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Again we use the pd.DataFrame.head() to take a peek!"},{"metadata":{"id":"hfQC5YXmw51m","outputId":"b89403ae-b3a4-4022-c19d-c6ed8490a1f2","trusted":true},"cell_type":"code","source":"ts.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting time-series data\n* Based on whatever time-series data we created, now we are plotting it with the help of Matplotlib library.\n\n* You can find more information regarding matplotlib here: https://matplotlib.org/"},{"metadata":{"id":"rDKASL4ooOpr","outputId":"02344b23-8ec1-4149-8423-c9677b135738","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\np=ts.reindex(ts.max().sort_values(ascending=False).index, axis=1)\np.iloc[:,:1].plot(marker='*',figsize=(10,4)).set_title('Daily Total Confirmed - UNITED STATES OF AMERICA',fontdict={'fontsize': 22})\np.iloc[:,2:10].plot(marker='*',figsize=(10,4)).set_title('Daily Total Confirmed - Major areas',fontdict={'fontsize': 22})\n\np_d=ts_d.reindex(ts.mean().sort_values(ascending=False).index, axis=1)\np_d.iloc[:,:1].plot(marker='*',figsize=(10,4)).set_title('Daily Total Death - Hubei',fontdict={'fontsize': 22})\np_d.iloc[:,2:10].plot(marker='*',figsize=(10,4)).set_title('Daily Total Death - Major areas',fontdict={'fontsize': 22})\n\np_r=ts_rec.reindex(ts.mean().sort_values(ascending=False).index, axis=1)\np_r.iloc[:,:1].plot(marker='*',figsize=(10,4)).set_title('Daily Total Recoverd - Hubei',fontdict={'fontsize': 22})\np_r.iloc[:,2:10].plot(marker='*',figsize=(10,4)).set_title('Daily Total Recoverd - Major areas',fontdict={'fontsize': 22})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now, saving the ts_r dataframe to our directory, by using the pd.to_csv() function."},{"metadata":{"id":"WS_ycGmSoY0t","trusted":true},"cell_type":"code","source":"ts_r=ts.reset_index()\nts_r=ts_r.rename(columns = {'index':'date'})\nts_r['date']=pd.to_datetime(ts_r['date'] ,errors ='coerce')\n#ts_r.to_csv(r'../input/populationv/ts_r2.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Error in importing rpy2 package.\n* I was trying to import the rpy2 package where in R can be used in python kernels, but there was an error for me on kaggle.\n\n* So im just writing the code here but not actually executing it, you can check my github repository here, for detailed output.\n\n* I will save all the necessary files for the step2 (npi prediction) and import them again during that."},{"metadata":{},"cell_type":"markdown","source":"* The rpy2 package helps in using R in python kernels.\n\n* You can find more information here: https://rpy2.github.io/doc/latest/html/introduction.html"},{"metadata":{"id":"qioKxLCwodQU","outputId":"aa3f5c63-121e-4fbc-b256-4b15a0db774d","trusted":true,"collapsed":true},"cell_type":"code","source":"#importing rpy2 package for using R.\nimport rpy2\n%load_ext rpy2.ipython","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Installing pracma and reshape packages.\n\n* You can find more information regarding pracma here: https://cran.r-project.org/package=pracma\n\n* You can find more information regarding reshape here: https://cran.r-project.org/package=reshape"},{"metadata":{"id":"_Lo3Tj8JodUC","outputId":"05bee040-7251-44b2-853c-ec2f2c091b6b","trusted":true},"cell_type":"code","source":"%%R\ninstall.packages('pracma')\ninstall.packages('reshape')","execution_count":null,"outputs":[]},{"metadata":{"id":"D6rsB9xXoyMT","outputId":"6eeccea0-0462-4e66-856b-2db40cf444a4","trusted":false},"cell_type":"code","source":"%%R\nrequire(pracma)\nrequire(Metrics)\nrequire(readr)\nall<- read_csv(\"../input/population/ts_r2.csv\")\nall$X1<-NULL\ndate<-all[,1]\ndate[nrow(date) + 1,1] <-all[nrow(all),1]+1\npred_all<-NULL\nfor (n in 2:ncol(all)-1) {\n  Y<-ts(data = all[n+1], start = 1, end =nrow(all)+1)  \n  sig_w<-0.01\n  w<-sig_w*randn(1,100) # acceleration which denotes the fluctuation (Q/R) rnorm(100, mean = 0, sd = 1)\n  sig_v<-0.01\n  v<-sig_v*randn(1,100)   \n  t<-0.45\n  phi<-matrix(c(1,0,t,1),2,2)\n  gama<-matrix(c(0.5*t^2,t),2,1)\n  H<-matrix(c(1,0),1,2)\n  #Kalman\n  x0_0<-p0_0<-matrix(c(0,0),2,1)\n  p0_0<-matrix(c(1,0,0,1),2,2)\n  Q<-0.01\n  R<-0.01\n  X<-NULL\n  X2<-NULL\n  pred<-NULL\n  for (i in 0:nrow(all)) {\n    namp <-paste(\"p\", i+1,\"_\",i, sep = \"\")\n    assign(namp, phi%*%(get(paste(\"p\", i,\"_\",i, sep = \"\")))%*%t(phi)+gama%*%Q%*%t(gama))\n    namk <- paste(\"k\", i+1, sep = \"\")\n    assign(namk,get(paste(\"p\", i+1,\"_\",i, sep = \"\"))%*%t(H)%*%(1/(H%*%get(paste(\"p\", i+1,\"_\",i, sep = \"\"))%*%t(H)+R)))\n    namx <- paste(\"x\", i+1,\"_\",i, sep = \"\")\n    assign(namx,phi%*%get(paste(\"x\", i,\"_\",i, sep = \"\")))\n    namE <- paste(\"E\", i+1, sep = \"\")\n    assign(namE,Y[i+1]-H%*%get(paste(\"x\", i+1,\"_\",i, sep = \"\")))\n    namx2 <- paste(\"x\", i+1,\"_\",i+1, sep = \"\")\n    assign(namx2,get(paste(\"x\", i+1,\"_\",i, sep = \"\"))+get(paste(\"k\", i+1, sep = \"\"))%*%get(paste(\"E\", i+1, sep = \"\")))\n    namp2 <- paste(\"p\", i+1,\"_\",i+1, sep = \"\")\n    assign(namp2,(p0_0-get(paste(\"k\", i+1, sep = \"\"))%*%H)%*%get(paste(\"p\", i+1,\"_\",i, sep = \"\")))\n    X<-rbind(X,get(paste(\"x\", i+1,\"_\",i,sep = \"\"))[1])\n    X2<-rbind(X2,get(paste(\"x\", i+1,\"_\",i,sep = \"\"))[2])\n    if(i>2){\n      remove(list=(paste(\"p\", i-1,\"_\",i-2, sep = \"\")))\n      remove(list=(paste(\"k\", i-1, sep = \"\")))\n      remove(list=(paste(\"E\", i-1, sep = \"\")))\n      remove(list=(paste(\"p\", i-2,\"_\",i-2, sep = \"\")))\n      remove(list=(paste(\"x\", i-1,\"_\",i-2, sep = \"\")))\n      remove(list=(paste(\"x\", i-2,\"_\",i-2, sep = \"\")))}\n  }\n  pred<-NULL\n  pred<-cbind(Y,X,round(X2,4))\n  pred<-as.data.frame(pred)\n  pred$region<-colnames(all[,n+1])\n  pred$date<-date$date\n  pred$actual<-rbind(0,(cbind(pred[2:nrow(pred),1])/pred[1:nrow(pred)-1,1]-1)*100)\n  pred$predict<-rbind(0,(cbind(pred[2:nrow(pred),2])/pred[1:nrow(pred)-1,2]-1)*100)\n  pred$pred_rate<-(pred$X/pred$Y-1)*100\n  pred$X2_change<-rbind(0,(cbind(pred[2:nrow(pred),3]-pred[1:nrow(pred)-1,3])))\n  pred_all<-rbind(pred_all,pred)\n}\npred_all<-cbind(pred_all[,4:5],pred_all[,1:3])\nnames(pred_all)[5]<-\"X2\"\npred_all=pred_all[with( pred_all, order(region, date)), ]\npred_all<-pred_all[,3:5]","execution_count":0,"outputs":[]},{"metadata":{"id":"uzDEpxb8o-Ly","trusted":false},"cell_type":"code","source":"p=%R pred_all\n","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Merging the R ouput due to the package problem."},{"metadata":{"id":"h7QADM06pEnE","trusted":false},"cell_type":"code","source":"\nt=ts\nt=t.stack().reset_index(name='confirmed')\nt.columns=['date', 'region','confirmed']\nt['date']=pd.to_datetime(t['date'] ,errors ='coerce')\nt=t.sort_values(['region', 'date'])\n\ntemp=t.iloc[:,:3]\ntemp=temp.reset_index(drop=True)\nfor i in range(1,len(t)+1):\n if(temp.iloc[i,1] is not temp.iloc[i-1,1]):\n    temp.loc[len(temp)+1] = [temp.iloc[i-1,0]+ pd.DateOffset(1),temp.iloc[i-1,1], 0] \ntemp=temp.sort_values(['region', 'date'])\np.set_index(temp.index,inplace=True)\n#temp=temp.reset_index(drop=True)\ntemp['Y']=p['Y']\ntemp['X']=p['X']\ntemp['X2']=p['X2']\n","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* For details regarding minimum and maximum temperatures, reference was taken from the dataset used in this kernel: https://www.kaggle.com/davidbnn92/weather-data/data"},{"metadata":{"id":"IF-Iw0cfpNuC","trusted":true},"cell_type":"code","source":"w=pd.read_csv('../input/weather/w.csv', sep=',', encoding='latin1')\nw['date']=pd.to_datetime(w['date'],format='%d/%m/%Y')\n#w['date']=pd.to_datetime(w['date'],errors ='coerce')\n\nw_forecast=pd.read_csv('../input/weather/w_forecast.csv', sep=',', encoding='latin1')\nw_forecast['date']=pd.to_datetime(w_forecast['date'],format='%d/%m/%Y')","execution_count":null,"outputs":[]},{"metadata":{"id":"vE-A78ggqQJJ","trusted":true},"cell_type":"code","source":"t=ts\nt=t.stack().reset_index(name='confirmed')\nt.columns=['date', 'region','confirmed']\nt['date']=pd.to_datetime(t['date'] ,errors ='coerce')\nt=t.sort_values(['region', 'date'])\n\n# Add 1 Future day for prediction\nt=t.reset_index(drop=True)\nfor i in range(1,len(t)+1):\n  if(t.iloc[i,1] is not t.iloc[i-1,1]):\n    t.loc[len(t)+1] = [t.iloc[i-1,0]+ pd.DateOffset(1),t.iloc[i-1,1], 0] \nt=t.sort_values(['region', 'date'])\nt=t.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training our regression algorithm."},{"metadata":{"id":"8nGigR6-qeF1","outputId":"90fdc5da-cd4f-4b1c-8bdc-209ad700517e","trusted":true,"collapsed":true},"cell_type":"code","source":"t['1_day_change']=t['3_day_change']=t['7_day_change']=t['1_day_change_rate']=t['3_day_change_rate']=t['7_day_change_rate']=t['last_day']=0\nfor i in range(1,len(t)):\n  if(t.iloc[i,1] is t.iloc[i-2,1]):\n    t.iloc[i,3]=t.iloc[i-1,2]-t.iloc[i-2,2]\n    t.iloc[i,6]=(t.iloc[i-1,2]/t.iloc[i-2,2]-1)*100\n    t.iloc[i,9]=t.iloc[i-1,2]\n  if(t.iloc[i,1] is t.iloc[i-4,1]):\n    t.iloc[i,4]=t.iloc[i-1,2]-t.iloc[i-4,2]\n    t.iloc[i,7]=(t.iloc[i-1,2]/t.iloc[i-4,2]-1)*100\n  if(t.iloc[i,1] is t.iloc[i-8,1]):\n    t.iloc[i,5]=t.iloc[i-1,2]-t.iloc[i-8,2]\n    t.iloc[i,8]=(t.iloc[i-1,2]/t.iloc[i-8,2]-1)*100\nt=t.fillna(0)  \nt=t.merge(temp[['date','region', 'X']],how='left',on=['date','region'])\nt=t.rename(columns = {'X':'kalman_prediction'}) \nt=t.replace([np.inf, -np.inf], 0)\nt['kalman_prediction']=round(t['kalman_prediction'])\ntrain=t.merge(confirmed[['region',' Population ']],how='left',on='region')\ntrain=train.rename(columns = {' Population ':'population'})\ntrain['population']=train['population'].str.replace(r\" \", '')\ntrain['population']=train['population'].str.replace(r\",\", '')\ntrain['population']=train['population'].fillna(1)\ntrain['population']=train['population'].astype('int32')\ntrain['infected_rate'] =train['last_day']/train['population']*10000\ntrain=train.merge(w,how='left',on=['date','region'])\ntrain=train.sort_values(['region', 'date'])\n### fill missing weather \nfor i in range(0,len(train)):\n    if(np.isnan(train.iloc[i,13])):\n        if(train.iloc[i,1] is train.iloc[i-1,1]):\n            train.iloc[i,13]=train.iloc[i-1,13]\n            train.iloc[i,14]=train.iloc[i-1,14]","execution_count":null,"outputs":[]},{"metadata":{"id":"4kYJ04aeq5E_","outputId":"cae0ae9b-121f-4bce-8e05-b18213f816a7","trusted":false},"cell_type":"code","source":"# Select region\nregion='China_Hubei'\nplace=0\n\nevaluation=pd.DataFrame(columns=['region','mse','rmse','mae'])\nfor i in range(1,len(t)):\n    if(t.iloc[i,1] is not t.iloc[i-1,1]):\n        ex=np.array(t.iloc[i-len(ts):i,10])\n        pred=np.array(t.iloc[i-len(ts):i,2])\n        evaluation=evaluation.append({'region': t.iloc[i-1,1], 'mse': np.power((ex - pred),2).mean(),'rmse':sqrt(mean_squared_error(ex,pred)),'mae': (abs(ex - pred)).mean()}, ignore_index=True)\np=t[t['region']==region][['date','region','confirmed','kalman_prediction']]\np=p.rename(columns = {'confirmed':'confirmedd'})\np.iloc[len(p)-1,2]=None\np=p.set_index(['date'])\np.iloc[:,1:].plot(marker='o',figsize=(16,8)).set_title('Kalman Prediction - Select Region to Change - {}'.format(p.iloc[0,0]))\nprint(evaluation[evaluation['region']==p.iloc[0,0]])\n","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Displaying the output for the above cell, by executing the code on jupyter-notebook. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from ipywidgets import Image\nf = open(\"../input/graphs/g1.png\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{"id":"FkkiPoWNdlpy","outputId":"340086ce-ad30-4222-d8df-fafd6f9168ae","trusted":false},"cell_type":"code","source":"plt.style.use('ggplot')\n# Select region\n#region= 'Israel_nan'\n#region='Brazil_nan'\n#region='Unites States_nan'\nregion='India_nan'\n#region='China_Hubei'\n\nevaluation=pd.DataFrame(columns=['region','mse','rmse','mae'])\nplace=0\nfor i in range(1,len(t)):\n  if(t.iloc[i,1] is not t.iloc[i-1,1]):\n    ex=np.array(t.iloc[i-len(ts):i,10])\n    pred=np.array(t.iloc[i-len(ts):i,2])\n    evaluation=evaluation.append({'region': t.iloc[i-1,1], 'mse': np.power((ex - pred),2).mean(),'rmse':sqrt(mean_squared_error(ex,pred)),'mae': (abs(ex - pred)).mean()}, ignore_index=True)\np=t[t['region']==region][['date','region','confirmed','kalman_prediction']]\np=p.rename(columns = {'confirmed':'confirmed'})\np.iloc[len(p)-1,2]=None\np=p.set_index(['date'])\np.iloc[30:,1:].plot(marker='o',figsize=(16,8)).set_title('Kalman Prediction - {}'.format(p.iloc[0,0]))\nprint(evaluation[evaluation['region']==p.iloc[0,0]])\n","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Displaying the output of the code in the above cell.\n\n* Choosing the region as India and displaying the results."},{"metadata":{"trusted":true},"cell_type":"code","source":"from ipywidgets import Image\nf = open(\"../input/pictures/g2.png\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{"id":"Y8MClOCqPn7H"},"cell_type":"markdown","source":"* By doing the below mentioned steps, we are trying to find out the importance of each variables, used for the the prediction, using random forest classifier provied by h20 package.\n\n* You can find further information regarding h20 here: http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/intro.html\n\n* Find more information on linear regression here: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"},{"metadata":{},"cell_type":"markdown","source":"# Importing the necessary packages for finding out the variable importance"},{"metadata":{"id":"p7hBdtF9nazz","outputId":"0094f54f-986a-4e7e-9d69-f8a013dd517c","trusted":true},"cell_type":"code","source":"!pip install h2o\nimport h2o\nfrom h2o.estimators import H2ORandomForestEstimator\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\nh2o.init(min_mem_size='8G')\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting the dates and giving more weight to the recent dates."},{"metadata":{"id":"GgD0p2HUB36a","trusted":false},"cell_type":"code","source":"train=train.fillna(0) \ntrain_df=train[train['date']<'2020-02-17']\nboots=train_df[train_df['date']>='2020-02-14'] #to give more weight for recent days\ntrain_df=train_df.append([boots[boots['date']>='2020-02-14']]*1000,ignore_index=True)\ntrain_df_hubei=train_df[train_df['region']=='China_Hubei']\ntest=train[train['date']>='2020-02-17']\ntest=test[test['date']<'2020-02-19']","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Taking into account, all the variables regarding the variable importance check."},{"metadata":{"id":"b5FZJ91nHXVV","trusted":false},"cell_type":"code","source":"x_col=[#'region',\n            '1_day_change', '3_day_change','7_day_change',\n             '1_day_change_rate', \n            '3_day_change_rate',\n             '7_day_change_rate', \n            'last_day', 'kalman_prediction','infected_rate', 'min', 'max'\n          ]","execution_count":0,"outputs":[]},{"metadata":{"id":"zCUbwajQHhyB"},"cell_type":"markdown","source":"* Here we are using the kalmin filtered inputs for our linear regression model, which are much accurate due to the usage of the immediate previous time step."},{"metadata":{"id":"GfKn9_37HaiY","trusted":false},"cell_type":"code","source":"x=train_df[x_col]\ny=train_df['confirmed']\nreg = LinearRegression().fit(x,y)\n\npred2=reg.predict(test[x_col]); pred2=pd.DataFrame(pred2); pred2=round(pred2)\npred2['confirmed']=test['confirmed'].values; pred2['date']=test['date'].values; pred2['region']=test['region'].values\n#pred2.iloc[:55]","execution_count":0,"outputs":[]},{"metadata":{"id":"NUdGPU_NIQsk","outputId":"210e63b4-fb88-4aa7-d3e1-3bf66449eed6","trusted":false},"cell_type":"code","source":"train_h20 = h2o.H2OFrame(train_df)\ntrain_h20_hubei = h2o.H2OFrame(train_df_hubei) # different model for Hubei\ntraining_columns = ['region','1_day_change', '3_day_change', '7_day_change', '1_day_change_rate', '3_day_change_rate',\n                    '7_day_change_rate', 'last_day', 'kalman_prediction','infected_rate', 'min', 'max'\n                   ]                 \n# Output parameter train against input parameters\nresponse_column = 'confirmed'\n\n# model = H2ORandomForestEstimator(ntrees=300, max_depth=12)\n# model.train(x=training_columns, y=response_column, training_frame=train_h20)\nmodel_hubei = H2ORandomForestEstimator(ntrees=300, max_depth=12)\nmodel_hubei.train(x=training_columns, y=response_column, training_frame=train_h20_hubei)\n\ntest_h20 = h2o.H2OFrame(test)\n#test_h20_hubei = h2o.H2OFrame(test_hubei)","execution_count":0,"outputs":[]},{"metadata":{"id":"pTjprpIjQcrf"},"cell_type":"markdown","source":"Feature importance for the Hubei model using RF"},{"metadata":{"id":"2XVHEifpQuYn","outputId":"306e49ca-6b3b-47cd-e583-fe0cf159a69d","trusted":false},"cell_type":"code","source":"model_hubei.varimp(True).iloc[:,:] # Feature importance for Hubei Model RF","execution_count":0,"outputs":[]},{"metadata":{"id":"IEKTGagGdyjK"},"cell_type":"markdown","source":"* The correlation matrix, for showing the variable importance."},{"metadata":{"id":"Taa9AtObdvzO","outputId":"ce54366d-c7cd-4bf6-88ae-1a72acb896cc","trusted":false},"cell_type":"code","source":"from string import ascii_letters\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"white\")\n# Compute the correlation matrix\ncorr = train.iloc[:,2:].corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.9, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nprint ('Correlation Matrix')","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Displaying the output of the code mentioned in the above cell"},{"metadata":{"trusted":true},"cell_type":"code","source":"from ipywidgets import Image\nf = open(\"../input/pictures/rf.png\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{"id":"l7ZEk94-d-f7","outputId":"f472b9f2-6cf5-4483-bb51-db6aaadaa74d","trusted":false},"cell_type":"code","source":"\nprint('Correlation To Confirmed') \nprint (corr.confirmed)","execution_count":0,"outputs":[]},{"metadata":{"id":"D9w-WInVfISR"},"cell_type":"markdown","source":"* A graph showing the variation of minimum and maximum temperatures in the india Region for example purpose."},{"metadata":{"id":"lrJaFHqYfCus","outputId":"3210eac9-2fc0-423a-ab98-fa62e9a6ef84","trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt\np=train[['date','region','min','max']].set_index('date')\np=p[p['region']=='India_nan']\np.iloc[:37,:].plot(marker='*',figsize=(12,4),color=['#19303f','#cccc00']).set_title('Daily Min/Max Temperature - India',fontdict={'fontsize': 20})\n","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Displaying the output of the code mentioned in the above cell."},{"metadata":{"trusted":true},"cell_type":"code","source":"from ipywidgets import Image\nf = open(\"../input/pictures/mm.png\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{"id":"2jbPzB3JgQmP"},"cell_type":"markdown","source":"* A table showing more infected areas vs temperature."},{"metadata":{"id":"hH4gS3bNgP8j","outputId":"7cb41372-d3fd-4352-9367-1644b2ff830e","trusted":false},"cell_type":"code","source":"avg_temp=train[['region','confirmed','min','max']]  # from 17-02-20\navg_temp=avg_temp.groupby(by='region').mean()\navg_temp=avg_temp.sort_values('confirmed',ascending=False)\nprint( 'Most infected Areas Avg Temperature')","execution_count":0,"outputs":[]},{"metadata":{"id":"auOs5yvagetO"},"cell_type":"markdown","source":"## **X Day ahead prediction using kalman filter**"},{"metadata":{"id":"-DXmR50bgt1V"},"cell_type":"markdown","source":"* Using the R interface again.\n"},{"metadata":{"id":"fH_1Y7upgmpj","outputId":"11a61d86-bdaa-446c-e2fd-e37dfbbadaae","trusted":false},"cell_type":"code","source":"\n%%R\ninstall.packages('reshape')","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Find more information regarding readr here: https://cran.r-project.org/package=readr\n\n* Find more information regarding metrics here: https://cran.r-project.org/web/packages/Metrics/Metrics.pdf"},{"metadata":{"id":"8EkAcJ3VhQNc","outputId":"677c0295-ae9b-45de-e662-80246b927598","trusted":false},"cell_type":"code","source":"%%R\nrequire(pracma)\nrequire(Metrics)\nrequire(readr)\nlibrary(reshape)\nall<- read_csv(\"..input/populationv/ts_r2.csv\")\nall$X1<-NULL\nfor (i in 1:30) { # Set i days prediction\n  if( i>1) {all<-all_new}\n  date<-all[,1]\n  date[nrow(date) + 1,1] <-all[nrow(all),1]+1\n  pred_all<-NULL\n  for (n in 2:ncol(all)-1) {\n    Y<-ts(data = all[n+1], start = 1, end =nrow(all)+1)  \n    sig_w<-0.01\n    w<-sig_w*randn(1,100) # acceleration which denotes the fluctuation (Q/R) rnorm(100, mean = 0, sd = 1)\n    sig_v<-0.01\n    v<-sig_v*randn(1,100)  \n    t<-0.45\n    phi<-matrix(c(1,0,t,1),2,2)\n    gama<-matrix(c(0.5*t^2,t),2,1)\n    H<-matrix(c(1,0),1,2)\n    #Kalman\n    x0_0<-p0_0<-matrix(c(0,0),2,1)\n    p0_0<-matrix(c(1,0,0,1),2,2)\n    Q<-0.01\n    R<-0.01\n    X<-NULL\n    X2<-NULL\n    pred<-NULL\n    for (i in 0:nrow(all)) {\n      namp <-paste(\"p\", i+1,\"_\",i, sep = \"\")\n      assign(namp, phi%*%(get(paste(\"p\", i,\"_\",i, sep = \"\")))%*%t(phi)+gama%*%Q%*%t(gama))\n      namk <- paste(\"k\", i+1, sep = \"\")\n      assign(namk,get(paste(\"p\", i+1,\"_\",i, sep = \"\"))%*%t(H)%*%(1/(H%*%get(paste(\"p\", i+1,\"_\",i, sep = \"\"))%*%t(H)+R)))\n      namx <- paste(\"x\", i+1,\"_\",i, sep = \"\")\n      assign(namx,phi%*%get(paste(\"x\", i,\"_\",i, sep = \"\")))\n      namE <- paste(\"E\", i+1, sep = \"\")\n      assign(namE,Y[i+1]-H%*%get(paste(\"x\", i+1,\"_\",i, sep = \"\")))\n      namx2 <- paste(\"x\", i+1,\"_\",i+1, sep = \"\")\n      assign(namx2,get(paste(\"x\", i+1,\"_\",i, sep = \"\"))+get(paste(\"k\", i+1, sep = \"\"))%*%get(paste(\"E\", i+1, sep = \"\")))\n      namp2 <- paste(\"p\", i+1,\"_\",i+1, sep = \"\")\n      assign(namp2,(p0_0-get(paste(\"k\", i+1, sep = \"\"))%*%H)%*%get(paste(\"p\", i+1,\"_\",i, sep = \"\")))\n      X<-rbind(X,get(paste(\"x\", i+1,\"_\",i,sep = \"\"))[1])\n      X2<-rbind(X2,get(paste(\"x\", i+1,\"_\",i,sep = \"\"))[2])\n      if(i>2){\n        remove(list=(paste(\"p\", i-1,\"_\",i-2, sep = \"\")))\n        remove(list=(paste(\"k\", i-1, sep = \"\")))\n        remove(list=(paste(\"E\", i-1, sep = \"\")))\n        remove(list=(paste(\"p\", i-2,\"_\",i-2, sep = \"\")))\n        remove(list=(paste(\"x\", i-1,\"_\",i-2, sep = \"\")))\n        remove(list=(paste(\"x\", i-2,\"_\",i-2, sep = \"\")))}\n    } \n    pred<-NULL\n    pred<-cbind(Y,X,round(X2,4))\n    pred<-as.data.frame(pred)\n    pred$region<-colnames(all[,n+1])\n    pred$date<-date$date\n    pred$actual<-rbind(0,(cbind(pred[2:nrow(pred),1])/pred[1:nrow(pred)-1,1]-1)*100)\n    pred$predict<-rbind(0,(cbind(pred[2:nrow(pred),2])/pred[1:nrow(pred)-1,2]-1)*100)\n    pred$pred_rate<-(pred$X/pred$Y-1)*100\n    pred$X2_change<-rbind(0,(cbind(pred[2:nrow(pred),3]-pred[1:nrow(pred)-1,3])))\n    pred_all<-rbind(pred_all,pred)\n  }\n  pred_all<-cbind(pred_all[,4:5],pred_all[,1:3])\n  names(pred_all)[5]<-\"X2\"\n  pred_all<-pred_all[,1:5]\n       \npred_all_today=pred_all[with( pred_all, order(region, date)), ]\nall_new=all\n#all_new[nrow(all_new),1]<-all_new[nrow(all),1]+1\ntemp<-with(pred_all_today, pred_all_today[date == all[nrow(all),1]+1, ])\ntemp<-cbind(temp[,1:2],temp[,4])\ntemp2<-reshape(temp, direction = \"wide\", idvar = \"date\", timevar = \"region\")\nrand_num<-runif(ncol(temp2)-1, 0.9, 1.05)\ntemp2[,2:ncol(temp2)]<-temp2[,2:ncol(temp2)]*rand_num\ncolnames(temp2)=colnames(all_new)\nall_new<-rbind(all_new,temp2)\nall_new[,2:ncol(all_new)]<-round(all_new[,2:ncol(all_new)])\nfor (i in 2:ncol(all_new)) {\n  all_new[nrow(all_new),i]=max(all_new[nrow(all_new)-1,i],all_new[nrow(all_new),i])}\n}","execution_count":0,"outputs":[]},{"metadata":{"id":"xRyVFYtfh4Zm","trusted":false},"cell_type":"code","source":"all_new=%R all_new","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Converting the date column values dtype to datetime format.\n\n* Find out more information here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html"},{"metadata":{"id":"0jw2SpU_h9QK","trusted":false},"cell_type":"code","source":"all_new['date']=pd.to_datetime(all_new['date'],unit='d')","execution_count":0,"outputs":[]},{"metadata":{"id":"3SM1sdtqiDOZ","outputId":"1f3b4ba3-e001-47aa-bac5-ae9b0be7ecc6","trusted":false},"cell_type":"code","source":"# Select region\nregion=['date','China_Anhui', 'China_Beijing',\n       'China_Chongqing', 'China_Fujian', 'China_Gansu',\n       'China_Guangdong', 'China_Guangxi', 'China_Guizhou',\n       'China_Hainan', 'China_Hebei', 'China_Heilongjiang', 'China_Henan','China_Hunan', \n       'China_Jiangsu', 'China_Jiangxi', 'China_Jilin', 'China_Liaoning',\n       'China_Ningxia', 'China_Qinghai', 'China_Shaanxi',\n       'China_Shandong', 'China_Shanghai', 'China_Shanxi',\n       'China_Sichuan',  'China_Xinjiang',\n       'China_Yunnan', 'China_Zhejiang', \n        'Hong Kong_Hong Kong','Japan_nan','Others_Diamond Princess cruise ship']\n#p_kalman=all_new[region]\n#p=all_new\n#p.iloc[len(p)-1,2]=None\n#p_kalman=p_kalman.set_index(['date'])\n#p_kalman.iloc[:,:].plot(marker='o',figsize=(24,14)).set_title('Kalman Prediction')\n\np_kalman2=all_new[['date','India_nan']]\np_kalman2=p_kalman2.set_index(['date'])\np_kalman2.iloc[:,:].plot(marker='o',figsize=(16,8)).set_title('Kalman Prediction - Select Region to Change - {}'.format(p_kalman2.columns[0]))\n","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Displaying the output of the above mentioned code."},{"metadata":{"trusted":true},"cell_type":"code","source":"from ipywidgets import Image\nf = open(\"../input/pictures/30day.png\", \"rb\")\nimage = f.read()\nImage(value=image)","execution_count":null,"outputs":[]},{"metadata":{"id":"QNTnYVFWkh9F","outputId":"8cf520b9-d5f1-4541-8b5d-a26137d20bef","trusted":false},"cell_type":"code","source":"all_new.info()","execution_count":0,"outputs":[]},{"metadata":{"id":"fG1bQXnGltDt"},"cell_type":"markdown","source":"All the countries having the predictions"},{"metadata":{"id":"St7UvHAImOqL"},"cell_type":"markdown","source":"# **Iterative Regression**"},{"metadata":{},"cell_type":"markdown","source":"* Fitting the regression model to our data."},{"metadata":{"id":"A0twXJiOl1VL","outputId":"73119352-4f7d-432f-95ab-4d6815bc66b7","trusted":false},"cell_type":"code","source":"t_iter=all_new.set_index(['date'])\nt_iter=t_iter.stack().reset_index(name='confirmed')\nt_iter.columns=['date', 'region','confirmed']\nt_iter['date']=pd.to_datetime(t_iter['date'] ,errors ='coerce')\nt_iter=t_iter.sort_values(['region', 'date'])\n\nt_iter=t_iter.reset_index(drop=True)\nfor i in range(1,len(t_iter)+1):\n  if(t_iter.iloc[i,1] is not t_iter.iloc[i-1,1]):\n    t_iter.loc[len(t_iter)+1] = [t_iter.iloc[i-1,0]+ pd.DateOffset(1),t_iter.iloc[i-1,1], 0] \nt_iter=t_iter.sort_values(['region', 'date'])\nt_iter=t_iter.reset_index(drop=True)\n\nt_iter['1_day_change']=t_iter['3_day_change']=t_iter['7_day_change']=t_iter['1_day_change_rate']=t_iter['3_day_change_rate']=t_iter['7_day_change_rate']=t_iter['last_day']=0\nfor i in range(1,len(t_iter)):\n  if(t_iter.iloc[i,1] is t_iter.iloc[i-2,1]):\n    t_iter.iloc[i,3]=t_iter.iloc[i-1,2]-t_iter.iloc[i-2,2]\n    t_iter.iloc[i,6]=(t_iter.iloc[i-1,2]/t_iter.iloc[i-2,2]-1)*100\n    t_iter.iloc[i,9]=t_iter.iloc[i-1,2]\n  if(t_iter.iloc[i,1] is t_iter.iloc[i-4,1]):\n    t_iter.iloc[i,4]=t_iter.iloc[i-1,2]-t_iter.iloc[i-4,2]\n    t_iter.iloc[i,7]=(t_iter.iloc[i-1,2]/t_iter.iloc[i-4,2]-1)*100\n  if(t_iter.iloc[i,1] is t_iter.iloc[i-8,1]):\n    t_iter.iloc[i,5]=t_iter.iloc[i-1,2]-t_iter.iloc[i-8,2]\n    t_iter.iloc[i,8]=(t_iter.iloc[i-1,2]/t_iter.iloc[i-8,2]-1)*100\nt_iter=t_iter.fillna(0)  \n\n# t_iter=t_iter.merge(temp[['date','region', 'X']],how='left',on=['date','region'])\n# t_iter=t_iter.rename(columns = {'X':'kalman_prediction'}) \nt_iter=t_iter.replace([np.inf, -np.inf], 0)\nt_iter['kalman_prediction']=round(t_iter['confirmed'])\ntest_iter=t_iter.merge(confirmed[['region',' Population ']],how='left',on='region')\ntest_iter=test_iter.rename(columns = {' Population ':'population'})\ntest_iter['population']=test_iter['population'].str.replace(r\" \", '')\ntest_iter['population']=test_iter['population'].str.replace(r\",\", '')\ntest_iter['population']=test_iter['population'].fillna(1)\ntest_iter['population']=test_iter['population'].astype('int32')\ntest_iter['infected_rate'] =test_iter['last_day']/test_iter['population']*10000\ntest_iter=test_iter.merge(w,how='left',on=['date','region'])\n#test_iter=test_iter.sort_values(['region', 'date'])\ntest_iter_temp=test_iter[np.isnan(test_iter['min'])]\ntest_iter_temp=test_iter_temp.drop(columns=['min', 'max'])\ntest_iter_temp=test_iter_temp.merge(w_forecast,how='left',on=['date','region'])\ntest_iter=test_iter.dropna()\ntest_iter=test_iter.append(test_iter_temp)\ntest_iter=test_iter.sort_values(['region', 'date'])\n### fill missing weather \nfor i in range(0,len(test_iter)):\n  if(np.isnan(test_iter.iloc[i,13])):\n    if(test_iter.iloc[i,1] is test_iter.iloc[i-1,1]):\n      test_iter.iloc[i,13]=test_iter.iloc[i-1,13]+abs(test_iter.iloc[i-1,13]*.01)\n      test_iter.iloc[i,14]=test_iter.iloc[i-1,14]+abs(test_iter.iloc[i-1,14]*.01)","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Filling the missing values with 0 values."},{"metadata":{"id":"F_SHDEnVmdP0","trusted":false},"cell_type":"code","source":"test_iter=test_iter.fillna(0) ","execution_count":0,"outputs":[]},{"metadata":{"id":"vvQE36ZTnDiU","trusted":false},"cell_type":"code","source":"pred=reg.predict(test_iter[x_col]); pred=pd.DataFrame(pred); pred.columns = ['prediction'];pred=round(pred)\npred['confirmed']=test_iter['confirmed'].values; pred['date']=test_iter['date'].values; pred['region']=test_iter['region'].values\nfor i in range(1,len(pred)):\n    if(pred.iloc[i,3] is pred.iloc[i-1,3]):\n      if(pred.iloc[i,0]<pred.iloc[i-1,1]):\n        pred.iloc[i,0]=pred.iloc[i-1,1]\npred = pred.pivot_table(index='date',columns='region',values='prediction') # pivot pred df","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Displaying the information about the datatypes present in different columns of the pred-DataFrame."},{"metadata":{"id":"cuRv8_YZjWKu","outputId":"a41296ed-1bf8-4690-e77d-b1ea10aca246","trusted":false},"cell_type":"code","source":"pred.info()","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now saving the above obtained predictions for using them in the npi prediction step."},{"metadata":{"id":"wNERy0nN4_Xs","trusted":false},"cell_type":"code","source":"d = pred\nd = d.T\nd.to_csv(r'../input/tsv/pred.csv')\n","execution_count":0,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now, we go into step 2 wherin we are going to try and predict the best applicable npis for a particular situation.\n\n* Again as a prelimnary step, we will be importing all the necessary packages, libraries and algorithms.\n\n* For more details regarding the algorithms and their usage on your own dataset, please find it here: https://scikit-learn.org/"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the libraries\nimport numpy as np\nimport pandas as pd\nimport datetime \nimport calendar \n# data visualization\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\nfrom sklearn.preprocessing import StandardScaler\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import ExtraTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing our cleaned and modified dataset.\n\n* Sources:\n\n* https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset \n\n* https://www.kaggle.com/davidoj/covid19-national-responses-dataset\n\n* https://www.kaggle.com/fernandol/countries-of-the-world\n\nAlso the prediction-values which I have saved in the last phase of step 1.\n\nSome details regarding the columns of mastersheet prediction are:\n\n* Each row is an entry/instance of a particular Npi getting implemented.\n\n* Country: This column represents the country to which the entry belongs to.\n\n* Net migration: The net migration rate is the difference between the number of immigrants (people coming into an area) and the number of emigrants (people leaving an area) throughout the year.\n\n* Population density: Population density is the number of individuals per unit geographic area, for example, number per square meter, per hectare, or per square kilometer.\n\n* Sex Ratio: The sex ratio is the ratio of males to females in a population.\n\n* Population age-distribution: Age distribution, also called Age Composition, in population studies, the proportionate numbers of persons in successive age categories in a given population. (0-14yrs/60+yrs %)\n\n* Health physicians per 1000 population: Number of medical doctors (physicians), including generalist and specialist medical practitioners, per 1 000 population.\n\n* Mobile cellular subscription per 100 inhabitants: Mobile cellular telephone subscriptions are subscriptions to a public mobile telephone service that provide access to the PSTN using cellular technology.\n\n* Active on day: The number of active cases of covid19 infections in that particular country on the day it was implemented.\n\n* Sevenday, twelveday and thirty day predictions are for active cases from the date it was implemented.\n\n* And the date-implemented is converted to whether it was a week-day or a week-end to make it usable for training.\n\n* The last column represents the category to which the npi that implemented belonged to.\n\nCategory 1: Public -health measures and social-distancing.\n\nCategory 2: Social-economic measures and movement-restrictions.\n\nCategory 3: Partial/complete lockdown.\n"},{"metadata":{},"cell_type":"markdown","source":"* To categorise the npis we followed a 5 step analysis:\n\n* Step 1: We choosed 6 different countries that have implemented atleast one of the above-mentioned npis.\n\n* Step 2: We had chosen a particular date wherein one of the npi was implemented.\n\n* Step 3: From that date (chosen) we had calculated 5day, 8day, 12day growth rate in the number of confirmed cases in that country.\n\n* Step 4: According to: 1) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4327893/\n    2) https://www.worldometers.info/coronavirus/coronavirus-incubation-period/\n    we took a reference that, over 50% of the people who are affected on day1 show symptoms by day5, over 30% of the people affected on           day1 show symptoms by day8 and the last 20% start showing symptoms by day12. Assuming that, they get a checkup as soon as they are             showing symptoms, we had calculated a cumilative growth rate.\n* Step 5: This cumilative growth rate was not very accurate due to the population densities of the countries being different. So, we had normalised the obtained scores from step4 by the population densities. That gave us the following results.\n\n* More information can be found here: https://archive.is/JNx03 and https://archive.is/UA3g14"},{"metadata":{},"cell_type":"markdown","source":"* [ (896.4961042933885, 'CHINA', 'SOCIAL DISTANCING'),\n*  (720.7571447424511, 'FRANCE', 'PUBLIC HEALTH MEASURES'),\n*  (578.0345389562175, 'SPAIN', 'SOCIAL AND ECONOMIC MEASURES'),\n*  (527.7087251438776, 'IRAN', 'MOV RESTRICTION'),\n*  (484.1021819976962, 'ITALY', 'PARTIAL LOCKDOWN'),\n*  (207.67676767676767, 'INDIA', 'COMPLETE LOCKDOWN')]\n\nEx: (Cumilative growthrate(normalised), Country Name, Measure-taken)"},{"metadata":{},"cell_type":"markdown","source":"* So the above analysis shows the decreasing order of growth rates, however this is not very accurate due to various other reasons, but this gives a rough estimate of the effectiveness/strength of the npis."},{"metadata":{},"cell_type":"markdown","source":"* With this categorisation in mind, let's go the next step."},{"metadata":{},"cell_type":"markdown","source":"So, the dataset that we used is: https://www.kaggle.com/davidoj/covid19-national-responses-dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/mastersheetpred/mastersheetprediction.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Noting down the information of the dataset for a quick look,"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# There are 54 countries in the dataset, which have more than 1000cases at the time of creating this notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.Country.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now extracting the input-features and the target by df.iloc function.\n* For more information regarding the df.iloc, please use this link: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = dataset.iloc[:, 0:13].values# input feature\ny = dataset.iloc[:, 13].values# label/target","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Checking the shape by using the numpy.ndarray.shape function.\n\n* Find more information here: https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The next step is encoding the categorical data, this was done using labelencoder and one-hot encoder.\n\n* You can find more information regarding the sklearn-preprocessing library here: http://scikit-learn.org/stable/modules/preprocessing.html\n\n* Instead of one-hot encoder, we used using ColumnTransformer here: http://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encoding the X\nct = ColumnTransformer([(\"Country\", OneHotEncoder(), [0])], remainder = 'passthrough')\nX = ct.fit_transform(X)\nX = X[:, 0:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now we split the dataset into train and test set.\n\n* We will used train_test_split for this part, find more information here: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We would be feature scaling the inputs.\n\n* We used standard StandardScaler function from sklearn's preprocessing library.\n\n* Find more information here: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train.toarray())\nX_test = sc.transform(X_test.toarray())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking the frequencies of our categories in the target array-(y)\n\n* We use the np.unique() function, find more information here: https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(y, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Class-imbalance problem.\n\n* We notice that there is a class-imbalance problem. The caegory 3 is a minority class which is only 2% of the dataset.\n\n* To tackle this problem we have used smote technique(over-sampling technique as we have less number of examples), find more information here: https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install smote-variants","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import smote_variants as sv\n\noversampler= sv.MulticlassOversampling(sv.distance_SMOTE())\n# X_samp and y_samp contain the oversampled train-dataset\nX_samp, y_samp= oversampler.sample(X_train, y_train)\n#oversampling the test dataset\nX_samptest, y_samptest= oversampler.sample(X_test, y_test.ravel())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_samp.shape, y_samptest.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The problem of multilabel classification.\n\n* Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes.\n\n* Our dataset suffers from multilabel classification problem, this is due to that many countries have implemented npis that belong to more than one category in a single day, this makes an entry which have all the input variables same but the target-category is different. \n\n* This makes it a difficult problem with the existing features.\n\n* Drawback: We did not have any strong differentiating feature in our dataset."},{"metadata":{},"cell_type":"markdown","source":"# To tackle this problem of multilabel classfication problem to an extent we used various multilearn models from sklearn."},{"metadata":{},"cell_type":"markdown","source":"# To use the multioutputclassifier, we encoded the category column."},{"metadata":{"trusted":true},"cell_type":"code","source":"yovs = pd.read_csv(\"../input/oversample/oversampletargets.csv\")\nyovs = yovs.iloc[:,1:]\nyovs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We use the multi-ouput classifier using the randomforest as the base-algorithm.\n\n* Find more information here: http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"#multi-output classifier and the random-forest classifier.\nfrom sklearn.multioutput import MultiOutputClassifier\nrandom_forest = RandomForestClassifier(n_estimators=1000, criterion = 'entropy')#with single column targets\nrandom_forest.fit(X_samp, y_samp)\n\nY_pred = random_forest.predict(X_samp)\n\nmulti_target_forest = MultiOutputClassifier(random_forest, n_jobs=-1)\nYpred = multi_target_forest.fit(X_samp, yovs).predict(X_samp)\nmulti_target_forest.score(X_samp, yovs)\nacc_multi = round(multi_target_forest.score(X_samp, yovs) * 100, 2)\nprint(acc_multi,\"trainset-multi-output\")\n\nrandom_forest.score(X_samp, y_samp)\nacc_random_forest = round(random_forest.score(X_samp, y_samp) * 100, 2)\nprint(acc_random_forest,\"trainset-Randomforest\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Hamming loss for the predictions on train set by randomforest classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import hamming_loss\nprint(\"Hamming loss = \",hamming_loss(y_samp,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We used the pipeline technique from sklearn.\n\n* Find more information here: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n\n* We used the NeighborhoodComponentsAnalysis and  KNeighborsClassifier with pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import (NeighborhoodComponentsAnalysis, KNeighborsClassifier)\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\nnca = NeighborhoodComponentsAnalysis(random_state=10)\nknn = KNeighborsClassifier(n_neighbors=1)\nnca_pipe = Pipeline([('nca', nca), ('knn', knn)])\nnca_pipe.fit(X_samp, y_samp)\n\nprint(nca_pipe.score(X_samp, y_samp)*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding the oob score using RF classifier.\n\n* Find more information here: https://en.wikipedia.org/wiki/Out-of-bag_error"},{"metadata":{},"cell_type":"markdown","source":"* We are finding the oob-score for our train set."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100, criterion = 'entropy', oob_score = True)\nrandom_forest.fit(X_samp, y_samp)\nY_prediction = random_forest.predict(X_samp)\n\nrandom_forest.score(X_samp, y_samp)\n\nacc_random_forest2 = round(random_forest.score(X_samp, y_samp) * 100, 2)\nprint(round(acc_random_forest2,2,), \"%\")\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We used Cross validation also and calculated mean accuracies.\n\n* For more information, refer this: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100, criterion = 'entropy')\nscores = cross_val_score(rf, X_samp, y_samp, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNeighborsClassifier\n\n* Find more information here: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 1) \nknn.fit(X_samp, y_samp)  \nY_pred = knn.predict(X_samp)  \nacc_knn = round(knn.score(X_samp, y_samp) * 100, 2)\nprint(acc_knn,\"trainset\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import hamming_loss\nprint(\"Hamming loss-trainset = \",hamming_loss(y_samp,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision tree-classifier\n* For more information: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_samp, y_samp)  \nY_pred = decision_tree.predict(X_samp)  \nacc_decision_tree = round(decision_tree.score(X_samp, y_samp) * 100, 2)\nprint(acc_decision_tree,\"trainset\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The hamming loss has gone down."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import hamming_loss\nprint(\"Hamming loss-trainset = \",hamming_loss(y_samp,Y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter tuning using GridsearchCV\n\n* We tuned the hyperparameters of the random-forest classifier using gridsearchcv.\n\n* Find more information here: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\nclf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\nclf.fit(X_samp, y_samp)\nclf.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* I'm not executing the above cell, because it will take a huge time to get completed, rather the output is displayed below."},{"metadata":{},"cell_type":"markdown","source":"{'criterion': 'gini',\n 'min_samples_leaf': 10,\n 'min_samples_split': 2,\n 'n_estimators': 100}"},{"metadata":{},"cell_type":"markdown","source":"* Retraining the Rf with the above mentioned hyperparameters."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier(criterion = \"gini\",    #the best parameters for the rf classifier\n                                       min_samples_leaf = 10, \n                                       min_samples_split = 2,   \n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_samp, y_samp)\nY_prediction = random_forest.predict(X_samptest)\n\nrandom_forest.score(X_samp, y_samp)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implementing classifier chain technique as it tackles multilabel classification problems.\n\n* For more information: http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.ClassifierChain.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from skmultilearn.problem_transform import ClassifierChain\nfrom sklearn.ensemble import RandomForestClassifier\n\n# initialize LabelPowerset multi-label classifier with a RandomForest\nclassifier = ClassifierChain(\n    classifier = RandomForestClassifier(n_estimators=200),\n    require_dense = [False, True]\n)\n\n# train\nclassifier.fit(X_samp, y_samp)\n\n# predict\npredictions = classifier.predict(X_samp)\npred2 = classifier.predict(X_samptest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy-train:\",metrics.accuracy_score(y_samp.reshape(-1,1), predictions.todense())*100, \"%\")\nprint(\"Accuracy-test:\",metrics.accuracy_score(y_samptest.reshape(-1,1), pred2.todense())*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using the onevsrest classifier.\n\n* For more details: http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.multiclass import OneVsRestClassifier\npred = OneVsRestClassifier(RandomForestClassifier(n_estimators=200, class_weight='balanced')).fit(X_samp, y_samp).predict(X_samp)\npred2 = OneVsRestClassifier(RandomForestClassifier(n_estimators=200, class_weight='balanced')).fit(X_samp, y_samp).predict(X_samptest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy-train:\",metrics.accuracy_score(y_samp, pred)*100, \"%\")\nprint(\"Accuracy-test:\",metrics.accuracy_score(y_samptest, pred2)*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* This is by far the best accuracy we have got on the train and test set."},{"metadata":{},"cell_type":"markdown","source":"# Next, RadiusNeighborsClassifier with radius =1.\n\n* It is considered to be a lazy algorithm.\n\n* This gives the best accuracy as this fits the best to our dataset. This is because the training-examples we used for classification are very closed to each other in the feature space.\n\n* For more information: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import RadiusNeighborsClassifier\nneigh = RadiusNeighborsClassifier(radius=1, weights='distance',algorithm='brute')\nneigh.fit(X_samp, y_samp)\nypredu = neigh.predict(X_samp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy-train:\",metrics.accuracy_score(y_samp, ypredu)*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import hamming_loss\nprint(\"Hamming loss = \",hamming_loss(y_samp,ypredu))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find more about the jaccard_score: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import jaccard_score\njs= jaccard_score(y_samp, ypredu, average=None)\njs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find more about the classification report: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\ntarget_names = ['class 1', 'class 2', 'class 3']\nprint(classification_report(y_samp, ypredu, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We plot the confusion matrix to see what's actually happening.\n\n* Find more information here: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndef plot_confusion_matrix(cm, classes, normalized=True, cmap='bone'):\n    plt.figure(figsize=[7, 6])\n    norm_cm = cm\n    if normalized:\n        norm_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        sns.heatmap(norm_cm, annot=cm, fmt='g', xticklabels=classes, yticklabels=classes, cmap=cmap)\n        plt.savefig('confusion-matrix.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ntarget_names = ['class 1', 'class 2', 'class 3']\ncm = confusion_matrix(y_samp, ypredu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(cm, classes=target_names)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The drawbacks of our approach.\n\n* This above-mentioned approach has many drawbacks, one of them is an incomplete dataset.\n\n* There are no good-differentiating features in the dataset.\n\n* In our approach we are not able to decide the effectiveness and a go-to plan of action for deploying npis.\n\n* All the data-points are very-similar to one-another, hence it is being difficult for the algorithm to learn."},{"metadata":{},"cell_type":"markdown","source":"# Further work:\n\n* There could be a set of strong differentiating features in the dataset, which will make the generalization easy.\n\n* There can be further categorisation of npis for better implementation of them.\n\n* The dataset can also be combined with economic parameters further, to understand the economic feasibilty of the npi-implementation.\n\n* It can further be used to predict the decrease in growth rates, once an npi is implemented to further note the real-time effectiveness of the npis in a particular demographic"},{"metadata":{},"cell_type":"markdown","source":"Note: This work is inspired from many data science resources. Any traces of replications, which may appear, is purely co-incidental."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}