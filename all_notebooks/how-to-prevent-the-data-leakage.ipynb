{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Hi all.  üôã‚Ä¢‚ôÇÔ∏è \n\n‚Äã\n#### We continue our **Beginner-Intermediate Friendly Machine Learning series**, which would help anyone who wants to learn or refresh the basics of ML.\n‚Äã\n#### What we have covered: \n\n#### [Beginner Friendly Detailed Explained EDAs ‚Äì For anyone at the beginnings of DS/ML journey](https://www.kaggle.com/general/253911#1393015) ‚úîÔ∏è\n‚Äã\n#### [BIAS & VARIANCE TRADEOFF](https://www.kaggle.com/kaanboke/ml-basics-bias-variance-tradeoff) ‚úîÔ∏è\n‚Äã\n#### [LINEAR ALGORITHMS](https://www.kaggle.com/kaanboke/ml-basics-linear-algorithms)  ‚úîÔ∏è\n‚Äã\n#### [NONLINEAR ALGORITHMS](https://www.kaggle.com/kaanboke/nonlinear-algorithms)  ‚úîÔ∏è\n‚Äã\n#### [The Most Used Methods to Deal with MISSING VALUES](https://www.kaggle.com/kaanboke/the-most-used-methods-to-deal-with-missing-values)  ‚úîÔ∏è\n‚Äã\n#### [Beginner Friendly End to End ML Project- Classification with Imbalanced Data](https://www.kaggle.com/kaanboke/beginner-friendly-end-to-end-ml-project-enjoy)  ‚úîÔ∏è\n‚Äã\n#### Today we will cover one of the main problems of the data preprocessing : **Data Leakage**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-13T06:21:01.619374Z","iopub.execute_input":"2021-08-13T06:21:01.619764Z","iopub.status.idle":"2021-08-13T06:21:01.633126Z","shell.execute_reply.started":"2021-08-13T06:21:01.619681Z","shell.execute_reply":"2021-08-13T06:21:01.631814Z"}}},{"cell_type":"markdown","source":"#### **By the way, when you like the topic, you can show it by supporting** üëç\n\n####  **Feel free to leave a comment in the notebook**. \n\n\n#### All the best ü§ò","metadata":{}},{"cell_type":"markdown","source":"![](https://miro.medium.com/max/1400/1*FUZS9K4JPqzfXDcC83BQTw.png)","metadata":{}},{"cell_type":"markdown","source":"Image Credit: https://miro.medium.com/","metadata":{"execution":{"iopub.status.busy":"2021-08-13T06:22:01.219768Z","iopub.execute_input":"2021-08-13T06:22:01.220163Z","iopub.status.idle":"2021-08-13T06:22:01.2262Z","shell.execute_reply.started":"2021-08-13T06:22:01.220125Z","shell.execute_reply":"2021-08-13T06:22:01.224934Z"}}},{"cell_type":"markdown","source":"<a id=\"toc\"></a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents</h3>\n    \n* [Data Leakage](#0)\n* [Target Leakage](#1)\n* [Training- Test Leakage](#2)\n* [How to Deal With Data Leakage](#3)\n* [Is Cross Validation Enough to Handle Data Leakage?](#4)\n* [Where is the Data Leakage?](#5)\n* [Using Pipeline](#6)\n* [Cross-Validation & Pipeline- Correct Data Preparation](#7)\n* [Conclusion](#8)\n* [References & Further Reading](#9)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"0\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>Data Leakage</b></font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"markdown","source":"> In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment.\n\n> Leakage is often subtle and indirect, making it hard to detect and eliminate. Leakage can cause a statistician or modeler to select a suboptimal model, which could be outperformed by a leakage-free model. \n\nReference: https://en.wikipedia.org/wiki/Leakage_(machine_learning)","metadata":{}},{"cell_type":"markdown","source":"![](https://smartwatermagazine.com/sites/default/files/styles/thumbnail-1180x647/public/water-pipe-2.jpg)","metadata":{}},{"cell_type":"markdown","source":"Image credit: https://smartwatermagazine.com/blogs/victoria-edwards/bunker-mentality-will-never-solve-worlds-water-leakage-problem","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \n\nfrom sklearn.model_selection import train_test_split,RepeatedStratifiedKFold,cross_val_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler,PowerTransformer,OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-13T15:06:36.782597Z","iopub.execute_input":"2021-08-13T15:06:36.783214Z","iopub.status.idle":"2021-08-13T15:06:36.793693Z","shell.execute_reply.started":"2021-08-13T15:06:36.783158Z","shell.execute_reply":"2021-08-13T15:06:36.792406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>Target Leakage</b></font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{"execution":{"iopub.status.busy":"2021-08-13T07:23:34.259198Z","iopub.execute_input":"2021-08-13T07:23:34.259979Z","iopub.status.idle":"2021-08-13T07:23:34.272833Z","shell.execute_reply.started":"2021-08-13T07:23:34.259882Z","shell.execute_reply":"2021-08-13T07:23:34.271009Z"}}},{"cell_type":"markdown","source":"- In the prediction modeling, we want get the best prediction on the target variable.\n- When we use the information, which shouldn't be expected to be present in the training, it causes the mess.\n- Let me give examples to clarify the issue.","metadata":{}},{"cell_type":"markdown","source":"#### Credit Card Application:\n\n- If we want to predict whether person receive a card or not \n- And we have a feature so called, 'paid_by_card'\n- We have to examine that, whether these payment were made by the card in question or not?\n- Don't think it is exaggerated, you will see a lot of similar examples, in your data science career.","metadata":{}},{"cell_type":"markdown","source":"- For further discussion on the target data leakage and different examples please refer to: https://www.researchgate.net/publication/221653692_Leakage_in_Data_Mining_Formulation_Detection_and_Avoidance","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>Training- Test Leakage</b></font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"markdown","source":"### Time Related Issues:","metadata":{}},{"cell_type":"markdown","source":"- Imagine we are dealing with the sale price.\n- We have a data between 2015-2020 sale prices.\n- If we randomly divide these years between test and training datsets, \n- We will inform the training data about the test data and also make useless our timeseries analysis.","metadata":{}},{"cell_type":"markdown","source":"### Data Preprocessing Issues:","metadata":{"execution":{"iopub.status.busy":"2021-08-13T08:16:50.689117Z","iopub.execute_input":"2021-08-13T08:16:50.689647Z","iopub.status.idle":"2021-08-13T08:16:50.694629Z","shell.execute_reply.started":"2021-08-13T08:16:50.689602Z","shell.execute_reply":"2021-08-13T08:16:50.693521Z"}}},{"cell_type":"markdown","source":"- All the preprocessing should be done with training data !!!\n- Featuring, scaling, normalization, missing value imputations, categorical value coding, etc.","metadata":{"execution":{"iopub.status.busy":"2021-08-13T08:21:08.736531Z","iopub.execute_input":"2021-08-13T08:21:08.736919Z","iopub.status.idle":"2021-08-13T08:21:08.742894Z","shell.execute_reply.started":"2021-08-13T08:21:08.736889Z","shell.execute_reply":"2021-08-13T08:21:08.741488Z"}}},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>How to Deal With Data Leakage</b></font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{"execution":{"iopub.status.busy":"2021-08-13T08:22:36.167935Z","iopub.execute_input":"2021-08-13T08:22:36.168264Z","iopub.status.idle":"2021-08-13T08:22:36.174561Z","shell.execute_reply.started":"2021-08-13T08:22:36.168235Z","shell.execute_reply":"2021-08-13T08:22:36.173173Z"}}},{"cell_type":"markdown","source":"- Remove leaky variables\n   - As we have mentioned in the credit card application prediction model, \n   - If we want to get minimze the contamination\n   - We have to remove 'fishy' variable 'paid_by_card' from our model.\n- Use cross validation\n-Use pipelines","metadata":{"execution":{"iopub.status.busy":"2021-08-13T08:26:07.456874Z","iopub.execute_input":"2021-08-13T08:26:07.457218Z","iopub.status.idle":"2021-08-13T08:26:07.462889Z","shell.execute_reply.started":"2021-08-13T08:26:07.457189Z","shell.execute_reply":"2021-08-13T08:26:07.461681Z"}}},{"cell_type":"markdown","source":"- In this study we will focus on data preparation part of the data leakage.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>Is Cross Validation Enough to Handle Data Leakage?</b></font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{"execution":{"iopub.status.busy":"2021-08-13T08:29:24.944946Z","iopub.execute_input":"2021-08-13T08:29:24.945432Z","iopub.status.idle":"2021-08-13T08:29:24.951425Z","shell.execute_reply.started":"2021-08-13T08:29:24.945397Z","shell.execute_reply":"2021-08-13T08:29:24.950387Z"}}},{"cell_type":"markdown","source":"- By using K-fold cross-validation, we  split the data k non-overlapping groups of rows. \n- And then our model is trained on the training dataset \n- And then our model is evaluated on the held-out fold. \n- For the data leakage part, we use train and test/validation seperately.\n","metadata":{}},{"cell_type":"markdown","source":"- Ok let's see everything in the action.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/water-potability/water_potability.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T15:06:36.795898Z","iopub.execute_input":"2021-08-13T15:06:36.796254Z","iopub.status.idle":"2021-08-13T15:06:36.853337Z","shell.execute_reply.started":"2021-08-13T15:06:36.796213Z","shell.execute_reply":"2021-08-13T15:06:36.852281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We have already worked on this data. I am not going to make a detailed analysis on this data. Please refer to [The Most Used Methods to Deal with MISSING VALUES](https://www.kaggle.com/kaanboke/the-most-used-methods-to-deal-with-missing-values)","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T15:06:36.85514Z","iopub.execute_input":"2021-08-13T15:06:36.855475Z","iopub.status.idle":"2021-08-13T15:06:36.87019Z","shell.execute_reply.started":"2021-08-13T15:06:36.855444Z","shell.execute_reply":"2021-08-13T15:06:36.868959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Ok we have mising values, and most of the machine learning algorithm can not tolerate the missing values \n- and we have to handle it.\n- Let's use mean imputation and by using Cross Validation evaluate our Logistic Regression model.","metadata":{}},{"cell_type":"markdown","source":"- First we fill the missing variable, by using each variable seperately.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/water-potability/water_potability.csv')\ndf['ph']= df['ph'].fillna(df['ph'].median())\ndf['Sulfate']= df['Sulfate'].fillna(df['Sulfate'].median())\ndf['Trihalomethanes']= df['Trihalomethanes'].fillna(df['Trihalomethanes'].median())\n\n\nX = df.drop('Potability', axis=1)\ny = df['Potability']\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n\nmodel = LogisticRegression(solver='liblinear')\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n\nresult = cross_val_score(model, X, y,  scoring='roc_auc',cv=cv, n_jobs=-1)\n\nprint(f'{round(np.mean(result),6)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T15:06:36.872124Z","iopub.execute_input":"2021-08-13T15:06:36.872479Z","iopub.status.idle":"2021-08-13T15:06:39.43531Z","shell.execute_reply.started":"2021-08-13T15:06:36.872448Z","shell.execute_reply":"2021-08-13T15:06:39.434319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We fill the missing values by using Simple Imputer.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/water-potability/water_potability.csv')\n\nX = df.drop('Potability', axis=1)\ny = df['Potability']\n\nimputer = SimpleImputer(strategy='median')\nX = imputer.fit_transform(X)\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n\nmodel = LogisticRegression(solver='liblinear')\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n\nresult = cross_val_score(model, X, y,  scoring='roc_auc',cv=cv, n_jobs=-1)\n\nprint(f'{round(np.mean(result),6)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T15:06:39.436705Z","iopub.execute_input":"2021-08-13T15:06:39.43699Z","iopub.status.idle":"2021-08-13T15:06:39.652709Z","shell.execute_reply.started":"2021-08-13T15:06:39.436961Z","shell.execute_reply":"2021-08-13T15:06:39.651453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Ok What we did?\n- First we fill the missing values with the 'median' scores.\n- Then we normalize the scales.\n- We use cross validation and seperated our training and validation data to evaluate.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>Where is the Data Leakage?</b></font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"markdown","source":"- **imputer = SimpleImputer(strategy='median')**\n\n- ![](https://thefinanalyst.com/wp-content/uploads/2021/04/ME.jpg)\n\n  - imputer uses dataset to get median scores for the variables which have missing values\n   - When we using cross validation to seperate training data,\n   - Training data has already knowledge on the test data about the median scores of the several features.\n   - Traning data knows the global median scores and has more knowledge on the global distribution of the features than it should.\n\n","metadata":{}},{"cell_type":"markdown","source":"- **scaler = MinMaxScaler()**\n   - As we have seen that, when we are using to normalize our data;\n   - We are using minimum nd maximum values of the features.","metadata":{}},{"cell_type":"markdown","source":"![](https://i.stack.imgur.com/EuitP.png)","metadata":{}},{"cell_type":"markdown","source":"\n- When we using cross validation to seperate training data,\n- Training data has already knowledge on the test data about the minimum and maximum scores of the features.\n- Because training data is scaled based on the global maximum and minimum values\n- Traning data has more knloedge on the global distribution of the features than it should.\n\n- Both of the results have a data leakage problem. \n- This can lead to make an incorrect estimation of model performance.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"6\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>Using Pipeline</b></font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"markdown","source":"- 'One definition of an ML pipeline is a means of automating the machine learning workflow by enabling data to be transformed and correlated into a model that can then be analyzed to achieve outputs. This type of ML pipeline makes the process of inputting data into the ML model fully automated.' (reference: https://algorithmia.com/blog/ml-pipeline)\n\n\n- When we use pipeline in our models, data pass through different automated steps before reaching the final output.\n\n- In our case, imputer will fit only on the training dataset, not the entire dataset or test set.\n\n- Let's see the coding.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/water-potability/water_potability.csv')\n\nX = df.drop('Potability', axis=1)\ny = df['Potability']\n\nmodel =LogisticRegression(solver='liblinear')\n\npipeline = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),('s',MinMaxScaler()),('m', model)]) \n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n\nresult = cross_val_score(pipeline, X, y,  scoring='roc_auc',cv=cv, n_jobs=-1)\n\nprint(f'{round(np.mean(result),6)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T15:06:39.654305Z","iopub.execute_input":"2021-08-13T15:06:39.654803Z","iopub.status.idle":"2021-08-13T15:06:40.124491Z","shell.execute_reply.started":"2021-08-13T15:06:39.654755Z","shell.execute_reply":"2021-08-13T15:06:40.123496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Ok we are seeing a litlle change in the model performance.\n- Becaues of the data leakage, one can expect better score on the first two model.\n- But there is very smal difference. \n- This might be because of the difficulty of the prediction task.\n- Important thing is, cross validation changes simply and incorrectly evaluating just the model.\n- By using pipeline, cross validation evaluate the entire pipleline of data preparation and model together.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"7\"></a>\n<font color=\"lightseagreen\" size=+2.5><b>Cross-Validation & Pipeline- Correct Data Preparation</b></font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-13T15:06:40.125592Z","iopub.execute_input":"2021-08-13T15:06:40.12618Z","iopub.status.idle":"2021-08-13T15:06:40.161288Z","shell.execute_reply.started":"2021-08-13T15:06:40.126141Z","shell.execute_reply":"2021-08-13T15:06:40.16042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We have already worked on this data. I am not going to make a detailed analysis on this data. Please refer to [Beginner Friendly end to end ML Project](https://www.kaggle.com/kaanboke/beginner-friendly-end-to-end-ml-project-enjoy)","metadata":{}},{"cell_type":"markdown","source":"- Before modeling:\n  - Handle the missing data.\n  - Change the categorical variables to numerical version for ML model\n  - Change the scale\n  - Normalize the distribution","metadata":{}},{"cell_type":"markdown","source":"- We will do everything by using pipeline, so that cross validation evaluate the entire pipleline of data preparation and model together.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv')\ndf=df.drop('id', axis=1)\ncategorical = [ 'hypertension', 'heart_disease', 'ever_married','work_type', 'Residence_type', 'smoking_status']\nnumerical = ['avg_glucose_level', 'bmi','age']\ny= df['stroke']\nX = df.drop('stroke', axis=1)\n\n\nmodel =LogisticRegression(solver='liblinear')\n\ntransformer = ColumnTransformer(transformers=[('imp',SimpleImputer(strategy='median'),numerical),('o',OneHotEncoder(),categorical)])\n\npipeline = Pipeline(steps=[('t', transformer),('p',PowerTransformer(method='yeo-johnson')),('m', model)])    \n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n\nresult = cross_val_score(pipeline, X, y,  scoring='roc_auc',cv=cv, n_jobs=-1)\n\nprint(f'{round(np.mean(result),3)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-13T15:06:40.162554Z","iopub.execute_input":"2021-08-13T15:06:40.163027Z","iopub.status.idle":"2021-08-13T15:06:43.533278Z","shell.execute_reply.started":"2021-08-13T15:06:40.162985Z","shell.execute_reply":"2021-08-13T15:06:43.532142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Ok let's go step by step.","metadata":{}},{"cell_type":"code","source":"transformer = ColumnTransformer(transformers=[('imp',SimpleImputer(strategy='median'),numerical),('o',OneHotEncoder(),categorical)])\n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T15:06:43.535477Z","iopub.execute_input":"2021-08-13T15:06:43.535833Z","iopub.status.idle":"2021-08-13T15:06:43.540674Z","shell.execute_reply.started":"2021-08-13T15:06:43.5358Z","shell.execute_reply":"2021-08-13T15:06:43.539878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- First we need to deal with the missing values in the numerical features by\n- Secondly we need to change categorical variable to ML readable numerical version by OneHotEncoder\n- Since we are dealing a part of the data we use Column Transformer for this task.","metadata":{}},{"cell_type":"code","source":"pipeline = Pipeline(steps=[('t', transformer),('p',PowerTransformer(method='yeo-johnson')),('m', model)])    \n","metadata":{"execution":{"iopub.status.busy":"2021-08-13T15:06:43.542504Z","iopub.execute_input":"2021-08-13T15:06:43.543177Z","iopub.status.idle":"2021-08-13T15:06:43.556259Z","shell.execute_reply.started":"2021-08-13T15:06:43.543142Z","shell.execute_reply":"2021-08-13T15:06:43.555236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We defined the steps of the pipeline.\n- First we put the what we have done in the column transformer\n- Then our numerical variables have skewness and different scales, we use the Power transformer to normalize them for Logistic regression model\n- We call the model","metadata":{}},{"cell_type":"code","source":"cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T15:06:43.557727Z","iopub.execute_input":"2021-08-13T15:06:43.558367Z","iopub.status.idle":"2021-08-13T15:06:43.56928Z","shell.execute_reply.started":"2021-08-13T15:06:43.558307Z","shell.execute_reply":"2021-08-13T15:06:43.568059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We have defined our cross validation evaluation method.\n- Since we are dealing imbalanced data, we selected stratified version of Kfold with 3 repeats.","metadata":{}},{"cell_type":"code","source":"result = cross_val_score(pipeline, X, y,  scoring='roc_auc',cv=cv, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-08-13T15:06:43.570726Z","iopub.execute_input":"2021-08-13T15:06:43.571354Z","iopub.status.idle":"2021-08-13T15:06:46.835981Z","shell.execute_reply.started":"2021-08-13T15:06:43.571318Z","shell.execute_reply":"2021-08-13T15:06:46.83505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- First we put the pipeline and our fetures and target variable\n- We defined our metric (Roc-Auc)\n- We put our cross validation method.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"8\"></a>\n<font color=\"darkblue\" size=+1.5><b>Conclusion</b></font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>","metadata":{}},{"cell_type":"markdown","source":"- Data preparation methods to the whole dataset results in data leakage that causes incorrect estimates of model performance.\n- To prevent this **data preparation must be prepared on the training dataset**.\n\n- Nothing else really. OK if you want, I will tell it one more time;\n- **Data preparation must be prepared on the training dataset**","metadata":{}},{"cell_type":"markdown","source":"#### By the way, when you like the topic, you can show it by supporting üëç\n\n####  **Feel free to leave a comment**. \n\n#### All the best ü§ò","metadata":{}},{"cell_type":"markdown","source":"- **Enjoy** ü§ò","metadata":{"execution":{"iopub.status.busy":"2021-08-13T14:42:24.198429Z","iopub.execute_input":"2021-08-13T14:42:24.199066Z","iopub.status.idle":"2021-08-13T14:42:24.214182Z","shell.execute_reply.started":"2021-08-13T14:42:24.198994Z","shell.execute_reply":"2021-08-13T14:42:24.21206Z"}}},{"cell_type":"markdown","source":"![](https://media.giphy.com/media/l2JJyDYEX1tXFmCd2/giphy.gif)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"9\"></a>\n<font color=\"darkblue\" size=+1.5><b>References & Further Reading</b></font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents</a>\n\n\n[Machine Learning - Beginner &Intermediate-Friendly BOOKS](https://www.kaggle.com/general/255972)","metadata":{}}]}