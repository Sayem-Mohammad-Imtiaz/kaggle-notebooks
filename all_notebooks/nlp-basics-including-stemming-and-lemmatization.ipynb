{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89515d5f9131c2e4b35d969ab43d372c19763db7"},"cell_type":"markdown","source":"**Stemming Words Code Examples**\n\nStemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\n#create an object of class PorterStemmer\nporter = PorterStemmer()\nlancaster=LancasterStemmer()\n#provide a word to be stemmed\nprint(\"Porter Stemmer\")\nprint(\"cats => \",porter.stem(\"cats\"))\nprint(\"trouble => \",porter.stem(\"trouble\"))\nprint(\"troubling =>\", porter.stem(\"troubling\"))\nprint(\"troubled => \",porter.stem(\"troubled\"))\nprint(\"Lancaster Stemmer\")\nprint(\"cats => \",lancaster.stem(\"cats\"))\nprint(\"trouble => \",lancaster.stem(\"trouble\"))\nprint(\"troubling =>\",lancaster.stem(\"troubling\"))\nprint(\"troubled => \",lancaster.stem(\"troubled\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84b05c4bfe6394ee3ef2c319f3e125e1faf85536"},"cell_type":"markdown","source":"**Stemming a  Complete Sentence**"},{"metadata":{"trusted":true,"_uuid":"a909dc83f715a806ad551b0e7acea95097d92147"},"cell_type":"code","source":"from nltk.tokenize import sent_tokenize, word_tokenize\ndef stemSentence(sentence):\n    token_words=word_tokenize(sentence)\n    token_words\n    stem_sentence=[]\n    for word in token_words:\n        stem_sentence.append(porter.stem(word))\n        stem_sentence.append(\" \")\n    return \"\".join(stem_sentence)\n\nsentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\nx=stemSentence(sentence)\nprint(x)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa6da7abea37807bc9a00a1bc8ae5034fe73b07b"},"cell_type":"markdown","source":"**Lemmatization**\n\nStemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n\nLemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. \n\nFor example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words.\n\nPython NLTK provides WordNet Lemmatizer that uses the WordNet Database to lookup lemmas of words.\n"},{"metadata":{"trusted":true,"_uuid":"6952be9b9e3279adcf5bfc227bb9bc8aa0ff6cd4"},"cell_type":"code","source":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\nsentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\npunctuations=\"?:!.,;\"\nsentence_words = nltk.word_tokenize(sentence)\nfor word in sentence_words:\n    if word in punctuations:\n        sentence_words.remove(word)\n\nsentence_words\nprint(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\nfor word in sentence_words:\n    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ad2a8e9501483b3b1c004030b96b48fa837660e"},"cell_type":"markdown","source":"In the above code output, you must be wondering that no actual root form has been given for any word, this is because they are given without context. \n\nYou need to provide the context in which you want to lemmatize that is the parts-of-speech (POS). This is done by giving the value for pos parameter in wordnet_lemmatizer.lemmatize.\n"},{"metadata":{"trusted":true,"_uuid":"399fe7f05121a2c91b766650c71808b90cf58943"},"cell_type":"code","source":"print(\"Lemmatization with POS Tagging \")\nprint(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\nfor word in sentence_words:\n    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3678c45b4e1d35213f684304e3e716dc99aad1e"},"cell_type":"markdown","source":"## Token Count"},{"metadata":{"trusted":true,"_uuid":"cf92283a81b0e540cc001699fd6c25068285fd6d"},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n# Import Counter\nfrom collections import Counter\n\n# Read text files\nf = open(\"../input/textdb/articles.txt\", \"r\")\narticle = f.read()\n#print()\n# Tokenize the article: tokens\ntokens = word_tokenize(article)\n\n# Convert the tokens into lowercase: lower_tokens\nlower_tokens = [t.lower() for t in tokens]\n\n# Create a Counter with the lowercase tokens: bow_simple\nbow_simple = Counter(lower_tokens)\n\n# Print the 10 most common tokens\nprint(bow_simple.most_common(10))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4886b23ba43fd7661d06235a00b30e80c0bc507"},"cell_type":"markdown","source":"**Text Pre-Processing in Practice**\n\nLets clean up text for better NLP results by removing stopwords, and lemmatizing. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n\n\n"},{"metadata":{"trusted":true,"_uuid":"4e13e93b1c95712d6f7c81e0c016eb67b006d6cf"},"cell_type":"code","source":"# Import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n# Import Counter\nfrom collections import Counter\n\n# Read text files\nf = open(\"../input/textdb/articles.txt\", \"r\")\narticle = f.read()\n#print()\n# Tokenize the article: tokens\ntokens = word_tokenize(article)\n\n# English Stop words\nenglish_stops = set(stopwords.words('english'))\n\n# Convert the tokens into lowercase: lower_tokens\nlower_tokens = [t.lower() for t in tokens]\n\n# Retain alphabetic words: alpha_only\nalpha_only = [t for t in lower_tokens if t.isalpha()]\n\n# Remove all stop words: no_stops\nno_stops = [t for t in alpha_only if t not in english_stops]\n\n# Instantiate the WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Lemmatize all tokens into a new list: lemmatized\nlemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n\n# Create the bag-of-words: bow\nbow = Counter(lemmatized)\n\n# Print the 10 most common tokens\nprint(bow.most_common(10))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78c7711f36102b0dfc76b5b35d60f3d4c36808a4"},"cell_type":"markdown","source":"## NER with NLTK\n\nYou're now going to have some fun with named-entity recognition! \n\nYour task is to use nltk to find the named entities in this article.\n\nWhat might the article be about, given the names you found?\n\nAlong with nltk, sent_tokenize and word_tokenize from nltk.tokenize have been pre-imported.\n"},{"metadata":{"trusted":true,"_uuid":"2049843ffc993822d4a7ea0c515ee79c7b4a6acc"},"cell_type":"code","source":"import nltk\n# Import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n# Import Counter\nfrom collections import Counter\n\n# Read text files\nf = open(\"../input/textdb/articles.txt\", \"r\")\narticle = f.read()\n#print()\n\n# Tokenize the article into sentences: sentences\nsentences = nltk.sent_tokenize(article)\n\n# Tokenize each sentence into words: token_sentences\ntoken_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n\n# Tag each tokenized sentence into parts of speech: pos_sentences\npos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n\n# Create the named entity chunks: chunked_sentences\nchunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n\n# Test for stems of the tree with 'NE' tags\nfor sent in chunked_sentences:\n    for chunk in sent:\n        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n            print(chunk)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79f369175cfc1537c8ab22353b65dd4b1ed3c3fe"},"cell_type":"markdown","source":"**Charting Practice**\n\nIn this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.\n\nYou'll use a defaultdict called ner_categories, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called chunked_sentences similar to the last exercise, but this time with non-binary category names.\n\nYou can use hasattr() to determine if each chunk has a 'label' and then simply use the chunk's .label() method as the dictionary key.\n"},{"metadata":{"trusted":true,"_uuid":"f16cb687771ff448ef1436bad617cd5b508c503a"},"cell_type":"code","source":"import nltk\n# Import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n# Import Counter\nfrom collections import Counter\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\n# Read text files\nf = open(\"../input/textdb/articles.txt\", \"r\")\narticle = f.read()\n\n# Tokenize the article into sentences: sentences\nsentences = nltk.sent_tokenize(article)\n\n# Tokenize each sentence into words: token_sentences\ntoken_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n\n# Tag each tokenized sentence into parts of speech: pos_sentences\npos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n\n# Create the named entity chunks: chunked_sentences\nchunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n\n# Create the defaultdict: ner_categories\nner_categories = defaultdict(int)\n\n# Create the nested for loop\nfor sent in chunked_sentences:\n    for chunk in sent:\n        if hasattr(chunk, 'label'):\n            ner_categories[chunk.label()] += 1\n            \n# Create a list from the dictionary keys for the chart labels: labels\nlabels = list(ner_categories.keys())\n\n# Create a list of the values: values\nvalues = [ner_categories.get(l) for l in labels]\n\n# Create the pie chart\nplt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n\n# Display the chart\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d13aa2bc4f0b5a7e18c7433077374cb012244e3b"},"cell_type":"markdown","source":"## Comparing NLTK with spaCy NER\n\nUsing the same text you used in the earlier exercise, you'll now see the results using spaCy's NER annotator. How will they compare?\n\nThe article has been pre-loaded as article. To minimize execution times, you'll be asked to specify the keyword arguments tagger=False, parser=False, matcher=False when loading the spaCy model, because you only care about the entity in this exercise.\n"},{"metadata":{"trusted":true,"_uuid":"653d5e1efac33d2ef81635315033167df596f574"},"cell_type":"code","source":"# Import spacy\nimport spacy\n\n# Read text files\nf = open(\"../input/textdb/articles.txt\", \"r\")\narticle = f.read()\n\n# Instantiate the English model: nlp\nnlp = spacy.load('en',tagger=False, parser=False, matcher=False)\n\n# Create a new document: doc\ndoc = nlp(article)\n\n# Print all of the found entities and their labels\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33168dd26701f6e42a72a0e5eaf3c047d2066657"},"cell_type":"markdown","source":"**CountVectorizer for text classification**\n\nIt's time to begin building your text classifier! \n\nThe data has been loaded into a DataFrame called df. Explore it in the IPython Shell to investigate what columns you can use. The .head() method is particularly informative.\n\nIn this exercise, you'll use pandas alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. \n\nTo begin, you'll set up a CountVectorizer and investigate some of its features.\n"},{"metadata":{"trusted":true,"_uuid":"64e159389144e3782e1b115d884c3f22477341ce"},"cell_type":"code","source":"# Import the necessary modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n#importing csv\ndf = pd.read_csv('../input/textdb3/fake_or_real_news.csv')\n# Print the head of df\nprint(df.head())\n\n# Create a series to store the labels: y\ny = df.label\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['text'],y,test_size=0.33, random_state=53)\n\n# Initialize a CountVectorizer object: count_vectorizer\ncount_vectorizer = CountVectorizer(stop_words='english')\n\n# Transform the training data using only the 'text' column values: count_train \ncount_train = count_vectorizer.fit_transform(X_train.values)\n\n# Transform the test data using only the 'text' column values: count_test \ncount_test = count_vectorizer.transform(X_test.values)\n\n# Print the first 10 features of the count_vectorizer\nprint(count_vectorizer.get_feature_names()[:10])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18eb7d21208fd89d03c4e8f27f7f920c577c0c66"},"cell_type":"markdown","source":"**TfidfVectorizer for text classification**\n\nSimilar to the sparse CountVectorizer created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a TfidfVectorizer and investigate some of its features.\nIn this exercise, you'll use pandas and sklearn along with the same X_train, y_train and X_test, y_test DataFrames and Series you created in the last exercise.\n"},{"metadata":{"trusted":true,"_uuid":"89714b1c7afe8a67f64699159c0770a87b85e217"},"cell_type":"code","source":"# Import the necessary modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n#importing csv\ndf = pd.read_csv('../input/textdb3/fake_or_real_news.csv')\n# Print the head of df\nprint(df.head())\n\n# Create a series to store the labels: y\ny = df.label\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['text'],y,test_size=0.33, random_state=53)\n\n\n# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n\n# Print the first 10 features\nprint(tfidf_vectorizer.get_feature_names()[:10])\n\n# Print the first 5 vectors of the tfidf training data\nprint(tfidf_train.A[:5])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f35947cf6528913a0cf39b1dca54cda681bdecf8"},"cell_type":"markdown","source":"**TfidfVectorizer for Text Classification**\n\nSimilar to the sparse CountVectorizer created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a TfidfVectorizer and investigate some of its features.\n\nIn this exercise, you'll use pandas and sklearn along with the same X_train, y_train and X_test, y_test DataFrames and Series you created in the last exercise.\n"},{"metadata":{"trusted":true,"_uuid":"02e3caa58385889958e1ec911b2e43aed92e16a5"},"cell_type":"code","source":"# Import the necessary modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#importing csv\ndf = pd.read_csv('../input/textdb3/fake_or_real_news.csv')\n# Print the head of df\nprint(df.head())\n\n# Create a series to store the labels: y\ny = df.label\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['text'],y,test_size=0.33, random_state=53)\n\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n\n# Print the first 10 features\nprint(tfidf_vectorizer.get_feature_names()[:10])\n\n# Print the first 5 vectors of the tfidf training data\nprint(tfidf_train.A[:5])\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df5cfbecbf86ca2e0cce08fed96f1ba1023f3837"},"cell_type":"markdown","source":"**Training and Testing the \"fake news\" Model with TfidfVectorizer**\n\nNow that you have evaluated the model using the CountVectorizer, you'll do the same using the TfidfVectorizer with a Naive Bayes model.\n\nThe training and test sets have been created, and tfidf_vectorizer, tfidf_train, and tfidf_test have been computed. Additionally, MultinomialNB and metricshave been imported from, respectively, sklearn.naive_bayesand sklearn.\n\n"},{"metadata":{"trusted":true,"_uuid":"cb4a70c1acd220f696ed6f124d3f90854050f8c9"},"cell_type":"code","source":"\n# Import the necessary modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\n#importing csv\ndf = pd.read_csv('../input/textdb3/fake_or_real_news.csv')\n# Print the head of df\nprint(df.head())\n\n# Create a series to store the labels: y\ny = df.label\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['text'],y,test_size=0.33, random_state=53)\n\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n\n# Create a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(tfidf_train,y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(tfidf_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test,pred)\nprint(\"Accuracy Score : \",score)\n\n# Calculate the confusion matrix: cm\ncm = metrics.confusion_matrix(y_test,pred, labels=['FAKE', 'REAL'])\nprint(\"Confusion Matrix : \\n\",cm)\n\n# Inspecting your model\n\nprint('Inspecting your model')\n\n# Get the class labels: class_labels\nclass_labels = nb_classifier.classes_\n\n# Extract the features: feature_names\nfeature_names = tfidf_vectorizer.get_feature_names()\n\n# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\nfeat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n\n# Print the first class label and the top 20 feat_with_weights entries\nprint('First class label and the top 20 feat_with_weights entries')\nprint(class_labels[0], feat_with_weights[:20])\n\n# Print the second class label and the bottom 20 feat_with_weights entries\nprint('Second class label and the bottom 20 feat_with_weights entries')\nprint(class_labels[1], feat_with_weights[-20:])\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}