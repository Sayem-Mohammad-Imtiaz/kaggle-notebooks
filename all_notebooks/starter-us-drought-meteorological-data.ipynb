{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> ## US Drought & Meteorological Data Starter Notebook\nThis notebook will walk you trough loading the data and create a Dummy Classifier, showing a range of F1 scores that correspond to random predictions if given theclass priors.","metadata":{}},{"cell_type":"markdown","source":"## Loading & Visualizing the Data\nIn this section, we load the training and validation data into numpy arrays and visualize the drought classes and meteorological attributes.","metadata":{}},{"cell_type":"markdown","source":"We load the json files for training, validation and testing into the ``files`` dictionary.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport os\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom scipy.interpolate import interp1d\nfrom sklearn.preprocessing import RobustScaler\n\nfiles = {}\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if 'train' in filename:\n            files['train'] = os.path.join(dirname, filename)\n        if 'valid' in filename:\n            files['valid'] = os.path.join(dirname, filename)\n        if 'test' in filename:\n            files['test'] = os.path.join(dirname, filename)","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-07-20T09:24:06.446423Z","iopub.execute_input":"2021-07-20T09:24:06.446796Z","iopub.status.idle":"2021-07-20T09:24:08.014085Z","shell.execute_reply.started":"2021-07-20T09:24:06.446705Z","shell.execute_reply":"2021-07-20T09:24:08.012688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following classes exist, ranging from no drought (``None``), to extreme drought (``D4``).\nThis could be treated as a regression, ordinal or classification problem, but for now we will treat it as 5 distinct classes.","metadata":{}},{"cell_type":"code","source":"class2id = {\n    'None': 0,\n    'D0': 1,\n    'D1': 2,\n    'D2': 3,\n    'D3': 4,\n    'D4': 5,\n}\nid2class = {v: k for k, v in class2id.items()}","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-20T09:24:08.016072Z","iopub.execute_input":"2021-07-20T09:24:08.016581Z","iopub.status.idle":"2021-07-20T09:24:08.024133Z","shell.execute_reply.started":"2021-07-20T09:24:08.016531Z","shell.execute_reply":"2021-07-20T09:24:08.022404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll define a helper method to load the datasets. This just walks through the json and discards the few samples that are corrupted.","metadata":{}},{"cell_type":"code","source":"dfs = {\n    k: pd.read_csv(files[k]).set_index(['fips', 'date'])\n    for k in files.keys()\n}","metadata":{"execution":{"iopub.status.busy":"2021-07-20T09:24:08.025999Z","iopub.execute_input":"2021-07-20T09:24:08.026806Z","iopub.status.idle":"2021-07-20T09:25:34.785001Z","shell.execute_reply.started":"2021-07-20T09:24:08.02675Z","shell.execute_reply":"2021-07-20T09:25:34.783842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also add a helper function to interpolate the drought values.","metadata":{}},{"cell_type":"code","source":"def interpolate_nans(padata, pkind='linear'):\n    \"\"\"\n    see: https://stackoverflow.com/a/53050216/2167159\n    \"\"\"\n    aindexes = np.arange(padata.shape[0])\n    agood_indexes, = np.where(np.isfinite(padata))\n    f = interp1d(agood_indexes\n               , padata[agood_indexes]\n               , bounds_error=False\n               , copy=False\n               , fill_value=\"extrapolate\"\n               , kind=pkind)\n    return f(aindexes)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T09:25:34.786764Z","iopub.execute_input":"2021-07-20T09:25:34.787118Z","iopub.status.idle":"2021-07-20T09:25:34.794456Z","shell.execute_reply.started":"2021-07-20T09:25:34.787083Z","shell.execute_reply":"2021-07-20T09:25:34.793513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We encode the day of year using sin/cos and add the data loading function `loadXY`.","metadata":{}},{"cell_type":"code","source":"def date_encode(date):\n    if isinstance(date, str):\n        date = datetime.strptime(date, \"%Y-%m-%d\")\n    return (\n        np.sin(2 * np.pi * date.timetuple().tm_yday / 366),\n        np.cos(2 * np.pi * date.timetuple().tm_yday / 366),\n    )\n\ndef loadXY(\n    df,\n    random_state=42, # keep this at 42\n    window_size=180, # how many days in the past (default/competition: 180)\n    target_size=6, # how many weeks into the future (default/competition: 6)\n    fuse_past=True, # add the past drought observations? (default: True)\n    return_fips=False, # return the county identifier (do not use for predictions)\n    encode_season=True, # encode the season using the function above (default: True) \n    use_prev_year=False, # add observations from 1 year prior?\n):\n    df = dfs[df]\n    soil_df = pd.read_csv(\"/kaggle/input/soil_data.csv\")\n    time_data_cols = sorted(\n        [c for c in df.columns if c not in [\"fips\", \"date\", \"score\"]]\n    )\n    static_data_cols = sorted(\n        [c for c in soil_df.columns if c not in [\"soil\", \"lat\", \"lon\"]]\n    )\n    count = 0\n    score_df = df.dropna(subset=[\"score\"])\n    X_static = np.empty((len(df) // window_size, len(static_data_cols)))\n    X_fips_date = []\n    add_dim = 0\n    if use_prev_year:\n        add_dim += len(time_data_cols)\n    if fuse_past:\n        add_dim += 1\n        if use_prev_year:\n            add_dim += 1\n    if encode_season:\n        add_dim += 2\n    X_time = np.empty(\n        (len(df) // window_size, window_size, len(time_data_cols) + add_dim)\n    )\n    y_past = np.empty((len(df) // window_size, window_size))\n    y_target = np.empty((len(df) // window_size, target_size))\n    if random_state is not None:\n        np.random.seed(random_state)\n    for fips in tqdm(score_df.index.get_level_values(0).unique()):\n        if random_state is not None:\n            start_i = np.random.randint(1, window_size)\n        else:\n            start_i = 1\n        fips_df = df[(df.index.get_level_values(0) == fips)]\n        X = fips_df[time_data_cols].values\n        y = fips_df[\"score\"].values\n        X_s = soil_df[soil_df[\"fips\"] == fips][static_data_cols].values[0]\n        for i in range(start_i, len(y) - (window_size + target_size * 7), window_size):\n            X_fips_date.append((fips, fips_df.index[i : i + window_size][-1]))\n            X_time[count, :, : len(time_data_cols)] = X[i : i + window_size]\n            if use_prev_year:\n                if i < 365 or len(X[i - 365 : i + window_size - 365]) < window_size:\n                    continue\n                X_time[count, :, -len(time_data_cols) :] = X[\n                    i - 365 : i + window_size - 365\n                ]\n            if not fuse_past:\n                y_past[count] = interpolate_nans(y[i : i + window_size])\n            else:\n                X_time[count, :, len(time_data_cols)] = interpolate_nans(\n                    y[i : i + window_size]\n                )\n            if encode_season:\n                enc_dates = [\n                    date_encode(d) for f, d in fips_df.index[i : i + window_size].values\n                ]\n                d_sin, d_cos = [s for s, c in enc_dates], [c for s, c in enc_dates]\n                X_time[count, :, len(time_data_cols) + (add_dim - 2)] = d_sin\n                X_time[count, :, len(time_data_cols) + (add_dim - 2) + 1] = d_cos\n            temp_y = y[i + window_size : i + window_size + target_size * 7]\n            y_target[count] = np.array(temp_y[~np.isnan(temp_y)][:target_size])\n            X_static[count] = X_s\n            count += 1\n    print(f\"loaded {count} samples\")\n    results = [X_static[:count], X_time[:count], y_target[:count]]\n    if not fuse_past:\n        results.append(y_past[:count])\n    if return_fips:\n        results.append(X_fips_date)\n    return results","metadata":{"execution":{"iopub.status.busy":"2021-07-20T09:25:34.796223Z","iopub.execute_input":"2021-07-20T09:25:34.796573Z","iopub.status.idle":"2021-07-20T09:25:34.821545Z","shell.execute_reply.started":"2021-07-20T09:25:34.796541Z","shell.execute_reply":"2021-07-20T09:25:34.820441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we add a helper to normalise the data.","metadata":{}},{"cell_type":"code","source":"scaler_dict = {}\nscaler_dict_static = {}\nscaler_dict_past = {}\n\n\ndef normalize(X_static, X_time, y_past=None, fit=False):\n    for index in tqdm(range(X_time.shape[-1])):\n        if fit:\n            scaler_dict[index] = RobustScaler().fit(X_time[:, :, index].reshape(-1, 1))\n        X_time[:, :, index] = (\n            scaler_dict[index]\n            .transform(X_time[:, :, index].reshape(-1, 1))\n            .reshape(-1, X_time.shape[-2])\n        )\n    for index in tqdm(range(X_static.shape[-1])):\n        if fit:\n            scaler_dict_static[index] = RobustScaler().fit(\n                X_static[:, index].reshape(-1, 1)\n            )\n        X_static[:, index] = (\n            scaler_dict_static[index]\n            .transform(X_static[:, index].reshape(-1, 1))\n            .reshape(1, -1)\n        )\n    index = 0\n    if y_past is not None:\n        if fit:\n            scaler_dict_past[index] = RobustScaler().fit(y_past.reshape(-1, 1))\n        y_past[:, :] = (\n            scaler_dict_past[index]\n            .transform(y_past.reshape(-1, 1))\n            .reshape(-1, y_past.shape[-1])\n        )\n        return X_static, X_time, y_past\n    return X_static, X_time","metadata":{"execution":{"iopub.status.busy":"2021-07-20T09:25:34.825013Z","iopub.execute_input":"2021-07-20T09:25:34.82557Z","iopub.status.idle":"2021-07-20T09:25:34.841941Z","shell.execute_reply.started":"2021-07-20T09:25:34.825533Z","shell.execute_reply":"2021-07-20T09:25:34.840445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_static_train, X_time_train, y_target_train = loadXY(\"train\")\nprint(\"train shape\", X_time_train.shape)\nX_static_valid, X_time_valid, y_target_valid = loadXY(\"valid\")\nprint(\"validation shape\", X_time_valid.shape)\nX_static_train, X_time_train = normalize(X_static_train, X_time_train, fit=True)\nX_static_valid, X_time_valid = normalize(X_static_valid, X_time_valid)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T09:59:42.373815Z","iopub.execute_input":"2021-07-20T09:59:42.37438Z","iopub.status.idle":"2021-07-20T10:15:24.126795Z","shell.execute_reply.started":"2021-07-20T09:59:42.374341Z","shell.execute_reply":"2021-07-20T10:15:24.125748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we use PyTorch to load the data.","metadata":{}},{"cell_type":"code","source":"batch_size = 128\noutput_weeks = 6","metadata":{"execution":{"iopub.status.busy":"2021-07-20T10:28:48.582772Z","iopub.execute_input":"2021-07-20T10:28:48.583169Z","iopub.status.idle":"2021-07-20T10:28:48.588368Z","shell.execute_reply.started":"2021-07-20T10:28:48.583134Z","shell.execute_reply":"2021-07-20T10:28:48.586717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_data = TensorDataset(\n    torch.tensor(X_time_train),\n    torch.tensor(X_static_train),\n    torch.tensor(y_target_train[:, :output_weeks]),\n)\ntrain_loader = DataLoader(\n    train_data, shuffle=True, batch_size=batch_size, drop_last=False\n)\nvalid_data = TensorDataset(\n    torch.tensor(X_time_valid),\n    torch.tensor(X_static_valid),\n    torch.tensor(y_target_valid[:, :output_weeks]),\n)\nvalid_loader = DataLoader(\n    valid_data, shuffle=False, batch_size=batch_size, drop_last=False\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T10:28:48.732997Z","iopub.execute_input":"2021-07-20T10:28:48.733457Z","iopub.status.idle":"2021-07-20T10:28:50.678793Z","shell.execute_reply.started":"2021-07-20T10:28:48.733417Z","shell.execute_reply":"2021-07-20T10:28:50.677749Z"},"trusted":true},"execution_count":null,"outputs":[]}]}