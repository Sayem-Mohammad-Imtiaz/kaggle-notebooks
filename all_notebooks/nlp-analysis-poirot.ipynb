{"cells":[{"metadata":{},"cell_type":"markdown","source":"# AI Trainee Journey - Practical part in NLP","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In this session, we will continue our AI Trainee Journey with NLP in practice.\n\nMore specifically, we will be working with a (freely available digital) book or novel, written by J.P.Lovecraft, called Call of Cthulhu. The book can be downloaded from [manybooks.net](https://manybooks.net/). \n\nFor your own quest/practice, you should download your own book, create a copy of this notebook and compare your results with results from this analysis.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 0. Reading in the libraries","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Because python without libraries is like a snake without legs.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport string # special operations on strings\nimport spacy # language models\n\nfrom matplotlib.pyplot import imread\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Additional module is needed that is not part of default Kaggle environment. Therefore we need to download it and install it first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!python -m spacy download en_core_web_md","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Reading in the input file and inspecting it","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"filename = '/kaggle/input/poirot3/Poirot-Investigates.txt' \nwith open(filename) as f:\n    book = f.readlines()\n# you may also want to remove whitespace characters like `\\n` at the end of each line\nbook[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(book)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Cleaning the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before fitting a machine learning or statistical model, we always have to clean the data. No models create meaningful results with messy data. is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Luckily, a book is usually already a clean document that went over spellchecking, editing, grammar overview, etc. So the words and sentences we get are usually without errors and readable. Unlike answers in some questionnaire, where people can write anything and make many mistakes even if they don't want to. But there are still some redundant parts of the text that we don't need in our analysis, so let's get to it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Empty lines\n\nWe don't need empty lines or empty string in our text as they don't contain any information. So it's easiest to remove them in the beginning.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"book = [x.strip() for x in book] # removes line breaks\nbook = [x for x in book if x] # removes empty strings, because they are considered in Python as False\nbook[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing redundant parts of the text from the book","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Because what we need to analyse is text of the book and not the author, name of the book or year of publication. Therefore we will remove the redundant parts from the text for analysis. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# we see that we don't need first 43 lines\ncore_book = book[43:]\ncore_book[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Joining the list into one string/text\ntext = ' '.join(core_book)\nlen(text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Punctuation String Translate\nIt really doesn't help us to have punctuation in inspecting words and their meaning, so let's get rid of it too.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"no_punc_text = text.translate(str.maketrans('', '', string.punctuation))\nno_punc_text[0:550]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lenght of text without punctuation\nlen(no_punc_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(text) - len(no_punc_text)\n#removed 9920 punctuation signs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stopwords\nStopwords are special case of words that work as a filler and usually don't hold any special meaning. We will be removing them later as we are comparing their occurence in the text with meaningful words. But let's take a look at what stopwords are. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Feature engineering** is a process of creating new variables for given dataset with an idea of improving model's prediction accuraccy or better description of dataset.\n\nFeatures can be:\n- numerical (number of words in a sentence)\n- categorical (what kind of sentence is it?)\n- boolean (Is the sentence longer than 50 signs? True/False)\n- ordinal (is the sentence short, medium or long?)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Tokenisation\nTokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\ntext_tokens = word_tokenize(no_punc_text)\nprint(text_tokens[0:50])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(text_tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After we tokenised the text, we can remove stopwords from it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nmy_stop_words = stopwords.words('english')\nmy_stop_words.append('the')\nno_stop_tokens = [word for word in text_tokens if not word in my_stop_words]\nprint(no_stop_tokens[0:40])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(no_stop_tokens)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lowercase\nSomething as simple as lowercasing all letters in all words helps a lot, because first letter in new sentence is uppercase by default and there are a few names of people and things also with uppercase. Lowercasing is standardizing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lower_words = [x.lower() for x in no_stop_tokens]\nprint(lower_words[0:25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stemming\n\nIn linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root formâ€”generally a written word form.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in lower_words]\nprint(stemmed_tokens[0:40])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lemmatisation\nIn computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. As a result, developing efficient lemmatisation algorithms is an open area of research. [2][3][4] (Wikipedia)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# NLP english language model of spacy library\nnlp = spacy.load('en')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert text into words with language properties, lemmas being one of them, but mostly POS, which will follow later\ndoc = nlp(' '.join(no_stop_tokens))\nprint(doc[0:40])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmas = [token.lemma_ for token in doc]\nprint(lemmas[0:25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Counting words\nConvert a collection of text documents to a matrix of token counts. Lemmas being more precise, let's use those as a token for counting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(lemmas)\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vectorizer.get_feature_names()[40:90])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.toarray())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum_words = X.sum(axis=0)\nwords_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\nwords_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\nwords_freq[0:25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Term Frequency - Inverse Document Frequency\n\nterm frequencyâ€“inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The tfâ€“idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tfâ€“idf is one of the most popular term-weighting schemes today. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tfâ€“idf.[2] (Wikipedia)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look this up yourself and fill in the code :) This was not part of theory, but it's a bonus task for special reasearch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvec = TfidfVectorizer(max_features=4000, stop_words='english')\nx = vec.fit_transform(lemmas)\nx.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2 = vectorizer.fit_transform(lemmas)\nfeatures = (vectorizer.get_feature_names())\n\n# Getting top ranking features\nsums2 = X2.sum(axis=0)\nranking_results = []\nfor col, term in enumerate(features):\n    ranking_results.append((term, sums2[0, col]))\nranking = pd.DataFrame(ranking_results, columns=['term', 'rank'])\n\nranking_dict = {}\nfor item in ranking.iterrows():\n    ranking_dict[item[1]['term']] = item[1]['rank']\n\nwords = (ranking.sort_values('rank', ascending=False))\nprint(\"\\n\\nWords head : \\n\", words.head(50))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part Of Speech Tagging","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech,[1] based on both its definition and its context. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc. (Wikipedia)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"one_block = book[94]\ndoc_block = nlp(one_block)\nspacy.displacy.render(doc_block, style='ent', jupyter=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for token in doc_block[0:20]:\n    print(token, token.pos_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Filtering for nouns and verbs only","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"There are many kinds of POS words. Exact list can be found at [spacy documentation](https://spacy.io/api/annotation#pos-universal). Those that we will look at are quite standard and basic.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nouns_verbs = [token.text for token in doc if token.pos_ in ('NOUN', 'VERB')]\nprint(nouns_verbs[5:25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Counting tokens again","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer()\n\nX = vectorizer.fit_transform(nouns_verbs)\nsum_words = X.sum(axis=0)\nwords_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\nwords_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\nwf_df = pd.DataFrame(words_freq)\nwf_df.columns = ['word', 'count']\nwf_df[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Sentiment analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Sentiment analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. In it's simplest form, it tries to identify, whether a sentence is positive or negative.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Lexicon (rule) based approach","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"afinn = pd.read_csv('/kaggle/input/bing-nrc-afinn-lexicons/Afinn.csv', sep=',', encoding='latin-1')\nafinn.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"afinn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from itertools import islice\n\ndef take(n, iterable):\n    \"Return first n items of the iterable as a list\"\n    return list(islice(iterable, n))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"affinity_scores = afinn.set_index('word')['value'].to_dict()\ntake(20, affinity_scores.items())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the emotions lexicon, we have lemmatized for, but we want to show original sentence and original form of words in the results? How to do this?\n\nIn following steps:\n1. put a unique id on each sentence (row)\n2. make a column for sentence\n3. calculate score for each sentence (row) by converting a word to lemmatized form only for the comparison and save it to new column \n4. order sentences by score to show top 10 and bottom 10","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import tokenize\nsentences = tokenize.sent_tokenize(\" \".join(core_book))\nsentences[5:15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_df = pd.DataFrame(sentences, columns=['sentence'])\nsent_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Custom function\nSometimes there is no predefined function that does everything that we want it to. Therefore, we define our own function that is specific for our use case. In this case, we want to score each word in a sentence in lemmatised form, but calculate the score for the whole original sentence.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en')\nsentiment_lexicon = affinity_scores\n\ndef calculate_sentiment(text: str = None) -> float:\n    sent_score = 0\n    if text:\n        sentence = nlp(text)\n        for word in sentence:\n            sent_score += sentiment_lexicon.get(word.lemma_, 0)\n    return sent_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test that it works\ncalculate_sentiment(text = 'Amazing boys, very good!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_df['sentiment_value'] = sent_df['sentence'].apply(calculate_sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# how many words are in the sentence?\nsent_df['word_count'] = sent_df['sentence'].str.split().apply(len)\nsent_df['word_count'].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_df.sort_values(by='sentiment_value').tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sentiment score of the whole book\nsent_df['sentiment_value'].sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Visualizing results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Barchart for top 10 nouns + verbs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wf_df[0:10].plot.bar(x='word', figsize=(12,8), title='Top verbs and nouns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Wordcloud\n\nWord Clouds are visual representations of words that give greater prominence to words that appear more frequently.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(background_color ='black', \n                       min_font_size = 10).generate(text)\nplt.figure(figsize = (12, 10), facecolor = None) \nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scatterplot\nA scatter plot is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed. The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_df.plot.scatter(x='word_count', y='sentiment_value', figsize=(12,8), title='Sentence sentiment value to sentence word count')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation\nIs there a correlation between word count in sentence and sentiment?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import pearsonr\ncorr, _ = pearsonr(sent_df['word_count'], sent_df['sentiment_value'])\ncorr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A: No, there isn't","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 6. Summary","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This concludes our NLP practical session with the most frequent tasks to be done in text analytics. Very often we want to know what is going on in a text without reading it all, especially when we have a lot of text with some similar features. \n\nThere are always some cleaning, preprocessing, feature engineering and visualisation steps in NLP analysis, no matter what the input dataset is.\n\nSentiment analysis, wordclouds, scatter plots are then just one way of how to look into the dataset. Other very popular technique we didn't look at here, is topic modeling or topic classification in texts. https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 7. Quest","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Your task in this quest is to use the book you downloaded from manybooks.net and \n\n**1. Do a sentiment analysis on the book to find 10 most positive sentences (with score) and 10 most negative sentences (with score). Calculate the final sentiment score for the whole book and print it somewhere.**\n\nand/or\n\n**2. Visualize the most frequent nons + verbs (or all words from original text) in a wordloud and plot a bar chart with number of top 10 words. Stemmed or lemmatized or without any modification. Change the background color to match your style**\n\nBONUS\n\n**3. Do a TF-IDF research and calculate these frequencies**","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}