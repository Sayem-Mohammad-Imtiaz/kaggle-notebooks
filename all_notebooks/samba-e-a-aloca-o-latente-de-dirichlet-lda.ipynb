{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://lh3.googleusercontent.com/proxy/z9ilC4bvRfBqWs83SCvP_xo8V7eGgsilTqugH0axpWybkxLXuZk87CApVuuOrbbeXRa10Ungs5oHfhnNeda5uyMdSjrwcg8uSJXFW1kZvLJmXERERxuZIVovtwlYB_xhYhgkewkGqBH0V0POEio)"},{"metadata":{},"cell_type":"markdown","source":"# Alocação Latente de Dirichlet (LDA)\n\n*\"No processamento de linguagem natural, a Alocação Latente de Dirichlet (LDA) é um <mark>modelo estatístico generativo que permite que conjuntos de observações sejam explicados por grupos não observados que explicam o porquê algumas partes dos dados são semelhantes</mark>. Por exemplo, se as observações são palavras coletadas em documentos, ele postula que cada documento é uma mistura de um pequeno número de tópicos e que a criação de cada palavra é atribuível a um dos tópicos do documento\"* - [Wikipédia](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n\n\n# LDAvis\n\nLDAvis são ferramentas para criar uma <mark>visualização interativa usando página web</mark>, de um modelo de tópico que foi ajustado a um corpus de dados de texto usando a **Alocação de Dirichlet Latente (LDA)**. Dado os parâmetros estimados do modelo de tópico, ele calcula várias estatísticas de resumo como entrada para uma visualização interativa criada com o D3.js que é acessada por meio de um navegador. <mark>O objetivo é ajudar os usuários a interpretar os tópicos em seu modelo de tópico LDA</mark>.\n\n* [LDAvis Introduction](https://cran.r-project.org/web/packages/LDAvis/vignettes/details.pdf)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re, nltk, gensim\nimport requests\nimport json\nfrom sklearn.externals import joblib\n\n# Sklearn\nfrom sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom pprint import pprint\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importando os Dados"},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = \"../input/brazilian-music-samba-lyrics/samba_dataset.csv\"\ndf = pd.read_csv(filename, sep=\"|\")\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tratando o texto"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cada letra de samba é um documento\ndata = [lyrics for lyrics in df.letra] \nprint(\"Temos %d documentos.\" %len(data))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenização dos docs"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removendo Stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"def removeStops(texts, stopwords):\n    texts_out = []\n    for sent in texts:\n        texts_out.append(\" \".join([token for token in sent if token not in stopwords]))\n    return texts_out\n\n\nstopwords = nltk.corpus.stopwords.words('portuguese')\nstopwords += [\"nao\", \"so\", \"pra\", \"pro\", \"pras\", \"pros\"]\n# Do lemmatization keeping only Noun, Adj, Verb, Adverb\ndata_without_stops = removeStops(data_words, stopwords)\n\n# sem stopwords\nprint(data_without_stops[:2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Criando a matriz Documento-Palavra\n\nO algoritmo de modelo de tópico LDA requer uma matriz de palavras do documento como entrada principal.\n\nVocê pode criar um usando o CountVectorizer. No código abaixo, eu configurei o CountVectorizer para considerar palavras que ocorreram pelo menos 10 vezes (min_df), remova palavras irrelevantes em inglês, converta todas as palavras em minúsculas e uma palavra pode conter números e alfabetos de pelo menos 3 para ser qualificado como uma palavra."},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(analyzer='word',       \n                             min_df=10,                        # minimum reqd occurences of a word \n                             # stop_words='english',             # remove stop words\n                             lowercase=True,                   # convert all words to lowercase\n                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n                             # max_features=50000,             # max number of uniq words\n                            )\n\n# data_vectorized = vectorizer.fit_transform(data_lemmatized)\ndata_vectorized = vectorizer.fit_transform(data_without_stops)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Verificando quão Esparsa é a Matriz\n\nBasta verificar a porcentagem de pontos diferentes de zero na matriz documento-palavra.\n\nComo a maioria das células nessa matriz será zero, estou interessado em saber qual porcentagem de células contém valores diferentes de zero."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Materialize the sparse data\ndata_dense = data_vectorized.todense()\n\n# Compute Sparsicity = Percentage of Non-Zero cells\nprint(\"Sparsicity: \", round(((data_dense > 0).sum()/data_dense.size)*100, 2), \"%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Treinando o Modelo LDA com Sklearn\n\nTudo está pronto para criar um modelo de Alocação Dirichlet Latente (LDA). Vamos inicializar um e chamar fit_transform () para criar o modelo LDA.\n\nNeste exemplo, eu defini os n_topics como 5. Mais tarde, encontraremos o número ideal usando grid search."},{"metadata":{"trusted":true},"cell_type":"code","source":"lda_model = LatentDirichletAllocation(n_components=5,              \n                                      max_iter=10,               \n                                      learning_method='online',   \n                                      random_state=100,          \n                                      batch_size=128,            \n                                      evaluate_every = -1,       \n                                      n_jobs = -1             \n                                     )\nlda_output = lda_model.fit_transform(data_vectorized)\n\nprint(lda_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Avaliando a performance do modelo com a perplexidade e probabilidade logarítmica\n\nUm modelo com maior probabilidade logarítmica e menor perplexidade (exp (-1. * Probabilidade logarítmica por palavra)) é considerado bom. Vamos verificar o nosso modelo."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Probabilidade logaritmica: quanto maior melhor\nprint(\"probabilidade logaritmica: \", round(lda_model.score(data_vectorized), 2))\n\n# Perplexidade: menor melhor.  exp(-1. * log-Probabilidade logaritmica por palavra)\nprint(\"Perplexidade: \", round(lda_model.perplexity(data_vectorized), 2))\n\nprint(\"Parâmetros:\")\npprint(lda_model.get_params())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A perplexidade pode não ser a melhor medida para avaliar modelos de tópicos, porque não considera o contexto e as associações semânticas entre as palavras. Isso pode ser capturado usando a medida de coerência de tópico."},{"metadata":{},"cell_type":"markdown","source":"# UsandoGridSearch para encontrar melhor modelo LDA \n\nO parâmetro de ajuste mais importante para modelos LDA é ```n_components``` (número de tópicos). Além disso, vamos pesquisar ```learning_decay``` (que controla a taxa de aprendizado) também.\n\nAlém desses, outros possíveis parâmetros de pesquisa podem ser ```learning_offset```  e ```max_iter```. Vale a pena experimentar se você tiver recursos de computação suficientes.\n\nA pesquisa em grade constrói vários modelos LDA para todas as combinações possíveis de valores de parâmetros no parâmetro param_grid. Portanto, <mark>esse processo pode consumir muito tempo e recursos</mark>."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define Search Param\nsearch_params = {'n_components': [5, 10, 15], 'learning_decay': [.5, .7, .9]}\n\n# Init the Model\nlda = LatentDirichletAllocation()\n\n# Init Grid Search Class\nmodel = GridSearchCV(lda, param_grid=search_params)\n\n# Do the Grid Search\nmodel.fit(data_vectorized)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Escolhendo o \"melhor\" modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Melhor modelo\nbest_lda_model = model.best_estimator_\n\n# Hiperparâmetros do modelo\nprint(\"Melhores parâmetros: \", model.best_params_)\n\n# probabilidade logarítmica\nprint(\"Melhor score de probabilidade logarítmica: \", model.best_score_)\n\n# Perplexidade\nprint(\"Perplexidade do modelo: \", best_lda_model.perplexity(data_vectorized))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Comparando os scores de performance dos modelos LDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame(model.cv_results_)\n\ncurrent_palette = sns.color_palette(\"Set2\", 3)\n\nplt.figure(figsize=(12,8))\n\nsns.lineplot(data=results,\n             x='param_n_components',\n             y='mean_test_score',\n             hue='param_learning_decay',\n             palette=current_palette,\n             marker='o'\n            )\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tópico dominante em cada documento\n\nPara classificar um documento como pertencente a um tópico específico, uma abordagem lógica é ver qual tópico tem a maior contribuição para esse documento e atribuí-lo.\n\nNa tabela abaixo, destaquei em verde todos os principais tópicos de um documento e atribuí o tópico mais dominante em sua própria coluna."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create Document - Topic Matrix\nlda_output = best_lda_model.transform(data_vectorized)\n\n# column names\ntopicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_components)]\n\n# index names\ndocnames = [\"Doc\" + str(i) for i in range(len(data))]\n\n# Make the pandas dataframe\ndf_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n\n# Get dominant topic for each document\ndominant_topic = np.argmax(df_document_topic.values, axis=1)\ndf_document_topic['dominant_topic'] = dominant_topic\n\n# Styling\ndef color_green(val):\n    color = 'green' if val > .1 else 'black'\n    return 'color: {col}'.format(col=color)\n\ndef make_bold(val):\n    weight = 700 if val > .1 else 400\n    return 'font-weight: {weight}'.format(weight=weight)\n\n# Apply Style\ndf_document_topics = df_document_topic.style.applymap(color_green).applymap(make_bold)\ndf_document_topics_first10 = df_document_topic[:10].style.applymap(color_green).applymap(make_bold)\ndf_document_topics_first10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Quantidade de Documentos em Cada Tópico"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\ndf_topic_distribution.columns = ['Topic Num', 'Num Documents']\ndf_topic_distribution","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizando nosso modelo LDA com o pyLDAvis"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Para vizualizar o gráfico, execute os comandos abaixo no seu jupyer notebook, isso irá abrir uma página no seu localhost:\n```python\npyLDAvis.enable_notebook()\npanel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\npyLDAvis.show(panel)```"},{"metadata":{},"cell_type":"markdown","source":"O gráfico interativo que será gerado terá a seguinte \"cara\":\n\n![](https://media.giphy.com/media/dt0myZkpmNjrW4IaXi/giphy.gif)"},{"metadata":{},"cell_type":"markdown","source":"# Top 5 palavras por tópico"},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = vectorizer.get_feature_names()\n\n# data_vectorized\ntopic_words = {}\nn_top_words = 5\n\nfor topic, comp in enumerate(best_lda_model.components_):\n    # for the n-dimensional array \"arr\":\n    # argsort() returns a ranked n-dimensional array of arr, call it \"ranked_array\"\n    # which contains the indices that would sort arr in a descending fashion\n    # for the ith element in ranked_array, ranked_array[i] represents the index of the\n    # element in arr that should be at the ith index in ranked_array\n    # ex. arr = [3,7,1,0,3,6]\n    # np.argsort(arr) -> [3, 2, 0, 4, 5, 1]\n    # word_idx contains the indices in \"topic\" of the top num_top_words most relevant\n    # to a given topic ... it is sorted ascending to begin with and then reversed (desc. now)    \n    word_idx = np.argsort(comp)[::-1][:n_top_words]\n\n    # store the words most relevant to the topic\n    topic_words[topic] = [vocab[i] for i in word_idx]\n    \nfor topic, words in topic_words.items():\n    print('Topic: %d' % topic)\n    print('  %s' % ', '.join(words))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Principais Artistas em cada Tópico"},{"metadata":{"trusted":true},"cell_type":"code","source":"# tranformando objeto style em um dataframe pandas\ndf2 = pd.DataFrame(data=df_document_topics.data, columns=df_document_topics.columns)\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# associando os interpretes aos tópicos \n# dos sambas que eles cantam\ndf2[\"artista\"] = df[\"artista\"].tolist()\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Artistas que mais aparecem dentro de cada tópico\ndf2.groupby([\"dominant_topic\"])['artista'].agg(pd.Series.mode).to_frame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os 5 artistas que mais aparecem no\n# tópico 0 e quantidade de sambas \ndf2[df2[\"dominant_topic\"]==0].groupby([\"artista\"]).size().sort_values(ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os 5 artistas que mais aparecem no\n# tópico 1 e quantidade de sambas \ndf2[df2[\"dominant_topic\"]==1].groupby([\"artista\"]).size().sort_values(ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os 5 artistas que mais aparecem no\n# tópico 2 e quantidade de sambas \ndf2[df2[\"dominant_topic\"]==2].groupby([\"artista\"]).size().sort_values(ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os 5 artistas que mais aparecem no\n# tópico 3 e quantidade de sambas \ndf2[df2[\"dominant_topic\"]==3].groupby([\"artista\"]).size().sort_values(ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# os 5 artistas que mais aparecem no\n# tópico 4 e quantidade de sambas \ndf2[df2[\"dominant_topic\"]==4].groupby([\"artista\"]).size().sort_values(ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reconhecimentos\n\nO que fiz acima foi um estudo inspirado no artigo escrito por Selva Prabhakaran:\n\n* [LDA in Python – How to grid search best topic models?](https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":1}