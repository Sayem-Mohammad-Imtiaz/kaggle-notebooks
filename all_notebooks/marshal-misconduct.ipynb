{"nbformat_minor":1,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Simple example kernel to get started with the data\n* I already cleaned up the possible targets, though you may wish to remove the \"classes\" that have <10 cases.","metadata":{"_uuid":"042adc270448a29e04270eaa3ff3366e012c9982","_cell_guid":"c46ef1f4-3834-4cbd-b8a9-0c1bc4b84064"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy import sparse\nfrom sklearn.preprocessing import LabelEncoder as LE # warning - using this can result in silly range features, vs using OHE from sklearn or Pandas's get_dummies(). \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"bdf959eb5d2e462d58bcb5cd51571c5ba2942766","_cell_guid":"99aba857-a635-4fd2-9b59-e629221012cd"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"path = '../input/'\nfilename = 'FederalAirMarshalMisconduct.csv'\n\ndf = pd.read_csv(path + filename,parse_dates=[\"Date Case Opened\"],infer_datetime_format=True)","metadata":{"collapsed":true,"_uuid":"f878728153bb556b748d4e500206f112d0dc326e","_cell_guid":"1d402b18-6657-425c-84a5-62d90ddc0664"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"df.head()","metadata":{"_uuid":"6f8947789a62759011ed918cb294fb6ead689d22","_cell_guid":"ab6c62af-fb30-4273-a15f-b26d2fb611e9"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"df.columns\n","metadata":{"_uuid":"5576454de9cb926bd58abbdc6e3a5c732d9bb28c","_cell_guid":"a408c9a0-2b40-4cbf-96db-8199e212f061"}},{"cell_type":"markdown","source":"Whoops, looks like some of the columns names got spaces added at their ends :( !  \nLet's fix that!\n\n* We could also remove spaces in variable/column names for easier work, but we can skip that - it's not as critical as 'deceptive' column names with hidden spaces ;)\n","metadata":{"_uuid":"44c6a58c6d351ae05572d1cbe9b4e5986b8cce57","_cell_guid":"ac695053-73d6-482a-a6ab-6a4166f72dae"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"df.rename(columns={\"Allegation \":\"Allegation\", \"Field Office \":'FieldOffice'},inplace=True)\nprint(df.columns)","metadata":{"_uuid":"b4fdf3a67a00c2da96d661994da669ac0273dd4d","_cell_guid":"c975760c-cee3-49ac-b6ce-4950bd2ff7a1"}},{"cell_type":"markdown","source":"Let's look at the charges (_Allegation_) against the marshals, and the trial outcomes (_Final Disposition_ /\t_target_)","metadata":{"_uuid":"98013345bf08ebfce27c631090d103c4390d7891","_cell_guid":"356863fa-e983-4994-aa9b-6eba560e6e07"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"df[\"Allegation\"].value_counts()","metadata":{"_uuid":"be594c1363fe17bad1b4d16b4d50beb9966acc35","_cell_guid":"768c117e-6bcb-46e1-8583-0521aaa21517"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"df['Final Disposition'].value_counts()","metadata":{"_uuid":"00c9d9d7c427fa05440e9501427fa7959f4bf61b","_cell_guid":"0ecbc282-8e73-49f4-830d-c337292ca50b"}},{"cell_type":"markdown","source":"After lower casing and replacing all the \" - X day[s]\", we have far less possible outcomes:","metadata":{"_uuid":"7cde98926b5bb7909d5cc6afcc6611f770235ce9","_cell_guid":"85b348ce-cdd4-4f0f-b244-86fb8cf5a491"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"df['target'].value_counts()","metadata":{"_uuid":"3f7eb0c51b7820d49510d42dbb08f4d1bfd062dc","_cell_guid":"ebaf11f0-d804-464b-b34c-ae479875fe7a"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"print(\"A naive majority classifier would get: %.4f Accuracy\" % (1833/df.shape[0]))","metadata":{}},{"cell_type":"markdown","source":"Classes with just 1 case aren't relevant. We'll remove cases/_target_ from the data where the target/outcome has less than 30 occurances.\n * This leaves us with about 8 classes. LEtter of counsel and verbal counsel might be similar, but i'm unsure, so we'll leave them as seperaet classes.","metadata":{}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"least_frequent_classes = df['target'].value_counts().tail(8).index","metadata":{}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"print(df.shape)\ndf = df.loc[~df.target.isin(least_frequent_classes)]\ndf.shape[0]","metadata":{}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"print(\"Check for nulls in the target column:\")\nprint(df.isnull().sum())\n# df.dropna(subset=\"target\",inplace=True,axis=1) # This gives errors on kaggle for some reason ? \ndf = df.loc[df.target.notnull()]\nprint(\"After cleaning:\",df.isnull().sum())","metadata":{}},{"cell_type":"markdown","source":"I already analyzed the data externally: there are strong features based on the data, notably time ranges and years, as they relate to some of the final dispositions (notably retirement). \n![](http://)* Here let's  get just  simple datetime  features. ","metadata":{"_uuid":"efe9eaab3c32569b05660620a39efe77742ce499","_cell_guid":"670f2893-240e-412f-8aaf-9bb080266931"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"df[\"Year\"] = df['Date Case Opened'].dt.year\ndf[\"Month\"] = df['Date Case Opened'].dt.month","metadata":{"collapsed":true,"_uuid":"f8b00ecc2b72f449de270b4eabbc7f472baa9fcc","_cell_guid":"9af0a072-dd69-4508-93ae-6d3655c23237"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"df.head()","metadata":{"_uuid":"1c90c9f11240e800d5619500941b90fba7264910","_cell_guid":"9184403e-7138-40a8-bec4-2c263ff30ead"}},{"cell_type":"markdown","source":"## Let's drop the columns we don't want, and keep just the subset for predictive model building: ","metadata":{"_uuid":"91de806608d06c69b17441712d5e1f8606246a21","_cell_guid":"2312f1a0-8383-4edb-8761-82d35ff9735a"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"df = df[[ 'FieldOffice', 'Allegation', 'target', 'Year', 'Month']]","metadata":{"collapsed":true,"_uuid":"0ee535ab06019475c43df20e726a92dbb632dd6b","_cell_guid":"6f08e4d4-189e-4499-825d-213aeb9c70c0"}},{"cell_type":"markdown","source":"### Encode the categorical feature of Office\n* can encode as OHE with p[andas's get_dummys , or via label encoding (which saves on spac/columnse but can give silly features involving range)","metadata":{}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# # Encode OHE the FieldOffice:\ndf = pd.get_dummies( df, columns = [\"FieldOffice\"] )\n\n# ### ALT:\n# df[\"FieldOffice\"] = LE.fit_transform(df[\"FieldOffice\"])","metadata":{"_uuid":"491cd513c25510eba96d159f5d348ca28395507f","_cell_guid":"7557db88-f2a1-4935-b3b6-401d26452f33"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"df.head()","metadata":{"_uuid":"5d8d7364334e715efcc05df6eec1987a9b58f2f4","_cell_guid":"83190657-59d5-4c7e-866f-482daf430a7b","scrolled":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"X_train, X_test, y_train, y_test = train_test_split(df.drop(\"target\",axis=1), df.target, random_state=42)","metadata":{"_uuid":"3d5627d5a6e0ac62157ce61b74f7b0911ad637bf","_cell_guid":"1a9941d0-334f-45f1-8883-8ad57489b752"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Bag of words features on the text\ntfidf = CountVectorizer(stop_words='english', max_features=200,min_df=3,ngram_range=(1, 2))\ntr_sparse = tfidf.fit_transform(X_train[\"Allegation\"])\nte_sparse = tfidf.transform(X_test[\"Allegation\"])","metadata":{}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"X_train = sparse.hstack([X_train.drop(\"Allegation\",axis=1), tr_sparse]).tocsr()\nX_test = sparse.hstack([X_test.drop(\"Allegation\",axis=1), te_sparse]).tocsr()","metadata":{"collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"fmodel = RandomForestClassifier(n_estimators=400, random_state=42, max_depth=9, max_features=30,class_weight=\"balanced\").fit(X_train, y_train)\nprediction = fmodel.predict(X_test)","metadata":{"_uuid":"67a392d47c058b05ca4ea657c0f4270d0ccbe571","_cell_guid":"d14b5f9f-159c-48d3-ac88-398fbf4eca22"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"# Data is hihgly imbalanced, so accuracy is meaningless. let'sWe could have a look at the AUC, but it's tricker to define for multiclass, so we'll leave it for now) : \n# score = roc_auc_score(y_test, prediction)\n# print(\"AUC on test set: %.2f\" % score)\n\nacc_score = accuracy_score(y_test, prediction)\nprint(\"Accuracy score on test set: %.2f\" % (100.*acc_score))","metadata":{"_uuid":"f3a586854abe1e2b2437b5ebeffa0350739da542","_cell_guid":"2f621bb5-7bd6-446f-97d6-597d3cbc94e1"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"print(classification_report(y_test, prediction))","metadata":{}},{"cell_type":"markdown","source":"### Further work:\n* word cloud and top features per class","metadata":{}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"","metadata":{"collapsed":true}}],"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}}}