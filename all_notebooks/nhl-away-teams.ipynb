{"cells":[{"metadata":{"papermill":{"duration":0.013412,"end_time":"2021-03-06T06:44:17.677347","exception":false,"start_time":"2021-03-06T06:44:17.663935","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# NHL Visiting Teams Analysis \n\n### By: John Palmer\n\nHi there. This is my first notebook on Kaggle that I'm using to practice ML techniques. I've grown up watching and playing hockey and figured this NHL data set would be a nice starting point. In this notebook, I create a model using XGBClassifier to predict whether the visiting hockey team will win their game. Still new to this, and definitely welcome to feedback. \n\nAs always, need to import the necessary tools first.   "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.237631,"end_time":"2021-03-06T06:44:18.925474","exception":false,"start_time":"2021-03-06T06:44:17.687843","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"import matplotlib\nimport matplotlib.pyplot as plt \nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport sklearn as sk \nimport random as rd\nimport re \nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc, plot_roc_curve, accuracy_score, roc_auc_score\nfrom sklearn.impute import KNNImputer\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, I will read in both datasets I will be using for the analysis. Specifically, the `team_data` dataset contains outcomes for both home and away teams. I figured that since these results are paired, \nthis might cause an inflation of model performance if left in this format. Therefore, I will remove all the home team fields from the dataset, and add back some of their fields as additional columns (matching them to their corresponding row entry)."},{"metadata":{"trusted":false},"cell_type":"code","source":"game_data = pd.read_csv(\"../input/nhl-game-data/game.csv\", index_col=0)\nteam_data = pd.read_csv(\"../input/nhl-game-data/game_teams_stats.csv\", index_col=0)\nteam_metadata = pd.read_csv(\"../input/nhl-game-data/team_info.csv\", index_col=0)\n\n# drop unnecessary columns\nteam_data = team_data.drop(['head_coach','goals', \"team_id\", \"powerPlayGoals\",\"startRinkSide\"], axis=1)\nteam_data.sort_index(axis=0, inplace=True)\n\n# Separate the home and away teams to prevent use of overlapping data\nhmask = team_data.HoA == \"home\"\nhome_data = team_data[hmask]\naway_data = team_data[~hmask]\n\n# Edit home and away datasets\nhome_data = home_data.drop([\"won\", \"settled_in\", \"faceOffWinPercentage\"], axis=1)\nhome_data = home_data.add_prefix(\"home_\")\naway_data = away_data.add_prefix(\"away_\")\n\n# Add specific home features to the away dataset\nteam_data = pd.concat([away_data, home_data], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I will address missing, duplicated, or problematic entries. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Remove missing entries with missing hits (easiest choice because of strong overlap with other NAs; i.e. giveaways, takeaways, blocked)\nteam_data = team_data[~team_data.away_hits.isnull()]\n\n# KNN Imputation to address missing \"faceOffWinPercentage\" values\nimputer = KNNImputer()\nteam_data.away_faceOffWinPercentage = imputer.fit_transform(np.array(team_data.away_faceOffWinPercentage).reshape(-1,1))\n\n\n# Remove uninformative values in \"settled_in\"\nteam_data = team_data[~(team_data.away_settled_in == \"tbc\")]\n\n# I found that there are a collection of games in the early 2000s where it seems that these 6 measures were not being recorded\n# Therefore, I opt to remove any entries where all six of these features are 0 \ntoRemove = ((team_data.away_giveaways== 0) & (team_data.away_takeaways == 0) \n          & (team_data.away_blocked == 0) & (team_data.home_giveaways == 0) \n          & (team_data.home_takeaways == 0) & (team_data.home_blocked == 0))\nteam_data = team_data[~toRemove]\n\n# Remove duplicated entries \nprint(\"Duplicated entries: {}\\n\".format(sum(team_data.index.duplicated())))\nteam_data = team_data[~team_data.index.duplicated()]\n\n# Columns used to split data, no longer needed \nteam_data = team_data.drop(['away_HoA', \"home_HoA\"], axis=1)\n\nprint(team_data.head())\nprint(team_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I noticed that there are both \"Standard\" timezones and \"Daylight\" timezones in the data, which I feel, adds unnecessary complexity to the data. \nTo remove this, I will use a regular expression to search for all daylight savings timezones and remap them to their \"Standard\" times. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Remove duplicated entries from the 2nd dataset\nprint(\"Duplicated entries: {}\\n\".format(sum(game_data.index.duplicated())))\ngame_data = game_data[~game_data.index.duplicated()]\n\n# Readjust the timezones (exclude daylight savings)\ndef timezone_change(tz):\n    res = re.search(\"[A-Z]DT\", tz) \n    if res != None:\n        return 1\n    else:\n        return 0 \n\n# Find the offset of the daylight savings timezones\noffset = game_data.venue_time_zone_tz.apply(timezone_change)\n\n# Adjust the timezones \ngame_data.venue_time_zone_offset = (game_data.venue_time_zone_offset - offset)\n\n# Plot the finalized timezones \nplt.hist(game_data.venue_time_zone_offset)\nplt.xlabel(\"Timezone Offset\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I perform feature engineering to add two new variables to the data. One is the time of the game (specifically the hour on a 24hr clock), and the second is the difference in timezone between the visiting team and that of the venue. "},{"metadata":{"papermill":{"duration":0.081861,"end_time":"2021-03-06T06:44:19.018355","exception":false,"start_time":"2021-03-06T06:44:18.936494","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# NEW FEATURE : \"game_time\"\nres = game_data.date_time_GMT.apply(lambda x: int(re.search(\"T(\\d+):\", x).group(1)))\n\ngame_data['game_time'] = (res + game_data.venue_time_zone_offset) % 24\nplt.figure()\nplt.hist(game_data['game_time'])  \nplt.xlabel(\"Time of Game (24 hr)\")\n\n# Timezone mappings for all team IDs\ntimezones = {1:5, 4:5, 26:8, 14:5, 6:5, 3:5, 5:5, 17:5, 28:8, 18:6, 23:8, 16:6, 9:5, \n             8:5, 30:6, 15:5, 19:6, 24:8, 27:7, 2:5, 10:5,13:5, 7:5, 20:7, 21:7, 25:6, 29:5, 52:6, 22:7, 54:8, 12:5, 53:7, 11:5}\n\n# These teams are not registered in the data set; remove them \nmask = game_data.away_team_id.isin([88, 87, 90])\nprint(sum(mask))\ngame_data = game_data.drop(game_data[mask].index)\n\n# NEW FEATURE : \"away_timezone\" and \"away_timechange\" \n# \"away_timechange\" : the difference between the away team's timezone and that of the venue\ngame_data['away_timezone']  = -game_data.away_team_id.apply(lambda x: timezones[x])\ngame_data['away_timechange'] = np.abs(game_data.away_timezone - game_data.venue_time_zone_offset)\n\nplt.figure()\nplt.hist(game_data['away_timechange'])\nplt.xlabel(\"Away Timezone Difference\")","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.07799,"end_time":"2021-03-06T06:44:19.107739","exception":false,"start_time":"2021-03-06T06:44:19.029749","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"toAdd = game_data[['season','type','game_time','away_timezone','away_timechange','venue_time_zone_offset']]\ntoAdd2 = toAdd.loc[team_data.index,:]\n\ndata = pd.concat([team_data, toAdd2], axis=1)\nprint(f\"Final Dataset Dimensions: {game_data.shape}\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, I am considering dropping the `season` feature from the data, but I will do a quick visualization to if there are any trends. "},{"metadata":{"papermill":{"duration":0.029693,"end_time":"2021-03-06T06:44:19.461453","exception":false,"start_time":"2021-03-06T06:44:19.43176","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"# calculate proportion of visiting team wins per season\nres = data.groupby('season').sum()['away_won'] / data.groupby('season').size()\n\nplot = sns.barplot(x=res.index, y=res, data=data)\nplot = plot.set_xticklabels(labels=res.index,rotation=70)\nplt.ylabel(\"Number of away-wins / Total Games\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interestingly, there seems to be some differences in the proportion of games won by visiting teams between different hockey seasons. The lower values in the earlier seasons may prove helpful to the model, so I am going to leave this feature in. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Collect categorical variables \ncategorical = [name for name in data.columns if data[name].dtype == 'object']\ncategorical = categorical + [\"season\", \"away_timezone\", \"venue_time_zone_offset\", \"game_time\"]\n\n# Collect numeric variables \nnumeric = [column for column in data.columns if column not in categorical]\nnumeric.remove(\"away_won\")\nprint(categorical)\nprint(numeric)\n\n# Set up core pipeline for preprocessing\ncategorical_transform = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown = \"ignore\"))])\n\nnum_transform = Pipeline(steps=[\n    ('scaler', StandardScaler())])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        ('num', num_transform, numeric),\n        ('cat', categorical_transform, categorical)])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.012574,"end_time":"2021-03-06T06:44:19.486912","exception":false,"start_time":"2021-03-06T06:44:19.474338","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Here, I split the data into the appropriate train and test sets. "},{"metadata":{"papermill":{"duration":0.032578,"end_time":"2021-03-06T06:44:19.532331","exception":false,"start_time":"2021-03-06T06:44:19.499753","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"label = LabelEncoder()\ny = pd.Series(label.fit_transform(data.away_won))\nX = data.iloc[:,1:]\ny.index = X.index\n#X_try = preprocess.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\ndef compute_performance(yhat, y, classes):\n    # First, get tp, tn, fp, fn\n    tp = sum(np.logical_and(yhat == classes[1], y == classes[1]))\n    tn = sum(np.logical_and(yhat == classes[0], y == classes[0]))\n    fp = sum(np.logical_and(yhat == classes[1], y == classes[0]))\n    fn = sum(np.logical_and(yhat == classes[0], y == classes[1]))\n\n    print(f\"tp: {tp} tn: {tn} fp: {fp} fn: {fn}\")\n    \n    # Accuracy\n    acc = (tp + tn) / (tp + tn + fp + fn)\n    \n    # Precision\n    # \"Of the ones I labeled +, how many are actually +?\"\n    precision = tp / (tp + fp)\n    \n    # Recall\n    # \"Of all the + in the data, how many do I correctly label?\"\n    recall = tp / (tp + fn)    \n    \n    # Sensitivity\n    # \"Of all the + in the data, how many do I correctly label?\"\n    sensitivity = recall\n    \n    # Specificity\n    # \"Of all the - in the data, how many do I correctly label?\"\n    specificity = tn / (fp + tn)\n    \n    # Print results\n    \n    print(\"Accuracy:\",round(acc,3),\"Recall:\",round(recall,3),\"Precision:\",round(precision,3),\n          \"Sensitivity:\",round(sensitivity,3),\"Specificity:\",round(specificity,3))\n","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.013112,"end_time":"2021-03-06T06:44:19.558878","exception":false,"start_time":"2021-03-06T06:44:19.545766","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Logistic Regression\n\nI'll start by modelling using a standard logistic regression model. Here, I create a pipeline and evaluate the model performance using cross-validation. I further examine the model performance by examining scores associated with the confusion matrix (using `compute_performance`)."},{"metadata":{"papermill":{"duration":0.801753,"end_time":"2021-03-06T06:44:20.373883","exception":false,"start_time":"2021-03-06T06:44:19.57213","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"log_model = Pipeline([\n    ('preprocess',preprocess),\n    ('log',LogisticRegression( max_iter=10000))\n])\n\ncv_score = cross_val_score(log_model, X_train, y_train, cv=8, scoring='roc_auc')\n\nprint(f\"Mean CV AUROC Score: {cv_score.mean()}\")\n\nlog_model.fit(X_train, y_train)\n\n\ny_test_pred = log_model.predict(X_test)\ny_test_prob = log_model.predict_proba(X_test)\n\ncompute_performance(y_test_pred, y_test, log_model.classes_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Polynomial with L1 Penalty"},{"metadata":{},"cell_type":"markdown","source":"Just out of curiosity, I want to see whether there are potentially non-linear relationships between features and outcomes. Here, I will experiment by adding a 2nd order polynomial transformation of the numeric features. I will also change the penalization of the model from `None` to the \"LASSO\" or L1 penalty. This is because I now have a lot of newly added features from the polynomial expansion and want to perform some implicit feature selection (setting coefficients to 0). "},{"metadata":{"trusted":false},"cell_type":"code","source":"poly_transform = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('poly', PolynomialFeatures(2, include_bias=False)),\n    ('scaler2', StandardScaler())])\n\npoly_preprocess = ColumnTransformer(\n    transformers=[\n        ('num', poly_transform, numeric),\n        ('cat', categorical_transform, categorical)])\n\npoly_log_model = Pipeline([\n    ('preprocess',poly_preprocess),\n    ('log',LogisticRegression(solver='saga',penalty='l1', max_iter=10000, n_jobs=8))\n])\n\n\ncv_score = cross_val_score(poly_log_model, X_train, y_train, cv=8, scoring='roc_auc', n_jobs=8)\nprint(f\"Mean CV AUROC Score: {cv_score.mean()}\")\n\npoly_log_model.fit(X_train, y_train)\nlog_model.fit(X_train, y_train)\n\n# Evaluate model performance on the test data\ny_test_pred = log_model.predict(X_test)\ny_test_prob = log_model.predict_proba(X_test)\n\ncompute_performance(y_test_pred, y_test, log_model.classes_)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There was a very small improvement in AUROC score, but no tangible improvement in model performance. Moving forward, I will continue with the base model without the polynomials transformation (though I will still be testing it off script). \n"},{"metadata":{},"cell_type":"markdown","source":"# Random Forest\n\nI want to gain more experience using random forest models, so I'll choose to fit one here. I want to see whether this offers any notable improvement over the standard logistic regression. To create the model, I will use my original preprocessor without polynomial transformations and tune the number of trees parameter before fitting. "},{"metadata":{"trusted":false},"cell_type":"code","source":"numTrees = np.linspace(50,550, num=21)\n\nrf_model = Pipeline([\n    ('preprocess',preprocess),\n    ('rf', RandomForestClassifier(oob_score=True, \n                               random_state=0,\n                               n_jobs=8))\n])\n\nerror = []\n\nfor i in range(len(numTrees)):\n  print(numTrees[i])\n  rf_model.named_steps['rf'].set_params(n_estimators=int(numTrees[i]))\n  rf_model.fit(X_train, y_train)\n  oob_error = 1 - rf_model.named_steps['rf'].oob_score_\n  error.append( oob_error)\n\nprint(error)\nplt.plot(numTrees, error)\nplt.ylabel(\"Out-of-bag Error\")\nplt.xlabel(\"Number of Trees\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the tuning above, it appears that 450 trees would be an appropriate choice for `n_estimators` given that no significant decreases in the out-of-bag error can be detected beyond this point. However, I also want to try a more involved approach using `GridSearchCV`. Using grid search will allow me to tune the `max_depth` parameter in addition to the `n_estimators`. Below, I will use 8-fold cross-validation on a smaller dataset to tune these parameters. "},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_model = Pipeline([\n    ('preprocess',preprocess),\n    ('rf', RandomForestClassifier(oob_score=True, \n                               random_state=0,\n                               n_jobs=8))\n])\n\nparams = dict({'rf__max_depth': [5, 10, 20, 30, 50],\n         'rf__n_estimators':[100, 200, 500, 800, 1000 ]})\n\n# Create a smaller sample size to tune parameters on \nX_tune = X_train.sample(frac=0.5)\ny_tune = y_train[X_tune.index]\n\ngrid_rf = GridSearchCV(rf_model, \n                      param_grid=params, \n                      cv = 5,\n                      scoring = 'roc_auc',\n                      n_jobs=8,\n                      refit = False,\n                      verbose=1)\ngrid_rf.fit(X_tune, y_tune)\n\ngrid_rf.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I now have my tuned parameters for the `max_depth` and `n_estimators` to use in a finalized random forest classifier model. Here, I'll fit that model to the training data and evaluate my performance on the testing dataset. "},{"metadata":{"trusted":false},"cell_type":"code","source":"rf_final = Pipeline([\n    ('preprocess',preprocess),\n    ('rf', RandomForestClassifier(oob_score=True,\n                                  n_estimators=800,\n                                  max_depth=20,\n                               random_state=0,\n                               n_jobs=8))\n])\n\nrf_final.fit(X_train, y_train)\n\n# Evaluate model performance on the test data\ny_rf_pred = rf_final.predict(X_test)\ny_rf_pred_prob = rf_final.predict_proba(X_test)\n\nprint(f\"RF AUROC Score : {roc_auc_score(y_test, y_rf_pred_prob[:,1])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting (XGBoost)"},{"metadata":{},"cell_type":"markdown","source":"Next, I want to test whether a gradient boosted decision tree model might outperform this random forest classifier. To approach this, I will create a dictionary of parameter values that I want to tune. Here, I am including the `learning_rate` as a new tunable parameter, in addition to the original `max_depth` and `n_estimators` used previously in the random forest. "},{"metadata":{"trusted":false},"cell_type":"code","source":"xg_model = Pipeline([\n    ('preprocess',preprocess),\n    ('xg', XGBClassifier(n_jobs=8, objective='binary:logistic', random_state=0 ,gamma=0.1))\n])\n\n\nparams = dict({'xg__learning_rate': [0.001, 0.01, 0.1,0.2],\n         'xg__max_depth': [2, 3, 4],\n         'xg__n_estimators':[100, 200,300, 400, 500 ]})\nprint(params)\n\n# Create a smaller sample size to tune parameters on \nX_tune = X_train.sample(frac=0.5)\ny_tune = y_train[X_tune.index]\n\ngrid_xg = GridSearchCV(xg_model, \n                      param_grid=params, \n                      cv = 5,\n                      scoring = 'roc_auc',\n                      n_jobs=8,\n                      refit = False,\n                      verbose=1)\ngrid_xg.fit(X_tune, y_tune)\n\ngrid_xg.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the tuned parameter values above, I will fit my finalized XGBoost classifier model to the training data and evaluate its performance on the testing dataset. "},{"metadata":{"papermill":{"duration":0.033418,"end_time":"2021-03-06T06:44:20.451103","exception":false,"start_time":"2021-03-06T06:44:20.417685","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"xg_final = Pipeline([\n    ('preprocess',preprocess),\n    ('xg', XGBClassifier(learning_rate=0.1,objective='binary:logistic', max_depth=3, n_estimators = 500, n_jobs=8, random_state=0 ,gamma=0.1, verbosity=0))\n])\n\nxg_final.fit(X_train, y_train)\n\ny_xg_pred = xg_final.predict(X_test)\ny_xg_pred_prob = xg_final.predict_proba(X_test)\n\nprint(f\"XG AUROC Score : {roc_auc_score(y_test, y_xg_pred_prob[:,1])}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This score was subject to some variability, but did not reach higher than 0.72. The random forest classifier appeared to perform better given its higher AUROC score, so I will focus on it for the remaining investigation. "},{"metadata":{},"cell_type":"markdown","source":"# Final Model\n\nI have chosen to further investigate my finalized random forest model, since it has the highest AUROC score of 0.73.\n\n### Feature Importance\n\nFirst, I want to see which features are contributing to the model the most. With random forests, this is quite straightforward using the `feature_importances_` attribute. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Variable importance\nimportances = rf_final.named_steps['rf'].feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# PLOTTING \nf,ax = plt.subplots(figsize=(6,8))\nplt.title(\"Variable Importance - XGBoosted Model\")\nsns.barplot(y=[X_train.columns[i] for i in indices[0:8]] , x=importances[indices[0:8]])","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.015322,"end_time":"2021-03-06T06:44:20.685899","exception":false,"start_time":"2021-03-06T06:44:20.670577","status":"completed"},"tags":[]},"cell_type":"markdown","source":"Here, we can see the importance scores of the different features. Rather interesting to see that the number of takeaways by both the home and away team appear to contribute the most to predicting a win or loss. That could perhaps be a loose indication of how aggressively one team is chasing the other for the puck.\n\nThe number of shots by both teams is a rather expected result. It seems reasonable that a team that gets more shots on net in a game is more likely to win.  \n\nOf the remaining features, `settled_in` was particularly interesting to see here. This indicates that a visiting team might be slightly more or less likely to win if the game proceeds into overtime. However, the importance of this feature is relatively low, so it is unreasonable to say anything conclusive here. "},{"metadata":{},"cell_type":"markdown","source":"### ROC Curve"},{"metadata":{"papermill":{"duration":0.041091,"end_time":"2021-03-06T06:44:20.742474","exception":false,"start_time":"2021-03-06T06:44:20.701383","status":"completed"},"tags":[],"trusted":false},"cell_type":"code","source":"fpr, tpr, _ = roc_curve(y_test, y_rf_pred_prob[:,1])\n\nax=sns.lineplot(x=fpr,y=tpr)\nax.set_xlabel(\"False Positive Rate\")\nax.set_ylabel(\"True Positive Rate\")\n\nprint(f\"AUROC: {auc(fpr, tpr)}\")","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.016261,"end_time":"2021-03-06T06:44:20.775193","exception":false,"start_time":"2021-03-06T06:44:20.758932","status":"completed"},"tags":[]},"cell_type":"markdown","source":"The AUROC score for the best model is not fantastic, however, I am not surprised by this as hockey is known to have a reasonable level of \"chance\". Generally, models seems to have difficulty delivering reliable predictions of hockey outcomes. That being said, this model still provides a reasonable improvement over the baseline accuracy of 0.48 that might be achieved with random guessing. "},{"metadata":{"papermill":{"duration":0.015418,"end_time":"2021-03-06T06:44:20.806712","exception":false,"start_time":"2021-03-06T06:44:20.791294","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Conclusions\n\n- In this notebook, I implement XGBoost using Python and Scikit-Learn to predict whether a visiting hockey team will win a game \n- I trained two logistic regression models, one with the standard features and another with a polynomial transformation of them\n- I trained a random forest model by tuning two parameters using standard iteration and using GridSearchCV\n- I trained a gradient boosted decision tree model by tuning three parameters using GridSearchCV\n- The random forest model performed best, yielding an AUC of about 0.73\n- Given that hockey is a relatively hard sport to predict outcomes in, my model's performance seems adequate and acceptable"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}