{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Beginner's Level Notebook \n## The following notebook is written only to practice the implementation of ANN on a basic dataset. \n## Basic hyperparameter optimisation is discussed.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport seaborn as sns\nimport pandas\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n%matplotlib inline","metadata":{"id":"njD7oTDl8JIR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")\n","metadata":{"id":"PNAJJcNV8aiW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(10)","metadata":{"id":"IUc32A3S9UYZ","outputId":"952dd1a9-6889-455c-a3f4-64e4b639768c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes","metadata":{"id":"mJ6yY8FB99Ov","outputId":"5ba3635a-6de2-4f65-952c-5dbbfa084b0b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"Outcome\"].value_counts()","metadata":{"id":"YKqr_QEuQLky","outputId":"cd6ed19e-1be2-47e6-fdb8-7f5dbf0af440","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"id":"Nfm-RvWF9hYH","outputId":"659f26ed-353e-44ac-a287-da6c20f70fda","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(7, 5))\nsns.countplot(data=data, x='Outcome')\nplt.show()","metadata":{"id":"NbdfCJJC-V5a","outputId":"f0f093ed-e1ff-44c8-8eee-a928432c3617","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape\n","metadata":{"id":"kFiN_ey68szg","outputId":"f49ad939-57b3-459a-f782-b8ebf33ed8f8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if any null value is present\ndata.isnull().values.any()","metadata":{"id":"txbIk3nO8vHS","outputId":"3bc61aa7-df5b-4c65-a9ce-b925862426f0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True)","metadata":{"id":"Bvi8YRBR85ow","outputId":"ed72419f-c056-4da1-b2f7-d2978d0c13d4","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.corr()","metadata":{"id":"y19wA3G19DiF","outputId":"a4c8f5af-6015-4fca-90a8-f2930ff28d46","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y=data[\"Outcome\"]\nY","metadata":{"id":"bQCV-Tl19NDq","outputId":"13d8787a-335f-4c85-feba-9eb7774eec1a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=data.drop(\"Outcome\",axis=1)","metadata":{"id":"g1UDSId6AFEu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head(10)","metadata":{"id":"hSQ8Jw0YADDa","outputId":"137241a7-afe6-43ee-a508-4dcb00869b98","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=X.astype(float)","metadata":{"id":"QNv0-x-a1Sdi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.dtypes","metadata":{"id":"Gjt0jrmc1J4_","outputId":"dce11633-3c6a-43d4-88ee-ac287ec1db74","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head(10)","metadata":{"id":"n7a0pwiR1afv","outputId":"fc45d96e-b0bf-4905-ef34-2cf3d2ca8feb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lst=[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\"]","metadata":{"id":"Q7uNryUFzdSA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in lst:\n  tem=X[i]\n  mean=X[i].mean()\n  for j in range(len(tem)):\n    if tem[j]==0:\n      tem[j]=mean\n  X.replace(i,tem,inplace=True)\n\n  \n  ","metadata":{"id":"Ii-8b5i4z4eS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head(10)","metadata":{"id":"CoCZmCuSz4hP","outputId":"40327561-9a89-44fd-d4be-69790eda3be3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(64, input_dim=8, activation='relu',kernel_initializer='glorot_uniform'))\nmodel.add(Dense(32,activation='relu',kernel_initializer='glorot_uniform'))\nmodel.add(Dense(16,activation='relu',kernel_initializer='glorot_uniform'))\nmodel.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"id":"qCcJxB3ASQ78","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model.fit(X, Y,validation_split=0.33, epochs=300)","metadata":{"id":"8_x1uc0ySQ5w","outputId":"8fe8dc94-ced9-4442-d32d-856d420cbb81","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['train', 'test'], loc='upper left')","metadata":{"id":"1di7p-ojSQ3J","outputId":"387760a2-212f-4a8f-c9c6-5bbb71be81b7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see epochs less than 300 are sufficient","metadata":{"id":"EGUcogjGW49F"}},{"cell_type":"markdown","source":"NOW LETS NORMALISE OUR DATASET ANT THEN FEED IT INTO OUR MODEL","metadata":{"id":"R15eNiBQW_8D"}},{"cell_type":"code","source":"from sklearn import preprocessing\n\nx = X.values \nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nx_transformed = pd.DataFrame(x_scaled)","metadata":{"id":"otO32EYkSQlf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2 = Sequential()\nmodel_2.add(Dense(64, input_dim=8, activation='relu',kernel_initializer='glorot_uniform'))\nmodel_2.add(Dense(32,activation='relu',kernel_initializer='glorot_uniform'))\nmodel_2.add(Dense(16,activation='relu',kernel_initializer='glorot_uniform'))\nmodel_2.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"id":"pKZMoQXhX32l","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_2=model_2.fit(x_transformed, Y,validation_split=0.33, epochs=100)","metadata":{"id":"f8EkU37kYMii","outputId":"304d003a-fb2b-4d12-dacb-66a93042362a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history_2.history['loss'])\nplt.plot(history_2.history['val_loss'])\nplt.legend(['train', 'test'], loc='upper left')","metadata":{"id":"WIJ-cpiWYSrO","outputId":"fe1d6b0c-31ab-4e06-e4ed-02d0056866f0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that after noramlising our dataset we need much less epochs (less than 50) and we also get much higher accuracy. ","metadata":{"id":"6phzoqyLZ6g5"}},{"cell_type":"markdown","source":"INTRODUCING BATCH SIZE  ","metadata":{}},{"cell_type":"code","source":"model_3 = Sequential()\nmodel_3.add(Dense(64, input_dim=8, activation='relu',kernel_initializer='glorot_uniform'))\nmodel_3.add(Dense(32,activation='relu',kernel_initializer='glorot_uniform'))\nmodel_3.add(Dense(16,activation='relu',kernel_initializer='glorot_uniform'))\nmodel_3.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel_3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_3=model_3.fit(x_transformed, Y,validation_split=0.33, epochs=100, batch_size=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history_3.history['loss'])\nplt.plot(history_3.history['val_loss'])\nplt.legend(['train', 'test'], loc='upper left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see here that using batch propagation is not much helpful in small datasets","metadata":{}}]}