{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Introduction\nGreetings! This is the first public kernel. In this kernel, I  will demonstrate few steps of NLP, few plots like wordcloud, frequency plot."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#general imports\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport seaborn as sns # plotting\nimport matplotlib.pyplot as plt # plotting\n%matplotlib inline\nimport os # accessing directory structure\n\n#NLP processing imports\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import sent_tokenize, word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nfrom nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nimport re\nimport spacy\n\n###Vader Sentiment\n#To install vaderSentiment\n!pip install vaderSentiment \nfrom vaderSentiment import vaderSentiment\nfrom textblob import TextBlob\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n####Lemmatization\nfrom nltk.stem import WordNetLemmatizer\n# Lemmatize with POS Tag\nfrom nltk.corpus import wordnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/twcs.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.loc[:10000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nRow, nCol = data.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#DataTypes\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"text\"] = data[\"text\"].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TEXT CLEANING"},{"metadata":{},"cell_type":"markdown","source":"### Remove urls,@mention, https"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+|@[^\\s]+')\n    return url_pattern.sub(r'', text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"textclean\"] = data[\"text\"].apply(lambda text: remove_urls(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find out the top 100 words which are getting used in the text of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_N = 100 #top 100 words\n\n#convert list of list into text\na = data['textclean'].str.lower().str.cat(sep=' ')\n\n# removes punctuation,numbers and returns list of words\nb = re.sub('[^A-Za-z]+', ' ', a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove all the stopwords from the text\nstop_words = list(get_stop_words('en'))         \nnltk_words = list(stopwords.words('english'))   \nstop_words.extend(nltk_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_tokens = word_tokenize(b) # Tokenization\nfiltered_sentence = [w for w in word_tokens if not w in stop_words]\nfiltered_sentence = []\nfor w in word_tokens:\n    if w not in stop_words:\n        filtered_sentence.append(w)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove characters which have length less than 2  \nwithout_single_chr = [word for word in filtered_sentence if len(word) > 2]\n\n# Remove numbers\ncleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**#### *Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.*"},{"metadata":{},"cell_type":"markdown","source":"# Lemmatization"},{"metadata":{},"cell_type":"markdown","source":"#### I am using Wordnet Lemmatizer with appropriate POS tag. \n#### Function to map word with its POS tag"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    return tag_dict.get(tag, wordnet.NOUN)\n\n# 1. Init Lemmatizer\nlemmatizer = WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatized_output = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in cleaned_data_title]\nlemmatized_output = [word for word in lemmatized_output if not word.isnumeric()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Frequency distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"word_dist = nltk.FreqDist(lemmatized_output)\ntop100_words = pd.DataFrame(word_dist.most_common(top_N),\n                    columns=['Word', 'Frequency'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x=\"Frequency\",y=\"Word\", data=top100_words.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WordCloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"def wc(data,bgcolor,title):\n    plt.figure(figsize = (80,80))\n    wc = WordCloud(background_color = bgcolor, max_words = 100,  max_font_size = 50)\n    wc.generate(' '.join(data))\n    plt.imshow(wc)\n    plt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc(lemmatized_output,'black','Common Words' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VADER + TEXTBLOB Sentiment Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"sent_analyser = SentimentIntensityAnalyzer()\ndef sentiment(text):\n    return (sent_analyser.polarity_scores(text)[\"compound\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Polarity\"] = data[\"textclean\"].apply(sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def senti(data):\n    if data['Polarity'] >= 0.05:\n        val = \"Positive\"\n    elif data['Polarity'] <= -0.05:\n        val = \"Negative\"\n    else:\n        val = \"Neutral\"\n    return val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Sentiment'] = data.apply(senti, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.set_style(\"whitegrid\")\nax = sns.countplot(x=\"Sentiment\", data=data, \n                  palette=dict(Neutral=\"blue\", Positive=\"Green\", Negative=\"Red\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ASPECT MINING/ OPINION MINING"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import spacy\nnlp = spacy.load(\"en_core_web_sm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pos(text):\n    doc = nlp(text)\n    # You want list of Verb tokens \n    aspects = [token.text for token in doc if token.pos_ == \"NOUN\"]\n    return aspects","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Aspects\"] = data[\"textclean\"].apply(pos)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is scope of improvement. I will learn and update it accordingly"},{"metadata":{},"cell_type":"markdown","source":"**P.S This is my first Kaggle Kernel and I am fairly new to python programming as well, hence my non usage of list comprehensions and functions might be evident. I highly encourage everyone to fork my code and add your own twists to increase the accuracy of both aspect extractions and sentiment analysis.**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}