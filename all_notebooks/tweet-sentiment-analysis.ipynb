{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Tweet Pre Processing**","execution_count":null},{"metadata":{"_uuid":"24b0baf5-6272-459c-b65d-e1b8d7cd61e0","_cell_guid":"1e5decab-0213-4bcb-af16-6e66845378d0","trusted":true},"cell_type":"code","source":"# Text Normalization\n\"\"\"\nCreated on Thu May 14 13:38:19 2020\n\n@author: rahul\n\"\"\"\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.tag import pos_tag\nfrom nltk.corpus import wordnet\nimport string\nimport pkg_resources\n\ndef replaceElongated(word):\n    \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon \"\"\"\n\n    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n    repl = r'\\1\\2\\3'\n    if wordnet.synsets(word):\n        return word\n    repl_word = repeat_regexp.sub(repl, word)\n    if repl_word != word:      \n        return replaceElongated(repl_word)\n    else:       \n        return repl_word\n    \ndef load_dict_contractions():    \n    return {\n        \"cant\":\"can not\",\n        \"dont\":\"do not\",\n        \"wont\":\"will not\",\n        \"ain't\":\"is not\",\n        \"amn't\":\"am not\",\n        \"aren't\":\"are not\",\n        \"can't\":\"cannot\",\n        \"'cause\":\"because\",\n        \"couldn't\":\"could not\",\n        \"couldn't've\":\"could not have\",\n        \"could've\":\"could have\",\n        \"daren't\":\"dare not\",\n        \"daresn't\":\"dare not\",\n        \"dasn't\":\"dare not\",\n        \"didn't\":\"did not\",\n        \"doesn't\":\"does not\",\n        \"don't\":\"do not\",\n        \"e'er\":\"ever\",\n        \"em\":\"them\",\n        \"everyone's\":\"everyone is\",\n        \"finna\":\"fixing to\",\n        \"gimme\":\"give me\",\n        \"gonna\":\"going to\",\n        \"gon't\":\"go not\",\n        \"gotta\":\"got to\",\n        \"hadn't\":\"had not\",\n        \"hasn't\":\"has not\",\n        \"haven't\":\"have not\",\n        \"he'd\":\"he would\",\n        \"he'll\":\"he will\",\n        \"he's\":\"he is\",\n        \"he've\":\"he have\",\n        \"how'd\":\"how would\",\n        \"how'll\":\"how will\",\n        \"how're\":\"how are\",\n        \"how's\":\"how is\",\n        \"I'd\":\"I would\",\n        \"I'll\":\"I will\",\n        \"I'm\":\"I am\",\n        \"I'm'a\":\"I am about to\",\n        \"I'm'o\":\"I am going to\",\n        \"isn't\":\"is not\",\n        \"it'd\":\"it would\",\n        \"it'll\":\"it will\",\n        \"it's\":\"it is\",\n        \"I've\":\"I have\",\n        \"kinda\":\"kind of\",\n        \"let's\":\"let us\",\n        \"mayn't\":\"may not\",\n        \"may've\":\"may have\",\n        \"mightn't\":\"might not\",\n        \"might've\":\"might have\",\n        \"mustn't\":\"must not\",\n        \"mustn't've\":\"must not have\",\n        \"must've\":\"must have\",\n        \"needn't\":\"need not\",\n        \"ne'er\":\"never\",\n        \"o'\":\"of\",\n        \"o'er\":\"over\",\n        \"ol'\":\"old\",\n        \"oughtn't\":\"ought not\",\n        \"shalln't\":\"shall not\",\n        \"shan't\":\"shall not\",\n        \"she'd\":\"she would\",\n        \"she'll\":\"she will\",\n        \"she's\":\"she is\",\n        \"shouldn't\":\"should not\",\n        \"shouldn't've\":\"should not have\",\n        \"should've\":\"should have\",\n        \"somebody's\":\"somebody is\",\n        \"someone's\":\"someone is\",\n        \"something's\":\"something is\",\n        \"that'd\":\"that would\",\n        \"that'll\":\"that will\",\n        \"that're\":\"that are\",\n        \"that's\":\"that is\",\n        \"there'd\":\"there would\",\n        \"there'll\":\"there will\",\n        \"there're\":\"there are\",\n        \"there's\":\"there is\",\n        \"these're\":\"these are\",\n        \"they'd\":\"they would\",\n        \"they'll\":\"they will\",\n        \"they're\":\"they are\",\n        \"they've\":\"they have\",\n        \"this's\":\"this is\",\n        \"those're\":\"those are\",\n        \"'tis\":\"it is\",\n        \"'twas\":\"it was\",\n        \"wanna\":\"want to\",\n        \"wasn't\":\"was not\",\n        \"we'd\":\"we would\",\n        \"we'd've\":\"we would have\",\n        \"we'll\":\"we will\",\n        \"we're\":\"we are\",\n        \"weren't\":\"were not\",\n        \"we've\":\"we have\",\n        \"what'd\":\"what did\",\n        \"what'll\":\"what will\",\n        \"what're\":\"what are\",\n        \"what's\":\"what is\",\n        \"what've\":\"what have\",\n        \"when's\":\"when is\",\n        \"where'd\":\"where did\",\n        \"where're\":\"where are\",\n        \"where's\":\"where is\",\n        \"where've\":\"where have\",\n        \"which's\":\"which is\",\n        \"who'd\":\"who would\",\n        \"who'd've\":\"who would have\",\n        \"who'll\":\"who will\",\n        \"who're\":\"who are\",\n        \"who's\":\"who is\",\n        \"who've\":\"who have\",\n        \"why'd\":\"why did\",\n        \"why're\":\"why are\",\n        \"why's\":\"why is\",\n        \"won't\":\"will not\",\n        \"wouldn't\":\"would not\",\n        \"would've\":\"would have\",\n        \"y'all\":\"you all\",\n        \"you'd\":\"you would\",\n        \"you'll\":\"you will\",\n        \"you're\":\"you are\",\n        \"you've\":\"you have\",\n        \"Whatcha\":\"What are you\",\n        \"luv\":\"love\",\n        \"sux\":\"sucks\",\n        \"couldn't\":\"could not\",\n        \"wouldn't\":\"would not\",\n        \"shouldn't\":\"should not\",\n        \"im\":\"i am\"\n        }\n\nsingle_word = list(string.ascii_lowercase)\n\nlemmatizer = WordNetLemmatizer()\nps = PorterStemmer()\nstop_words = set(stopwords.words('english'))-set(['not', 'no'])\n\ndef normalization(text):\n    text = str(text).lower()\n    \n    # Unicodes\n    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n    text = re.sub(r'[^\\x00-\\x7f]',r'',text)\n    \n    # URL\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    \n    # User Tag\n    text = re.sub('@[^\\s]+',' ',text)\n    \n    # Hash Tag\n    text = re.sub(r'#([^\\s]+)', r' ', text)\n    \n    # Number\n    text = ''.join([i for i in text if not i.isdigit()])      \n    \n    \n    # Punctuation\n    #text = ' '.join([char for char in text if char not in string.punctuation])\n    for sym in string.punctuation:\n        text = text.replace(sym, \" \")\n    \n    # Elongated Words\n    for word in text.split():\n        text = text.replace(word, replaceElongated(word))\n    \n    # Contraction\n    CONTRACTIONS = load_dict_contractions()\n    text = text.replace(\"â€™\",\"'\")\n    words = text.split()\n    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n    text = \" \".join(reformed)\n    \n    \"\"\"\n    # Lemmatization \n    for word in text.split():\n        text = text.replace(word, lemmatizer.lemmatize(word))\n        \n    # Stemming\n    for word in text.split():\n        text = text.replace(word, ps.stem(word))\n    \n    # Correct Spell\n    max_edit_distance_lookup = 2\n    suggestions = sym_spell.lookup_compound(text, max_edit_distance_lookup)\n    for suggestion in suggestions:\n        text = ''.join(suggestion.term)    \n    \n    # Stop words\n    text = \" \".join([word for word in text.split() if not word in stop_words])\n    \n    # Remove Extras\n    # Source: https://www.geeksforgeeks.org/part-speech-tagging-stop-words-using-nltk-python/\n    # Source: https://stackoverflow.com/questions/39634222/is-there-a-way-to-remove-proper-nouns-from-a-sentence-using-python\n    tagged_text = pos_tag(text.split())\n    edited_text = [word for word, tag in tagged_text if tag != 'NN' and tag != 'NNS' and tag != 'NNP' and tag != 'NNPS' and tag != 'PRP' and tag != 'PRP$']\n    text = ' '.join(edited_text)\n    \"\"\"\n    # Single Character   \n    text = ' '.join( [w for w in text.split() if len(w)>1 and w != 'a' and w != 'i'])\n         \n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Import Library**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom collections import Counter","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Import Dataset**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\ndf_train_2=pd.read_csv('../input/complete-tweet-sentiment-extraction-data/tweet_dataset.csv')\ndf_test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_train.drop(df_train[df_train.sentiment=='neutral'].index, axis=0, inplace=True)\n#df_train_2.drop(df_train[df_train_2.new_sentiment=='neutral'].index, axis=0, inplace=True)\n#df_test.drop(df_test[df_test.sentiment=='neutral'].index, axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Label Encoding and conversion into categorial vector**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_2.info()\ndf_train_2.drop(columns=['sentiment', 'author', 'old_text', 'aux_id'], inplace=True)\ndf_train_2=df_train_2.rename(columns={'new_sentiment':'sentiment'})\ndf_train_2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()\ndf_train=df_train[['textID', 'text', 'selected_text', 'sentiment']]\ndf_train_2.info()\n\ndf_train=df_train.append(df_train_2)\ndf_train['sentiment'].replace('', np.nan, inplace=True)\ndf_train.dropna(subset=['sentiment'], inplace=True)\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train['sentiment'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['encoded_sentiment']=encoder.fit_transform(df_train['sentiment'])\ndf_train=pd.get_dummies(df_train, columns=['sentiment'])\ndf_train=df_train[['textID', 'text', 'encoded_sentiment', 'sentiment_negative', 'sentiment_neutral', 'sentiment_positive', 'selected_text']]\n#df_train=df_train[['textID', 'text', 'encoded_sentiment', 'sentiment_negative', 'sentiment_positive', 'selected_text']]\n\ndf_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['encoded_sentiment']=encoder.fit_transform(df_test['sentiment'])\ndf_test=pd.get_dummies(df_test, columns=['sentiment'])\ndf_test=df_test[['textID', 'text', 'encoded_sentiment', 'sentiment_negative', 'sentiment_neutral', 'sentiment_positive']]\n#df_test=df_test[['textID', 'text', 'encoded_sentiment', 'sentiment_negative', 'sentiment_positive']]\n\ndf_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['text'] = df_train['text'].apply(normalization)\ndf_train['text'].replace('', np.nan, inplace=True)\ndf_train.dropna(subset=['text'], inplace=True)\n\ndf_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['text'] = df_test['text'].apply(normalization)\ndf_test['text'].replace('', np.nan, inplace=True)\ndf_test.dropna(subset=['text'], inplace=True)\n\ndf_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pos = len(df['encoded_sentiment'][df.encoded_sentiment == 2])\npos = len(df_train['encoded_sentiment'][df_train.encoded_sentiment == 2])\nneu = len(df_train['encoded_sentiment'][df_train.encoded_sentiment == 1])\nneg = len(df_train['encoded_sentiment'][df_train.encoded_sentiment == 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_count(sentence):\n    return len(str(sentence).split())\ndf_train['word_count'] = df_train['text'].apply(word_count)\n\n#pos_sen_len = df['word_count'][df.encoded_sentiment == 2]\npos_sen_len = df_train['word_count'][df_train.encoded_sentiment == 2]\nneu_sen_len = df_train['word_count'][df_train.encoded_sentiment == 1]\nneg_sen_len = df_train['word_count'][df_train.encoded_sentiment == 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Plotting distribution**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.xlim(0, 35, 5)\nplt.xlabel('word count')\nplt.ylabel('frequency')\nplt.hist([pos_sen_len, neu_sen_len, neg_sen_len], color=['r', 'g', 'b'], alpha=0.5, label=['positive', 'neutral', 'negative'])\n#plt.hist([pos_sen_len, neg_sen_len], color=['r', 'b'], alpha=0.5, label=['positive', 'negative'])\nplt.legend(loc='upper right')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words = []\nfor line in list(df_train['text']):\n    words = str(line).split()\n    for word in words:\n        all_words.append(word.lower())     \nprint(Counter(all_words).most_common(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()\n#X=(df_train.iloc[:, 1].values).astype('U')\ny=(df_train.iloc[:, 3:6].values).astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()\n#X_test=(df_test.iloc[:, 1].values).astype('U')\ny_test=(df_test.iloc[:, 3:6].values).astype('int32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Entire dataset shuffling**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom sklearn.utils import shuffle\nX, y = shuffle(X, y, random_state=0)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Createing Glove Tweet Vector Dictonary**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Glove Word Vocab\nfrom tqdm import tqdm\ndef read_glove_vecs(glove_file):\n    with open(glove_file, 'r') as f:\n        words=set()\n        word_to_vec_map=dict()\n        for line in tqdm(f):\n            line=line.strip().split()\n            curr_word=''.join(line[:-300])\n            words.add(curr_word)\n            word_to_vec_map[curr_word]=np.array(line[-300:], dtype=np.float32)\n            \n        i=1\n        words_to_index=dict()\n        index_to_words=dict()\n        for w in sorted(words):\n            words_to_index[w]=i\n            index_to_words[i]=w\n            i+=1\n    return words_to_index, index_to_words, word_to_vec_map\nword_to_index, index_to_word, word_to_vec_map=read_glove_vecs('../input/glove840b/glove.840B.300d.txt')\n\nprint(len(word_to_index))\nprint(list(word_to_index.items())[:5])\nprint(list(word_to_vec_map.items())[:2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Import Keras Library**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\nfrom keras import regularizers, callbacks\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional, SpatialDropout1D, GlobalMaxPooling1D, BatchNormalization, PReLU\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import RMSprop, Adam, Adamax, SGD\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.losses import BinaryCrossentropy\nfrom keras.metrics import Precision, Recall, CategoricalAccuracy\nnp.random.seed(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Counting the words which have not been used in word embeddidng**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\navoided_word = list()\n\ndef sentences_to_indices(data, word_to_index, max_len):\n    m=data.shape[0]\n    indices=np.zeros((m, max_len))\n    for i in range(m):\n        sentence_words=data[i].lower().split()\n        j=0\n        for w in sentence_words:\n            if w in word_to_index:\n                indices[i, j]=word_to_index[w]\n            else:\n                avoided_word.append(w)\n            j+=1\n    return indices\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import text, sequence\ntk=text.Tokenizer(num_words=200000)\n\ndf_train.text=df_train.text.astype(str)\ndf_test.text=df_test.text.astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tk.fit_on_texts(list(df_train.text.values)+list(df_test.text.values))\nX_text_indices=tk.texts_to_sequences(df_train.text.values)\nX_test_text_indices=tk.texts_to_sequences(df_test.text.values)\n\nmaxlen=-1\nfor text in X_text_indices:\n    if len(text)>maxlen:\n        maxlen=len(text)\nfor text in X_test_text_indices:\n    if len(text)>maxlen:\n        maxlen=len(text)       \nprint(maxlen)\n\nX_text_indices=sequence.pad_sequences(X_text_indices, maxlen=maxlen)\nprint(X_text_indices.shape)\n\nX_test_text_indices=sequence.pad_sequences(X_test_text_indices, maxlen=maxlen)\nprint(X_test_text_indices.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index=tk.word_index\nprint(len(word_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Custom Word Embedding**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix=np.zeros((len(word_index)+1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector=word_to_vec_map.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i]=embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Creating pre trained word embedding**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ndef pretrained_embedding_layer(word_to_vec_map, word_to_index):\n    vocab_len=len(word_to_index)+1\n    emb_dim=word_to_vec_map['cucumber'].shape[0]\n    emb_matrix=np.zeros((vocab_len, emb_dim))\n    for word, idx in word_to_index.items():\n        emb_matrix[idx, :]=word_to_vec_map[word]\n        #emb_matrix[idx, :]=np.pad(word_to_vec_map[word], (0, (word_to_vec_map['cucumber'].shape[0]-word_to_vec_map[word].shape[0])), 'constant')\n    embedding_layer=Embedding(input_dim=vocab_len, output_dim=emb_dim, trainable=False)\n    embedding_layer.build((None,))\n    embedding_layer.set_weights([emb_matrix])\n    return embedding_layer\n\nembedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\nprint(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Evaluating maximum length of input string**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nmaxLen=-1\nfor text in X:\n    if len(text.split())>maxLen:\n        maxLen=len(text.split())\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ndef Sentiment_Extraction(input_shape, word_to_vec_map, word_to_index):    \n    sentence_indices=Input(shape=input_shape, dtype='int32')\n    embedding_layer=pretrained_embedding_layer(word_to_vec_map, word_to_index)\n    embeddings=embedding_layer(sentence_indices)\n    #embeddings=SpatialDropout1D(0.5)(embeddings) \n    \n    X=LSTM(units=300, return_sequences=False, recurrent_dropout=0.2, dropout=0.2)(embeddings)\n    X=BatchNormalization()(X)\n    \n    X=Dense(200)(X)\n    X=PReLU()(X)\n    X=Dropout(0.2)(X)\n    X=BatchNormalization()(X)\n\n    X=Dense(200)(X)\n    X=PReLU()(X)\n    X=Dropout(0.2)(X)\n    X=BatchNormalization()(X)\n        \n    #X=LSTM(units=128, return_sequences=False)(X)\n    #X=Dropout(0.5)(X)\n    X=Dense(units=2)(X)\n    X=Activation(activation='softmax')(X)\n        \n    model=Model(inputs=sentence_indices, outputs=X)\n    return model\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model=Sentiment_Extraction((maxLen,), word_to_vec_map, word_to_index)\n#model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\nmodel.add(Embedding(len(word_index)+1, 300, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n\nmodel.add(LSTM(units=300, return_sequences=False, recurrent_dropout=0.2, dropout=0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(200))\nmodel.add(PReLU())\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(200))\nmodel.add(PReLU())\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(200))\nmodel.add(PReLU())\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(200))\nmodel.add(PReLU())\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(units=3))\nmodel.add(Activation(activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy' , optimizer='adam', metrics=['accuracy'])\n#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'precision', 'recall'])\n#model.compile(loss='categorical_crossentropy' , optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_text_indices = sentences_to_indices(X, word_to_index, maxLen)\nfrom sklearn.model_selection import train_test_split\nX_train_text_indices, X_dev_text_indices, y_train, y_dev = train_test_split(X_text_indices, y, test_size=0.1, random_state=42)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#callbacks = [EarlyStopping(monitor='val_loss', patience=3),] #ReduceLROnPlateau(monitor='val_loss', factor=0.01, patience=1),]\n#history = model.fit(X_text_indices, y, epochs=50, batch_size=32, validation_split=0.25, callbacks=callbacks ,shuffle=False)\n#history = model.fit(X_text_indices, y, epochs=50, batch_size=32, validation_split=0.2, shuffle=True, verbose=1)\nhistory = model.fit(X_train_text_indices, y_train, epochs=50, batch_size=128, validation_data=(X_dev_text_indices, y_dev), shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Loss vs epooch**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model train vs validation loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model train vs validation accuracy')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('tweet_sentiment_extraction.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\nmodel = load_model('tweet_sentiment_extraction.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"para = model.evaluate(X_dev_text_indices, y_dev)\nprint()\nprint(\"Test loss :\", para[0], 'Test accuracy :', para[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_dev_pred = model.predict(X_dev_text_indices)\n\nfor i in range(len(y_dev_pred)):\n    y_dev_pred[i] = np.argmax(y_dev_pred[i])\n\ny_dev_pred = y_dev_pred[:, 0]\nprint(y_dev_pred)\n\nfor i in range(len(y_dev)):\n    y_dev[i] = np.argmax(y_dev[i])\ny_dev = y_dev[:, 0]\nprint(y_dev)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm_dev = confusion_matrix(y_dev, y_dev_pred)\nprint(cm_dev)\n\ntotal=sum(sum(cm_dev))\n\naccuracy = (cm_dev[0,0]+cm_dev[1,1]+cm_dev[2,2])/total\nprint('Accuracy:', accuracy)\n\nsensitivity = cm_dev[0,0]/(cm_dev[0,0]+cm_dev[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = cm_dev[1,1]/(cm_dev[1,1]+cm_dev[0,1])\nprint('Specificity : ', specificity)\n\"\"\"\naccuracy = (cm_dev[0,0]+cm_dev[1,1])/total\nprint('Accuracy:', accuracy)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = model.predict(X_test_text_indices)\n\nfor i in range(len(y_test_pred)):\n    y_test_pred[i] = np.argmax(y_test_pred[i])\ny_test_pred = y_test_pred[:, 0]\n\nfor i in range(len(y_test)):\n    y_test[i] = np.argmax(y_test[i])\ny_test = y_test[:, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm_test = confusion_matrix(y_test, y_test_pred)\nprint(cm_test)\n\ntotal=np.sum(cm_test)\n\naccuracy = (cm_test[0,0]+cm_test[1,1]+cm_test[2,2])/total\nprint('Accuracy:', accuracy)\n\nsensitivity = cm_test[0,0]/(cm_test[0,0]+cm_test[1,0])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = cm_test[1,1]/(cm_test[1,1]+cm_test[0,1])\nprint('Specificity : ', specificity)\n\"\"\"\naccuracy = (cm_test[0,0]+cm_test[1,1])/total\nprint('Accuracy:', accuracy)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\ndef jaccard_distance(y_true, y_pred, smooth=100):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n    return (1 - jac) * smooth, intersection, sum_, jac\nscore, intersection, sum_, jac = jaccard_distance(y_true=y_test.astype('int32'), y_pred=y_test_pred.astype('int32'))\nprint(jac)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}