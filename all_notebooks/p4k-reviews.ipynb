{"cells":[{"metadata":{"_uuid":"f49968d969cfc61fd2044d302fd0886530098464"},"cell_type":"markdown","source":"Let's Investigate P4K scores"},{"metadata":{"_uuid":"16d7434603f65c4a72ea45fcd33a7b453979595b"},"cell_type":"markdown","source":"Setup and Data Preparation"},{"metadata":{"trusted":true,"_uuid":"6a578212ca894e40456b8eb874fedb6809dce2d0"},"cell_type":"code","source":"# %matplotlib inline\n\n#Display all outputs from cells\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n#Import Packages\n\n#fs\nimport os as os\n\n#Data manipulation\nimport numpy as np\nimport pandas as pd\n\n#Plotting\nimport plotnine as p9\nimport matplotlib.pyplot as plt\n\n#iteration\nimport itertools as it\n\n#Wordclouds\nfrom wordcloud import WordCloud\nimport wordcloud\n\n#Combinatorics\nimport collections\n\n#regex\nimport re\n\n#Dates\nimport datetime as dt\n\n#NLTK\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import subjectivity\nfrom nltk.sentiment import SentimentAnalyzer\nfrom nltk.sentiment.util import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n#scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n\n#Math\nfrom math import sqrt\n\n#remove non-alpha numeric text from str\ndef stripNonAlphaNum(text):\n    import re\n    return re.compile(r'\\W+', re.UNICODE).split(text)\n\n\n#Import data\ndf_p4k = pd.read_csv(filepath_or_buffer = \"../input/p4kreviews.csv\",\n                     encoding='latin1')\n\n#Remove the row count\ndf_p4k.drop(columns=df_p4k.columns[0],\n            inplace= True)\n\n#Convert genre to categorical\ndf_p4k['genre'] = df_p4k['genre'].astype('category')","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"895689ed9c1075267c102274dc04a33d35ec4bdf"},"cell_type":"code","source":"#Open console\n#%qtconsole","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"89b9056b5bdd2dc0cf286a43ba55edf7f526aac9"},"cell_type":"markdown","source":"Do some simple summary plots"},{"metadata":{"trusted":true,"_uuid":"f0c417aac952688bc0a916e6fb56cb4293ce8ab5"},"cell_type":"code","source":"p9.ggplot(data=df_p4k, mapping=p9.aes(x = \"score\")) + p9.geom_density()","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eba86e1900b5ef389ceaeed1c9705d23df61a00b"},"cell_type":"code","source":"df_p4k_sum = df_p4k.groupby(\"genre\").mean()\ndf_p4k_sum","execution_count":25,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa30767951ffa7da100538813b6e43e34ed80624"},"cell_type":"code","source":"df_p4k_best =  df_p4k[df_p4k['best'] == 1]\np9.ggplot(data=df_p4k_best, mapping=p9.aes(x = \"score\")) + p9.geom_density()","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ee2edcd7915d94d62b36de435d4abfe078a1685"},"cell_type":"code","source":"p9.ggplot(data=df_p4k, mapping=p9.aes(x = \"score\")) + p9.facet_wrap(\"~genre\") + p9.geom_density()","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"cf05acef63679303085f576bb00a4fd8e0d10bcf"},"cell_type":"markdown","source":"Word Clouds of the review"},{"metadata":{"trusted":true,"_uuid":"d6f2b15413aa21ec515e46b133a1760f5e2a809a"},"cell_type":"code","source":"wc_text = \" \".join(df_p4k['review'].head(10).as_matrix().astype('str'))\nwc_text = \" \".join(stripNonAlphaNum(wc_text))\np4k_wordcloud = WordCloud().generate(wc_text)\nwordcloud = WordCloud(max_font_size=40).generate(wc_text)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\n","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"c93962970afb8b069926e51e712da62ba54761f2"},"cell_type":"markdown","source":"Split word clouds by genre"},{"metadata":{"trusted":true,"_uuid":"804e6160390d6f0e9210f1d43a719848260aa59b"},"cell_type":"code","source":"for g in df_p4k['genre'].cat.categories:\n    wc_text = \" \".join(df_p4k[df_p4k['genre'] == g]['review'].head(100).as_matrix().astype('str')).lower()\n    wc_text = \" \".join(stripNonAlphaNum(wc_text))\n    p4k_wordcloud = WordCloud().generate(wc_text)\n    wordcloud = WordCloud(max_font_size=40).generate(wc_text)\n    plt.figure()\n    plt.title(\"word cloud for \" + g)\n    plt.imshow(wordcloud, interpolation=\"bilinear\")","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"59a663084ae41b70e222301fd948e25c71f3b354"},"cell_type":"markdown","source":"Remove the most common words"},{"metadata":{"trusted":false,"_uuid":"8dc422a0b4ffdcfb40caffd39e8e24ed644f12aa"},"cell_type":"code","source":"#Most common words\n\n#Find the most common words\nwc_text = \" \".join(df_p4k['review'].head(100).as_matrix().astype('str')).lower()\ntop_words = stripNonAlphaNum(wc_text)\ntop_words = collections.Counter(top_words)\n\n#Use this to create regex filter\nword_filter = top_words.most_common(100)\nword_filter = [x[0] for x in word_filter]\nword_filter = \"|\".join(word_filter)\n\nregex = re.compile(word_filter)\n\n#Filter out the matching words and recreate a huge string\nwc_text = filter(lambda x: not regex.match(x), wc_text.split())\nwc_text = \" \".join(wc_text)\n\np4k_wordcloud = WordCloud().generate(wc_text)\nwordcloud = WordCloud(max_font_size=40).generate(wc_text)\nplt.figure()\nplt.title(\"word cloud (with common words removed)\")\nplt.imshow(wordcloud, interpolation=\"bilinear\")","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"0fc690473c3802be98b8ce0abf1116dfb936a3f9"},"cell_type":"markdown","source":"Split by genre (with common words removed)"},{"metadata":{"trusted":false,"_uuid":"c2c765e0c1a6c0c05fed78ee6d88d3fd48471685"},"cell_type":"code","source":"\nfor g in df_p4k['genre'].cat.categories:\n    wc_text = \" \".join(df_p4k[df_p4k['genre'] == g]['review'].head(100).as_matrix().astype('str')).lower()\n    wc_text = \" \".join(stripNonAlphaNum(wc_text))\n    wc_text = filter(lambda x: not regex.match(x), wc_text.split())\n    wc_text = \" \".join(wc_text)\n    p4k_wordcloud = WordCloud().generate(wc_text)\n    wordcloud = WordCloud(max_font_size=40).generate(wc_text)\n    plt.figure()\n    plt.title(\"word cloud for \" + g)\n    plt.imshow(wordcloud, interpolation=\"bilinear\")","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"3152a58e26ca15b89cff0470672a67a43f87dd82"},"cell_type":"markdown","source":"There is still significant overlap, let's compute the differences between genres."},{"metadata":{"trusted":false,"_uuid":"64fa7b240ef5e5a5afc50d33285355ea53e55cb6"},"cell_type":"code","source":"#Compute differences between word clouds\n\n#Just pick a few categories so jupyter doesn't cry about too many plots\nfor g in it.combinations(df_p4k['genre'].cat.categories[0:3], 2):\n    \n    if g[0] == g[1]:\n        continue\n    \n    #Create genre 1 word count\n    wc_text = \" \".join(df_p4k[df_p4k['genre'] == g[0]]['review'].head(100).as_matrix().astype('str')).lower()\n    top_words = stripNonAlphaNum(wc_text)\n    top_words = filter(lambda x: not regex.match(x), top_words)\n    top_words = collections.Counter(top_words)\n    \n    df_top_words = pd.DataFrame(list(top_words.items()),\n                                columns = ['name', 'count1'])\n    \n    #Create genre 2 word count\n    wc_text = \" \".join(df_p4k[df_p4k['genre'] == g[1]]['review'].head(100).as_matrix().astype('str')).lower()\n    top_words2 = stripNonAlphaNum(wc_text)\n    top_words2 = filter(lambda x: not regex.match(x), top_words2)\n    top_words2 = collections.Counter(top_words2)\n    \n    df_top_words2 = pd.DataFrame(list(top_words2.items()),\n                                columns = ['name', 'count2'])\n    \n    #Full join \n    df_compare = df_top_words.merge(df_top_words2,\n                                    how = \"outer\",\n                                    on = \"name\")\n    df_compare['count1'] = df_compare['count1'].fillna(0)\n    df_compare['count2'] = df_compare['count2'].fillna(0)\n    \n    df_compare['diff1'] = df_compare['count1'] - df_compare['count2']\n    df_compare['diff2'] = df_compare['count2'] - df_compare['count1']\n    \n    genre1_text = \" \".join(df_compare[df_compare['diff1'] > 0]['name'])\n    genre2_text = \" \".join(df_compare[df_compare['diff2'] > 0]['name'])\n    \n    WordCloud().generate(genre1_text)\n    wordcloud = WordCloud(max_font_size=40).generate(genre1_text)\n    plt.figure()\n    plt.title(\"word cloud for words appearing more in \\n\" + g[0] + \" reviews than in \" + g[1] + \" reviews.\")\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    \n    WordCloud().generate(genre2_text)\n    wordcloud = WordCloud(max_font_size=40).generate(genre2_text)\n    plt.figure()\n    plt.title(\"word cloud for words appearing more in \\n\" + g[1] + \" reviews than in \" + g[0] + \" reviews.\")\n    plt.imshow(wordcloud, interpolation=\"bilinear\")","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"071d60b11252013ae791f1b25e711c5c593c4dc8"},"cell_type":"markdown","source":"Let's try a different kind of visualisation"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"1187f725c5c1402454be8b3f88236c5824116d4f"},"cell_type":"code","source":"df_plot_data = pd.DataFrame([],\n                            columns=[''])\n\nfor g in it.combinations(df_p4k['genre'].cat.categories, 2):\n    \n    if g[0] == g[1]:\n        continue\n    \n    #Create genre 1 word count\n    wc_text = \" \".join(df_p4k[df_p4k['genre'] == g[0]]['review'].head(100).as_matrix().astype('str')).lower()\n    top_words = stripNonAlphaNum(wc_text)\n    top_words = filter(lambda x: not regex.match(x), top_words)\n    top_words = collections.Counter(top_words)\n    \n    df_top_words = pd.DataFrame(list(top_words.items()),\n                                columns = ['name', 'count1'])\n    \n    #Create genre 2 word count\n    wc_text = \" \".join(df_p4k[df_p4k['genre'] == g[1]]['review'].head(100).as_matrix().astype('str')).lower()\n    top_words2 = stripNonAlphaNum(wc_text)\n    top_words2 = filter(lambda x: not regex.match(x), top_words2)\n    top_words2 = collections.Counter(top_words2)\n    \n    df_top_words2 = pd.DataFrame(list(top_words2.items()),\n                                columns = ['name', 'count2'])\n    \n    #Full join \n    df_compare = df_top_words.merge(df_top_words2,\n                                    how = \"outer\",\n                                    on = \"name\")\n    df_compare['count1'] = df_compare['count1'].fillna(0)\n    df_compare['count2'] = df_compare['count2'].fillna(0)\n    \n    df_compare['genre1'] = g[0]\n    df_compare['genre2'] = g[1]\n    \n    df_compare['diff1'] = df_compare['count1'] - df_compare['count2']\n    df_compare['diff2'] = df_compare['count2'] - df_compare['count1']\n    \n    df_plot_data = df_plot_data.append(df_compare)\n    \n#Drop empty col\ndf_plot_data = df_plot_data.drop('', axis = 1)\n\n#Find top 5 positive differences for each genre\ndf_plot_data2 = df_plot_data.sort_values(by = [\"genre1\", \"genre2\", \"diff1\"], ascending = [True, True, False]).groupby(by = [\"genre1\", 'genre2']).head(5)\ndf_plot_data2['name'] = pd.Categorical(df_plot_data2['name'], categories=df_plot_data2['name'].unique())\n\n","execution_count":13,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"12dbe0b392636ec4ddc555fcebbb7daf687c681a"},"cell_type":"code","source":"\n# #Plot the top word differences for each genre\n# #Faceting in p9 doesn't seem to work with scales free categorical scales\n# (p9.ggplot(data=df_plot_data,\n#           mapping=p9.aes(x = \"name\", y = \"diff1\")) +\n#  p9.facet_wrap(facets = \"~ genre1\", scales='free_y') + \n#  p9.geom_col())\n\n#Instead we can just plot each genre combination\nfor i in df_plot_data2[['genre1', 'genre2']].drop_duplicates().head(1).itertuples():\n    # (df_plot_data['genre1'] == i[1]) and (df_plot_data['genre1'] == i[2])\n    tmp_df = df_plot_data2.loc[lambda df: (df['genre1'] == i[1]) & (df['genre2'] == i[2])]\n    \n    tmp_df['name'] = pd.Categorical(tmp_df['name'], categories = tmp_df['name'].unique())\n    tmp_df = tmp_df.sort_values(by = [\"genre1\", \"genre2\", \"diff1\"], ascending = [True, True, False]).groupby(by = [\"genre1\", 'genre2']).head(5)\n    \n    # print(tmp_df)\n    # \n    print(p9.ggplot(data = tmp_df) +\n    p9.geom_col(p9.aes(x = 'name', y = 'diff1')) +\n          p9.labs(title = 'The 5 words that have the highest difference in utilisation \\n between ' + i[1] + ' and ' + i[2] + ' reviews.',\n                  x = \"Word\",\n                  y = \"Difference in # times utilised\"))\n","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"18f3bd1a7b16837cfbab549142f8e4d5804377db"},"cell_type":"markdown","source":"That was not very good. We tend to get the same words for each genre. Instead we could take the top difference for each word."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d0a75a14a8bddb6b06823033a90a661c4e4ce5d7"},"cell_type":"code","source":"tmp_plot_data = df_plot_data.sort_values(by = ['name', 'diff1'], ascending= [True, False]).groupby('name').head(1).sort_values('diff1', ascending=False).head(20)\ntmp_plot_data['name'] = pd.Categorical(tmp_plot_data['name'], categories=tmp_plot_data['name'].unique())\n\n(p9.ggplot(data = tmp_plot_data) +\n p9.geom_col(p9.aes(x = \"name\",\n                 y = \"diff1\",\n                 fill = 'genre1',\n                colour = 'genre2'))+\n p9.theme(axis_text_x = p9.element_text(rotation=90)))","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"2889c28cbf1bc2eaa89491fd7e8e5fe7b09e35e1"},"cell_type":"markdown","source":"That is still pretty shitty. How about a genre x genre grid with geom_tile representing the biggest difference between each genre"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ca936a28464c1566ecb063406a535d441033c274"},"cell_type":"code","source":"# tmp_plot_data = df_plot_data.sort_values('diff1', ascending=False).groupby(['genre1', 'genre2']).head(1).sort_values(['genre1', 'genre2'], ascending=False)\ntmp_plot_data = df_plot_data.sort_values('diff1', ascending=False).groupby('name').head(1).groupby(['genre1', 'genre2']).head(1).sort_values(['genre1', 'genre2'], ascending=False)\n\ntmp_plot_data['genre1'] = pd.Categorical(tmp_plot_data['genre1'], categories= tmp_plot_data['genre1'].unique())\ntmp_plot_data['genre2'] = pd.Categorical(tmp_plot_data['genre2'], categories= tmp_plot_data['genre1'].unique())\n#Careful, if you have NaN category values these will get plotted at the first level of the category.\ntmp_plot_data = tmp_plot_data[tmp_plot_data['genre1'].notnull()]\ntmp_plot_data = tmp_plot_data[tmp_plot_data['genre2'].notnull()]\n\n#Can mirror it, but I think it's clearer without the mirror\n# tmp_plot_data2 = pd.DataFrame.copy(tmp_plot_data)\n# # tmp_plot_data2 = tmp_plot_data2[tmp_plot_data2['genre2'] != 'Electronic']\n# # tmp_plot_data2 = tmp_plot_data2[tmp_plot_data2['genre1'] != 'Electronic']\n# tmp_plot_data2['tmp_genre'] = tmp_plot_data2['genre1']\n# tmp_plot_data2['genre1'] = tmp_plot_data2['genre2']\n# tmp_plot_data2['genre2'] = tmp_plot_data2['tmp_genre']\n# tmp_plot_data2['diff1'] = abs(tmp_plot_data2['diff2'])\n# tmp_plot_data2 = tmp_plot_data2.drop(\"tmp_genre\", axis = 1)\n# tmp_plot_data2 = tmp_plot_data2[tmp_plot_data2['genre1'].notnull()]\n# tmp_plot_data2 = tmp_plot_data2[tmp_plot_data2['genre2'].notnull()]\n# \n# \n# tmp_plot_data = tmp_plot_data.append(tmp_plot_data2)\n\n(p9.ggplot(data = tmp_plot_data) + \np9.geom_text(p9.aes(x = \"genre2\",\n                     y = 'genre1',\n                     label = 'name',\n                     color = 'diff1'))+\n p9.labs(title = 'largest differences in word frequency between genres 1 & 2 \\n note: each word can only appear once'))","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"7284bf7d04d8005227136954773e5dac1429639a"},"cell_type":"markdown","source":"Look at how sentiment score of the review correlates to actual score"},{"metadata":{"trusted":false,"_uuid":"969ce1f5ed9aaac1ef95d0385a3279825c6298c2"},"cell_type":"code","source":"df_review_sent_agg = pd.DataFrame()\n\nanalyzer = SentimentIntensityAnalyzer()\n\nuninteretable_reviews = int(0)\n\n#Look out there is a missing review\n# df_p4k.iloc[13300]\n\n#Look out there is a garbage review\n# df_p4k.iloc[17166]\n\nfor index, row in df_p4k.iterrows():\n    \n    #Skip missing rows / where the review is not a string\n    if not isinstance(row['review'], str):\n        uninteretable_reviews += 1\n        continue\n    \n    \n    token = sent_tokenize(row['review'])\n    scores = list([[\"compound\",\n                  \"neg\",\n                  \"neu\",\n                  \"pos\",\n                   \"album\",\n                   \"artist\"]])\n    \n    #Skip reviews where no sentences can be tokenized from the review str\n    if len(token) == 0:\n        uninteretable_reviews += 1\n        continue\n    \n    for j in token:\n        #Create sentiment score\n        score = analyzer.polarity_scores(j)\n        \n        #Return list of dict values sorted by key (same order as above)\n        score = sorted(list(score.items()), key = lambda x: x[1])\n        score = [y[1] for y in score]\n        \n        #Add static columns of album/artist\n        score = list(score) + [row['album'], row['artist']]\n        scores.append(score)\n    \n    #Create data frame of scores and summarise the mean sentiment\n    df_sents = pd.DataFrame(scores, columns = scores.pop(0))\n    df_sents = df_sents.groupby(['album', 'artist']).aggregate(np.mean)\n    #Need to reset index after grouped operation. This is like having to ungroup() in dplyr syntax\n    df_sents = df_sents.reset_index()\n    \n    #If you don't ungroup previously the grouped columns they will appear in the DF but not in its columns attribute\n    df_review_sent_agg = df_review_sent_agg.append(df_sents)\n    \n#Print the number of missing/unusable reviews\nif uninteretable_reviews > 0:\n    print(str(uninteretable_reviews) + ' reviews could not be parsed.')\n    \n#Join back the other fields\ndf_review_sent_agg = df_review_sent_agg.merge(right = df_p4k, \n                                              how = 'inner',\n                                              on = ['album', 'artist'])","execution_count":209,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b7fdbcd6c17e7faa12f91e950cb923ee5532a357"},"cell_type":"code","source":"#Check the results\ndf_review_sent_agg","execution_count":210,"outputs":[]},{"metadata":{"_uuid":"f554015a72a88aa3e810a40fa8efc9e6d6c1a9d4"},"cell_type":"markdown","source":"Now let's try and model the review score on the aggregated sentiment score (and genre I guess)"},{"metadata":{"trusted":false,"_uuid":"161dd3a38345a46a5a945333998dddde83955cee"},"cell_type":"code","source":"#Does this look like this is going to work well\ndf_review_sent_agg.sort_values('pos', ascending= False).head(5)\n\ndf_review_sent_agg.sort_values('neg', ascending= False).head(5)\n\ndf_review_sent_agg.sort_values('neu', ascending= False).head(5)\n\ndf_review_sent_agg.sort_values('compound', ascending= False).head(5)\n\n#Probably not. Goddamn hipster reviewers.","execution_count":211,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6d8f302c5374f15869eb7914128f7512b0a69112"},"cell_type":"code","source":"#One-hot encode the genre variables\n\n#First you have to enumerate the string var\nlabel_encode = LabelEncoder()\nlabel_encode = label_encode.fit(df_review_sent_agg.genre)\n\ndf_review_sent_agg['genre_int'] = label_encode.transform(df_review_sent_agg.genre)\n\n#Then one hot encode the genre category enumeration\nonehot_encode = OneHotEncoder(sparse = False)\nencoded_genre = onehot_encode.fit_transform(df_review_sent_agg.genre_int.values.reshape(-1, 1))\nencoded_genre = pd.DataFrame(encoded_genre)\nencoded_genre.columns = label_encode.classes_\n\n#Column bind the encoded variables\ndf_review_sent_agg = pd.concat([df_review_sent_agg, encoded_genre], axis = 1)\n\n#Convert date to ordinal\ndf_review_sent_agg['date'] = pd.to_datetime(df_review_sent_agg['date'])\ndf_review_sent_agg['date_int'] = df_review_sent_agg['date'].apply(lambda x: x.toordinal())\n\n#Define split\ndf_train, df_test, response_train, response_test = train_test_split(df_review_sent_agg,\n                                                                    df_review_sent_agg['score'],\n                                                                    test_size=0.25,\n                                                                    random_state=0)\n\n#select feature columns\ndf_train = df_train.iloc[:, np.r_[2:6, 12:22]]\ndf_test = df_test.iloc[:, np.r_[2:6, 12:22]]\n\n\n#Fit linear model\nlm = LinearRegression()\nlm.fit(df_train, response_train)","execution_count":229,"outputs":[]},{"metadata":{"_uuid":"6824efa43e09e511806ca6b97c47c2dcc6e7608d"},"cell_type":"markdown","source":"Evaluate the resuls."},{"metadata":{"trusted":true,"_uuid":"7719afc29ee710303900222d58b8fa5d99d2e453"},"cell_type":"code","source":"#Predict the test set scores\ntest_predictions = lm.predict(df_test)\n\n#Find the RMSE of the predictions\nrmse = sqrt(mean_squared_error(response_test, test_predictions))\nrmse\n\n#Find the SMAPE of the predictions\nsmape = abs(test_predictions - response_test) / (abs(test_predictions) - abs(response_test) / 2)\nsmape = np.sum(smape) / len(test_predictions)\nsmape\n\n#Create a data frame and plot the error dist\ndf_lm_eval = pd.DataFrame.from_dict({\"actual\": response_test,\n                       \"predicted\": test_predictions})\n\ndf_lm_eval['error'] = df_lm_eval['predicted'] - df_lm_eval['actual']\n\n(p9.ggplot(data=df_lm_eval) +\n p9.geom_density(mapping = p9.aes(x = 'error')))\n","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"fef4cc7a61e777425759ef04c220db5f958059b6"},"cell_type":"markdown","source":"So the results from the lm were biased and highly inaccurate. How sad."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"nbformat":4,"nbformat_minor":1}