{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Brain Tumor Detector with ResNet (using Fastai)\n\n\n\n\n0. [Introduction](#0)\n\n1.  [Preparation](#1)\n\n    1.1 [Packages](#1.1)\n    \n    1.2 [Data](#1.2)\n\n2. [Classification model](#2)","metadata":{}},{"cell_type":"markdown","source":"# 0. Introduction <a id=0></a>\n\nIn modern medicine, neuroimaging provides an essential tool for physicians to diagnose intracranial injuries or diseases, such as tumors. Computer vision/ML models can be trained to assist medics and radiologists in analyzing patients' scans and their use is becoming more and more widespread. Using fastai we show how to train a convolutional neural network which can identify the presence of a tumor in a brain scan with an accuracy of over 99%.\n\n## 1. Preparation <a id=1></a>\n\n### 1.1 Packages <a id=1.1></a>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport plotly\nimport plotly.express as px\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\n\ninit_notebook_mode(connected=True)\ncf.set_config_file(sharing='public',theme='white',offline=True)\n\n!pip install -Uqq fastbook\n\nimport fastbook\nfastbook.setup_book()\n\nfrom fastai.data.all import *\nfrom fastai.vision.all import *\nfrom fastai.text.all import *\nfrom fastbook import *\n\nwarnings.filterwarnings(action='ignore', category=UserWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-05T17:23:19.249632Z","iopub.execute_input":"2021-06-05T17:23:19.24997Z","iopub.status.idle":"2021-06-05T17:23:25.791428Z","shell.execute_reply.started":"2021-06-05T17:23:19.249931Z","shell.execute_reply":"2021-06-05T17:23:25.79036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Data <a id=1.2></a>\n\nWe merge the two following datasets, both contaning images from brain scans.\n\n1. https://www.kaggle.com/navoneel/brain-mri-images-for-brain-tumor-detection\n2. https://www.kaggle.com/preetviradiya/brian-tumor-dataset\n\nFor convenience we create a dataframe which keeps track of all the images' paths, the corresponding labels (Tumor/No tumor) and pixel sizes.","metadata":{}},{"cell_type":"code","source":"path1 = Path('../input/brian-tumor-dataset/Brain Tumor Data Set/Brain Tumor Data Set')\npath2 = Path('../input/brain-mri-images-for-brain-tumor-detection')\nim_paths = L(*get_image_files(path1), *get_image_files(path2))\n\nlbl_dct = {'no': 'No tumor', 'yes': 'Tumor', 'Healthy':'No tumor', 'Brain Tumor':'Tumor'}\n\ndf = pd.DataFrame({'image':im_paths})\ndf['px_size'] = df.apply(lambda x: PILImage.create(x.image).size, axis=1)\ndf['label'] = df.apply(lambda x: lbl_dct[parent_label(x.image)], axis=1)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:23:32.47377Z","iopub.execute_input":"2021-06-05T17:23:32.474098Z","iopub.status.idle":"2021-06-05T17:24:09.806168Z","shell.execute_reply.started":"2021-06-05T17:23:32.474063Z","shell.execute_reply":"2021-06-05T17:24:09.805353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.value_counts('label')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:24:14.339156Z","iopub.execute_input":"2021-06-05T17:24:14.339572Z","iopub.status.idle":"2021-06-05T17:24:14.350694Z","shell.execute_reply.started":"2021-06-05T17:24:14.339542Z","shell.execute_reply":"2021-06-05T17:24:14.34968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset in not perfectly balanced, with an excess of scans from partients with tumors.\n\nIn order to collate images into batches to pass to the neural network, we need all images to have the same size. We look at the sizes' distribution in order to choose the correct resize shape.","metadata":{}},{"cell_type":"code","source":"w, h = list(zip(*df['px_size'].values))\n\nsizesdf = pd.DataFrame({'width':w, 'height':h, 'label':df.label.values})\n\nfig = px.scatter(sizesdf, x='width', y='height', color='label', labels={'x':'heigth', 'y':'width'},\n           marginal_x='violin', marginal_y='violin', title='Image sizes (pixel)', height=600)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:24:19.283229Z","iopub.execute_input":"2021-06-05T17:24:19.283607Z","iopub.status.idle":"2021-06-05T17:24:20.362376Z","shell.execute_reply.started":"2021-06-05T17:24:19.283576Z","shell.execute_reply":"2021-06-05T17:24:20.36154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the images are, for the most part, approximately square-shaped. We will reshape all images to be of size (360, 360). The resizing is performed by squishing: we avoid cropping since it might miss the tumor location, adding noise to the data.\n\n## 2. Classification model <a id=2></a>\n\nLet us create the train and validation dataloaders to feed to the neural network: we do it by using the high-level API provided by fastai, which automatically performs the image preprocessing and the label numericalization.","metadata":{}},{"cell_type":"code","source":"dblock = DataBlock.from_columns(blocks=(ImageBlock, CategoryBlock),\n                               get_items=lambda x: (x.image, x.label),\n                               item_tfms=Resize(360, method='squish'),\n                               #batch normalization\n                               batch_tfms=Normalize.from_stats(*imagenet_stats)) \n\ndls = dblock.dataloaders(df)\n\ndls.show_batch(max_n=15)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:24:27.279102Z","iopub.execute_input":"2021-06-05T17:24:27.279459Z","iopub.status.idle":"2021-06-05T17:24:34.613746Z","shell.execute_reply.started":"2021-06-05T17:24:27.279429Z","shell.execute_reply":"2021-06-05T17:24:34.607738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Size of training set: {len(dls.train.items)}\")\nprint(f\"Size of validation set: {len(dls.valid.items)}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:24:38.162154Z","iopub.execute_input":"2021-06-05T17:24:38.162541Z","iopub.status.idle":"2021-06-05T17:24:38.167897Z","shell.execute_reply.started":"2021-06-05T17:24:38.162509Z","shell.execute_reply":"2021-06-05T17:24:38.166805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We train a residual neural network using the `resnet34` architecture, with cross entropy as loss function. We use *transfer learning*: for the first two epochs we train only the NN head, then we unfreeze the encoder's layers and train all weigths for 8 more epochs. This is done with the `fine_tune` method of fastai's `Learner` class.","metadata":{}},{"cell_type":"code","source":"learn = cnn_learner(dls, resnet34, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n\nlearn.model","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:24:40.442052Z","iopub.execute_input":"2021-06-05T17:24:40.442393Z","iopub.status.idle":"2021-06-05T17:24:45.164945Z","shell.execute_reply.started":"2021-06-05T17:24:40.442363Z","shell.execute_reply":"2021-06-05T17:24:45.163941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.fine_tune(8, freeze_epochs=2)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:24:49.661925Z","iopub.execute_input":"2021-06-05T17:24:49.662243Z","iopub.status.idle":"2021-06-05T17:34:25.508478Z","shell.execute_reply.started":"2021-06-05T17:24:49.662214Z","shell.execute_reply":"2021-06-05T17:34:25.507613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Without many tweakings, the approach based on convolutional neural networks has produced a model with an accuracy of 99.3% on the validation set! Let us take a look at the confusion matrix and the corresponding classification report.","metadata":{}},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\n\ninterp.plot_confusion_matrix()\n\ninterp.print_classification_report()","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:34:36.790062Z","iopub.execute_input":"2021-06-05T17:34:36.790424Z","iopub.status.idle":"2021-06-05T17:34:46.70209Z","shell.execute_reply.started":"2021-06-05T17:34:36.790385Z","shell.execute_reply":"2021-06-05T17:34:46.701055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The few errors are spread evenly among the two labels: the unbalanced dataset does not appear to have induced any bias in the model. We can look at the seven misclassified images.","metadata":{}},{"cell_type":"code","source":"interp.plot_top_losses(k=7, figsize=(15,6))","metadata":{"execution":{"iopub.status.busy":"2021-06-05T17:39:23.44338Z","iopub.execute_input":"2021-06-05T17:39:23.443736Z","iopub.status.idle":"2021-06-05T17:39:23.964252Z","shell.execute_reply.started":"2021-06-05T17:39:23.443707Z","shell.execute_reply":"2021-06-05T17:39:23.963451Z"},"trusted":true},"execution_count":null,"outputs":[]}]}