{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.preprocessing import sequence, text\nfrom keras.layers import Embedding, Dense, LSTM, GRU, Conv1D, MaxPooling1D, Flatten","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndata = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndata.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndata['sentiment'].value_counts()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text()\n\ndef clean_text(raw_text):\n    text = remove_html(raw_text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['review'] = data['review'].apply(clean_text)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['sentiment'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tokenize the words in reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"# maximum number of words to keep, based on word frequency\nvocab_size = 10000\n\ntokenizer = text.Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(data['review'])\nsequences = tokenizer.texts_to_sequences(data['review'])\nword_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# maximum length of all sequences\nmax_len = 100\n\nx = sequence.pad_sequences(sequences, maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentiments = {\n    'positive': 1,\n    'negative': 0\n}\n\ny = np.asarray(data['sentiment'].map(sentiments))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Split data****\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ntrain_samples = 40000\n\nx_train = x[:train_samples]\ny_train = y[:train_samples]\n\nx_test = x[train_samples:]\ny_test = y[train_samples:]\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load Glove embedding vector"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_glove(path):\n    \n    embedding_index = {}\n    for line in open(path):\n        values = line.split()\n        word = values[0]\n        coeff = np.asarray(values[1:], dtype='float32')\n        embedding_index[word] = coeff\n    \n    return embedding_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nembedding_index = load_glove('../input/datasettxt/glove.6B.100d.txt')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # Prepare embedding matrix\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 100\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, index in word_index.items():\n    if index < vocab_size:\n        vector = embedding_index.get(word);\n        if vector is not None:\n            embedding_matrix[index] = embedding_index.get(word)\n\nprint('Shape of embedding matrix:', embedding_matrix.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_LSTM_model(units = 32, dropout = 0):\n    \n    model = Sequential([\n        Embedding(vocab_size, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False),\n        LSTM(units, dropout=dropout),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n    \n    return model\n\n\ndef get_GRU_model(units = 32, dropout = 0):\n    \n    model = Sequential([\n        Embedding(vocab_size, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False),\n        GRU(units, dropout=dropout),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n    \n    return model\n\n\ndef get_CNN_model(filters = 32, filter_size = 7, pool_size = 5):\n    \n    model = Sequential([\n        Embedding(vocab_size, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False),\n        Conv1D(filters, filter_size, activation='relu'),\n        MaxPooling1D(pool_size),\n        Flatten(),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LSTM_model = get_LSTM_model()\nLSTM_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GRU_model = get_GRU_model()\nGRU_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nCNN_model = get_CNN_model()\nCNN_model.summary()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 5\nbatch_size = 32\nval_split = 0.2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LSTM_history = LSTM_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=val_split)\nLSTM_model.save('LSTM_imdb_sentiment_analysis.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GRU_history = GRU_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=val_split)\nGRU_model.save('GRU_imdb_sentiment_analysis.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CNN_history = CNN_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=val_split)\nCNN_model.save('CNN_imdb_sentiment_analysis.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graph(history, title = 'accuracy and loss graphs'):\n    \n    acc_values = history.history['acc']\n    val_acc_values = history.history['val_acc']\n\n    loss_values = history.history['loss']\n    val_loss_values = history.history['val_loss']\n\n    epochs_range = range(1, epochs + 1)\n\n    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n    fig.suptitle(title)\n    \n    ax[0].plot(epochs_range, acc_values, label='Training accuracy')\n    ax[0].plot(epochs_range, val_acc_values, label='Validation accuracy')\n    ax[0].set(xlabel='Epochs', ylabel='Accuracy')\n    ax[0].legend()\n    ax[0].set_title('Accuracy')\n\n    ax[1].plot(epochs_range, loss_values, label='Training loss')\n    ax[1].plot(epochs_range, val_loss_values, label='Validation loss')\n    ax[1].set(xlabel='Epochs', ylabel='Loss')\n    ax[1].legend()\n    ax[1].set_title('Loss')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nplot_graph(LSTM_history, 'LSTM Model')\nplot_graph(GRU_history, 'GRU Model')\nplot_graph(CNN_history, 'CNN Model')\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_model(model):\n    scores = model.evaluate(x_test, y_test)\n    print('Loss: {}'.format(scores[0]))\n    print('Accuracy: {}'.format(scores[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('LSTM Model')\ntest_model(LSTM_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('GRU Model')\ntest_model(GRU_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('CNN Model')\ntest_model(CNN_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}