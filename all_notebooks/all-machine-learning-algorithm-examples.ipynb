{"cells":[{"metadata":{"_uuid":"f562a12518e22bf3b12c2fcd38726abadbf7cc9e"},"cell_type":"markdown","source":"In this kernell, we will see some of the machine learning algorithms and their performances."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/StudentsPerformance.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9824793da1c55d537442b6bb46523b76616f2cfe"},"cell_type":"code","source":"df.info() ##Â Lets see what we have in the data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46eb94d2959dcc62ab69bf467ccc12a12b0a2b28"},"cell_type":"markdown","source":"We can start with logistic regression."},{"metadata":{"trusted":true,"_uuid":"bf4802789e4735b5058ef901ff4bcb3e6cafe42b"},"cell_type":"code","source":"df.gender=[1 if each==\"male\" else 0for each in df.gender]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ce90304afea09c0ac6e81f430dc35a7b4718ee0"},"cell_type":"code","source":"needed_data=df.drop([\"race/ethnicity\",\"parental level of education\"\n                    ,\"lunch\", \"test preparation course\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0cb3838e3c26729e9a2629dda2bb5fb8c2d76c9"},"cell_type":"code","source":"needed_data.head() # This one includes features which we want.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8753ff02a43f5f3e28d0318cfc9c0f3a9d39d0be"},"cell_type":"code","source":"\nmale=needed_data[needed_data.gender==1]\nfemale=needed_data[needed_data.gender==0]\n#plt.plot(female[\"math score\"])\n#plt.show()\n\nplt.plot(female[\"math score\"],color=\"red\")\nplt.plot(male[\"math score\"],color=\"blue\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef5eb9ce4d492b3ddeb58fdeddc8367316295bf8"},"cell_type":"markdown","source":"As we can see from the upper graph, it is hard to estimate values."},{"metadata":{"trusted":true,"_uuid":"fc9027a4cb8aca2e6e73cb5e4ad5356a2dde3cf1"},"cell_type":"code","source":"y=needed_data.gender.values\nx_Data=needed_data.drop([\"gender\"], axis=1)\n\nx=(x_Data-np.min(x_Data))/(np.max(x_Data)-np.min(x_Data)) ## normalization\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a8f5804e8ebb05bcec3bcd6db647105f67985a6"},"cell_type":"markdown","source":"After the normalization process we need to determine our train and test datas.\n"},{"metadata":{"trusted":true,"_uuid":"c78b49f07decde36be3a1ce96ff16779f3b75093"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train, x_test , y_train, y_test =train_test_split(x,y, test_size=0.15, random_state=42)\n\nx_train=x_train.T\nx_test=x_test.T\ny_train=y_train.T\ny_test=y_test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bef5f3c32fc653f72ae8182f6cd7680df7ae905c"},"cell_type":"code","source":"def weights_and_bias(dimension): ## function for initializing of weight and bias\n    w=np.full((dimension,1),0.01)\n    b=0.0\n    return w,b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"577783e9e0b8f83af25b92ac2e27023b72661d86"},"cell_type":"code","source":"def sigmoid (z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84d221628d75ca3af6b646c7c8811af70d4d5191"},"cell_type":"code","source":"def forwardAndBackwardProg (w,b,x_train,y_train):\n\n    z=np.dot(w.T,x_train) + b\n    y_head=sigmoid(z)\n\n    loss=-(y_train*np.log(y_head)+(1-y_train)*np.log(1-y_head))\n    cost=(np.sum(loss))/x_train.shape[1]\n\n    derivative_weight=np.dot(x_train,((y_head-y_train).T))/x_train.shape[1]\n    derivative_bias=np.sum(y_head-y_train)/x_train.shape[1]\n\n    gradients= {\"d_weight\" :derivative_weight , \"d_bias\" : derivative_bias}\n\n    return gradients,cost\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"334ca51d46cb11fee4c0c382db39d12e4eb96f52"},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate , numOfIteration):\n    index=[]\n    cost_list=[]\n    cost_list2=[]\n    \n    for i in range(numOfIteration):\n        gradients,cost=forwardAndBackwardProg (w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        w=w-learning_rate * gradients[\"d_weight\"]\n        b=b-learning_rate * gradients[\"d_bias\"]\n        if i%100==0:\n            \n            index.append(i)\n            cost_list2.append(cost)\n            print(\"Cost after iteration %i : %f\" %(i,cost))\n        \n    parameters={\"weight\" : w , \"bias\" : b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"cost\")\n    plt.title(\"Cost graph\")\n    plt.show()\n    return parameters,gradients,cost_list\n\n        \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"165bd7ee3462daa6b49eed15be049d5ddbefab04"},"cell_type":"code","source":"def predict(w,b, x_test):\n    z=sigmoid(np.dot(w.T,x_test))\n    y_prediction=np.zeros((1,x_test.shape[1]))\n    \n    for i in range (z.shape[1]):\n        if z[0,i] >=0.5:\n            y_prediction[0,i]=1\n        else:\n            y_prediction[0,i]=0\n    return y_prediction\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba3faf26eedc2a0c472a7b9761976cb4c9eb9d46"},"cell_type":"code","source":"def logistic_reg(x_train,x_test,y_train,y_test,learning_rate,numOfIteration):\n    dimension=x_train.shape[0]\n    w,b= weights_and_bias(dimension)\n    parameters,gradients,cost_list=update(w, b, x_train, y_train, learning_rate , numOfIteration) \n    y_prediction_test=predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    print(\"test accuracy:{} %\" .format(100-np.mean(np.abs(y_prediction_test-y_test))*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"ffe136f6a79889359d34e93e618261687105af60"},"cell_type":"code","source":"logistic_reg(x_train,x_test,y_train,y_test,learning_rate=1,numOfIteration=300)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f33ca1126123f2494f62e8209125eb62adac6c6e"},"cell_type":"markdown","source":"With logistic regression our test accuracy is test accuracy:58.666666666666664 % Actually this ratio is little bit low."},{"metadata":{"_uuid":"87c265a0a770595a76c0be598ada4ac0332ee4e4"},"cell_type":"markdown","source":"Also we can use KNN Algorithm for this data\n\n"},{"metadata":{"trusted":true,"_uuid":"8bf493e8adeedcf1ffc32e960458f9150f0fcba1"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test , y_train, y_test =train_test_split(x,y, test_size=0.15, random_state=42)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=3) #n_neighbor is k\nknn.fit(x_train,y_train)\nprediction=knn.predict(x_test)\nknn.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e3aaa39d16e50603bf58f687f1c33a5e2f6508a"},"cell_type":"markdown","source":"In the upper part we found 87 % accuracy with KNN.  We can also show accuracy with SVM(Support Vector Machine)\n"},{"metadata":{"trusted":true,"_uuid":"f1d51b3c8b67fc46302ee3cacb0c490a4722358d"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test , y_train, y_test =train_test_split(x,y, test_size=0.15, random_state=42)\n\nfrom sklearn.svm import SVC\nsvm=SVC(random_state=42)\nsvm.fit(x_train,y_train)\nsvm.score(x_test,y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a40464e68e83352c821d9c2432973512c8ca9b0"},"cell_type":"markdown","source":"We have found 90% with SVM, next algorithm is Naive Bayes"},{"metadata":{"trusted":true,"_uuid":"efcff0ebb453021873b9663926370bc19c2a25af"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test , y_train, y_test =train_test_split(x,y, test_size=0.15, random_state=42)\n\nfrom sklearn.naive_bayes import GaussianNB\n\nnb=GaussianNB()\nnb.fit(x_train,y_train)\nnb.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8422dc3565b4d373c2ce0359beb1a8dfe49ee72"},"cell_type":"markdown","source":"59 % accuracy with Naive Bayes classification, the other algorithm in the below is decision three classification"},{"metadata":{"trusted":true,"_uuid":"deaffcf1ced5b4cf58ca753a8b3f98795b447d40"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test , y_train, y_test =train_test_split(x,y, test_size=0.15, random_state=42)\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ndt.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dc9c7fb856928118e6cafd98823ced8f7221979"},"cell_type":"markdown","source":"78 % with Decision Three and the last algorithm is Random Forest"},{"metadata":{"trusted":true,"_uuid":"9043753b6a57a68d3ca2031139b5df6e5bd0a6e8","scrolled":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test , y_train, y_test =train_test_split(x,y, test_size=0.15, random_state=42)\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=200, random_state=42)   # 200 sub sample\nrf.fit(x_train,y_train)\nrf.score(x_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a188f274b7d8a2ca1f4514582f3c9abbf2549fb"},"cell_type":"markdown","source":"Random forest has 86% accuracy, so when we want to make a list of all classification algorithms we have:\n\n* 1. SVM = 90%\n*  2.KNN= 87%\n*  3.Random Forest=86%\n*   4.Decision Tree=80%\n*  5.Naive Bayes=59%\n*  6.Logistic Regression=58%\n"},{"metadata":{"_uuid":"41179f27439dc192906b2049d5b0b0b6ff784861"},"cell_type":"markdown","source":"Furthermore we can plot their confusion matrixes for detailed information. In the below you can find the confusion matrix of SVM.\n"},{"metadata":{"trusted":true,"_uuid":"b466f32954eb320ebd6086e392e6398653f7f58d"},"cell_type":"code","source":"y_pred=knn.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5, linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aef02b7f72dd48a5a68af471be8391e34ac45783"},"cell_type":"markdown","source":"Confusion Matrix of svm"},{"metadata":{"trusted":true,"_uuid":"d3d49f0c32ce289854f3ebf9b70d79704d06c9d4"},"cell_type":"code","source":"y_pred=svm.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5, linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b78e59f07a59b4116dbfdc98a2cd44ee17d1d4d"},"cell_type":"markdown","source":"Confusion Matrix of Naive bayes"},{"metadata":{"trusted":true,"_uuid":"fda5879190a584b072dc445603729ee765a37b7d"},"cell_type":"code","source":"y_pred=nb.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5, linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a0b8d19c379860db09787f40ec9f14d3561079a"},"cell_type":"markdown","source":"Confusion Matrix of Decision Tree"},{"metadata":{"trusted":true,"_uuid":"cc2a7e268350618d182829d6ce30c024765c662c"},"cell_type":"code","source":"y_pred=dt.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5, linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6436fa790692c065725aed78631855a778b36b72"},"cell_type":"markdown","source":"And finally, Confusion Matrix of Random Forest"},{"metadata":{"trusted":true,"_uuid":"f7983f91b74df2f804cd4049d5ed868506ecbcf2"},"cell_type":"code","source":"y_pred=rf.predict(x_test)\ny_true=y_test\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_true,y_pred)\n\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5, linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c542ae025848f60264f76bf8e9a85e8e2c885ba"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}