{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Hackerearths ML competition Reduce Marketing Waste by pycaret & catboost**","metadata":{}},{"cell_type":"markdown","source":"**Note 1:** *Every line was written by me, with methods invented by other people, with PC given by wife*\n\n**Note 2:** *Sorry for my eng*\n\n**Note 3:** *Kurt would have loved python*\n\n**Note 4:** *I started too late, so my first model's predictions were taken only. (It was 80%)\nHalf an hour was not enough for me to drop useless features and train my model again! I was upset.*","metadata":{}},{"cell_type":"code","source":"# libraries I'll need\n# for some reason pycaret make unfriendly to standard matplotlib\n!pip install matplotlib==3.1.1\n!pip install pycaret\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_theme(palette='pastel')\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom pycaret.regression import *\nfrom catboost import CatBoostRegressor\nfrom catboost import cv\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 1.** Downloading and getting familiarize with data","metadata":{}},{"cell_type":"code","source":"#downloading\ntrain_data = pd.read_csv('../input/hackerearths-reduce-marketing-waste/train.csv')\ntest_data = pd.read_csv('../input/hackerearths-reduce-marketing-waste/test.csv')\n# using the best weirds-searching method (imho)\ntrain_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#yet again\ntest_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# viewing a couple simple things\n# train_data.head()\ntest_data.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some lines below to know target column\nfor i in train_data.columns:\n    if i not in test_data.columns:\n        print('-------------------------------------\\n| Need to find:', i, '|\\n-------------------------------------')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looks like regression task\ntrain_data.Success_probability.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at target's distribution\nsns.histplot(train_data.Success_probability, kde=True, color='r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_data = train_data[(train_data.Success_probability > 40) & (train_data.\\\nSuccess_probability < 101)]\n# checking size\nlen(good_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nice\nsns.histplot(good_data.Success_probability, element='step', kde=True, color='r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# by the way, let's find all features we'll fill\nprint('Good_data:\\n')\nfor i in good_data.drop('Success_probability', axis=1).columns:\n    x = good_data[i].isna().sum()\n    if x > 0:\n        print('good_data_' + i + ' - ' + str(x))\nprint('---------------------------------\\n\\n', 'Test_data:\\n')\nfor i in test_data.columns:\n    x = test_data[i].isna().sum()\n    if x > 0:\n        print('test_data_' + i + ' - ' + str(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 2.** EDA","metadata":{}},{"cell_type":"code","source":"# looking at our data again to find hidden features\ngood_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# crating some useful lists for keeping features:\nto_drop = []\nto_get_dummies = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# notice that we have six different types of companies\ncompanies = ['ltd', 'llc', 'inc', 'group', 'plc', 'sons']\n\ndef company(s):\n    for i in range(6):\n        if s.lower().endswith(companies[i]):\n            return companies[i]\n    return 'other'\n\ngood_data.Lead_name = good_data.Lead_name.apply(company)\ntest_data.Lead_name = test_data.Lead_name.apply(company)\n\nto_get_dummies.append('Lead_name')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Contact_no feature is useless\nto_drop.append('Contact_no')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filling missing Industry values\ngood_data.Industry.loc[good_data.Industry.isna()] = 'Other'\ntest_data.Industry.loc[test_data.Industry.isna()] = 'Other'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# what do these banks need,they'd better give me a job\ndf = good_data.Industry.value_counts().to_frame()\\\n.rename(columns={'Industry' : 'Number_of_deals'})\npd.concat([df.head(), df.tail()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we have many different ways to solve \"Industry\" problem, so there are:\n# 1) making something like bank/not_bank feature, \n# 2) putting low-used values (Number_of_deals < X, e.g. X = 20) in \"Other\" value\n# 3) creating new groups (for instance IT, that would include Web, Web development, etc.)\n# 4) sending to get_dummies without any changes\n# 5) dropping Industry feature\n# but let's go by the 2nd way:\n\ndf = df.head(40)\n\ndef foo(s):\n    return s if s in df.index else 'Other'\n        \ngood_data.Industry = good_data.Industry.apply(foo)\ntest_data.Industry = test_data.Industry.apply(foo)\n\nto_get_dummies.append('Industry')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at \"Deal_value\" and \"Weighted_amount\" features\ngood_data[['Deal_value', 'Weighted_amount']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# line below gives good opportunity for data recovery, but it seems no good...\n# good_data[good_data.Deal_value.isnull()][['Deal_value', 'Weighted_amount']]\ngood_data[good_data.Weighted_amount.isnull()][['Deal_value', 'Weighted_amount']].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# making temporary dataframes\ngood_wa = good_data[(good_data.Weighted_amount\\\n.isnull() == False) & (good_data.Deal_value.isnull() == False)]\n\ntest_wa = test_data[(test_data.Weighted_amount\\\n.isnull() == False) & (test_data.Deal_value.isnull() == False)]\n\ngood_wa.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting string to numbers\ndef to_number(s):\n    return float(s[:-1])\n\ngood_wa.Deal_value = good_wa.Deal_value.apply(to_number)\ngood_wa.Weighted_amount = good_wa.Weighted_amount.apply(to_number)\n\ntest_wa.Deal_value = test_wa.Deal_value.apply(to_number)\ntest_wa.Weighted_amount = test_wa.Weighted_amount.apply(to_number)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we know that Weighted_amount ten times more \"missed\" than Deal_value but...\nsns.jointplot(data=good_wa, x='Deal_value', y='Weighted_amount', color='r')\n# its two almost mutually (linearly) dependent features!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some cells below used for both dataframes at once (it would work better with creating some funcs)\n# let's find this dependence coefficient\nthis_coef = (good_wa.Deal_value / good_wa.Weighted_amount).mean()\nthis_t_coef = (test_wa.Deal_value / test_wa.Weighted_amount).mean()\nthis_coef","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deal_value/Success_probability graph\nsns.jointplot(data=good_wa, x='Deal_value', y='Success_probability', color='r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# at first glance \"Weighted_amount\" feature gives a slightly better correlation, so...\nsns.jointplot(data=good_wa, x='Weighted_amount', y='Success_probability', color='r')\n# ...let's check","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cotcha\nsns.heatmap(good_wa[['Deal_value', 'Weighted_amount', 'Success_probability']]\\\n            .corr(), annot=True, cmap=\"YlGnBu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# $$$\nwa_list = good_data[good_data.Weighted_amount.isnull() == False].Weighted_amount.to_list()\nwa_test = test_data[test_data.Weighted_amount.isnull() == False].Weighted_amount.to_list()\nwa_list[:5]\n# making lists with available values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# before dropping \"Deal value\", we'll make some magics to fill some Weighted_amount's NaNs\n\ngood_data.Weighted_amount.loc[good_data[(good_data.Deal_value.isnull() == False) & (good_data.Weighted_amount\\\n.isnull())].index] = good_data[(good_data.Deal_value.isnull() == False) & (good_data.Weighted_amount\\\n.isnull())].Deal_value.apply(to_number) / this_coef\n\ntest_data.Weighted_amount.loc[test_data[(test_data.Deal_value.isnull() == False) & (test_data.Weighted_amount\\\n.isnull())].index] = test_data[(test_data.Deal_value.isnull() == False) & (test_data.Weighted_amount\\\n.isnull())].Deal_value.apply(to_number) / this_t_coef","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# deleting $\ndef fill_vals(s):\n    return float(s[:-1]) if s in wa_list else s\n\ngood_data.Weighted_amount = good_data.Weighted_amount.apply(fill_vals)\n\ndef fill_vals_2(s):\n    return float(s[:-1]) if s in wa_test else s\n\ntest_data.Weighted_amount = test_data.Weighted_amount.apply(fill_vals_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we need to know mean of \"Weighted_amount\" feature\nmean_wa = good_data[good_data.Weighted_amount.isnull() == False].Weighted_amount.mean()\nmean_t_wa = test_data[test_data.Weighted_amount.isnull() == False].Weighted_amount.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filling all missed values, by the way, appending useless feature to \"to_drop\" list\ndef round_(s):\n    return round(s, 2)\n\ngood_data.Weighted_amount = good_data.Weighted_amount.fillna(mean_wa).apply(round_)\ntest_data.Weighted_amount = test_data.Weighted_amount.fillna(mean_wa).apply(round_)\n\nto_drop.append('Deal_value')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# it's time to explore the time\ndef times(data):\n    data['Year_of_creation'] = pd.to_datetime(data.Date_of_creation).dt.year.apply(str)\n    data['Month_of_creation'] = pd.to_datetime(data.Date_of_creation).dt.month.apply(str)\n    \ntimes(good_data)\ntimes(test_data)\n\nprint('Train_years:', list(good_data.Year_of_creation.unique()))\nprint('Test_years:', list(test_data.Year_of_creation.unique()))\n\nto_get_dummies.append('Year_of_creation')\nto_get_dummies.append('Month_of_creation')\nto_drop.append('Date_of_creation')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# easy to convert because \"Pitch\" feature has two values only\ndef pitch_to(df):\n    \n    def to_zero(s):\n        return 0 if s == 'Product_1' else 1\n\n    df.Pitch = df.Pitch.apply(to_zero)\n\npitch_to(good_data)\npitch_to(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sending to get_dummies\nprint('Lead_revenue: ', good_data.Lead_revenue.unique())\nprint('Fund_category: ',good_data.Fund_category.unique())\n\nto_get_dummies.append('Lead_revenue')\nto_get_dummies.append('Fund_category')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('TRAIN:\\nContries are', ' and '.join(good_data.Geography\\\n.dropna().unique()), 'with', good_data.Location.nunique(), 'states')\nprint('------------------------------------------')\nprint('TEST:\\nContries are', ' and '.join(good_data.Geography\\\n.dropna().unique()), 'with', test_data.Location.nunique(), 'states')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there are some cases with missed locations,\n# however, geography features are available at the same time\ngood_data[good_data.Location.isna()][['Geography', 'Location']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check for missed countries, so Randy Ramos will send to Texas later\ntest_data[(test_data.Location.isna()) & (test_data.Geography.isna())][['Geography', 'Location', 'POC_name']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def state_USA(s):\n    \n    return s[-2:].isupper()\n\ndef state_USA2(s):\n    \n    return s[-2:]\n\nprint('TRAIN:')\n\ngood_data.Location = good_data.Location.fillna('Other')\nstates = good_data[good_data.Location.apply(state_USA)].Location.apply(state_USA2)\n\nprint(states.value_counts().to_frame().T)\nstates_list = list(states)\nstates_set = set(states)\nprint('-----------------------------\\nNumber of states in data: ' + str(states.value_counts().shape[0]))\n\nUSA_states_taken_from_WWW = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"OR\", \"DE\", \"FL\",\n                             \"GA\", \"HI\" , \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\",\n                             \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\",\n                             \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", 'PA', 'RI', 'SC',\n                             'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI',\"WY\"]\n\nprint('-----------------------------\\nNumber of states: ' + str(len(USA_states_taken_from_WWW)))\n\nfor i in USA_states_taken_from_WWW:\n    if i not in states_set:\n        print('-----------------------------\\nMissed state: '+ i)\n# and now we are choosing again, one of to ways:\n# 1) use random.choice(states_list)\n# 2) use RI instead of NaN-values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the 2nd way \ngood_data.Location.loc[good_data[good_data.Location.apply(state_USA)].index] = states\ngood_data.Location.loc[good_data[good_data.Location == 'Other'].index] = 'RI'\ngood_data[good_data.Location == 'RI']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking changes\ngood_data[good_data.Geography.isna()][['Geography', 'Location']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hoped for easter eggs, I even had been searching on U.S. Bureau's of\n# Economic Analysis site for a few hours, but...\nset_India_states = set(good_data[good_data.Location.apply(state_USA) == False].Location)\nstrange_list, x = [], []\n\nfor i in set_India_states:\n    if len(i.strip().split(' ')) > 2:\n        strange_list.append(i)\n        print(i)\n        \nfor i in strange_list:\n    set_India_states.remove(i)\n    x.append(good_data.loc[good_data[good_data.Location == i].index])\n\ndat = pd.concat(x, axis=0)\ndat\n# neither site, nor data, nor Answer to the Ultimate Question of Life,\n# the Universe, and Everything, do not show me any hint","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# good\ndef filling_missed_countries(s):\n    \n    return 'India' if s in set_India_states else 'USA'\n\ngood_data.Geography = good_data.Location.apply(filling_missed_countries)\nprint(good_data.Geography.unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# repeating with test\ntest_data[test_data.Location.isna()][['Geography', 'Location']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# repeating with test\ndef state_USA(s):\n    \n    return s[-2:].isupper()\n\ndef state_USA2(s):\n    \n    return s[-2:]\n\nprint('TEST:')\n\ntest_data.Location = test_data.Location.fillna('Other')\nstates = test_data[test_data.Location.apply(state_USA)].Location.apply(state_USA2)\n\nprint('Group by states:\\n\\n', states.value_counts().to_frame().T)\nstates_list = list(states)\nstates_set = set(states)\nprint('\\n-----------------------------\\nNumber of states in data: ' + str(states.value_counts().shape[0]))\n\nUSA_states_taken_from_WWW = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"OR\", \"DE\", \"FL\",\n                             \"GA\", \"HI\" , \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\",\n                             \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\",\n                             \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", 'PA', 'RI', 'SC',\n                             'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI',\"WY\"]\n\nprint('---------------------\\nNumber of states: ' + str(len(USA_states_taken_from_WWW)))\n\nfor i in USA_states_taken_from_WWW:\n    if i not in states_set:\n        print('-----------------\\nMissed state: '+ i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Randy Ramos at home, by the way, Texas was part of Mexico until 1836\ntest_data.Location.loc[test_data[test_data.Location.apply(state_USA)].index] = states\ntest_data.Location.loc[test_data[(test_data.\\\nLocation == 'Other') & (test_data.Geography.isna())].index] = 'TX'\ntest_data.Location.loc[test_data[test_data.Location == 'Other'].index] = 'RI'\ntest_data[test_data.Location == 'RI']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_India_states = set(test_data[test_data.Location.apply(state_USA) == False].Location)\nstrange_list, x = [], []\n\nfor i in set_India_states:\n    if len(i.strip().split(' ')) > 2:\n        strange_list.append(i)\n        print(i)\n        \n\nfor i in strange_list:\n    set_India_states.remove(i)\n    x.append(test_data.loc[test_data[test_data.Location == i].index])\n\ndat = pd.concat(x)\ndat","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def filling_missed_countries(s):\n    \n    return 'India' if s in set_India_states else 'USA'\n\ntest_data.Geography = test_data.Location.apply(filling_missed_countries)\nprint(test_data.Geography.unique())\n# phewww... next time, we'll make some function without such complexity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sorry to guys from the USA, but first seven places received by guys from India\ndef binomial_geo(s):\n    \n    return 1 if s == 'India' else 0\n\nfor data in [good_data, test_data]:\n    data['Country'] = data.Geography.apply(binomial_geo)\n\nto_drop.append('Geography')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_1 = good_data.Location.value_counts().to_frame()\\\n.rename(columns={'Location' : 'Number_of_deals'})\ndf_1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_2 = test_data.Location.value_counts().to_frame()\\\n.rename(columns={'Location' : 'Number_of_deals'})\ndf_2.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# should I drop Location feature? or...\n# should I drop Country feature? or...\n# let it be\n# but we need to find...\nto_other_1, to_other_2 = [], []\nfor i in df_1.index:\n    if i not in df_2.index:\n        to_other_1.append(i)\n\nfor i in df_2.index:\n    if i not in df_1.index:\n        to_other_2.append(i)      \nprint('Missed in test: ' + ', '.join(to_other_1) + '\\nMissed in train:' + ', '.join(to_other_2))\n# ...values to drop","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def to_oth(s):\n    return s if s in df_1.head(50).index else 'Other'\n\ntest_data.Location = test_data.Location.apply(to_oth)\ngood_data.Location = good_data.Location.apply(to_oth)\ngood_data.Location.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_get_dummies.append('Location')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting some useful (probably) features \ndef separate(s):\n    return set(map(lambda x: x.strip(), s.split('/')))\n\ngood_data.Designation = good_data.Designation.apply(separate)\ntest_data.Designation = test_data.Designation.apply(separate)\n\n\ngood_data.Designation.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# still working\nset_designation = set()\n\nfor i in good_data.Designation:\n    set_designation.update(i)\n\nfor i in test_data.Designation:\n    set_designation.update(i)\n\n# we make something like one hot engoding there    \nfor i in set_designation:\n    good_data[i] = pd.Series([1 if i in j else 0 for j in good_data\\\n                              .Designation], index=good_data.index)\n    test_data[i] = pd.Series([1 if i in j else 0 for j in test_data\\\n                              .Designation], index=test_data.index)\nto_drop.append('Designation')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sent to get_dummies\nto_get_dummies.append('Lead_source')\ngood_data.Lead_source.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lvl_of_meeting_feature looks good\nto_get_dummies.append('Level_of_meeting')\nsns.boxplot(data=good_data, x='Level_of_meeting', y='Success_probability')\n# and we see it's importance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preparing for get_dummies\ngood_data.Last_lead_update = good_data.Last_lead_update.fillna('Other_1')\ngood_data.Last_lead_update.loc[good_data[good_data.Last_lead_update == '?'].index] = 'Other_2'\n\ntest_data.Last_lead_update = test_data.Last_lead_update.fillna('Other_1')\ntest_data.Last_lead_update.loc[test_data[test_data.Last_lead_update == '?'].index] = 'Other_2'\n\nprint(good_data.Last_lead_update.unique())\nprint(test_data.Last_lead_update.unique())\nto_get_dummies.append('Last_lead_update')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we will assume that number of given cases are almost equal\ndf1 = good_data.Last_lead_update.value_counts().to_frame()\\\n.rename(columns={'Last_lead_update' : 'TRAIN'})\ndf2 = test_data.Last_lead_update.value_counts().to_frame()\\\n.rename(columns={'Last_lead_update' : 'TEST'})\ndf1.join(df2).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def llu(s):\n    k = 0\n    for i in good_data.Last_lead_update.unique():\n        k += 1\n        if i == s:\n            return k\n\n# The most strange thing is...\nsns.boxplot(data=good_data, x=good_data.Last_lead_update.apply(llu), y='Success_probability')\n# ...any position (e. g \"Did not hear back after Level 1\") is as good as others","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# good to us is Internal_POC's identity\nipsum = 0\nip = good_data.Internal_POC.unique()\nprint(len(ip), ip)\nfor i in ip:\n    if i in test_data.Internal_POC.unique():\n        ipsum += 1\nprint(ipsum)\nto_get_dummies.append('Internal_POC')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gotten lucky again\n# print(good_data.Resource.unique())\n\ngood_data.Resource = good_data.Resource.fillna('Other')\ntest_data.Resource = test_data.Resource.fillna('Other')\n\nprint(good_data.Resource.unique())\nprint(test_data.Resource.unique())\nto_get_dummies.append('Resource')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# well...\nfor data in [good_data, test_data]:\n    print(data.Internal_rating.unique())\n# ...not good... ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ...cause we have a large dependence between\n# Success_probability & Internal_rating\nsns.boxplot(data=good_data, x='Internal_rating', y='Success_probability')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def visualize(df):\n    print(data.Internal_rating.value_counts().to_frame().reset_index()\\\n    .rename(columns={'Internal_rating' : 'Amount',\n                 'index' : 'Internal_rating'}))\n\nvisualize(good_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_data[test_data.Internal_rating == -1]\\\n.shape[0] + test_data[test_data.Internal_rating == 82.34].shape[0], 'lines with missed Internal_rating :(')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there is a good one\n\ngraph_2 = sns.boxplot(data=good_data, x='Lead_revenue', y='Success_probability')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# useless feature\nto_drop.append('Lead_POC_email')\ngood_data.Lead_POC_email.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping Hiring_candidate_role because\nfor i in test_data.Hiring_candidate_role:\n    if i in good_data.Hiring_candidate_role:\n        print(i)\nto_drop.append('Hiring_candidate_role')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one more useless\nto_drop.append('POC_name')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# there are no feature with missed values, except for the \"POC_name\" feature to be deleted\ngood_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 3.** Preparing for our model","metadata":{}},{"cell_type":"code","source":"# check our to_get_dummies list, there are 8 features\nto_get_dummies","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check our to_drop list\nto_drop","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_data = pd.get_dummies(good_data, columns=to_get_dummies)\ntest_data = pd.get_dummies(test_data, columns=to_get_dummies)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking for few columns again, to drop 1 degree of freedom per one old feature\n# it makes our model performance better\ngood_data.columns.to_list()[20:40]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_too = ['Industry_Biotech/Healthcare',\n            'Lead_revenue_100 - 500 Million',\n            'Level_of_meeting_Level 1',\n            'Year_of_creation_2019',\n            'Lead_name_inc',\n            'Resource_Other',\n            'Location_PA',\n            'Internal_POC_Young,Valerie K',\n            'Last_lead_update_more than a month',\n            'Lead_source_Others',\n            'Fund_category_Category 4',\n            'Month_of_creation_12'\n           ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_data = good_data.drop(to_drop + drop_too, axis=1)\ntest_data = test_data.drop(to_drop + drop_too, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preparing test data for using by two models, they are: \n# with \"Internal_rating\" feature\ntest_with_ir = test_data[(test_data.Internal_rating > 0) & (test_data\\\n                                                        .Internal_rating < 10)]\n\ntest_with_ir.Internal_rating = test_with_ir.Internal_rating.apply(int)\n\ntest_with_ir = pd.get_dummies(test_with_ir, columns=['Internal_rating'])\n\ntest_with_ir = test_with_ir.drop('Internal_rating_5', axis=1)\n\n# without \"Internal_rating\" feature\"\ntest_without = test_data[(test_data.Internal_rating < 0) | (test_data\\\n               .Internal_rating > 10)].drop('Internal_rating', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same\ngood_without = good_data.drop('Internal_rating', axis=1)\n\ngood_with_ir = pd.get_dummies(good_data, columns=['Internal_rating']).drop('Internal_rating_5', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_feats_1 = good_with_ir.drop(['Success_probability', 'Weighted_amount', 'Deal_title'], axis=1).columns.to_list()\ncat_feats_2 = good_without.drop(['Success_probability', 'Weighted_amount', 'Deal_title'], axis=1).columns.to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 4.** Building first model & working with it by using pycaret & catboost","metadata":{}},{"cell_type":"markdown","source":"**Note 5:** *Don't panic and don't devide your data by* **X** *and* **Y**, *pycaret can handle it himself, put name of your y-vector to \"target=\"*","metadata":{}},{"cell_type":"code","source":"# this model is giving to us opportunity to compare different models, by different metrics, but\n# we need MSE\nmodel_1 = setup(data = good_with_ir.drop('Deal_title', axis=1),\n             numeric_features=['Weighted_amount'],\n             target = 'Success_probability',\n             categorical_features=cat_feats_1,\n             ignore_low_variance=True,\n             fold = 4,\n             silent = True,\n             normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# it prefers catboost\nbest_model = compare_models(sort='MSE')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# its time to prepare our first model for catboost\ny_1 = good_with_ir.Success_probability\nX_1 = good_with_ir.drop(['Deal_title', 'Success_probability'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creation of simple model to investigate features better\n# catboost haven't MSE metric included\n# I already competed, so I'm not going to add MSE to this model, but it's possible\ncat = CatBoostRegressor(eval_metric='RMSE',\n                        n_estimators=2000,\n                       depth=5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we, actually, will not draw any graph, we only need to know which features should to be dropped\nX_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, test_size=0.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat.fit(X_train_1, y_train_1,\n        verbose=False,\n        eval_set=(X_test_1, y_test_1),\n        cat_features=cat_feats_1,\n        use_best_model=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking mean squared error\nprint('MSE:', np.sqrt(metrics.mean_squared_error(y_test_1, cat.predict(X_test_1))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table_1 = pd.DataFrame(cat.get_feature_importance(), X_train_1.columns).reset_index()\\\n.rename(columns={'index' : 'Feature',\n                0 : 'Importance'})\n\ntable_1.sort_values(by='Importance', ascending=False).head(15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_features_1 = table_1[table_1.Importance > 1.4].Feature.to_list()\ngood_features_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting by categorical and numeric\ngood_cat = good_features_1[0]\n\n#  better train data\nX_1 = X_1[good_features_1]\n#  better test data\nX_P_1 = test_with_ir[good_features_1]\n\ngood_features_1 = good_features_1[1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# competition was stopped there, so I rewrite a little, test data was completed by this moment too\n# ohhh... he apologizes again\ncat_0 = CatBoostRegressor(eval_metric='RMSE',\n                          n_estimators=1000,\n                          depth=5,\n                          cat_features=good_features_1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# setting\nparams_grid = {'learning_rate': [0.03, 0.1],\n        'depth': [4, 10],\n        'l2_leaf_reg': [3, 10, 25]}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# searching best parameters\ngrid_search_result = cat_0.grid_search(params_grid, \n                                       X=X_1, \n                                       y=y_1,\n                                       cv=5,\n                                       search_by_train_test_split=True,\n                                       verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# so they are\nparam_dict = grid_search_result['params']\nparam_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# harder better faster stronger\ncat_1 = CatBoostRegressor(eval_metric='RMSE',\n                          n_estimators=1000,\n                          **param_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fitting\ncat_1.fit(X_1,\n          y_1,\n          cat_features=good_features_1,\n          verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# results of with-Internal_rating part\nthe1st = cat_1.predict(X_P_1, verbose=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# deleting Internal_rating\ngood_features_1 = good_features_1[:-4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using pycaret again and then comparing\n# model_2 = setup(data = good_without,\n#              numeric_features=['Weighted_amount'],\n#              target = 'Success_probability',\n#              categorical_features=cat_feats_2,\n#              ignore_low_variance=True,\n#              fold = 4,\n#              silent = True,\n#              normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# it says catboost again\n# compare_models()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_features_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# so repeating\n\ncat_2 = CatBoostRegressor(eval_metric='RMSE',\n                          n_estimators=1000,\n                          **param_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_features_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_features_1.append(good_cat)\n\nX_2 = X_1[good_features_1]\nX_P_2 = test_without[good_features_1]\n\ngood_features_1 = good_features_1[:-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_2.fit(X_2,\n          y_1,\n          cat_features=good_features_1,\n          verbose=0)\n\n\nthe2nd = cat_2.predict(X_P_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_with_ir['Success_probability'] = the1st\ntest_without['Success_probability'] = the2nd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_with_ir = test_with_ir[['Deal_title', 'Success_probability']]\ntest_without = test_without[['Deal_title', 'Success_probability']]        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans = pd.concat([test_with_ir, test_without]).sort_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now let me sleep a little\nans.to_csv('sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}