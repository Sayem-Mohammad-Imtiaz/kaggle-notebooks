{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction:\nThis case study aims to give us an idea of applying EDA in a real business scenario. In this case study, we develop a basic understanding of risk analytics in banking and financial services and understand how data is used to minimise the risk of losing money while lending to customers.\n\n\n## Business Objectives:\nThis case study aims to identify patterns which indicate if a client has difficulty paying their installments which may be used for taking actions such as denying the loan, reducing the amount of loan, lending (to risky applicants) at a higher interest rate, etc. This will ensure that the consumers capable of repaying the loan are not rejected. Identification of such applicants using EDA is the aim of this case study.The company wants to understand the driving factors (or driver variables) behind loan default, i.e. the variables which are strong indicators of default. The company can utilise this knowledge for its portfolio and risk assessment.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing Libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns',150)\npd.set_option('display.max_info_columns', 150)\npd.set_option('display.max_rows',150)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing Datasets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading pplication_data  datasets\napp_data = pd.read_csv(\"../input/loan-defaulter/application_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Understanding Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display top 5 rows of app_data dataframe\napp_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing shape of application_data dataset\nprint(f'Shape of app_data : {app_data.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> app_data.info() only showing data types of columns. No info about null_values, lets use describe fn. to get some more insight","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Some columns of applition_dataset also have null values as count values are different","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Preprocessing (Cleaning and Fixing Data)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### For app_data dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Unwanted Col.\nunwanted_col=['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE',\n       'FLAG_PHONE', 'FLAG_EMAIL','REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','FLAG_EMAIL','CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT',\n       'REGION_RATING_CLIENT_W_CITY','DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3','FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6',\n       'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',\n       'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15','FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18',\n       'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21','OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE',\n        'OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE','EXT_SOURCE_2','EXT_SOURCE_3']\napp_data.drop(unwanted_col, inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Column wise Null percentage in app_data\nnull_percentage_app = round(app_data.isnull().sum()/app_data.shape[0]*100, 2)\nprint(null_percentage_app)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting list of columns which have more than or equal to 45% missing values\napp_colToDrop = list(null_percentage_app[null_percentage_app >= 45].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'No. of col. to be drop: {len(app_colToDrop)}')\napp_colToDrop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping Columns having more than 45% of missing values\napp_data.drop(app_colToDrop, axis= 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rechecking column wise Null percentage in app_data\nnull_percentage_app = round(app_data.isnull().sum()/app_data.shape[0]*100, 2)\nprint(null_percentage_app)\nprint(f'New shape of app_data: {app_data.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting columns with missing values between 0% and 45%\nmissing_value_col_app = list((null_percentage_app[null_percentage_app > 0].index))\nmissing_value_col_app","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(app_data.AMT_GOODS_PRICE.describe())\nsns.distplot(app_data.AMT_GOODS_PRICE, hist=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y = app_data.AMT_GOODS_PRICE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> AMT_GOODS_PRICE represents price of goods for which loans has been taken, which can be an important metric for identifying Defaulters. This col. shows high standard deviation and has multi-modal distribution curve, is left-skewed and contain outliers that can be seen in the boxplot . \n\n> For this col. **median is ideally** suited but it is only 0.09% so, I would recommend **removing rows with missing values**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(app_data.OCCUPATION_TYPE.value_counts())\napp_data.OCCUPATION_TYPE.value_counts().plot(kind ='bar', figsize = (10,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> OCCUPATION_TYPE is a important col. and it is a categoriacal value with around 31.35 % missing value. So, ideally we replace it with **mode or most frequent** category but in this analysis the best step will be to **remove the rows having missing values** in occupation_type. As Occupation plays major role in defaulting or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(app_data.EXT_SOURCE_3.describe())\n# sns.boxplot(y = app_data.EXT_SOURCE_3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns.distplot(app_data.EXT_SOURCE_3, hist= False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Recommendation to use **Mean** to impute missing values of EXT_SOURCE_3 as there are not ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nAmt_req_credit = list(enumerate(missing_value_col_app[-6:]))\nfor i in Amt_req_credit:\n    print('\\n'+i[1])\n    print(f'No. of unique values: {app_data[i[1]].nunique()}')\n    print(app_data[i[1]].value_counts())\n    plt.figure(figsize=(5,10))\n    plt.subplot(len(Amt_req_credit), 1, i[0]+1 )\n    plt.title(i[1])\n    app_data[i[1]].value_counts().plot(kind= 'bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Use **Mode** to impute missing value in these col. \n('AMT_REQ_CREDIT_BUREAU_HOUR',\n 'AMT_REQ_CREDIT_BUREAU_DAY',\n 'AMT_REQ_CREDIT_BUREAU_WEEK',\n 'AMT_REQ_CREDIT_BUREAU_MON',\n 'AMT_REQ_CREDIT_BUREAU_QRT',\n 'AMT_REQ_CREDIT_BUREAU_YEAR')\n as they are categorical columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Checking Data and DataTypes of the Columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### > Fixing Days_Employed Col","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data.DAYS_EMPLOYED.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> First value i.e 365243 DAYS ~= 1000.years which is not possible. So, we replace it with 0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data.DAYS_EMPLOYED.replace(365243, 0, inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data.DAYS_EMPLOYED = -1*app_data.DAYS_EMPLOYED","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data.DAYS_EMPLOYED.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking ORGANIZATION_TYPE col\napp_data.ORGANIZATION_TYPE.value_counts(normalize = True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can see that **'XNA'** value which to our knowledge represents **'Not Available'** can be also treated as invalid or missing value. This is 18% of total rows which we can **remove from our analysis**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing XNA rows from the CODE_GENDER.\napp_data.drop(app_data[app_data.ORGANIZATION_TYPE == 'XNA'].index, axis=0, inplace=True)\napp_data.ORGANIZATION_TYPE.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking ORGANIZATION_TYPE col\napp_data.CODE_GENDER.value_counts(normalize = True)*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can see that **'XNA'** value which to our knowledge represents **'Not Available'** can be also treated as invalid or missing value. This is 0.0013% of total rows which we will **remove from our analysis**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing XNA rows from the CODE_GENDER.\napp_data.drop(app_data[app_data.CODE_GENDER == 'XNA'].index, axis=0, inplace=True)\napp_data.CODE_GENDER.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining fun. to fix other col.\nOther_colToFix = ['DAYS_BIRTH','DAYS_ID_PUBLISH','DAYS_REGISTRATION']\ndef fixCol(arr):\n    for i in arr:\n        app_data[i] = -1 * app_data[i]\n        print('\\n'+i)\n        print(app_data[i].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fixCol(Other_colToFix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Casting all other columns data type to numeric data type\n\nnum_col=['TARGET',\n          'CNT_CHILDREN',\n          'AMT_INCOME_TOTAL',\n          'AMT_CREDIT','AMT_ANNUITY',\n          'REGION_POPULATION_RELATIVE','DAYS_BIRTH',\n          'DAYS_EMPLOYED',\n          'DAYS_REGISTRATION',\n          'DAYS_ID_PUBLISH',\n          'HOUR_APPR_PROCESS_START',\n          'LIVE_REGION_NOT_WORK_REGION',\n          'REG_CITY_NOT_LIVE_CITY',\n          'REG_CITY_NOT_WORK_CITY',\n          'LIVE_CITY_NOT_WORK_CITY']\n\napp_data[num_col]=app_data[num_col].apply(pd.to_numeric)\napp_data.head(5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Finding Outliers for Numerical Col","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"For_Outliers = list(enumerate(['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_GOODS_PRICE']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in For_Outliers:\n    print('\\n'+i[1])\n    print('-'*30)\n    print(app_data[i[1]].describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 1. For AMT_INCOME_TOTAL: There is lot of variation from 75% to max. So, this column is highly probale of having outliers, which can be confirmed using boxplot later.\n\n> 2. For AMT_CREDIT: There is considerable amt. of variation in different quartiles of data, but describe fn. doesn't give clear repersentation about outliers in this col.. We need to use boxplot for that.\n\n> 3. For AMT_ANNUITY : This col. has large amt. of variation in the last quartile which shows that this col. is suffering from outliers, which can be confirm by box plot later.\n\n> 4. For AMT_GOODS_PRICE : In this col. describe fn. does not give any strong information about outliers in the col.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in For_Outliers:\n    plt.figure(figsize=(10,15))\n    plt.subplot(len(For_Outliers), 1, i[0]+1)\n    sns.boxplot(app_data[i[1]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 1. For AMT_INCOME_TOTAL: This col. has outlier as we can clearly see a single point on extreme right in its respective boxplot. We can remove these.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data.AMT_CREDIT.quantile([0.5, 0.8, 0.85, 0.9, 0.95, 0.97, 0.99,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data[app_data.AMT_CREDIT > 1200000].AMT_CREDIT.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 2. For AMT_CREDIT: This col. has some outliers but they are significant for our analysis. There is large variation in 99% and max value. Here we can **Cap the outliers** to some value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data.AMT_ANNUITY.quantile([0.5, 0.8, 0.85, 0.9, 0.95, 0.97, 0.99, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data[app_data.AMT_ANNUITY > 60000].AMT_ANNUITY.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 3. For AMT_ANNUITY : For this col. majority distribution is smooth but there are certain outliers in the last 1%. The **best is to remove the extreme values and cap rest**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data.AMT_GOODS_PRICE.quantile([0.5, 0.8, 0.85, 0.9, 0.95, 0.97, 0.99, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data[app_data.AMT_GOODS_PRICE > 1800000].AMT_GOODS_PRICE.dropna().describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> 4. For AMT_GOODS_PRICE : For this col. majority distribution is smooth but there are certain outliers in the last 1%. The best is **to remove the extreme values and cap rest**.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Binning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating bins for AMT_INCOME_TOTAL\nbins = [0,100000,200000,300000,400000,500000,600000,700000,800000, 900000, 1000000, 100000000]\nslot = ['0-100000','100000-200000','200000-300000','300000-400000','400000-500000','500000-600000',\n        '600000-700000','700000-800000','800000-900000','900000-1000000', '1000000 +']\n\napp_data['AMT_INCOME_RANGE']=pd.cut(app_data['AMT_INCOME_TOTAL'], bins, labels=slot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data['AMT_INCOME_RANGE'].value_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating bins for AMT_CREDIT\n\nbins = [0,150000,200000,250000,300000,350000,400000,450000,500000,550000,600000,650000,700000,750000,800000,850000,900000,1000000000]\nslots = ['0-150000', '150000-200000','200000-250000', '250000-300000', '300000-350000', '350000-400000','400000-450000',\n        '450000-500000','500000-550000','550000-600000','600000-650000','650000-700000','700000-750000','750000-800000',\n        '800000-850000','850000-900000','900000 and above']\n\napp_data['AMT_CREDIT_RANGE']=pd.cut(app_data['AMT_CREDIT'], bins=bins, labels=slots)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"app_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Analysis ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for Imbalance dataset w.r.t. TARGET col\nvalue_Count_Target = app_data.TARGET.value_counts(normalize = True)*100\nvalue_Count_Target.plot(kind= 'bar')\nprint(value_Count_Target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> This is highly Imbalanced Dataset as approx. 92% data belong to target value of 0 and only approx. 8% belong to target value of 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting dataset w.r.t Traget == 0 & Target == 1\nTarget_0 = app_data[app_data.TARGET == 0]\nTarget_1 = app_data[app_data.TARGET == 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Target_0.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Target_1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Univariate Analysis for Categorical Variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining a function to plot the countplot for different categories\ndef UniVarCatPlot(title, hue = None, rotation=None, col_y = None, col_x = None):\n    sns.set_style('whitegrid')\n    sns.set_context('talk')\n    plt.rcParams[\"axes.labelsize\"] = 20\n    plt.rcParams['axes.titlesize'] = 30\n    plt.rcParams['axes.titlepad'] = 30\n    \n    if col_x:\n        col_name = col_x\n        plt.figure(figsize=(30,25))\n    else:\n        col_name = col_y\n        plt.figure(figsize=(15,38))\n\n    #   1st subplot for Target_1    \n    plt.subplots_adjust(hspace=0.5)\n    plt.subplot(2,1,1)\n    \n    title1 = title + ' for Target_0 (Client with NO Payment Difficulty)'\n    plt.title(title1)\n\n    #   Adjusting scale for horizonatl plot    \n    if col_x:\n        plt.yscale('log')\n        plt.xticks(rotation = rotation)\n    else:\n        plt.xscale('log')\n        plt.yticks(rotation = rotation)\n        \n    sns.countplot(data = Target_0, x = col_x, y = col_y, order=Target_0[col_name].value_counts().index, hue=hue, palette='dark')\n    \n\n    #   2nd subplot for Target_1\n    plt.subplot(2,1,2)\n    plt.xticks(rotation = rotation)\n    title2 = title + ' for Target_1 (Client with Payment Difficulty)'\n    plt.title(title2)\n\n    #   Adjusting scale for horizonatl plot\n    if col_x:\n        plt.yscale('log')\n        plt.xticks(rotation = rotation)\n    else:\n        plt.xscale('log')\n        plt.yticks(rotation = rotation)\n        \n    sns.countplot(data = Target_1, x = col_x, y= col_y, order=Target_1[col_name].value_counts().index, hue=hue, palette='dark')\n    plt.legend(loc = 'upper right', fontsize = 'large')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count plot for income range with wrt gender\nUniVarCatPlot(col_x = 'AMT_INCOME_RANGE', title= 'Count Plot for Income Range', hue='CODE_GENDER')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Observation from the count plot\n    1. More female application for credit\n    2. Majority of income range lies between 0 and 4,00,000\n    3. Less count in 9,00,000 - 10,00,000. But sudden increase in count for 10,00,000+ interval.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count Plot for contract type wrt gender\nUniVarCatPlot(col_x = 'NAME_CONTRACT_TYPE', title= 'Count Plot for Contract Type', hue='CODE_GENDER')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Target_0\n> Count  for  contract type Cash loans is singnificantly higher than Revolving loans\n> Females has higher count in this category.\n\nFor Target_1\n> Count  for  contract type Cash loans is singnificantly higher than Revolving loans\n> Females has higher count in this category, with no male in Revolving loans.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count Plot for type of education wrt gender\nUniVarCatPlot(col_x = 'NAME_EDUCATION_TYPE', title= 'Count Plot for Education Type', hue='CODE_GENDER')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">For both Target_0 and Target_1 the count plot gives similar pattern with credit for Secondary Education type counts max.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count Plot for cdifferent housing type\nUniVarCatPlot(col_x = 'NAME_HOUSING_TYPE', title= 'Count Plot for Different Housing Type', hue='CODE_GENDER')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">For both Target_0 and Target_1 the count plot gives similar pattern with credit for House/apartment housing type counts max.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count Plot for contract type wrt gender\nUniVarCatPlot(col_x = 'OCCUPATION_TYPE', title= 'Count Plot for Different Occupation Type', rotation= 45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Target_0\n>1. Laborers have highest count.\n\nFor Target_1\n>1. Similar to Target_0, LABORERS highest count.\n\nSuggesting Bank to be careful in giveing loans to top 5 occupation type i.e Laborers, Sales_Staff, Drivers, Core Staff, Managers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"UniVarCatPlot(col_y = 'ORGANIZATION_TYPE', title= 'Count Plot for Different Organization Type')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Target_0\n>1. Clients which have applied for credits are from most of the organization type ‘Business entity Type 3’ , ‘Self employed’, ‘Other’ , ‘Medicine’ and ‘Government’.\n2. Less clients are from Industry type 8,type 6, type 10, religion and trade type 5, type 4.\n\nFor Target_1\n>1. Clients which have applied for credits are from most of the organization type ‘Business entity Type 3’ , ‘Self employed’ , ‘Other’ , ‘Medicine’ and ‘Government’.\n2. Less clients are from Industry type 8,type 6, type 10, religion and trade type 5, type 4.\n3. Same as type 0 in distribution of organization type.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Correlation between variables for Target_0\ncorr_0 = Target_0.corr()\n# sns.heatmap(corr_0)\ncorr_0_df = corr_0.where(np.triu(np.ones(corr_0.shape), k=1).astype(np.bool))\ncorr_0_df = corr_0_df.unstack().reset_index()\ncorr_0_df.columns = ['Variable_1', 'Variable_2', 'Correlation']\ncorr_0_df.dropna(subset = ['Correlation'], inplace = True)\ncorr_0_df['Correlation'] = round(corr_0_df['Correlation'],2)\ncorr_0_df['Correlation'] = abs(corr_0_df['Correlation'])\ncorr_0_df.sort_values(by = 'Correlation', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Correlation between variables for Target_1\ncorr_1 = Target_1.corr()\ncorr_1_df = corr_1.where(np.triu(np.ones(corr_1.shape), k=1).astype(np.bool))\ncorr_1_df = corr_1_df.unstack().reset_index()\ncorr_1_df.columns = ['Variable_1', 'Variable_2', 'Correlation']\ncorr_1_df.dropna(subset = ['Correlation'], inplace = True)\ncorr_1_df['Correlation'] = round(corr_1_df['Correlation'],2)\ncorr_1_df['Correlation'] = abs(corr_1_df['Correlation'])\ncorr_1_df.sort_values(by = 'Correlation', ascending = False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">The highest correlation is almost same in both Target_0 and Target_1 dataframe and between same variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Matrix for Target_0\nplt.figure(figsize=(10, 8))\nplt.rcParams['axes.titlesize'] = 20\nplt.rcParams['axes.titlepad'] = 30\nplt.rcParams['xtick.labelsize']=12\nplt.rcParams['ytick.labelsize']=12\nplt.title('Correlattion of Target 0')\nsns.heatmap(corr_0.iloc[2:-6,2:-6], cmap ='RdYlGn')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Matrix for Target_1\nplt.figure(figsize=(10, 8))\nplt.rcParams['axes.titlesize'] = 20\nplt.rcParams['axes.titlepad'] = 30\nplt.rcParams['xtick.labelsize']=12\nplt.rcParams['ytick.labelsize']=12\nplt.title('Correlattion of Target 1')\nsns.heatmap(corr_1.iloc[2:-6,2:-6], cmap ='RdYlGn')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can see that both Correlation matrix for Target_0 and Target_1 are almost similar.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Univariate Analysis for Numerical Variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining a function to plot the boxplot for different numerical col.\ndef UniVarNumPlot(title, col):\n    sns.set_style('whitegrid')\n    sns.set_context('talk')\n    plt.figure(figsize=(15,6))\n    plt.rcParams[\"axes.labelsize\"] = 12\n    plt.rcParams['xtick.labelsize']=12\n    plt.rcParams['ytick.labelsize']=12\n    plt.rcParams['axes.titlesize'] = 14\n    plt.rcParams['axes.titleweight'] = 12\n    plt.rcParams['axes.titlepad'] = 30\n    \n\n    #   1st subplot for Target_1    \n    plt.subplots_adjust(wspace=1.5)\n    plt.subplot(1,2,1)\n    title1 = title + ' for Target_0'\n    plt.title(title1)\n    plt.yscale('log')    \n    sns.boxplot(data = Target_0, x = col, orient = 'v')\n    \n\n    #   2nd subplot for Target_1\n    plt.subplot(1,2,2)\n    title2 = title + ' for Target_1'\n    plt.title(title2)\n    plt.yscale('log')\n    sns.boxplot(data = Target_1, x = col, orient = 'v')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boxplot plot for Total Income between Target_0 and Target_1\nUniVarNumPlot(col ='AMT_INCOME_TOTAL', title = 'Distribution of Client Income' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Target_0 Client Income\n\n>Some outliers are noticed in income amount.\nThe third quartiles is very slim for income amount.\n\n\nFor Target_1 Client Income\n\n>Some outliers are noticed in income amount.\nThe third quartiles is very slim for income amount.\nMost of the clients of income are present in first quartile.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boxplot plot for Credit Amt between Target_0 and Target_1\nUniVarNumPlot(col ='AMT_CREDIT', title = 'Distribution of Credit Amount' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Both Target_0 and Target_1 has similar type of boxplot.\n\n> Some outliers are noticed in credit amount.\nThe first quartile is bigger than third quartile for credit amount which means most of the credits of clients are present in the first quartile.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Boxplot plot for Credit Amt between Target_0 and Target_1\nUniVarNumPlot(col ='AMT_ANNUITY', title = 'Distribution of Annuity Amount' )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> For Target_0\nSome outliers are noticed in credit amount.\nThe first quartile is bigger than third quartile for credit amount which means most of the credits of clients are present in the first quartile.\n\n> For Target_1\nSome outliers are noticed in annuity amount.\nThe first quartile is bigger than third quartile for annuity amount which means most of the annuity clients are from first quartile.\n \n> Both Target_0 and Target_1 has similar type of boxplot for Annuity amount.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Bivariate analysis for numerical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for Bi- variate Boxplot analysis\ndef BiVarPlot(data, col_x, col_y, hue, title, scale=None):\n    plt.rcParams[\"axes.labelsize\"] = 12\n    plt.rcParams[\"axes.labelpad\"] = 12\n    plt.rcParams['xtick.labelsize']=12\n    plt.rcParams['ytick.labelsize']=12\n    plt.rcParams['axes.titlesize'] = 14\n    plt.rcParams['axes.titleweight'] = 12\n    plt.rcParams['axes.titlepad'] = 30\n    plt.figure(figsize=(16,12))\n    plt.xticks(rotation=0)\n    if scale:\n        plt.yscale(scale)\n    sns.boxplot(data = data, x= col_x, y= col_y, hue= hue, orient='v')\n    plt.title(title)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plotting for Credit amount for Target_0\nBiVarPlot(data = Target_0,\n          col_x='NAME_EDUCATION_TYPE',\n          col_y='AMT_CREDIT',\n          hue='NAME_FAMILY_STATUS',\n          title='Credit amount vs Education Status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> From the above box plot we can say that Family status of 'civil marriage', 'marriage' and 'separated' of Academic degree education are having higher number of credits than others.\n\n> Most of the outliers are from Education type 'Higher education' and 'Secondary'. \n\n> Civil marriage for Academic degree is having most of the credits in the third quartile.\n\n>These all above mentioned categories of people have no difficulty in paying back the loan.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plotting for Credit amount for Target_1\nBiVarPlot(data = Target_1,\n          col_x='NAME_EDUCATION_TYPE',\n          col_y='AMT_CREDIT',\n          hue='NAME_FAMILY_STATUS',\n          title='Credit amount vs Education Status')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Married people with Academic degree has highest Credit amount than all other education type. And other family status has neligible credit. This shows that Married people with Academic degree are having more difficulties in paying back the loan.\n\n> While  most of the outliers are Secondary/Secodary special.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plotting for Income amount in logarithmic scale for Target_0\nBiVarPlot(data = Target_0,\n          col_x='NAME_EDUCATION_TYPE',\n          col_y='AMT_INCOME_TOTAL',\n          hue='NAME_FAMILY_STATUS',\n          title='Income amount vs Education Status',\n          scale='log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">From above boxplot for Education type 'Higher education' the income amount is mostly equal with family status same goes for 'Secondary Education'.\n\n>Less outlier are having for Academic degree but there income amount is little higher that Higher education. Lower secondary are have less income amount than others.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plotting for Income amount in logarithmic scale for Target_1\nBiVarPlot(data = Target_1,\n          col_x='NAME_EDUCATION_TYPE',\n          col_y='AMT_INCOME_TOTAL',\n          hue='NAME_FAMILY_STATUS',\n          title='Income amount vs Education Status',\n          scale='log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Read previous_application data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading previous_application and application_data  datasets\nprev_application = pd.read_csv('../input/loan-defaulter/previous_application.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display top 5 rows of prev_application dataframe\nprev_application.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing shape of both dataset\nprint(f'Shape of prev_application : {prev_application.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_application.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_application.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Column wise Null percentage in prev_application\nnull_percentage_prev = round(prev_application.isnull().sum()/prev_application.shape[0]*100, 2)\nprint(null_percentage_prev)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">Looks like there are some missing values present in some columns of the previous_application dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the list of columns with more than 45% of null values \nprev_colToDrop = list(null_percentage_prev[null_percentage_prev >= 35].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'No. of col. to be drop: {len(prev_colToDrop)}')\nprev_colToDrop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping the columns from prev_application dataframe\nprev_application.drop(prev_colToDrop, axis= 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rechecking column wise Null percentage in prev_application\nnull_percentage_prev = round(prev_application.isnull().sum()/prev_application.shape[0]*100, 2)\nprint(null_percentage_prev)\nprint(f'New shape of prev_application: {prev_application.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting columns with missing values between 0% and 35%\nmissing_value_col_prev = list(null_percentage_prev[null_percentage_prev > 0].index)\nmissing_value_col_prev","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(prev_application.NAME_CASH_LOAN_PURPOSE.value_counts()/prev_application.shape[0]*100,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> there are majority of value as 'XAN' and 'XAP' combined account to 96% of rows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping rows conating 'XNA' and 'XAP'\nprev_application=prev_application.drop(prev_application[prev_application['NAME_CASH_LOAN_PURPOSE']=='XNA'].index)\nprev_application=prev_application.drop(prev_application[prev_application['NAME_CASH_LOAN_PURPOSE']=='XAP'].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prev_application.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging prev_application and app_data on SK_ID_CURR\nmerged_df = pd.merge(left = app_data, right=prev_application, how='inner', on='SK_ID_CURR', suffixes='_o')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the col\nmerged_df = merged_df.rename({'NAME_CONTRACT_TYPE_' : 'NAME_CONTRACT_TYPE',\n                         'AMT_CREDIT_':'AMT_CREDIT',\n                         'AMT_ANNUITY_':'AMT_ANNUITY',\n                         'WEEKDAY_APPR_PROCESS_START_' : 'WEEKDAY_APPR_PROCESS_START',\n                         'HOUR_APPR_PROCESS_START_':'HOUR_APPR_PROCESS_START',\n                         'NAME_CONTRACT_TYPEo':'NAME_CONTRACT_TYPE_PREV',\n                         'AMT_CREDITo':'AMT_CREDIT_PREV',\n                         'AMT_ANNUITYo':'AMT_ANNUITY_PREV',\n                         'WEEKDAY_APPR_PROCESS_STARTo':'WEEKDAY_APPR_PROCESS_START_PREV',\n                         'HOUR_APPR_PROCESS_STARTo':'HOUR_APPR_PROCESS_START_PREV'}, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing Unwanted columns\nmerged_df.drop(['WEEKDAY_APPR_PROCESS_START',\n              'HOUR_APPR_PROCESS_START',\n              'REG_REGION_NOT_LIVE_REGION', \n              'REG_REGION_NOT_WORK_REGION',\n              'LIVE_REGION_NOT_WORK_REGION',\n              'REG_CITY_NOT_LIVE_CITY',\n              'REG_CITY_NOT_WORK_CITY', \n              'LIVE_CITY_NOT_WORK_CITY',\n              'WEEKDAY_APPR_PROCESS_START_PREV',\n              'HOUR_APPR_PROCESS_START_PREV', \n              'FLAG_LAST_APPL_PER_CONTRACT',\n              'NFLAG_LAST_APPL_IN_DAY',\n              'NAME_GOODS_CATEGORY',\n              'SELLERPLACE_AREA',\n              'NAME_SELLER_INDUSTRY'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for Univariate Analysis on merged_df using count plot\ndef UniVariatePlot(dataframe, col_y, hue, title):\n    sns.set_style('whitegrid')\n    sns.set_context('talk')\n\n    plt.figure(figsize=(15,25))\n    plt.rcParams[\"axes.labelsize\"] = 20\n    plt.rcParams['axes.titlesize'] = 22\n    plt.rcParams['axes.titlepad'] = 30\n    \n    plt.xscale('log')\n    plt.title(title)\n    sns.countplot(data = merged_df,\n                  y= col_y,\n                  order=merged_df[col_y].value_counts().index,\n                  hue = hue,\n                  palette='dark')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of contract status in logarithmic scale\nUniVariatePlot(dataframe=merged_df,\n               col_y = 'NAME_CASH_LOAN_PURPOSE', \n               hue='NAME_CONTRACT_STATUS', \n               title= 'Distribution of contract status with purposes')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Most rejection of loans came from purpose 'repairs'.\n\n> For education purposes we have equal number of approves and rejection.\n\n> Payign other loans and buying a new car is having significant higher rejection than approves.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of Target\nUniVariatePlot(dataframe=merged_df, \n               col_y='NAME_CASH_LOAN_PURPOSE', \n               hue='TARGET', \n               title='Distribution occupation type with Target.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Loan purposes with 'Repairs' are facing more difficulites in payment on time.\n\n> There are few places where client with no loan payment difficulty is significant higher than client facing difficulties in payment. They are 'Buying a garage', 'Business developemt', 'Buying land','Buying a new car' and 'Education' Hence we can focus on these purposes for which the client is having for minimal payment difficulties.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"UniVariatePlot(dataframe=merged_df, \n               col_y='OCCUPATION_TYPE', \n               hue='NAME_CONTRACT_STATUS', \n               title='Distribution occupation type with Target.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Refused contract status are more than Approved in every occupation type.\n\n> Labours have highest count while IT staff has least count for credit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"UniVariatePlot(dataframe=merged_df, \n               col_y='OCCUPATION_TYPE', \n               hue='TARGET', \n               title='Distribution occupation type with Target.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> All occupation type has higher count for target_0 which is no difficultly in payment of loan.\n\n> IT Staff has significant +ve difference between target_0 and target_1, which represent IT Staff are Safer occupation type to give loan.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Performing Bivariate Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for Bivariate Analysis on merged_df using boxplot\ndef BiVariatePlot(dataframe, col_x, col_y, hue, title):\n    sns.set_style('whitegrid')\n    sns.set_context('talk')\n\n    plt.figure(figsize=(16,12))\n    plt.rcParams[\"axes.labelsize\"] = 20\n    plt.rcParams['axes.titlesize'] = 22\n    plt.rcParams['axes.titlepad'] = 30\n    \n    plt.yscale('log')\n    plt.title(title)\n    plt.xticks(rotation=90)\n    sns.boxplot(data = merged_df,\n                x = col_x,\n                y= col_y,\n                hue = hue,\n                orient='v')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plotting for Credit amount in logarithmic scale\nBiVariatePlot(dataframe=merged_df,\n              col_x='NAME_CASH_LOAN_PURPOSE',\n              col_y='AMT_CREDIT_PREV',\n              hue='NAME_INCOME_TYPE',\n              title='Prev Credit amount vs Loan Purpose')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Loan for 'Buying a new car, Buying a holiday Home/Land, Buying a Houe or an annex,  Buying a Home, Buying a Garage' is igher compared to other loan purpose. This implies that people are taking more credit for buying new things and assets.\n\n> Student and Pensioner income type people has negligible credit.\n\n> Commercial Associates and State Servant has applied for significant amount of credit.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plotting for Credit amount prev vs Housing type in logarithmic scale\nplt.figure(figsize=(16,12))\nplt.xticks(rotation=90)\nplt.yscale('log')\nsns.barplot(data =merged_df, y='AMT_CREDIT_PREV',hue='TARGET',x='NAME_HOUSING_TYPE')\nplt.title('Prev Credit amount vs Housing type')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Here Office apartment and House/apartment is having higher credit of target type 0 i.e. Client with No payment difficulties.\nWhile Co-op apartment have high credit of target type 1 i.e. Client with payment difficulties.\n\n> So, Bank should avoid giving loans to housing of Co-op apartments and focus more on Office apartment and House/apartment types of housing.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**RECOMMENDATION TO BANK**\n1. Banks should focus more on contract type ‘Student’ ,’pensioner’ and ‘Businessman’ with housing ‘type an d should avoid ‘Co-op apartment’ housing type for successful payments.\n\n2. Banks should focus less on income type ‘Working’ as they are having most number of unsuccessful payments.\n\n3. Also with loan purpose ‘Repair’ is having higher number of unsuccessful payments on time.\n\n4. Get as much as clients from housing type ‘With parents’ as they are having least number of unsuccessful payments.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}