{"cells":[{"metadata":{},"cell_type":"markdown","source":"# FastAI Text Classification - Beginner Tutorial\n\nBuilding on the knowledge & experience gained from the [FastAI Image Classification](https://www.kaggle.com/kkhandekar/fastai-beginner-tutorial) tutorial, we shall attempt to perform a text classification in this notebook.\n\nAnd to keep things interesting, we shall be using the [Kick Starter NLP Dataset](https://www.kaggle.com/oscarvilla/kickstarter-nlp).\n\nOnwards with the scripting ...\n\nCourse of action:\n\n* Libraries\n* Load, Prep & Explore Data\n* Text Data Pre-Processing\n* Build & Train Model\n* Predictions\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install contractions -q","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re,string,unicodedata\nimport contractions #import contractions_dict\n\n#FastAI\nimport fastai\nfrom fastai import *\nfrom fastai.text import * \n\n#Functional Tool\nfrom functools import partial\n\n#Garbage\nimport gc\n\n#NLTK\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\n\n#SK Learn libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn import metrics\nfrom sklearn.compose import ColumnTransformer\n\n#Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load, Prep & Explore Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load data\nurl = '../input/kickstarter-nlp/df_text_eng.csv'\nraw_data = pd.read_csv(url, header='infer')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking the columns\nraw_data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the purpose of this tutorial, we are only interested in the \"blurb\" & the \"state\" columns. So, we shall create a seperate dataframe that will only consist these 2 columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating a seperate dataframe\ndata = raw_data[['blurb','state']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inspect the shape of the dataframe\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check for null/missing values in the new dataframe\ndata.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the records with null/missing values\ndata = data.dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the records per state\ndata.groupby('state').size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Encoding the State label to convert them to numerical values\nlabel_encoder = LabelEncoder() \n\n#Applying to the dataset\ndata['state']= label_encoder.fit_transform(data['state']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inspect the newly created dataframe\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Text Data Pre-Processing\n\nWe all have the habit of cleaning our fruits/veggies before eating them, so in the similar way it is always a good practice to clean text data before feeding it to the model. This will stop the Model from spewing incorrect results. \n\nIn this step we will only focus on cleaning/pre-processing the \"blurb\" column since the \"state\" columns is a categorical column.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Remove special characters & retain alphabets\ndata['blurb'] = data['blurb'].str.replace(\"[^a-zA-Z]\", \" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lowering the case\ndata['blurb'] = data['blurb'].str.lower()\n\n#stripping leading spaces (if any)\ndata['blurb'] = data['blurb'].str.strip()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing punctuations\nfrom string import punctuation\n\ndef remove_punct(text):\n  for punctuations in punctuation:\n    text = text.replace(punctuations, '')\n  return text\n\n#apply to the dataset\ndata['blurb'] = data['blurb'].apply(remove_punct)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to remove macrons & accented characters\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\n#applying the function on the clean dataset\ndata['blurb'] = data['blurb'].apply(remove_accented_chars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to expand contractions\ndef expand_contractions(con_text):\n  con_text = contractions.fix(con_text)\n  return con_text\n\n#applying the function on the clean dataset\ndata['blurb'] = data['blurb'].apply(expand_contractions) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing Stopwords\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords \n#stop_words = stopwords.words('english')\nstopword_list = set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The stopword remover function ingests the text in bite size portions. To achieve this we will have to tokenize (split) our text. Tokenization can be done in 2 ways\n\n1. Using the Split function\n2. Using a Tokenizer function\n\nWe shall implement the tokenization using the second option. NLTK provides a functions for doing just that (check the libraries section above!)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#instantiating the tokenizer function\ntokenizer = ToktokTokenizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to remove stopwords\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n\n#applying the function\ndata['blurb_norm'] = data['blurb'].apply(remove_stopwords) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping the \"blurb\" column\ndata = data.drop(['blurb'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Inspect the dataframe after stopword removal\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this stage, our \"blurb\" column is cleaned and ready to be fed to the Model. We will take a backup of this dataset just incase something goes wrong !","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Databack\ndata_bkup = data.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build & Train Model\n\nBefore building the model we need to split the dataset into Training & Test/Validation data. We will split the data into 90:10 ratio where 90% of the data will be used for training & remaining 10% for test/validation.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#data split\ntrain_data, test_data = train_test_split(data, test_size = 0.1, random_state = 12, stratify=data['state'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reseting index for test_data\ntest_data.reset_index(drop=True, inplace=True)\n\n#resting index for train_data\ntrain_data.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Shape of train & test data\nprint(\"Training Data Shape - \",train_data.shape, \" Test Data Shape - \", test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, here comes the fun part ..\n\n![](https://i2.wp.com/neptune.ai/wp-content/uploads/fastai_logo.png?fit=406%2C194&ssl=1)\n\n\nWe need to prep our text data for 2 different models i.e. **Language & Classification Model**. This can be done using the FastAI libraries\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Language Model\nlang_mod = TextLMDataBunch.from_df(train_df= train_data, valid_df=test_data, path='')\n\n#Classification Model\nclass_mod = TextClasDataBunch.from_df(path='', train_df=train_data, valid_df=test_data, vocab=lang_mod.train_ds.vocab, bs=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a language learner based on the language model (lang_mod) created above.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lang_learner = language_model_learner(lang_mod, arch = AWD_LSTM, pretrained = True, drop_mult=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding the learning rate for language learner\nlang_learner.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the Recorder Plot\nlang_learner.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And just like in the FastAI Image Classification tutorial we will use the **One Cycle** approach to train our language learner.  The learning rate is chosen based on the plot above.\nThe learning rate = 1e-2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training the language learner model\nlang_learner.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: Observe that we have achieved an accuracy of ~ 14% , which really bad.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Saving the language learner encoder\nlang_learner.save_encoder('fai_langlrn_enc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's use the \"class_mod\" object created above to build a classifier and then fine-tune our language learner.","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"class_learner = text_classifier_learner(class_mod, drop_mult=0.3, arch = AWD_LSTM, pretrained = True)\nclass_learner.load_encoder('fai_langlrn_enc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding the learning rate of this class_learner\nclass_learner.lr_find()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting the Recorder Plot for the class learner\nclass_learner.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The learning rate from the above plot is 1e-03","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training the Class Learner Model\nclass_learner.fit_one_cycle(1, 1e-3, moms=(0.8,0.7))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can observe the accuracy has increased drastically. The current accuracy is at ~ 64% which is strictly OK for the purpose of this tutorial.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#saving the Class Learner Model\nclass_learner.save_encoder('fai_classlrn_enc_tuned')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#free memory\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions\n\nWe will now try to get the predictions for the validation set  (test data) from our learner object","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class_learner.show_results()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions\npred, trgt = class_learner.get_preds()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Confusion matrix\nprediction = np.argmax(pred, axis = 1)\npd.crosstab (prediction, trgt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prediction on Test Dataset\ntest_dataset = pd.DataFrame({'blurb': test_data['blurb_norm'], 'actual_state' : test_data['state'] })\ntest_dataset = pd.concat([test_dataset, pd.DataFrame(prediction, columns = ['predicted_state'])], axis=1)\n\ntest_dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion:** Here we conclude the tutorial for Text Classification using FastAI. As we can clearly observe that the model has an average accuracy due to which not all of the predicted_state is correct. \n\nThe next step from here is to fine-tune the model to increase the accuracy but that I shall keep it for some other day. Thank you for reading.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}