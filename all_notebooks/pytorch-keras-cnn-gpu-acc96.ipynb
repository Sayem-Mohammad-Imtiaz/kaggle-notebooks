{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n# for keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Sequential\n\n# for pytorch\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-30T07:24:35.096878Z","iopub.execute_input":"2021-07-30T07:24:35.097229Z","iopub.status.idle":"2021-07-30T07:24:35.103691Z","shell.execute_reply.started":"2021-07-30T07:24:35.097197Z","shell.execute_reply":"2021-07-30T07:24:35.102744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MNIST Classifier Using Keras and PyTorch CNNs and Transfer Learning with PyTorch","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv('../input/chinese-mnist-digit-recognizer/chineseMNIST.csv')\ndataset.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:24:35.105374Z","iopub.execute_input":"2021-07-30T07:24:35.105807Z","iopub.status.idle":"2021-07-30T07:24:45.345051Z","shell.execute_reply.started":"2021-07-30T07:24:35.105772Z","shell.execute_reply":"2021-07-30T07:24:45.344254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select the data\nfeatures = dataset.values[:, :dataset.shape[1]-2]\nchars = dataset['character'].values\n\nfeatures.shape, chars.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:24:45.346779Z","iopub.execute_input":"2021-07-30T07:24:45.347121Z","iopub.status.idle":"2021-07-30T07:24:47.233293Z","shell.execute_reply.started":"2021-07-30T07:24:45.347086Z","shell.execute_reply":"2021-07-30T07:24:47.232389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# receives data\ndef process_data(x):\n    images = [] # all the images\n    # is each row in x, each image\n    for img in x:\n        # reshape the flatten data\n        image = img.reshape(64,64,1)\n        images.append(image)\n    # return the images in an apropiate format\n    return np.array(images).astype('float32')/255\n\n# recieves labels\ndef process_target(chars, num_classes):\n    target = [] # is the result\n    class_names = {} # other result\n    count = count_values(chars) # count the characters\n    ###### add the labels for the dict\n    for key, i in zip(count.keys(), range(num_classes)):\n        class_names[key] = i\n    ###### create the labels data, the numbers\n    labs = class_names.keys()\n    for char in chars:\n        pos = class_names[char] # position of the 1\n        row = []\n        for i in range(num_classes):# create the target [0,0,0...,1,...]\n            if pos != i:\n                row.append(0)\n            else:\n                row.append(1)\n        target.append(row)\n    return np.array(target).astype('float32'), class_names\n\n\ndef count_values(arr):\n    dic = {}\n    for val in arr:\n        if val not in dic.keys():\n            dic[val] = 1\n        else:\n            dic[val] += 1\n    return dic\n\n# get the images from the df as arrays\nX = process_data(features)\n\n# and obtain the target data from the characters\nY, class_names = process_target(chars, num_classes=15)\n\nX.shape, Y.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:24:47.234906Z","iopub.execute_input":"2021-07-30T07:24:47.235249Z","iopub.status.idle":"2021-07-30T07:24:54.352724Z","shell.execute_reply.started":"2021-07-30T07:24:47.235212Z","shell.execute_reply":"2021-07-30T07:24:54.351905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the data, as keras training goes first, for now there won't be val set\nx_train_k, x_test_k, y_train_k, y_test_k = train_test_split(X, Y, test_size=.2, random_state=314)\ny_train_k.shape, y_test_k.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:24:54.354044Z","iopub.execute_input":"2021-07-30T07:24:54.354395Z","iopub.status.idle":"2021-07-30T07:24:54.437669Z","shell.execute_reply.started":"2021-07-30T07:24:54.354356Z","shell.execute_reply":"2021-07-30T07:24:54.43667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Keras Model Training\n","metadata":{}},{"cell_type":"code","source":"#keras.backend.clear_session()\n\ninput_shape = (64,64,1) # the dimension of the data\nnum_classes = 15 # the number of classes\n\nkeras_model = Sequential([\n\n    layers.InputLayer(input_shape=input_shape),\n\n    # convolutional part with relu and later pooling\n    layers.Conv2D(filters=32, kernel_size=5, activation='relu'),\n    layers.MaxPooling2D(pool_size=2),\n\n    # flatten the data, as it comes with (64,64,1) shape\n    layers.Flatten(),\n    \n    # dense part, with neurons\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(rate=.4), # it helps to prevent overfitting\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(rate=.4),\n    layers.Dense(256, activation='relu'),\n    layers.Dropout(rate=.4),\n    layers.Dense(num_classes, activation='softmax')\n])\n\n# compile the model\nkeras_model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics = ['accuracy'],\n)\n\nkeras_hist = keras_model.fit(\n    x_train_k,\n    y_train_k,\n    batch_size=128,\n    epochs=20,\n    validation_split=.1,# as there's no val data\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:24:54.43913Z","iopub.execute_input":"2021-07-30T07:24:54.439517Z","iopub.status.idle":"2021-07-30T07:25:10.142004Z","shell.execute_reply.started":"2021-07-30T07:24:54.43948Z","shell.execute_reply":"2021-07-30T07:25:10.14118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\n\n# plot the loss function\nplt.subplot(1,2,1)\nplt.plot(keras_hist.history['loss'], label='train')\nplt.plot(keras_hist.history['val_loss'], label='validation')\nplt.title('Loss Function')\nplt.grid(True)\nplt.legend()\n\n# and the accuracy\nplt.subplot(1,2,2)\nplt.plot(keras_hist.history['accuracy'], label='train')\nplt.plot(keras_hist.history['val_accuracy'], label='validation')\nplt.grid(True)\nplt.title('Accuracy')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:10.145075Z","iopub.execute_input":"2021-07-30T07:25:10.145459Z","iopub.status.idle":"2021-07-30T07:25:10.484214Z","shell.execute_reply.started":"2021-07-30T07:25:10.145406Z","shell.execute_reply":"2021-07-30T07:25:10.48336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Class PyTorch\n","metadata":{}},{"cell_type":"code","source":"# First split the data for pytorch\nx_train_p, x_test_p, y_train_p, y_test_p = train_test_split(features.astype('float32'), chars, test_size=.2, random_state=314)\nx_train_p, x_val_p, y_train_p, y_val_p = train_test_split(x_train_p, y_train_p, test_size=.1, random_state=314)\n\ny_train_p.shape, y_test_p.shape, y_val_p.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:10.486037Z","iopub.execute_input":"2021-07-30T07:25:10.486398Z","iopub.status.idle":"2021-07-30T07:25:15.488341Z","shell.execute_reply.started":"2021-07-30T07:25:10.48636Z","shell.execute_reply":"2021-07-30T07:25:15.487441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    # data will be a numpy array, in this case: features\n    # labels will be tha characters, in this case: labels\n\n    ######################### THE \"BASIC\" NEEDED FUNCTIONS\n    \n    def __init__(self, data, labels, labels_ids=[]):\n        # process the labels and the data\n        self.process_data(data)\n        self.process_labels(labels, labels_ids) # this labels ids is too important, so\n                                                # read bellow why and check the function\n    \n    def __len__(self):\n        return self.labels.shape[0]\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels_n[idx]\n    \n    ######################### THE PROCESSING FUNCTIONS\n    \n    # function inside the class for process the data\n    def process_data(self, data):\n        # convert data to a torch tensor\n        self.data = torch.from_numpy(data)\n        # normalize the data\n        self.data = self.data/255\n        # reshape the tensor, keep the batch and chanel dim and reshape the images\n        self.data = self.data.view(self.data.shape[0], 1, 64, 64)\n        \n    # function inside the class for process the target\n    def process_labels(self, labels, labels_ids):\n        # count all the different values in labels\n        self.distrib, self.num_labels = self.__count_values(labels)\n        \n        # verify if there are existant ids !important\n        if labels_ids == []:\n            # set an id for each label if there's no labels\n            self.id_labels = {}\n            for label, i in zip(self.distrib.keys(), range(self.num_labels)):\n                self.id_labels[label] = i\n\n        # if there are existant labels use them\n        else:\n            self.id_labels = labels_ids   \n            \n        # then create an array with the ids\n        ids = [] # will be converted in an array\n        for label in labels:\n            # append the id of the label\n            ids.append(self.id_labels[label])\n        # use one hot encoding for the labels\n        self.labels = []\n        self.labels_n = [] # labels without encoding\n        for i in ids:\n            # append the normal label\n            self.labels_n.append(i)\n            # append the one hot encoded label\n            self.labels.append(self.__one_hot(i))\n        # convert to a numpy array, this is because the final dtype\n        # was torch.float64 and the data is torch.float32\n        self.labels = np.array(self.labels).astype('float32')\n        # and convert to torch tensor\n        self.labels = torch.from_numpy(self.labels)\n        self.labels_n = torch.tensor(self.labels_n, dtype=torch.int64)\n        \n    \n    ######################### EXTRA FUNCTIONS\n    \n    # extra function to count the different items from an array\n    def __count_values(self, arr):\n        dic = {}\n        for val in arr:\n            if val not in dic.keys():\n                dic[val] = 1\n            else:\n                dic[val] += 1\n        return dic, len(dic.keys())\n    \n    # extra function to make one hot encoding for the labels\n    def __one_hot(self, label):\n        res = np.zeros((self.num_labels))\n        res[label]+=1\n        return res\n    \n    # extra function to decode from one hot to numbers\n    def decode(self, labels): # recieve a torch tensor, a prediction\n        # select the indexs of the max elements in each label\n        decoded = torch.argmax(labels, dim=1).numpy()\n        return decoded\n        \n        \n\n#### DATASET CLASS ATRIBUTES: (the ones I use)\n# data: all the tensors (images) with shape (bacth,1,64,64), pytorch tensor\n# labels: all the labels encoded with one hot encoding, pytorch tensor\n# distrib: a dict that contains how many data there are from each label {character: number}\n# id_labels: a dict with and id for each label or character {character:id}\n## as this has to be the same for every dataset it will be passed as a param\n## for the test and val datasets. The data was shuffled, not being passing the dict\n## as a param for the others will cause diferent ids and LOW val and test accuracy.\n# labels_n: a list the labels but without one hot encoding, are the ids from id_labels\n# num_labels: how many different labels or classes there are\n\n\n# use the class for create dataset objects\n# since the keras processing has created a dict of ids, these classes\n# are going to use the same ids so the models predict \"the same\"\ntrain_set = CustomDataset(x_train_p, y_train_p, class_names)\n# the next ones will contain the same ids of train_set\ntest_set = CustomDataset(x_test_p, y_test_p, train_set.id_labels)\nval_set = CustomDataset(x_val_p, y_val_p, train_set.id_labels)\n\n\n# see some data of the datasets\n# the lengths of each dataset, the dtypes of data and labels, and an example of the labels\nlen(train_set), len(test_set), test_set[0][0].shape, test_set[0][0].dtype, test_set[0][1].dtype, test_set[1000][1]","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:15.489703Z","iopub.execute_input":"2021-07-30T07:25:15.490044Z","iopub.status.idle":"2021-07-30T07:25:15.696673Z","shell.execute_reply.started":"2021-07-30T07:25:15.490014Z","shell.execute_reply":"2021-07-30T07:25:15.695687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# then use the module DataLoader to 'adapt' the datasets for the model\nbatch_size = 128\n\ntrain_loader = DataLoader(\n    dataset=train_set,\n    batch_size=batch_size,\n    shuffle=False # the datasets are already shuffled\n)\n\ntest_loader = DataLoader(\n    dataset=test_set,\n    batch_size=batch_size,\n    shuffle=False # the datasets are already shuffled\n)\n\nval_loader = DataLoader(\n    dataset=val_set,\n    batch_size=batch_size,\n    shuffle=False # the datasets are already shuffled\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:15.698145Z","iopub.execute_input":"2021-07-30T07:25:15.698624Z","iopub.status.idle":"2021-07-30T07:25:15.713547Z","shell.execute_reply.started":"2021-07-30T07:25:15.698586Z","shell.execute_reply":"2021-07-30T07:25:15.712575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Network(nn.Module):\n    # in the init are going to be defined the layers\n    def __init__(self):\n        super(Network, self).__init__()\n        # define the layers, here the order is not important\n        # CONVOLUTIONAL LAYERS\n        # here there's no input shape, only the number of channels and the\n        # output number of chanels\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n        self.pool1 = nn.MaxPool2d(2)\n        # DENSE LAYERS\n        # the first para is like the input_shape, the seccond the number\n        # of outputs, like the number of neurons\n        self.dense1 = nn.Linear(32*30*30, 256)\n        self.dense2 = nn.Linear(256, 256)\n        self.dense3 = nn.Linear(256, 256)\n        self.dense4 = nn.Linear(256, 15)\n        # DROPOUT LAYER\n        self.dropout = nn.Dropout(0.3)\n        \n    # here we define the forward propagation of the net\n    def forward(self, x): # and use the not trainable layers\n        # convolutional process\n        x = F.relu(self.pool1(self.conv1(x)))\n        # apply the flatten process, conserving the batch dim\n        x = torch.flatten(x,1)\n        # dense process with droput layers\n        x = self.dropout(F.relu(self.dense1(x)))\n        x = self.dropout(F.relu(self.dense2(x)))\n        x = self.dropout(F.relu(self.dense3(x)))\n        x = self.dense4(x)\n        return x\n        \n        \npt_model = Network()\n\n# this is a prediction from the model, if we play and modificate the\n# forward function we will see how the x.shape is changing\npt_model(torch.randn((32,1,64,64)))\n\n# SET THE DEVICE\n# if we hace a gpu (with cuda) set the device as the gpu, else in cpu\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npt_model = pt_model.to(device) # move the model to the selected device\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:15.71491Z","iopub.execute_input":"2021-07-30T07:25:15.715239Z","iopub.status.idle":"2021-07-30T07:25:15.821085Z","shell.execute_reply.started":"2021-07-30T07:25:15.715206Z","shell.execute_reply":"2021-07-30T07:25:15.8201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 0.002\n\n# this is like instance the loss function\ncriterion = nn.CrossEntropyLoss()\n# the optimizer receives the model weights and a learning rate\noptimizer = optim.Adam(pt_model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:15.822479Z","iopub.execute_input":"2021-07-30T07:25:15.822826Z","iopub.status.idle":"2021-07-30T07:25:15.82783Z","shell.execute_reply.started":"2021-07-30T07:25:15.822787Z","shell.execute_reply":"2021-07-30T07:25:15.826832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define a metric\ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n    model.eval() # is like swich the model mode, this changes the\n    # behave of layers like Dropouts Layers, BatchNorm Layers\n\n    with torch.no_grad(): # deactivcate the back propagation,\n    # it will reduce memory and speed up computations\n\n        for x,y in loader:\n            # move the data and targets to the device\n            x = x.to(device)\n            y = y.to(device)\n            # obtain the scores\n            scores = model(x)\n            # we ned the max from the second dim\n            _, preds = scores.max(1)\n            # select the correct preds and sum them\n            num_correct += (preds == y).sum()\n            # count the num of samples\n            num_samples += preds.shape[0]\n    \n    # calculate the accuracy, float since numbers are tensors\n    acc = float(num_correct) / float(num_samples)\n    print(f'Got {num_correct} / {num_samples} with accuracy {acc*100}%')\n\n    # switch the model to train mode\n    model.train()\n    \n    return acc","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:15.829168Z","iopub.execute_input":"2021-07-30T07:25:15.829555Z","iopub.status.idle":"2021-07-30T07:25:15.838348Z","shell.execute_reply.started":"2021-07-30T07:25:15.829519Z","shell.execute_reply":"2021-07-30T07:25:15.837167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train variables\nepochs = 24\npt_hist = {\n    'accuracy': [],\n    'val accuracy': []\n}\n\n# train the network\nfor epoch in range(epochs):\n    # this iters the the data and targets, and with an id\n    # data and targets are the batch for each train step\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # move the data and taregets to the model's device\n        data = data.to(device)\n        targets = targets.to(device)\n\n        # forward propagation, predict and measure the error\n        scores = pt_model(data)\n        loss = criterion(scores, targets)\n        \n        # backward propagation, use the error to fit the weights\n        optimizer.zero_grad() # clean the gradient, it's needed\n        # contains the directions to redice the error value of\n        # each prediction made, else is going to be adding value and fail\n        ## back propagation of the gradient and fit the weights\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n        \n    print(f'==> Epoch{epoch}')\n    # check the accuracy from train and val in each epoch\n    acc = check_accuracy(train_loader, pt_model)\n    val_acc = check_accuracy(val_loader, pt_model)\n    \n    # regist the accuacy values\n    pt_hist['accuracy'].append(acc)\n    pt_hist['val accuracy'].append(val_acc)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:15.839584Z","iopub.execute_input":"2021-07-30T07:25:15.839971Z","iopub.status.idle":"2021-07-30T07:25:41.106027Z","shell.execute_reply.started":"2021-07-30T07:25:15.839935Z","shell.execute_reply":"2021-07-30T07:25:41.104699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now Transfer Learning\nThats somthing new I want to try with Py/torch. The idea is that once the model params has ben optimized, or trained, `The convolutional part has already been optimized`, so data that flows through that layer `isn't going to change any more` if I *freeze* the convolutional part for train. In other words, `convolutional layer is trained and I will train new linear neurons without changing the convolutional part`.","metadata":{}},{"cell_type":"code","source":"# then, freeze the convolutional part\n\n# iterate the model params\nfor param in pt_model.parameters():\n    print(param.shape)\n    break # only the first, the conv\n    \n# the printed shape is torch.Size([32, 1, 5, 5])\n# are 32 outs and a 5x5 kernel of 1 chanel, so that\n# are the params of the convolutional part\n\n# that property is like \"if the param is trainable\"\nif param.requires_grad == True:\n    # set as not trainable param\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:41.1073Z","iopub.execute_input":"2021-07-30T07:25:41.107659Z","iopub.status.idle":"2021-07-30T07:25:41.113486Z","shell.execute_reply.started":"2021-07-30T07:25:41.107623Z","shell.execute_reply":"2021-07-30T07:25:41.112556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if it has the correct effect\nfor param in pt_model.parameters():\n    print(param.shape)\n    print(param.requires_grad)\n    break","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:41.114788Z","iopub.execute_input":"2021-07-30T07:25:41.115115Z","iopub.status.idle":"2021-07-30T07:25:41.1235Z","shell.execute_reply.started":"2021-07-30T07:25:41.115083Z","shell.execute_reply":"2021-07-30T07:25:41.122578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Restart the Dense Layers","metadata":{}},{"cell_type":"code","source":"# this is like \"undo the train\"\n\n# redefine the layers\npt_model.dense1 = nn.Linear(32*30*30, 256)\npt_model.dense2 = nn.Linear(256, 256)\npt_model.dense3 = nn.Linear(256, 256)\npt_model.dense4 = nn.Linear(256, 15)\n\n# move the model to the device\npt_model = pt_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:41.124772Z","iopub.execute_input":"2021-07-30T07:25:41.125313Z","iopub.status.idle":"2021-07-30T07:25:41.188676Z","shell.execute_reply.started":"2021-07-30T07:25:41.125278Z","shell.execute_reply":"2021-07-30T07:25:41.187936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# redefine the optimizer for the new parameters\n\n# this optimizer receives the model trainable weights and a learning rate\nnew_optimizer = optim.Adam(filter(lambda p: p.requires_grad, pt_model.parameters()), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:41.189765Z","iopub.execute_input":"2021-07-30T07:25:41.190085Z","iopub.status.idle":"2021-07-30T07:25:41.194788Z","shell.execute_reply.started":"2021-07-30T07:25:41.190053Z","shell.execute_reply":"2021-07-30T07:25:41.193618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train variables\nepochs = 24\npt_hist = {\n    'accuracy': [],\n    'val accuracy': [],\n    'loss': []\n}\n\n# train the network\nfor epoch in range(epochs):\n    # this iters the the data and targets, and with an id\n    # data and targets are the batch for each train step\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # move the data and taregets to the model's device\n        data = data.to(device)\n        targets = targets.to(device)\n\n        # forward propagation, predict and measure the error\n        scores = pt_model(data)\n        loss = criterion(scores, targets)\n        \n        # backward propagation, use the error to fit the weights\n        new_optimizer.zero_grad() # clean the gradient, it's needed\n        # contains the directions to redice the error value of\n        # each prediction made, else is going to be adding value and fail\n        ## back propagation of the gradient and fit the weights\n        loss.backward()\n\n        # gradient descent or adam step\n        new_optimizer.step()\n        \n    print(f'==> Epoch{epoch}')\n    # check the accuracy from train and val in each epoch\n    acc = check_accuracy(train_loader, pt_model)\n    val_acc = check_accuracy(val_loader, pt_model)\n    \n    # regist the accuacy values\n    pt_hist['accuracy'].append(acc)\n    pt_hist['val accuracy'].append(val_acc)\n    pt_hist['loss'].append(loss.item())","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:25:41.198284Z","iopub.execute_input":"2021-07-30T07:25:41.19866Z","iopub.status.idle":"2021-07-30T07:26:01.964095Z","shell.execute_reply.started":"2021-07-30T07:25:41.198624Z","shell.execute_reply":"2021-07-30T07:26:01.962841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the accuracy increase\nx = np.arange(len(pt_hist['accuracy']))\nplt.figure(figsize=(10,6))\nplt.plot(x, pt_hist['accuracy'], label='train')\nplt.plot(x, pt_hist['val accuracy'], label='val')\nplt.title('Accuracy')\nplt.grid(True)\nplt.legend()\nplt.ylim(.7,1.1)\nplt.xlabel('Epoch')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:26:01.965702Z","iopub.execute_input":"2021-07-30T07:26:01.96604Z","iopub.status.idle":"2021-07-30T07:26:02.128524Z","shell.execute_reply.started":"2021-07-30T07:26:01.966005Z","shell.execute_reply":"2021-07-30T07:26:02.127509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate the Models Individually","metadata":{}},{"cell_type":"code","source":"results = keras_model.evaluate(x_test_k, y_test_k, batch_size=64)\nprint(\"test loss, test acc:\", results)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:26:02.129946Z","iopub.execute_input":"2021-07-30T07:26:02.130403Z","iopub.status.idle":"2021-07-30T07:26:02.364868Z","shell.execute_reply.started":"2021-07-30T07:26:02.130364Z","shell.execute_reply":"2021-07-30T07:26:02.363779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = check_accuracy(test_loader, pt_model)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:26:02.367529Z","iopub.execute_input":"2021-07-30T07:26:02.367775Z","iopub.status.idle":"2021-07-30T07:26:02.446886Z","shell.execute_reply.started":"2021-07-30T07:26:02.36775Z","shell.execute_reply":"2021-07-30T07:26:02.446112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Join the models\nMake a function to predict using the two models at the same time.","metadata":{}},{"cell_type":"code","source":"# see the shapes of the predictions and how to predict\npt_model.eval()\npt_sample = torch.unsqueeze(test_set[0][0], 0).to(device)\npt_model(pt_sample).shape, type(pt_sample) == torch.Tensor, pt_sample.to('cpu').numpy().shape","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:26:02.448107Z","iopub.execute_input":"2021-07-30T07:26:02.448459Z","iopub.status.idle":"2021-07-30T07:26:02.457896Z","shell.execute_reply.started":"2021-07-30T07:26:02.448409Z","shell.execute_reply":"2021-07-30T07:26:02.456941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ks_sample = x_test_k[0].reshape(1,64,64,1)\nkeras_model.predict(ks_sample).shape, type(ks_sample) == np.ndarray, ks_sample.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:26:02.459582Z","iopub.execute_input":"2021-07-30T07:26:02.460033Z","iopub.status.idle":"2021-07-30T07:26:02.543135Z","shell.execute_reply.started":"2021-07-30T07:26:02.459995Z","shell.execute_reply":"2021-07-30T07:26:02.542388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Ensemble:\n    # models = [(model, 'keras' or 'pytorch')]\n    # datasets = [(test_loader, 'pt_loader'), ((x_test_k, y_test_k), 'numpy')]\n    def __init__(self, models=[], datasets=[]):\n        self.models = []\n        self.mtypes = []\n        self.sets = datasets\n\n        # read the models and the model types\n        for model, mtype in models:\n            self.models.append(model)\n            self.mtypes.append(mtype)  \n        \n    def add_model(self, model, mtype):\n        self.models.append(model)\n        self.mtypes.append(mtype)\n    \n    def add_set(self, dataset, datatype):\n        self.sets.append((dataset, datatype))\n    \n    # extra function to decode from one hot to numbers\n    def decode(self, x): # recieve a torch tensor, a prediction\n        # select the indexs of the max elements in each label\n        x = torch.tensor(x)\n        decoded = torch.argmax(x, dim=1).numpy()\n        return decoded\n    \n    def predict(self, x):        \n        # create a x for each mtype \n        x_dic = {}\n        if type(x) == np.ndarray:\n            x_dic['keras'] = x                     # keep the batch dim\n            x_dic['pytorch'] = torch.from_numpy(x).to(device).view(x.shape[0], 1, 64, 64)\n        else:\n            x_dic['pytorch'] = x.to(device)\n            x_dic['keras'] = x.to('cpu').numpy().reshape(x.shape[0], 64, 64, 1)\n        \n        # predict with the models\n        preds = []\n        for model, mtype in zip(self.models, self.mtypes):\n            if mtype == 'keras':\n                p = np.squeeze(model.predict(x_dic['keras']))\n                preds.append(p)\n            else:\n                with torch.no_grad():\n                    p = model(x_dic['pytorch']).to('cpu')\n                    p = np.squeeze(p.detach().numpy())\n                    preds.append(p)\n        \n        # sum the preds and obtain the mean\n        sum_preds = sum(preds)/len(self.models)\n        # reshape for the decode\n        sum_preds = sum_preds.reshape(-1, 15)\n        # return the decoded labels\n        return self.decode(sum_preds)\n    \n    def evaluate(self):\n        num_correct = 0\n        num_samples = 0\n        preds, answers = [], []\n        for ds, datatype in self.sets:\n            print('evaluation on -> ' + datatype)\n            if datatype == 'numpy': # there are x_test and x_train\n                x_set, y_set = ds\n                y_set = self.decode(y_set)\n                for x, y in zip(x_set, y_set):\n                    # predict each image with both models\n                    pred = self.predict(x.reshape(1, 64, 64, 1))[0]\n                    # append the predictions and the answers\n                    preds.append(pred)\n                    answers.append(y)\n                    # count the answers\n                    if pred == y:\n                        num_correct += 1\n                    num_samples += 1\n                    \n            elif datatype == 'pt_loader': # the pytorch loader\n                for batch_x, batch_y in ds: # this will return a batch\n                    for x, y in zip(batch_x, batch_y):\n                        # predict each image with both models\n                        pred = self.predict(x.view(1, 1, 64, 64))[0]\n                        # append the predictions and the answers\n                        preds.append(pred)\n                        answers.append(y)\n                        # count the answers\n                        if pred == y.item():\n                            num_correct += 1\n                        num_samples += 1\n        \n        return num_correct/num_samples, answers, preds\n                    \n    \n\nmodel = Ensemble(\n    models = [(keras_model, 'keras'), (pt_model, 'pytorch')],\n    datasets = [((x_test_k, y_test_k), 'numpy'), (test_loader, 'pt_loader')]\n)\n\n# it will take a while\nacc, predictions, answers = model.evaluate()\nacc","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:26:02.544303Z","iopub.execute_input":"2021-07-30T07:26:02.544631Z","iopub.status.idle":"2021-07-30T07:29:19.837355Z","shell.execute_reply.started":"2021-07-30T07:26:02.544604Z","shell.execute_reply":"2021-07-30T07:29:19.836561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A confusion Matrix","metadata":{}},{"cell_type":"code","source":"# define the matrix with the real classes and the predicted\nm = confusion_matrix(answers, predictions)\n# the labels for the plot\nlabels = list(class_names.values()) # the characters throw warnings\nplt.figure(figsize=(20, 8))\n# create the plot\nheatmap = sns.heatmap(m, xticklabels=labels, yticklabels=labels, annot=True, fmt='d', color='blue')\n# labels for the axes\nplt.xlabel('Predicted Class')\nplt.ylabel('True Class')\nplt.title('Confusion Matrix')\nplt.show()\n# print the ids and the labels\nprint('Labels and ids:')\nprint(test_set.id_labels.keys())\nprint(test_set.id_labels.values())","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:29:19.838651Z","iopub.execute_input":"2021-07-30T07:29:19.839054Z","iopub.status.idle":"2021-07-30T07:29:20.8945Z","shell.execute_reply.started":"2021-07-30T07:29:19.839017Z","shell.execute_reply":"2021-07-30T07:29:20.893529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# See some Predictions","metadata":{}},{"cell_type":"code","source":"# preds must be like [[real, pred]]\ndef plot_images(imgs, dims, figsize, title_size, preds=[]):\n    plt.figure(figsize=figsize)\n    for img, i, in zip(imgs, np.arange(imgs.shape[0])):\n        plt.subplot(dims[0], dims[1], i+1)\n        # plot in a different color if the prediction is wrong\n        plt.imshow(np.squeeze(img), cmap=('gray' if preds[i][1] == preds[i][0] else 'magma'))\n        plt.axis('off')\n        title = f'Image {i+1}'\n        if preds != []:\n            title = f'Real: {preds[i][0]}, Pred: {preds[i][1]}'\n        plt.title(title, fontsize=title_size)\n    plt.show()\n\n# select some images\nsample_images = x_test_k[:25]\npredicts = model.predict(sample_images)\nanswers = model.decode(y_test_k[:25])\n\n# adecuate the preds and answers for the plot\ndata = []\nfor p,a in zip(predicts, answers):\n    data.append((a,p))\n    \nplot_images(sample_images, dims=(5,5), figsize=(20,20), title_size=22, preds=data)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T07:29:20.895823Z","iopub.execute_input":"2021-07-30T07:29:20.89616Z","iopub.status.idle":"2021-07-30T07:29:22.589424Z","shell.execute_reply.started":"2021-07-30T07:29:20.896121Z","shell.execute_reply":"2021-07-30T07:29:22.588602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nThis model implementation was interesting. Is the first time I use **Transfer Learning** in PyTorch. Was good and fun trying to join to models of two different libraries.","metadata":{}}]}