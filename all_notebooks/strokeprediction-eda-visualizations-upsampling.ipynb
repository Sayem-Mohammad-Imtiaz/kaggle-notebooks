{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## About the Data\n<b>Data Dictionary</b><br>\n1. id: unique identifier\n2. gender: \"Male\", \"Female\" or \"Other\"\n3. age: age of the patient\n4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n6. ever_married: \"No\" or \"Yes\"\n7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n8. Residence_type: \"Rural\" or \"Urban\"\n9. avg_glucose_level: average glucose level in blood\n10. bmi: body mass index\n11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n12. stroke: 1 if the patient had a stroke or 0 if not","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, plot_confusion_matrix, roc_curve, roc_auc_score, auc\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_PATH = \"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(DATA_PATH)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of uniqe id's in data: {len(data.id.unique())}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>OBSERVATION: </b>We can notice that the number of unique id's is equal to the number of rows. Therfore there is no duplicacy in the data.","metadata":{}},{"cell_type":"code","source":"# Dropping the id column\ndata.drop(columns = [\"id\"], inplace = True)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_columns = list(data.columns)\ncategorical_data_cols  = [column for column in all_columns if len(data[column].unique())<=5]\ncontinuous_data_cols  = [column for column in all_columns if column not in categorical_data_cols]\nprint(f\"Continuos Data Columns: {', '.join(continuous_data_cols)}\")\nprint(f\"Categorical Data Columns: {', '.join(categorical_data_cols)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Univariate Analysis","metadata":{}},{"cell_type":"code","source":"stroke_val_counts = data[\"stroke\"].value_counts()\n\nprint(f\"Non Stroke: {stroke_val_counts[0] / sum(stroke_val_counts)}%\")\nprint(f\"Stroke: {stroke_val_counts[1] / sum(stroke_val_counts)}%\")\n\ndata[\"stroke\"].value_counts().plot(kind = \"bar\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>OBSERVATION: </b>From the above plot we can clearly notice that the dataset is higly imbalanced dataset. We need to do some upsampling to balance the data.","metadata":{}},{"cell_type":"code","source":"for column in categorical_data_cols[:-1]:\n    print(f\"Number of NaN values in {column}: {data[column].isnull().sum()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (17,19))\ni = 1\nfor column in categorical_data_cols[:-1]:\n    plt.subplot(4, 2, i)\n    sns.countplot(x = data[column], hue = data[\"stroke\"])\n    i+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>OBSERVATIONS:</b><br>\n1. The number of male and female having stroke are almost equal in number.\n2. The people suffering and not suffering with hypertension have almost same and no sign of heart stroke. This may be due to the fact that the number of records with stroke \"1\" is very less.\n3. The married people are showing more signs for heart stroke\n4. The people who are having private jobs are more prone to heart attack.","metadata":{}},{"cell_type":"code","source":"for column in categorical_data_cols[:-1]:\n    plt.figure(figsize = (9,5))\n    type_count = data.groupby(column)[\"stroke\"].sum()\n    x = type_count.index\n    y = type_count.values\n    plt.barh(x, y)\n\n    for index, value in enumerate(y):\n        plt.text(value, index,\n                 value)\n\n    plt.title(f\"{column} vs Stroke\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in categorical_data_cols:\n    print(f\"Unique values in {column} are: {', '.join([str(i) for i in data[column].unique()])}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>OBSERVATION: </b>Here smoking status is an ordinal variable and remaining are nominal variables. Let us do ordinal encoding for the ordinal variable and one hot encoding for nominal variables.","metadata":{}},{"cell_type":"code","source":"married_map = {\n    \"Yes\":1,\n    \"No\":0\n}\nresidence_map = {\n    \"Urban\":1,\n    \"Rural\":2\n}\n\nord_encoder = OrdinalEncoder()\ndata[\"smoking_status\"] = ord_encoder.fit_transform(data[\"smoking_status\"].values.reshape(-1, 1))\n\ndata[\"ever_married\"] = data[\"ever_married\"].map(married_map)\ndata[\"Residence_type\"] = data[\"Residence_type\"].map(residence_map)\n\ndata = pd.get_dummies(data, columns = [\"gender\", \"work_type\"], drop_first = True)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in continuous_data_cols:\n    print(f\"Number of NaN values in {column}: {data[column].isnull().sum()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the distribution of the data\nplt.figure(figsize = (11, 9))\ni = 1\nfor column in continuous_data_cols:\n    plt.subplot(2, 2, i)\n    sns.histplot(data[column], bins = 50)\n    i+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the distribution of the data\nplt.figure(figsize = (11, 9))\ni = 1\nfor column in continuous_data_cols:\n    plt.subplot(2, 2, i)\n    sns.kdeplot(data[column])\n    i+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the outliers in the data\nplt.figure(figsize = (11, 9))\ni = 1\nfor column in continuous_data_cols:\n    plt.subplot(2, 2, i)\n    sns.boxplot(x = data[column])\n    i+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<b>OBSERVATION: </b>Since the data for BMI is right skewed let us fill the missing values in the bmi with median, since median is not affected by the ouliers.","metadata":{}},{"cell_type":"code","source":"data[\"bmi\"].fillna(value = data[\"bmi\"].median(), inplace = True)\nprint(f\"Number of missing values in BMI: {data['bmi'].isnull().sum()}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the distribution of the data\nplt.figure(figsize = (11, 9))\ni = 1\nfor column in continuous_data_cols:\n    plt.subplot(2, 2, i)\n    sns.kdeplot(data[column])\n    i+=1\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The BMI column is in the form of log normal distribution. Let us apply log transformation to convert it into Normal Distribution.","metadata":{}},{"cell_type":"code","source":"data[\"bmi\"] = np.log(data[\"bmi\"])\nsns.kdeplot(data[\"bmi\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bivariate Analysis","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (7,7))\nsns.pairplot(data[continuous_data_cols+[\"stroke\"]], hue = \"stroke\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (9,7))\nsns.heatmap(data[continuous_data_cols].corr(), annot = True, center = 0)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data.drop(columns = [\"stroke\"])\ny = data[\"stroke\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the data for training and testing","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 24)\nprint(f\"Train Data: {X_train.shape}, {y_train.shape}\")\nprint(f\"Train Data: {X_test.shape}, {y_test.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Upsampling using SMOTE","metadata":{}},{"cell_type":"code","source":"counter = Counter(y_train)\nprint(f\"Before Upsampling: {counter}\")\n\nupsample = SMOTE()\nX_train, y_train = upsample.fit_resample(X_train, y_train)\ncounter = Counter(y_train)\nprint(counter)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After Upsampling\nprint(f\"Train Data: {X_train.shape}, {y_train.shape}\")\nprint(f\"Train Data: {X_test.shape}, {y_test.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scaling the Data","metadata":{}},{"cell_type":"code","source":"std_scaler  = StandardScaler()\nX_train = std_scaler.fit_transform(X_train)\nX_test = std_scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking for best baseline model using Cross Validation","metadata":{}},{"cell_type":"code","source":"all_models = {\n    \"xgb_model\":XGBClassifier(eval_metric = \"logloss\",random_state=18,use_label_encoder=False),\n    \"rf_model\":RandomForestClassifier(random_state = 18),\n    \"logistic_model\":LogisticRegression(),\n    \"svm_model\":SVC(),\n    \"ada_model\":AdaBoostClassifier(RandomForestClassifier(random_state = 18))\n}\n\nfor model_name in all_models:\n    print(f\"Model Name: {model_name}\")\n    cv_score = cross_val_score(all_models[model_name],X_train, y_train, cv = 5)\n    print(cv_score)\n    print(f\"Mean Score: {np.mean(cv_score)}\")\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SVM Model","metadata":{}},{"cell_type":"code","source":"svm_model = SVC()\nsvm_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions = svm_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(svm_model, X_test, y_test)\nplt.show()\n\nprint()\n\nprint(\"On Train Data\")\npredictions = svm_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(svm_model, X_train, y_train)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameter Tuning for SVM Model","metadata":{}},{"cell_type":"code","source":"param_grid = {'C': [0.1, 1, 10, 100, 1000], \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']} \n  \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 0)\ngrid.fit(X_train, y_train)\n\nprint(\"Best Params:\",grid.best_params_)\nprint(\"Best Estimator\", grid.best_estimator_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm_model = SVC(C=10, gamma=1)\nsvm_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions = svm_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(svm_model, X_test, y_test)\nplt.show()\n\nprint()\n\nprint(\"On Train Data\")\npredictions = svm_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(svm_model, X_train, y_train)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_auc(model, X, y):\n    probs = model.predict_proba(X)\n    preds = probs[:,1]\n    fpr, tpr, threshold = roc_curve(y, preds)\n    roc_auc = auc(fpr, tpr)\n    \n    print(\"AUC Score\",roc_auc_score(y, preds))\n\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNN Classifier","metadata":{}},{"cell_type":"code","source":"error_rate = []\nfor i in range(1, 50):\n    pipeline = KNeighborsClassifier(n_neighbors = i)\n    pipeline.fit(X_train, y_train)\n    predictions = pipeline.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f\"Accuracy at k = {i} is {accuracy}\")\n    error_rate.append(np.mean(predictions != y_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,50),error_rate,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nprint(\"Minimum error:-\",min(error_rate),\"at K =\",error_rate.index(min(error_rate))+1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_model = KNeighborsClassifier(n_neighbors = 2)\nknn_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions = knn_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(knn_model, X_test, y_test)\nplt.show()\n\nprint()\n\nprint(\"On Train Data\")\npredictions = knn_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(knn_model, X_train, y_train)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Model","metadata":{}},{"cell_type":"code","source":"rf_model = RandomForestClassifier(random_state = 24)\nrf_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions = rf_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(rf_model, X_test, y_test)\nplt.show()\n\nplot_roc_auc(rf_model, X_test, y_test)\n\nprint()\n\nprint(\"On Train Data\")\npredictions = rf_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(rf_model, X_train, y_train)\nplt.show()\nplot_roc_auc(rf_model, X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost Model","metadata":{}},{"cell_type":"code","source":"xgb_model = XGBClassifier(eval_metric = \"logloss\",random_state=18,use_label_encoder=False)\nxgb_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions =xgb_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(xgb_model, X_test, y_test)\nplt.show()\nplot_roc_auc(xgb_model, X_test, y_test)\n\n\nprint()\n\nprint(\"On Train Data\")\npredictions =xgb_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(xgb_model, X_train, y_train)\nplt.show()\nplot_roc_auc(xgb_model, X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adaboost Model","metadata":{}},{"cell_type":"code","source":"ada_model = AdaBoostClassifier()\nada_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions = ada_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(ada_model, X_test, y_test)\nplt.show()\nplot_roc_auc(ada_model, X_test, y_test)\n\n\nprint()\n\nprint(\"On Train Data\")\npredictions = ada_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(ada_model, X_train, y_train)\nplt.show()\nplot_roc_auc(ada_model, X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"voting_model = VotingClassifier(\n    [\n        (\"svm_model\", SVC()),\n        (\"xgb_model\", XGBClassifier(eval_metric = \"logloss\",random_state=18,use_label_encoder=False)),\n        (\"ada_model\", AdaBoostClassifier())\n    ]\n)\n\nvoting_model.fit(X_train, y_train)\n\nprint(\"On Test Data\")\npredictions = voting_model.predict(X_test)\nprint(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_test, predictions)}\")\nprint(f\"Precision: {precision_score(y_test, predictions)}\")\nprint(f\"Recall: {recall_score(y_test, predictions)}\")\nplot_confusion_matrix(voting_model, X_test, y_test)\nplt.show()\n\n\nprint()\n\nprint(\"On Train Data\")\npredictions = voting_model.predict(X_train)\nprint(f\"Accuracy: {accuracy_score(y_train, predictions)}\")\nprint(f\"F1 Score: {f1_score(y_train, predictions)}\")\nprint(f\"Precision: {precision_score(y_train, predictions)}\")\nprint(f\"Recall: {recall_score(y_train, predictions)}\")\nplot_confusion_matrix(voting_model, X_train, y_train)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}