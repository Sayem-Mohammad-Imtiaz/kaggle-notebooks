{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas\nimport seaborn\nimport matplotlib.pyplot as plot\nimport numpy as np\nfrom scipy.stats import zscore\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import NearestNeighbors\n\n# Enter the route to your datasets below\n\ndatasetContacts = pandas.read_csv(\"../input/tarea2/DatasetContactos.csv\") \ndatasetTrxs = pandas.read_csv(\"../input/tarea2/DatasetTrxs.csv\")\ndataset = pandas.merge(datasetTrxs, datasetContacts, on='CUST_ID', how='inner')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-29T19:30:02.135617Z","iopub.execute_input":"2021-05-29T19:30:02.135941Z","iopub.status.idle":"2021-05-29T19:30:02.186304Z","shell.execute_reply.started":"2021-05-29T19:30:02.13591Z","shell.execute_reply":"2021-05-29T19:30:02.185186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parte 1\n\nEDA para ambos datasets.\nLo primero que se hizo fue modificar el csv de contactos ya que tenia los valores separados por ';' en vez de ',' y eso no permitia leerlo como csv.\nAdemas se mergeo ambos datasets en uno solo usando el CUST_ID para tener toda la informacion en un solo dataset.\nA continuacion, se imprimen las primeras 15 filas del dataset obtenido.","metadata":{}},{"cell_type":"code","source":"dataset.head(15)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:02.198652Z","iopub.execute_input":"2021-05-29T19:30:02.19898Z","iopub.status.idle":"2021-05-29T19:30:02.230409Z","shell.execute_reply.started":"2021-05-29T19:30:02.198939Z","shell.execute_reply":"2021-05-29T19:30:02.229459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En primer lugar, obtendremos informacion sobre el tamaño, los valores nulos y el tipo de los features del dataset.\nPodemos ver que todas las columnas son numericas salvoel CUST_ID y el PHONE.\nAdemas, hay valores nulos en MINIMUM_PAYMENTS y en CREDIT_LIMIT. Se trabajara para eliminarlos y que no afecten la clusterizacion mas adelante.","metadata":{}},{"cell_type":"code","source":"dataset.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:02.236536Z","iopub.execute_input":"2021-05-29T19:30:02.237088Z","iopub.status.idle":"2021-05-29T19:30:02.252387Z","shell.execute_reply.started":"2021-05-29T19:30:02.237015Z","shell.execute_reply":"2021-05-29T19:30:02.25164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Luego, se obtienen algunas estadisticas de las columnas numericas del dataset que ayudaran a comprender mejor los datos y su distribucion.\nSe tienen el promedio, la desviacion estandar y los principales cuartiles para cada feature.","metadata":{}},{"cell_type":"code","source":"dataset.describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:02.2535Z","iopub.execute_input":"2021-05-29T19:30:02.254132Z","iopub.status.idle":"2021-05-29T19:30:02.324355Z","shell.execute_reply.started":"2021-05-29T19:30:02.254093Z","shell.execute_reply":"2021-05-29T19:30:02.323273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parte 2\n\nEn esta parte, se limpiara el dataset. En primer lugar, se eliminaran los outliers que son valores atipicos que pueden afectar las predicciones. Se puede ver que en los datos habia 1.387 filas con outliers.\nLuego, se eliminaran los valores nulos que ya se vio que habia en CREDIT_LIMIT y MINIMUM_PAYMENTS.\nLa decision que se tomo fue eliminar las filas con outliers y con valores nulos, no rellenarlos con valores esperados.\nLuego de esto, el dataset que se usara para clusterizar, queda con 7420 entradas.","metadata":{}},{"cell_type":"code","source":"print(dataset.shape)\nnewDataset = dataset.drop(['PHONE', 'CUST_ID'], axis='columns')\nz = np.abs(zscore(newDataset))\noutliers_position = []\nfor i in np.where(z > 3):\n    outliers_position = np.concatenate((outliers_position, i))\nfinal_outliers = list(dict.fromkeys(outliers_position))\nprint(len(final_outliers))\ndataset = dataset.drop(final_outliers)\nprint(dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:02.326343Z","iopub.execute_input":"2021-05-29T19:30:02.326941Z","iopub.status.idle":"2021-05-29T19:30:02.342975Z","shell.execute_reply.started":"2021-05-29T19:30:02.326869Z","shell.execute_reply":"2021-05-29T19:30:02.341836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.dropna(subset=['CREDIT_LIMIT', 'MINIMUM_PAYMENTS'])","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:02.344594Z","iopub.execute_input":"2021-05-29T19:30:02.34503Z","iopub.status.idle":"2021-05-29T19:30:02.355473Z","shell.execute_reply.started":"2021-05-29T19:30:02.344863Z","shell.execute_reply":"2021-05-29T19:30:02.354129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:02.357052Z","iopub.execute_input":"2021-05-29T19:30:02.357407Z","iopub.status.idle":"2021-05-29T19:30:02.382594Z","shell.execute_reply.started":"2021-05-29T19:30:02.357377Z","shell.execute_reply":"2021-05-29T19:30:02.381189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parte 3\n\nEn primer lugar, se seleccionaron algunos features que se creyo que podian resultar en una buena clusterizacion de los clientes del banco.\nLos seleccionados fueron:\n* BALANCE_FREQUENCY: Frecuencia de actualización del saldo. Esto nos ayudara a ver que tan intenso es el uso de la tarjeta de credito.\n* PURCHASES: Importe de las compras realizadas. Nos indicara que tanto dinero el cliente gasta usando su tarjeta, y por ende indicara si tiene un mayor o menor nivel adquisitivo.\n* ONEOFF_PURCHASES_FREQUENCY: Compras frecuentes son realizadas de una sola vez. Nos indicara con que frecuencia el cliente decide comprar con la tarjeta de credito en un solo pago.\n* PURCHASES_INSTALLMENTS_FREQUENCY: Frecuencia de compras en cuotas. Analogo al feature anterior, nos indica que tanto el cliente decide comprar en cuotas.\n* CREDIT_LIMIT: Limite de crédito. Asumiremos que si el limite de credito es mas grande, es porque el banco considero que tenian un buen nivel adquisitivo.\n* PRC_FULL_PAYMENT: Porcentaje de pagos totales hechos por el usuario. Nos indica que tantas veces el cliente paga todo el monto de su tarjeta de credito y esto tambien esta asociando a un mayor poder adquisitivo.\n* MINIMUM_PAYMENTS: Cantidad de pagos mínimos hechos por el usuario. Si esta cantidad es muy alta, consideramos que el cliente no tiene tan buen nivel adquisitivo ya que no le es posible pagar la factura completa.\n\n\n","metadata":{}},{"cell_type":"code","source":"selected_features = [\n    'BALANCE_FREQUENCY',\n    'PURCHASES',\n    'PURCHASES_INSTALLMENTS_FREQUENCY',\n    'CREDIT_LIMIT',\n    'PRC_FULL_PAYMENT',\n    'MINIMUM_PAYMENTS',\n    'ONEOFF_PURCHASES_FREQUENCY'\n]","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:02.384158Z","iopub.execute_input":"2021-05-29T19:30:02.384583Z","iopub.status.idle":"2021-05-29T19:30:02.389665Z","shell.execute_reply.started":"2021-05-29T19:30:02.384538Z","shell.execute_reply":"2021-05-29T19:30:02.388298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se uso MinMaxScaler para normalizar las columnas elegidas a valores entre 0 y 1 para clusterizar. Esto se realiza para evitar que se le de mayor importancia a columnas con diferencias grandes entre los valores. ","metadata":{}},{"cell_type":"code","source":"selected_data = dataset[selected_features]\nscaler = MinMaxScaler()\n\nscaler.fit(selected_data)\nnormalized_data = scaler.transform(selected_data)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:02.391995Z","iopub.execute_input":"2021-05-29T19:30:02.392757Z","iopub.status.idle":"2021-05-29T19:30:02.40962Z","shell.execute_reply.started":"2021-05-29T19:30:02.392714Z","shell.execute_reply":"2021-05-29T19:30:02.408565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para decidir cuantos principal components se usarian para la clusterizacion, nos fijamos en cuales se contenia la mayoria de la informacion. Se puede ver que en el 0, 1 y 2 ya se tiene mas del 80%, por eso se usaran 3 PC para la clusterizacion.","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=7)\nprincipalComponents = pca.fit_transform(normalized_data)\n\nfeatures = range(pca.n_components_)\nplot.bar(features, pca.explained_variance_ratio_, color='black')\nplot.xlabel('PCA features')\nplot.ylabel('variance %')\nplot.xticks(features)\n\nPCA_components = pandas.DataFrame(principalComponents)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:02.411112Z","iopub.execute_input":"2021-05-29T19:30:02.411399Z","iopub.status.idle":"2021-05-29T19:30:02.621192Z","shell.execute_reply.started":"2021-05-29T19:30:02.41137Z","shell.execute_reply":"2021-05-29T19:30:02.620365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A continuacion, se obtiene el cluster asociado a cada fila en el dataset.","metadata":{}},{"cell_type":"code","source":"pca = PCA(3)\nPCA_dataset = pca.fit_transform(normalized_data)\n\nkmeans = KMeans(n_clusters=2)\nlabel = kmeans.fit_predict(PCA_dataset)\nprint(label)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:02.623265Z","iopub.execute_input":"2021-05-29T19:30:02.623649Z","iopub.status.idle":"2021-05-29T19:30:04.009628Z","shell.execute_reply.started":"2021-05-29T19:30:02.623608Z","shell.execute_reply":"2021-05-29T19:30:04.008945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se grafican los clusters y sus centros.","metadata":{}},{"cell_type":"code","source":"centers = np.array(kmeans.cluster_centers_)\n\nplot.figure(figsize=(15,15))\nuniq = np.unique(label)\n\nfor i in uniq:\n  plot.scatter(PCA_dataset[label == i , 0] , PCA_dataset[label == i , 1] , label = i)\nplot.xlabel('PC1')\nplot.ylabel('PC2')\nplot.scatter(centers[:,0], centers[:,1], marker=\"x\", color='k')\nplot.legend()\nplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:04.01287Z","iopub.execute_input":"2021-05-29T19:30:04.01429Z","iopub.status.idle":"2021-05-29T19:30:04.321345Z","shell.execute_reply.started":"2021-05-29T19:30:04.014246Z","shell.execute_reply":"2021-05-29T19:30:04.320367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se inserta una nueva columna en el dataset que indique a que cluster pertenece el cliente.","metadata":{}},{"cell_type":"code","source":"dataset.insert(20, \"CLUSTER\", label, True)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:04.322416Z","iopub.execute_input":"2021-05-29T19:30:04.32266Z","iopub.status.idle":"2021-05-29T19:30:04.326747Z","shell.execute_reply.started":"2021-05-29T19:30:04.322636Z","shell.execute_reply":"2021-05-29T19:30:04.325692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset.head(15)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:04.32791Z","iopub.execute_input":"2021-05-29T19:30:04.328219Z","iopub.status.idle":"2021-05-29T19:30:04.366829Z","shell.execute_reply.started":"2021-05-29T19:30:04.328193Z","shell.execute_reply":"2021-05-29T19:30:04.366204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A continuacion se obtienen las estadisticas de los datos incluidos en cada cluster.\nPodemos ver que en el cluster 0 hacen un uso mas intensivo de la tarjeta y que el gasto que se realiza, en promedio, es mucho mayor. Se podria suponer, entonces que en el cluster 0 quedaron asignados los clientes con mayor poder adquisitivo. Por lo tanto, a ellos se les ofreceria la tarjeta Platinum. Ademas, vemos que hacen mas pagos completos que los del cluster 1, pero de cualquier manera, hay muchos realizando pagos minimos que es algo que no deseamos para la tarjeta platinum. Por eso, solo ofreceriamos esta tarjeta a aquellos que tengan un numero de pagos minimos menor a 700, que es un numero cercano al cuartil del 75%.","metadata":{}},{"cell_type":"code","source":"dataset[dataset['CLUSTER'] == 0].describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:04.368618Z","iopub.execute_input":"2021-05-29T19:30:04.368876Z","iopub.status.idle":"2021-05-29T19:30:04.442528Z","shell.execute_reply.started":"2021-05-29T19:30:04.36885Z","shell.execute_reply":"2021-05-29T19:30:04.441604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[dataset['CLUSTER'] == 1].describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:04.445714Z","iopub.execute_input":"2021-05-29T19:30:04.445987Z","iopub.status.idle":"2021-05-29T19:30:04.515232Z","shell.execute_reply.started":"2021-05-29T19:30:04.445961Z","shell.execute_reply":"2021-05-29T19:30:04.514214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se filtra el dataset para obtener solo los clientes a los que se puede llamar a ofrecer el producto.","metadata":{}},{"cell_type":"code","source":"dataset_to_contact = dataset[dataset['CONTACTS'] < 2]\ndataset_to_contact.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:04.516665Z","iopub.execute_input":"2021-05-29T19:30:04.517084Z","iopub.status.idle":"2021-05-29T19:30:04.551707Z","shell.execute_reply.started":"2021-05-29T19:30:04.51702Z","shell.execute_reply":"2021-05-29T19:30:04.55074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parte 4\n\nDensity-based spatial clustering of applications with noise (DBSCAN) es un algoritmo de clusterización basado en densidad.\nPara clusterizar (lograr el agrupamiento de conjuntos de objetos no etiquetados, para lograr construir subconjuntos de datos conocidos como Clusters), DBSCAN clasifica a los puntos de la siguiente manera: \n* Puntos \"core\": son los puntos interiores de un cluster, cuando tienen, por lo menos, un número mínimo de puntos a una distancia E de p (minPts). \n* Puntos \"border\" (frontera): tienen menos de minPts a una distancia E de p, y están en el vecindario de algún punto core.\n* Puntos noise (ruido): cualquier punto que no forma parte de un cluster core ni del border\n\nPara decidir cual va a ser el epsilon, se busca el punto de mayor curvatura en la siguiente grafica. Y se cree que es en 0.15","metadata":{}},{"cell_type":"code","source":"neighbors = NearestNeighbors(n_neighbors=50)\nneighbors_fit = neighbors.fit(normalized_data)\ndistances, indices = neighbors_fit.kneighbors(normalized_data)\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplot.plot(distances)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:04.552856Z","iopub.execute_input":"2021-05-29T19:30:04.553128Z","iopub.status.idle":"2021-05-29T19:30:04.962911Z","shell.execute_reply.started":"2021-05-29T19:30:04.553101Z","shell.execute_reply":"2021-05-29T19:30:04.961962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dbscan = DBSCAN(eps=0.15, min_samples=40)\nlabel_db = dbscan.fit_predict(normalized_data)\n\nn_clusters_ = len(set(label_db)) - (1 if -1 in label_db else 0)\nn_noise_ = list(label_db).count(-1)\n\ndataset.insert(21, \"DBSCAN\", label_db, True)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:04.964153Z","iopub.execute_input":"2021-05-29T19:30:04.9644Z","iopub.status.idle":"2021-05-29T19:30:05.310227Z","shell.execute_reply.started":"2021-05-29T19:30:04.964376Z","shell.execute_reply":"2021-05-29T19:30:05.309296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se obtienen 2 clusters y 2.439 puntos considerados como ruido. Estos valores se pueden modificar al cambiar los valores de epsilon y el minimo de puntos que tiene que haber en cada cluster.\nA continuacion podemos ver que en el cluster 0 quedan 4.883 puntos y en el 1 quedan solo 98.\nCreemos que la clusterizacion obtenida usando k-means es mucho mas representativa y divide a los clientes en 2 tipos bien definidos, pero en el caso de la clusterizacion con DBSCAN no nos es posible inferir nada de los clusters. \nDe cualquier manera, si hubiera que elegir a que cluster ofrecerle la tarjeta platinum, seria al cluster 1. ","metadata":{}},{"cell_type":"code","source":"dataset[dataset['DBSCAN'] == 0].describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:05.311374Z","iopub.execute_input":"2021-05-29T19:30:05.311631Z","iopub.status.idle":"2021-05-29T19:30:05.381264Z","shell.execute_reply.started":"2021-05-29T19:30:05.311596Z","shell.execute_reply":"2021-05-29T19:30:05.380316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[dataset['DBSCAN'] == 1].describe()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:05.382441Z","iopub.execute_input":"2021-05-29T19:30:05.382695Z","iopub.status.idle":"2021-05-29T19:30:05.449978Z","shell.execute_reply.started":"2021-05-29T19:30:05.38267Z","shell.execute_reply":"2021-05-29T19:30:05.448952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parte 5 \n\nLas preguntas nos ayudarian a reconstruir los features que se usaron para la clusterizacion y serian:\n\n* Del 1 al 5, que tanto utilizaria una tarjeta de credito?\n* Cuanto dinero cree que gastaria usando la tarjeta de credito?\n* Del 1 al 5, que tanto utilizaria la tarjeta de credito para comprar en cuotas?\n* Del 1 al 5, que tanto utilizaria la tarjeta de credito para comprar en una unica cuota?\n* Cual seria el limite minimo para tu tarjeta de credito ideal?\n* Cuantas veces al año crees que pagarias el total de la factura? Y el minimo?\n\nSe realiza un arbol que dependiendo de la respuesta del futuro cliente toma un camino y le asigna una tarjeta. Para poder usar el arbol de decision, se debe mapear las respuestas a los valores que trae el dataset. Por ejemplo las frecuencias serias del 0 al 1, no del 1 al 5.","metadata":{}},{"cell_type":"code","source":"X = dataset[selected_features]\nY = dataset['CLUSTER']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n\ntreeClass = DecisionTreeClassifier(max_depth=len(selected_features))\ntreeClass = treeClass.fit(X_train, Y_train)\n\nfig = plot.figure(figsize=(100,100))\nplot_tree(treeClass, \n           feature_names=selected_features,  \n           class_names=['Tarjeta Internet Global', 'Tarjeta Platinum'])","metadata":{"execution":{"iopub.status.busy":"2021-05-29T19:30:05.451371Z","iopub.execute_input":"2021-05-29T19:30:05.451724Z","iopub.status.idle":"2021-05-29T19:30:13.509492Z","shell.execute_reply.started":"2021-05-29T19:30:05.451687Z","shell.execute_reply":"2021-05-29T19:30:13.508463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parte 6\n\nPara concluir, esta tarea logró que se llevaran a práctica los conocimientos teóricos de algoritmos de aprendizaje automático no supervisado, y se pudieron observar muchas cosas.\n\nEn primer lugar, al no ser supervisado, no hay una columna independiente para realizar el análisis, sino que se debe elegir y seleccionar las features que se consideren relevantes. Para hacer esta selección, se debió evaluar principalmente cuál era el objetivo del análisis, y entender en profundidad la utilidad de cada uno de los features. Previo a la selección final de features, se realizo un recorrido por otras que parecían relevantes pero luego se notó claramente que no eran tan importantes como podía parecer en primera instancia.\n\nComo el dataset en sí mismo no trae etiquetadas las filas en clases, la custerización es responsabilidad completa del desarrollador. En este caso específico, evaluar qué clientes recibirían qué tarjeta.\n\nAl tener que elegir las columnas importantes para realizar el análisis, se debe entender sobre el tema a trabajar, y, además, se debe evaluar previamente si no existen outliers que puedan perjudicar el estudio final. Es por esto que es muy importante realizar el análisis completo sobre un dataset \"limpio\", es decir, sin outliers alevosos.\n\nPara terminar, se concluye que en este tipo de algoritmos de aprendizaje automático no supervisado es más difícil evaluar los resultados. No hay algo bien o algo mal. Solamente se realizó la separación de clientes para entregar al banco y el banco luego de contactarlos podrá evaluar la efectividad del algoritmo. Pero sin llegar a ese paso, no se puede saber si efectivamente fue realizado correctamente o no.\n","metadata":{}}]}