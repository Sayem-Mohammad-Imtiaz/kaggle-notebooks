{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"reviews_df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tokens(sentence):\n    tokens = sentence.split(' ')\n    tokens = [ele for ele in tokens if ele]\n    return tokens\n\nreviews_df['tokens'] = reviews_df.review.apply(get_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_token(token):\n    #lowercase it \n    token = token.lower()\n    \n    #strip the spaces\n    token = token.strip()\n    \n    return token\n\nvocab = []\nfor tokens in reviews_df.tokens.to_list():\n    vocab.extend(tokens)\nvocab = [clean_token(token) for token in vocab]\nvocab = [token for token in vocab if token]\nvocab = list(set(vocab))\nprint('total vocabulary: ', len(vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2index = {}\nfor index, token in enumerate(vocab):\n    word2index[token] = index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_data = reviews_df.sentiment.to_list()\n\n#convert positive and negative to 0 and 1\ntarget_data = [1 if sentiment == 'positive' else 0 for sentiment in target_data]\ntarget_data[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_data = []\nreviews_tokens = reviews_df.tokens.to_list()\nfor index, tokens in enumerate(reviews_tokens):\n     sentence2index = []\n     for token in tokens:\n            token = clean_token(token)\n            if token:\n                word_index = word2index[token]\n                sentence2index.append(word_index)\n     input_data.append(sentence2index)\ninput_data[0][:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha, iterations, hidden_size = (0.01, 2, 100)\n\nnp.random.seed(1)\n\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\n#2 layer network\nweights_0_1 = 0.2 * np.random.random((len(vocab), hidden_size)) - 0.1\nweights_1_2 = 0.2 * np.random.random((hidden_size, 1)) - 0.1 \n\ncorrect, error, total = (0, 0, 0)\n\nfor iteration in range(iterations):\n    for i in range(len(input_data) - 1000):\n        x, y = (input_data[i], target_data[i])\n\n        #word embedding\n        layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))\n\n        layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n\n        delta_2 = layer_2 - y\n        delta_1 = delta_2.dot(weights_1_2.T)\n\n        if np.abs(delta_2) < 0.5:\n            correct += 1  \n        total += 1\n\n        error += delta_2**2\n\n        weights_0_1[x] -= delta_1 * alpha\n        weights_1_2 -= np.outer(layer_1, delta_2) * alpha\n\nprint('Train Accuracy: ', (correct/total) * 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total, correct = (0, 0)\nfor i in range(len(input_data) - 1000, len(input_data)):\n    x, y  = (input_data[i], target_data[i])\n    \n    layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))\n    layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n    \n    delta_2 = layer_2 - y\n    \n    if np.abs(delta_2) < 0.5:\n        correct += 1\n    \n    total +=1\n\nprint('Test Accuracy:', (correct/total)* 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nimport math\n\ndef get_similar_words(target_word):\n    target_word_index = word2index[target_word]\n    scores = Counter()\n    for word, index in word2index.items():\n        \n        #calculate euclidean distance between two word weights\n        weight_differences = weights_0_1[target_word_index] - weights_0_1[index]\n        squared_differences = weight_differences**2\n        scores[word] = -math.sqrt(sum(squared_differences))\n    \n    return scores.most_common(20)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_similar_words('beautiful')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_similar_words('terrible')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_similar_words('money')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fill in the blanks**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\nrandom.seed(1)\n\ninput_dataset = list()\ncontactenated = list()\nfor review_tokens in reviews_df.tokens.to_list():\n    sentence2index = []\n    for token in review_tokens:\n        token = clean_token(token)\n        if token:\n            sentence2index.append(word2index[token])\n            contactenated.append(word2index[token])\n    input_dataset.append(sentence2index)\n\ncontactenated = np.array(contactenated)\n    \nrandom.shuffle(input_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha, iteration = (0.05, 2)\nhidden_size, window, negative = (50, 2, 5)\n\nweights_0_1 = (np.random.rand(len(vocab), hidden_size) - 0.5) * 0.2\nweights_1_2 = np.random.rand(len(vocab), hidden_size) * 0\n\nlayer_2_target = np.zeros(negative + 1)\nlayer_2_target[0] = 1\n\nfor rev_i, review in enumerate(input_dataset * iteration):\n    for target_i in range(len(review)):\n        target_samples = [review[target_i]] + list(contactenated[(np.random.rand(negative)*len(contactenated)).astype('int').tolist()])\n        \n        left_context = review[max(0, target_i - window): target_i]\n        right_context = review[target_i+1: target_i + min(len(review), target_i + window)]\n        \n        layer_1 = np.mean(weights_0_1[left_context + right_context], axis=0)\n        layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))\n        \n        delta_2 = layer_2 - layer_2_target\n        delta_1 = delta_2.dot(weights_1_2[target_samples])\n        \n        weights_0_1[left_context+right_context] -= delta_1 * alpha\n        weights_1_2[target_samples] -= np.outer(delta_2, layer_1) * alpha\n         \n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_similar_words('terrible')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}