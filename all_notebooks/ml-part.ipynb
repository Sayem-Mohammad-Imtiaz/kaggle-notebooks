{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  **Prediction PART**"},{"metadata":{},"cell_type":"markdown","source":"**At this stages, we developped three ML models to predict whether a visitor to the e-commerce page will make a purchase or not. \nThen, this model can help to find the right consumer - who have the intention to purchase - analyze the data of potential consumer in real time and push direct marketing strategies to all those people.**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\nimport sklearn.metrics as metrics\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nfrom sklearn.metrics import classification_report\ndf = pd.read_csv(\"../input/online-shoppers-intention/online_shoppers_intention.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing**\n> We will first transform the categorical variables into a numeric variable then split our data into train and test parts. Then we will plot the repartition of the two classes in the df\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(df,columns):\n    df.dropna(inplace=True) \n    df.replace(['Returning_Visitor', 'New_Visitor','Other'],[1,2,3], inplace=True)\n    df.replace(['Jan','Feb','Mar','Apr','May','June','Jul','Aug','Sep','Oct','Nov','Dec'],[1,2,3,4,5,6,7,8,9,10,11,12],inplace=True)\n    if columns!=[]: \n        df=df[columns]\n    return df\n\ndef Split(df):\n    X = df.drop('Revenue', axis=1)\n    y = df['Revenue']\n    obj_escalar = StandardScaler()\n    X_standardization = obj_escalar.fit_transform(X)\n    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n    return X_train, X_test, Y_train, Y_test\n \ndf=preprocessing(df,[])\nX_train, X_test, Y_train, Y_test=Split(df)\ndf['Revenue'].value_counts().plot.bar()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We observe that the data are imbalanced. There is a lot more 'Revenue = False' observations than 'Revenue = True'. We will therefore have to take it into account in our models."},{"metadata":{},"cell_type":"markdown","source":"# > **BASELINE LOGISTIC REGRESSION**"},{"metadata":{},"cell_type":"markdown","source":"> * The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n> * Cfloat, default=1.0 Inverse of regularization strength; must be a positive float --> smaller values specify stronger regularization."},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = LogisticRegression(class_weight='balanced')\nmodel1.fit(X_train, Y_train)\ny_pred=model1.predict(X_test)\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_pred),3)))  \nprint(\"\\nRecall\\t{}\".format(round(metrics.recall_score(Y_test, y_pred),3)))  \nmodel1.get_params()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find Best parameters using Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"parametres = {\"C\": [0.001, 0.008, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06,0.07, 0.1]}\nmodel1_gs = GridSearchCV(model1, param_grid=parametres,\n                         cv = 10, scoring='accuracy')\nmodel1_gs.fit(X_train, Y_train)\nprint(model1_gs.best_params_, \"\\nAcc: {}\".format(round(model1_gs.best_score_,3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_search = pd.DataFrame.from_dict(model1_gs.cv_results_)\nplt.xlabel('C')\nplt.ylabel('Acc')\n_ = plt.plot(df_search['param_C'], df_search['mean_test_score'], 'x')\nprint('Best_parameter',model1_gs.best_params_, \"\\nAcc: {}\".format(round(model1_gs.best_score_,3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find Best threshold"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = LogisticRegression(class_weight='balanced', C=model1_gs.best_params_.get(\"C\"))\nmodel1.fit(X_train, Y_train)\ny_test_pred_prob = model1.predict_proba(X_test)\ny_test_pred_prob_pos = y_test_pred_prob[np.where(Y_test == 1)[0]]\ny_test_pred_prob_neg = y_test_pred_prob[np.where(Y_test == 0)[0]]\n\ndef representation_seuil(x_1, x_0, n_bins=11, title='This figure represents in blue the probabilities assigned by the model to data that are 1, and in red the probabilities assigned to the data that are 0', label_1='Clase 1', \n                          label_0='Clase 0', density=0):\n    bins = n_bins\n    plt.hist(x_1, bins, density = density, alpha=0.5, label=label_1, color='blue')    \n    plt.hist(x_0, bins, density = density, alpha=0.5, label=label_0, color='red')\n    plt.title(title)\n    plt.legend(loc='best') \n    \nrepresentation_seuil(y_test_pred_prob_pos[:, 1], y_test_pred_prob_neg[:, 1], n_bins=21, density=0)    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Because the sensitivity represents the percentage of true positive on all the positives values, it is a data which is important for our cases since we need to find which visitors have the intention to buy (Revenue=True)"},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD = 0.5\ny_THRESHOLD = 1*(y_test_pred_prob[:, 1] > THRESHOLD)\n\nprint(u\"Matriz de confusión\\n\", metrics.confusion_matrix(Y_test, y_THRESHOLD))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_THRESHOLD),3)))  \nprint(\"Sensitivity\\t{}\".format(round(metrics.recall_score(Y_test, y_THRESHOLD),3)))\nprint(u\"Precision\\t{}\".format(round(metrics.precision_score(Y_test, y_THRESHOLD),3))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD = 0.6\ny_THRESHOLD = 1*(y_test_pred_prob[:, 1] > THRESHOLD)\n\nprint(u\"Matrice de confusion\\n\", metrics.confusion_matrix(Y_test, y_THRESHOLD))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_THRESHOLD),3)))  \nprint(\"Sensitivity\\t{}\".format(round(metrics.recall_score(Y_test, y_THRESHOLD),3)))\nprint(u\"Precision\\t{}\".format(round(metrics.precision_score(Y_test, y_THRESHOLD),3))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD = 0.55\ny_THRESHOLD = 1*(y_test_pred_prob[:, 1] > THRESHOLD)\n\nprint(u\"Matrice de confusion\\n\", metrics.confusion_matrix(Y_test, y_THRESHOLD))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_THRESHOLD),3)))  \nprint(\"Sensitivity\\t{}\".format(round(metrics.recall_score(Y_test, y_THRESHOLD),3)))\nprint(u\"Precision\\t{}\".format(round(metrics.precision_score(Y_test, y_THRESHOLD),3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We will keep the threshold of 0.55 which allows to have a good trade-off between accuracy and recall"},{"metadata":{},"cell_type":"markdown","source":"**Features importance**\n> We want to observe which variables contribute the most to the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"importance = model1.coef_[0]\nimportant_features=[]\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature:',df.columns[i],'Score:', v)\n    if (v>0.04 or v<-0.04):\n        important_features.append(df.columns[i])\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()\n\nimportant_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" > We are keeping features that score> 0.04 or <-0.04 to see if our model's performance will improve."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/online-shoppers-intention/online_shoppers_intention.csv\")\ncolumns=important_features+['Revenue']\ndf_less_features=preprocessing(df,columns)\nX_train, X_test, Y_train, Y_test=Split(df_less_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = LogisticRegression(class_weight='balanced')\nparametres = {\"C\": [0.001, 0.008, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06,0.07, 0.1]}\nmodel1_gs = GridSearchCV(model1, param_grid=parametres,\n                         cv = 10, scoring='accuracy')\nmodel1_gs.fit(X_train, Y_train)\nprint(model1_gs.best_params_, \"\\nAcc: {}\".format(round(model1_gs.best_score_,3)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = LogisticRegression(class_weight='balanced', C=model1_gs.best_params_.get(\"C\"))\nmodel1.fit(X_train, Y_train)\ny_test_pred_prob = model1.predict_proba(X_test)\ny_test_pred_prob_pos = y_test_pred_prob[np.where(Y_test == 1)[0]]\ny_test_pred_prob_neg = y_test_pred_prob[np.where(Y_test == 0)[0]]\n\ndef representation_seuil(x_1, x_0, n_bins=11, title='', label_1='Classe 1', \n                          label_0='Classe 0', density=0):\n    bins = n_bins\n    plt.hist(x_1, bins, density = density, alpha=0.5, label=label_1, color='blue')    \n    plt.hist(x_0, bins, density = density, alpha=0.5, label=label_0, color='red')\n    plt.title(title)\n    plt.legend(loc='best') \n    \nrepresentation_seuil(y_test_pred_prob_pos[:, 1], y_test_pred_prob_neg[:, 1], n_bins=21, density=0)    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD = 0.6\ny_threshold = 1*(y_test_pred_prob[:, 1] > THRESHOLD)\n\nprint(u\"Matriz de confusion\\n\", metrics.confusion_matrix(Y_test, y_threshold))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_threshold),3)))  \nprint(\"Sensitivity\\t{}\".format(round(metrics.recall_score(Y_test, y_threshold),3)))\nprint(u\"Precision\\t{}\".format(round(metrics.precision_score(Y_test, y_threshold),3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD = 0.55\ny_threshold = 1*(y_test_pred_prob[:, 1] > THRESHOLD)\n\nprint(u\"Matriz de confusion\\n\", metrics.confusion_matrix(Y_test, y_threshold))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_threshold),3)))  \nprint(\"Sensitivity\\t{}\".format(round(metrics.recall_score(Y_test, y_threshold),3)))\nprint(u\"Precision\\t{}\".format(round(metrics.precision_score(Y_test, y_threshold),3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"THRESHOLD = 0.5\ny_threshold = 1*(y_test_pred_prob[:, 1] > THRESHOLD)\n\nprint(u\"Matriz de confusion\\n\", metrics.confusion_matrix(Y_test, y_threshold))\nprint(\"\\nAccuracy\\t{}\".format(round(metrics.accuracy_score(Y_test, y_threshold),3)))  \nprint(\"Sensitivity\\t{}\".format(round(metrics.recall_score(Y_test, y_threshold),3)))\nprint(u\"Precision\\t{}\".format(round(metrics.precision_score(Y_test, y_threshold),3)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Results**\n* We will keep the threshold of 0.5 which allows us to have the best deal between accuracy and recall.\n* We can observe that when we relaunch the model with only the most important features, we get better results. The other variables therefore made noise in the previous model. \n* We thus obtain in our best logistic regression model: 0,89 accuracy, 0,70 recall."},{"metadata":{},"cell_type":"markdown","source":"# > > MODEL 2 RANDOM FOREST"},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = RandomForestClassifier(n_estimators = 30,max_depth = 10,random_state = 101)\nmodel2.fit(X_train,Y_train)\npred = model2.predict(X_test)\nprint('Results Random Forest with no optimization')\nprint(classification_report(Y_test,pred))\nprint(model2.score(X_test,Y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Optimization of the Random Forest Classifier using GridSearch"},{"metadata":{"trusted":true},"cell_type":"code","source":" param_grid = {\n    'n_estimators' : [60,100],\n    'max_depth' : [10,15],\n    'min_samples_leaf' : [2,4],\n    'min_samples_split': [2,4]\n}\n\ngridsearch = GridSearchCV(estimator=model2,param_grid=param_grid,verbose = 1)\ngridsearch.fit(X_train,Y_train)\ngridsearch.best_params_\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = RandomForestClassifier(n_estimators = 60,max_depth = 10,min_samples_leaf = 3, min_samples_split = 2,random_state = 101)\nmodel2.fit(X_train,Y_train)\npred = model2.predict(X_test)\nprint(classification_report(Y_test,pred))\n\nfrom sklearn.metrics import accuracy_score\nAcc = accuracy_score(Y_test,pred)\nprint('Accuracy',Acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Results**\n* We thus obtain in our best logistic regression model: 0.91 accuracy, 0.62 recall.\n* The logistic regression is finally a better model than random forest to predict shopping intentions"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}