{"cells":[{"metadata":{},"cell_type":"markdown","source":"<div align='center'><font size=\"5\" color='#353B47'>Introduction to Time Series Analysis</font></div>\n<div align='center'><font size=\"4\" color=\"#353B47\">Good practices and forecasting methods</font></div>\n<br>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"**<font color=\"blue\" size=\"4\">What is a Time Series ?</font>**\n\n> A Time Series is a **time-indexed** series of data. In Finance, a time series tracks the movement of the chosen data points, such as a *security’s price*, over a specified period of time with data points recorded at **regular intervals**."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://toocooltrafficschool.com/wp-content/uploads/2019/02/what-is-tlsae740.png\" width=\"500\">"},{"metadata":{},"cell_type":"markdown","source":"**<font color=\"blue\" size=\"4\">Why is it used for ?</font>**\n\n> Time series analysis can be useful to see how a given asset, security, or economic variable changes over time. It can also be used to examine how the changes associated with the chosen data point compare to shifts in other variables over the same time period.\n\n> For example, suppose you wanted to analyze a time series of daily closing stock prices for a given stock over a period of one year. You would obtain a list of all the closing prices for the stock from each day for the past year and list them in chronological order. "},{"metadata":{},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Regular libraries for data manipulation\nimport pprint\nimport numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\n\n# Visualization\nfrom pandas.plotting import lag_plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistical tools for time series analysis\nfrom scipy import signal\nimport statsmodels.api as sm\n!pip install pmdarima\nfrom pmdarima.arima import auto_arima\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.tools import diff\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing\nfrom statsmodels.tsa.stattools import adfuller, kpss, acf, grangercausalitytests\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf,month_plot,quarter_plot\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\nsns.set_style(\"whitegrid\")\nplt.rc('xtick', labelsize=15) \nplt.rc('ytick', labelsize=15)\n\nfrom pylab import rcParams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import data"},{"metadata":{},"cell_type":"markdown","source":"**<font color=\"blue\" size=\"4\">What is the source of the data ?</font>**\n\nThe data is originally imported from a preprocessed dataset I created scrapping <a href=\"https://markets.businessinsider.com/stocks/air_liquide-stock\">this website</a> via *beautifulsoup* library. It is composed of daily stocks of 40 most valuables French Companies (CAC40) from January 2010 to April 2020. \n\n**For this notebook, I will only focus on Air Liquide Stocks.**"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/cac40-stocks-dataset/preprocessed_CAC40.csv', usecols = ['Name','Date','Open','Closing_Price','Daily_High','Daily_Low','Volume'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filter the dataframe on Air Liquide stocks\nair_liquide = data[data['Name'] == 'Air Liquide'].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting 'Date' to datetime object\nair_liquide['Date'] = pd.to_datetime(air_liquide['Date'])\nair_liquide['Year'] = air_liquide['Date'].dt.year\nair_liquide['Month'] = air_liquide['Date'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'air_liquide shape: {air_liquide.shape[0]} rows, {air_liquide.shape[1]} columns')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"air_liquide.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <div id=\"summary\">Summary</div>\n\n**<font size=\"2\"><a href=\"#chap1\">1. Line plot of observations</a></font>**\n**<br><font size=\"2\"><a href=\"#chap2\">2. Treatment of missing values</a></font>**\n**<br><font size=\"2\"><a href=\"#chap3\">3. Seasonality</a></font>**\n**<br><font size=\"2\"><a href=\"#chap4\">4. Time series components</a></font>**\n**<br><font size=\"2\"><a href=\"#chap5\">5. Stationarity</a></font>**\n**<br><font size=\"2\"><a href=\"#chap6\">6. Autocorrelation</a></font>**\n**<br><font size=\"2\"><a href=\"#chap7\">7. Lag Scatter Plot</a></font>**\n**<br><font size=\"2\"><a href=\"#chap8\">8. Moving Average</a></font>**\n**<br><font size=\"2\"><a href=\"#chap9\">9. Exponential Smoothing</a></font>**\n**<br><font size=\"2\"><a href=\"#chap10\">10. Forecasting with Auto ARIMA</a></font>**"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap1\">1. Line plot of observations</div>"},{"metadata":{},"cell_type":"markdown","source":"A **line plot** is the a graph that displays data as points or check marks above a number line, showing the frequency of each value. It helps to identify:\n\n* **Outliers** or exagerated values\n\n* **Missing observations** (in most cases an error messages appear)\n\n* **Variation** of the data\n\n* **Seasonality and trend** of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Line plot\nfig, ax = plt.subplots(figsize=(15, 6))\nsns.lineplot(air_liquide['Date'], air_liquide['Open'] )\n\n# Formatting\nax.set_title('Open Price', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax.set_xlabel('Year', fontsize = 16, fontdict=dict(weight='bold'))\nax.set_ylabel('Price', fontsize = 16, fontdict=dict(weight='bold'))\nplt.tick_params(axis='y', which='major', labelsize=16)\nplt.tick_params(axis='x', which='major', labelsize=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font color=\"blue\" size=\"4\">What can we say about Air Liquide stocks ?</font>**\n\nThe data **don't seem to show seasonality** within each year. There is rather a **trend** than a **cyclic behavior**. We can see that the price is falling sharply early 2020 due to the COVID19."},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n----"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap2\">2. Treatment of missing values</div>"},{"metadata":{},"cell_type":"markdown","source":"Most of the time, the problem must be dealt with on a case-by-case basis. But there are different ways to input missing values:\n\n* **Forward filling and Backward Fill**: Forward filling means fill missing values with previous data. Backward filling means fill missing values with next data point.\n\n* **Linear interpolation**: Method of curve fitting using linear polynomials to construct new data points within the range of a discrete set of known data points.\n\n* **Quadratic interpolation**: Method of curve fitting using non linear polynomials to construct new data points within the range of a discrete set of known data points.\n\n* **Mean of nearest neighbors**\n\n* **Mean of seasonal**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check NaN \nair_liquide.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1 missing value in <font color='red'>'Open'</font> column and 134 misising values in <font color='red'>'Volume'</font> column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get index of the missing value in Open column\nindex_open_missing = air_liquide[pd.isnull(air_liquide['Open'])].index\nprint(list(index_open_missing)[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I replace the missing value by the Open price value of the previous day (backward fill)\nair_liquide['Open'] = air_liquide['Open'].fillna(method='bfill')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Percentage of missing values in Volume: {round(sum(pd.isnull(air_liquide['Volume']))/air_liquide.shape[0],2)}\\n\")\nprint(air_liquide[air_liquide['Volume']==0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing values of the <font color='red'>'Volume'</font> column correspond to 5% of observations. Plus, There is no observation for which the value is 0. Let's make the assumption that a missing value corresponds to 0 volume of stocks traded."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imputation\nimputer = SimpleImputer(strategy='constant', fill_value=0)\nair_liquide_plus = imputer.fit_transform(air_liquide)\n\n# Imputation removed column names; put them back\nimputed_air_liquide = pd.DataFrame(air_liquide_plus)\nimputed_air_liquide.columns = air_liquide.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace , by . so Volume can be converted as float\nimputed_air_liquide['Volume'] = imputed_air_liquide['Volume'].apply(lambda x : str(x))\nimputed_air_liquide['Volume'] = pd.to_numeric(imputed_air_liquide['Volume'].apply(lambda x : x.replace(',','',1)))\n\n# Convert object to numeric \nimputed_air_liquide['Open'] = pd.to_numeric(imputed_air_liquide['Open'])\nimputed_air_liquide['Closing_Price'] = pd.to_numeric(imputed_air_liquide['Closing_Price'])\nimputed_air_liquide['Daily_High'] = pd.to_numeric(imputed_air_liquide['Daily_High'])\nimputed_air_liquide['Daily_Low'] = pd.to_numeric(imputed_air_liquide['Daily_Low'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Daily Volume Lineplot\nfig, ax = plt.subplots(figsize=(15, 6))\nsns.lineplot(imputed_air_liquide['Date'], imputed_air_liquide['Volume'] )\n\nax.set_title('Daily Volume', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax.set_xlabel('Year', fontsize = 16, fontdict=dict(weight='bold'))\nax.set_ylabel('Price', fontsize = 16, fontdict=dict(weight='bold'))\nplt.tick_params(axis='y', which='major', labelsize=16)\nplt.tick_params(axis='x', which='major', labelsize=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is a lot of noise because I plotted on a daily scale. By chosing the mean of volume traded per month instead of considering the daily volume traded, the graphis likely to be more readable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregating the Time Series to a monthly scaled index\ny = imputed_air_liquide[['Date','Volume']].copy()\ny.set_index('Date', inplace=True)\ny.index = pd.to_datetime(y.index)\ny = y.resample('1M').mean()\ny['Date'] = y.index\n\n# Plot the Monthly Volume Lineplot\nfig, ax = plt.subplots(figsize=(15, 6))\nsns.lineplot(y['Date'], y['Volume'] )\n\nax.set_title('Monthly Volume', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax.set_xlabel('Year', fontsize = 16, fontdict=dict(weight='bold'))\nplt.tick_params(axis='y', which='major', labelsize=16)\nplt.tick_params(axis='x', which='major', labelsize=16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graph is still noisy but more readable. It's up to you to chose the best period interval and balance the readability/information compromise. \n\n* **Small period interval**: <font color='red'>Noisy signal</font>, <font color='green'>more information</font>\n* **Big period interval**: <font color='green'>Easy to read signal</font>, <font color='red'>loss of information</font>"},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n----"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap3\">3. Seasonality</div>"},{"metadata":{},"cell_type":"markdown","source":"**<font color=\"blue\" size=\"4\">What is seasonality ?</font>**\n\nIn Time Series data, seasonality is the presence of variations that **occur** at **specific regular intervals less than a year**, such as **weekly**, **monthly**, or **quarterly**."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"imputed_air_liquide['Year'] = imputed_air_liquide['Date'].dt.year\nimputed_air_liquide['Month'] = imputed_air_liquide['Date'].dt.month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputed_air_liquide['Year'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"variable = 'Open'\nfig, ax = plt.subplots(figsize=(15, 6))\n\nsns.lineplot(imputed_air_liquide['Month'], imputed_air_liquide[variable], hue = imputed_air_liquide['Year'])\nax.set_title('Seasonal plot of Price', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax.set_xlabel('Month', fontsize = 16, fontdict=dict(weight='bold'))\nax.set_ylabel('Price', fontsize = 16, fontdict=dict(weight='bold'))\nax.legend(labels = [str(2010+i) for i in range(11)], bbox_to_anchor=(1.1, 1.05))\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n\nsns.boxplot(imputed_air_liquide['Year'], imputed_air_liquide[variable], ax=ax[0])\nax[0].set_title('Year-wise Box Plot\\n(The Trend)', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax[0].set_xlabel('Year', fontsize = 16, fontdict=dict(weight='bold'))\nax[0].set_ylabel('Price', fontsize = 16, fontdict=dict(weight='bold'))\n\nsns.boxplot(imputed_air_liquide['Month'], imputed_air_liquide[variable], ax=ax[1])\nax[1].set_title('Month-wise Box Plot\\n(The Seasonality)', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax[1].set_xlabel('Month', fontsize = 16, fontdict=dict(weight='bold'))\nax[1].set_ylabel('Price', fontsize = 16, fontdict=dict(weight='bold'))\n\nfig.autofmt_xdate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Seasonal plot of Price**\n\nWe can see directly that 2019 was a very good year for the firm. The share price rose from 95 euros to 125 euros. This is the biggest increase over the last 10 years.\n\n**Year wise Box Plot**\n\nFew outliers, no seasonality and an uptrend for the last decade except for 2016. This small slowdown is characterized by the completion of the Airgas acquisition, in 2016, and its first contribution to the Group's performance. You can read the full 2016 report <a href = 'https://www.airliquide.com/sites/airliquide.com/files/2016/07/29/air-liquide-h1-2016-results.pdf'>here</a>\n\n**Month wise Box Plot**\n\nThe variance between each class is really small, it shows clearly that there is no effect of seasonality."},{"metadata":{},"cell_type":"markdown","source":"<img src = \"https://static.vecteezy.com/system/resources/previews/000/126/617/non_2x/vector-tool-box.jpg\">"},{"metadata":{},"cell_type":"markdown","source":"<font color='blue' size='4'>A powerful visualization</font>\n\n<br>On a whole, these plots allow you to:\n\n* Check **seasonal pattern** if it exists\n* Identity the **years in which the pattern changes**\n* Identify **large jumps or drops**\n\nIn the trend and seasonality boxplot:\n\n* Check instantaneously **trend and seasonality**\n* Check **outliers**"},{"metadata":{},"cell_type":"markdown","source":"<font color='blue' size='4'>A powerful visualization you can add to your toolbox</font>"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# Aggregating the Time Series to a monthly scaled index\ny = imputed_air_liquide[['Date','Open','Closing_Price']].copy()\ny.set_index('Date', inplace=True)\ny.index = pd.to_datetime(y.index)\ny = y.resample('1M').mean()\n\n# The magic\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\n\nmonth_plot(y['Open'], ax=ax[0]);\nax[0].set_ylabel('Open', fontsize = 16, fontdict=dict(weight='bold'))\n\nmonth_plot(y['Closing_Price'], ax=ax[1]);\nax[1].set_ylabel('Closing_Price', fontsize = 16, fontdict=dict(weight='bold'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph performs a groupby function to see more clearly the months of seasonality. We can see that there is no seasonality here. The mean of 10 years air liquide open and closing price per month is quite the same."},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap4\">4. Time series components</div>"},{"metadata":{},"cell_type":"markdown","source":"If we assume an additive decomposition, then we can write\n\n$$y_t = S_t + T_t + R_t$$\n\nwhere $y_t$ is the data, $S_t$ is the seasonal component, $T_t$ is the trend-cycle component and $R_t$ is the residual component, all at period $t$.\nAlso,for a multiplicative decomposition, we have\n\n$$y_t = S_t * T_t * R_t$$\n\nThe additive decomposition is the most appropriate if the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the level of the time series. When the variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional to the level of the time series, then a multiplicative decomposition is more appropriate. Multiplicative decompositions are common with economic time series.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregating the Time Series to a monthly scaled index\ny = imputed_air_liquide[['Date','Open']].copy()\ny.set_index('Date', inplace=True)\ny.index = pd.to_datetime(y.index)\ny = y.resample('1M').mean()\n\n# Setting rcparams\nrcParams['figure.figsize'] = 15, 12\nrcParams['axes.labelsize'] = 20\nrcParams['ytick.labelsize'] = 16\nrcParams['xtick.labelsize'] = 16\n\n# Using statistical tools of statsmodel library\ndecomposition = sm.tsa.seasonal_decompose(y, model='multiplicative', freq = 12)\ndecomp = decomposition.plot()\ndecomp.suptitle('Open decomposition', fontsize=22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='blue' size='4'>How to read this ?</font>\n\n<br>The three components are shown separately in the bottom three panels. These components can be multiplied together to reconstruct the data shown in the top panel. We can see that the seasonal component changes slowly over time. But this doesn't mean years far apart won't have different seasonal patterns.\n\nThe residual component shown in the bottom panel is what is left over when the seasonal and trend-cycle components have been subtracted from the data.\n\nIf the seasonal component is removed from the original data, the resulting values are the “seasonally adjusted” data. For an additive decomposition, the seasonally adjusted data are given by $y_t - S_t$, and for multiplicative data, the seasonally adjusted values are obtained using $y_t / S_t$."},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n----"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap5\">5. Stationarity</div>"},{"metadata":{},"cell_type":"markdown","source":"<font color='blue' size='4'>What describes best stationarity ?</font>\n\n<br>A stationary Time Series is one whose properties do not depend on the time at which the series is observed. Thus, time series with trends, or with seasonality, are not stationary \n\nA time series with cyclic behavior (but with no trend or seasonality) is stationary.\n\n<font color='blue' size='4'>What is stationarity used for ?</font>\n\n<br>Most statistical forecasting methods are designed to work on a stationary time series. The first step in the forecasting process is typically to do some transformation to convert a non-stationary series to stationary. Forecasting a stationary series is relatively easier and the forecasts are more reliable.\n\nWe know that linear regression works best if the predictors (X variables) are not correlated against each other. So, stationarizing the series solves this problem since it removes any persistent autocorrelation, thereby making the predictors (lags of the series) in the forecasting models nearly independent.\n\n<font color='blue' size='4'>Really interesting, but how to make a Time Series stationary ?</font>\n\n<br>Well, several ways exist:\n\n* Difference the series once or more times (subtracting the next value by the current value)\n\n* Take the log of the series (helps to stabilize the variance of a time series.)\n\n* Take the $n_{th}$ root of the series\n\n* Combinations of the above\n\nBut first, to test if a time series is stationary we can:\n\n* Look at the time plot.\n\n* Split the series into 2 parts and compute descriptive statistics. If they differ, then it is not stationary.\n\n* Perform statistical tests called Unit Root Tests like Augmented Dickey Fuller test (ADF Test), Kwiatkowski-Phillips-Schmidt-Shin — KPSS test (trend stationary), and Philips Perron test (PP Test).\n\nThe most commonly used is the ADF test, where the null hypothesis is that the time series possesses a unit root (or random walk with drift) and is non-stationary. So, if the P-Value in ADF test is less than the significance level (0.05), you reject the null hypothesis and the series is stationary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for stationarity\ndef adf_test(series, title=''):\n    \"\"\"\n    Pass in a time series and an optional title, returns an ADF report\n    \"\"\"\n    print('Augmented Dickey-Fuller Test: {}'.format(title))\n    # .dropna() handles differenced data\n    result = adfuller(series.dropna(),autolag='AIC') \n    \n    labels = ['ADF test statistic','p-value','# lags used','# observations']\n    out = pd.Series(result[0:4],index=labels)\n\n    for key,val in result[4].items():\n        out['critical value ({})'.format(key)]=val\n        \n    # .to_string() removes the line \"dtype: float64\"\n    print(out.to_string())          \n    \n    if result[1] <= 0.05:\n        print(\"Strong evidence against the null hypothesis\")\n        print(\"Reject the null hypothesis\")\n        print(\"Data has no unit root and is stationary\")\n    else:\n        print(\"Weak evidence against the null hypothesis\")\n        print(\"Fail to reject the null hypothesis\")\n        print(\"Data has a unit root and is non-stationary\")\n        \n# Aggregating the Time Series to a monthly scaled index\ny = imputed_air_liquide[['Date','Open']].copy()\ny.set_index('Date', inplace=True)\ny.index = pd.to_datetime(y.index)\ny = y.resample('1M').mean()\n        \nadf_test(y['Open'],title='') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=2, ncols=2,figsize=(15, 11))\n\ny['OpenDiff1'] = diff(y['Open'],k_diff=1)\ny['OpenDiff2'] = diff(y['Open'],k_diff=2)\ny['OpenDiff3'] = diff(y['Open'],k_diff=3)\n\ny['Open'].plot(title=\"Initial Data\",ax=ax[0][0]).autoscale(axis='x',tight=True);\ny['OpenDiff1'].plot(title=\"First Difference Data\",ax=ax[0][1]).autoscale(axis='x',tight=True);\ny['OpenDiff2'].plot(title=\"Second Difference Data\",ax=ax[1][0]).autoscale(axis='x',tight=True);\ny['OpenDiff3'].plot(title=\"Third Difference Data\",ax=ax[1][1]).autoscale(axis='x',tight=True);\n\nfig.autofmt_xdate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n----"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap6\">6. Autocorrelation</div>"},{"metadata":{},"cell_type":"markdown","source":"<font color='blue' size='4'>Correlation, Autocorrelation, Partial Autocorrelation ? It looks like all the same, what is the difference ?</font>\n\n<br>**Correlation** measures the extent of a linear relationship between two variables.\n\n**Autocorrelation** measures the linear relationship between lagged values of a time series, for example between $y_t$ and $y_{t-1}$. If a series is significantly autocorrelated, that means, the previous values of the series (lags) may be helpful in predicting the current value.\n\n![](http://)**Partial autocorrelations** measure the linear dependence of one variable after removing the effect of other variable(s) that affect both variables. That is, the partial autocorrelation at lag **k** is the autocorrelation between $y_t$ and $y_{t} + y_{t+k}$ that is not accounted for by lags 1 through k−1. \n\nWe essentially plot out the relationship between the previous day’s/month’s residuals versus the real values of the current day. In general, we expect the partial autocorrelation to drop off quite quickly."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2,figsize=(15, 6))\nautocorr = acf(imputed_air_liquide['Open'], nlags=60, fft=False)\nprint(autocorr)\n\nplot_acf(imputed_air_liquide['Open'].tolist(), lags=60, ax=ax[0], fft=False);\nplot_pacf(imputed_air_liquide['Open'].tolist(), lags=60, ax=ax[1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For autocorrelation, the y-axis is the value for the correlation between a value and its lag. The lag is on the x-axis. The zero-lag has a correlation of 1 because it correlates with itself perfectly. \n\nThe autocorrelation plot shows that most of the spikes are not statistically significant. This indicates that the returns are not highly correlated, as shown here."},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n-----"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap7\">7. Lag Scatter Plot</div>"},{"metadata":{},"cell_type":"markdown","source":"<font color='blue' size='4'>Definition</font>\n\n<br>A useful type of plot to explore the relationship between each observation and a lag of that observation is called the scatter plot.\nIt plots the observation at time t on the x-axis and the lag1 observation (t-1) on the y-axis.\n\n* If the points cluster along a diagonal line from the bottom-left to the top-right of the plot, it suggests a **positive correlation** relationship.\n\n* If the points cluster along a diagonal line from the top-left to the bottom-right, it suggests a **negative correlation** relationship.\n\n* Either relationship is good as they can be modeled.\n\n* More points tighter into the diagonal line suggests a stronger relationship and more spread from the line suggests a weaker relationship.\n\n* A ball in the middle or a spread across the plot suggests a weak or no relationship."},{"metadata":{"trusted":true},"cell_type":"code","source":"lag_plot(y['Open']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we have a **positive correlation** relationship."},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap8\">8. Moving Average (MA)</div>"},{"metadata":{},"cell_type":"markdown","source":"<font color='blue' size='4'>The maths behind MA</font>\n\n<br>Moving average is the estimation of the trend-cycle at time t, and is obtained by averaging the values of the time series within k periods of t. Observations that are nearby in time are also likely to be close in value. Therefore, the average eliminates some of the randomness in the data, leaving a smooth trend-cycle component.\n\nIt can be written as\n\n$$ \\hat{T_t} = \\frac{1}{m}\\sum_{j=-k}^{k}y_{t+j}$$\n\nwhere $m = 2k + 1$. That is, the estimate of the trend-cycle at time $t$ is obtained by averaging values of the time series within k periods of $t$."},{"metadata":{},"cell_type":"markdown","source":"* Smaller windows will lead to more noise.\n\n* It will always lag by the size of the window.\n\n* It will never reach the full peak or valley of the data due to the averaging.\n\n* It does not really inform you about possible future behavior, all it really does is describe trends in your data.\n\n* Extreme historical values can skew MA significantly."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Aggregating the Time Series to a monthly scaled index\ny = imputed_air_liquide[['Date','Open']].copy()\ny.set_index('Date', inplace=True)\ny.index = pd.to_datetime(y.index)\ny = y.resample('1M').mean()\n\ny['MA3'] = y.rolling(window=3).mean() \ny.plot(figsize=(15,6));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap9\">9. Exponential Smoothing</div>"},{"metadata":{},"cell_type":"markdown","source":"Exponential smoothing assigns **exponentially decreasing weights** for **newest** to **oldest** observations. In other words, the older the data, the less priority (“weight”) the data is given. Newer data is seen as more relevant and is assigned more weight. Smoothing parameters usually denoted by $\\alpha$ ( $0<\\alpha\\leq1$) determine the weights for observations. Exponential smoothing is usually used to make **short term forecasts**, as longer term forecasts using this technique **can be quite unreliable**."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting parameters value\nspan = 3\n# Weights of observations\nalpha = 2/(span+1)\n\n# Plot Simple exponential smoothing\ny['ES3'] = SimpleExpSmoothing(y['Open']).fit(smoothing_level = alpha, optimized = False).fittedvalues.shift(-1)\ny[['Open','ES3']].plot(figsize=(15,6));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='blue' size='4'>Simple, Double, Triple exponential Smoothing ?</font>\n\n<br>**Simple Exponential Smoothing** employs a **weighted moving average with exponentially decreasing weights**.\n\n**Double Exponential Smoothing** adds a second smoothing factor $\\beta$ ($0<\\beta\\leq1$) that addresses trends in the data. The benefit here is that the model can anticipate future increases or decreases where the level model would only work from recent calculations. We can also address different types of change (growth/decay) in the trend. If a time series displays a straight-line sloped trend, you would use an additive adjustment. If the time series displays an exponential (curved) trend, you would use a multiplicative adjustment.\n\n**Triple Exponential Smoothing** (also called the Multiplicative Holt-Winters), the method is usually more reliable for parabolic trends or data that shows trends and seasonality."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Double and Triple exponential smoothing\ny['DESmul3'] = ExponentialSmoothing(y['Open'], trend = 'add').fit().fittedvalues.shift(-1)\ny['TESmul3'] = ExponentialSmoothing(y['Open'], trend = 'add', seasonal = 'add', seasonal_periods = 12).fit().fittedvalues.shift(-1)\ny[['Open', 'TESmul3', 'DESmul3']].plot(figsize = (15,6));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n----"},{"metadata":{},"cell_type":"markdown","source":"# <div id=\"chap10\">10. Forecasting with Auto ARIMA</div>"},{"metadata":{},"cell_type":"markdown","source":"Now we are going to create an ARIMA model and will train it with the closing price of the stock on the train data. So let us split the data into training and test set and visualize it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reverse index so the dataframe is from oldest to newest values\nimputed_air_liquide = imputed_air_liquide.reindex(index=imputed_air_liquide.index[::-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split data into train and validation set 90/10\nair_liquide_train, air_liquide_val = imputed_air_liquide[:int(len(imputed_air_liquide)*0.9)], imputed_air_liquide[int(len(imputed_air_liquide)*0.9):]\n\n# Index disappeared, put them back\nair_liquide_val = air_liquide_val.set_index('Date', drop=False)\nair_liquide_train = air_liquide_train.set_index('Date', drop=False)\n\n# Line plot\nfig, ax = plt.subplots(figsize=(15, 6))\nsns.lineplot(air_liquide_train['Date'], air_liquide_train['Open'], color = 'black')\nsns.lineplot(air_liquide_val['Date'], air_liquide_val['Open'], color = 'blue')\n\n# Formatting\nax.set_title('Open Price', fontsize = 20, loc='center', fontdict=dict(weight='bold'))\nax.set_xlabel('Year', fontsize = 16, fontdict=dict(weight='bold'))\nax.set_ylabel('Price', fontsize = 16, fontdict=dict(weight='bold'))\nplt.tick_params(axis='y', which='major', labelsize=16)\nplt.tick_params(axis='x', which='major', labelsize=16)\nplt.legend(loc='upper right' ,labels = ('train', 'test'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its time to choose parameters p,q,d for ARIMA model. Last time we chose the value of p,d, and q by observing the plots of ACF and PACF but now we are going to use Auto ARIMA to get the best parameters without even plotting ACF and PACF graphs.\n\nAuto ARIMA: Automatically discover the optimal order for an ARIMA model.\nThe auto_arima function seeks to identify the most optimal parameters for an ARIMA model, and returns a fitted ARIMA model. This function is based on the commonly-used R function, forecast::auto.arima.\nThe auro_arima function works by conducting differencing tests (i.e., Kwiatkowski–Phillips–Schmidt–Shin, Augmented Dickey-Fuller or Phillips–Perron) to determine the order of differencing, d, and then fitting models within ranges of defined start_p, max_p, start_q, max_q ranges. If the seasonal optional is enabled, auto_arima also seeks to identify the optimal P and Q hyper- parameters after conducting the Canova-Hansen to determine the optimal order of seasonal differencing, D."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nmodel_autoARIMA = auto_arima(air_liquide_train['Open'])\n\n\"\"\", \n                             start_p = 0, \n                             start_q = 0,\n                             test = 'adf', # use adftest to find optimal 'd'\n                             max_p = 3,\n                             max_q = 3, # maximum p and q\n                             m = 7, # frequency of series\n                             seasonal = False,\n                             start_P = 0, \n                             D = 0, \n                             trace = True,\n                             error_action = 'ignore',  \n                             stepwise = True\n\"\"\"\n\nprint(model_autoARIMA.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In comment, you can see the parameters is used according to the documentation and the Time Series I have. However I wasn't really satisfied by the prediction (The best model was a ARMA(1,1) whose were decreasing whereas the trend is increasing since last 10 years.\nTbh, I got some difficulties tuning up this model, I guess <a href='https://alkaline-ml.com/pmdarima/tips_and_tricks.html'>this</a> could help, If you have any suggestions of a right way to do it, don't hesitate to add a comment, I would really appreciate."},{"metadata":{},"cell_type":"markdown","source":"Next, create an ARIMA model with provided optimal parameters p, d and q."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = ARIMA(air_liquide_train['Open'], order = (1, 1, 2))\n# disp=-1: no output\nfitted = model.fit(disp = -1)\nprint(fitted.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's start forecast the stock prices on the test dataset keeping 95% confidence level."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Forecast 260 next observations \nfc, se, conf = fitted.forecast(260, alpha=0.05)  # 95% confidence\nfc_series = pd.Series(fc, index=air_liquide_val.index)\nlower_series = pd.Series(conf[:, 0], index=air_liquide_val.index)\nupper_series = pd.Series(conf[:, 1], index=air_liquide_val.index)\n\nplt.figure(figsize=(12,5), dpi=100)\nplt.plot(air_liquide_train['Open'], label='training')\nplt.plot(air_liquide_val['Open'], color = 'blue', label='Actual Stock Price')\nplt.plot(fc_series, color = 'orange',label='Predicted Stock Price')\nplt.fill_between(lower_series.index, lower_series, upper_series, \n                 color='k', alpha=.10)\nplt.title('Air Liquide Stock Price Prediction')\nplt.xlabel('Date')\nplt.ylabel('Actual Stock Price')\nplt.legend(loc='upper left', fontsize=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see our model did quite handsomely. Let us also check the commonly used accuracy metrics to judge forecast results:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Report performance\nmse = mean_squared_error(air_liquide_val['Open'], fc)\nprint('MSE: '+str(mse))\nmae = mean_absolute_error(air_liquide_val['Open'], fc)\nprint('MAE: '+str(mae))\nrmse = math.sqrt(mean_squared_error(air_liquide_val['Open'], fc))\nprint('RMSE: '+str(rmse))\nmape = np.mean(np.abs(fc - air_liquide_val['Open'])/np.abs(air_liquide_val['Open']))\nprint('MAPE: '+str(mape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Around 11% MAPE(Mean Absolute Percentage Error) implies the model is about 89% accurate in predicting the test set observations. End to end, the value is close. However, the model could not predict the higher than normal growth for the past year, but it couldn't predict the covid19 crisis which involved a decreased in Open prices."},{"metadata":{},"cell_type":"markdown","source":"**<font size=\"2\"><a href=\"#summary\">Back to summary</a></font>**\n\n---"},{"metadata":{},"cell_type":"markdown","source":"# References\n\n* https://www.machinelearningplus.com/time-series/time-series-analysis-python/\n\n* https://www.udemy.com/python-for-time-series-data-analysis/\n\n* https://machinelearningmastery.com/time-series-data-visualization-with-python/\n\n* https://www.statisticshowto.com/exponential-smoothing/\n\n* https://otexts.com/fpp2/components.html"},{"metadata":{},"cell_type":"markdown","source":"<hr>\n<br>\n<div align='justify'><font color=\"#353B47\" size=\"4\">Thank you for taking the time to read this notebook. I hope that I was able to answer your questions or your curiosity and that it was quite understandable. <u>any constructive comments are welcome</u>. They help me progress and motivate me to share better quality content. I am above all a passionate person who tries to advance my knowledge but also that of others. If you liked it, feel free to <u>upvote and share my work.</u> </font></div>\n<br>\n<div align='center'><font color=\"#353B47\" size=\"3\">Thank you and may passion guide you.</font></div>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}