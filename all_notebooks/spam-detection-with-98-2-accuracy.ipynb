{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Importing Libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom nltk.corpus import stopwords\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading the dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv',encoding = 'latin-1')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dropping the empty unwanted column in next step we are changing the column names for better understanding then\nMapping the label to the numeric format for the classification.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis = 1,inplace = True)\ndata.rename(columns = {'v1':'Label','v2':'Text'},inplace = True)\ndata['Label'] = data.Label.map({'ham':0,'spam':1})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Plotting the Count of datas**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data.Label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking for duplicate and removing those duplicate values from the data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('Label').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We found that in **ham** there is 309(4825-4516)duplicates and in **spam** 94(747-653)duplicates"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping duplicate rows\ndata = data.drop_duplicates()\ndata.groupby('Label').describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding Length column which is depending upon the length of the text in the row"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Length'] = data['Text'].apply(len)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"plotting the hist depending upon the length of the Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10,6))\ndata[data.Label==0].Length.plot(bins = 40,kind = 'hist',color = 'green',label = 'ham messages',alpha = 0.7)\ndata[data.Label== 1].Length.plot(bins = 10,kind = 'hist',color = 'red',label = 'Spam messages',alpha = 0.8)\nplt.legend()\nplt.xlabel('Length')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['Label']==0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data['Label']==1].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above After Comparing the both we have find that Spam messages have more length than Ham messages"},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = SnowballStemmer(\"english\")\nstop = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing Stopwords\ndata['Clean_text'] = data['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n#Adding a column to show the length of the Text after removing the Stopwords\ndata['stopword_len'] = data['Clean_text'].apply(len)\n#Removing special character,digits,...\ndata['Clean_text']= data['Clean_text'].str.replace('''[,”“’''--./<“>-?1234567890-=\\|+_)(*&^%$#@!`~:\"{}]''','',case=False)\n#Removing the Links,websites,...\ndata['Clean_text']= data['Clean_text'].str.replace('http\\S+|www.\\S+', '', case=False)\n#Making the text to lower case\ndata['Clean_text']= data['Clean_text'].map(lambda x: x.lower())\n#Tokenization\ndata['Clean_text']= data.apply(lambda row: nltk.word_tokenize(row['Clean_text']), axis=1)\n#Stemming\ndata['Clean_text']= data['Clean_text'].apply(lambda row:[stemmer.stem(y) for y in row])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After cleaning the Text "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next step is to join the tokens together"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Final_text'] = ''\nfor i in data.index:\n    text = ' '.join(data['Clean_text'][i])\n    data['Final_text'][i] = text\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now drop the dirty columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['Text','Length','Clean_text','stopword_len'],inplace = True,axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data.Final_text\ny = data.Label","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the data into 80% for training and 20% for testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"size = round(len(y)*0.8)\nx_train = x[:size]\nx_test = x[size:]\ny_train = y[:size]\ny_test = y[size:]\nprint('x_train',x_train.shape)\nprint('x_test',x_test.shape)\nprint('y_train',y_train.shape)\nprint('y_test',y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now need to apply the machine learning algorithm.\n\n* 1st we need to take term frequency.\n* 2nd for high frequency value we need to reduce the frequency that is known as inverse document frequency"},{"metadata":{"trusted":true},"cell_type":"code","source":"vect = CountVectorizer()\nvect.fit(x_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fitting vocabulary"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = vect.fit_transform(x_train)\nx_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = vect.transform(x_test)\nx_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tsne_plot(x, y):\n\n    # Setting the plotting background\n    sns.set(style =\"whitegrid\")\n      \n    tsne = TSNE(n_components = 2, random_state = 0)\n      \n    # Reducing the dimensionality of the data\n    X_transformed = tsne.fit_transform(x)\n      \n    plt.figure(figsize =(10, 6))\n  \n    # Building the scatter plot\n    plt.scatter(X_transformed[np.where(y == 0), 0], \n                X_transformed[np.where(y == 0), 1],\n                marker ='o', linewidth =1,\n                alpha = 0.8, label ='hem')\n    plt.scatter(X_transformed[np.where(y == 1), 0],\n                X_transformed[np.where(y == 1), 1],\n                marker ='o', linewidth =1,\n                alpha = 0.8, label ='spam')\n  \n    # Specifying the location of the legend\n    plt.legend(loc ='best')\n\n\ntsne_plot(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB()\nnb.fit(x_train, y_train)\ny_pred_class = nb.predict(x_test)\nprint('Accuracy : ',metrics.accuracy_score(y_test, y_pred_class))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}