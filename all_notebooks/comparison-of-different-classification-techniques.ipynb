{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nCreated on Mon May 18 00:02:31 2020\n\n@author: Ankit\n\"\"\"\n\n\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat May  9 03:05:02 2020\n\n@author: Ankit\n\"\"\"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n\n#models\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier  \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import recall_score\n\n\n\n#Downloading dataset\ndata=pd.read_csv(r'../input/biomechanical-features-of-orthopedic-patients/column_3C_weka.csv')\nprint(data)\ntop_data=data.head(3)\ndata.info()\n\n# Categorical boolean mask\ncategorical_feature_mask = data.dtypes==object\n# filter categorical columns using mask and turn it into a list\ncategorical_cols = data.columns[categorical_feature_mask].tolist()\n# instantiate labelencoder object\nle = LabelEncoder()\n# apply le on categorical feature columns\ndata[categorical_cols] = data[categorical_cols].apply(lambda col: le.fit_transform(col))\ndata[categorical_cols].head(10)\nprint(data)\n\n#I split data on 30% in the test dataset, the remaining 70% - in the training dataset\ntrain, test, target, target_test = train_test_split(data[[\"pelvic_incidence\",\"pelvic_tilt\",\"lumbar_lordosis_angle\",\"sacral_slope\",\"pelvic_radius\",\"degree_spondylolisthesis\"]], data[[\"class\"]], test_size=0.3, random_state=1)\ntrain.info()\ntest.info()\n\n\n##CONFUSION MATRIX FUNCTION\ndef confusionmatrix(y_test,y_predict,x='name of model'):\n    cm=metrics.confusion_matrix(y_test,y_predict)\n    recall=print(round(recall_score(y_test, y_predict, average='macro')*100,2))\n    plt.figure(figsize=(10,7))\n    sn.heatmap(cm,annot=True)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(x)\n    return [ plt.show(),recall ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(train, target)\nacc_log = round(logreg.score(train, target) * 100, 2)\nacc_test_log = round(logreg.score(test, target_test) * 100, 2)\n\n\n# visualization using confusion matrix\nconfusionmatrix(target_test,logreg.predict(test),x='Logistic regression')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Support Vector Machines\nsvc = SVC()\nsvc.fit(train, target)\nacc_svc = round(svc.score(train, target) * 100, 2)\n\nacc_test_svc = round(svc.score(test, target_test) * 100, 2)\n\n# visualization using confusion matrix\nconfusionmatrix(target_test,svc.predict(test),x='support vector machine')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Decision Tree Classifier\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(train, target)\nacc_decision_tree = round(decision_tree.score(train, target) * 100, 2)\n\nacc_test_decision_tree = round(decision_tree.score(test, target_test) * 100, 2)\n\n# visualization using confusion matrix\nconfusionmatrix(target_test,decision_tree.predict(test),x='Decision Tree Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Random Forest\n\nacc_final_random_forest= []\nacc_test_final_random_forest= []\n\n\n\nfor m in range(80,126):\n    random_forest=RandomForestClassifier(n_estimators= m,random_state=1)\n    random_forest.fit(train, target)\n    acc_random_forest=round(random_forest.score(train,target) * 100, 2)\n    acc_final_random_forest.append(acc_random_forest)\n    acc_test_random_forest=round(random_forest.score(test, target_test) * 100, 2)\n    acc_test_final_random_forest.append(acc_test_random_forest)\n\n\n#here it is visible that max. accuracy of test data is at n = 80\nplt.figure()        \nl = range(80,126)\nfor j in range(len(l)):     \n    plt.plot( l, acc_test_final_random_forest)\n    plt.xlabel('Values of n_estimators')\n    plt.ylabel('Accuracy of test data')\n    plt.title('Variation of accuracy of prediction with different n_estimators values in random forest')\n\n#visualization using confusion matrix for maximum accuracy as it comes at n=80\nrandom_forest=RandomForestClassifier(n_estimators= 80,random_state=1)\nrandom_forest.fit(train, target)\nconfusionmatrix(target_test,random_forest.predict(test),x='Random Forest')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Ridge Classifier\n\nridge_classifier = RidgeClassifier()\nridge_classifier.fit(train, target)\nacc_ridge_classifier = round(ridge_classifier.score(train, target) * 100, 2)\n\n\nacc_test_ridge_classifier = round(ridge_classifier.score(test, target_test) * 100, 2)\n\n\n\n# visualization using confusion matrix\nconfusionmatrix(target_test,ridge_classifier.predict(test),x='Ridge Classifier')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##KNN CLASSIFIER\n\nacc_knn_classifier= np.empty((10, 1))\n\n\nacc_test_knn_classifier= np.empty((10, 1))\n\n\nfor i in range(0,10):\n      knn =KNeighborsClassifier(n_neighbors=i+1)\n      knn.fit(train, target)\n      acc_knn_classifier[i,:]=round(knn.score(train, target) * 100, 2)\n      acc_test_knn_classifier[i,:]= round(knn.score(test, target_test) * 100, 2)\n      \n      \n      \n    \n     \n#here it is visible that max. accuracy of test data is with value of n=9   \nplt.figure()        \nl = range(1,11)\nfor j in range(len(l)):     \n    plt.plot( l, acc_test_knn_classifier)\n    plt.xlabel('Values of n_neighbors')\n    plt.ylabel('Accuracy of test data')\n    plt.title('Variation of accuracy of prediction with different n values in knn method')\n    \n    \n#visualization using confusion matrix for maximum accuracy as it comes at n_neighbors=9\nknn =KNeighborsClassifier(n_neighbors=9)\nknn.fit(train, target)\nconfusionmatrix(target_test,knn.predict(test),x='KNN CLASSIFIER')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## K-mean CLASSIFIER\n\nacc_k_mean_classifier= np.empty((10, 1))\nacc_test_k_mean_classifier= np.empty((10, 1))\n\nfor p in range(0,10):\n      k_mean =KMeans(n_clusters=p+1)\n      k_mean.fit(train, target)\n      acc_k_mean_classifier[p,:]=round(knn.score(train, target) * 100, 2)\n      acc_test_k_mean_classifier[p,:]= round(knn.score(test, target_test) * 100, 2)\n      \n     \n#here we can see the number of cluster are not able to affect the accuracy of test data   \nplt.figure()       \nm = range(1,11)\nfor k in range(len(m)):     \n    plt.plot( m, acc_test_k_mean_classifier)\n    plt.xlabel('Values of n_clusters')\n    plt.ylabel('Accuracy of test data')\n    plt.title('Variation of accuracy of prediction with different n values in k-mean')\n    \n#visualization using confusion matrix for maximum accuracy as it comes at n_neighbors=9\nk_mean =KMeans(n_clusters=2)\nk_mean.fit(train, target)\nconfusionmatrix(target_test,k_mean.predict(test),x='K-mean CLASSIFIER')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##DETAILS\ndetails_data=data.describe()\nprint(details_data)\n\n##Models evaluation\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines','Decision Tree Classifier', 'Random Forest','Ridge Classifier','KNN CLASSIFIER','K-mean CLASSIFIER'],\n    \n    'Score_train': [acc_log, acc_svc,acc_decision_tree, acc_final_random_forest[0],acc_ridge_classifier,acc_test_knn_classifier[8,:],acc_k_mean_classifier[1,:]],\n    'Score_test': [acc_test_log, acc_test_svc,acc_test_decision_tree, acc_test_final_random_forest[0], acc_test_ridge_classifier,acc_test_knn_classifier[8,:],acc_test_k_mean_classifier[1,:]]})\n\n\nmodels['Score_diff'] = abs(models['Score_train'] - models['Score_test'])\nmodels.sort_values(by=['Score_diff'], ascending=True)\nprint(models)\n\n\n## Final Plot\nplt.figure()\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['Score_train'], label = 'Score_train')\nplt.plot(xx, models['Score_test'], label = 'Score_test')\nplt.legend()\nplt.title('Score of 7 popular models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('Score, %')\nplt.xticks(xx, rotation='vertical')\nplt.show()\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}