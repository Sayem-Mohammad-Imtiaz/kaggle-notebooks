{"cells":[{"metadata":{"_cell_guid":"8bec90e6-23f2-4aae-8b4a-e9ef999391b7","_uuid":"ae07d14dcb75d8188c49d57cdc54fc36f3397d02","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv('../input/W1data.csv')\ndf.head() ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5212493d-23b7-40d4-b1d6-6ce70a3631ca","_uuid":"17597c0a6ba90e709149929aeca1999e2e74df32","collapsed":true,"trusted":true},"cell_type":"code","source":"# Get labels\ny = df[['Cultivar 1', 'Cultivar 2', 'Cultivar 3']].values\n\n# Get inputs; we define our x and y here.\nX = df.drop(['Cultivar 1', 'Cultivar 2', 'Cultivar 3'], axis = 1)\nX.shape, y.shape # Print shapes just to check\nX = X.values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b48153f-dd53-4bff-b04f-e6dfb036a59b","_uuid":"6a2e1f0befbdb14f9085f58718ab64d9887e08a2","collapsed":true,"trusted":true},"cell_type":"code","source":"#First we are importing all the libraries\n\n# Package imports\n# Matplotlib is a matlab like plotting library\nimport matplotlib\nimport matplotlib.pyplot as plt\n# SciKitLearn is a useful machine learning utilities library\nimport sklearn\n# The sklearn dataset module helps generating datasets\nimport sklearn.datasets\nimport sklearn.linear_model\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"90023c7a-8059-4117-98b3-82ceb3e5877a","_uuid":"de660f4b64992b03558fc14ee176479b4b7afc29","trusted":true},"cell_type":"code","source":"# Now we define all our functions\n\ndef softmax(z):\n    #Calculate exponent term first\n    exp_scores = np.exp(z)\n    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n\ndef softmax_loss(y,y_hat):\n    # Clipping value\n    minval = 0.000000000001\n    # Number of samples\n    m = y.shape[0]\n    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n    return loss\n\ndef loss_derivative(y,y_hat):\n    return (y_hat-y)\n\ndef tanh_derivative(x):\n    return (1 - np.power(x, 2))\n\n# This is the forward propagation function\ndef forward_prop(model,a0):\n    \n    #Start Forward Propagation\n    \n    # Load parameters from model\n    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n    \n    # Do the first Linear step \n    # Z1 is the input layer x times the dot product of the weights + our bias b\n    z1 = a0.dot(W1) + b1\n    \n    # Put it through the first activation function\n    a1 = np.tanh(z1)\n    \n    # Second linear step\n    z2 = a1.dot(W2) + b2\n    \n    # Second activation function\n    a2 = np.tanh(z2)\n    \n    #Third linear step\n    z3 = a2.dot(W3) + b3\n    \n    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n    a3 = softmax(z3)\n    \n    #Store all results in these values\n    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n    return cache\n\n# This is the BACKWARD PROPAGATION function\ndef backward_prop(model,cache,y):\n\n    # Load parameters from model\n    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n    \n    # Load forward propagation results\n    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n    \n    # Get number of samples\n    m = y.shape[0]\n    \n    # Calculate loss derivative with respect to output\n    dz3 = loss_derivative(y=y,y_hat=a3)\n\n    # Calculate loss derivative with respect to second layer weights\n    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n    \n    # Calculate loss derivative with respect to second layer bias\n    db3 = 1/m*np.sum(dz3, axis=0)\n    \n    # Calculate loss derivative with respect to first layer\n    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n    \n    # Calculate loss derivative with respect to first layer weights\n    dW2 = 1/m*np.dot(a1.T, dz2)\n    \n    # Calculate loss derivative with respect to first layer bias\n    db2 = 1/m*np.sum(dz2, axis=0)\n    \n    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n    \n    dW1 = 1/m*np.dot(a0.T,dz1)\n    \n    db1 = 1/m*np.sum(dz1,axis=0)\n    \n    # Store gradients\n    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n    return grads\n\n#TRAINING PHASE\ndef initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n    # First layer weights\n    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n    \n    # First layer bias\n    b1 = np.zeros((1, nn_hdim))\n    \n    # Second layer weights\n    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n    \n    # Second layer bias\n    b2 = np.zeros((1, nn_hdim))\n    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n    b3 = np.zeros((1,nn_output_dim))\n    \n    \n    # Package and return model\n    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n    return model\n\ndef update_parameters(model,grads,learning_rate):\n    # Load parameters\n    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n    \n    # Update parameters\n    W1 -= learning_rate * grads['dW1']\n    b1 -= learning_rate * grads['db1']\n    W2 -= learning_rate * grads['dW2']\n    b2 -= learning_rate * grads['db2']\n    W3 -= learning_rate * grads['dW3']\n    b3 -= learning_rate * grads['db3']\n    \n    # Store and return parameters\n    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n    return model\ndef predict(model, x):\n    # Do forward pass\n    c = forward_prop(model,x)\n    #get y_hat\n    y_hat = np.argmax(c['a3'], axis=1)\n    return y_hat\ndef calc_accuracy(model,x,y):\n    # Get total number of examples\n    m = y.shape[0]\n    # Do a prediction with the model\n    pred = predict(model,x)\n    # Ensure prediction and truth vector y have the same shape\n    pred = pred.reshape(y.shape)\n    # Calculate the number of wrong examples\n    error = np.sum(np.abs(pred-y))\n    # Calculate accuracy\n    return (m - error)/m * 100\nlosses = []\ndef train(model,X_,y_,learning_rate, epochs=20000, print_loss=False):\n    # Gradient descent. Loop over epochs\n    for i in range(0, epochs):\n\n        # Forward propagation\n        cache = forward_prop(model,X_)\n        #a1, probs = cache['a1'],cache['a2']\n        # Backpropagation\n        \n        grads = backward_prop(model,cache,y_)\n        # Gradient descent parameter update\n        # Assign new parameters to the model\n        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n    \n        # Pring loss & accuracy every 100 iterations\n        if print_loss and i % 100 == 0:\n            a3 = cache['a3']\n            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n            y_hat = predict(model,X_)\n            y_true = y_.argmax(axis=1)\n            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n    return model\nnp.random.seed(0)\n# This is what we return at the end\nmodel = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\nmodel = train(model,X,y,learning_rate=0.07,epochs=4500,print_loss=True)\nplt.plot(losses)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cf85a92b-2bc6-4b57-8cd6-53b523add48a","_uuid":"6aca14a72b394b8bc0d83fae7134fd4f8bcbfce2","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}