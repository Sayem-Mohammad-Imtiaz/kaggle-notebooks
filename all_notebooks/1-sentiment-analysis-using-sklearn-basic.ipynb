{"cells":[{"metadata":{"_uuid":"6c855a626abe3c046a31a0d6b1cb72b7e89f791b"},"cell_type":"markdown","source":"# Nafisur Rahman\nnafisur21@gmail.com<br>\nhttps://www.linkedin.com/in/nafisur-rahman"},{"metadata":{"_uuid":"150f8a7e2f90802b9aa5e0e82e1b83344ce9a6ee"},"cell_type":"markdown","source":"# Sentiment Analysis\nFinding the sentiment (positive or negative) from IMDB movie reviews."},{"metadata":{"_uuid":"10c4660127c0d5da0bffeedc6e695091bf6c267d"},"cell_type":"markdown","source":"## About this Project\nThis is a kaggle project based on kaggle dataset of \"Bag of Words Meets Bags of Popcorn\". Original dataset can be found from stanford website http://ai.stanford.edu/~amaas/data/sentiment/.<br>\nThe labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. <br>\n* id - Unique ID of each review\n* sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n* review - Text of the review"},{"metadata":{"_uuid":"fb2e3f1b0995fd9ca4535f43981f457263338342"},"cell_type":"markdown","source":"## A. Loading libraries and Dataset"},{"metadata":{"_uuid":"84bfea32c730a7f732f7ec2b3fff08f95deb5cac"},"cell_type":"markdown","source":"### Importing Packages"},{"metadata":{"trusted":true,"_uuid":"7254f803e90569095a7e664eeed6a306de3afaa0"},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":1,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"618d8f780237f29a3a966690cc1389d0f470f39b"},"cell_type":"code","source":"import nltk\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport random\nimport matplotlib.pyplot as plt\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom nltk.stem import SnowballStemmer\nstemmer=SnowballStemmer('english')\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom nltk.tokenize import word_tokenize\n\n%matplotlib inline","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d2e6e5ea34cd6409317b2cc5b3a7a159f5b92434"},"cell_type":"markdown","source":"### Loading the dataset"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"5b0f6d42fc23a225f6ce860572fb0c548688e8f6"},"cell_type":"code","source":"raw_data_train=pd.read_csv('../input/labeledTrainData.tsv',sep='\\t')\nraw_data_test=pd.read_csv('../input/testData.tsv',sep='\\t')","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"ba542dccec382575a4a30a8494a9c75a2b28370f"},"cell_type":"markdown","source":"### Basic visualization of dataset"},{"metadata":{"trusted":true,"_uuid":"974f56b77c852772b002e2211d86d3e3b65f7ccc"},"cell_type":"code","source":"print(raw_data_train.shape)\nprint(raw_data_test.shape)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11b3f2f6b1b316ef386e3919bdccfcc69d337905"},"cell_type":"code","source":"raw_data_train.info()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f232e4219781cc4827c18ed717cfae34f914d7a"},"cell_type":"code","source":"raw_data_test.info()","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"480729785418b1dc61b5679f80a9fd85825c5a0b"},"cell_type":"code","source":"raw_data_train.head()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1e09cd8840c8b9589dc9769decb0dd51734ca21"},"cell_type":"code","source":"raw_data_test.head()","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e808a6f37d0722f28dbc8a371d0812267120e836"},"cell_type":"code","source":"raw_data_train['review'][0]","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"3ea1fdcb87953dae9ff9ed69297f732891a87e99"},"cell_type":"markdown","source":"## B. Data Cleaning and Text Preprocessing"},{"metadata":{"_uuid":"9745c2401e6ec0e2187b3a93b02a69c84c30ead9"},"cell_type":"markdown","source":"Removing tags and markup"},{"metadata":{"trusted":true,"_uuid":"7822dc58703926e6b37e6e9ddaca1a9b82a8b302"},"cell_type":"code","source":"from bs4 import BeautifulSoup\nsoup=BeautifulSoup(raw_data_train['review'][0],'lxml').text\nsoup","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"49cd0a8470c67976dd20b4b66bdcc7fc556b8c62"},"cell_type":"markdown","source":"Removing non-letters"},{"metadata":{"trusted":true,"_uuid":"bfb09e8ee0df22529aed1edcd0235422eafb369c"},"cell_type":"code","source":"import re\nre.sub('[^a-zA-Z]',' ',raw_data_train['review'][0])","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"78cf1a6202cc469ee0179820e94bde5832b97e03"},"cell_type":"markdown","source":"Word tokenization"},{"metadata":{"trusted":true,"_uuid":"023e396badf307c7ca471ec6cdd3af01aa1fd2af"},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nword_tokenize((raw_data_train['review'][0]).lower())[0:20]","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"b70356975a7b5a00a72c6aecb899800cb38f4cf5"},"cell_type":"markdown","source":"Removing stopwords"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"8f793626a7b805b6f3117353863dd42b80a98c42"},"cell_type":"code","source":"from nltk.corpus import stopwords\nfrom string import punctuation\nCstopwords=set(stopwords.words('english')+list(punctuation))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2848119bfae4594f66bf18ab95e2beef804d6f6b"},"cell_type":"code","source":"[w for w in word_tokenize(raw_data_train['review'][0]) if w not in Cstopwords][0:20]","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"2ab12d90878b86e7620afb5f49e900e3098b44c5"},"cell_type":"markdown","source":"### Defining a function that will perform the preprocessing task at one go"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"a4e61aa9ae6d8472281897b42b7921460ef43acb"},"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer\nstemmer=SnowballStemmer('english')\nfrom nltk.stem import WordNetLemmatizer\nlemma=WordNetLemmatizer()\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nCstopwords=set(stopwords.words('english')+list(punctuation))\ndef clean_review(df):\n    review_corpus=[]\n    for i in range(0,len(df)):\n        review=df[i]\n        review=BeautifulSoup(review,'lxml').text\n        review=re.sub('[^a-zA-Z]',' ',review)\n        review=str(review).lower()\n        review=word_tokenize(review)\n        #review=[stemmer.stem(w) for w in word_tokenize(str(review).lower()) if w not in Cstopwords]\n        review=[lemma.lemmatize(w) for w in review ]\n        review=' '.join(review)\n        review_corpus.append(review)\n    return review_corpus","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e51433436316b2051508cf65c7f8767e359dbb51"},"cell_type":"code","source":"df=raw_data_train['review']\nclean_train_review_corpus=clean_review(df)\nclean_train_review_corpus[0]","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"335097d1c647de4991ccfa1c90f57bbf18f038eb"},"cell_type":"code","source":"df1=raw_data_test['review']\nclean_test_review_corpus=clean_review(df1)\nclean_test_review_corpus[0]","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df8debb28f4f70807b1fdf5dc424d11d8e50b2f2"},"cell_type":"code","source":"df=raw_data_train\ndf['clean_review']=clean_train_review_corpus\ndf.head()","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"c1f7c79487ed661da70a42a9620e6967c566d93d"},"cell_type":"markdown","source":"## C. Creating Features\n1. Bag of Words (CountVectorizer)\n2. tf\n3. tfidf"},{"metadata":{"_uuid":"9849d1b1b158705a5744baca8e4a58429ef9a6a9"},"cell_type":"markdown","source":"### 1. Bag of Words model"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"5917d01043067b9c70c5bb12323d608f366186e7"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"2b8da80485f70a1a793633ae69d55068519bfd6f"},"cell_type":"markdown","source":"To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 5000 most frequent words (remembering that stop words have already been removed)."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"97f198de89412e07ba3e150d04dbf88abd37d932"},"cell_type":"code","source":"cv=CountVectorizer(max_features=20000,min_df=5,ngram_range=(1,2))","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"487eb74cdab15331776c97938970fbee0fc19be7"},"cell_type":"code","source":"X1=cv.fit_transform(df['clean_review'])\nX1.shape","execution_count":21,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"c158287329d77a5a26cda0f3da48cd8dce6cc1d4"},"cell_type":"code","source":"train_data_features = X1.toarray()","execution_count":22,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b8728909f37ca21fa507074acd491532c43c395"},"cell_type":"code","source":"y=df['sentiment'].values\ny.shape","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"052a1140c18a847ec1e0efedb80efb76622ef474"},"cell_type":"markdown","source":"## D. Machine Learning"},{"metadata":{"_uuid":"2a28ac4e23a891f8b887d2939078e3f556e8d82d"},"cell_type":"markdown","source":"#### Splitting data into Training and Test set"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"7c3ac8a1fc4aa9f41420074422e5492d19fed9a2"},"cell_type":"code","source":"X=train_data_features","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0676bf17122181121cf58f28820d8b04c7634bd"},"cell_type":"code","source":"# train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bab2baa25bd7221b19bbd40f6ae2011b2327ba2"},"cell_type":"code","source":"# average positive reviews in train and test\nprint('mean positive review in train : {0:.3f}'.format(np.mean(y_train)))\nprint('mean positive review in test : {0:.3f}'.format(np.mean(y_test)))","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"f44d2000595bab738bfad3a0c943d2039e605d7d"},"cell_type":"markdown","source":"### 1. Naive Bayes Classifier"},{"metadata":{"trusted":true,"_uuid":"0e132830babeaaf07b2135f2698fc6d9921088c9"},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nmodel_nb=MultinomialNB()\nmodel_nb.fit(X_train,y_train)\ny_pred_nb=model_nb.predict(X_test)\nprint('accuracy for Naive Bayes Classifier :',accuracy_score(y_test,y_pred_nb))\nprint('confusion matrix for Naive Bayes Classifier:\\n',confusion_matrix(y_test,y_pred_nb))","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"295ec5a71a70ffe1d7de77a28a4dafe2dc445eb5"},"cell_type":"code","source":"# get the feature names as numpy array\nfeature_names = np.array(cv.get_feature_names())\n\n# Sort the coefficients from the model\nsorted_coef_index = model_nb.coef_[0].argsort()\n\n# Find the 10 smallest and 10 largest coefficients\n# The 10 largest coefficients are being indexed using [:-11:-1] \n# so the list returned is in order of largest to smallest\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"3859aef6a725ec4df2a1e134faa5688641787bde"},"cell_type":"markdown","source":"### 2. Random Forest"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"258eeeb6000f3820f00ad62ae7f89afaa867dea3"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":29,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"ee987f20bcf71c1272a68235aaa7e52938637464"},"cell_type":"code","source":"model_rf=RandomForestClassifier(random_state=0)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f79446201f9f2394ba682738d4322df97d9c5e5"},"cell_type":"code","source":"# %%time\n# from sklearn.model_selection import GridSearchCV\n# parameters = {'n_estimators':[100,200],'criterion':['entropy','gini'],\n#               'min_samples_leaf':[2,5,7],\n#               'max_depth':[5,6,7]\n#                }\n# grid_search = GridSearchCV(estimator = model_rf,\n#                            param_grid = parameters,\n#                            scoring = 'accuracy',\n#                            cv = 10,\n#                            n_jobs = -1)\n# grid_search = grid_search.fit(X_train, y_train)\n# best_accuracy = grid_search.best_score_\n# best_parameters = grid_search.best_params_\n# print('Best Accuracy :',best_accuracy)\n# print('Best parameters:\\n',best_parameters)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb4b1cbb5d0b23725352029f1352c8024e58418c"},"cell_type":"code","source":"%%time\nmodel_rf=RandomForestClassifier()\nmodel_rf.fit(X_train,y_train)\ny_pred_rf=model_rf.predict(X_test)\nprint('accuracy for Random Forest Classifier :',accuracy_score(y_test,y_pred_rf))\nprint('confusion matrix for Random Forest Classifier:\\n',confusion_matrix(y_test,y_pred_rf))","execution_count":32,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"f762c42c28e2fd239c4c8b60cbea4d80f76fe139"},"cell_type":"markdown","source":"### 3. Logistic Regression"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"1250affa9423c0d99b787c3b1d126971ca45fa4d"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression as lr","execution_count":33,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"0569e280dfda58e6f20cb2eeeb8a529dc98f1944"},"cell_type":"code","source":"model_lr=lr(random_state=0)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05180db2afcbf53928ed1264af1a37c2253c1366"},"cell_type":"code","source":"%%time\nmodel_lr=lr(penalty='l2',C=1.0,random_state=0)\nmodel_lr.fit(X_train,y_train)\ny_pred_lr=model_lr.predict(X_test)\nprint('accuracy for Logistic Regression :',accuracy_score(y_test,y_pred_lr))\nprint('confusion matrix for Logistic Regression:\\n',confusion_matrix(y_test,y_pred_lr))\nprint('F1 score for Logistic Regression :',f1_score(y_test,y_pred_lr))\nprint('Precision score for Logistic Regression :',precision_score(y_test,y_pred_lr))\nprint('recall score for Logistic Regression :',recall_score(y_test,y_pred_lr))\nprint('AUC: ', roc_auc_score(y_test, y_pred_lr))","execution_count":36,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75e5219b04ef825563e59f61694b14fa9cc86b8b"},"cell_type":"code","source":"# get the feature names as numpy array\nfeature_names = np.array(cv.get_feature_names())\n\n# Sort the coefficients from the model\nsorted_coef_index = model_lr.coef_[0].argsort()\n\n# Find the 10 smallest and 10 largest coefficients\n# The 10 largest coefficients are being indexed using [:-11:-1] \n# so the list returned is in order of largest to smallest\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","execution_count":37,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"1627c4390a1f8663f5b43cb5070adc3fab7141fe"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}