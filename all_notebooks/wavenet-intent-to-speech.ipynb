{"cells":[{"metadata":{"_uuid":"5514ff7b-bf9f-4a05-9f3e-6504bc393d32","_cell_guid":"7d7e7541-9b7b-4937-8b7c-6a9151df3ccf","trusted":true},"cell_type":"code","source":"## Using Google's WaveNet to create a\n## user-intent-to-speech call-center engine\n\nimport pandas as pd \nimport numpy as np\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nfrom keras.metrics import top_k_categorical_accuracy\ndef top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom glob import glob\nimport gc\ngc.enable()\ndef get_available_gpus():\n    from tensorflow.python.client import device_lib\n    local_device_protos = device_lib.list_local_devices()\n    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n\n## IMPT!: spectogram and speech PRE-PROCESSED prior\n#TRAIN_PATH = '../input/train-npy/'\n#TEST_PATH = '../input/test-npy/'\n#TRAIN_PATH = '../input/train-5000-npy/'\n#TEST_PATH = '../input/test-5000-npy/'\nTRAIN_PATH = '../input/train-npy-10000/'\nTEST_PATH = '../input/test-npy-10000/'\n\n# male\n#train_X_male = np.load(TRAIN_PATH + 'train_X_male.npy')\n#train_Y_male = np.load(TRAIN_PATH + 'train_Y_male.npy')\n#test_X_male = np.load(TEST_PATH + 'test_X_male.npy')\n#test_Y_male = np.load(TEST_PATH + 'test_Y_male.npy')\n\n#train_X_male = np.load(TRAIN_PATH + 'train_X_male_5000.npy')\n#train_Y_male = np.load(TRAIN_PATH + 'train_Y_male_5000.npy')\n#test_X_male = np.load(TEST_PATH + 'test_X_male_5000.npy')\n#test_Y_male = np.load(TEST_PATH + 'test_Y_male_5000.npy')\n\ntrain_X_male = np.load(TRAIN_PATH + 'train_X_male_10000.npy')\ntrain_Y_male = np.load(TRAIN_PATH + 'train_Y_male_10000.npy')\ntest_X_male = np.load(TEST_PATH + 'test_X_male_10000.npy')\ntest_Y_male = np.load(TEST_PATH + 'test_Y_male_10000.npy')\n\n\n# female\n#train_X_female = np.load(TRAIN_PATH + 'train_X_female.npy')\n#train_Y_female = np.load(TRAIN_PATH + 'train_Y_female.npy')\n#test_X_female = np.load(TEST_PATH + 'test_X_female.npy')\n#test_Y_female = np.load(TEST_PATH + 'test_Y_female.npy')\n\n#train_X_female = np.load(TRAIN_PATH + 'train_X_female_5000.npy')\n#train_Y_female = np.load(TRAIN_PATH + 'train_Y_female_5000.npy')\n#test_X_female = np.load(TEST_PATH + 'test_X_female_5000.npy')\n#test_Y_female = np.load(TEST_PATH + 'test_Y_female_5000.npy')\n\ntrain_X_female = np.load(TRAIN_PATH + 'train_X_female_10000.npy')\ntrain_Y_female = np.load(TRAIN_PATH + 'train_Y_female_10000.npy')\ntest_X_female = np.load(TEST_PATH + 'test_X_female_10000.npy')\ntest_Y_female = np.load(TEST_PATH + 'test_Y_female_10000.npy')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPT: make sure train_X and train_Y dimensions match\nprint(train_X_male)\nprint(train_Y_male)\nprint(train_X_male.shape)\nprint(train_Y_male.shape)\nprint(train_X_female.shape)\nprint(train_Y_female.shape)\n# IMPT: check duration match for train X and Y\nassert(train_X_male.shape[1] == train_Y_male.shape[1])\nassert(train_X_female.shape[1] == train_Y_female.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X = train_X_female\ntrain_y = train_Y_female","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## IMPT!: WaveNet Mu-Law Quantize \n\n## Quantize and Categorize audio output y in order to use Softmax according to paper\n## Softmax final activation according to paper performs better than linear \n\n# IMPT: need to keep MIN_VALUE and MAX_VALUE constants for inverse later! \nMIN_Y = np.min(train_y)\nMAX_Y = np.max(train_y) \n\n# mu-law transformation according to paper \n#train_y = 2 * (train_y - MIN_Y) / (MAX_Y - MIN_Y) - 1 # normalize to -1 and 1\n#assert(np.min(train_y) == -1 and np.max(train_y) == 1) # check\n#mu = 255\n#train_y = np.sign(train_y) * np.log(1 + mu * np.abs(train_y)) / (np.log(1 + mu)) # mu-law transformation\n\n# quantize from 0 to 256 categorical labels acording to paper \n#NO_BINS = 256\nNO_BINS = 512\n_, bin_labels = np.histogram(train_y, NO_BINS-1, range=(MIN_Y, MAX_Y))\nquant_y = np.empty((train_y.shape[0], train_y.shape[1])) \nfor i in range(train_y.shape[0]):\n    # IMPT: return bin labels of range(256)\n    quant_y[i,:] = pd.cut(train_y[i,:], NO_BINS, right=False, labels=range(NO_BINS))    \nassert(len(np.unique(quant_y)) == NO_BINS) # check\ntrain_y = quant_y # assign \n\n# one hot encoding to *2-dimensional* 256 categorical for softmax\n# IMPT!: ouput only kept at 2-dimesional: (num_data * audio_duration x 256 one hot categories)\ntrain_y = to_categorical(train_y) # assign \nassert(np.sum(train_y[0]) != 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Conv1D, Input, Activation, AveragePooling1D, Add, Multiply, GlobalAveragePooling1D\nfrom keras.layers import AveragePooling2D, GlobalAveragePooling2D\nfrom keras.models import Model\n\n# parameters\nn_filters = 64\ndilation_depth = 8\n# softmax performs better according to paper\n# but need to perform mu-law quantization to 256 categoricals according to paper \nfinal_activation = 'softmax' \n#final_activation = 'linear'\nscale_ratio = 1\nkernel_size = 2\npool_size_1 = 4\npool_size_2 = 8\nSTROKE_COUNT = 196\nTRAIN_SAMPLES = 750\nVALID_SAMPLES = 75\nTEST_SAMPLES = 50\n\ninput_shape = train_X.shape[1:]\noutput_shape = train_y.shape[1:]\n\ndef residual_block(x, i):\n    tanh_out = Conv1D(n_filters, \n                      kernel_size, \n                      dilation_rate = kernel_size**i, \n                      padding='causal', \n                      name='dilated_conv_%d_tanh' % (kernel_size ** i), \n                      activation='tanh'\n                      )(x)\n    sigm_out = Conv1D(n_filters, \n                      kernel_size, \n                      dilation_rate = kernel_size**i, \n                      padding='causal', \n                      name='dilated_conv_%d_sigm' % (kernel_size ** i), \n                      activation='sigmoid'\n                      )(x)\n    z = Multiply(name='gated_activation_%d' % (i))([tanh_out, sigm_out])\n    skip = Conv1D(n_filters, 1, name='skip_%d'%(i))(z)\n    res = Add(name='residual_block_%d' % (i))([skip, x])\n    return res, skip\nx = Input(shape=input_shape, name='original_input')\nskip_connections = []\nout = Conv1D(n_filters, 2, dilation_rate=1, padding='causal', name='dilated_conv_1')(x)\nfor i in range(1, dilation_depth + 1):\n    out, skip = residual_block(out,i)\n    skip_connections.append(skip)\nout = Add(name='skip_connections')(skip_connections)\nout = Activation('relu')(out)\n\nout = Conv1D(n_filters, pool_size_1, strides = 1, padding='same', name='conv_5ms', activation = 'relu')(out)\n#out = AveragePooling1D(pool_size_1, padding='same', name='downsample_to_200Hz')(out)\n#out = AveragePooling1D(padding='same', name='downsample_to_200Hz')(out)\n\nout = Conv1D(n_filters, pool_size_2, padding='same', activation='relu', name='conv_500ms')(out)\n#out = Conv1D(output_shape[0], pool_size_2, padding='same', activation='relu', name='conv_500ms_target_shape')(out)\nout = Conv1D(output_shape[1], pool_size_2, padding='same', activation='relu', name='conv_500ms_target_shape')(out)\n#out = AveragePooling1D(pool_size_2, padding='same',name = 'downsample_to_2Hz')(out)\n\n#out = Conv1D(output_shape[0], (int) (input_shape[0] / (pool_size_1*pool_size_2)), padding='same', name='final_conv')(out)\nout = Conv1D(output_shape[1], (int) (input_shape[0] / (pool_size_1*pool_size_2)), padding='same', name='final_conv')(out)\n#out = GlobalAveragePooling1D(name='final_pooling')(out)\nout = Activation(final_activation, name='final_activation')(out)\n\nwavenet_model = Model(x, out)  \nwavenet_model.compile(optimizer='adam', \n      loss='categorical_crossentropy',\n      metrics=['accuracy'])\nwavenet_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 3000 # 1000 epochs approximately acc of 0.7131, 2 hours run time\n#epochs = 50\nwavenet_model.fit(train_X, train_y, epochs = epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## IMPT: Save model \n\nacc = wavenet_model.evaluate(train_X, train_y)[1]\nprint(\"Accuracy: {}\".format(acc))\nmodel_json = wavenet_model.to_json()\nwith open(\"wavenet_model_{}_epochs_{}_acc.json\".format(epochs, acc), \"w\") as json_file:\n    json_file.write(model_json)\n\nwavenet_model.save_weights(\"wavenet_model_{}_epochs_{}_acc.h5\".format(epochs, acc))\n\n# check\nassert(os.path.exists(\"wavenet_model_{}_epochs_{}_acc.json\".format(epochs, acc)) == True)\nassert(os.path.exists(\"wavenet_model_{}_epochs_{}_acc.h5\".format(epochs, acc)) == True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted = wavenet_model.predict(train_X)\npredicted = np.argmax(predicted, axis=2) # take argmax axis=1\nprint(predicted[0]) # check\nprint(predicted.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Inverse predicted back to wav data using bin_labels, MAX_Y and MIN_Y from before\n\n# reverse: reverse quantize -> reverse mu-law -> reverse normalize -1 1\n\n# reverse quantization from bin_labels  \npredicted_wav = np.zeros((predicted.shape[0], predicted.shape[1])) \nfor i in range(predicted.shape[0]):\n    predicted_wav[i,:] = bin_labels[predicted[i]] \nassert(np.sum(predicted_wav) != 0)\nprint(predicted_wav[0]) # check\n\n# inverse mu-law function \n#mu = 255 \n#predicted_wav = (np.exp(predicted_wav * np.log(1 + mu)) - 1) / mu \n#assert(np.sum(predicted_wav) != 0)\n#print(predicted_wav[0])\n\n# re-normalize back to wav from -1 1\n# IMPT: need MAX_Y and MIN_Y constants before not new np.min\n#predicted_wav = (predicted_wav + 1 / 2) * (MAX_Y - MIN_Y) + MIN_Y  \n#print(predicted_wav[0]) # check","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.io.wavfile\npredicted_0 = predicted_wav[0]\noriginal_0 = train_Y_female[0]\nsr = 4410\n\nprint(\"Predicted\")\nprint('Data=wave amplitude L/R stereo channels:', predicted_0)\nprint('Sampling rate=timesteps/seconds:', sr)\nprint('Audio length=data.shape[0]/sr:', predicted_0.shape[0]/sr, 'seconds')\nprint('Lowest amplitude L channel:', min(predicted_0))\nprint('Highest amplitude L channel:', max(predicted_0))\nprint('Data file written as predicted_0.wav and original_0.wav')\nprint(\"\\n\")\n\nprint(\"Original\")\nprint('Data=wave amplitude L/R stereo channels:', original_0)\nprint('Sampling rate=timesteps/seconds:', sr)\nprint('Audio length=data.shape[0]/sr:', original_0.shape[0]/sr, 'seconds')\nprint('Lowest amplitude L channel:', min(original_0))\nprint('Highest amplitude L channel:', max(original_0))\nprint('Data file written as predicted_0.wav and original_0.wav')\n\n# IMPT: need to cast as np.int16 if not audio will not work\nscipy.io.wavfile.write('predicted_0.wav', 44100, np.int16(predicted_0))\nscipy.io.wavfile.write('original_0.wav', 44100, np.int16(original_0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import IPython\nIPython.display.Audio('predicted_0.wav')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import IPython\nIPython.display.Audio('original_0.wav')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot waves\nimport matplotlib.pyplot as plt\nimport scipy.io.wavfile\npred_sr, pred_data = scipy.io.wavfile.read('predicted_0.wav')\nori_sr, ori_data = scipy.io.wavfile.read('original_0.wav')\nplt.title(\"Predicted (blue) and Original (orange) data\")\nplt.plot(pred_data)\nplt.plot(ori_data)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}