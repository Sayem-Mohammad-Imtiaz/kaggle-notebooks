{"cells":[{"metadata":{"_cell_guid":"6c53202d-5c34-4859-e7e9-8ef5c7068287","_uuid":"717bb968c36b9325c7d4cae5724a3672e49ff243","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89c8c923-c0bf-7b35-9ab8-e63f00b74e5a","_uuid":"d2bc3bbd2ea3961c49e6673145a0a7226c160e58","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/Sentiment.csv')\n\n# we check how much null entries in each columns\n\nprint(\"data_is_null \\n\",data.isnull().sum())\n\n# Keeping only the neccessary columns\ndata = data[['text','sentiment']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f5049ed46dc4643e4a2b8e7ad5746649f5ae5dd"},"cell_type":"code","source":"data[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0731137ce0796a4b8da50060e2e3561d0d4e516"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce8fd8ea74ca639893881b5cdf78a37bf91f45b2"},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee5a3539f1678a142c79eeb42e3c25e706362dec"},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f65e6460821f505950c2c2a882c8307c9fe7d95"},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa300ed1eeecccad9f5d35b84062c4d2a1c79afc"},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff4c10166d405f530a1783e8b58982b71ecf9c99"},"cell_type":"code","source":"print(data.size)\ndata[:5]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"43632d2d-6160-12ce-48b0-e5eb1c207076","_uuid":"d0f8b4542106a279f7398db7285ae5e370b2e813","trusted":true},"cell_type":"code","source":"# it will remove all Neutral values from data\ndata = data[data.sentiment != \"Neutral\"]\nprint(data[:2])\n\n# it will remove all the eg:-  RT @NancyLeeGrahn:  \ndata['text'] = data['text'].apply(lambda x: x.lower())\nprint(data[:2])\n#print(data)\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\nprint(data[:2])\nprint(data.dtypes)\nprint(data[ data['sentiment'] == 'Positive'].size)\nprint(data[ data['sentiment'] == 'Negative'].size)\nprint(data[data['sentiment']!='Neutral'].size)\n\nfor idx,row in data.iterrows():\n    #print(idx,row)\n    row[0] = row[0].replace('rt',' ')\n    \nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nprint(X[:5])\n\n\n# pad: to make all input of same length\nX = pad_sequences(X)\nprint(X[:5])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1ba3cf60-a83c-9c21-05e0-b14303027e93","_uuid":"05cb9ef0ec9e0a4067e3ab7c1bda7b2c1211feda","trusted":true},"cell_type":"code","source":"# Keras offers an Embedding layer that can be used for neural networks on text data.\n'''\nsource from: machinelearningmastery.com\nThe Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n\nIt must specify 3 arguments:\n\ninput_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\noutput_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\ninput_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\nFor example, below we define an Embedding layer with a vocabulary of 200 (e.g. integer encoded words from 0 to 199, inclusive), a vector space of 32 dimensions in which words will be embedded, and input documents that have 50 words each.\n\n\ne = Embedding(200, 32, input_length=50)\n1\ne = Embedding(200, 32, input_length=50)\n\n\n'''\nembed_dim = 128\nlstm_out = 196\nimport time\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nstart=time.time()\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(\"Time to compile model:\",time.time()-start)\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b35748b8-2353-3db2-e571-5fd22bb93eb0","_uuid":"a380bbfae2d098d407b138fc44622c9913a31c07","trusted":true},"cell_type":"code","source":"print((data['sentiment']).values)\nY = pd.get_dummies(data['sentiment']).values\nprint(Y)\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d5e499ac-2eba-6ff7-8d9a-ff65eb04099b","_uuid":"d0b239912cf67294a9f5af6883bb159c44318fc7","trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nbatch_size = 32\ntqdm(model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a970f412-722f-6d6d-72c8-325d0901ccef","_uuid":"7872f6ea819a5d4d08394ba6db8436f9cb2cfe1c","trusted":true},"cell_type":"code","source":"validation_size = 1500\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1add73e9-c6fb-7e4c-8715-ea92f519d2a6","_uuid":"f80e9f3cf281adb3ab0357cbf6f886eb1dce3005","trusted":true},"cell_type":"code","source":"pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    \n    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(Y_validate[x]) == 0:\n        neg_cnt += 1\n    else:\n        pos_cnt += 1\n\n\n\nprint(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"24c64f46-edd1-8d0b-7c7c-ef50fd26b2fd","_uuid":"d9aac68e2013b3beffb6a764cc5b85be83073e66","trusted":true},"cell_type":"code","source":"twt = ['Meetings: ram is a good man.']\n#vectorizing the tweet by the pre-fitted tokenizer instance\ntwt = tokenizer.texts_to_sequences(twt)\n#padding the tweet to have exactly the same shape as `embedding_2` input\ntwt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\nprint(twt)\nsentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}