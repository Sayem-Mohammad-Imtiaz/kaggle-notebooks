{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\ndata = pd.read_csv('../input/fer2013/fer2013.csv') # load data from csv\ndata.head() # Gives first 5 rows","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\n\nBeim Abrufen der Trainingsvariablen X und y aus Pixeln bzw. Emotionsspalten der CSV Datei werden diese in ein Numpy-Arrays konvertiert. Wir fügen unserem Feature-Vektor mithilfe der Funktion np.expand_dims () eine zusätzliche Dimension hinzu. Dies geschieht, um die Eingabe für unser CNN geeignet zu machen, das wir später entwerfen werden. Sowohl Funktionen als auch Beschriftungen werden als .npy-Dateien gespeichert, die später verwendet werden.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nwidth, height = 48, 48\n\ndatapoints = data['pixels'].tolist()\n\n#getting features for training\nX = []\nfor xseq in datapoints:\n    xx = [int(xp) for xp in xseq.split(' ')]\n    xx = np.asarray(xx).reshape(width, height)\n    X.append(xx.astype('float32'))\n\nX = np.asarray(X)\nX = np.expand_dims(X, -1)\n\n#getting labels for training\ny = pd.get_dummies(data['emotion']).to_numpy()\n\n#storing them using numpy\nnp.save('fdataX', X)\nnp.save('flabels', y)\n\nprint(\"Preprocessing Done\")\nprint(\"Number of Features: \"+str(len(X[0])))\nprint(\"Number of Labels: \"+ str(len(y[0])))\nprint(\"Number of examples in dataset:\"+str(len(X)))\nprint(\"X,y stored in fdataX.npy and flabels.npy respectively\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Developing the Model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys, os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\n\nnum_features = 64\nnum_labels = 7\nbatch_size = 64\nepochs = 100\nwidth, height = 48, 48\n\nx = np.load('./fdataX.npy')\ny = np.load('./flabels.npy')\n\nx -= np.mean(x, axis=0)\nx /= np.std(x, axis=0)\n\n#for xx in range(10):\n#    plt.figure(xx)\n#    plt.imshow(x[xx].reshape((48, 48)), interpolation='none', cmap='gray')\n#plt.show()\n\n#splitting into training, validation and testing data\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=41)\n\n#saving the test samples to be used later\nnp.save('modXtest', X_test)\nnp.save('modytest', y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Design und Ausführung des CNN Modell\n\nDer Output zeigt uns  das sequentiell ausgeführte Modell. Ein sequentielles Modell ist ein linearer Stapel von Ebenen, bei dem Ebenen übereinander gelegt werden, während wir jedesmal von der Eingabeebene zur Ausgabeebene übergehen.\n\nmodel.add (Conv2D ()) - Ist eine 2D-Faltung, die die Faltungsoperation wie am Anfang dieses Beitrags beschrieben ausführt.   Hier verwenden wir eine 3x3-Kernelgröße und eine gleichgerichtete lineare Einheit (ReLU) als Aktivierungsfunktion.\n\nmodel.add (BatchNormalization ()) - Führt die Batch-Normalisierungsoperation für Eingaben in die nächste Ebene aus, sodass unsere Eingaben in einer bestimmten Skala von 0 bis 1 angezeigt werden, anstatt über den gesamten Ort verteilt zu sein.\n\nmodel.add (MaxPooling2D ()) - Diese Funktion führt die Pooling-Operation für die Daten aus, wie am Anfang des Beitrags erläutert. Wir nehmen in diesem Modell ein Pooling-Fenster von 2x2 mit 2x2 Schritten.\n\nmodel.add (Dropout ()) - Wie oben erläutert, ist Dropout eine Technik, bei der zufällig ausgewählte Neuronen während des Trainings ignoriert werden. Sie werden zufällig \"abgebrochen\". Dies reduziert das Overfitting.\n\nmodel.add (Flatten ()) - Dies reduziert nur die Eingabe von ND auf 1D und hat keinen Einfluss auf die Stapelgröße.\n\nmodel.add (Dense ()) - Gemäß Keras Documentation implementiert Dense die Operation: output = Aktivierung, wobei die Aktivierung die elementweise Aktivierungsfunktion ist, die als Aktivierungsargument übergeben wird. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#desinging the CNN\nmodel = Sequential()\n\nmodel.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', input_shape=(width, height, 1), data_format='channels_last', kernel_regularizer=l2(0.01)))\nmodel.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(2*2*2*num_features, kernel_size=(3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(2*2*2*num_features, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(2*2*num_features, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(2*num_features, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(num_labels, activation='softmax'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trainieren des Modells","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#Dies ist ein ziemlich einfacher Codeabschnitt, bei dem zuerst das Modell mit categoryical_crossentropy als Loss-Function und mit der Verwendung des Adam-Optimierers kompiliert wird. Wir verwenden Accuracy als Metrik für die Validierung. Außerdem passen wir das Modell mit der festen Chargengröße (64 hier), den Epochen (hier 100) und den Validierungsdaten an, die wir durch Aufteilen der Trainingsdaten zuvor erhalten haben. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compliling the model with adam optimixer and categorical crossentropy loss\nmodel.compile(loss=categorical_crossentropy,\n              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),\n              metrics=['accuracy'])\n\n#training the model\nmodel.fit(np.array(X_train), np.array(y_train),\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(np.array(X_valid), np.array(y_valid)),\n          shuffle=True)\n\n#saving the  model to be used later\nfer_json = model.to_json()\nwith open(\"fer.json\", \"w\") as json_file:\n    json_file.write(fer_json)\nmodel.save_weights(\"fer.h5\")\nprint(\"Saved model to disk\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}