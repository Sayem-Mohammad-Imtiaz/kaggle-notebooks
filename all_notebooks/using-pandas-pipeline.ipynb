{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install hiplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/jobathon-analytics-vidhya/train.csv')\ntest = pd.read_csv('/kaggle/input/jobathon-analytics-vidhya/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import hiplot as hip\ndata = train.drop(['Region_Code', 'ID'], axis = 1).to_dict(orient = 'records')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# THIS IS THE BOMBS !!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"hip.Experiment.from_iterable(data).display()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Response'] = -1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from functools import wraps\nimport datetime as dt\n\ndef log_step(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        tic = dt.datetime.now()\n        result = func(*args, **kwargs)\n        time_taken = str(dt.datetime.now() - tic)\n        print(f\"just ran step {func.__name__} shape={result.shape} took {time_taken}s\")\n        return result\n    return wrapper","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@log_step\ndef LGBM(train, test): \n\n    NUM_OF_BOOST_ROUND = 10000\n    EARLY_STOPPING = 300\n    SEED = 2021  \n\n    params = {\n        'cat_features': cat_features_index,\n        'metric': 'auc',\n        'seed': SEED,\n        'n_estimators': NUM_OF_BOOST_ROUND\n    }\n\n    clf = LGBMClassifier(**params)\n    clf.fit(X_train, y_train, eval_set = (X_valid, y_valid), early_stopping_rounds = 100, verbose = -1)\n    \n    ypred_lgb = clf.predict_proba(X_valid)[:,1]\n    print(metrics.roc_auc_score(y_valid, ypred_lgb))\n    \n    return metrics.roc_auc_score(y_valid, ypred_lgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@log_step\ndef categorical(data):\n    \n    \"\"\"\n    Find categorical columns and Label Encode them \n    \"\"\"\n\n    cat_columns = []\n\n    for col in data.select_dtypes('object').columns:\n        print(col)\n        cat_columns.append(col)\n        le = LabelEncoder()\n        data[col] = le.fit_transform(data[col])\n        \n        cat_features_index = [i for i, col in enumerate(train.columns) if col in cat_columns]\n    \n    return data, cat_features_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@log_step\ndef encoding(data):\n\n    \"\"\"\n    One Hot Encoding and Label Encoding \n    \"\"\"\n    le = LabelEncoder()\n    data['Holding_Policy_Duration'] = le.fit_transform(data['Holding_Policy_Duration'])\n\n    var_mod = ['Accomodation_Type', 'Reco_Insurance_Type', 'Is_Spouse','Health Indicator', 'Holding_Policy_Duration']\n\n    for i in var_mod:\n        data[i] = le.fit_transform(data[i])\n\n    # One Hot Encoding : \n    data = pd.get_dummies(data, columns = ['Accomodation_Type', 'Reco_Insurance_Type', 'Is_Spouse','Health Indicator', 'Holding_Policy_Duration'])\n    \n    return data\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@log_step\ndef preprocess(data):\n\n    data['Holding_Policy_Type'] = data['Holding_Policy_Type'].astype(str)\n\n    data['Reco_Policy_Cat'] = data['Reco_Policy_Cat'].astype(str)\n\n    data['Region_Code'] = data['Region_Code'].astype(str)\n    \n    return data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@log_step\ndef impute(data):\n    \n    data['Holding_Policy_Duration'] = data['Holding_Policy_Duration'].fillna(str(0.0))\n    data['Holding_Policy_Type'] = data['Holding_Policy_Type'].fillna('no_policies')\n    data['Health Indicator'] = data['Health Indicator'].fillna(data['Health Indicator'].mode()[0])\n    \n    return data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@log_step\ndef feature_engineering(data):\n    \n    for i in range(len(data.drop('Response', axis = 1).columns)): \n    \n        for j in range(i):\n\n            a = data.columns[i]\n            b = data.columns[j]\n            data[a+'_'+b] = data[a].astype(str)+'_'+data[b].astype(str)\n\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@log_step\ndef start_pipeline(dataf):\n    return dataf.copy() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = (train\n      .pipe(start_pipeline)\n      .pipe(impute)\n      .pipe(feature_engineering)\n      .pipe(encoding))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = (test\n      .pipe(start_pipeline)\n      .pipe(impute)\n      .pipe(feature_engineering)\n      .pipe(encoding))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold = (.4 * (1- .4)))\ny = train_df.select_dtypes(include = ('boolean'))\nsel.fit_transform(train_df[y])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TRAIN TEST SPLIT"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train_df.select_dtypes(exclude='object')\ntest = test_df.select_dtypes(exclude='object')\n\n# Seperate Features and Target\nX= train_df.drop(columns = ['Response'], axis=1)\ny= train_df['Response']\n\nX_train,X_valid,y_train,y_valid = train_test_split(X,y,test_size=0.2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = train_df.select_dtypes(include = 'int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new = SelectKBest(chi2, k = 2).fit_transform(X1, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2 = train_df.select_dtypes(include = ['int'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EXTRA TREE CLASSIFIER FEATURE SELECTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.select_dtypes(exclude = 'object')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,\n                                        criterion = 'entropy', max_features = 2)\n\nextra_tree_forest.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = extra_tree_forest.feature_importances_\n\nfeature_importance_normalized = np.std([tree.feature_importances_ for tree in extra_tree_forest.estimators_],\n                                      axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfigure(figsize = (20, 18))\n\nplt.bar(X.columns, feature_importance_normalized)\nplt.xlabel('Feature_Labels')\nplt.ylabel('Feature Importances')\nplt.title('Comparison of Different Feature Importances')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance_normalized > feature_importance_normalized.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = []\n\nfor i in range(len(feature_importance_normalized)):\n    \n    if feature_importance_normalized[i] > feature_importance_normalized.mean():\n        \n        a.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(feature_importance_normalized)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(X.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Columns with feature importances extracted from Extra tree classifier\nmat = X.iloc[:,a]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mat.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 20 % data as validation set\nX_train,X_valid,y_train,y_valid = train_test_split(train_df,y,test_size=0.2,random_state=22)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb = LGBMClassifier(boosting_type='gbdt',n_estimators=500,max_depth=7,learning_rate=0.04,objective='binary',metric='auc',is_unbalance=True,\n                 colsample_bytree=0.5,reg_lambda=2,reg_alpha=2,random_state=101,n_jobs=-1)\n\n\n\nlgb.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(roc_auc_score(y_valid,lgb.predict_proba(X_valid)[:,1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.astype(int)\nX_valid = X_valid.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.select_dtypes(exclude = 'int').astype(str)\ntest_df = test_df.select_dtypes(exclude = 'int').astype(str)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_columns = []\n\nfor col in train_df.select_dtypes('object').columns:\n    print('Train:',col)\n    cat_columns.append(col)\n    le = LabelEncoder()\n    train_df[col] = le.fit_transform(train_df[col])\n    \n\nfor col in test_df.select_dtypes('object').columns:\n    print('Test:',col)\n\n    le = LabelEncoder()\n    test_df[col] = le.fit_transform(test_df[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"cat_features_index = [i for i, col in enumerate(train.columns) if col in cat_columns]\ncat_features_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_OF_BOOST_ROUND = 10000\nEARLY_STOPPING = 300\nparams = {\n    'cat_features': cat_features_index,\n    'eval_metric': 'AUC',\n    'random_seed': SEED,\n    'n_estimators': NUM_OF_BOOST_ROUND,\n}\n\nSEED = 2021","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bst = CatBoostClassifier(**params, early_stopping_rounds=EARLY_STOPPING)\n_ = bst.fit(X_train, y_train, eval_set=(X_valid,y_valid), plot=True, verbose=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# I tried Feature Selection using Extra Tree Classifier but it gave a much less score 0.62\n# But when I took in all columns it is giving me a much higher score. Why ? \n# What can be done ??"},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install hiplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import hiplot as hip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install -U streamlit hiplot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}