{"cells":[{"metadata":{"_uuid":"110e04e85dd6c7fa170602d2e645b12221dec6f7"},"cell_type":"markdown","source":"Team Name: 404NA <br>\nAnish M Rao 01FB16ECS062 <br>\nMalaika Vijay 01FB16ECS189 <br>\nManasa Jagadeesh 01FB16ECS471"},{"metadata":{"_kg_hide-input":false,"_uuid":"818aff8ede5afb1708a64e72abc6aefeb44ae129","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\nfrom sklearn.metrics import mean_squared_error,classification_report,accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_uuid":"8241191e9c8a51ad093c6d492e70fb377c144df4","trusted":true,"scrolled":true},"cell_type":"code","source":"print(os.listdir('../input'))\ndata = pd.read_csv('../input/Absenteeism_at_work.csv', delimiter=',')\nX_orig=data.drop('Absenteeism time in hours',axis=1)\ny=data['Absenteeism time in hours']\nscaler=MinMaxScaler()\nX = scaler.fit_transform(X_orig)\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=13,test_size=0.2)\n#Note:\n# Not scaling the target values because it will influence the mean squared error and other parameters.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5323de954f369448393a8d00b672603d73591f8c"},"cell_type":"markdown","source":"DECISION TREE CLASSIFIER:\nThis piece of code tests values of max_depth and min_samples_split to determine the optimal value to be used in the Decision Tree Classifier"},{"metadata":{"trusted":true,"_uuid":"4b5cdf17b577baa1b3d2a075bef5cebe109cd3bc"},"cell_type":"code","source":"# acc=[]\n# for i in range(2,30):\n#     for j in range(2,20):\n#         classifier=DecisionTreeClassifier(criterion='entropy',max_depth=i,min_samples_split=j,random_state=13)\n#         classifier.fit(X_train,y_train)\n#         preds=classifier.predict(X_test)\n#         acc.append((i,j,accuracy_score(y_test,preds)))\n# acc[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e8c0bfbab74f37f45ab035d136cc3721ea04f02"},"cell_type":"code","source":"classifier=DecisionTreeClassifier(criterion ='entropy', random_state = 13, max_depth=5, min_samples_split=2)\nclassifier.fit(X_train,y_train)\npred=classifier.predict(X_test)\nprint(classification_report(y_test,pred))\nprint(\"Accuracy: \",accuracy_score(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14c8a01f438708aac6bd925d46cc5edd27087c0b"},"cell_type":"markdown","source":"**K-Nearest-Neighbors Regression**\nFind attributes with high correlation first, to prune some of them"},{"metadata":{"trusted":true,"_uuid":"773bf95c4b36a95673ed0d0d692fdb0a5cb478cb"},"cell_type":"code","source":"def get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(data, 15))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83bd85c6979ba4c2b530535205c78db97b6d2e3d"},"cell_type":"code","source":"#Selected attributes for regression after pruning:\nx_columns = ['Reason for absence',  'Month of absence','Day of the week',\n             'Transportation expense', 'Distance from Residence to Work', 'Age', 'Work load Average/day ', \n             'Hit target', 'Disciplinary failure', 'Education']\n\nX_reduced = X_orig[x_columns]\nX_reduced = scaler.fit_transform(X_reduced)\nX_train,X_test,y_train,y_test=train_test_split(X_reduced,y,random_state=13,test_size=0.2)\n\nrmses = []\nall_predictions = dict()\nks = range(1,19*2+1)\n\n#picking k based on lowest rmse value\nfor k in ks:\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    predictions = knn.predict(X_test)\n    all_predictions[k] = predictions\n    rmses.append(np.sqrt(mean_squared_error(y_test, predictions)))\n\nplt.plot(ks, rmses)\nplt.show()\noptimum_k = ks[rmses.index(min(rmses))]\nprint('K :', optimum_k, '\\nrmse:', min(rmses))\nfinal_preds = all_predictions[optimum_k]\nknn = KNeighborsRegressor(n_neighbors=optimum_k)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c0e46cbc1fecd17bd01db12f2e13ef6b9c8bf4b"},"cell_type":"markdown","source":"**Conclusion**\nThe models cannot be compared directly from the metrics we have calculated since one is a classification method and other is a regression one. However, based on the nature of the features and the target variable (being continuous), a regression method will fit this problem better. This reflects in the reasonable RMSE value of the KNN regression model."},{"metadata":{"_uuid":"09dfed5a3c6f8dfb7672aa23c24c54348771c8be"},"cell_type":"markdown","source":""}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}