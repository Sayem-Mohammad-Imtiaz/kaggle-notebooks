{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1.Data preparation and visualization ","metadata":{}},{"cell_type":"markdown","source":"### 1.1 Read data","metadata":{}},{"cell_type":"code","source":"raw_data = pd.read_csv(\"/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Remove Duplicates(if any)","metadata":{}},{"cell_type":"code","source":"data = raw_data.drop_duplicates(subset=\"customerID\",keep=\"first\")\ndata.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 **Explortary analysis**","metadata":{}},{"cell_type":"code","source":"raw_data.describe(include=\"object\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"SeniorCitizen\"] = data[\"SeniorCitizen\"].astype(object)\ndata=data.replace(\" \",np.nan)\ndata[\"TotalCharges\"] = data[\"TotalCharges\"].astype(\"float64\")\ndata.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cate_vars = [\"gender\",\"SeniorCitizen\",\"Partner\",\"Dependents\",\"PhoneService\",\"MultipleLines\",\n             \"InternetService\",\"OnlineSecurity\",\"OnlineBackup\",\"DeviceProtection\",\n             \"TechSupport\",\"StreamingTV\",\"StreamingMovies\",\"Contract\",\"PaperlessBilling\",\n            \"PaymentMethod\"]\nnum_vars = [\"tenure\",\"MonthlyCharges\",\"TotalCharges\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfor var in num_vars:\n    plt.figure(figsize=(10,2))\n    sns.boxplot(x = var,y =\"Churn\",data = data, showmeans= True)\n    plt.title(i)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes = plt.subplots(8,2,figsize = (20,15))\naxes = axes.flatten()\n\nfor i in range(len(cate_vars)):\n    sns.countplot(y = cate_vars[i], hue = \"Churn\", data = data, ax = axes[i])\n    axes[i].set_title(cate_vars[i])\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(\"Churn\",data = data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the data is not balanced. There are serval ways to address that. One esay way is to use AUC as our scoring method","metadata":{}},{"cell_type":"markdown","source":"##### Plot a pairplot","metadata":{}},{"cell_type":"code","source":"sns.pairplot(data = data, hue=\"Churn\", vars = num_vars, height = 8, kind = \"reg\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  2. Feature Engineering\n### 2.1 remove irelevant data","metadata":{}},{"cell_type":"code","source":"data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.drop(\"customerID\", axis = 1, inplace = True)\ndata.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 One-hot encoding","metadata":{}},{"cell_type":"code","source":"data = pd.get_dummies(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data.iloc[:,:45]\ny = data.iloc[:,46]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Split training and testing set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nseed = 666\nX_train,X_test,y_train,y_test= train_test_split(X, y, test_size = 0.3,\n                                                stratify = y,\n                                                random_state = seed\n                                               )\nprint(\"X_train's shape:{}, y_train's shape:{}\".format(X_train.shape,y_train.shape))\nprint(\"X_test's shape:{}, y_test's shape:{}\".format(X_test.shape, y_test.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Variance Threshold","metadata":{}},{"cell_type":"markdown","source":"##### 2.3.1 Filter - VarianceThreshold","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\n# VAR(X) = P(1-P)\nthreshold = 0.95\nvt = VarianceThreshold()\nvt.fit(X_train)\nfeat_vt = X.columns[vt.variances_ > threshold*(1-threshold)]\nprint(\"Features left:\", len(vt.variances_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 2.3.2 Filter - Chi-2 test","metadata":{"trusted":true}},{"cell_type":"code","source":"from sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.preprocessing import MinMaxScaler\n# make sure all features are positive before applying chi2 test\nX_minmax = MinMaxScaler(feature_range = (0,1)).fit_transform(X_train)\nmodel = SelectKBest(score_func = chi2, k = \"all\")\nX_scores = model.fit(X_minmax, y_train)\nchi2_table = pd.DataFrame({\"Features\": X_train.columns,\n                           \"Chi2 Score\": X_scores.scores_}\n                         )\nchi2_table = chi2_table.sort_values([\"Chi2 Score\"], ascending = False)\nchi2_20 = chi2_table.loc[:20, \"Features\"].values # return an array, not a list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.3.3 Recursive Feature Elimination(RFE)","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe = RFE(LogisticRegression(max_iter = 100000), \n          n_features_to_select = 20\n         )\nrfe.fit(X_train, y_train)\nrfe_scores = pd.DataFrame({\"Features\":X_train.columns,\n                          \"RFE Score\":rfe.ranking_})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rfe_scores = rfe_scores.sort_values([\"RFE Score\"],\n                                    ascending = False\n                                   )\nrfe_20 = rfe_scores.loc[:20,\"Features\"].values\nrfe_20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.3.4 Feature Importance Top 20","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train,y_train)\nfeat_imp_scores = pd.DataFrame({\"Feature\":X_train.columns,\n                               \"Importance\":model.feature_importances_}\n                              )\nfeat_imp_scores = feat_imp_scores.sort_values([\"Importance\"],ascending=  False)\nfeat_imp_20 = feat_imp_scores.loc[:20,\"Feature\"].values\nfeat_imp_20","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2.3.4 Final Feature Selection ","metadata":{}},{"cell_type":"code","source":"# no Variance \nfinal_feat = np.concatenate([rfe_20,feat_imp_20,chi2_20])\nfinal_feat = np.unique(final_feat)\nprint(final_feat)\nprint(len(final_feat))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get dataframe\nX_train =X_train.loc[:,final_feat]\nX_test = X_test.loc[:,final_feat]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.Model Selection","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Model Spot Check","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier,VotingClassifier\nfrom sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {\"Linear Regression\":LinearRegression(),\n          \"LDA\": LinearDiscriminantAnalysis(),\n          \"CART\":DecisionTreeClassifier(),\n          \"SVM\": SVC(),\n         \"AdaBoost\":AdaBoostClassifier(),\n         \"Random Forest\": RandomForestClassifier(),\n         \"Gradient Boosting\": GradientBoostingClassifier()}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scoring = \"roc_auc\"\nseed = 666\nfor clf in models:\n    score = cross_val_score(models[clf], X_train, y_train, \n                            scoring = scoring, cv = 3\n                           )\n    print(\"{0}:{1:.3f} +/-{2:.3f} \".format(clf,np.mean(score), np.std(score)))\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the ensemble models usually have a better performance. Here we will select **Linear Regression**, **RandomForest**, **Adaboost** and **Gradient Boosting** for our final models ","metadata":{}},{"cell_type":"markdown","source":"### 3.2 Model Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score,accuracy_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### 3.2.1 Logistic Regression","metadata":{}},{"cell_type":"code","source":"def LR_model(X_train_cv, y_train_cv, X_val, y_val, C, penalty = \"l2\"):\n    scaler = StandardScaler().fit(X_train_cv)\n    X_train_cv = scaler.transform(X_train_cv)\n    X_val = scaler.transform(X_val) \n    if penalty == \"l2\":\n        clf = LogisticRegression(C = C, penalty=\"l2\")\n    else:\n        clf = LogisticRegression(C = C, solver = \"liblinear\",penalty=\"l1\")\n    clf.fit(X_train_cv,y_train_cv)\n    y_val_pred = clf.predict(X_val)\n    y_val_proba = clf.predict_proba(X_val)[:, 1]\n    auc = roc_auc_score(y_val, y_val_proba)\n    acc = accuracy_score(y_val,y_val_pred)\n    return auc, acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def LR_cv(X,y,Cs,penalty):\n    kf = KFold(n_splits = 3)\n    scores = {}\n    for train_idx,val_idx in kf.split(X):\n        X_train_cv, y_train_cv = X_train.iloc[train_idx,:],y_train.iloc[train_idx]\n        X_val, y_val = X_train.iloc[val_idx,:], y_train.iloc[val_idx]\n        for C in Cs:\n            score,_ = LR_model(X_train_cv,y_train_cv,\n                            X_val,y_val,\n                               C,penalty)          \n            if C not in scores:\n                scores[C] = []\n                scores[C].append(score)\n            else:\n                scores[C].append(score)\n    return scores\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Cs = [0.01, 0.1, 0.3, 1, 3, 10, 30]\naucs_l1 = LR_cv(X_train, y_train,Cs, penalty = \"l1\")\naucs_l2 = LR_cv(X_train, y_train,Cs, penalty = \"l2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_best_params(dict1):\n    best_mean_auc = 0\n    mean_aucs = []\n    for key in dict1:\n        mean_auc = np.mean(dict1[key])\n        if best_mean_auc < mean_auc:\n            best_mean_auc = mean_auc\n            best_C = key\n        mean_aucs.append(mean_auc)\n    return best_C, best_mean_auc, mean_aucs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_C_l1, best_aucs_l1, mean_aucs_l1 = get_best_params(aucs_l1)\nprint(\"L1:\", best_C_l1, best_aucs_l1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_C_l2, best_aucs_l2,mean_aucs_l2 = get_best_params(aucs_l2)\nprint(\"L2:\",best_C_l2, best_aucs_l2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_mean_auc_LR(Cs,aucs,penalty):\n    plt.plot(Cs, aucs, label = penalty)\n    plt.xlabel(\"Cs\")\n    plt.ylabel(\"AUCs\")\n    plt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_mean_auc_LR(Cs,mean_aucs_l1,\"L1\")\nplot_mean_auc_LR(Cs,mean_aucs_l2,\"L2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, 12 is only slightly better than l1.","metadata":{}},{"cell_type":"markdown","source":"##### 3.2.2 Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\ndef rf_model(X_train_cv, y_train_cv, X_val, y_val, lf, sp):\n    scaler = StandardScaler().fit(X_train_cv)\n    \n    X_train_cv = scaler.transform(X_train_cv)\n    X_val = scaler.transform(X_val)\n    \n    clf = RandomForestClassifier(min_samples_leaf=lf,\n                                 min_samples_split= sp,n_jobs = -1\n                                )\n    \n    clf.fit(X_train_cv,y_train_cv)\n    \n    y_val_pred = clf.predict(X_val)\n    y_val_proba = clf.predict_proba(X_val)[:,1]\n    \n    auc = metrics.roc_auc_score(y_val, y_val_proba)\n    acc = metrics.accuracy_score(y_val, y_val_pred)\n    \n    return auc, acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rf_cv(X_train, y_train, leaf, split, cv):\n    kf = KFold(n_splits=cv)\n    aucs={}\n    \n    for train_idx, val_idx in kf.split(X_train):\n        X_train_cv, y_train_cv = X_train.iloc[train_idx,:], y_train.iloc[train_idx]\n        X_val, y_val = X_train.iloc[val_idx,:],y_train.iloc[val_idx]\n        for lf in leaf:\n            for sp in split:\n                auc, acc = rf_model(X_train_cv, y_train_cv,\n                                    X_val, y_val,\n                                    lf,sp\n                                   )\n                key = str(lf)+\"&\"+str(sp) \n                if key in aucs:\n                    aucs[key].append(auc)\n                else:\n                    aucs[key]=[]\n                    aucs[key].append(auc)\n                    \n    return aucs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"leaf = np.geomspace(5, 200, num=10, dtype=int)\nsplit = np.geomspace(10, 2000, num=10, dtype=int)\n\naucs = rf_cv(X_train, y_train, leaf, split, cv=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rf_get_best_param(dict1):\n    mean_aucs=[]\n    best_mean_auc = 0\n    best_params=(0,0)\n    for key in dict1:\n        mean_auc = np.mean(dict1[key])\n        mean_aucs.append(mean_auc)\n        if best_mean_auc < mean_auc:\n            best_mean_auc = mean_auc\n            best_params = tuple(map(int,key.split(\"&\")))\n    return best_params,best_mean_auc, mean_aucs\n\nbest_params_rf, best_mean_auc, mean_aucs = rf_get_best_param(aucs)\nprint(\"Best leaf is {} and best split is {}\".format(best_params_rf[0], best_params_rf[1]))\nprint(\"Best mean_auc is \", best_mean_auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rf_plot(leaf,split,mean_aucs):\n    for i in range(len(leaf)):\n        plt.plot(split,mean_aucs[len(split)*i:len(split)*i+len(split)],\n                label = \"leaf=\"+str(leaf[i]))\n    plt.xlabel(\"split\")\n    plt.ylabel(\"mean auc score\")\n    plt.legend()\nrf_plot(leaf, split, mean_aucs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.3 Adaboost","metadata":{}},{"cell_type":"code","source":"def ada_model(X_train_cv, y_train_cv, X_val, y_val, n = 100):\n    from sklearn.tree import DecisionTreeClassifier\n    \n    dt_stump =  DecisionTreeClassifier(max_depth = 1, min_samples_leaf = 1)\n    \n    clf = AdaBoostClassifier(base_estimator = dt_stump, n_estimators = n)\n    #No need to standardize training data in tree models    \n    clf.fit(X_train_cv,y_train_cv)\n    \n    y_pred = clf.predict(X_val)\n    y_val_proba = clf.predict_proba(X_val)[:,1]\n    \n    auc = metrics.roc_auc_score(y_val,y_val_proba)\n    acc = metrics.accuracy_score(y_val,y_pred)\n    \n    return auc, acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ada_cv(X_train, y_train, ns, cv = 3):\n    aucs = {}\n    kf = KFold(n_splits = cv)\n    \n    for train_idx, val_idx in kf.split(X_train):\n        X_train_cv,y_train_cv = X_train.iloc[train_idx,:], y_train.iloc[train_idx]\n        X_val, y_val = X_train.iloc[val_idx,:], y_train.iloc[val_idx]\n        \n        for n in ns:\n            auc, _ = ada_model(X_train_cv, y_train_cv,\n                               X_val, y_val, n = n\n                              )\n            if n in aucs:\n                aucs[n].append(auc)\n            else:\n                aucs[n] = []\n                aucs[n].append(auc)\n    return aucs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ns = np.array([50,100,400,600,1000,2000])\naucs = ada_cv(X_train, y_train, ns, cv = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_mean_auc(dict1):\n    best_mean_auc = 0\n    best_n = 0\n    mean_aucs = []\n    for key in dict1:\n        mean_auc = np.mean(dict1[key])\n        mean_aucs.append(mean_auc)\n        if best_mean_auc < mean_auc:\n            best_mean_auc  = mean_auc\n            best_n = key\n    return best_n, best_mean_auc, mean_aucs\nbest_n,best_mean_auc,mean_aucs = get_mean_auc(aucs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best n_estimators:\",best_n)\nprint(\"Best mean auc score:\", best_mean_auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ada_plot(ns,mean_aucs):\n    plt.plot(ns,mean_aucs)\nada_plot(ns,mean_aucs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = GridSearchCV(estimator=AdaBoostClassifier(), scoring = \"roc_auc\",\n                     param_grid={\"n_estimators\":ns}\n                    )\nmodel.fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.4 XgBoost","metadata":{"trusted":true}},{"cell_type":"code","source":"def xgb_model(X_train_cv, y_train_cv, X_val, y_val, lf, sp):\n    clf = GradientBoostingClassifier(min_samples_split = sp, min_samples_leaf= lf)\n    clf.fit(X_train_cv,y_train_cv)\n    \n    y_val_pred = clf.predict(X_val)\n    y_val_proba = clf.predict_proba(X_val)[:, 1]\n    \n    acc = metrics.accuracy_score(y_val,y_val_pred)\n    auc = metrics.roc_auc_score(y_val,y_val_proba)\n    \n    feat_imp = clf.feature_importances_\n    \n    return auc,acc,feat_imp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def xgb_cv(X_train, y_train, leaf, split, cv = 5):\n    kf = KFold(n_splits = cv)\n    aucs = {}\n    \n    for train_idx, val_idx in kf.split(X_train):\n        X_train_cv, y_train_cv = X_train.iloc[train_idx, :], y_train.iloc[train_idx]\n        X_val, y_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx]\n        \n        for sp in split:\n            for lf in leaf:\n                auc, acc, imp = xgb_model(X_train_cv, y_train_cv,\n                                          X_val, y_val,\n                                          lf= lf, sp = sp\n                                         )\n                key = str(sp) + \"&\" + str(lf)\n                if key in aucs:\n                    aucs[key].append(auc)\n                else:\n                    aucs[key] = []\n                    aucs[key].append(auc)                    \n    return aucs ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbsplit= np.geomspace(10, 1500, num = 8, dtype = int)\ngbleaf = np.geomspace(5, 750, num = 6, dtype = int)\ngbaucs = xgb_cv(X_train, y_train, leaf = gbleaf, split = gbsplit,cv =5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def xgb_get_best_param(dict1):\n    mean_aucs = []\n    best_mean_auc = 0\n    best_params = ()\n    for key in dict1:\n        mean_auc = np.mean(dict1[key])\n        mean_aucs.append(mean_auc)\n        if best_mean_auc < mean_auc:\n            best_mean_auc = mean_auc\n            best_params = tuple(map(int,key.split(\"&\")))\n    return best_params, best_mean_auc, mean_aucs\nbest_params, best_mean_auc, mean_aucs = xgb_get_best_param(gbaucs)\nprint(\"Best split is {}\\nBest leaf is {}\".format(*best_params))\nprint(\"Best mean auc is {:.4f}\".format(best_mean_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 Final Prediction\n\nGiven the test results after hyparameters tuning, Xgboost's performance is the best. Hence, I will use the it for the final prediction.","metadata":{}},{"cell_type":"code","source":"final_auc,acc,imp = xgb_model(X_train,y_train,X_test,y_test, sp = 10, lf = 275)\nprint(\"Final AUC is {}\\nFinal accuracy is {}\".format(final_auc,acc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_feat_imp(index,feat_imp):\n    a = dict(zip(index,imp))\n    a = sorted(a.items(), key = lambda x:x[1], reverse = True)\n    a = dict(a)\n    plt.figure(figsize = (7,15))\n    sns.barplot(list(a.values()),list(a.keys()))\n    \nplot_feat_imp(list(X_test.columns), imp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As for the feature importance, we can see that the most important features are(Descending):\n* contract Month to Month,\n* tenure, \n* Online security-No,\n* Internet Survice-Fiber optic,\n* Monthly Charges\n* Total Charges\n* Payment Method - Electronic Check\n* It Support - No","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}