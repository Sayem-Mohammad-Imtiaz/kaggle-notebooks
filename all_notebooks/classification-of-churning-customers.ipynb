{"cells":[{"metadata":{"_uuid":"6dd1135d6e9beb22c48569106934f725b82cca2e"},"cell_type":"markdown","source":"This is a simple kernel, where we preprocess the dataset and then train a logistics regression and XGBClassifier in order to classify possible churning customers. "},{"metadata":{"_uuid":"171f483907e074e60467f2f9e9b1e5102bbee859"},"cell_type":"markdown","source":"****\nFirst we import data processing libraries and check the data for missing and duplicate values."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\ndata = pd.read_csv('../input/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n\n#check for missing data\nmissing_data = data.isnull().sum(axis=0).reset_index()\nmissing_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"384d77c80468f4ee81d0d0df45c6d073c1f21016","collapsed":true},"cell_type":"code","source":"#check for duplicates\nduplicates = data.duplicated().sum(axis=0)\nprint(duplicates == True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5927f570c4a8ab93c79035b4d5f22eb1dcaeaf67"},"cell_type":"markdown","source":"There is no missing or duplicate data, so we move on to analysis of the features to examine which could be the most interesting ones. We plot the features using **seaborn**."},{"metadata":{"trusted":true,"_uuid":"daead4bb869afcd0d3d42071fdd2e6904c089a39","collapsed":true},"cell_type":"code","source":"#plot features in relation to Churn\nfor i, predictor in enumerate(data.drop(columns=['Churn', 'TotalCharges', 'MonthlyCharges'])):#['gender', 'PhoneService', 'MultipleLines', 'InternetService','SeniorCitizen', 'tenure', 'Contract', 'PaperlessBilling', 'PaymentMethod']):\n    plt.figure(i)\n    sns.countplot(data=data, x=predictor, hue='Churn')\n\nsns.lmplot(data=data, x='TotalCharges', y='MonthlyCharges', hue='Churn',fit_reg=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a66609da0c8976be4bc4e226f4f9e9c1c618eb83"},"cell_type":"markdown","source":"**Choosing predictors**\nGood predictors seem to be *Dependents, tenure, PhoneService, InternetService, InternetServices extras (but not streaming), Contract, Billing/PaymentMethod.* From Total/MonthlyCharges it looks that *MonthlyCharges* is a better predictor of churn - the higher the charge the more likely is the customer to churn.\n\n\nIn the next code section we drop the columns that look like weak predictors and use encoding techniques to encode the categorical variables. We will use numerical encoding as well as one hot encoding, since some of the categorical variables have clear hierarchy (good for numeric encoding) and some do not. "},{"metadata":{"trusted":true,"_uuid":"b7f8bbabda864135b6c9f87699971236ef2c9120","collapsed":true},"cell_type":"code","source":"#data preprocessing\n#aggregate and encode categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n#custID only serves to identify the customer, should have no influence on the propensity to churn\n#also drop other weak predictors\ndata = data.drop(columns=['customerID','StreamingMovies',\"StreamingTV\", 'gender', 'TotalCharges', 'MultipleLines'])\n\n#change the value 'No internet service' in the internet addon variables to 'No' as they signify the same thing \nencodedData = data.replace(to_replace='No internet service', value='No')\n\n#one hot encode categorical variables that do not have explicit hierarchy - 'InternetService', 'PaymentMethod', 'Contract'\nohe_data = pd.get_dummies(encodedData,columns=['InternetService', 'PaymentMethod', 'Contract'])\n\n#encode the rest of categorical variables to numeric\nencodedData = ohe_data.apply(lambda x: LabelEncoder().fit_transform(x) if x.dtype == 'object' else x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7297cacf8575d6c159783edadd74e103288d6f9c"},"cell_type":"markdown","source":"Maybe there is correlation between number of activated addon services and the propensity to churn? Families vs. Individuals? Let's see."},{"metadata":{"trusted":true,"_uuid":"e87e51f2414efc3a4c44bb9045e7ee1db4183657","collapsed":true},"cell_type":"code","source":"#add features that aggregate some of the columns to better analyze their influence\nencodedData['famSize'] = encodedData[['Partner', 'Dependents']].sum(axis=1)\nencodedData['InternetServicesAddons'] = encodedData[['OnlineSecurity','OnlineBackup',\n                                                    'DeviceProtection', 'TechSupport']].sum(axis=1)\n#analyze the aggregated variables\nplt.figure()\nsns.countplot(data=encodedData, x='famSize', hue='Churn')\nplt.figure()\nsns.countplot(data=encodedData, x='InternetServicesAddons', hue='Churn')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af28e552324c410ae79c7376716b45a37cd9e606"},"cell_type":"markdown","source":"It appers that there is a relationship between size of the customers family, the number of internet services they use and the likelihood to churn.\n\nWe have selected independent variables we believe have influence on the churn (dependent variable), the next step is to train some classification models and evaluate them. We will use traditional** logisitics regression **as well as **gradient boosted random forest** and use ROC curve as a metric to see which performs better.  We will also test a 3 sets of predictors."},{"metadata":{"trusted":true,"_uuid":"2236fa47b529fce71769051789b76c3153dff2be","collapsed":true},"cell_type":"code","source":"#train and validate classifier\nfrom sklearn import linear_model, metrics\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\n\n#define dependent variable\ny = encodedData.Churn\n\n#define sets of predictors\n\n#in this set we will use the aggregated variables for internet service addons and family size\nX = encodedData.drop(columns=['Churn', 'OnlineSecurity','OnlineBackup','DeviceProtection',\n                                'TechSupport'])\n\n#here we will not use the aggregated variables\nX2 = encodedData.drop(columns=['famSize', 'InternetServicesAddons','Churn'])\n\nd = {0: \"X\", 1: \"X2\"}\n\nfor i, predictors in enumerate([X, X2]):\n    print('Results with predictors %s:' % d[i])\n    xTrain, xTest, yTrain, yTest = train_test_split(predictors,y)\n\n    #train and evaluate linear model\n    linearModel = linear_model.LogisticRegression() \n    linearModel.fit(xTrain, yTrain)\n\n    predictionsLinear = linearModel.predict(xTest)\n\n    fpr, tpr, t = metrics.roc_curve(yTest, predictionsLinear)\n    auroc = metrics.auc(fpr, tpr)\n\n    print(\"AUC of Linear Model: %f\" % (auroc))\n\n    XGBeval = {}\n    #train and evaluate XGBModel using a number of trees as a parameter\n    #we also use a greedier learning_rate and maxdepth to try and better fit the data altough we risk overfitting\n    for n in range(10, 200, 10):\n        xgbModel = XGBClassifier(maxdepth=9, n_estimators=n, learning_rate=0.1, seed=42)\n        xgbModel.fit(xTrain, yTrain)\n\n        predictionsXGB = xgbModel.predict(xTest)\n\n        fprX, tprX, tX = metrics.roc_curve(yTest, predictionsXGB)\n        aurocX = metrics.auc(fprX, tprX)\n        XGBeval[aurocX] = n\n    best = max(XGBeval.keys())\n    print(\"best performing XGBModel -> AUC : %f, trees = %d\" % (best, XGBeval[best]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"The classifiers are fairly close in performance with the  predictor sets making only small difference.\nThat being said, with ROC at ~0.7 the performance could likely be improved.\n\n**Things to contemplate:**\n* using an automated technique to select best features and to optimize the XGBModel"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}