{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Unsupervised Machine Learning on Wholesale Customers Data**","metadata":{}},{"cell_type":"code","source":"\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-11T05:26:19.096957Z","iopub.execute_input":"2021-08-11T05:26:19.097338Z","iopub.status.idle":"2021-08-11T05:26:19.104201Z","shell.execute_reply.started":"2021-08-11T05:26:19.097304Z","shell.execute_reply":"2021-08-11T05:26:19.103208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:19.106495Z","iopub.execute_input":"2021-08-11T05:26:19.106945Z","iopub.status.idle":"2021-08-11T05:26:19.117174Z","shell.execute_reply.started":"2021-08-11T05:26:19.106902Z","shell.execute_reply":"2021-08-11T05:26:19.116153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Reading the CSV data file and creating the data frame**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nwholesale_data = pd.read_csv('../input/wholesale-customers-data-set/Wholesale customers data.csv')\n\n\n# just printing random observations and attributes.\nprint(wholesale_data.sample(5))\nprint(\"\\n\\n\")\n\n# dropping of the attributes 'Channel' and 'Region'\n# 'Channel' represents the hotel, cafe or retail store\n# 'Region' represents the customer region \n# dropping of the attributes 'Channel' & ' Region' won't affect the clustering, as we are trying to relate the customers to the products\n# they buy in order to maximize the business\nwholesale_data.drop(labels=['Channel', 'Region'], axis=1, inplace=True)\nprint(wholesale_data.head(5))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:19.119101Z","iopub.execute_input":"2021-08-11T05:26:19.119403Z","iopub.status.idle":"2021-08-11T05:26:19.141189Z","shell.execute_reply.started":"2021-08-11T05:26:19.119373Z","shell.execute_reply":"2021-08-11T05:26:19.139941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Now to check for null values in the dataset**","metadata":{}},{"cell_type":"code","source":"# Gives us the basic analysis information of the wholesale customer dataset \nwholesale_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:19.142661Z","iopub.execute_input":"2021-08-11T05:26:19.142979Z","iopub.status.idle":"2021-08-11T05:26:19.157091Z","shell.execute_reply.started":"2021-08-11T05:26:19.142949Z","shell.execute_reply":"2021-08-11T05:26:19.155898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Since the above attributes have non-null values we do not need to do null checks for the attributes**","metadata":{}},{"cell_type":"markdown","source":"# **Let's take a look at the basic statisical data**\n","metadata":{}},{"cell_type":"code","source":"wholesale_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:19.158678Z","iopub.execute_input":"2021-08-11T05:26:19.159009Z","iopub.status.idle":"2021-08-11T05:26:19.203401Z","shell.execute_reply.started":"2021-08-11T05:26:19.158972Z","shell.execute_reply":"2021-08-11T05:26:19.202629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plotting the results of describe, can observe that mean >> median in almost all the cases: distribution are scre\n","metadata":{}},{"cell_type":"code","source":"wholesale_data.describe().transpose()[['mean','50%']].plot.barh(figsize=(10,6))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:19.204286Z","iopub.execute_input":"2021-08-11T05:26:19.204533Z","iopub.status.idle":"2021-08-11T05:26:19.466596Z","shell.execute_reply.started":"2021-08-11T05:26:19.204508Z","shell.execute_reply":"2021-08-11T05:26:19.465453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Now let us perform 'Standardization' and 'Decomposition'** **(Pre processing)**\n\n# **Previously we saw the features 'Channel' & 'Region' removed. These values have a low magnitude, whereas the other features like fresh, milk, grocery, frozen, detergents_paper, delicassen seem to have a higher magnitude and it is very necessary to bring these data to the same magnitude, because K- means algorithm is distance based and can have adverse effect with magnitude. Hence similar magnitude is preferred.**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nstandard_scaler = StandardScaler()\n# making sample mean= 0 and std=1\nscaled_data = standard_scaler.fit_transform(wholesale_data)\n\n_temp_PCA=PCA(6)\n_temp_PCA.fit_transform(scaled_data)\n\nplt.bar(range(1,7),_temp_PCA.explained_variance_ratio_,color='black')\nplt.xlabel('PCA dims')\nplt.title('Variance ratio by 6 features')\nplt.ylabel('variance preserve')\nplt.xticks(range(1,7))\ntraining_PCA_data = PCA(2).fit_transform(scaled_data)\n\n# print(scaled_data)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:19.467915Z","iopub.execute_input":"2021-08-11T05:26:19.468211Z","iopub.status.idle":"2021-08-11T05:26:19.734023Z","shell.execute_reply.started":"2021-08-11T05:26:19.468177Z","shell.execute_reply":"2021-08-11T05:26:19.733026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(scaled_data,columns=['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']).describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:19.735461Z","iopub.execute_input":"2021-08-11T05:26:19.735854Z","iopub.status.idle":"2021-08-11T05:26:19.770042Z","shell.execute_reply.started":"2021-08-11T05:26:19.735807Z","shell.execute_reply":"2021-08-11T05:26:19.768891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_PCA_DF = pd.DataFrame(training_PCA_data,columns=['d1','d2'])\ntraining_PCA_DF.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:19.772957Z","iopub.execute_input":"2021-08-11T05:26:19.77335Z","iopub.status.idle":"2021-08-11T05:26:19.786145Z","shell.execute_reply.started":"2021-08-11T05:26:19.773315Z","shell.execute_reply":"2021-08-11T05:26:19.784909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_PCA_DF.plot.scatter('d1','d2',alpha=.1,s=100,color='BLACK',figsize=(8,5))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:19.788787Z","iopub.execute_input":"2021-08-11T05:26:19.789334Z","iopub.status.idle":"2021-08-11T05:26:20.005642Z","shell.execute_reply.started":"2021-08-11T05:26:19.789236Z","shell.execute_reply":"2021-08-11T05:26:20.004503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Now to determine the number of clusters for the K-means algorithm, this can be calculated in 2 ways:**\n\n# 1. Elbow Method\n# 2. Silhouette Method","metadata":{}},{"cell_type":"markdown","source":"# **Using Elbow Method**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Using elbow methods to determine the number of clusters for the K-Means algorithm\n# WCSS = Within Cluster Sum of Squares\nwcss = [] \nfor i in range(1, 25):\n    km = KMeans(n_clusters = i, init = 'k-means++', \n                max_iter = 300, n_init = 10, random_state = 0)\n    km.fit(training_PCA_DF)\n    wcss.append(km.inertia_)\nplt.plot(range(1, 25), wcss)\nplt.legend(['wcss'])\nplt.title('The Elbow Method', fontsize = 20)\nplt.xlabel('Number of Clusters')\nplt.ylabel('wcss')\nplt.figure(figsize = (10,5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:20.007064Z","iopub.execute_input":"2021-08-11T05:26:20.007421Z","iopub.status.idle":"2021-08-11T05:26:22.647388Z","shell.execute_reply.started":"2021-08-11T05:26:20.007388Z","shell.execute_reply":"2021-08-11T05:26:22.646201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Using Silhouette Method**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score\n\n# Averaging the clusters with silhouette methods \nsil_avg=[]\ncluster_numbers = [4, 5, 6,7] \nprint(\"Average Silhouette Method\\n\")\nfor one_cluster in cluster_numbers: \n    cluster = KMeans(n_clusters = one_cluster) \n    cluster_labels = cluster.fit_predict(training_PCA_DF) \n    silhouette_avg = silhouette_score(training_PCA_DF, cluster_labels)\n    sil_avg.append([one_cluster,silhouette_avg])\n    print(f\"For clusters = {one_cluster}\")\n    print(f\"The average silhouette score for {one_cluster} is = {silhouette_avg}\")\nsil_avg=np.array(sil_avg)\nplt.plot(sil_avg[:,0],sil_avg[:,1],linestyle='dashed')\nplt.xlabel('Number of Clusters')\nplt.ylabel('avg_sil')\nplt.legend(['avg_sil'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:22.648532Z","iopub.execute_input":"2021-08-11T05:26:22.648905Z","iopub.status.idle":"2021-08-11T05:26:23.368941Z","shell.execute_reply.started":"2021-08-11T05:26:22.648871Z","shell.execute_reply":"2021-08-11T05:26:23.366097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **As we can from the above averages that most values are closer to 5, meaning that number of cluster are fixed to 5**","metadata":{}},{"cell_type":"markdown","source":"# **Now we perform the K-Means clustering and plot the results in a scatterplot**","metadata":{}},{"cell_type":"code","source":"print(training_PCA_DF.sample(10))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:23.369978Z","iopub.execute_input":"2021-08-11T05:26:23.370258Z","iopub.status.idle":"2021-08-11T05:26:23.379583Z","shell.execute_reply.started":"2021-08-11T05:26:23.370231Z","shell.execute_reply":"2021-08-11T05:26:23.378339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clusters_K_Means = 5\nrandom_state_K_Means = 0\n\nkmean = KMeans(n_clusters=clusters_K_Means, random_state=\n               random_state_K_Means).fit(training_PCA_DF)\nkmean_Y = kmean.predict(training_PCA_DF)\nlab = kmean.labels_\n# print(np.unique(lab))\nplt.figure(figsize=(10,5))\nplt.title(f\"K- Means with cluster value = {clusters_K_Means}\",fontsize=15)\nplt.scatter(training_PCA_DF['d1'], training_PCA_DF['d2'],c = kmean_Y, s=105, \n        alpha=0.6,marker='o')\nplt.xlabel(\"X Axis\")\nplt.ylabel(\"Y Axis\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:23.381382Z","iopub.execute_input":"2021-08-11T05:26:23.381858Z","iopub.status.idle":"2021-08-11T05:26:23.615653Z","shell.execute_reply.started":"2021-08-11T05:26:23.381814Z","shell.execute_reply":"2021-08-11T05:26:23.614458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy.cluster.hierarchy as shc\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.cluster import AgglomerativeClustering\n\n#Agglomerative Clustering\n\nagglometric_clustering= AgglomerativeClustering(n_clusters=clusters_K_Means,affinity = 'euclidean',linkage = 'ward')\nagglometric_clustering_y = agglometric_clustering.fit_predict(training_PCA_DF)\nplt.figure(figsize =(10,5))\nplt.scatter(training_PCA_DF['d1'], training_PCA_DF['d2'],c = agglometric_clustering_y, s=80, alpha=0.6,marker='o')\nplt.title('Agglomerative Clustering',fontsize = 20)\nplt.show()\n\nplt.figure(figsize=(10,5))\nplt.title('Agglomerative Clustering : Dendrogram',fontsize = 20)\ndend=shc.dendrogram(shc.linkage(training_PCA_DF,method='ward') ,truncate_mode='level', p=5) \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:23.617278Z","iopub.execute_input":"2021-08-11T05:26:23.617693Z","iopub.status.idle":"2021-08-11T05:26:24.183748Z","shell.execute_reply.started":"2021-08-11T05:26:23.61765Z","shell.execute_reply":"2021-08-11T05:26:24.182508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster=AgglomerativeClustering(n_clusters=clusters_K_Means,affinity='euclidean',linkage='ward')\ncluster.fit_predict(training_PCA_DF)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:24.185355Z","iopub.execute_input":"2021-08-11T05:26:24.185833Z","iopub.status.idle":"2021-08-11T05:26:24.203043Z","shell.execute_reply.started":"2021-08-11T05:26:24.185786Z","shell.execute_reply":"2021-08-11T05:26:24.201882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import Birch\n\n\n#birch clustering\nbirch_clustering = Birch(branching_factor=500, n_clusters=clusters_K_Means, threshold=1.5)\nbirch_clustering.fit(training_PCA_DF)\nlabels = birch_clustering.predict(training_PCA_DF)\n\nplt.title('Birch Clustering',fontsize = 20)\nplt.scatter(training_PCA_DF['d1'], training_PCA_DF['d2'], c=labels,alpha=0.6,marker='o',s=150)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:24.204376Z","iopub.execute_input":"2021-08-11T05:26:24.204943Z","iopub.status.idle":"2021-08-11T05:26:24.421398Z","shell.execute_reply.started":"2021-08-11T05:26:24.204909Z","shell.execute_reply":"2021-08-11T05:26:24.42057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\n\n# mini batch clustering\n\nminibatch_clustering = MiniBatchKMeans(n_clusters=clusters_K_Means, random_state=random_state_K_Means)\nminibatch_clustering.fit(training_PCA_DF)\n\nlabels = minibatch_clustering.predict(training_PCA_DF)\nplt.title('MiniBatchKMeans clustering',fontsize = 20)\nplt.scatter(training_PCA_DF['d1'], training_PCA_DF['d2'], c=labels,alpha=0.6,marker='o',s=150)\nplt.figure(figsize=(20,15))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-11T05:26:24.4226Z","iopub.execute_input":"2021-08-11T05:26:24.422892Z","iopub.status.idle":"2021-08-11T05:26:24.62192Z","shell.execute_reply.started":"2021-08-11T05:26:24.422865Z","shell.execute_reply":"2021-08-11T05:26:24.620858Z"},"trusted":true},"execution_count":null,"outputs":[]}]}