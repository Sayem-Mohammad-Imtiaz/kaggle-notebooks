{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Note: this notebook was run on Kaggle, so you need to modify path to use in Colab"},{"metadata":{"id":"LUjfjHcxvuIL"},"cell_type":"markdown","source":"# Lab 2-1: Groupby operations"},{"metadata":{"id":"8511u3SyvuIN"},"cell_type":"markdown","source":"Some imports:"},{"metadata":{"id":"-82Kv9gfvuIN","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ntry:\n    import seaborn\nexcept ImportError:\n    pass\n\npd.options.display.max_rows = 10","execution_count":null,"outputs":[]},{"metadata":{"id":"SgvEhpi6vuIS"},"cell_type":"markdown","source":"## Some 'theory': the groupby operation (split-apply-combine)\n\nThe \"group by\" concept: we want to **apply the same function on subsets of your dataframe, based on some key to split the dataframe in subsets**\n\nThis operation is also referred to as the \"split-apply-combine\" operation, involving the following steps:\n\n* **Splitting** the data into groups based on some criteria\n* **Applying** a function to each group independently\n* **Combining** the results into a data structure\n\nSimilar to SQL `GROUP BY`"},{"metadata":{"id":"7HDLk72VvuIT"},"cell_type":"markdown","source":"The example of the image in pandas syntax:"},{"metadata":{"id":"rUowYLQlvuIU","outputId":"8cf37cd5-a473-4078-b7ee-62d16dd6240b","trusted":true},"cell_type":"code","source":"df = pd.DataFrame({'key':['A','B','C','A','B','C','A','B','C'],\n                   'data': [0, 5, 10, 5, 10, 15, 10, 15, 20]})\ndf","execution_count":null,"outputs":[]},{"metadata":{"id":"k76KZA4HvuIY"},"cell_type":"markdown","source":"Using the filtering and reductions operations we have seen in the previous notebooks, we could do something like:\n\n\n    df[df['key'] == \"A\"].sum()\n    df[df['key'] == \"B\"].sum()\n    ...\n\nBut pandas provides the `groupby` method to do this:"},{"metadata":{"id":"7sZCBXeJvuIY","outputId":"c64f56c7-2e00-4562-adc3-5d898ddd8762","trusted":true},"cell_type":"code","source":"df.groupby('key').aggregate(np.sum)  # 'sum'","execution_count":null,"outputs":[]},{"metadata":{"id":"6inTzR8LvuIc","outputId":"43a20c61-cf3d-4295-ee6e-22dc351e6cd8","trusted":true},"cell_type":"code","source":"df.groupby('key').sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"kHZ1FdNMvuIf"},"cell_type":"markdown","source":"And many more methods are available. "},{"metadata":{"id":"NI0FP_cKvuIf"},"cell_type":"markdown","source":"## And now applying this on some real data"},{"metadata":{"id":"EbuAiUR5vuIg"},"cell_type":"markdown","source":"We go back to the titanic survival data:"},{"metadata":{"id":"FNdeNUISvuIg","outputId":"db2dc9ca-633c-47a9-b33f-abae0517f789","trusted":true},"cell_type":"code","source":"!git clone https://gist.github.com/michhar/2dfd2de0d4f8727f873422c5d959fff5\n# Here is just the sample dir, you should correct your dir\ndf = pd.read_csv(\"./2dfd2de0d4f8727f873422c5d959fff5/titanic.csv\")","execution_count":null,"outputs":[]},{"metadata":{"id":"8WPptFkQvuIk","outputId":"508c89fc-90d2-4562-f554-b6920a479c0b","trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"af8lcwpkvuIn"},"cell_type":"markdown","source":"<div class=\"alert alert-success\">\n    <b>EXERCISE</b>: Using groupby(), calculate the average age for each sex.\n</div>"},{"metadata":{"clear_cell":true,"id":"zruicCatvuIn","outputId":"4fab941d-b85c-434b-fd25-9114c6ba0b75","trusted":true},"cell_type":"code","source":"df.groupby(\"Sex\")[\"Age\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"c6hZ4XoivuIp"},"cell_type":"markdown","source":"<div class=\"alert alert-success\">\n    <b>EXERCISE</b>: Calculate the average survival ratio for all passengers.\n</div>"},{"metadata":{"clear_cell":true,"id":"e-UmtEKbvuIq","outputId":"cb4882e0-ebc9-40cd-81e7-62b07eb1b40a","trusted":true},"cell_type":"code","source":"df[\"Survived\"].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"QygWemyVvuIs"},"cell_type":"markdown","source":"<div class=\"alert alert-success\">\n    <b>EXERCISE</b>: Calculate this survival ratio for all passengers younger that 25 (remember: filtering/boolean indexing).\n</div>"},{"metadata":{"clear_cell":true,"id":"W5IYMYCnvuIt","outputId":"ff8fd980-927f-4489-bd0c-402f1b63b821","trusted":true},"cell_type":"code","source":"df[df[\"Age\"] <= 25][\"Survived\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"09eHPDn2vuIw"},"cell_type":"markdown","source":"<div class=\"alert alert-success\">\n    <b>EXERCISE</b>: Is there a difference in this survival ratio between the sexes? (tip: write the above calculation of the survival ratio as a function)\n</div>"},{"metadata":{"clear_cell":true,"id":"YOt79OpVvuIw","outputId":"627a5c1a-31f5-4b43-db55-27f9c0cd3a29","trusted":true},"cell_type":"code","source":"df.groupby(\"Sex\")[\"Survived\"].mean()","execution_count":null,"outputs":[]},{"metadata":{"id":"-y18TPbgvuI4"},"cell_type":"markdown","source":"<div class=\"alert alert-success\">\n    <b>EXERCISE</b>: Make a bar plot of the survival ratio for the different classes ('Pclass' column).\n</div>"},{"metadata":{"clear_cell":true,"id":"3T1VNPKLvuI5","outputId":"17c51326-219d-49d9-b9fb-96b50e0a4e1d","trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(x=\"Pclass\", hue=\"Survived\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"id":"267_jnShvuI7"},"cell_type":"markdown","source":"If you are ready, more groupby exercises can be found in the \"Advanded groupby operations\" notebook."},{"metadata":{"id":"pYnWScaldGwR"},"cell_type":"markdown","source":"# 2-3 KNN for Diabetes"},{"metadata":{"id":"yoGP5Z8KdGwb"},"cell_type":"markdown","source":"Predict if a person gets infected by diabetes. Use the Diabetes dataset."},{"metadata":{"id":"0jrME9Exl08g","trusted":true},"cell_type":"code","source":"# load data\nimport sklearn\nfrom sklearn import datasets\ndataset = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")\ndataset","execution_count":null,"outputs":[]},{"metadata":{"id":"PH5yiDJ8nsTs"},"cell_type":"markdown","source":"## 1) Data discovery"},{"metadata":{"id":"tsbqWCfsn3AD","outputId":"b7ac416d-7271-4601-a849-0254f639985f","trusted":true},"cell_type":"code","source":"# get how many instances (rows) and how many attributes (columns)\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"YvxCV_v3oATQ","outputId":"5182c9be-f649-45c4-e6d6-d17e0268032d","trusted":true},"cell_type":"code","source":"# show basic info: max, min, mean of dataset columns\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"Nkn18fOKoB-2","outputId":"4b24b498-a121-4142-bb5a-8abd4c2f10f2","trusted":true},"cell_type":"code","source":"# display statistical data of columns (including categorical columns)\ndataset.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"id":"4dF4q-Z3oIO8","outputId":"d0ddcd1b-68c4-4a56-ac6f-254727be84a3","trusted":true},"cell_type":"code","source":"# show some first rows\ndataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"id":"3KSf7FK-oKTW","outputId":"21ffc25e-a216-4790-fefe-46e06f080cf8","trusted":true},"cell_type":"code","source":"# show some last rows\ndataset.tail(3)","execution_count":null,"outputs":[]},{"metadata":{"id":"rTABCJtBoN-U","outputId":"53e9958d-ab9f-4045-9de4-3729991f47b8","trusted":true},"cell_type":"code","source":"# numbers of instances (rows) that belong to each class. \ndataset.groupby(\"Outcome\").size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Another way\ndataset[\"Outcome\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"Gv3slM1Noeec"},"cell_type":"markdown","source":"### Visualization"},{"metadata":{"id":"daj5coaIol5c","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"IxEaRvYvondc"},"cell_type":"markdown","source":"### Pairplot"},{"metadata":{"id":"rZY5GUuDomeF","outputId":"f8e08625-12f5-4646-a9ee-e3fa44dca011","trusted":true},"cell_type":"code","source":"sns.pairplot(dataset, hue=\"Outcome\", height=3, markers=[\"o\", \"s\"])","execution_count":null,"outputs":[]},{"metadata":{"id":"0YGqhRtepFB8"},"cell_type":"markdown","source":"### Boxplot"},{"metadata":{"id":"um4XUAUcpHUL","outputId":"9717bf5c-4b12-42b6-8a30-c13eb43b8644","trusted":true},"cell_type":"code","source":"plt.figure()\ndataset.boxplot(by=\"Outcome\", figsize=(15, 10))","execution_count":null,"outputs":[]},{"metadata":{"id":"kPYO0za6pKdK"},"cell_type":"markdown","source":"## 2) Data preprocessing"},{"metadata":{"id":"T7o_jOMGpOJ6","outputId":"02d3de19-0a46-44c9-9bc4-42865a00b7fc","trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":null,"outputs":[]},{"metadata":{"id":"gGjDXuoqzIGS","trusted":true},"cell_type":"code","source":"X = dataset[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']].values\ny = dataset[\"Outcome\"].values","execution_count":null,"outputs":[]},{"metadata":{"id":"IUbNONoewCTh"},"cell_type":"markdown","source":"####  Spliting dataset into training set and test set"},{"metadata":{"id":"qYshOTb7lmzF","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"id":"YLrovXOuw7BM","outputId":"10f467d1-7dbf-40fb-af59-fc6ae82b738f","trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"DCXWwcgqw7g1","outputId":"2afa6859-0bd1-47e9-be61-e347f21ee792","trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"UpiDanZ2w8qE","outputId":"77f91e10-58d7-4654-9ab3-f68110664dbf","trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"JMU-22EXw_vU","outputId":"33bb8574-32e3-49c8-a5ee-40cd2ad498cb","trusted":true},"cell_type":"code","source":"y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"Y_tBD53uyrtp"},"cell_type":"markdown","source":"## 3) Using KNN for classification"},{"metadata":{"id":"oRNNt1P0yyZB"},"cell_type":"markdown","source":"####  Build model"},{"metadata":{"id":"k-L2_zoqysCx","trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"id":"mNMQGu9xy0yY"},"cell_type":"markdown","source":"#### Change ``k`` to find the best value"},{"metadata":{"id":"YDVMdyyOy1vp","trusted":true},"cell_type":"code","source":"# Setup arrays to store training and test accuracies\nneighbors = np.arange(1, 11)\ntrain_accuracy =np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i, k in enumerate(neighbors):\n    # Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    # Fit the model\n    knn.fit(X_train, y_train)\n    \n    # Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n    \n    # Compute accuracy on the test set\n    test_accuracy[i] = knn.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"dqtnYIvuzkOi","outputId":"177a0f07-8cd7-4f19-f7f7-8ae8af8ab207","trusted":true},"cell_type":"code","source":"# Generate plot\nplt.title('k-NN Varying number of neighbors')\nplt.plot(neighbors, test_accuracy, label='Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label='Training accuracy')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"74MuBt2a2lIT"},"cell_type":"markdown","source":"#### Choose ``k = 6``"},{"metadata":{"id":"ZIcjEaK22twU","outputId":"7156184b-53b7-4b4f-8057-0d2f9b1cafb4","trusted":true},"cell_type":"code","source":"classifier = KNeighborsClassifier(n_neighbors=6)\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"g2hvztmB2077"},"cell_type":"markdown","source":"####  Prediction"},{"metadata":{"id":"Gvjy81AJ21cd","outputId":"4a62f727-a339-443e-a61c-576752160711","trusted":true},"cell_type":"code","source":"# Predicting on the test set\ny_pred = classifier.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"id":"Xg4YnJBG3B54"},"cell_type":"markdown","source":"##### Accuracy"},{"metadata":{"id":"Itmi91HS3CnO","outputId":"1b051405-adef-4f0b-fc94-ea8f487b7576","trusted":true},"cell_type":"code","source":"# Using accuracy_score\naccuracy = accuracy_score(y_test, y_pred) * 100\nprint(f\"Accuracy of our model is equal {round(accuracy, 2)}%.\")","execution_count":null,"outputs":[]},{"metadata":{"id":"AHwT2nYv3vzA","outputId":"0ed6bd35-0630-4eb9-e19a-b3213ebe747b","trusted":true},"cell_type":"code","source":"knn.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seeds = list(range(20))\nfor seed in seeds:\n    from sklearn.model_selection import train_test_split\n    X = dataset[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']].values\n    y = dataset[\"Outcome\"].values\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n    # Setup arrays to store training and test accuracies\n    neighbors = np.arange(1, 35)\n    train_accuracy = np.empty(len(neighbors))\n    test_accuracy = np.empty(len(neighbors))\n\n    for i, k in enumerate(neighbors):\n        # Setup a knn classifier with k neighbors\n        knn = KNeighborsClassifier(n_neighbors=k)\n\n        # Fit the model\n        knn.fit(X_train, y_train)\n\n        # Compute accuracy on the training set\n        train_accuracy[i] = knn.score(X_train, y_train)\n\n        # Compute accuracy on the test set\n        test_accuracy[i] = knn.score(X_test, y_test)\n    mean_acc = np.array(test_accuracy)\n    print(f\"Seed: {seed}, best test acc mean: {np.max(mean_acc)}, best k: {np.argmax(mean_acc)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\nKNN is extremely robust to the data. With different split, we get different result. See code above.\n\nI personally not recommend using KNN as baseline because unstability.\n"},{"metadata":{},"cell_type":"markdown","source":"# Lab 2-4: CustomerChurn & BigMart Sales"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ncustomer = pd.read_csv(\"../input/ai-lab-24/CustomerChurn.csv\")\ncustomer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"customer[\"International plan\"] = LabelEncoder().fit_transform(customer[\"International plan\"])\ncustomer[\"Voice mail plan\"] = LabelEncoder().fit_transform(customer[\"Voice mail plan\"])\ncustomer[\"Churn\"] = LabelEncoder().fit_transform(customer[\"Churn\"])\ncustomer[\"State\"] = LabelEncoder().fit_transform(customer[\"State\"])\ncustomer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(\"Churn\", data=customer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to imbalance label, we should use stratified split."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(\n    customer.drop([\"Churn\"], axis=1), customer[\"Churn\"], test_size=0.2, random_state=42, stratify = customer[\"Churn\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"k_nei = list(range(1, 30))\ntr, val = [], []\nfor i in k_nei:\n    knn = KNeighborsClassifier(n_neighbors=i).fit(X_train, y_train)\n    tr.append(knn.score(X_train, y_train))\n    val.append(knn.score(X_test, y_test))\n\nimport matplotlib.pyplot as plt\nplt.plot(tr)\nplt.plot(val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking Classifer, an advanced technique"},{"metadata":{"trusted":true},"cell_type":"code","source":"gaussnb = GaussianNB().fit(X_train, y_train)\nknn = KNeighborsClassifier(n_neighbors=10).fit(X_train, y_train)\nlr = LogisticRegression(max_iter=5000).fit(X_train, y_train)\nrf = RandomForestClassifier(n_estimators=300, random_state=42).fit(X_train, y_train) # My favorite algorithm, actually it's GBDT\ngaussnb.score(X_test, y_test), knn.score(X_test, y_test), lr.score(X_train, y_train), rf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators =[\n    (\"rf\", GaussianNB()),\n    (\"knn\", KNeighborsClassifier(n_neighbors=15)),\n    (\"gaussnb\", RandomForestClassifier(random_state=41569))\n]\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=5000))\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BigMart sales data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/bigmart-sales-data/Train.csv\")\ntest = pd.read_csv(\"../input/bigmart-sales-data/Test.csv\")\nsales = pd.concat([train, test], axis=0)\nsales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The best way to fill NaN values, to me, is not to fill it."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Outlet_Size\"].fillna(\"NotAvail\", inplace=True)\ntest[\"Outlet_Size\"].fillna(\"NotAvail\", inplace=True)\nsales[\"Outlet_Size\"].fillna(\"NotAvail\", inplace=True)\nsales.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.columns\nsales[['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility',\n       'Item_Type', 'Item_MRP', 'Outlet_Identifier',\n       'Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type',\n       'Outlet_Type']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales[\"Item_Fat_Content\"].unique()\nd = {\n    \"Low Fat\": 0,\n    \"Regular\": 1,\n    \"low fat\": 0,\n    \"LF\": 0,\n    \"reg\": 1\n}\nsales[\"Item_Fat_Content\"] = sales[\"Item_Fat_Content\"].map(d)\ntrain[\"Item_Fat_Content\"] = train[\"Item_Fat_Content\"].map(d)\ntest[\"Item_Fat_Content\"] = test[\"Item_Fat_Content\"].map(d)\nsales","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = [\"Item_Identifier\", \"Item_Fat_Content\", \"Item_Type\", \"Outlet_Identifier\", \"Outlet_Size\", \"Outlet_Location_Type\", \"Outlet_Type\"]\nfor col in categorical_cols:\n    print(sales[col].value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Baselines\nCatBoost is a GBDT library like XGBoost or LightGBM. This is my favorite ML algorithm.\n\nNOT AVAILABLE IN COLAB YET!"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, val, train_labels, val_labels  = train_test_split(train.drop(\"Item_Outlet_Sales\", axis=1), \n                                                         train[\"Item_Outlet_Sales\"], random_state=34125, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import catboost as cb\n\ntrain_pool = cb.Pool(train, train_labels, cat_features=categorical_cols)\nval_pool = cb.Pool(val, val_labels, cat_features=categorical_cols)\ntest_pool = cb.Pool(test, cat_features=categorical_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = cb.CatBoostRegressor(iterations=1000)\nmodel.fit(train_pool, eval_set=val_pool, verbose=100, plot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = test[[\"Item_Identifier\", \"Outlet_Identifier\"]]\nsub[\"Item_Outlet_Sales\"] = model.predict(test_pool).clip(0)\nsub.to_csv(\"submission.csv\", index=False)\nsub","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Public leaderboard RMSE score: 1151\nTest score is not available until competition ends. https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}