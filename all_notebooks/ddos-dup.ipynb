{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom IPython.display import display\nimport gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Loading","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# define datatype for the columns in the dataset\n# loading data with predefined datatype helps improve RAM utilization\ndtypes = {\n    'Src IP': 'category',\n    'Src Port': 'uint16',\n    'Dst IP': 'category',\n    'Dst Port': 'uint16',\n    'Protocol': 'category',\n    'Flow Duration': 'uint32',\n    'Tot Fwd Pkts': 'uint32',\n    'Tot Bwd Pkts': 'uint32',\n    'TotLen Fwd Pkts': 'float32',\n    'TotLen Bwd Pkts': 'float32',\n    'Fwd Pkt Len Max': 'float32',\n    'Fwd Pkt Len Min': 'float32',\n    'Fwd Pkt Len Mean': 'float32',\n    'Fwd Pkt Len Std': 'float32',\n    'Bwd Pkt Len Max': 'float32',\n    'Bwd Pkt Len Min': 'float32',\n    'Bwd Pkt Len Mean': 'float32',\n    'Bwd Pkt Len Std': 'float32',\n    'Flow Byts/s': 'float32',\n    'Flow Pkts/s': 'float32',\n    'Flow IAT Mean': 'float32',\n    'Flow IAT Std': 'float32',\n    'Flow IAT Max': 'float32',\n    'Flow IAT Min': 'float32',\n    'Fwd IAT Tot': 'float32',\n    'Fwd IAT Mean': 'float32',\n    'Fwd IAT Std': 'float32',\n    'Fwd IAT Max': 'float32',\n    'Fwd IAT Min': 'float32',\n    'Bwd IAT Tot': 'float32',\n    'Bwd IAT Mean': 'float32',\n    'Bwd IAT Std': 'float32',\n    'Bwd IAT Max': 'float32',\n    'Bwd IAT Min': 'float32',\n    'Fwd PSH Flags': 'category',\n    'Bwd PSH Flags': 'category',\n    'Fwd URG Flags': 'category',\n    'Bwd URG Flags': 'category',\n    'Fwd Header Len': 'uint32',\n    'Bwd Header Len': 'uint32',\n    'Fwd Pkts/s': 'float32',\n    'Bwd Pkts/s': 'float32',\n    'Pkt Len Min': 'float32',\n    'Pkt Len Max': 'float32',\n    'Pkt Len Mean': 'float32',\n    'Pkt Len Std': 'float32',\n    'Pkt Len Var': 'float32',\n    'FIN Flag Cnt': 'category',\n    'SYN Flag Cnt': 'category',\n    'RST Flag Cnt': 'category',\n    'PSH Flag Cnt': 'category',\n    'ACK Flag Cnt': 'category',\n    'URG Flag Cnt': 'category',\n    'CWE Flag Count': 'category',\n    'ECE Flag Cnt': 'category',\n    'Down/Up Ratio': 'float32',\n    'Pkt Size Avg': 'float32',\n    'Fwd Seg Size Avg': 'float32',\n    'Bwd Seg Size Avg': 'float32',\n    'Fwd Byts/b Avg': 'uint32',\n    'Fwd Pkts/b Avg': 'uint32',\n    'Fwd Blk Rate Avg': 'uint32',\n    'Bwd Byts/b Avg': 'uint32',\n    'Bwd Pkts/b Avg': 'uint32',\n    'Bwd Blk Rate Avg': 'uint32',\n    'Subflow Fwd Pkts': 'uint32',\n    'Subflow Fwd Byts': 'uint32',\n    'Subflow Bwd Pkts': 'uint32',\n    'Subflow Bwd Byts': 'uint32',\n    'Init Fwd Win Byts': 'uint32',\n    'Init Bwd Win Byts': 'uint32',\n    'Fwd Act Data Pkts': 'uint32',\n    'Fwd Seg Size Min': 'uint32',\n    'Active Mean': 'float32',\n    'Active Std': 'float32',\n    'Active Max': 'float32',\n    'Active Min': 'float32',\n    'Idle Mean': 'float32',\n    'Idle Std': 'float32',\n    'Idle Max': 'float32',\n    'Idle Min': 'float32',\n    'Label': 'category'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the data\ndf = pd.read_csv(\n    '/kaggle/input/ddos-datasets/ddos_balanced/final_dataset.csv',\n    dtype=dtypes,\n    parse_dates=['Timestamp'],\n    usecols=[*dtypes.keys(), 'Timestamp'],\n    engine='c',\n    low_memory=True\n)\ndel dtypes\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_mem_usage(df):\n    mb = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(mb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling missing values\nWe find the % of missing values for each column. If a column has more than 50% missing values, then we drop the entire column. If the column has less than 5% missing values, then we drop those rows where the column value is missing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"colsToDrop = np.array([])\ndropnaCols = np.array([])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing = df.isna().sum()\nmissing = pd.DataFrame({'count': missing, '% of total': missing/len(df)*100}, index=df.columns)\nmissing.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that only Flow Byts/s has about 0.2% missing values. We therefore drop the corresponding rows.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"colsToDrop = np.union1d(colsToDrop, missing[missing['% of total'] >= 50].index.values)\ndropnaCols = missing[(missing['% of total'] > 0) & (missing['% of total'] <= 5)].index.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handling incorrect/corrupt data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"From the data statistics computed earlier, we can see that some columns have only one value. Such columns will not provide any significant information for our classification task. We will therefore drop these columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"colsToDrop = np.union1d(colsToDrop, ['Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg'])\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now see some statistics for the categorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# counting unique values and checking for skewness in the data\nrowbuilder = lambda col: {'col': col, 'unique_values': df[col].nunique(), 'most_frequent_value': df[col].value_counts().index[0],'frequency': df[col].value_counts(normalize=True).values[0]}\nfrequency = [rowbuilder(col) for col in df.select_dtypes(include=['category']).columns]\nstats = pd.DataFrame(frequency).sort_values(by='frequency', ascending=False)\nstats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that some categorical variables have very high dominance of a single category. For classification task, such categorical variables will be of little use. We will therefore drop those columns where the dominance of the most frequent category is more than 95%","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"skewed = stats[stats['frequency'] >= 0.95]\ncolsToDrop = np.union1d(colsToDrop, skewed['col'].values)\ncolsToDrop\ndel skewed\ndel rowbuilder\ndel frequency\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also observe that some columns have infinity values. ML algorithms cannot work on infinity values. There are two ways to handle this. First, impute the infinity values to contain very large numbers less than infinity. Second, drop the rows that contain infinity values. In our case, only ~ 2% of the data contains infinity values. Thus we will adopt the second strategy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Flow Byts/s'].replace(np.inf, np.nan, inplace=True)\ndf['Flow Pkts/s'].replace(np.inf, np.nan, inplace=True)\ndropnaCols = np.union1d(dropnaCols, ['Flow Byts/s', 'Flow Pkts/s'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colsToDrop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropnaCols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# perform actual drop\ndf.drop(columns=colsToDrop, inplace=True)\ndf.dropna(subset=dropnaCols, inplace=True)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also observe from the data statistics that some columns have negative values. Based on our understanding of the variables, negative values indicate incorrect/faulty data. We will therefore filter out all the negative values from our dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"negValCols = ['Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Max', 'Flow IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Max', 'Bwd IAT Min']\nfor col in negValCols:\n    df = df[df[col] >= 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print_mem_usage(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train-test split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df, df[\"Label\"]):\n    traindf = df.iloc[train_index]\ntraindf.to_csv('train.csv', index=False)\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# traindf = pd.read_csv(\n#     'train.csv',\n#     dtype=dtypes,\n#     parse_dates=['Timestamp'],\n#     engine='c',\n#     low_memory=True\n# )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the target variable\nlabelCount = traindf['Label'].value_counts(normalize=True)*100\nax = sns.barplot(x=labelCount.index, y=labelCount.values)\nax1 = ax.twinx()\nax.set_ylabel('Frequency [%]')\nax1.set_ylabel(\"Count (in millions)\")\nax1.set_ylim(0, len(traindf)/10**6)\nax.set_ylim(0, 100)\nplt.title('Target Variable')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our target variable is very balanced. Dataset contains almost equal instances of ddos and benign network activity.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt = pd.crosstab(traindf['Protocol'], traindf['Label'])\ncnt = cnt.stack().reset_index().rename(columns={0: 'Count'})\nsns.barplot(x=cnt['Protocol'], y=cnt['Count'], hue=cnt['Label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getNetworkClass(col):\n    networkClasses = col.str.split('.',n=1, expand=True)[0]\n    networkClasses = networkClasses.astype('uint8')\n    networkClasses = pd.cut(\n        networkClasses,\n        bins=[0, 127, 191, 223, 239, np.inf],\n        labels=['A', 'B', 'C', 'D', 'E'],\n        include_lowest=True\n    )\n    return networkClasses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"srcNetworkClass = getNetworkClass(traindf['Src IP'])\ndstNetworkClass = getNetworkClass(traindf['Dst IP'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt = pd.crosstab(srcNetworkClass, traindf['Label'], rownames=['Class'])\ncnt = cnt.stack().reset_index().rename(columns={0: 'Count'})\nsns.barplot(x=cnt['Class'], y=cnt['Count'], hue=cnt['Label'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that ddos attacks Source IPs belong to primarily Class A & Class B networks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cnt = pd.crosstab(dstNetworkClass, traindf['Label'], rownames=['Class'])\ncnt = cnt.stack().reset_index().rename(columns={0: 'Count'})\nsns.barplot(x=cnt['Class'], y=cnt['Count'], hue=cnt['Label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del srcNetworkClass\ndel dstNetworkClass\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = traindf.select_dtypes(exclude=['category', 'datetime64[ns]']).columns\nfwd_cols = [col for col in num_cols if 'Fwd' in col]\nbwd_cols = [col for col in num_cols if 'Bwd' in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = traindf[fwd_cols].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = np.triu(np.ones_like(corr, dtype=np.bool))\nplt.subplots(figsize=(10,10))\nsns.heatmap(corr, mask=mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getCorrelatedFeatures(corr):\n    correlatedFeatures = set()\n    for i in range(len(corr.columns)):\n        for j in range(i):\n            if abs(corr.iloc[i, j]) > 0.8:\n                correlatedFeatures.add(corr.columns[i])\n    return correlatedFeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlatedFeatures = set()\ncorrelatedFeatures = correlatedFeatures | getCorrelatedFeatures(corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = traindf[bwd_cols].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = np.triu(np.ones_like(corr, dtype=np.bool))\nplt.subplots(figsize=(10,10))\nsns.heatmap(corr, mask=mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlatedFeatures = correlatedFeatures | getCorrelatedFeatures(corr)\ncorrelatedFeatures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So our intuition was correct. There is high correlation in data. Lets drop these columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf.drop(columns=correlatedFeatures, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets check correlation between forward & backward direction predictors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = set(traindf.select_dtypes(exclude=['category', 'datetime64[ns]']).columns)\ncols = [col for col in num_cols if 'Fwd' in col or 'Bwd' in col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = traindf[cols].corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = np.triu(np.ones_like(corr, dtype=np.bool))\nplt.subplots(figsize=(10,10))\nsns.heatmap(corr, mask=mask)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"correlatedFeatures = correlatedFeatures | getCorrelatedFeatures(corr)\ntraindf.drop(columns=getCorrelatedFeatures(corr), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = traindf.select_dtypes(exclude=['category', 'datetime64[ns]']).columns\nskew = traindf[num_cols].skew().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skew","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A skew value of greater than 1 determines very high skew in the data. We will need to perform log transformation to lower the skew","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Data Pipeline\n### Pipeline for numerical columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"del traindf\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df, df[\"Label\"]):\n    traindf = df.iloc[train_index]\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logTransformation(X):\n    for col in X.columns:\n        X.loc[X[col] == 0] = 1\n    return np.log10(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dropCorrelatedFeatures(X):\n    return X.drop(columns=correlatedFeatures)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_pipeline = Pipeline([\n    ('dropCorrelatedFeatures', FunctionTransformer(dropCorrelatedFeatures)),\n    ('logTransformation', FunctionTransformer(logTransformation))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = list(traindf.columns[(traindf.dtypes != 'category') &  (traindf.dtypes != 'datetime64[ns]')])\nX = num_pipeline.transform(traindf[num_cols])\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pipeline for categorical data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def addNetworkClasses(X):\n    X['SrcIPClass'] = getNetworkClass(X['Src IP'])\n    X['DstIPClass'] = getNetworkClass(X['Dst IP'])\n    return X.drop(columns=['Src IP', 'Dst IP'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_pipeline = Pipeline([\n    ('AddNewCols', FunctionTransformer(addNetworkClasses)),\n    ('OrdinalEncoding', OrdinalEncoder())\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Full Pipeline","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cols = list(traindf.columns[(traindf.dtypes != 'category') &  (traindf.dtypes != 'datetime64[ns]')])\ncat_cols = list(traindf.columns[traindf.dtypes == 'category'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_pipeline = ColumnTransformer([\n    ('numColTransformer', num_pipeline, num_cols),\n    ('catColTransformer', cat_pipeline, cat_cols)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = full_pipeline.fit_transform(traindf)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}