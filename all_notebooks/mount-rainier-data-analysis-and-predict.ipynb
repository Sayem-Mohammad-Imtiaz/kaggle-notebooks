{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"#### we are to work on data climbing and weather , data was collected for the period between 2014 and 2015 , for some period we have data for climbing not for weather and the opposite for other period as we will see later , our object here is to study the trend off attempts , and styudy the effect of the weather on success percentage , so we can predict number od attempts and success percentage in the future.\n\n#### for that purpose we have to get the summation of attempts and successes for each day , then get the weather data for these days to study how attempts vary , and study the effect of weather on successsful percentage."},{"metadata":{},"cell_type":"markdown","source":"### First : we load necessary libraries "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom xgboost import XGBRegressor\nimport statsmodels.api as sm\nfrom pylab import rcParams\nimport warnings\nimport itertools\nfrom sklearn.linear_model import LinearRegression\n\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cluster import KMeans\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Second : We load the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Any results you write to the current directory are saved as output.\n\ndata_climb = pd.read_csv('../input/mount-rainier-weather-and-climbing-data/climbing_statistics.csv')\ndata_wheather = pd.read_csv('../input/mount-rainier-weather-and-climbing-data/Rainier_Weather.csv')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### then we explore it to make sure of appearence about missing values "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_climb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_wheather.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_climb.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_wheather.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### we notice that both datasets contain no missing value but also date columns are read as strings that should be fixed\n#### we notice also that there is more than a record for a single date in climb data , we just need to know number of unique dates"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_climb.Date.drop_duplicates().count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### to process date columns we need to transform them from string to be read as date time columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_wheather['Date'] = pd.to_datetime(data_wheather['Date'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_wheather.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_wheather.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_wheather['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_climb['Date'] = pd.to_datetime(data_climb['Date'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_climb.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Third : Aggregating climb data per date :"},{"metadata":{},"cell_type":"markdown","source":"#### since we have multiple attempts per day , we need to get the summation of attempts and successes per day , and about route , as a categorical column , we get the mode , the most common route per day as follows :"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_climb.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s=data_climb.groupby(['Date'])[ 'Attempted' , 'Succeeded' , 'Success Percentage'].agg({'Attempted' :'sum' ,\n                                                                                       'Succeeded' :'sum' ,\n                                                                                        'Success Percentage':'mean' ,\n                                                                                         'Route' : lambda x:x.value_counts().index[0]}).sort_values(by = 'Attempted' , ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### then we recalculate success percentage as sum of successes of the day / sum of attempts of this day "},{"metadata":{"trusted":true},"cell_type":"code","source":"s['Success Percentage'] = s['Succeeded'] / s['Attempted']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fourth : merging the 2 datasets "},{"metadata":{},"cell_type":"markdown","source":"#### as stated in the introduction , we need to study the effect of weather on success percentage , so we need to merge the 2 datasets\n\n#### since we have for some dates weather statistics only and for some other dates we have climb statistics only , so we choose outer mode for merging  "},{"metadata":{"trusted":true},"cell_type":"code","source":"s_pooled = pd.merge(right=data_wheather  , left = s , how = 'outer', on ='Date')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_pooled.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### regarding missing values in the pooled dataset due to merging , we assume days in which no records for climbing data , we assume they had zero attempts , successes "},{"metadata":{"trusted":true},"cell_type":"code","source":"s_pooled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in ['Attempted' , 'Succeeded' , 'Success Percentage'    ] : \n    s_pooled[i] = np.where ((np.isnan(s_pooled[i]) == 1) &  (np.isnan(s_pooled['Battery Voltage AVG']) == 0) , 0 , s_pooled[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_pooled.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fifth : studying number of attempts "},{"metadata":{},"cell_type":"markdown","source":"#### we first make sure that date column is the index of our dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"s_pooled=s_pooled.set_index('Date')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### we plot the series of attempt "},{"metadata":{"trusted":true},"cell_type":"code","source":"Attempt = s_pooled['Attempted']\nAttempt.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Attempt.plot(figsize=(15, 6))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### from the above plot we notice that attempts act as a seasonal time series , so we 'd better to study it using ARIMA time series process , not to study it using regression and weather columns as independent variables "},{"metadata":{"trusted":true},"cell_type":"code","source":"Attempt.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### first we analyze the attempts series by  Seasonal decomposition using moving averages, we set freq = 30 since we deal with date data , so season here is month "},{"metadata":{"trusted":true},"cell_type":"code","source":"decomposition = sm.tsa.seasonal_decompose(Attempt, model='additive' , freq = 30)\ndecomposition.plot()\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### then we use ARIMA models , for more check the following link :https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b\n\n#### the idea is we fit several models for our series , and take the model with the least AIC , as the best fitting model for the series "},{"metadata":{"trusted":true},"cell_type":"code","source":"p = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x{}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a =[]\nb=[]\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(Attempt,\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n            results = mod.fit()\n            c=param\n            e=param_seasonal\n            d=results.aic\n            a.append((c,e))\n            b.append(d)\n        except:\n            continue","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### to choose the best model , we put all models and their diagnostics in a dataset , choose the least AIC"},{"metadata":{"trusted":true},"cell_type":"code","source":"models=pd.DataFrame({'model':a,'AIC':b})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.loc[models['AIC'] == models.AIC.min(),:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### since we got the best timeseries model , we fit it , plot the diagnostics"},{"metadata":{"trusted":true},"cell_type":"code","source":"mod = sm.tsa.statespace.SARIMAX(Attempt,\n                                order=(1, 0, 1),\n                                seasonal_order=(0, 0,1, 12),\n                                enforce_stationarity=False,\n                                )\nresults = mod.fit()\nprint(results.summary().tables[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### we notice that all parameters are significant except seasonal moving average "},{"metadata":{"trusted":true},"cell_type":"code","source":"results.plot_diagnostics(figsize=(16, 8))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We notice that model is fitting well , residuals are close to normality "},{"metadata":{},"cell_type":"markdown","source":"### Sixth : Predict Success percentage"},{"metadata":{},"cell_type":"markdown","source":"#### We will work on success percentage as a dependent column on weather columns , so we first do some descriptive statistics , distribution plots for each column of them "},{"metadata":{"trusted":true},"cell_type":"code","source":"features =['Battery Voltage AVG' , 'Temperature AVG' ,'Relative Humidity AVG' ,'Wind Speed Daily AVG' ,'Wind Direction AVG','Solare Radiation AVG'] \nfor i in features:\n  print('distribution of ' , i)\n  print(s_pooled.loc[(np.isnan(s_pooled[i]) ==0)][i].describe())\n  sns.distplot(s_pooled.loc[(np.isnan(s_pooled[i]) ==0)][i])\n  plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### we notice that all columns close to normal distribution except the severe right skewness of 'wind speed AVG' and 'solar radiation AVG' , we may need log transformation to get rid of this skewness but it may lead to less goodness of fit in practice \n#### we take only needed columns , weather columns and successful percentage in a new dataset \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"s_pooled2=s_pooled.loc[ :, ['Battery Voltage AVG' , 'Temperature AVG' ,'Relative Humidity AVG' ,'Wind Speed Daily AVG' ,'Wind Direction AVG','Solare Radiation AVG','Success Percentage' , 'Route']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_pooled2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_pooled2.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### we find that there are missing values in weather columns since we have days we have climb statistics for but with no data for weather , in this case we don't include these cases in our analysis.\n\n#### We shoulf notice that the sample here will be onlt 464 cases which we divide to train and test data , so results shoulf be taken carefully , we would have more accurate results if we had a larger sample \n\n#### we have the categorical columns if route which we should include in our analysis , we will make binary column for each category to include in our analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"s_pooled2['Route'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s_pooled2 = pd.get_dummies(s_pooled2 , columns=['Route'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### from the above frequencies table we take a binary column to include in our analysis for the most common 3 route patterns \n\n#### before that let's check correlation between the columns "},{"metadata":{"trusted":true},"cell_type":"code","source":"s_pooled2.loc[np.isnan(s_pooled2['Battery Voltage AVG']) ==0,['Battery Voltage AVG','Temperature AVG','Relative Humidity AVG','Wind Speed Daily AVG','Wind Direction AVG'\n                ,'Solare Radiation AVG','Success Percentage']].corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### we notice that the most correlating column with success percentage is solar radiation , which makes sense , helping the climber with warmness and light \n\n#### also we notice high correlation between temprature and solar radiation , which violates one of regression assumptions which is absence of collinearity between regressors , so we can exclude it from analysis"},{"metadata":{},"cell_type":"markdown","source":"#### then let's have a look on data"},{"metadata":{"trusted":true},"cell_type":"code","source":"s_pooled2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### so regarding data we will predict success percentage through we will take cases only with valid data for weather columns , include columns for the most common 3 route patterns  , and exclude temprature AVG column to maintain absence of collinearity assumption for regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = s_pooled2.loc[np.isnan(s_pooled2['Battery Voltage AVG']) ==0,['Battery Voltage AVG','Relative Humidity AVG','Wind Speed Daily AVG','Wind Direction AVG'\n                ,'Solare Radiation AVG' , 'Route_Disappointment Cleaver' ,'Route_Gibralter Ledges' ,'Route_Ingraham Direct']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = s_pooled2.loc[np.isnan(s_pooled2['Battery Voltage AVG']) ==0,'Success Percentage']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### then we divide data into train and test datasets "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Organize our data for training\nX = x_train\nY = y_train\nX, X_Val, Y, Y_Val = train_test_split(X, Y , train_size =0.7 , shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### then we predict successful percentage using simple linear regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nreg = LinearRegression().fit(X, Y)\npred_0=pd.DataFrame(reg.predict(X_Val)).set_index(Y_Val.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_0=pd.DataFrame(reg.predict(X_Val)).set_index(Y_Val.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_0.columns=['pred']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" pred_0['pred']=np.where(pred_0['pred'] <0 , 0 , pred_0['pred'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the r2 score\nprint(r2_score(Y_Val, np.round(pred_0['pred'],0)))\n# Print the mse score\nprint(mean_squared_error(Y_Val,np.round(pred_0['pred'],0)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### as we see , result is not satisfactory at all , we even notice that R2 is by close to zero and in some run times it gives negative values , we have to try another algoruthm for regression"},{"metadata":{},"cell_type":"markdown","source":"#### we can try XGB regressor as a powerful algorithm for regression and lead to many winning solution in kaggle competitions"},{"metadata":{},"cell_type":"markdown","source":"#### first we redivide data to train and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Organize our data for training\nX = x_train\nY = y_train\nX, X_Val, Y, Y_Val = train_test_split(X, Y , train_size =0.7 , shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### to get the best classifier we use grid search , we fit the model on train set with several combination of parameters we grid on  with the default for other parameters , and the best model that gives the best diagnostics\n\n#### for better performance , we start with grid search on learning rate and number of estimators "},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# A parameter grid for XGBoost\nparams = {'learning_rate': [ 0.01, 0.03 , 0.05 ,0.1] , 'n_estimators' :[100,  300 , 500 , 800 , 1000] , 'max_depth' : [i for i in range(8)]\n}\n\n# Initialize XGB and GridSearch\nxgb = XGBRegressor(nthread=-1 , objective = 'reg:squarederror') \n\ngrid1 = GridSearchCV(xgb, params)\ngrid1.fit(X, Y)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Print the r2 score\nprint(r2_score(Y_Val, grid1.best_estimator_.predict(X_Val))) \n# Print the mse score\nprint(mean_squared_error(Y_Val, grid1.best_estimator_.predict(X_Val))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid1.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### here we have the best estimator is with learning rate of 0.01 and number of estimators of 500\n\n#### and in some run times it gives learning rate of 0.03 and number of estimators of 100\n\n#### let's continue our grid search for the rest of important parameters "},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\n\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = { 'learning_rate' : [0.01] , 'n_estimators':[500] , 'subsample' : [i/10.0 for i in range(11)] ,'min_child_weight' :[0.5 , 1 , 1.5 , 2] , 'colsample_bytree' : [i / 10.0 for i in range(11)]}\n                \n\n# Initialize XGB and GridSearch\nxgb = XGBRegressor(nthread=-1 , objective = 'reg:squarederror') \n\ngrid2 = GridSearchCV(xgb, params)\ngrid2.fit(X, Y)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the r2 score\nprint(r2_score(Y_Val, grid2.best_estimator_.predict(X_Val))) \n# Print the mse score\nprint(mean_squared_error(Y_Val, grid2.best_estimator_.predict(X_Val))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid2.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### we got the best estimator that gives us the the least MSE with a considerably high R2 \n#### let's get the importance score of features "},{"metadata":{"trusted":true},"cell_type":"code","source":"model=grid2.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ft = pd.Series(model.feature_importances_ , index=x_train.columns)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ft.plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Here we see that  solar radiation AVG  has the highest importance score , followed by Route_Disappointment cleaver"},{"metadata":{},"cell_type":"markdown","source":"### To sum up \n#### we tried to take the most advantage of our data to fit a model to describe variety of attempts and successful percentage \n#### , but these results should be taken with caution since the sample is small , and regarding SARIMA model we used to fit a model for Attempts , we would get a better model had we had a longer series"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}