{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This code is heavily inspired by this tutorial: https://www.kaggle.com/ab971631/beginners-guide-to-text-generation-pytorch\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nimport string\nimport unidecode\nimport random\nimport torch\nimport torch.nn as nn\n\n#from architecture import recursive\n\nfrom shutil import copyfile\ncopyfile(src = \"../input/recursive/recursive.py\", dst = \"../working/recursive.py\")\nimport recursive\n\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device == 'cuda':\n    torch.backends.cudnn.benchmark = False\n    torch.cuda.manual_seed_all(SEED)\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train = pd.read_csv(\"data/lyrics.csv/lyrics.csv\") \ndf_train = pd.read_csv(\"../input/lyrics/lyrics.csv\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=0\npop_lyrics = list()\nwhile i < len(df_train.index):\n    if df_train['genre'][i] == 'Pop' and type(df_train['lyrics'][i]) == str:\n        pop_lyrics.append(df_train['lyrics'][i])\n    i += 1\n\ndef joinStrings(text):\n    return ' '.join(string for string in text)\n\npop_text = joinStrings(pop_lyrics[:10])\nlen(pop_text.split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop = set(nltk.corpus.stopwords.words('english'))\nexclude = set(string.punctuation) \nlemma = nltk.stem.wordnet.WordNetLemmatizer()\ndef clean(doc):\n        stop_free = \" \".join([i for i in doc.split() if i not in stop])\n        punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n        return normalized\ntest_sentence = clean(pop_text).lower().split()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n            for i in range(len(test_sentence) - 2)]\nchunk_len=len(trigrams)\nprint(trigrams[:3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab = set(test_sentence)\nvoc_len=len(vocab)\nword_to_ix = {word: i for i, word in enumerate(vocab)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp=[]\ntar=[]\nfor context, target in trigrams:\n    context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n    inp.append(context_idxs)\n    targ = torch.tensor([word_to_ix[target]], dtype=torch.long)\n    tar.append(targ)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(inpt, target, deco, deco_optimizer, chunklen):\n    if device == 'cuda':\n        hidden = deco.init_hidden().cuda()\n    else:\n        hidden = deco.init_hidden()\n    deco.zero_grad()\n    loss = 0\n    \n    for c in range(chunklen):\n        if device == 'cuda':\n            output, hidden = deco(inpt[c].cuda(), hidden)\n            loss += criterion(output, target[c].cuda())\n        else:\n            output, hidden = deco(inpt[c], hidden)\n            loss += criterion(output, target[c])\n\n    loss.backward()\n    deco_optimizer.step()\n\n    return loss.data.item() / chunklen","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time, math\n\ndef time_since(since):\n    s = time.time() - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 300\nprint_every = 100\nplot_every = 10\nhidden_size = 100\nn_layers = 1\nlr = 0.015\n\ndecoder = recursive.RNN(voc_len, hidden_size, voc_len, n_layers)\ndecoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\nstart = time.time()\nall_losses = []\nloss_avg = 0\nif device == 'cuda':\n    decoder.cuda()\nfor epoch in range(1, n_epochs + 1):\n    loss = train(inp, tar, decoder, decoder_optimizer, chunk_len)       \n    loss_avg += loss\n\n    if epoch % print_every == 0:\n        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 50, loss))\n#         print(evaluate('ge', 200), '\\n')\n\n    if epoch % plot_every == 0:\n        all_losses.append(loss_avg / plot_every)\n        loss_avg = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n%matplotlib inline\n\nplt.figure()\nplt.plot(all_losses)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.save(decoder.state_dict(), \"model/trigram_pop_10.pth\")\ntorch.save(decoder.state_dict(), \"trigram_pop_10.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## All pop songs","metadata":{}},{"cell_type":"markdown","source":"Let's now do the same for all pop songs! I'll write a function so we can use this for other parts of the data set as well.","metadata":{}},{"cell_type":"code","source":"def prepare(data):\n    text = joinStrings(data)\n    test_sentence = clean(text).lower().split()\n    trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n            for i in range(len(test_sentence) - 2)]\n    chunk_len =len(trigrams)\n    vocab = set(test_sentence)\n    voc_len =len(vocab)\n    word_to_ix = {word: i for i, word in enumerate(vocab)}\n    inp=[]\n    tar=[]\n    for context, target in trigrams:\n        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n        inp.append(context_idxs)\n        targ = torch.tensor([word_to_ix[target]], dtype=torch.long)\n        tar.append(targ)\n    return(chunk_len, voc_len, inp, tar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunk_len_allpop, voc_len_allpop, inp_allpop, tar_allpop = prepare(pop_lyrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs_allpop = 10\nprint_every_allpop = 100\nplot_every_allpop = 10\nhidden_size_allpop = 100\nn_layers_allpop = 1\nlr_allpop = 0.015\n\ndecoder_allpop = recursive.RNN(voc_len_allpop, hidden_size, voc_len_allpop, n_layers_allpop)\ndecoder_optimizer_allpop = torch.optim.Adam(decoder_allpop.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\nstart = time.time()\nall_losses_allpop = []\nloss_avg_allpop = 0\nif device == 'cuda':\n    decoder_allpop.cuda()\nfor epoch in range(1, n_epochs_allpop + 1):\n    loss = train(inp_allpop, tar_allpop, decoder_allpop, decoder_optimizer_allpop, chunk_len_allpop)       \n    loss_avg_allpop += loss\n\n    if epoch % print_every == 0:\n        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs_allpop * 50, loss))\n\n    if epoch % plot_every == 0:\n        all_losses_allpop.append(loss_avg_allpop / plot_every)\n        loss_avg_allpop = 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nplt.figure()\nplt.plot(all_losses_allpop)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch.save(decoder_allpop.state_dict(), \"model/trigram_allpop.pth\")\ntorch.save(decoder_allpop.state_dict(), \"trigram_allpop.pth\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}