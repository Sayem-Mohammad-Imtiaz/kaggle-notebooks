{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Wine Classification\nIn this kernel we will use the data from [Wine Varaieties Dataset](https://www.kaggle.com/brynja/wineuci) to perform a simple classification to predict the wine class."},{"metadata":{},"cell_type":"markdown","source":"# Loading libraries and dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all, we start by loading the dataset using `pd.read_csv()` function"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/Wine.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that columns are given arbitrary numbers. The real columns names are provided in the dataset page, we will assign the columns names to the real ones:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns = ['class','alcohol','malicAcid','ash','ashalcalinity','magnesium','totalPhenols','flavanoids','nonFlavanoidPhenols','proanthocyanins','colorIntensity','hue','od280_od315','proline']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Missing values"},{"metadata":{},"cell_type":"markdown","source":"First we check whether the data contains any missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are %d missing values in total.' % data.isna().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data is clean and has no missing values, no further processing is needed"},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Count of different wine classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"sn.countplot(data['class'], palette='Blues_d');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Variables correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\nfig, ax = plt.subplots(figsize=(10,10))\nsn.heatmap(corr,ax=ax, cmap=sn.diverging_palette(20, 220, n=200), square=True, annot=True, cbar_kws={'shrink': .8})\nax.set_xticklabels(data.columns, rotation=45, horizontalalignment='right');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The variable the least correlated with the target variable (class) is **ash**, we can drop it but we will leave this for feature elimination."},{"metadata":{},"cell_type":"markdown","source":"# 4. Split train/test data"},{"metadata":{},"cell_type":"markdown","source":"First, we need to seperate the variables and the target from the original dataset as follows:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(['class'], axis=1)\nY = data['class']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we will use `train_test_split()` from `sklearn.model_selection` to split it further into training and testing subsets."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_state = 2\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=random_state, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `train_test_split()` function takes the following arguments:\n* `X`: the variables, the whole dataset, without the target variable (wine class)\n* `Y`: the target variable, which is the wine class\n* `test_size`: represents the proportion of the original data to be used as testing set (here I chose 30%)\n* `shuffle`: since the original dataset is grouped by the wine class, it is preferable to rearrange everythign randomly, so we set `shuffle` to `True`"},{"metadata":{},"cell_type":"markdown","source":"# 5. Feature Elimination"},{"metadata":{},"cell_type":"markdown","source":"Some variables may not be predictive for the target wine class, we will use feature elimination to try to eliminate them in order to improve the data quality we will feed into the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = LogisticRegression(solver='liblinear', multi_class='auto')\nselector = RFECV(estimator, step=1, cv = StratifiedKFold(10));\nselector.fit(X, Y);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.xlabel('Number of Features')\nplt.ylabel('Cross Validation Score')\ngrid_scores = plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_, zorder = 3);\nbest_number = plt.scatter(selector.n_features_, np.max(selector.grid_scores_), color='red', zorder = 5);\nplt.legend([best_number],['Optimal Number of Features'], loc='lower right');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recrusive Feature Elimination didn't eliminate any feature, so apparently all features contribute to the clasification.\nWe will keep all of them."},{"metadata":{},"cell_type":"markdown","source":"# Building the models"},{"metadata":{},"cell_type":"markdown","source":"It is always a good idea to try many classifiers and compair their results, and pick the one with best accuracy. Different algorithms may perform differently on different datasets.\nWe will try the following models:\n* Logistic Regression\n* Support Vector Classifier\n* Naive Bayes\n* K-Nearest Neighbours\n* Decision Trees\n* Multi-Layer Perceptron\n* XGBoost Classifier"},{"metadata":{},"cell_type":"markdown","source":"We need to import the mentioned classifiers:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will build a list of tuples containing the name of the classifier and the classifier itself:"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = []\nclassifiers.append(('Logistic Regression', LogisticRegression(solver='liblinear', multi_class='auto')))\nclassifiers.append(('Support Vector Classifier', SVC(kernel='linear')))\nclassifiers.append(('GaussianNB', GaussianNB()))\nclassifiers.append(('K-Nearest Neighbors',KNeighborsClassifier(n_neighbors=3)))\nclassifiers.append(('Decision Tree', DecisionTreeClassifier()))\nclassifiers.append(('Multi-Layer Perceptron', MLPClassifier(hidden_layer_sizes=(15),solver='sgd',learning_rate_init=0.01,max_iter=500)))\nclassifiers.append(('eXtreme Gradient Boosting', XGBClassifier()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Models ranking"},{"metadata":{},"cell_type":"markdown","source":"To evaluate the performance of our models, we will ues the `cross_val_score()` function"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = StratifiedKFold(n_splits=10, random_state=random_state)\ncv_results = []\nfor name, classifier in classifiers:\n    result = cross_val_score(classifier, X, Y, cv=kfold);\n    cv_results.append((name, result));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame(cv_results, columns=['classifier','cvscore'])\nresults['cvscore'] = [np.mean(i) for i in results['cvscore']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sn.set_style('whitegrid')\nax = sn.barplot(x='cvscore',y='classifier', data=results.sort_values('cvscore'), palette='Blues_d')\nax.set(xlabel='Cross Validation Score', ylabel='');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The best performing model is: %s\\nWith Cross-Validation Score of: %.2f' % (results.iloc[results['cvscore'].idxmax()][0], results.iloc[results['cvscore'].idxmax()][1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We notice that **GuassianNB** scored the highest, so this the model that we will pick"},{"metadata":{},"cell_type":"markdown","source":"We use the training split we created earlier in order to train our model, then we will use it to predict the class of testing samples:"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimator = GaussianNB()\nestimator.fit(X_train, Y_train)\nY_predict = estimator.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To evaluate the accuracy of our predictions, we will use `accuracy_score()`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint('Prediction accuracy is: %.2f' % (100*accuracy_score(Y_predict, Y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check for the variables that our model predicted wrong"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[Y_predict != Y_test]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"The dataset was clean and didn't require any real preprocessing and missing values handling.\nAlso the variables were really predictive for the target variable, many models scored very high (+90%) and the best model scored %96.30"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}