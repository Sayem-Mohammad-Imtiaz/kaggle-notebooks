{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Probabilistic model of TPTP problems\n\nAuthor: [Filip Bártek](bartefil@fel.cvut.cz)\n\n## Academic course\n\nThis project was completed for the course Probabilistic Models of Uncertainty (XP33PMD).\n\n- Course:\n    - Name: Probabilistic Models of Uncertainty\n    - Code: XP33PMD\n    - Web site: https://cw.fel.cvut.cz/wiki/courses/xp33pmd/start\n- Semester: Summer 2020/2021\n- University: [Czech Technical University in Prague](https://www.cvut.cz/)\n\n## Task description\n\n[The TPTP (Thousands of Problems for Theorem Provers)](http://www.tptp.org/) is a library of test problems for automated theorem proving systems.\nIn version 7.4.0, the library contains 23 291 problems.\nThe problems come from various sources and domains – the distribution is heterogeneous.\n\nEach of the problems in the library is specified in one of the following forms:\n\n1. Clause normal form (CNF) - 8118 problems\n2. First-order form (FOF) - 8935 problems\n3. Typed first-order form (TFF) - 2234 problems\n4. Polymorphic typed higher-order form (THF) - 4004 problems\n\nIn this project, I concentrate on the FOF segment of TPTP because it is related to other research projects I work on.\n\nEach FOF problem in TPTP is [annotated with several features](http://www.tptp.org/TPTP/TR/TPTPTR.shtml#HeaderSection), for example:\n\n- [Domain](http://www.tptp.org/TPTP/TR/TPTPTR.shtml#DomainStructure) (for example Set theory or Software verification)\n- Source (indicating where the problem was originally published)\n- Status (for example Theorem, Satisfiable, or Open)\n- Rating (real number in the range 0.0 to 1.0 indicating the difficulty of the problem for the state-of-the-art automated theorem provers)\n- Effective order (propositional, effectively propositional, or really first-order)\n- Counts of elements (syntactic features):\n    - Formulae\n    - Atoms\n    - Predicates\n    - Functors\n    - Variables\n\nBesides these, we consider the problem file size as an additional feature.\n\nThe main task of this project is to create a probabilistic model of the FOF problems of TPTP v7.4.0.\nEach problem is to be represented by a small set of features.\nThe model is to be an instance of [Bayesian network](https://en.wikipedia.org/wiki/Bayesian_network).\nThe model is to be used to identify outlier problems and the possible causes of the relative rarity of these problems.","metadata":{}},{"cell_type":"markdown","source":"## Implementation\n\nThe solution is implemented in Python.\nThe Bayesian network functionality is implemented in the [pgmpy Python package](https://pgmpy.org/).\n\n### Initialization","metadata":{}},{"cell_type":"code","source":"# Install the required Python packages\n\n!pip install daft orderedset pgmpy pickle5","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-16T10:21:25.2368Z","iopub.execute_input":"2021-06-16T10:21:25.237291Z","iopub.status.idle":"2021-06-16T10:21:48.177439Z","shell.execute_reply.started":"2021-06-16T10:21:25.237168Z","shell.execute_reply":"2021-06-16T10:21:48.17656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the logger\n\nimport logging\n\n# Remove the malfunctioning default handler (apparently installed by Kaggle)\nlogger = logging.getLogger()\nwhile len(logger.handlers) > 0:\n    logger.removeHandler(logger.handlers[0])\n\nlogging.basicConfig(level=logging.DEBUG)\nlogging.getLogger('matplotlib').setLevel(logging.INFO)\n\nlog = logging.getLogger(__name__)\n\nlog.info('Logger initialized.')","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-06-16T10:21:48.179433Z","iopub.execute_input":"2021-06-16T10:21:48.179768Z","iopub.status.idle":"2021-06-16T10:21:48.188229Z","shell.execute_reply.started":"2021-06-16T10:21:48.179733Z","shell.execute_reply":"2021-06-16T10:21:48.187116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the filesystem-based cache\n\nimport appdirs\nimport joblib\n\nmemory = joblib.Memory(appdirs.user_cache_dir('strategizer'))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:21:48.190538Z","iopub.execute_input":"2021-06-16T10:21:48.191011Z","iopub.status.idle":"2021-06-16T10:21:48.283653Z","shell.execute_reply.started":"2021-06-16T10:21:48.190963Z","shell.execute_reply":"2021-06-16T10:21:48.282937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data preparation\n\nA table of problems with their features has been prepared in advance and published as the Kaggle dataset [filipbartek/tptp-v740-fof](https://www.kaggle.com/filipbartek/tptp-v740-fof).\nWe load the table as a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).","metadata":{}},{"cell_type":"code","source":"# Load the table of FOF problems with features\n\nimport pickle5\n\nfilename = '/kaggle/input/tptp-v740-fof/problems.pkl'\nwith open(filename, 'rb') as f:\n    data = pickle5.load(f)\n\ndata","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:21:48.284989Z","iopub.execute_input":"2021-06-16T10:21:48.285433Z","iopub.status.idle":"2021-06-16T10:21:48.402413Z","shell.execute_reply.started":"2021-06-16T10:21:48.285388Z","shell.execute_reply":"2021-06-16T10:21:48.401723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We restrict the dataset to 11 features.Some of the features are categorical, while others are numeric.\nBoth floating point and integer numeric features are included.","metadata":{}},{"cell_type":"code","source":"# Filter the problem features\n\nwhitelist = ['domain', 'size', 'source', 'rating', 'status', 'effective_order', 'formulae', 'atoms', 'predicates', 'functors', 'variables']\ndata = data.filter(whitelist)\ndata.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:21:48.403458Z","iopub.execute_input":"2021-06-16T10:21:48.403894Z","iopub.status.idle":"2021-06-16T10:21:48.519913Z","shell.execute_reply.started":"2021-06-16T10:21:48.403848Z","shell.execute_reply":"2021-06-16T10:21:48.519151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of the features are categorical, while others are numeric.\nBoth floating point and integer numeric features are included.","metadata":{}},{"cell_type":"code","source":"data.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:21:48.522781Z","iopub.execute_input":"2021-06-16T10:21:48.523153Z","iopub.status.idle":"2021-06-16T10:21:48.531404Z","shell.execute_reply.started":"2021-06-16T10:21:48.523121Z","shell.execute_reply":"2021-06-16T10:21:48.530229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We model each problem feature by a discrete random variable.\nWe discretize the numeric features by binning into 10 intervals.\nProblem difficulty rating is binned into intervals of uniform length\nand the other numeric features are binned into intervals by quantiles.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef discretize_column(data, column, max_unique=None, bins=10, labels_str=False, cut=None):\n    if cut is None:\n        cut = lambda x: pd.qcut(x, 10, duplicates='drop')\n    x = data[column]\n    if isinstance(x.dtype, pd.CategoricalDtype):\n        log.info(f'{column}: Skipping because the column is categorical.')\n        return\n    unique_count = len(x.unique())\n    if max_unique is not None and len(x.unique()) <= max_unique:\n        log.info(f'{column}: Skipping because the column has {unique_count}<{max_unique} unique values.')\n        return\n    x = cut(data[column])\n    log.info(f'{column}: Discretized {unique_count} unique values to {len(x.unique())} bins.')\n    if labels_str:\n        x.cat.rename_categories(str, inplace=True)\n    data[column] = x\n\ndef discretize(data, max_unique=None, labels_str=False, cut=None):\n    for column in data.columns:\n        discretize_column(data, column, max_unique=max_unique, labels_str=labels_str, cut=cut)\n\nif 'rating' in data:\n    discretize_column(data, 'rating', cut=lambda x: pd.cut(x, 10, duplicates='drop'))\ndiscretize(data)\ndata.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:21:48.533142Z","iopub.execute_input":"2021-06-16T10:21:48.53354Z","iopub.status.idle":"2021-06-16T10:21:48.663157Z","shell.execute_reply.started":"2021-06-16T10:21:48.533503Z","shell.execute_reply":"2021-06-16T10:21:48.66194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To ensure that the model does not overfit to the training data,\nwe split the data into two sets: train set and test set.\nThe training will be performed on the train set,\nwhile the evaluation will be performed on both the train set and the test set in isolation.","metadata":{}},{"cell_type":"code","source":"# Split the problems into train set and test set\n\nfrom sklearn.model_selection import train_test_split\n\ndata_train, data_test = train_test_split(data, test_size=0.5, random_state=0)\ndatasets = {\n    'train': data_train,\n    'test': data_test\n}\n\nfor dataset_name, dataset in datasets.items():\n    print(f'Number of {dataset_name} problems: {len(dataset)}')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:21:48.666538Z","iopub.execute_input":"2021-06-16T10:21:48.666974Z","iopub.status.idle":"2021-06-16T10:21:49.697712Z","shell.execute_reply.started":"2021-06-16T10:21:48.666937Z","shell.execute_reply":"2021-06-16T10:21:49.696542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model estimation\n\nWe estimate a Bayesian network from the train data.\nThe estimation proceeds in two steps:\n\n1. Structure estimation\n2. Parameter estimation\n\npgmpy tutorial notebook: [Learning Bayesian Networks from Data](https://github.com/pgmpy/pgmpy_notebook/blob/master/notebooks/9.%20Learning%20Bayesian%20Networks%20from%20Data.ipynb)\n\n#### Structure estimation\n\nThe structure estimation is performed by local hill climb search.\nThe search starts at a completely disconnected network\nand iteratively adds and removes edges\nso as to maximize the Bayesian Information Criterion (BIC) score (see Section 18.3.5 in [1]).","metadata":{}},{"cell_type":"code","source":"# Estimate the structure of a Bayesian network\n\nimport pgmpy.estimators\n\n# https://github.com/pgmpy/pgmpy/blob/dev/examples/Structure%20Learning%20in%20Bayesian%20Networks.ipynb\ndef estimate_structure_hc(data, scoring_method, show_progress=True):\n    Estimator = pgmpy.estimators.HillClimbSearch\n\n    est = Estimator(data, scoring_method=scoring_method)\n\n    # The estimated structure is cached because the estimation is computationally expensive.\n    estimate_cached = memory.cache(Estimator.estimate, ignore=['show_progress'], verbose=0)\n    return estimate_cached(est, scoring_method=scoring_method, show_progress=show_progress)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:25:12.990077Z","iopub.execute_input":"2021-06-16T10:25:12.99052Z","iopub.status.idle":"2021-06-16T10:25:12.997303Z","shell.execute_reply.started":"2021-06-16T10:25:12.990484Z","shell.execute_reply":"2021-06-16T10:25:12.996071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dag = estimate_structure_hc(datasets['train'], pgmpy.estimators.BicScore(datasets['train']))","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:25:17.074908Z","iopub.execute_input":"2021-06-16T10:25:17.075314Z","iopub.status.idle":"2021-06-16T10:25:17.188494Z","shell.execute_reply.started":"2021-06-16T10:25:17.075282Z","shell.execute_reply":"2021-06-16T10:25:17.187134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dag.to_daft(latex=False, node_pos='circular').render(150)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:22:09.820964Z","iopub.execute_input":"2021-06-16T10:22:09.821383Z","iopub.status.idle":"2021-06-16T10:22:10.311557Z","shell.execute_reply.started":"2021-06-16T10:22:09.821352Z","shell.execute_reply":"2021-06-16T10:22:10.310376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We evaluate the resulting DAG using a number of metrics on both the train and the test data.","metadata":{}},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nscoring_methods = {\n    'K2': pgmpy.estimators.K2Score,\n    'BDeu(1)': lambda d: pgmpy.estimators.BDeuScore(d, equivalent_sample_size=1),\n    'BDeu(10)': lambda d: pgmpy.estimators.BDeuScore(d, equivalent_sample_size=10),\n    'BDeu(n)': lambda d: pgmpy.estimators.BDeuScore(d, equivalent_sample_size=len(d)),\n    'BIC': pgmpy.estimators.BicScore\n}\n\nmodel = pgmpy.models.BayesianModel(dag.edges)\nfor score_name, Score in tqdm(scoring_methods.items(), unit='method', desc='Evaluating the DAG'):\n    for dataset_name, dataset in datasets.items():\n        score = Score(dataset)\n        s = memory.cache(type(score).score, verbose=0)(score, model)\n        print(f'{score_name} score on {dataset_name} dataset: {s} ({s / len(dataset)} per problem)')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:25:40.144887Z","iopub.execute_input":"2021-06-16T10:25:40.145287Z","iopub.status.idle":"2021-06-16T10:25:40.889004Z","shell.execute_reply.started":"2021-06-16T10:25:40.145251Z","shell.execute_reply":"2021-06-16T10:25:40.887939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Parameter estimation\n\nOnce a locally optimal structure is found,\nthe Bayesian network parameters (that is the conditional probability distributions)\nare estimated by Bayesian parameter estimation using the Bayesian Dirichlet equivalent uniform (BDeu) prior (see section 18.3.6.3 in [1]).","metadata":{}},{"cell_type":"code","source":"import warnings\n\nimport pgmpy.estimators\nimport pgmpy.inference\nimport pgmpy.models\nimport sklearn\n\nclass Model(sklearn.base.BaseEstimator):\n    # sklearn estimator wrapper of BayesianModel\n    def __init__(self, edges, prior_type='BDeu', equivalent_sample_size=5):\n        self.edges = edges\n        self.prior_type = prior_type\n        self.equivalent_sample_size = equivalent_sample_size\n    \n    def fit(self, data):\n        self.model = pgmpy.models.BayesianModel(self.edges)\n        state_names = {column: list(data[column].cat.categories) for column in data}\n        self.model.fit(data, state_names=state_names, estimator=pgmpy.estimators.BayesianEstimator, prior_type=self.prior_type, equivalent_sample_size=self.equivalent_sample_size)\n    \n    def score(self, data):\n        return self.log_probability(data).mean()\n    \n    def log_probability(self, data):\n        model = self.model.copy()\n        model.remove_nodes_from(model.nodes - data.columns)\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', module='pgmpy')\n            return pgmpy.inference.bn_inference.BayesianModelProbability(model).log_probability(data)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:22:15.16391Z","iopub.execute_input":"2021-06-16T10:22:15.164339Z","iopub.status.idle":"2021-06-16T10:22:15.180333Z","shell.execute_reply.started":"2021-06-16T10:22:15.164292Z","shell.execute_reply":"2021-06-16T10:22:15.179536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BDeu is parameterized by one parameter: the equivalent sample size.\nWe find the optimum value of the parameter by cross-validation [3].\nWe optimize the logarithm of the predicted probability of the validation data.","metadata":{}},{"cell_type":"code","source":"import scipy\n\ndef f(equivalent_sample_size):\n    dataset = datasets['train']\n    model = Model(dag.edges, equivalent_sample_size=equivalent_sample_size)\n    # We invert the score because f will be minimized.\n    return -sklearn.model_selection.cross_val_score(model, dataset, error_score='raise').mean()\n\nbounds = (1, len(datasets['train']))\nres = scipy.optimize.minimize_scalar(f, bracket=bounds, bounds=bounds, method='bounded', options={'disp': 3})\nprint(res)\nequivalent_sample_size = res.x","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:22:15.181999Z","iopub.execute_input":"2021-06-16T10:22:15.182639Z","iopub.status.idle":"2021-06-16T10:23:12.558756Z","shell.execute_reply.started":"2021-06-16T10:22:15.182592Z","shell.execute_reply":"2021-06-16T10:23:12.557342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we fit the CPDs on the whole train dataset using the golden equivalent sample size.","metadata":{}},{"cell_type":"code","source":"model = Model(dag.edges, equivalent_sample_size=equivalent_sample_size)\nmodel.fit(datasets['train'])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:23:12.560434Z","iopub.execute_input":"2021-06-16T10:23:12.560746Z","iopub.status.idle":"2021-06-16T10:23:12.908011Z","shell.execute_reply.started":"2021-06-16T10:23:12.560715Z","shell.execute_reply":"2021-06-16T10:23:12.906932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference\n\n#### Problem probability distribution\n\nThe model predicts a probability for each problem in the dataset.\nWe plot the distribution of predicted problem probabilities.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nprobs = {}\nfor name, dataset in datasets.items():\n    problem_log_probabilities = model.log_probability(dataset)\n    print(f'Mean predicted log probability of {name} problem: {problem_log_probabilities.mean()}')\n    probs[name] = problem_log_probabilities\n\nproblems = np.concatenate([v.index for v in datasets.values()])\ndata_annotated = data.copy()\ndata_annotated.loc[problems, 'log_probability'] = np.concatenate(tuple(probs.values()))\ndata_annotated.loc[problems, 'probability'] = np.exp(np.concatenate(tuple(probs.values())))\ndata_annotated.loc[problems, 'dataset'] = np.repeat(tuple(probs.keys()), [len(v) for v in probs.values()])\n\ndata_annotated.replace([np.inf, -np.inf], np.nan, inplace=True)\n\nprint('Number of problems excluded because they have zero predicted probability:', data_annotated.log_probability.isnull().sum())\n\nplt.figure(figsize=(16, 4))\nax = sns.histplot(data_annotated, x='log_probability', hue='dataset')\nax.set(xlabel='Log probability', ylabel='Problem count')\nplt.show()","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2021-06-16T10:23:12.90931Z","iopub.execute_input":"2021-06-16T10:23:12.909633Z","iopub.status.idle":"2021-06-16T10:23:14.414125Z","shell.execute_reply.started":"2021-06-16T10:23:12.909599Z","shell.execute_reply":"2021-06-16T10:23:14.412705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Predicting probability distributions\n\nWe begin by visualizing some probability distributions predicted by the model. Inferences are performed by variable elimination.","metadata":{}},{"cell_type":"code","source":"import pgmpy.inference\n\n# http://pgmpy.org/inference.html\ninference = pgmpy.inference.VariableElimination(model.model)","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:23:14.415698Z","iopub.execute_input":"2021-06-16T10:23:14.416085Z","iopub.status.idle":"2021-06-16T10:23:14.424458Z","shell.execute_reply.started":"2021-06-16T10:23:14.416049Z","shell.execute_reply":"2021-06-16T10:23:14.423336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ndef plot_inference(inference, variables, evidence=None, **kwargs):\n    q = inference.query(variables=variables, evidence=evidence, show_progress=False)\n    plot_distribution(q, variables=variables, evidence=evidence, **kwargs)\n\ndef plot_distribution(q, variables=None, evidence=None, conditional=False, title=None, horizontal=False):\n    if len(q.scope()) == 1:\n        assert variables is None or variables == q.scope()\n        if horizontal:\n            ax = sns.barplot(y=q.state_names[q.scope()[0]], x=q.values)\n            ax.set(title=title, ylabel=q.scope()[0], xlabel=label(q.scope(), evidence))\n        else:\n            ax = sns.barplot(x=q.state_names[q.scope()[0]], y=q.values)\n            ax.set(title=title, xlabel=q.scope()[0], ylabel=label(q.scope(), evidence))\n            ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n    elif len(q.scope()) == 2:\n        if variables is None:\n            variables = q.scope()\n        if conditional:\n            q = q.divide(q.marginalize([variables[0]], inplace=False), inplace=False)\n        values = q.values\n        if variables != q.scope():\n            values = values.transpose()\n        ax = sns.heatmap(values,\n                         xticklabels=q.state_names[variables[1]],\n                         yticklabels=q.state_names[variables[0]])\n        ax.set(title=label(variables, evidence, conditional=conditional), xlabel=variables[1], ylabel=variables[0])\n    else:\n        raise RuntimeError('Too many variables to plot.')\n\ndef plot_distributions(distributions, title=None):\n    variable = next(iter(distributions.values())).scope()[0]\n    assert all(d.scope() == [variable] for d in distributions.values())\n    distribution_lengths = [len(d.values) for d in distributions.values()]\n    data_dict = {\n        'distribution': pd.Series(np.repeat(tuple(distributions.keys()), distribution_lengths), dtype='category'),\n        'value': np.concatenate(tuple(d.state_names[d.scope()[0]] for d in distributions.values())),\n        'probability': np.concatenate(tuple(d.values for d in distributions.values()))\n    }\n    df = pd.DataFrame(data_dict)\n    ax = sns.catplot(data=df, hue='distribution', x='value', y='probability', kind='bar')\n    ax.set(title=title)\n    ax.set(xlabel=variable)\n    ax.set_xticklabels(rotation=90)\n\ndef label(variables, evidence, conditional=False):\n    if conditional:\n        event = variables[0]\n        conditions = variables[1:]\n    else:\n        event = ','.join(variables)\n        conditions = []\n    if evidence is not None:\n        conditions.extend(f'{k}={v}' for k, v in evidence.items())\n    if len(conditions) == 0:\n        return f'P({event})'\n    else:\n        condition = ','.join(conditions)\n        return f'P({event}|{condition})'","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:23:14.426128Z","iopub.execute_input":"2021-06-16T10:23:14.426585Z","iopub.status.idle":"2021-06-16T10:23:14.452353Z","shell.execute_reply.started":"2021-06-16T10:23:14.426535Z","shell.execute_reply":"2021-06-16T10:23:14.451577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nplot_inference(inference, ['domain'])\nplt.show()\n\nplt.figure(figsize=(4, 24))\nplot_inference(inference, ['source'], horizontal=True)\nplt.show()\n\nplot_inference(inference, ['rating'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:23:14.45364Z","iopub.execute_input":"2021-06-16T10:23:14.45428Z","iopub.status.idle":"2021-06-16T10:23:17.070465Z","shell.execute_reply.started":"2021-06-16T10:23:14.454232Z","shell.execute_reply":"2021-06-16T10:23:17.069316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that since the model was trained with a positive prior, all the source values, including those that do not appear in the train data, are assigned a non-zero probability.","metadata":{}},{"cell_type":"code","source":"import yaml\n\nsources_train = np.unique(datasets['train'].source)\ndistribution = inference.query(variables=['source'], show_progress=False)\nd = {s: float(distribution.get_value(source=s)) for s in data.source.cat.categories if s not in sources_train}\nprint(f'Sources not included in the train dataset and their probabilities:\\n{yaml.dump(d)}')","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:32:46.958861Z","iopub.execute_input":"2021-06-16T10:32:46.959231Z","iopub.status.idle":"2021-06-16T10:32:47.048988Z","shell.execute_reply.started":"2021-06-16T10:32:46.959195Z","shell.execute_reply":"2021-06-16T10:32:47.048008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 4))\nplot_inference(inference, ['rating', 'domain'])\nplt.show()\n\nplt.figure(figsize=(16, 4))\nplot_inference(inference, ['rating', 'domain'], conditional=True)\nplt.show()\n\nplt.figure(figsize=(16, 4))\nplot_inference(inference, ['status', 'domain'], conditional=True)\nplt.show()\n\nplt.figure(figsize=(4, 24))\nplot_inference(inference, ['source', 'rating'])\nplt.show()\n\nplot_inference(inference, ['rating', 'size'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:23:17.180242Z","iopub.execute_input":"2021-06-16T10:23:17.180659Z","iopub.status.idle":"2021-06-16T10:23:21.947791Z","shell.execute_reply.started":"2021-06-16T10:23:17.180614Z","shell.execute_reply":"2021-06-16T10:23:21.946485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The filesize of the problem and the number of atoms, predicates, functors, and variables are positively correlated with the number of formulae.","metadata":{}},{"cell_type":"code","source":"for variable in ['size', 'atoms', 'predicates', 'functors', 'variables']:\n    plot_inference(inference, [variable, 'formulae'])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T10:23:21.94939Z","iopub.execute_input":"2021-06-16T10:23:21.949807Z","iopub.status.idle":"2021-06-16T10:23:23.971928Z","shell.execute_reply.started":"2021-06-16T10:23:21.949761Z","shell.execute_reply":"2021-06-16T10:23:23.970678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problems with 62 to 96 formulae and 4 to 5 functors seem to form a large atypical cluster.\nPlotting their domain distribution shows that they appear in SWC more frequently and HWV less frequently.","metadata":{}},{"cell_type":"code","source":"def plot_inference_relative(variable, evidence):\n    distribution_0 = inference.query([variable], show_progress=False)\n    distribution_1 = inference.query([variable], evidence, show_progress=False)\n    log_ratio = np.log(distribution_1.values) - np.log(distribution_0.values)\n    state_names = list(inference.state_names_map[variable].values())\n    ax = sns.barplot(x=state_names, y=log_ratio)\n    evidence_str = ','.join(f'{k}={v}' for k, v in evidence.items())\n    ax.set(xlabel=variable, ylabel='Logarithm of probability ratio', title=f'log [P({variable}|{evidence_str}) / P({variable})]')\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n    plt.show()\n    \n    plot_distributions({f'P({variable})': distribution_0, f'P({variable}|{evidence_str})': distribution_1})\n    plt.show()\n\nplt.figure(figsize=(16, 4))\nplot_inference_relative('domain', {'functors': inference.state_names_map['functors'][1], 'formulae': inference.state_names_map['formulae'][5]})","metadata":{"execution":{"iopub.status.busy":"2021-06-16T11:01:36.601859Z","iopub.execute_input":"2021-06-16T11:01:36.602407Z","iopub.status.idle":"2021-06-16T11:01:38.458185Z","shell.execute_reply.started":"2021-06-16T11:01:36.602366Z","shell.execute_reply":"2021-06-16T11:01:38.457474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Outlier analysis\n\nWe analyze the outlier problems by plotting some conditional probability distributions of interest.\n\nWe analyze each problem $p$ in isolation.\nLet $(F_1=f_1, F_2=f_2, ..., F_n=f_n)$ be the features of problem $p$.\nFor each feature $F_i$, we calculate the conditional probability that $F_i=f_i$ given the other features of $p$, that is $(F_1=f_1, ..., F_{i-1}=f_{i-1}, F_{i+1}=f_{i+1}, ..., F_n=f_n)$.\nWe denote this probability as $P(F_i=f_i|\\mathrm{problem}=p)$.\n\nIf this probability is below the threshold 0.05, we plot the distribution $P(F_i|\\mathrm{problem}=p)$ to see which values of $F_i$ the model considers more likely.\n\nSimilarly, we plot $P(F_i=f_i,F_j=f_j|\\mathrm{problem}=p)$ for each pair of features $F_i, F_j$.","metadata":{}},{"cell_type":"code","source":"import itertools\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom orderedset import OrderedSet\n\ndef analyze_problem(problem_name, plot_threshold=0.01):\n    problem = data.loc[problem_name]\n\n    def compute_probability(hypothesis, evidence=None, plot_threshold=None):\n        if evidence is None:\n            evidence_dict = {k: v for k, v in problem.iteritems() if k not in hypothesis}\n        else:\n            if len(set(hypothesis) & set(evidence)) != 0:\n                return np.nan\n            evidence_dict = {k: problem[k] for k in evidence}\n        distribution = inference.query(variables=hypothesis, evidence=evidence_dict, show_progress=False)\n        coordinates = tuple(distribution.name_to_no[v][problem[v]] for v in distribution.variables)\n        probability = distribution.values[coordinates]\n        if evidence is None and plot_threshold is not None and probability < plot_threshold:\n            value = ','.join(f'{column}={problem[column]}' for column in distribution.variables)\n            variables_str = ','.join(hypothesis)\n            plot_distributions({\n                f'P({variables_str}|problem={problem_name})': distribution,\n                f'P({variables_str})': inference.query(variables=hypothesis, show_progress=False)\n            }, title=f'P({value}|problem={problem_name})={probability}')\n            plt.show()\n        return probability\n    \n    n = len(problem)\n    single_probabilities = np.fromiter((compute_probability([v], plot_threshold=plot_threshold) for v in problem.index), dtype=float, count=n)\n    ticklabels = np.array([f'{k}={v}' for k, v in problem.iteritems()])\n    perm = np.argsort(single_probabilities)[::-1]\n\n    ax = sns.barplot(x=ticklabels[perm], y=single_probabilities[perm])\n    ax.set(xlabel='X', ylabel=f'P(X|problem={problem_name})')\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n    plt.show()\n    \n    pair_probabilities = np.fromiter((compute_probability([h], [v]) for h, v in\n                                      itertools.product(problem.index, repeat=2)),\n                                     dtype=float,\n                                     count=n * n).reshape((n, n))\n\n    ax = sns.heatmap(pair_probabilities[perm, :][:, perm], xticklabels=ticklabels[perm], yticklabels=ticklabels[perm])\n    ax.set(title=f'P(Y|X)', xlabel='X', ylabel='Y')\n    plt.show()\n    \n    pair_probabilities = np.fromiter((compute_probability(variables) for variables in\n                                      itertools.product(problem.index, repeat=2)),\n                                     dtype=float,\n                                     count=n * n).reshape((n, n))\n\n    ax = sns.heatmap(pair_probabilities[perm, :][:, perm], xticklabels=ticklabels[perm], yticklabels=ticklabels[perm])\n    ax.set(title=f'P(X,Y|problem={problem_name})', xlabel='X', ylabel='Y')\n    plt.show()\n    \n    pair_probabilities = np.fromiter((compute_probability(variables, []) for variables in\n                                      itertools.product(problem.index, repeat=2)),\n                                     dtype=float,\n                                     count=n * n).reshape((n, n))\n\n    ax = sns.heatmap(pair_probabilities[perm, :][:, perm], xticklabels=ticklabels[perm], yticklabels=ticklabels[perm])\n    ax.set(title='P(X,Y)', xlabel='X', ylabel='Y')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-16T11:04:48.129753Z","iopub.execute_input":"2021-06-16T11:04:48.130161Z","iopub.status.idle":"2021-06-16T11:04:48.153716Z","shell.execute_reply.started":"2021-06-16T11:04:48.130124Z","shell.execute_reply":"2021-06-16T11:04:48.152971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we plot the conditional distributions of the features of the most probable problem in the train set.","metadata":{}},{"cell_type":"code","source":"analyze_problem(data_annotated.where(data_annotated.dataset == 'train').nlargest(1, 'log_probability').index[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T12:41:24.734684Z","iopub.execute_input":"2021-06-16T12:41:24.73511Z","iopub.status.idle":"2021-06-16T12:41:56.406215Z","shell.execute_reply.started":"2021-06-16T12:41:24.735076Z","shell.execute_reply":"2021-06-16T12:41:56.405003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We contrast this with the least probable problem in the train set.","metadata":{}},{"cell_type":"code","source":"analyze_problem(data_annotated.where(data_annotated.dataset == 'train').nsmallest(1, 'log_probability').index[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T12:17:17.043603Z","iopub.execute_input":"2021-06-16T12:17:17.04414Z","iopub.status.idle":"2021-06-16T12:17:50.55348Z","shell.execute_reply.started":"2021-06-16T12:17:17.044101Z","shell.execute_reply":"2021-06-16T12:17:50.552654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem SWV483+2 is a software verification problem.\nThe low number of predicates (2) and functors (2) are unusual in combination with the high number of formulae (75), atoms (213) and variables (2195).\nThe predicates used in this problem are the equality predicate and a 33-ary predicate that specifies the state of the system modeled by the problem.","metadata":{}},{"cell_type":"markdown","source":"Most probable problem in the test set:","metadata":{}},{"cell_type":"code","source":"analyze_problem(data_annotated.where(data_annotated.dataset == 'test').nlargest(1, 'log_probability').index[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T12:17:50.555002Z","iopub.execute_input":"2021-06-16T12:17:50.555624Z","iopub.status.idle":"2021-06-16T12:18:22.128955Z","shell.execute_reply.started":"2021-06-16T12:17:50.555579Z","shell.execute_reply":"2021-06-16T12:18:22.127852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Least probable problem in the test set:","metadata":{}},{"cell_type":"code","source":"analyze_problem(data_annotated.where(data_annotated.dataset == 'test').nsmallest(1, 'log_probability').index[0])","metadata":{"execution":{"iopub.status.busy":"2021-06-16T12:18:22.130551Z","iopub.execute_input":"2021-06-16T12:18:22.130855Z","iopub.status.idle":"2021-06-16T12:18:57.018646Z","shell.execute_reply.started":"2021-06-16T12:18:22.130805Z","shell.execute_reply":"2021-06-16T12:18:57.017459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discussion\n\nThe selected typical problems for the train (`LAT355+4`) and the test (`LAT349+4`) dataset\nshare the same feature representation.\nThe original cause of the similarity of these problems is most likely due to them coming from the same source (`Urb08`) and domain (`LAT`).\n\nThe selected atypical problems (`SWV483+2` and `SWV485+2`) differ in the following features:\n`size`, `formulae`, `atoms`, and `variables`.\n\nIf we were to search for a probability distribution that generalizes into new problem domains and sources,\nit would be useful to form the validation set for hyperparameter tuning\nby splitting the train set along the divisions between the domains and sources.","metadata":{}},{"cell_type":"markdown","source":"## References\n\n1. Daphne Koller and Nir Friedman. 2009. [Probabilistic Graphical Models: Principles and Techniques](https://mitpress.mit.edu/books/probabilistic-graphical-models) - Adaptive Computation and Machine Learning. The MIT Press.\n2. Tomi Silander and Petri Kontkanen and Petri Myllymaki. 2012. [On Sensitivity of the MAP Bayesian Network Structure to the Equivalent Sample Size Parameter](https://arxiv.org/abs/1206.5293).\n3. https://en.wikipedia.org/wiki/Cross-validation_(statistics)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}