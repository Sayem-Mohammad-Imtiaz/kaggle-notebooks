{"nbformat_minor":1,"nbformat":4,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.1","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python"}},"cells":[{"source":"**Обобщённые линейные модели**\n\nОбобщённые линейные модели - семейство регрессионных моделей вида\n$$\n\\hat{Y} = g^{-1}(f(X)\\cdot\\beta + \\varepsilon)\n$$\n\nНесмотря на то, что $f$ и $g$ могут быть нелинейными функциями и $Y$ в результате может весьма нелинейно зависеть от $Х$, модель все равно остается *линейной* относительно параметров $\\beta$.\n\nВ этой модели зависимая переменная принадлежит экспоненциальному семейству, а монотонная дифференцируемая функция $g$ называется функцией связи. $f(X)$ - некоторое преобразование над признаками объектов.  Плотность распределения в экспоненциальном семействе определяется соотношением\n\n$$\nf(y,\\theta, \\varphi) = exp\\left(\\frac{y\\cdot\\theta-b(\\theta)}{\\alpha(\\varphi)}+c(y,\\varphi)\\right)\n$$\n\nПримеры распределений из экспоненциального семейства: нормальное, гамма, бета, Бернулли, Дирихле и многие другие.\n\n**Мотивация**\n\nПростейшая линейная модель - линейная регрессия. Более сложные регрессионные модели применяются в тех случаях, когда диагностика показала\nнесостоятельность простых регрессионных моделей. Вот несколько наиболее распространенных\nслучаев, когда применяются сложные регрессионные модели:\n1. Нелинейность модели\n2. Ненормальное распределение остатков\n3. Неодинаковое распределение остатков\n4. Зависимость остатков\n\n**Пример -  линейная регрессия**\n\nМодель линейной регрессии представляет собой линейную функцию от аргументов, где каждый аргумент - числовое значение признака из признакового описания объекта.\n\n$$\n\\hat{y} = w_0 + w_1x_1 + \\ldots + w_1x_n = \\sum_{i=0}^{n}w_ix_i=\\vec{w}^{T}\\cdot \\vec{X}\n$$\n\n\nмы умножаем каждый признак $x_i$ на соответствующий ему вес $w_i$\n\nВ реальном мире на каждое наблюдение накладывается ошибки (шумовая компонента) $\\varepsilon$, тогда для каждого индивидуального набдюдения $y_i$ получаем следующую модель:\n\n$$\ny_i = \\vec{w}^{T}\\cdot \\vec{X}+ \\varepsilon\n$$\n\nВ линейной регрессии на ошибки накладываются ограничения\n\n1. матожидание случайных ошибок равно нулю: $\\forall i:E[\\varepsilon_i]=0$;\n2. дисперсия случайных ошибок одинакова и конечна, это свойство называется гомоскедастичностью: $\\forall  i: Var(\\varepsilon_i)=\\sigma^2< \\inf$;\n3. случайные ошибки не скоррелированы: $\\forall i\\neq j: Cov(\\varepsilon_i, \\varepsilon_j)=0$ .\n\n*Обучить модель* - значит, найти веса $w_0,\\ldots,w_n$ используя матрицу объекты-признаки $X$ и известные значения целевой переменной $y$. Мы хотим чтобы модель максимально хорошо определяла, какое значение целевой переменной $y_i$ соответствует объекту $X_i$. Формально понятие \"хорошести\" определяет функция потерь.\n\n*Функция потерь* - количественное выражение того, сколько мы теряем в случе неправильного решения. Для линейной регрессии функция потерь (Loss functioon) представляет собой среднее значение квадратов отклонения прогнозных и реальных значений (mean squared error).\n\n$$\nL(X,y,w) = \\frac{1}{2n} \\sum_{i=0}^{n}(y - \\hat{y})^2 = \\frac{1}{2n} \\sum_{i=0}^{n}(y - w^{T}\\cdot X)^2\n$$\n\nЗадача обучения - найти такие веса $w$, при которых функция потерь $L$ принимает минимальное значение на обучающей выборке.\n\n**Важное примечание:** В реальных задачах многими условиями можно пренебречь - например, выбирать другие функции потерь (сумма модулей ). Ошибки $\\varepsilon$ могут не подчиняться условиям (1,2,3) - в этом случае наши оценки весов $\\hat{w}$ перестанут быть лучшими среди линейных и несмещенных.\n\nПостроим простую линейную регрессию на данных \"House sale prices for King County\"","cell_type":"markdown","metadata":{"_cell_guid":"f2807e07-efc9-4ea9-9a2d-13c3f9405460","_uuid":"932dc9f62146cfdbbb4c5bef8e1ae6eced2a6047"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"b4a9d5a0-ebba-47d7-9323-666264c3bc17","_uuid":"8c6b5c891055361a63b01455464a53d050d99b21"},"source":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\nwarnings.filterwarnings('ignore') # отключаем сообщения об ошибках\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n%matplotli"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"3b0ad7d6-3ebd-43d3-a402-4b1d5bff3568","_uuid":"791655194e294a4a85e58c5402fbf6871a4c73ab"},"source":"raw_data = pd.read_csv(\"../input/kc_house_data.csv\")\nprint(raw_data.columns)\nraw_data.head(3)"},{"source":"Удаляем некоторые столбцы - год постройки и координаты. Price назначим целевой переменной. Удалим поле id (вопрос - почему?)","cell_type":"markdown","metadata":{"_cell_guid":"2e6eb8fd-55f6-40f7-a2f2-f1944e2fa9c7","_uuid":"d1cd7333573b27aa2d5339f0ea0f9a1ffb16e1e2"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"a74447a4-065a-4c21-955a-208c3c1ea694","_uuid":"c24e3298b39fdbf0ae22534afd356ad27f5b2956"},"source":"# удаляем неинтересные колонки\ndrop_cols = ['zipcode','lat','long', 'yr_built', 'yr_renovated', 'date', 'id']\ndata = raw_data.drop(drop_cols, axis = 1)\ntarget = data['price']\ndata = data.drop(['price'], axis = 1)\ndata.head(1)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"40148591-aea4-474c-ab48-051d045a20d0","_uuid":"dc29ca36f5d7aae8dcc697555a036f774313fe8b"},"source":"data.describe(percentiles=[])"},{"source":"Разбиваем данные на тест и валидацию","cell_type":"markdown","metadata":{"_cell_guid":"af3d6878-ac9d-4b52-9c5e-610c22de606d","_uuid":"a623d6fc47ae12937a3a7bdf3d960e214a9fa876"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"3b646b57-4231-4883-8a98-50bb314ecb0e","_uuid":"898cfafde000fa207c8c5260dd584cfeb20343d1"},"source":"X_train, X_test, y_train, y_test = \\\n    train_test_split(data, target, random_state=42, train_size=0.8, shuffle=True)\nprint (\"train size={}, test_size={}, total_size={}\".format(\n    X_train.shape[0], X_test.shape[0], data.shape[0])\n)"},{"source":"Обучаем модель линейной регрессии","cell_type":"markdown","metadata":{"_cell_guid":"e1eaaca3-080a-4915-ba30-468934245132","_uuid":"d15a735e6de2f9e9430c60955717224790a313c0"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"0faef4e4-31a5-496f-9606-71fe886845f1","_uuid":"b1d58453723f977252dab13acff81d2671191cb6"},"source":"from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression(normalize=False)\n# обучаем линейную модель на обучающей выборке\nmodel.fit(X_train, y_train)\nprint(\"num_ftrs = {}, num_coeff = {} \".format(X_train.shape[1], len(model.coef_)))\nreg_coeff = dict(zip(data.columns, model.coef_))\nprint(reg_coeff)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"00b7d5df-df2c-4204-99b9-766237bce557","_uuid":"cc78df0645a0afe8c23039bf846aee180ddbedb7"},"source":"y_pred_train = model.predict(X_train)\nprint(\"Качество на тесте {}\".format(mean_squared_error(y_train, y_pred_train)))\ny_pred = model.predict(X_test)\nprint(\"Качество на контроле {}\".format(mean_squared_error(y_test, y_pred)))\n\nsns.barplot(x = X_train.columns, y=model.coef_)\nplt.xticks(rotation=90)"},{"source":"В силу того, что у коэффициентов очень разные масштабы, получили неадекватные коэффициенты регрессии\n\nИз коробки можно применить нормализацию данных, где для каждого значения признака $x_j$ выполняем\n\n$$\n\\overline{\\mu_j} = \\frac{1}{n}\\sum_{i=0}^{n}x_{ij}, \\overline{\\sigma_j} = \\frac{1}{n}\\sqrt{\\sum_{i=0}^{n}(x_{ij} - \\mu_j )^2}\n$$\nПолучаем новые признаки - нормализованные\n$$\nx_{new} = \\frac{\\overline{\\mu_j}} { \\overline{\\sigma_j}}\n$$\n\nВ sklearn такое преобразование делает StandartScaler","cell_type":"markdown","metadata":{"_cell_guid":"71761ab0-87d8-4f56-b902-0336b133cbd1","_uuid":"805b0dc591d357c73cd296d370781dfc200b2c78"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"eebb22f0-a378-4ef1-947e-0d7b87bc1953","collapsed":true,"_uuid":"b7dfe5e66c9bf37c4534ed2c944639e2d0b2071f"},"source":"from sklearn.preprocessing import StandardScaler\nX_train_scale = pd.DataFrame(StandardScaler().fit_transform(X_train), columns = X_train.columns)\nX_train_scale.set_index(X_train.index, inplace = True)\nX_test_scale = pd.DataFrame(StandardScaler().fit_transform(X_test), columns = X_test.columns)\nX_test_scale.set_index(X_test.index, inplace = True)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"b9857325-4a6b-48f5-a982-a09bda23d135","_uuid":"708c817195b4d9aa00381c6eca86f2944450883c"},"source":"X_train_scale.mean()"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"49393aaa-3c6a-4568-a7e3-51a27c64eae6","_uuid":"628bbdbe642ecb30ef5ebf40b33239c1d4a9d86a"},"source":"model_norm = LinearRegression(normalize=False)\nmodel_norm.fit(X_train_scale, y_train)\nreg_coeff_norm = dict(zip(data.columns, model_norm.coef_))\nprint(reg_coeff_norm)"},{"source":"Изменение масштаба помогло, коэффициенты начали выравниваться, но качество модель улучшилась слабо - качество осталось отвратительным.","cell_type":"markdown","metadata":{"_cell_guid":"fb454e53-170a-4844-b14b-01e415afe419","_uuid":"f714f6bc8d16c2b5d8b6f2d5c0dcacb780067bbe"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"5625f4e3-7d21-4f71-96e7-f70924a63b9b","_uuid":"b862e2dd1499c558910341987a732bf7473886fd"},"source":"y_pred_train = model_norm.predict(X_train_scale)\nprint(\"Качество на тесте {}\".format(mean_squared_error(y_train, y_pred_train)))\ny_pred = model_norm.predict(X_test_scale)\nprint(\"Качество на контроле {}\".format(mean_squared_error(y_test, y_pred)))\n\nsns.barplot(x = X_train_scale.columns, y=model_norm.coef_)\nplt.xticks(rotation=90)"},{"source":"Способ ограничить коэффициенты регрессии - регуляризация\n\nL2-регуляризация применяется в гребневой регрессии","cell_type":"markdown","metadata":{"_cell_guid":"878d5269-959a-4d58-9090-0c9305c37cac","_uuid":"616072d82f5004bc5d5122a0f6e24cac41a8aaa0"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"6d9a24ad-ab6a-48e7-a219-19e6120bba1e","_uuid":"3d6a3502b23f5eebc13f673d41dc6c1a85d73fed"},"source":"from sklearn.linear_model import Ridge\n\nridge_model = Ridge(alpha=0.5)\nridge_model.fit(X_train_scale, y_train)\nreg_coeff_ridge = dict(zip(data.columns, ridge_model.coef_))\nprint(reg_coeff_ridge)"},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"scrolled":true,"_cell_guid":"9db8c42e-bf52-432f-b11c-e4f6c3e343ed","_uuid":"e21741851c980d23282d8568ffd72b951e50de4d"},"source":"y_pred_train = ridge_model.predict(X_train_scale)\nprint(\"Качество на тесте {}\".format(mean_squared_error(y_train, y_pred_train)))\ny_pred = ridge_model.predict(X_test_scale)\nprint(\"Качество на контроле {}\".format(mean_squared_error(y_test, y_pred)))\n\nsns.barplot(x = X_train_scale.columns, y=ridge_model.coef_)\nplt.xticks(rotation=90)"},{"source":"RMSE метрика принимает огромные значения, что бы мы не делали. С признаками сделали всё, что можно - видимо беда с целевой переменной, проверим распределение target.","cell_type":"markdown","metadata":{"_cell_guid":"f2d23c78-2222-4581-bed2-2d3c7b925f9b","collapsed":true,"_uuid":"aeeadc23eb5d6f8da0dfb2e3783fe04a45d63b46"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"bcaef851-5697-4dc3-af7e-61dcfc5b9510","_uuid":"9650e0e5f1eddc4b7aab6cdc1a53d9fa08d7c6b7"},"source":"target.plot.hist(bins=100)"},{"source":"Мы видим, что у распределения очень тяжёлый хвост - целевая переменная имеет распределение, далёкое от нормального. Из-за этого модель плохо работает\n\nПроведём resudal analysis - посмотрим ошибки по всем объектам выборки","cell_type":"markdown","metadata":{"_cell_guid":"435fc0dd-33ac-4013-9e99-88fd07da9fa3","_uuid":"0af519d857ccd35454314fe1bcd6f08b921dac8a"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"4e40e34c-447a-4414-8d60-5f2cc7e7b872","_uuid":"a48dfbebc9acbe2568885192ec3ec6745fdfcde2"},"source":"resudal_vec = y_pred_train - y_train\nresudal_vec.plot.hist(bins=100)"},{"source":"1% самых экстремальных значений - это выбросы.  Они слишком сильно превышают среднее значение по выборке - из-за этого распределение целевой переменной не похоже на распределение экспоненциального семейства.\n\nВыкинем выбросы и снова посмотрим на остатки.","cell_type":"markdown","metadata":{"_cell_guid":"0c5bc55d-3087-4255-bded-7190efb8333b","_uuid":"e950a16984f2ed8d2df2dcc0d0f55fb4d26d7c40"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"ec7184bb-c791-445f-a8c8-d5eabbba8691","_uuid":"1e3b110f7bdd794b586d666c8cbb6d09ed2a4027"},"source":"filter_outlier = resudal_vec > resudal_vec.quantile(q=0.01)\nresudal_vec[filter_outlier].hist(bins=100)"},{"source":"Распределение стало более симметричным. Можем удалить из обучающей выборки и посмотреть, как изменилось качество.","cell_type":"markdown","metadata":{"_cell_guid":"cf09f0b3-e363-4375-a512-c4a3ef017556","_uuid":"189ab796e8906c0dbfeb5f3bf5f86115d496a038"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"69273f78-8c2b-47ac-898a-9a40ab0a440a","_uuid":"bd8d161a42d2d620be86306ac1044feee4681983"},"source":"model_norm = LinearRegression(normalize=False)\nmodel_norm.fit(X_train_scale[filter_outlier], y_train[filter_outlier])\nreg_coeff_norm = dict(zip(data.columns, model_norm.coef_))\nprint(reg_coeff_norm)"},{"source":"Коэффициенты стали выглядеть более осмысленно - теперь они как минимум в одном масштабе","cell_type":"markdown","metadata":{"_cell_guid":"ceedad3e-a3c1-44a8-8dfa-e6934a62d2a9","_uuid":"1abce10e386c1146d0457e129e952855cb6e6a4d"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_cell_guid":"f87c94ac-1249-48e3-aee0-717e87752ff1","_uuid":"7c310f3842d6eb57149af2f0038bf410f4337e1e"},"source":"y_pred_train = model_norm.predict(X_train_scale[filter_outlier])\nprint(\"Качество на тесте {}\".format(mean_squared_error(y_train[filter_outlier], y_pred_train)))\ny_pred = model_norm.predict(X_test_scale)\nprint(\"Качество на контроле {}\".format(mean_squared_error(y_test, y_pred)))\n\nsns.barplot(x = X_train_scale.columns, y=model_norm.coef_)\nplt.xticks(rotation=90)"},{"source":"Качество на тесте сильно улучшилось, на контрол","cell_type":"markdown","metadata":{"_cell_guid":"8277ceba-d7ae-4d24-97a2-d98411837a4b","_uuid":"1d8d366b20156ef0fd9f7a278a672cfd060faba1"}},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"f440f2c20c1464312b8ffc7f4a791ff88093df7c","_cell_guid":"32a77921-f0c8-4dfd-b067-88345ed7b25c","collapsed":true},"source":""},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"a874003ef33626bbdc3ba43754b1399b4fa83b04","_cell_guid":"8d5d7fba-9a00-496a-8bf2-866750cb77cf","collapsed":true},"source":""},{"outputs":[],"execution_count":null,"cell_type":"code","metadata":{"_uuid":"87de51f57e9116f120960066cb3f634d7b70b75f","_cell_guid":"29ef32f1-3f01-4eaa-b9c2-cbde2a3c2ffd","collapsed":true},"source":""}]}