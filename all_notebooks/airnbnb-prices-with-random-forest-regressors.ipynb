{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Airbnb Listings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n#Import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pyodbc\nimport time\nimport seaborn as sns\nfrom scipy import stats\nfrom collections import Counter\nimport geopy.distance\nfrom IPython import get_ipython\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom yellowbrick.regressor import ResidualsPlot\nfrom sklearn import preprocessing\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GridSearchCV\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read .csv file\ndf = pd.read_csv('../input/berlin-airbnb-data/listings_summary.csv', sep = ',')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Inspect column types\ndf.info(verbose=True, null_counts=True)\n\n#Inspect duplicates on primary key(single column)\ndf['id'].duplicated().sum()\n#There are no duplicates on the primary key (lisitng id)\n\n\n#Denoting duplicate whole rows (for all columns)\n#df.duplicated().sum()\n\n#Investigating nans\nnulls = df.isnull().sum().reset_index()\n\n#Or investigating from custom function \ndef missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns\n\nmissing_values_table(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Investigate column values for our analysis and ML model (We may drop some columns after the investigation)\ndf['price'].value_counts()\ndf['host_location'].value_counts() #Create new column with 2 values Germany/ No Germany\ndf['host_response_time'].value_counts() #Many nulls\ndf['host_response_rate'].value_counts() # #Drop due to imbalanced values, nulls and meaning\ndf['host_is_superhost'].value_counts()\ndf['host_has_profile_pic'].value_counts()\ndf['host_identity_verified'].value_counts()\ndf['neighbourhood_cleansed'].value_counts()\ndf['neighbourhood_group_cleansed'].value_counts()\ndf['market'].value_counts() #Drop due to imbalanced values, obvious meaning\ndf['property_type'].value_counts() #Drop due to imbalanced and many values\ndf['bed_type'].value_counts()\ndf['amenities'].value_counts() #Split, create new columns for main attributes\ndf['calendar_updated'].value_counts() #No meaning \ndf['has_availability'].value_counts() #No meaning\ndf['is_location_exact'].value_counts() #Drop due to imbalanced values\ndf['requires_license'].value_counts() #Drop due to imbalanced values\ndf['instant_bookable'].value_counts()\ndf['is_business_travel_ready'].value_counts() #Drop due to imbalanced values\ndf['cancellation_policy'].value_counts() \ndf['bed_type'].value_counts() #No meaning for almost 700 values different from real bed\ndf['require_guest_profile_picture'].value_counts() ##May drop due to imbalanced values, will work with this\ndf['require_guest_phone_verification'].value_counts() #May drop due to imbalanced values, will work with this\ndf['experiences_offered'].value_counts() #Contains only None value which is not considered as null\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The selected dropped columns have been chosen based on the below criteria<br>\n1)Contain special values, much information that is not recorder properly<br>\n2)Contain many nulls<br>\n3)Have the same meaning with other columns<br>\n4)They don't give value to our scope<br>\n5)Combination<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['street', 'neighbourhood', 'city', 'state', 'smart_location', 'host_id', 'host_acceptance_rate', 'thumbnail_url', 'jurisdiction_names', 'xl_picture_url', 'medium_url', 'square_feet', \\\n         'monthly_price', 'license', 'weekly_price', 'host_acceptance_rate', 'thumbnail_url', 'jurisdiction_names',\\\n         'xl_picture_url', 'medium_url', 'square_feet', 'monthly_price', 'license', 'weekly_price',\\\n         'listing_url', 'scrape_id', 'last_scraped', 'experiences_offered', 'picture_url', 'host_url',\\\n         'host_name', 'host_about', 'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood',\\\n         'host_listings_count', 'host_verifications', 'country_code', 'country'\\\n         , 'calendar_last_scraped', 'first_review', 'last_review', 'calendar_updated',\\\n         'has_availability', 'market', 'host_response_rate', 'host_response_time',\\\n         'property_type', 'is_location_exact', 'requires_license', 'is_business_travel_ready', 'zipcode', 'bed_type'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We will keep only neigbourhood_cleansed, neighbourhood_group to work with the area of the listing because of false inserted rows\nfor the columns street, city, state, neighbourhood(we will keep the cleanses and group), smart_location. <br>\nMany rows contain \"Berlin\" written in several languages and values like plain 'X' etc. <br>\n- We drop host_id because of the existence of the column host_listings monthly & weekly prices would be also good variables to predict but we got insufficient data, many nulls.\n- Drop columns that contain information for NLP Processing If that's the scope <br>\nWe won't use them on this 1st attempt. We can investigate for the future.   \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['name', 'summary', 'space', 'description', 'neighborhood_overview', 'notes', 'transit', 'access',\\\n         'interaction', 'house_rules'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Inspect column types again\ndf.info(verbose=True, null_counts=True)\n\n#Convert column types\ndf['id'] = df['id'].astype(str) #df['id'] = df['id'].astype('str') for pandas version >= 1.0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Price columns contain special character (dollar sign) and need to be conveted to float \n#First we investigate if these columns contain nans\ndf.price.isna().sum()\n#0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.cleaning_fee.isna().sum()\n#7280","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.extra_people.isna().sum()\n#0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.security_deposit.isna().sum()\n#9624","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['cleaning_fee'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['security_deposit'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There's a fair number of nans for cleaning_fee and security_deposit.<br>\nWe need to decide the way we will proceed with this information. We got 3 options.\n\n  1. Drop this columns. \n  We don't want to drop of course the rows containing nans. \n\n  2. Replace with 0 suppose the user did not fill this kind of information because filling this field was not mandatory\n  This solution is preferable because the scenario has a high probability to be realistic. \n  There are 2233 and 6674 zero values respectively for these 2 columns(cleaning_fee and security_deposit)\n\n  3. Replace with average value of non nans\n  We may test this method too, but if scenario 2 is true we will leverage these columns. \n\nNan values will be replaced first because if we convert to float and replace afterwards,\nwe get wrong values after conversion, nan are also converted as float values. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replace nans\ndf.cleaning_fee.fillna('$0.00', inplace=True)\ndf.security_deposit.fillna('$0.00', inplace=True)\n\n#First we clear dollar signs, the lstrip() method removes any leading characters\ndf['price'] = df['price'].map(lambda x: x.lstrip('$'))\ndf['cleaning_fee'] = df['cleaning_fee'].map(lambda x: x.lstrip('$'))\ndf['security_deposit'] = df['security_deposit'].map(lambda x: x.lstrip('$'))\ndf['extra_people'] = df['extra_people'].map(lambda x: x.lstrip('$'))\n\n#Then we clear commas\ndf['price'] = df['price'].str.replace(',', '')\ndf['cleaning_fee'] = df['cleaning_fee'].str.replace(',', '')\ndf['security_deposit'] = df['security_deposit'].str.replace(',', '')\ndf['extra_people'] = df['extra_people'].str.replace(',', '')\n\n# Convert safely to float\ndf['price'] = df['price'].astype(float)\ndf['cleaning_fee'] = df['cleaning_fee'].astype(float)\ndf['security_deposit'] = df['security_deposit'].astype(float)\ndf['extra_people'] = df['extra_people'].astype(float)\n\n#Get a view of values min max etc in order to investigate if coolumns contain strange values(negatives etc)\ndf['price'].describe() #There are zero price listings, we will cover this below\ndf['cleaning_fee'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['security_deposit'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['extra_people'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_table(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are somes nans for review score columns. The vast majority of these nans(19,8%) has to do with 0 number of reviews.\nHaving this in mind we will first drop these columns for our predictions and investigate them after the 1st attempt\nPredicting price based on reviews only for some listings does not make sense. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['review_scores_value', 'review_scores_checkin', 'review_scores_location', 'review_scores_communication'\\\n         , 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_rating', 'reviews_per_month'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because of the fact that majority of hosts are located at Berlin (75%), 143 missing values for \nhost_location will be replaced with DE. Below we will create a new column from 'Germany' and 'DE' values "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['host_location'].fillna('DE', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Replacements below refer to <100 records for each column\n#host_since\n#Repalce these 51 records with 01/01/2015, 2015 is the mode. 2014 follows.\ndf['host_since'].fillna('2015-01-01', inplace=True)\ndf['host_since'] = (pd.to_datetime(df['host_since'], format='%Y-%m-%d'))\n\n#host_total_listings_count\ndf['host_total_listings_count'].value_counts()\n#16315 records for value 1 \ndf['host_total_listings_count'].fillna(1, inplace=True)\n \n#host_is_superhost\ndf['host_is_superhost'].value_counts()\n#20642 records for value f\ndf['host_is_superhost'].fillna('f', inplace=True)\n\n#host_has_profile_pic\ndf['host_has_profile_pic'].value_counts()\n#24430 records for value t\ndf['host_has_profile_pic'].fillna('t', inplace=True)\n#Maybe we can drop this column if we dont face significant differences at prices \n\n#host_identity_verified\ndf['host_identity_verified'].value_counts()\n#15818 records for value t\ndf['host_identity_verified'].fillna('f', inplace=True)\n\ndf['host_identity_verified'].value_counts()\n#15818 records for value t\ndf['host_identity_verified'].fillna('f', inplace=True)\n\n#cancellation_policy\ndf['cancellation_policy'].value_counts()\n#15818 records for value t\ndf['cancellation_policy'].fillna('flexible', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because of the numeric type of beds, bathrooms, bedrooms combined with the fact that nans are not many,\nrows with nans for these columns will be dropped."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[pd.notnull(df['beds'])]\ndf = df[pd.notnull(df['bathrooms'])]\ndf = df[pd.notnull(df['bedrooms'])]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create new columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create amanities columns\n# We want to extract all the information from amenities column in order to create new variables\namanities_processed = df['amenities'].map(lambda x: x.lstrip('{'))\namanities_processed = amanities_processed.map(lambda x: x.rstrip('}'))\namanities_processed = amanities_processed.str.replace('\"','')\n\n#https://stackoverflow.com/questions/2600191/how-can-i-count-the-occurrences-of-a-list-item\nl = list(amanities_processed.values)\nl = (\",\".join(l))\ntest = l.split(',')\nfinal = Counter(test)\n\n#There are 175 different amentities elements. \n#https://stackoverflow.com/questions/62567406/pandas-check-if-a-substring-exists-in-another-column-then-create-a-new-column-w\n#We will create some columns based on amentities we imagine that can affect the price of a listing and are not so\n#common (i.e wifi)\n\ndf['hair_dryer'] = df['amenities'].map(lambda x: 'Yes' if 'Hair dryer' in x else 'No')\ndf['laptop_workspace'] = df['amenities'].map(lambda x: 'Yes' if 'Laptop friendly workspace' in x else 'No')\ndf['iron'] = df['amenities'].map(lambda x: 'Yes' if 'Iron' in x else 'No')\ndf['hot_water'] = df['amenities'].map(lambda x: 'Yes' if 'Hot water' in x else 'No')\ndf['tv'] = df['amenities'].map(lambda x: 'Yes' if 'TV' in x else 'No')\ndf['family_kid_friendly'] = df['amenities'].map(lambda x: 'Yes' if 'Family/kid friendly' in x else 'No')\ndf['refrigerator'] = df['amenities'].map(lambda x: 'Yes' if 'Refrigerator' in x else 'No')\ndf['cooking_basics'] = df['amenities'].map(lambda x: 'Yes' if 'Cooking basics' in x else 'No')\ndf['oven'] = df['amenities'].map(lambda x: 'Yes' if 'Oven' in x else 'No')\ndf['elevator'] = df['amenities'].map(lambda x: 'Yes' if 'Elevator' in x else 'No')\ndf['free_street_parking'] = df['amenities'].map(lambda x: 'Yes' if 'Free street parking' in x else 'No')\ndf['smoking'] = df['amenities'].map(lambda x: 'Yes' if 'Smoking allowed' in x else 'No')\n\n#Drop amenities \ndf.drop(['amenities'], axis=1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create column which indicates distance from center based on long, lat \n\n#Get Berlin coordinates\n#https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n#https://www.latlong.net/place/berlin-germany-9966.html\n\ndef distance_center(row): \n    berlin_coords = (52.520008, 13.404954)\n    listing_coords = (row['latitude'],row['longitude'])\n    return geopy.distance.distance(listing_coords, berlin_coords).km\n    \n    \ndf['distance'] = df.apply(lambda row: distance_center(row), axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['distance'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create new column for host_location with 2 values in/out of Germany\n#Previously, we replaced nans of host location with DE. After searching the values\n#we create a new column host_in_germany to inspect if hosts located to other countries differ their listings' prices.\ndef host_in_germany(row):\n   if (('Germany' in row['host_location']) | ('DE' in row['host_location'])):\n       return 'Yes'\n   else:\n       return 'No'\n\ndf['host_in_germany'] = df.apply(lambda row: host_in_germany(row), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['host_in_germany'].value_counts() # We got few 1885 listings from hosts located outside from Germany\n\n#Drop host_location\ndf.drop(['host_location'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create column total_months_hosting from today - host_since (in months)\ndf['host_total_months'] = ((pd.to_datetime('today') - df['host_since'])/np.timedelta64(1, 'M'))\ndf['host_total_months'] = df['host_total_months'].astype(int)\n\n#Drop host_since\ndf.drop(['host_since'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"#EDA\n#We first inspect our dependent variable\ndf['price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#There are records with 0 price value. We will drop these rows\ndf = df.loc[df['price'] >0]\n#6 records deleted\n\n#Create boxplot to inspect values\nax = sns.boxplot( y=\"price\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Limit price to 1000\nax = sns.boxplot( y=\"price\", data=df)\nax.set_ylim([0, 1000]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Computing IQR\nq1 = df['price'].quantile(0.25)\nq3 = df['price'].quantile(0.75)\niqr = q3 - q1\nprint(iqr)\n\nprint(q3+(1.5*iqr))\n\n\noutliers= df.loc[df['price'] > q3+(1.5*iqr)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got 1641 records considered as outliers. These records may include more expensive listing.<br>\nWe are going to delete these rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_without_outliers = df.loc[df['price'] <= q3+(1.5*iqr)]\n\n#Boxplot\nax = sns.boxplot( y=\"price\", data=df_without_outliers)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Matrix before removing outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create correlation matrix\n#inspect correlations\ndf_without_outliers.columns\n\n#Let's see first the correlation of price and all these selected variables for the dataframe\n#which includes the outliers\ncorr_df = df.loc[:, ['host_total_listings_count',\n        'accommodates', 'bathrooms', 'bedrooms',\n       'beds', 'security_deposit', 'cleaning_fee', 'guests_included',\n       'extra_people', 'minimum_nights', 'maximum_nights', 'availability_30',\n       'availability_60', 'availability_90', 'availability_365',\n       'number_of_reviews','calculated_host_listings_count',  'distance',\n       'host_total_months', 'price']]\n\n#Inspect corr matrix (default value for pearson correlation)\ncorrMatrix = corr_df.corr()\nfig, ax = plt.subplots(figsize=(30,30)) \nax = sns.heatmap(corrMatrix, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's not a significant correlation of any variable with price"},{"metadata":{},"cell_type":"markdown","source":"## Correlation Matrix after removing outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_df_without_outliers = df_without_outliers.loc[:, ['host_total_listings_count',\n        'accommodates', 'bathrooms', 'bedrooms',\n       'beds', 'security_deposit', 'cleaning_fee', 'guests_included',\n       'extra_people', 'minimum_nights', 'maximum_nights', 'availability_30',\n       'availability_60', 'availability_90', 'availability_365',\n       'number_of_reviews','calculated_host_listings_count',  'distance',\n       'host_total_months', 'price']]\n\n#Inspect corr matrix (default value for pearson correlation)\ncorr_matrix_outliers = corr_df_without_outliers.corr()\nfig, ax = plt.subplots(figsize=(30,30)) \nax = sns.heatmap(corr_matrix_outliers, annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have some worthy correlations when excluding all the outliers. <br>\nAccomodates, bedrooms, beds, cleaning fee, guests_included are positive correlated with price<br>\nMaybe listings count should not be included in our independent variables list\nbecause of the negative correlation for total host private rooms and the fact \nthat all other host listing variables are positive correlated. <br>\nThis can be due to randomness and not a pattern. It does not make sense at first sight. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now  check spearman corrs\nspearman_matrix = df_without_outliers.corr(method='spearman')\nspearman_matrix_df = spearman_matrix\nspearman_matrix_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pearson & spearman results do not differ. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Inspect price boxplots for different categorical variables\nsns.boxplot(x='host_is_superhost', y='price', data=df_without_outliers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='host_has_profile_pic', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='host_identity_verified', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='neighbourhood_group_cleansed', y='price', data=df_without_outliers)\n#There different price distributions per group\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='room_type', y='price', data=df_without_outliers) \n#Entire are clearly more expensive\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='instant_bookable', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='cancellation_policy', y='price', data=df_without_outliers)\n#Price differs for different policies values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='require_guest_profile_picture', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='require_guest_phone_verification', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='hair_dryer', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='laptop_workspace', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='iron', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='hot_water', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='tv', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='family_kid_friendly', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='refrigerator', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='cooking_basics', y='price', data=df_without_outliers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='oven', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='elevator', y='price', data=df_without_outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='free_street_parking', y='price', data=df_without_outliers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='smoking', y='price', data=df_without_outliers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='host_in_germany', y='price', data=df_without_outliers) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our new variable has not an impact on price."},{"metadata":{},"cell_type":"markdown","source":"Most remarkable differences in price occur for Tv and smoking\nListings with tv and smoke free seem to be a bit more expensive\n"},{"metadata":{},"cell_type":"markdown","source":"## Pairplot for variables that seem to be crucial for predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_pairplot = ['price', 'accommodates', 'guests_included', 'beds']\nsns.pairplot(df_without_outliers[columns_pairplot])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning Model\n"},{"metadata":{},"cell_type":"markdown","source":"We will follow 2 approches for predicting the price\n 1.  approach contains host info\n 2.  approach does not contain host info. It will only contain listing data\n\nTry this because we want to inspect if the airbnb platform host data affect the price.<br>\nMethod 1 predicts based completely with all available listing features. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Select columns as indepentent variables\ndf_without_outliers.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['host_is_superhost', 'host_total_listings_count',\n       'host_has_profile_pic', 'host_identity_verified',\n       'neighbourhood_group_cleansed', 'room_type', 'accommodates', 'bathrooms', 'bedrooms',\n       'beds', 'security_deposit', 'cleaning_fee', 'guests_included',\n       'extra_people', 'minimum_nights', 'maximum_nights', 'availability_30',\n       'availability_60', 'availability_90', 'availability_365',\n       'number_of_reviews', 'instant_bookable',\n       'cancellation_policy', 'require_guest_profile_picture',\n       'require_guest_phone_verification', 'hair_dryer',\n       'laptop_workspace', 'iron', 'hot_water', 'tv', 'family_kid_friendly',\n       'refrigerator', 'cooking_basics', 'oven', 'elevator',\n       'free_street_parking', 'smoking', 'distance', 'host_in_germany',\n       'host_total_months']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove ltm reviews, long, lat"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Split  categorical and numeical varialbes\n#categorical = list(df_without_outliers.select_dtypes(include=['object']).columns)\n#numerical = list(df_without_outliers.select_dtypes(include=['float64', 'int64']).columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By one-hot encoding a categorical variable, we are inducing sparsity into the dataset which is undesirable."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create copy\nml_data = df_without_outliers.copy()\n\nle = preprocessing.LabelEncoder()\nfor column_name in ml_data.columns:\n    if ml_data[column_name].dtype == object:\n        ml_data[column_name] = le.fit_transform(ml_data[column_name])\n    else:\n        pass\n    \n#Split dataset to independent and dependent variables\nX = ml_data[cols]\ny = ml_data['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grid search\nregressor = RandomForestRegressor(random_state = 0)\n\n#n_estimatos = The number of trees in the forest.\n\n#min_samples_split specifies the minimum number of samples required to split an internal node,\n#while min_samples_leaf specifies the minimum number of samples required to be at a leaf node.\n\n#max_features is the size of the random subsets of features to consider when splitting a node.\n#So max_features is what you call m. When max_features=\"auto\", m = p and no feature subset \n#selection is performed in the trees, so the \"random forest\" is actually a bagged ensemble of\n#ordinary regression trees.\n\n#Bootstrap is set to True by default\n\nparam_grid = { \n            \"n_estimators\"      : [20,30,50],\n            \"max_features\"      : [\"auto\", \"sqrt\", \"log2\"],\n            \"min_samples_split\" : [4,6,8],\n            }\n\ngrid = GridSearchCV(regressor, param_grid, n_jobs=-1, cv=5)\ngrid.fit(X_train, y_train)\n\nprint((\"best logistic regression from grid search: %.3f\"\n       % grid.score(X_test, y_test)))\n\nprint(grid.best_params_)\n\n#score = R^2 of the prediction.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = RandomForestRegressor(n_estimators = 50, min_samples_split = 8, random_state = 0)\nregressor.fit(X_train, y_train)\n    \npreds = regressor.predict(X_test)\n\nrf_mse = mean_squared_error(y_test, preds)\nrf_rmse = np.sqrt(rf_mse)\nprint('rf_rmse', rf_rmse) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(regressor, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\nrmse_scores = np.sqrt(-scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rmse_scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For every fold our rmse ranges from 16 to 18. \nFurthermore, the most important here is that the rmse is stable without big differences per fold. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"ml_data['price'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ml_data['price'].std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will plot residuals to see if our errors are randomly distributed and the model does not suffer from heteroskedasticity, multicollinearity\n\nOne of the best explanations out there: \n\nMulticollinearity:\n\nYou sleep well before the same tests you study well for, so you do not know what to attribute the gains for. In order to come to some conclusions, you have to try studying without sleeping, or sleeping without studying.\n\nHeteroskedasticity:\n\nWhen you study for a test, you consistently get a score between 85 and 95. When you don’t study the results are more variable; your scores are between 60 and 90. <br>\n\nHeteroscedasticity refers to cases where the residuals have a non-constant variance"},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ResidualsPlot(regressor)\nvisualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_test, y_test)  # Evaluate the model on the test data\nvisualizer.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Residual = Observed – Predicted <br>\nPositive values for the residual (on the y-axis) mean the prediction was too low, and negative values mean the prediction was too high; 0 means the guess was exactly correct.\nThe error of our predictions is increased when the actual price is between 20 to 60. <br>\nModel has made some high prediction for low priced listings and some low predictions for high priced listings. <br>\nModel has NOT made high prediction for high priced listings (Max price at training set was 138) <br>\nDependent variable (price) has only positive values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importances\nimportance = regressor.feature_importances_\n\nfeats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(X.columns, regressor.feature_importances_):\n    feats[feature] = importance #add the name/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\nimportances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Room type, distance, accomodates seem to be the most important features"},{"metadata":{},"cell_type":"markdown","source":"### Calculate R^2 and Adj. R^2"},{"metadata":{"trusted":true},"cell_type":"code","source":"R2 = r2_score(y_test, preds)   \nAdj_r2 = 1-(1-R2)*(18266-1)/(18266-40-1)\nprint(Adj_r2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method 2 (Let's remove any host and platform information)"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_method_2 = ['neighbourhood_group_cleansed','room_type', 'accommodates', 'bathrooms', 'bedrooms',\n       'beds', 'hair_dryer','laptop_workspace', 'iron', 'hot_water', 'tv', 'family_kid_friendly',\n       'refrigerator', 'cooking_basics', 'oven', 'elevator',\n       'free_street_parking', 'distance']\n\nX = ml_data[cols_method_2]\ny = ml_data['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grid search\nregressor = RandomForestRegressor(random_state = 0)\n\n#n_estimatos = The number of trees in the forest.\n\n#min_samples_split specifies the minimum number of samples required to split an internal node,\n#while min_samples_leaf specifies the minimum number of samples required to be at a leaf node.\n\n#max_features is the size of the random subsets of features to consider when splitting a node.\n#So max_features is what you call m. When max_features=\"auto\", m = p and no feature subset \n#selection is performed in the trees, so the \"random forest\" is actually a bagged ensemble of\n#ordinary regression trees.\n\n#Bootstrap is set to True by default\n\nparam_grid = { \n            \"n_estimators\"      : [20,30,50],\n            \"max_features\"      : [\"auto\", \"sqrt\", \"log2\"],\n            \"min_samples_split\" : [4,6,8],\n            }\n\ngrid = GridSearchCV(regressor, param_grid, n_jobs=-1, cv=5)\ngrid.fit(X_train, y_train)\n\nprint((\"best logistic regression from grid search: %.3f\"\n       % grid.score(X_test, y_test)))\n\nprint(grid.best_params_)\n\n#score = R^2 of the prediction.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"max_features is the size of the random subsets of features to consider when splitting a node.<br>\nIf max_features=\"auto\" then no feature subset selection is performed in the trees.<br>\nIf max_features != 'auto' then we have a 'real' RF"},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor = RandomForestRegressor(max_features = 'sqrt', min_samples_split= 8, n_estimators = 50, random_state = 0)\nregressor.fit(X_train, y_train)\n    \npreds = regressor.predict(X_test)\n\nrf_mse = mean_squared_error(y_test, preds)\nrf_rmse = np.sqrt(rf_mse)\nprint('rf_train_rmse', rf_rmse) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_method_2 = cross_val_score(regressor, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\nrmse_scores_method_2 = np.sqrt(-scores_method_2)\nrmse_scores_method_2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For every fold our rmse ranges from 18 to 20. <br>\nWe are now sure that host and platform data as it was expected affect the price of a listing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"visualizer = ResidualsPlot(regressor)\nvisualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_test, y_test)  # Evaluate the model on the test data\nvisualizer.show() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our predictions are a bit more skewed to left and right. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importances, get importance\nimportance = regressor.feature_importances_\n\nfeats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(X.columns, regressor.feature_importances_):\n    feats[feature] = importance #add the name/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\nimportances.sort_values(by='Gini-importance').plot(kind='bar', rot=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again, room type, distance, accommodates remain the most important features. \nDistance coeff importance is increased versus previous model"},{"metadata":{},"cell_type":"markdown","source":"### Calculate R^2 and Adj. R^2"},{"metadata":{"trusted":true},"cell_type":"code","source":"R2 = r2_score(y_test, preds)   \nAdj_r2 = 1-(1-R2)*(16980-1)/(16980-18-1)\nprint(Adj_r2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary "},{"metadata":{},"cell_type":"markdown","source":"- Including host and platform data can help us predict a listing's price. \n\n- Comparing the statistics (mean, std, median, quantiles etc) with the rmse and our model's error we doubt about the efficiency of the model.<br>\n\n- The listings we deal with are not so expensive to accept without doubt our model's accuracy. Imagine we schedule a  5 days trip and our accomodation cost comes to 5 * 40 (per night) = 200 euro. A fair prediction of our model  would be 5*57 = 285 euro. \n  How should we react in such an increase?\n  If the prediction was lower than the actual value how should we react when we have to pay 115 euro? We may had some serious     concerns about the quality of service there.<br>\n- We can also make acceptable bins (tight ranges) for price and convert this regression to a classification problem\n\n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}