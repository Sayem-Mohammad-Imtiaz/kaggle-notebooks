{"cells":[{"metadata":{},"cell_type":"markdown","source":"# K - Means Overview\n\nThis is a pure Python implementation of the K-Means clustering algorithm.\n\nhttps://www.naftaliharris.com/blog/visualizing-k-means-clustering/"},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:44:54.167244Z","iopub.status.busy":"2020-04-23T08:44:54.166706Z","iopub.status.idle":"2020-04-23T08:45:05.973676Z","shell.execute_reply":"2020-04-23T08:45:05.971845Z","shell.execute_reply.started":"2020-04-23T08:44:54.166949Z"},"trusted":true},"cell_type":"code","source":"import math\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom copy import deepcopy\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics.pairwise import euclidean_distances\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:05.976777Z","iopub.status.busy":"2020-04-23T08:45:05.976382Z","iopub.status.idle":"2020-04-23T08:45:09.143455Z","shell.execute_reply":"2020-04-23T08:45:09.142692Z","shell.execute_reply.started":"2020-04-23T08:45:05.976723Z"},"trusted":true},"cell_type":"code","source":"from bokeh.plotting import figure, show, ColumnDataSource\nimport bokeh.models as bmo\nfrom bokeh.io import output_notebook\noutput_notebook()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simulated Data\nSKlearn has a couple different methods for simulating data for testing and development purposes. We're generating a serries of fake cluster data with 5 clusters and 500 observations. THe data will only have two dimensions."},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.145046Z","iopub.status.busy":"2020-04-23T08:45:09.144765Z","iopub.status.idle":"2020-04-23T08:45:09.16192Z","shell.execute_reply":"2020-04-23T08:45:09.160567Z","shell.execute_reply.started":"2020-04-23T08:45:09.145Z"},"trusted":true},"cell_type":"code","source":"#Going to make fake data using Sklearn\nX, y = make_blobs(n_samples=500, centers=5, n_features=2,\n                  random_state=745)\n\ndf = pd.DataFrame({'X_1': X[:,0],\n                  'X_2':X[:,1],\n                  'Y':y})\n\ndf['group'] = df['Y'].astype('str')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Charting in Bokeh"},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.163356Z","iopub.status.busy":"2020-04-23T08:45:09.163154Z","iopub.status.idle":"2020-04-23T08:45:09.198035Z","shell.execute_reply":"2020-04-23T08:45:09.19643Z","shell.execute_reply.started":"2020-04-23T08:45:09.163322Z"},"trusted":true},"cell_type":"code","source":"deloitte_palette = [\"#000000\",\"#86BC25\",\"#C4D600\",\"#43B02A\",\"#046A38\",\"#2C5234\", \"#0097A9\", \"#62B5E5\",\n                   \"#00A3E0\", \"#0076A8\", \"#012169\"]\n\nsource = ColumnDataSource(df)\n\ncolor_map = bmo.CategoricalColorMapper(factors=df['group'].unique(), palette=deloitte_palette)\n\np = figure()\n\np.circle(x='X_1', y=\"X_2\", radius=.23, \n         fill_alpha=0.6, source=source, \n         fill_color={'field' : 'group', 'transform': color_map})","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.200085Z","iopub.status.busy":"2020-04-23T08:45:09.199675Z","iopub.status.idle":"2020-04-23T08:45:09.355704Z","shell.execute_reply":"2020-04-23T08:45:09.354724Z","shell.execute_reply.started":"2020-04-23T08:45:09.200036Z"},"trusted":true},"cell_type":"code","source":"show(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Means Algorithm: *By-Hand Example*\n\n1. [Choose $k$ initial centroids (note that $k$ is an input)](#Step1)\n2. For each point $p$:\n  - Find distance to each centroid\n  - Assign point to nearest centroid\n3. Recalculate centroid positions\n4. Repeat steps 2-3 until stopping criteria met"},{"metadata":{},"cell_type":"markdown","source":"### Step 1: Choosing Initial Centroids <a id=Step1></a>\n\nThere are several options to pick an initial centroid positions:\n1. Randomly (may yield divergent behavior)\n2. Perform alternative clustering task, use resulting centroids as initial k-means centroids\n3. Start with global centroid, choose point at max distance, repeat (but might select outlier)"},{"metadata":{},"cell_type":"markdown","source":"#### Random initial centroids"},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.357466Z","iopub.status.busy":"2020-04-23T08:45:09.357049Z","iopub.status.idle":"2020-04-23T08:45:09.380891Z","shell.execute_reply":"2020-04-23T08:45:09.379086Z","shell.execute_reply.started":"2020-04-23T08:45:09.357419Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"k = 5\n\nr = np.random.randint(low=0, high=X.shape[0], size=k)\ninitial = X[r,:]\n\nprint(\"Our initial centroids:\")\ninitial","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.384243Z","iopub.status.busy":"2020-04-23T08:45:09.383213Z","iopub.status.idle":"2020-04-23T08:45:09.417174Z","shell.execute_reply":"2020-04-23T08:45:09.411659Z","shell.execute_reply.started":"2020-04-23T08:45:09.383774Z"},"trusted":true},"cell_type":"code","source":"p.diamond_cross(x=initial[:,0], \n                y=initial[:,1], \n                size=20, \n                color=\"#386CB0\", \n                fill_color=None, \n                line_width=2)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.424984Z","iopub.status.busy":"2020-04-23T08:45:09.424185Z","iopub.status.idle":"2020-04-23T08:45:09.536168Z","shell.execute_reply":"2020-04-23T08:45:09.534851Z","shell.execute_reply.started":"2020-04-23T08:45:09.42465Z"},"trusted":true},"cell_type":"code","source":"show(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 2: Assessing Similarity\n\nHow do you determine which centroid a given point is most similar to? The similarity criterion is determined by the measure we choose. In the case of k-means clustering, the most common similarity metric is *__Euclidean distance:__*\n\n$$ d(x_1,x_2) = \\sqrt{\\sum_{i=1}^N(x_{1i} - x_{2i})^2} $$\n\nBoth `numpy` and `sklearn` have implementations of euclidian which we can leverage. "},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.540055Z","iopub.status.busy":"2020-04-23T08:45:09.538169Z","iopub.status.idle":"2020-04-23T08:45:09.553332Z","shell.execute_reply":"2020-04-23T08:45:09.551475Z","shell.execute_reply.started":"2020-04-23T08:45:09.538899Z"},"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import euclidean_distances\n\ndist = euclidean_distances(X, initial)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dist","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.556569Z","iopub.status.busy":"2020-04-23T08:45:09.555919Z","iopub.status.idle":"2020-04-23T08:45:09.563713Z","shell.execute_reply":"2020-04-23T08:45:09.561674Z","shell.execute_reply.started":"2020-04-23T08:45:09.55646Z"},"trusted":true},"cell_type":"code","source":"cluster = np.argmin(dist, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 3: Recompute the Center\nHow do we recompute the positions of the centers at each iteration of the algorithm?\n\nWe calculate the centroid at the geometric center of our new assigned clusters. `Pandas` has significanly eaiser ways to perform group by operations over `numpy`."},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.566263Z","iopub.status.busy":"2020-04-23T08:45:09.565312Z","iopub.status.idle":"2020-04-23T08:45:09.623908Z","shell.execute_reply":"2020-04-23T08:45:09.621285Z","shell.execute_reply.started":"2020-04-23T08:45:09.566171Z"},"trusted":true},"cell_type":"code","source":"points = pd.DataFrame.from_records(X, columns=['x', 'y'])\npoints['cluster'] = cluster\n\npoints.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.627401Z","iopub.status.busy":"2020-04-23T08:45:09.626638Z","iopub.status.idle":"2020-04-23T08:45:09.663402Z","shell.execute_reply":"2020-04-23T08:45:09.66144Z","shell.execute_reply.started":"2020-04-23T08:45:09.627281Z"},"trusted":true},"cell_type":"code","source":"centroids = points.groupby('cluster').mean()\ncentroids.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.668576Z","iopub.status.busy":"2020-04-23T08:45:09.667709Z","iopub.status.idle":"2020-04-23T08:45:09.780018Z","shell.execute_reply":"2020-04-23T08:45:09.778165Z","shell.execute_reply.started":"2020-04-23T08:45:09.668473Z"},"trusted":true},"cell_type":"code","source":"p.diamond_cross(x=centroids.x, \n                y=centroids.y, \n                size=20, \n                color=\"#ff0000\", \n                fill_color=None, \n                line_width=2)\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.786647Z","iopub.status.busy":"2020-04-23T08:45:09.781525Z","iopub.status.idle":"2020-04-23T08:45:09.998323Z","shell.execute_reply":"2020-04-23T08:45:09.997495Z","shell.execute_reply.started":"2020-04-23T08:45:09.786549Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"old = pd.DataFrame.from_records(initial, columns=[\"x_old\", \"y_old\"])\ncentroids = pd.concat([centroids,old],axis=1)\ncentroids.head()\n\ndef x_line(row):\n    return [row['x'],row['x_old']]\n\ndef y_line(row):\n    return [row['y'], row['y_old']]\n\ncentroids['xs'] = centroids.apply(x_line, axis=1)\ncentroids['ys'] = centroids.apply(y_line, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:09.999371Z","iopub.status.busy":"2020-04-23T08:45:09.999151Z","iopub.status.idle":"2020-04-23T08:45:10.089823Z","shell.execute_reply":"2020-04-23T08:45:10.087459Z","shell.execute_reply.started":"2020-04-23T08:45:09.999332Z"},"trusted":true},"cell_type":"code","source":"p.multi_line(xs=centroids['xs'], ys=centroids['ys'],color=\"navy\", alpha=0.3, line_width=4)\nshow(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Step 4: Converge\nWe iterate until some stopping criteria are met; in general, suitable convergence is achieved in a small number of steps. \n\nStopping criteria can be based on the centroids (eg, if positiosn change by no more than $\\epsilon$) or on the points (if no more than x% change clusters between iterations).\n\nUp to this point, we have been using illustrative examples of the steps. Now, we will wrap up our work in a KMeans class with some helper functions to iterate through the steps and fit the model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"np.inf","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:10.09215Z","iopub.status.busy":"2020-04-23T08:45:10.091699Z","iopub.status.idle":"2020-04-23T08:45:10.127252Z","shell.execute_reply":"2020-04-23T08:45:10.125385Z","shell.execute_reply.started":"2020-04-23T08:45:10.092079Z"},"trusted":true},"cell_type":"code","source":"def centroid(data):\n    \"\"\"Find the centroid of the given data.\"\"\"\n    return np.mean(data, 0)\n\n\ndef sse(data):\n    \"\"\"Calculate the SSE of the given data.\"\"\"\n    u = centroid(data)\n    return np.sum(np.linalg.norm(data - u, 2, 1))\n\n\nclass KMeansClusterer:\n    \"\"\"The standard k-means clustering algorithm.\"\"\"\n\n    def __init__(self, data=None, k=2, min_gain=0.01, max_iter=100,\n                 max_epoch=10, verbose=True):\n        \"\"\"Learns from data if given.\"\"\"\n        if data is not None:\n            self.fit(data, k, min_gain, max_iter, max_epoch, verbose)\n\n    def fit(self, data, k=2, min_gain=0.01, max_iter=100, max_epoch=10,\n            verbose=True):\n        \"\"\"Learns from the given data.\n        Args:\n            data:      The dataset with m rows each with n features\n            k:         The number of clusters\n            min_gain:  Minimum gain to keep iterating\n            max_iter:  Maximum number of iterations to perform\n            max_epoch: Number of random starts, to find global optimum\n            verbose:   Print diagnostic message if True\n        Returns:\n            self\n        \"\"\"\n        # Pre-process\n        self.data = np.matrix(data)\n        self.k = k\n        self.min_gain = min_gain\n        self.meta = []\n\n        # Perform multiple random init for global optimum\n        min_sse = np.inf\n        for epoch in range(max_epoch):\n\n            # Randomly initialize k centroids\n            indices = np.random.choice(len(data), k, replace=False)\n            u = self.data[indices, :]\n\n            # Loop\n            t = 0\n            old_sse = np.inf\n            while True:\n                t += 1\n\n                # Cluster assignment\n                C = [None] * k\n                for x in self.data:\n                    j = np.argmin(np.linalg.norm(x - u, 2, 1))\n                    C[j] = x if C[j] is None else np.vstack((C[j], x))\n\n                # Centroid update\n                for j in range(k):\n                    u[j] = centroid(C[j])\n\n                # Loop termination condition\n                if t >= max_iter:\n                    break\n                new_sse = np.sum([sse(C[j]) for j in range(k)])\n                gain = old_sse - new_sse\n                if verbose:\n                    line = \"Epoch {:2d} Iter {:2d}: SSE={:10.4f}, GAIN={:10.4f}\"\n                    print(line.format(epoch, t, new_sse, gain))\n                if gain < self.min_gain:\n                    if new_sse < min_sse:\n                        min_sse, self.C, self.u = new_sse, C, u\n                    break\n                else:\n                    old_sse = new_sse\n\n            if verbose:\n                print('')  # blank line between every epoch\n\n        return self","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-04-23T08:45:10.13015Z","iopub.status.busy":"2020-04-23T08:45:10.129197Z","iopub.status.idle":"2020-04-23T08:45:13.244777Z","shell.execute_reply":"2020-04-23T08:45:13.242521Z","shell.execute_reply.started":"2020-04-23T08:45:10.130074Z"},"trusted":true},"cell_type":"code","source":"t = KMeansClusterer(data=X, k=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Using K means clustering from scikit learn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Identifying appropriate K using Elbow method"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2= df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=['Y','group']\ndf2.drop(columns=cols,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distortions = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(df2)\n    distortions.append(kmeanModel.inertia_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting distortions or SSE of all the K values"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. The above code shows that inertia's change is no more significant post K=4 and hence we can go ahead with K=4"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_object = KMeans(n_clusters=4,random_state=123)\ny_groups=kmeans_object.fit_predict(df2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Identifying the cluster centers for 4 clusters\ncentroids=kmeans_object.cluster_centers_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":4}