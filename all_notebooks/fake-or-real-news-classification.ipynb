{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n<font color=\"blue\" size=+2.5><b>2.2 Library Import</b></font>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# !pip install fastai\n# import libraries\nimport missingno as msno\nimport fastai\nfrom fastai import *\nfrom fastai.text import * \nimport pandas as pd\nimport numpy as np\nfrom functools import partial\nimport io\nimport os","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6.1\"></a>\n# <font color=\"blue\" size=+2.5><b>Data Loading</b></font>"},{"metadata":{},"cell_type":"markdown","source":"../input/textdb3/fake_or_real_news.csv"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/textdb3/fake_or_real_news.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train[[\"text\",\"label\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[:1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lm = (TextList.from_df(train_df)\n           #Inputs: all the text files in path\n            .split_by_rand_pct(0.20)\n           #We randomly split and keep 20% for validation\n            .label_for_lm()           \n           #We want to do a language model so we label accordingly\n            .databunch(bs=128))\ndata_lm.save('tmp_lm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_lm.show_batch()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can create a language model based on the architecture \n[AWD_LSTM](https://docs.fast.ai/text.models.html#AWD_LSTM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Language model AWD_LSTM\nlearn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"9\"></a>\n<font color=\"blue\" size=+2.5><b> 3.4 Model Summary </b></font>\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"print('Model Summary:')\nprint(learn.layer_groups)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"10\"></a>\n<font color=\"blue\" size=+2.5><b> 3.5 Finding LR </b></font>\n"},{"metadata":{},"cell_type":"markdown","source":"**Lets train our language model. First, we call lr_find to analyze and find an optimal learning rate for our problem, then we fit or train the model for a few epochs. Finally we unfreeze the model and runs it for a few more epochs. So we have a encoder trained and ready to be used for our classifier and it is recorded on disk.**"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"11\"></a>\n<font color=\"blue\" size=+2.5><b> 3.6 HyperParameter Tuning For Model Training </b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(1, 1e-2)\nlearn.save('lm_hyper')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"12\"></a>\n<font color=\"blue\" size=+2.5><b> 3.7 Saving Model After Training </b></font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save_encoder('ft_enc')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"13\"></a>\n<font color=\"blue\" size=+2.5><b> 4. Building and Training a Text Classifier </b></font>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"14\"></a>\n<font color=\"blue\" size=+2.5><b> 4.1 Loading Data For Text Classification </b></font>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_clas = (TextList.from_df(train_df, cols=[\"text\"], vocab=data_lm.vocab)\n             .split_by_rand_pct(0.3)\n             .label_from_df('label')\n             .databunch(bs=32))\n\ndata_clas.save('tmp_class')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"15\"></a>\n<font color=\"blue\" size=+2.5><b> 4.2 Loading Text Classification Model </b></font>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"help(learn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**we will have to load the encoder previously trained (the language model).**"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"learn.load_encoder('ft_enc')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, the training cycle is repeated: lr_find, freeze except last layer,..., unfreeze the model and saving the final trained model.**"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"learn.freeze_to(-1)\nlearn.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"help(learn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(1, 1e-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('stage1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"22\"></a>\n<font color=\"blue\" size=+2.5><b>5.1 Interpret the results</b></font>\n<br/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.vision import ClassificationInterpretation\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(6,6), dpi=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interp = TextClassificationInterpretation.from_learner(learn)\ninterp.show_top_losses(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"23\"></a>\n<font color=\"blue\" size=+2.5><b>5.3 Save and Load Model</b></font>\n<br/>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.export()\nlearn.model_dir = \"/kaggle/working\"\nlearn.save(\"stage-1\",return_path=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"24\"></a>\n<font color=\"blue\" size=+2.5><b>5.4 Sources</b></font>\n<br/>\n* [Fastai MOOC](https://course.fast.ai/)\n* [Fastai library](https://docs.fast.ai/)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"18\"></a>\n<font color=\"blue\" size=+2.5><b>Feedback and Support</b></font>\n<br/>\n* Your feedback is much appreciated\n* Please UPVOTE if you LIKE this notebook\n* Comment if you have any doubts or you found any errors in the notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}