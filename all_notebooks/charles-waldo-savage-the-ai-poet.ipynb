{"cells":[{"metadata":{},"cell_type":"markdown","source":"# *Charles Waldo Savage* \n## Charles Waldo Savage is an American AI poet who consists of several Long-Short-Term Memory Neural Network Layers. Charles learned to write poems based off of a collection of 108 Robert Frost Poems."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating Curriculum for Charles\n\n## Charles does not actually undertand written language. He is a computer. So we need to transform 108 Robert Frost poems into a format which is more suitable for him to understand."},{"metadata":{},"cell_type":"markdown","source":"## Preview Data"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/robert-frost-collection/robert_frost_collection.csv')\ndf = df.iloc[1:]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing the data"},{"metadata":{},"cell_type":"markdown","source":"### Concatenate all poems into a single string"},{"metadata":{"trusted":true},"cell_type":"code","source":"content = list(df['Content'])\n\ntext=''\nfor c in content:\n    text+=c +'\\n'\nprint('First 37 characters:\\n')\nprint(text[:37])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Map Characters to integers and vice versa"},{"metadata":{"trusted":true},"cell_type":"code","source":"# All unique characters\nvocab = sorted(set(text))\n\n# Creating a mapping from unique characters to indices\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Example: First 37 characters mapped to their index position. This is the format that Charles will be reading."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('{} --> {}'.format(repr(text[:37]), text_as_int[:37]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How Charles will be learning\nThe way that Charles learns how to write is by learning to predict the next character given a sequence of characters So if we give Charles the input:<br>\n- `Whose woods these `<br>\n\nWe want charles to be able to output:\n- `are I think I know.`"},{"metadata":{},"cell_type":"markdown","source":"### Creating training examples and targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"seq_length = 50\nexamples_per_epoch = len(text)//(seq_length+1)\n\n# Create training examples / targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n    print(idx2char[i.numpy()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(10):\n    print(repr(''.join(idx2char[item.numpy()])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for input_example, target_example in  dataset.take(1):\n    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Creating Charles"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nLSTM_units = 1024","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(vocab_size, embedding_dim, LSTM_units, batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                                  batch_input_shape=[batch_size, None]),\n        tf.keras.layers.LSTM(LSTM_units,\n                            return_sequences=True,\n                            stateful=True,\n                            recurrent_initializer='glorot_uniform'),\n        \n        tf.keras.layers.LSTM(LSTM_units,\n                            return_sequences=True,\n                            stateful=True,\n                            recurrent_initializer='glorot_uniform'),\n        \n        tf.keras.layers.Dense(vocab_size)\n    ])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(\n    vocab_size=len(vocab),\n    embedding_dim=embedding_dim,\n    LSTM_units=LSTM_units,\n    batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss=loss)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Directory where the checkpoints will be saved\ncheckpoint_dir = './training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 40\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.train.latest_checkpoint(checkpoint_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(vocab_size, embedding_dim, LSTM_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_text(model, start_string):\n    # Evaluation step (generating text using the learned model)\n\n    # Number of characters to generate\n    num_generate = 250\n\n    # Converting our start string to numbers (vectorizing)\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    # Empty string to store our results\n    text_generated = []\n\n    # Low temperature results in more predictable text.\n    # Higher temperature results in more surprising text.\n    # Experiment to find the best setting.\n    temperature = 0.5\n\n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # Pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A few poems by Charles"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(generate_text(model, start_string=\"Where\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(generate_text(model, start_string=\"why\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(generate_text(model, start_string=\"The\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(generate_text(model, start_string=\"In\"))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}