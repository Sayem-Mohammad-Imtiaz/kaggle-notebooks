{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Preprocessing for **String** data\n\n01. BoW / Binary BoW\n02. n-Grams\n03. TF-IDF\n04. Word2Vec (SOTA) / Avg. Word2Vec / Weighted Word2Vec"},{"metadata":{},"cell_type":"markdown","source":"# **01. Bag of Words Representation**\n\n   > Separate words become different features ($feat_k$)\n   > \n   > $x_i^{feat_k} = \\text{count of k-th word in string }x_i \\,\\,\\,\\, (where\\, 0 < k < size_{vocabluary})$\n\nFor Binary BoW,\n   > $x_i^{feat_k} \\begin{cases}\n      0, & \\text{if count of k-th word in string 𝑥𝑖 }=0 \\\\\n      1, & \\text{if count of k-th word in string 𝑥𝑖 }>0\n    \\end{cases}$ \n\n***As we are simply counting number of $k^{th}$ word*** in BoW representation, it can be further **IMPROVED** by following techniques --\n   - **Stopword removal:** Remove useless frequently occuring words (*and, is, or, the* etc.) \n       > Semantic information is lost\n   - **Stemming:** Change similar words to their base words (\"do\" / \"done\" / \"doing\" $\\rightarrow$ \"do\" (base word))\n       > *snowball* stemmer powerful compared to *porter* stemmer (old technique)\n   - **Lemmatisation:** Same as stemming but,\n       > base word of stem might not be an actual word whereas, lemma is an actual language word.\n   \n### **Note**\n\n- $size_{vocabluary} \\propto sparsity_{x_i}$ (As vocabulary increases, sparsity if $x_i$ increases)\n- To know how similar / dissimilar two vectors are, take [Norm](https://www.kaggle.com/l0new0lf/l1-l2-ln-norm) of their **difference** *(Norm gives size of a vector)*\n- [L1 Norm](https://www.kaggle.com/l0new0lf/l1-l2-ln-norm) of Binary BoW is `= number of different words` \n\n### **Disadvantages**\n\n- **Compute:** In case of Logistic regression, `n_dims = n_vocab` $\\Rightarrow$ Large training and inference time\n- **Space:** Sparse vector\n- **Inherent:**\n    - Simple counts \n        - *Doesn't incorporate semantics*\n        - *Information is lost*\n        - *Sequence info ignored*\n    - Problem w/ *heteronyms* (`Lead` in pencil is not same as `Lead` in leader)\n    - Problem w/ synonyms (same meaning words w/ difft spelling will be made into different features instead of making same)"},{"metadata":{},"cell_type":"markdown","source":"# **02. n-Grams**\n\n> Reatains sequence information. *( n = number of neighbors)*\n\n**BoW** where separate **consecutive-groups-of-words** become different features ($feat_k$). <br>\nSize of group is given by `n` in n-Grams\n\n### **Note (Main Disadvantage)**\n\n- $size_{vocabluary} \\propto n \\propto sparsity_{x_i}\\,\\,\\,$ (As `n` increases, sparsity and vocabulary increases)\n- Extremely sparse vector\n\n### **Advantage over simple BoW**\n\n- Partial sequence info is retained\n    > eg. \"do not\" -> [\"do\", \"not\"] is not informative but bigram,  \"do not\" -> [\"do not\"] is!"},{"metadata":{},"cell_type":"markdown","source":"# **03. TF-IDF**\n\n> Based on two key ideas - \n> - Normalize BoW counts **in datasample** [by sum](https://www.kaggle.com/l0new0lf/02-08-normalisation-vs-standardisation-vs-probs) i.e to probabilities **(Property of datasample)**\n> - Penalize more frequent and reward less frequently words **(Property of dataset)**\n\n"},{"metadata":{},"cell_type":"markdown","source":"**TERM FREQUENCY (TF):** Converts simple counts to probabilities by dividing w/ sum of all words' counts **in datasample** <br>\n**INVERSE DOC. FREQ. (IDF):** Penalize more frequent and reward less frequently words **in dataset**\n\n<br><br>\n**$$\\text{TF-IDF(}x_i^{feat_k}) = \\text{TF} * \\text{IDF}$$**\n<br><br>\n\n$\\text{TF(}x_i^{feat_k}) = \\frac{\\text{count of k-th word in }x_i}{\\text{Sum of counts of all words in x_i} }$ where $x_i$ is a datasample (string or document)\n<br>\n<br>\n$\\text{IDF(}x_i^{feat_k}) = \\log{\\bigg( \\frac{n}{n_{\\text{with k-th word}}} \\bigg)}$ where **$n$** is total number of samples in dataset and **$n_{\\text{with k-th word}}$** is number of samples w/ **atleast** one k-th word.\n\n> Can even use method similar to [laplace smoothing]() in IDF equation \n\n### **NOTE**\n\n- **TF** Normalizes [by sum](https://www.kaggle.com/l0new0lf/02-08-normalisation-vs-standardisation-vs-probs) by **ROW**\n- As IDF is inverse, `IDF >>> TF`. Not good as IDF can nullify impact of TF in overall value. Hence, **use log(IDF)** to monotonically decrease it's value.\n- IDF is **small** `~ log(1)` for frequent words and **large** `~ log(num_of_samples)` for rare words\n    \n### **DISADVANTAGE**\n\n- Doesn't take semantic meaning into account\n- Sparse vector\n\n### **Ponder**\n\nWhat if **IDF** Inverse-Normalizes original counts [by sum](https://www.kaggle.com/l0new0lf/02-08-normalisation-vs-standardisation-vs-probs) by **COLUMN** i.e $\\log{ \\bigg( \\frac{N_X^{feat_k}}{N_{x_i}^{feat_k}} \\bigg)}$ Where, $N_X^{feat_k}$ is count of k-th word in whole dataset  $X$ and $N_{x_i}^{feat_k}$ is count of k-th word in sample $x_i$\n"},{"metadata":{},"cell_type":"markdown","source":"# **04. WORD2VEC**\n\n> [2013 Paper](https://arxiv.org/pdf/1301.3781.pdf) *Takes semantic meaning into consideration unlike any methods above.*\n>\n> - Converts a given word (text) to any d-sized dense vector.\n> $d \\propto \\text{information retained}$\n> - Takes neighbors of the word into consideration (while training)\n\nCan be understood w/ help of *Matrix Factorisation / Deep Learning*\n\n## **Advantages**\n\n- Dense vector! (unlike any methods above)\n- Incorporate semantic meaning (unlike any methods above)\n    - vector distance preserves similarity of different words\n    - vector direction preserves temporal information (tenses) and relationships b/w words\n    - if $\\vec{v_1}$ is related to $\\vec{v_2}$ and $\\vec{v_3}$ is related to $\\vec{v_4}$, $\\,\\,\\, (\\vec{v_1} - \\vec{v2} ) \\,\\, // \\,\\, (\\vec{v_3} - \\vec{v4} )$ Thus, relationship is maintained! \n    \n\n## **Disadvantages**\n\n- Black box\n- Need extremely large training set\n\n## **Sentence to Vector**\n\nEither of three methods are applicable\n\n- Use complex SOTA techniques like *Sentence2Vec* (Need retraining and large corpus of data)\n- Take avg. of element word vectors \n$$\\vec{\\text{sentence vector}} = \\frac{1}{N_{\\text{num of words}}} \\sum_{i=0}^{N_{\\text{num of words}}}{\\text{w2v}(word_i)}$$\n\n- Take TF-IDF weighted avg. element of word vectors\n\n$$\\vec{\\text{sentence vector}} = \\frac{1}{\\sum_{i=0}^{N_{\\text{num of words}}}{\\text{TF-IDF}_i}} \\sum_{i=0}^{N_{\\text{num of words}}}{\\text{TF-IDF}_i * \\text{w2v}(word_i)}$$\n\n> In both above cases, resulant sentence vector will be of **same dims** as of individual word vec"},{"metadata":{},"cell_type":"markdown","source":"# DATA"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\n\ndf = pd.read_csv(\"../input/twitter-sentiment-analysis-hatred-speech/train.csv\")\n\n# helper function to remove twitter handles\ndef remove_pattern(input_text, pattern):\n    r = re.findall(pattern, input_text) # retuns a list with substrings with 'pattern'\n    for i in r:\n        input_text = re.sub(i, \"\", input_text) # remove pattern \n    return input_text\n\n# to lower\ndf['to_lower'] = df['tweet'].apply(lambda x: x.lower())\n# find pattern for twitter handles using regex\n# @user\ndf['handle_removed'] = np.vectorize(remove_pattern)(df['to_lower'], \"@[\\w]*\")\n# 1. convert pandas series to string\n# 2. call replace method on string\n# 3. Use regex to replace everything except [a-z] and [A-Z] with space (\" \")\n# 4. use \"[^a-zA-Z#]\" to retain hash-symbol (not doing it here)\ndf['puncs_removed'] = df['handle_removed'].str.replace(\"[^a-zA-Z]\", \" \")\n\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to numpy array\ndf['puncs_removed'].values[:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df['puncs_removed'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> **Num of samples = n = 31962**"},{"metadata":{},"cell_type":"markdown","source":"# CODE"},{"metadata":{},"cell_type":"markdown","source":"# **01. BoW / Binary Bow / n-Grams**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\nbow = CountVectorizer(\n            stop_words     = 'english',\n            binary         = False, # True -> Binary BoW,\n            ngram_range    = (1,1), # (1, 1) -> only unigrams, (1, 2) -> unigrams and bigrams, and (2, 2) -> only bigrams\n            #vocabulary    = Mapping / iterable (custom vocabulary)\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use `fit_transform` with training data (seeing first time => generates vocabulary)\n# use `transform` with test data (not seeing first time => needs vocabulary - already genearated by `fit_transform`)\nfeatures = bow.fit_transform(df['puncs_removed'].values)\n\nprint(features.shape)\nprint(type(features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> - For `n = 31962` samples, `37255` dimension vector\n> - Vocabulary size is `37255`\n> - returns sparse matrix (saves memory)\n\nUse `features.todense()` or `features.toarray()` to convert to dense `np.ndarray`"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(dir(bow))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"vocabulary size is: \", len(bow.vocabulary_))\n\n# method 1:\n# out of 37255 \nprint(\"method 1:\")\nfor vocab_word, index in bow.vocabulary_.items():\n    if index == 0   : print(\"feature 0 repesents word: \", vocab_word)\n    if index == 100 : print(\"feature 100 repesents word: \", vocab_word)\n        \n# method 2: \nprint(\"method 2:\")\nprint(f\"feature 0 repesents word: {bow.get_feature_names()[0]}\")\nprint(f\"feature 100 repesents word: {bow.get_feature_names()[100]}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# `transform` instead of `fit_transform` for test-data\nbow.transform(['hi']) # (1x37255)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **02. BoW Improvement**\n\n- Tokenize (space separated list)\n- Stopword removal\n- Stemming / Lemmatisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer, SnowballStemmer\nfrom nltk.corpus import stopwords\n\n''' \n# Lemmatisation:\n# base word of stem might not be an actual word whereas, lemma is an actual language word\n\n>>> from nltk.stem import WordNetLemmatizer\n>>> wnl = WordNetLemmatizer()\n>>> print(wnl.lemmatize('dogs'))\ndog\n>>> print(wnl.lemmatize('churches'))\nchurch\n>>> print(wnl.lemmatize('aardwolves'))\naardwolf\n\nUSAGE: instead of stemming pd.series below, use\n`.apply(lambda x: [wnl.lemmatize(i) for i in x])`\n'''\n\n\nstopwords = set( stopwords.words('english') )\n\n# SnowballStemmer stemmer better\n#stemmer = PorterStemmer('english')\nstemmer = SnowballStemmer('english')\n\n# Tokenize before stemming. \n# Tokenize: Split into particular words i.e into list\ntokenized_tweet = df['puncs_removed'].apply(lambda x: x.split())\n\n# Stopword removal (in-place)\ntokenized_tweet = tokenized_tweet.apply(lambda tokens: [i if i not in stopwords else '' for i in tokens])\n\n# Stemming (can be replaced w/ Lemmatisation)\n# Iterate over every word in each list \n# So that `having` and `have` both can be converted into `have`\nstemmed_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n\n# convert list of words into a line\nfor i in range(len(stemmed_tweet)):\n    stemmed_tweet[i] = ' '.join(stemmed_tweet[i])\ndf[\"processed\"] = stemmed_tweet\n\n# display\ndf[['puncs_removed', 'processed']].head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **03. TF-IDF**\n\n> *Same usage as BoW above*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# use exactly same as BoW\n# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\ntfidf = TfidfVectorizer(\n            stop_words   = 'english',\n            ngram_range  = (1,2) # uni as well as bi\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use `fit_transform` with training data (seeing first time => generates vocabulary)\n# use `transform` with test data (not seeing first time => needs vocabulary - already genearated by `fit_transform`)\ntfidf_features = tfidf.fit_transform(df['processed'].values)\n\nprint(tfidf_features.shape)\nprint(type(tfidf_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_topn_tfidfs_of_a_sample(tfidf_features, sample_idx, n=25):\n    \"\"\"\n    tfidf_features  : np.ndarray of dims (num_samples, num_tfidf_feats)\n    sample_idx      : row_idx\n    \"\"\"\n    tfidfs_of_a_row = tfidf_features[sample_idx].toarray().flatten() # (1x31194) -> (31194,)\n    desc_idxs = np.argsort(tfidfs_of_a_row)[::-1][:n]\n    \n    top_n_tfidfs = tfidfs_of_a_row[desc_idxs]\n    top_n_tfidfs_featwords = np.array(tfidf.get_feature_names())[desc_idxs]\n    \n    return top_n_tfidfs_featwords, top_n_tfidfs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze top-n TF-IDFs of a >>data-sample 0 and 1<<\n# inspiration: http://buhrmann.github.io/tfidf-analysis\nbar_xs_0, bar_ys_0 = get_topn_tfidfs_of_a_sample(tfidf_features, 0, n=25)\nbar_xs_1, bar_ys_1 = get_topn_tfidfs_of_a_sample(tfidf_features, 1, n=25)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axarr = plt.subplots(1, 2)\nfig.set_size_inches(12,5)\n\n# sample at idx 0\nsns.barplot(bar_ys_0, bar_xs_0, ax=axarr[0])\naxarr[0].set_title('For sample at idx 0')\naxarr[0].grid()\n\n# sample at idx 1\nsns.barplot(bar_ys_1, bar_xs_1, ax=axarr[1])\naxarr[1].set_title('For sample at idx 1')\naxarr[1].grid()\n\nfig.tight_layout(pad=3.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **04. WORD2VEC**\n\nWith genism,\n\n- Can train custom model (w/ custom data)\n- Can use pretrained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"! curl -O \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec, KeyedVectors\npretrained_model = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make sure for pretrained model, input word **exists in google-news vocabulary**\n\n> Better used lemmatisation(base word exists in vocab) instead of stemming"},{"metadata":{"trusted":true},"cell_type":"code","source":"# word -> 300 dim vec (pretrained)\npretrained_model['test'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# similarity using norm of distance vector \n# output is normalized (between 0,1)\npretrained_model.similarity('king', 'queen')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_model.most_similar('king')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To accept words that **do not exist** in google-news vocabulary, train custom model\n\nInput is list of data samples (which is again list of individual words)\n```\n[\n    ['w1', 'w2', ..., 'wm'],\n    ['w1', 'w2', ..., 'wm'],\n    ['w1', 'w2', ..., 'wm'],\n    .\n    .\n    .\n]\n```\n\nWord2Vec parameters (custom model)\n- list of sentences (sentence is list of words) as shown above\n- **min_count:** min number of occurances of word required to create a unique vector for it\n- **size:** dim of output vector (larger the better)\n- **workers:*** cpu cores to use\n\n> Note: \n> - vocabulary size i.e data size must be large (to compensate for almost any random valid input at test time)\n> - Here, used **stemmed** text. **Better use raw punc_removed text**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = df['processed'].values # ndarray of stemmed\n\nlist_of_list_of_list_of_words = []\nfor sentence in sentences:\n    list_of_list_of_list_of_words.append(\n        sentence.split()\n    )\n    \nlist_of_list_of_list_of_words[:2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train custom model\ncustom_model = Word2Vec(\n                list_of_list_of_list_of_words,\n                min_count     = 1,\n                size          = 300,\n                workers       = 4\n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_model.wv['lyft'].shape # custom vocabulary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_model.wv.most_similar('lyft')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **04. Sentence to vectors**"},{"metadata":{},"cell_type":"markdown","source":"**A. Using Avg Word2vec** (see formula above)"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_sentences = df['processed'].values # ndarray of stemmed sentences\nlist_of_sentences[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\navg_w2v_sentences = []\nfor sentence in tqdm(list_of_sentences):\n    w2vs = []\n    for word in sentence.split():\n        w2vs.append(custom_model.wv[word])\n        \n    w2vs = np.array(w2vs)\n    avg_w2v_sentence_vec = np.sum(w2vs, axis=0) / len(sentence)\n    avg_w2v_sentences.append(avg_w2v_sentence_vec)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_w2v_sentences = avg_w2v_sentences\n\nprint(avg_w2v_sentences[0].shape)\nprint(len(avg_w2v_sentences))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**B. Using TF-IDF weighted w2v** (see formula above)\n\n> Just like Avg Word2vec above, use TF-IDF data and genism w2v to compute the vector representing sentence sentence"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}