{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classification of Text Messages"},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement"},{"metadata":{},"cell_type":"markdown","source":"The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam."},{"metadata":{},"cell_type":"markdown","source":"We will be using this dataset to build a machine learning model to classify if a message is ham (Legitimate) or spam. We will be using some NLP techniques to preprocess the data and use seaborn to do some visualization."},{"metadata":{},"cell_type":"markdown","source":"## Importing the Libraries and the Dataset"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nprint(os.listdir(\"../input\"))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"email_data = pd.read_csv('../input/spam.csv', encoding = 'latin-1' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"email_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop the columns with no values."},{"metadata":{"trusted":true},"cell_type":"code","source":"email_data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis  = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"email_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"email_data = email_data.rename(columns = {\"v1\": \"label\", \"v2\": \"text\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"email_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(email_data['label'], label = \"Count of the Labels\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the dataset is not balanced. There are more data that are classified as ham other than spam. "},{"metadata":{},"cell_type":"markdown","source":"Let's compute the length of the reviews to add as a new column."},{"metadata":{"trusted":true},"cell_type":"code","source":"email_data['length'] = email_data['text'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"email_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking a look at the distribution of the text lengths"},{"metadata":{"trusted":true},"cell_type":"code","source":"email_data['length'].hist(bins = 50, color = 'g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the histogram, we can see that most of the reviews are about have less than 50 words."},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing / Data Cleaning\n"},{"metadata":{},"cell_type":"markdown","source":"Before the data can be fed to a Machine Learning model,it is required that we clean the data first for the model to understand and process each review well."},{"metadata":{},"cell_type":"markdown","source":"We will define a function that will remove all the punctuations and all the common words."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's import the needed libraries\nimport string\nfrom nltk.corpus import stopwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_review(review):\n    remove_punctuation = [word for word in review if word not in string.punctuation]\n    join_characters = ''.join(remove_punctuation)\n    remove_stopwords = [word for word in join_characters.split() if word.lower() not in stopwords.words('english')]\n    cleaned_review = remove_stopwords\n    return cleaned_review","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see if the function that we define is working."},{"metadata":{"trusted":true},"cell_type":"code","source":"#The original data\nemail_data['text'][4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying the cleaning function\nclean_review(email_data['text'][4])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use the function that we created later. "},{"metadata":{},"cell_type":"markdown","source":"# Applying Count Vectorizer in the Dataset"},{"metadata":{},"cell_type":"markdown","source":"We will be applying count vectorizer to the dataset to convert the reviews into a matrix of token counts."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n#Define the clean_review function as an argument of the CountVectorizer.\ncount_vectorizer = CountVectorizer(analyzer = clean_review)\n#Fit the countvectorizer to the dataset\nemail_countvec = count_vectorizer.fit_transform(email_data['text'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Taking a look at the Feature names"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(count_vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here's the shape of the new Vectorized Data."},{"metadata":{"trusted":true},"cell_type":"code","source":"email_countvec.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From only 3 columns, we can see now that every word in the whole dataset have its own column."},{"metadata":{},"cell_type":"markdown","source":"## Training the Classification Model"},{"metadata":{},"cell_type":"markdown","source":"Before Initializing and Training the model. We will first split the data into training and testing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's have a quick look at the data once again\nemail_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we will be using the label column as our dependent variable. We need to encode it into binary variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using LabelEncoder from sklearn\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nemail_data['label'] = le.fit_transform(email_data['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assign X and y\nX = email_countvec\ny = email_data['label']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting the Data into Training and Testing"},{"metadata":{},"cell_type":"markdown","source":"We will be using 75% of the data for training and 25% for testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Training"},{"metadata":{},"cell_type":"markdown","source":"We will be using Naive Bayes Classifier as our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclassifier = MultinomialNB() #Using the default parameters\nclassifier.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluating the Model"},{"metadata":{},"cell_type":"markdown","source":"After training the Model we will proceed into evaluating its performance. Using confusion matrix and the classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making predictions to the Test Set\ny_predictions = classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\ncm = confusion_matrix(y_test, y_predictions)\n\n#Taking a look at the Confusion Matrix with a Heatmap\nsns.heatmap(cm, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The model actually produces a pretty good results with just using some of the basic techniques in NLP."},{"metadata":{},"cell_type":"markdown","source":"# Using TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfid = TfidfVectorizer()\ntfidvec = tfid.fit_transform(email_data['text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidvec.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfid.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfidvec[:,:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assigning X2 and y2\nX2 = tfidvec\ny2 = email_data['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size = 0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclassifier2 = MultinomialNB() #Using the default parameters\nclassifier2.fit(X_train2, y_train2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making predictions to the Test Set\ny_predictions2 = classifier2.predict(X_test2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\ncm2 = confusion_matrix(y_test2, y_predictions2)\n\n#Taking a look at the Confusion Matrix with a Heatmap\nsns.heatmap(cm2, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test2, y_predictions2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We can see that by using Term Frequency - Inverse Document Frequency, we can improve the performance of our model."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}