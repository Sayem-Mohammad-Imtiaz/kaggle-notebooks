{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* > # EE226 - Coding 2\n## Streaming algorithm & Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"### Streaming: DGIM","metadata":{}},{"cell_type":"markdown","source":"DGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the *stream_data.txt* (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stream=np.loadtxt('../input/coding2/stream_data.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"1. Set the window size to 1000, and count the number of 1-bits in the current window.","metadata":{}},{"cell_type":"code","source":"class DGIM():\n    def __init__(self,window_size):\n        self.bucket_list = {}\n        self.window_size=window_size\n        self.current_time=0\n        self.bucket=np.array([])\n    def sort_buck_list(self):\n        #整理bucket的函数，负责合并bucket\n        nowkey=0\n        while nowkey in self.bucket_list:\n            if len(self.bucket_list[nowkey])>2:\n                timestamp=self.bucket_list[nowkey][1] #使用结尾时间作为一个bucket的时间戳\n                \n                self.bucket_list[nowkey]=self.bucket_list[nowkey][2:]\n                \n                if not nowkey+1 in self.bucket_list:\n                    self.bucket_list[nowkey+1]=np.array([timestamp])\n                else:\n                    self.bucket_list[nowkey+1]=np.append(self.bucket_list[nowkey+1],timestamp)\n            nowkey=nowkey+1\n    def filt_buck_list(self):\n        #筛选bucket的函数,负责去掉window外的bucket\n        for key in self.bucket_list:\n            \n            self.bucket_list[key]=self.bucket_list[key][self.bucket_list[key]>=0]\n        for key,value in self.bucket_list.items():\n            #print(key,value)\n            for i in range(len(value)):\n                self.bucket= np.append(self.bucket,2**key)\n        if self.current_time>self.window_size:\n            self.bucket[-2]=self.bucket[-2]/2 #最后一个bucket除2\n        \n    def stream_input(self,bit):\n        #读取新数据的函数\n        if self.current_time<self.window_size:\n            if bit==1: \n                if not 0 in self.bucket_list:\n                    self.bucket_list[0]=np.array([self.current_time])\n                else:\n                    self.bucket_list[0]=np.append(self.bucket_list[0],self.current_time)\n                \n               \n                \n        else:\n            for keys in self.bucket_list:\n                self.bucket_list[keys]=self.bucket_list[keys]-1\n            if bit==1:\n                if not 0 in self.bucket_list:\n                    self.bucket_list[0]=np.array([self.window_size-1])\n                else:\n                    self.bucket_list[0]=np.append(self.bucket_list[0],self.window_size-1)\n                \n        self.sort_buck_list()\n       \n        self.current_time=self.current_time+1    \n               \n        \n        \n\n\n\n    def query(self):\n        \n        return self.bucket","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Write a function that accurately counts the number of 1-bits in the current window, and compare the difference between its running time and space and the DGIM algorithm.","metadata":{}},{"cell_type":"code","source":"\ntimestamp=0 #设置初始时间戳\nwindowlength=1000 #设置窗长\ndgim=DGIM(windowlength)\nfor i,bit in enumerate(stream[0:timestamp+windowlength]):\n    dgim.stream_input(bit)\n\n\ndgim.filt_buck_list()\n\n\nimport time\ntime_start=time.time()\nprint(\"dgim:\",np.sum(dgim.query()))\n\ntime_end=time.time()\nprint('totally cost dgim',time_end-time_start)\nprint('totally space dgim',len(dgim.query()))\ntime_start=time.time()\nvsum=0\nfor v in range(windowlength):\n    vsum=vsum+stream[timestamp+v]\nprint(\"sum:\",vsum)\n\ntime_end=time.time()\nprint('totally cost sum',time_end-time_start)\nprint('totally space sum',windowlength)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Locality Sensitive Hashing","metadata":{}},{"cell_type":"markdown","source":"The locality sensitive hashing (LSH) algorithm is efficient in near-duplicate document detection. In this coding, you're given the *docs_for_lsh.csv*, where the documents are processed into set of k-shingles (k = 8, 9, 10). *docs_for_lsh.csv* contains 201 columns, where column 'doc_id' represents the unique id of each document, and from column '0' to column '199', each column represents a unique shingle. If a document contains a shingle ordered with **i**, then the corresponding row will have value 1 in column **'i'**, otherwise it's 0. You need to implement the LSH algorithm and ask the problems below.","metadata":{}},{"cell_type":"markdown","source":"### Your task","metadata":{}},{"cell_type":"markdown","source":"Use minhash algoirthm to create signature of each document, and find 'the most similar' documents under Jaccard similarity. \nParameters you need to determine:\n1) Length of signature (number of distinct minhash functions) *n*. Recommanded value: n > 20.\n\n2) Number of bands that divide the signature matrix *b*. Recommanded value: b > n // 10.","metadata":{}},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = (np.loadtxt(\"../input/coding2/docs_for_lsh.csv\",delimiter=',',skiprows=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#数据预处理，去掉第一行，并转置\ndata1=data[:,1:]\ndata1.shape\ndata1=data1.T\ndata1.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef signature(data,nums):\n    #建立signature\n    h,w=data.shape\n    \n    #print(rand_mat.shape)\n    signature =np.zeros((nums,w))\n    weight=np.arange(h,0,-1)\n    weight=weight[:,np.newaxis]\n    for i in tqdm(range(nums)):\n        #随机打乱\n        permute=np.random.permutation(h)\n        rand_mat=data[permute,:]\n        rand_mat=weight*rand_mat\n        \n        pos=np.argmax(rand_mat,axis=0)\n        signature[i,:]=pos\n        \n        \n        \n    return signature\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#建立长度为80的signature\na=signature(data1[:,:],80)\nprint(a.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef lsh(data,bucketsize):\n    #哈希函数，将bucket中的数据，先乘以2的k次方的权重，之后相加，最后除以500取余数\n\n    all_table=[]\n    num_feat,num_file=data.shape\n    num_buck=int(num_feat/bucketsize)\n    powers_of_two = 1 << np.arange(bucketsize - 1, -1, -1)\n    for i in range(0,num_feat,bucketsize):\n        table={}\n        now_row=data[i:i+bucketsize,:]\n        \n        index = (powers_of_two.dot(now_row))%400\n        for data_idx,idx in enumerate(index):\n            if idx not in table:\n                # If no list yet exists for this bin, assign the bin an empty list.\n                table[idx] = []\n            table[idx].append(data_idx)\n        all_table.append(table)\n    return all_table","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#得到所有bucket,bucket size=5 一共16个bucket\nc=lsh(a,5)\nprint(a.shape)\nprint(len(c))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Problem: For document 0 (the one with id '0'), list the **30** most similar document ids (except document 0 itself). You can valid your results with the [sklearn.metrics.jaccard_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html) function.\n\nTips: You can adjust your parameters to hash the documents with similarity *s > 0.8* into the same bucket.","metadata":{}},{"cell_type":"code","source":"sim_score=np.zeros(a.shape[1])\n#print(sim_score[0])\nfor table in c:\n    for score,pos in table.items():\n        #print(score,pos)\n        if 0 in pos:\n            for p in pos:\n                sim_score[p]=sim_score[p]+1\n\nind=np.argsort(sim_score[:])\n\ne=ind[-31:-1]\n\ntop_30=e[::-1]\nsearch_data=data1[:,0]\nprint(\"index fo top 30 similar document:\",top_30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import jaccard_score\nprint('top 30 most similar documents:')\nprint(\"index  similarity\")\nfor idx in top_30:\n    print(idx,jaccard_score(search_data,data1[:,idx]))\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}