{"cells":[{"metadata":{"_uuid":"4e19c4ffea62e93f6f83a823bc838480875c118e","_cell_guid":"aef26a60-77e5-4139-a66c-ddf63495424d"},"cell_type":"markdown","source":"**Predicting Breast Cancer From Nuclear Shape**\n\n**The nucleus is an organelle present within all eukaryotic cells, including human cells.  Abberant nuclear shape can be used to identify cancer cells (e.g. pap smear tests and the diagnosis of cervical cancer).  Likewise, a growing body of literature suggests that there is some connection between the shape of the nucleus and human disease states such as cancer and aging. As such, the quantitative analysis of nuclear size and shape has important biomedical applications.**\n\nTechnicians can use a microscope to observe tissue samples that were taken from patients who are suspected to have breast cancer.  By looking at the size and shape of the nuclei present within these tissue samples, doctors can determine whether a given sample appears to be benign (\"B\") or malignant (\"M\").  It would be hepful to have an automated method that can quickly determine if a sample is benign or malignant.  Here in this document I demonstrate a methodology to predict if a sample is benign or malignant given measurements of nuclear shape that were made from digital images of fine needle aspirates of breast tissue masses from clinical samples.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f94358dc3c26b090bc4ceea7dd9b64318fdcb53b","_cell_guid":"0a6edab8-c4fb-46ec-8656-a9746004a22e","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nPNG_location = \"/kaggle/input/webster-2009/webster 2009.png\"\ndef plotPNG(a):\n    \"\"\"\n    Plot a PNG Image w/ Matplotlib\n    \"\"\"\n    PNG = cv2.imread(PNG_location)\n    PNG = cv2.resize(PNG, (512,256))\n    plt.imshow(cv2.cvtColor(PNG, cv2.COLOR_BGR2RGB)); plt.axis('off')\n    return\nplotPNG(PNG_location)\nprint(\"Webster, M., Witkin, K.L., and Cohen-Fix, O. (2009). Sizing up the nucleus: nuclear shape, size and nuclear-envelope assembly. J. Cell Sci. 122, 1477â€“1486.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7769563fa470af1fd653c00da47d774b1b56d028","_cell_guid":"fe25413f-ef68-4d73-b9fc-c62d02312254"},"cell_type":"markdown","source":"*Step 1: Import Modules*","outputs":[],"execution_count":null},{"metadata":{"_uuid":"89c36db584a806eadbc5c847f12399eb14e08495","_cell_guid":"9e81f190-5b54-45af-8697-6e6dd5b62e54","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#from __future__ import print_function\n#import os\nimport itertools\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix, make_scorer, accuracy_score \nfrom sklearn.model_selection import GridSearchCV, learning_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neural_network import MLPClassifier as MLPC\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6311f84d09c51ede7cf5ef22646086f44e61e1b9","_cell_guid":"8366fc9c-6bc4-4cc5-a812-872a320e3da5"},"cell_type":"markdown","source":"*Step 2: Explore Data - See Column Titles, Raw Data, and Null Counts*","outputs":[],"execution_count":null},{"metadata":{"_uuid":"6674578ac8f8f173c38c950580b7c76b1fca0e9e","_cell_guid":"1bc90c89-2cab-4ebf-8310-b1e0e8d5c49b","trusted":true},"cell_type":"code","source":"sizeMeasurements = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\ndef describeData(a):\n    \"\"\" \n    Print column titles, first few values, and null value counts\n    \"\"\"  \n    print('\\n Column Values: \\n\\n', a.columns.values, \"\\n\")\n    print('\\n First Few Values: \\n\\n', a.head(), \"\\n\")\n    print('\\n Null Value Counts: \\n\\n', a.isnull().sum(), \"\\n\")\ndescribeData(sizeMeasurements)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0e99c549058dee11cc7e23633d16ed7fbb60ed1","_cell_guid":"07fa67ac-bb28-4fc0-a7aa-cc0ef594e06f"},"cell_type":"markdown","source":"*Step 3: Plot Data - Compare the size and shape of the nuclei from malignant samples to the size and shape of the nuclei from benign samples.*","outputs":[],"execution_count":null},{"metadata":{"_uuid":"cf6bfb5e031da4c5eba40b97d2d8a4cfb4527402","_cell_guid":"863ccf30-2cd7-477d-b544-1097dad1ab9a","scrolled":true,"trusted":true},"cell_type":"code","source":"def plotSizeDistribution(a):\n    \"\"\" \n    Plot size distribution for benign vs malignant samples\n    \"\"\"  \n    sns.set_style(\"whitegrid\")\n    distributionOne = sns.FacetGrid(a, hue=\"diagnosis\",aspect=2.5)\n    distributionOne.map(plt.hist, 'area_mean', bins=30)\n    distributionOne.add_legend()\n    distributionOne.set_axis_labels('area_mean', 'Count')\n    distributionOne.fig.suptitle('Area vs Diagnosis (Blue = Malignant; Orange = Benign)')\n    distributionTwo = sns.FacetGrid(a, hue=\"diagnosis\",aspect=2.5)\n    distributionTwo.map(sns.kdeplot,'area_mean',shade=True)\n    distributionTwo.set(xlim=(0, a['area_mean'].max()))\n    distributionTwo.add_legend()\n    distributionTwo.set_axis_labels('area_mean', 'Proportion')\n    distributionTwo.fig.suptitle('Area vs Diagnosis (Blue = Malignant; Orange = Benign)')\nplotSizeDistribution(sizeMeasurements)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"128acd27a96e16c79ea0fca851de4aa06e5f239a","_cell_guid":"41f6b2dc-d09a-4100-8abc-cf016139cf8e"},"cell_type":"markdown","source":"This confirms my prediction that healthy nuclei have a default size and that cancer cells have a wide range of sizes, typically greater than the default size.\n\nIn addition to being larger than healthy cells, cancer cells are often mishapen.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"f4e5377b0eb4f41f8fa79068e59df1975f7aa6e9","_cell_guid":"b9a3f637-e95a-4d64-b8f4-b224ec65bb8e","trusted":true},"cell_type":"code","source":"def plotConcaveDistribution(a):\n    \"\"\" \n    Plot shape distribution for benign vs malignant samples\n    \"\"\"  \n    sns.set_style(\"whitegrid\")\n    distributionOne = sns.FacetGrid(a, hue=\"diagnosis\",aspect=2.5)\n    distributionOne.map(plt.hist, 'concave points_mean', bins=30)\n    distributionOne.add_legend()\n    distributionOne.set_axis_labels('concave points_mean', 'Count')\n    distributionOne.fig.suptitle('# of Concave Points vs Diagnosis (Blue = Malignant; Orange = Benign)')\n    distributionTwo = sns.FacetGrid(a, hue=\"diagnosis\",aspect=2.5)\n    distributionTwo.map(sns.kdeplot,'concave points_mean',shade= True)\n    distributionTwo.set(xlim=(0, a['concave points_mean'].max()))\n    distributionTwo.add_legend()\n    distributionTwo.set_axis_labels('concave points_mean', 'Proportion')\n    distributionTwo.fig.suptitle('# of Concave Points vs Diagnosis (Blue = Malignant; Orange = Benign)')\nplotConcaveDistribution(sizeMeasurements)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b7b25daa39464882a1144c5b5c4a388436d5539","_cell_guid":"edc488eb-1eae-49fe-9fa5-27d0d0a25ab5"},"cell_type":"markdown","source":"This confirms my prediction that healthy nuclei are typically circular/elliptical and that cancer cells are mishapen and have lots of concave points.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e802d01f927ba9b75abfb5afbaa105148eed6206","_cell_guid":"8055e015-1481-4c84-b457-6580a386562e"},"cell_type":"markdown","source":"*Step 4: Preprocess Data*","outputs":[],"execution_count":null},{"metadata":{"_uuid":"52cdfe639045c741b17474d9ad7c2bb2585d2186","_cell_guid":"6157422b-be3f-4dbd-9ef2-94908c34e065"},"cell_type":"markdown","source":"Next I will pre-process the data so that it is ready for analysis.  First I will convert the labels \"B\" and \"M\" to 0 and 1, respectively.\nNext I will scale the values by using the sklearn.preprocessing.scale function which functions by subtracting the mean and then dividing by the standard deviation in order to generate values that are centered around zero.  You can learn more about the sklearn.preprocessing.scale function at the following link: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html \n\nBy doing this we will have data with numerical Y values and X values that are centered around zero.  This type of data is compatible with a wide variety of different classification algorithms.\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"13ecacebd473a2db67eb44351b6174bbb705d7ff","_cell_guid":"5b3bd1c6-72bb-4603-9530-eb4b5eca023a","collapsed":true,"trusted":true},"cell_type":"code","source":"def diagnosisToBinary(a):\n    \"\"\" \n    convert diagnosis to binary label\n    \"\"\" \n    a[\"diagnosis\"] = a[\"diagnosis\"].astype(\"category\")\n    a[\"diagnosis\"].cat.categories = [0,1]\n    a[\"diagnosis\"] = a[\"diagnosis\"].astype(\"int\")\ndiagnosisToBinary(sizeMeasurements)\n\nxValues = sizeMeasurements.drop(['diagnosis', 'Unnamed: 32', 'id'], axis=1)\nyValues = sizeMeasurements['diagnosis']\nxValuesScaled = preprocessing.scale(xValues)\nxValuesScaled = pd.DataFrame(xValuesScaled, columns = xValues.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dde7e0735025c31458e1dfb3ecfaa96c2beb16d0","_cell_guid":"45899938-cae9-448e-9018-886f3d34140b"},"cell_type":"markdown","source":"Another data preprocessing step that we can do is Principal Component Analysis (PCA).  With PCA we transform our features to make them less correlated  via a process that involves dimensionality reduction.  For more information, see the following documentaion: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html, http://scikit-learn.org/stable/modules/decomposition.html#pca\n\nFurthermore, we will also need to split up our training data, setting aside 20% of the training data for testing, such that we can avoid potentially overfitting the data.  The train_test_split function accomplishes this, as described in the following documentation:\nhttp://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nhttp://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"d67a1deb609debaf67e57204c3b90381f5c8986f","_cell_guid":"824c74f5-c746-4d8c-9387-40c83d4bc5bb","collapsed":true,"trusted":true},"cell_type":"code","source":"variance_pct = .99 # Minimum percentage of variance we want to be described by the resulting transformed components\npca = PCA(n_components=variance_pct) # Create PCA object\nX_transformed = pca.fit_transform(xValuesScaled,yValues) # Transform the initial features\nxValuesScaledPCA = pd.DataFrame(X_transformed) # Create a data frame from the PCA'd data\n\nX_trainOriginal, X_testOriginal, Y_trainOriginal, Y_testOriginal = train_test_split(xValues, yValues, test_size=0.2)\nX_trainScaled, X_testScaled, Y_trainScaled, Y_testScaled = train_test_split(xValuesScaled, yValues, test_size=0.2)\nX_trainScaledPCA, X_testScaledPCA, Y_trainScaledPCA, Y_testScaledPCA = train_test_split(xValuesScaledPCA, yValues, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d492e6b3725123d968c2efde828edd627e34f3e6","_cell_guid":"0c56a06f-9891-4e6f-88b8-dfff3e441246","trusted":true},"cell_type":"code","source":"print(\"\\nFeature Correlation Before PCA:\\n\")\ng = sns.heatmap(X_trainOriginal.corr(),cmap=\"BrBG\",annot=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18cfb04623690da74a806687afc0683e559cffe2","_cell_guid":"44485015-2ea5-45f0-a1b1-8ecae2eb89c0","trusted":true},"cell_type":"code","source":"print(\"\\nFeature Correlation After PCA:\\n\")\ni = sns.heatmap(X_trainScaledPCA.corr(),cmap=\"BrBG\",annot=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d9f01940e3e135521a1fe7bb7a58cd49756859b","_cell_guid":"f0804378-c153-489c-856a-2ff3330e2334","trusted":true},"cell_type":"code","source":"print('\\n First Few Values, Original: \\n\\n', xValues.head(), \"\\n\\n\")\nprint('First Few Values, Scaled: \\n\\n,',xValuesScaled.head(),'\\n\\n')\nprint('First Few Values, After PCA: \\n\\n,',xValuesScaledPCA.head(),'\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00cec6c1410f217f035d2e518cf0f78d500aa8b3","_cell_guid":"07b43349-b2ae-44d3-85c4-d97b3132f73b"},"cell_type":"markdown","source":"*Step 5: Define Helper Functions - for Plotting Learning Curve and Confusion Matrix*","outputs":[],"execution_count":null},{"metadata":{"_uuid":"2037fa549d2ccbc0c93f75c9d1340a3db81ab3ce","_cell_guid":"512ac0dc-850d-46f5-8c51-9baf396b77c1","collapsed":true,"trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Plots a learning curve. http://scikit-learn.org/stable/modules/learning_curve.html\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    return plt\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ndict_characters = {0: 'Malignant', 1: 'Benign'}\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a97fe15de551293ce029dcc8fbc709fd3dc9a3b","_cell_guid":"6daaff78-8d9c-4d81-a61f-65d07f99dbf6"},"cell_type":"markdown","source":"*Step 6: Evaluate Classification Models*","outputs":[],"execution_count":null},{"metadata":{"_uuid":"91a96b300c5e56715a9c120355ab5146f0d3f970","_cell_guid":"71be6011-3f90-4415-a1d3-f3e37161f5be"},"cell_type":"markdown","source":"Now I will try running a few different classification algorithms such as Logistic Regression and Support Vector Machines.  I will do this both with and without the data scaling step in order to illustrate the importance of this preprocessing step.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"74123e13e3184d019269bf82fd56ea8357641259","_cell_guid":"62a3ded3-04fd-461e-a767-d07bc5f02a9c","trusted":true},"cell_type":"code","source":"def compareABunchOfDifferentModelsAccuracy(a, b, c, d):\n    \"\"\"\n    compare performance of classifiers on X_train, X_test, Y_train, Y_test\n    http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\n    http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score\n    \"\"\"    \n    print('\\nCompare Multiple Classifiers: \\n')\n    print('K-Fold Cross-Validation Accuracy: \\n')\n    names = []\n    models = []\n    resultsAccuracy = []\n    models.append(('LR', LogisticRegression()))\n    models.append(('RF', RandomForestClassifier()))\n    models.append(('KNN', KNeighborsClassifier()))\n    models.append(('SVM', SVC()))\n    models.append(('LSVM', LinearSVC()))\n    models.append(('GNB', GaussianNB()))\n    models.append(('DTC', DecisionTreeClassifier()))\n    models.append(('GBC', GradientBoostingClassifier()))\n    #models.append(('LDA', LinearDiscriminantAnalysis()))\n    for name, model in models:\n        model.fit(a, b)\n        kfold = model_selection.KFold(n_splits=10, random_state=7)\n        accuracy_results = model_selection.cross_val_score(model, a,b, cv=kfold, scoring='accuracy')\n        resultsAccuracy.append(accuracy_results)\n        names.append(name)\n        accuracyMessage = \"%s: %f (%f)\" % (name, accuracy_results.mean(), accuracy_results.std())\n        print(accuracyMessage) \n    # Boxplot\n    fig = plt.figure()\n    fig.suptitle('Algorithm Comparison: Accuracy')\n    ax = fig.add_subplot(111)\n    plt.boxplot(resultsAccuracy)\n    ax.set_xticklabels(names)\n    ax.set_ylabel('Cross-Validation: Accuracy Score')\n    plt.show()\n\nprint(\"Before Data Scaling:\")\ncompareABunchOfDifferentModelsAccuracy(X_trainOriginal, Y_trainOriginal, X_testOriginal, Y_testOriginal)\nprint(\"After Data Scaling:\")\ncompareABunchOfDifferentModelsAccuracy(X_trainScaled, Y_trainScaled, X_testScaled, Y_testScaled)\nprint(\"After PCA:\")\ncompareABunchOfDifferentModelsAccuracy(X_trainScaledPCA, Y_trainScaledPCA, X_testScaledPCA, Y_testScaledPCA)\n\ndef defineModels():\n    \"\"\"\n    This function just defines each abbreviation used in the previous function (e.g. LR = Logistic Regression)\n    \"\"\"\n    print('\\nLR = LogisticRegression')\n    print('RF = RandomForestClassifier')\n    print('KNN = KNeighborsClassifier')\n    print('SVM = Support Vector Machine SVC')\n    print('LSVM = LinearSVC')\n    print('GNB = GaussianNB')\n    print('DTC = DecisionTreeClassifier')\n    print('GBC = GradientBoostingClassifier \\n\\n')\n    #print('LDA = LinearDiscriminantAnalysis')\ndefineModels()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56c193f8435c0b97ff200f804b0b4d8fd5d76ca4","_cell_guid":"a462943d-32ce-42f1-afd4-00d95c7186f3"},"cell_type":"markdown","source":"In order to choose between these classification algorithms, I will plot learning curves where I illustrate the relationship between the accuracy score and the cross validation score for increasing sample sizes.  I will do this to make sure that we are not overfitting the training data.  We want a learning curve where the vali score gets close to converging with the training score but does not quite converge. ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"e236055c6ce999823ecd8a2c7c32284ebdf34900","_cell_guid":"0f9485d8-3e0d-4037-b98a-a591b8843657","trusted":true},"cell_type":"code","source":"def plotLotsOfLearningCurves(a,b):\n    \"\"\"Now let's plot a bunch of learning curves\n    # http://scikit-learn.org/stable/modules/learning_curve.html\n    \"\"\"\n    models = []\n    models.append(('LR', LogisticRegression()))\n    models.append(('RF', RandomForestClassifier()))\n    models.append(('KNN', KNeighborsClassifier()))\n    models.append(('SVM', SVC()))\n    models.append(('LSVM', LinearSVC()))\n    models.append(('GNB', GaussianNB()))\n    models.append(('DTC', DecisionTreeClassifier()))\n    models.append(('GBC', GradientBoostingClassifier()))\n    #models.append(('LDA', LinearDiscriminantAnalysis()))\n    #models.append(('MLP', MLPC()))\n    for name, model in models:\n        plot_learning_curve(model, 'Learning Curve For %s Classifier'% (name), a,b, (0.8,1), 10)\nplotLotsOfLearningCurves(X_trainScaledPCA, Y_trainScaledPCA)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adb384eceed827627a9de257486860136ddccfbd","_cell_guid":"caf51f4d-7780-466a-90ec-b41a8e8d0af5"},"cell_type":"markdown","source":"The learning curve for Support Vector Machine looks pretty good.  Let's explore the Support Vector Machine (SVM) approach in a little more detail.  Next I will plot confusion plots for SVM where the Y-Axis represents the *True* labels (\"Malignant\" or \"Benign\") while the X-Axis represents the *Predicted* labels (generated by the Support Vector Machine). ","outputs":[],"execution_count":null},{"metadata":{"_uuid":"1ab1a5bbf806623ab77e01378bf153177e668be8","_cell_guid":"acb007f0-034e-4cd5-991a-3822976f1a34","trusted":true},"cell_type":"code","source":"def selectParametersForSVM(a, b, c, d):\n    model = SVC()\n    parameters = {'C': [0.01, 0.1, 0.5, 1.0, 5.0, 10, 25, 50, 100],\n                  'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n    accuracy_scorer = make_scorer(accuracy_score)\n    grid_obj = GridSearchCV(model, parameters, scoring=accuracy_scorer)\n    grid_obj = grid_obj.fit(a, b)\n    model = grid_obj.best_estimator_\n    model.fit(a, b)\n    print('Selected Parameters for SVM:\\n')\n    print(model,\"\\n\")\n    kfold = model_selection.KFold(n_splits=10, random_state=7)\n    accuracy = model_selection.cross_val_score(model, a,b, cv=kfold, scoring='accuracy')\n    mean = accuracy.mean() \n    stdev = accuracy.std()\n    print('Support Vector Machine - Training set accuracy: %s (%s)' % (mean, stdev))\n    print('')\n    prediction = model.predict(c)\n    cnf_matrix = confusion_matrix(d, prediction)\n    np.set_printoptions(precision=2)\n    class_names = dict_characters \n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names,title='Confusion matrix')\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n    plot_learning_curve(model, 'Learning Curve For SVM Classifier', X_trainScaledPCA, Y_trainScaledPCA, (0.85,1), 10)\nprint(\"\\nAfter Data Scaling:\\n\")\nselectParametersForSVM(X_trainScaled, Y_trainScaled,  X_testScaled, Y_testScaled)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"420520585d478258d9e7c23af0eea5ba7b69ecf0","_cell_guid":"ab3ff503-467c-4258-891d-91a113cdb958","trusted":true},"cell_type":"code","source":"print(\"\\nAfter PCA:\\n\")\nselectParametersForSVM(X_trainScaledPCA, Y_trainScaledPCA,  X_testScaledPCA, Y_testScaledPCA)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5fdda5583117faf70e98be225d023c49735314a","_cell_guid":"e32de210-b72f-4664-88d1-c642dc4b0171"},"cell_type":"markdown","source":"Next I will try using neural networks in order to make these same predictions.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"5be00b75c42a25f5e905b753fcb43dcc4e0ed834","_cell_guid":"9a8dcbd1-2813-42f1-b0fc-0282d595f463","trusted":true},"cell_type":"code","source":"def selectParametersForMLPC(a, b, c, d):\n    \"\"\"http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n    http://scikit-learn.org/stable/modules/grid_search.html#grid-search\"\"\"\n    model = MLPC()\n    parameters = {'verbose': [False],\n                  'activation': ['logistic', 'relu'],\n                  'max_iter': [1000, 2000], 'learning_rate': ['constant', 'adaptive']}\n    accuracy_scorer = make_scorer(accuracy_score)\n    grid_obj = GridSearchCV(model, parameters, scoring=accuracy_scorer)\n    grid_obj = grid_obj.fit(a, b)\n    model = grid_obj.best_estimator_\n    model.fit(a, b)\n    print('Selected Parameters for Multi-Layer Perceptron NN:\\n')\n    print(model)\n    print('')\n    kfold = model_selection.KFold(n_splits=10)\n    accuracy = model_selection.cross_val_score(model, a,b, cv=kfold, scoring='accuracy')\n    mean = accuracy.mean() \n    stdev = accuracy.std()\n    print('SKlearn Multi-Layer Perceptron - Training set accuracy: %s (%s)' % (mean, stdev))\n    print('')\n    prediction = model.predict(c)\n    cnf_matrix = confusion_matrix(d, prediction)\n    np.set_printoptions(precision=2)\n    class_names = dict_characters \n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names,title='Confusion matrix')\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n    plot_learning_curve(model, 'Learning Curve For MLPC Classifier', a, b, (0.85,1), 10)\nprint(\"Before Data Scaling:\\n\")\nselectParametersForMLPC(X_trainOriginal, Y_trainOriginal,  X_testOriginal, Y_testOriginal)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3665b45854687268ba32b6f04d7849525e95ed94","_cell_guid":"b606b9b6-6b77-4697-b88c-ca94fc092e19","trusted":true},"cell_type":"code","source":"print(\"After Data Scaling:\\n\")\nselectParametersForMLPC(X_trainScaled, Y_trainScaled,  X_testScaled, Y_testScaled)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b80a534c9945bdfa0bbb39f8bb216095a34bfbb","_cell_guid":"a182ba37-cbd0-4655-88d9-5038162740f1","trusted":true},"cell_type":"code","source":"print(\"After PCA:\\n\")\nselectParametersForMLPC(X_trainScaledPCA, Y_trainScaledPCA,  X_testScaledPCA, Y_testScaledPCA)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7561beb7d917a6eccf11d5861f28aea8cf07a256","_cell_guid":"e4cf950d-89ad-42d9-8a9c-315548bb598a"},"cell_type":"markdown","source":"Next I will try building a custom neural network using the Keras library.  https://keras.io/models/sequential/\nThis is just to compare what we would do in Keras to what we would do in Sklearn,","outputs":[],"execution_count":null},{"metadata":{"_uuid":"35dbe611fdace5f15380b596cd68b5fde9fa3fc7","_cell_guid":"64c9f71f-0eeb-47ab-a276-5fcf57c86876","trusted":true},"cell_type":"code","source":"def runSimpleKeras(a,b,c,d):\n    \"\"\" Build and run Two different NNs using Keras\"\"\"\n    #global kerasModelOne # eventually I should get rid of these global variables and use classes instead.  in this case i need these variables for the submission function.\n    # kerasModelOne: simple network consisting of only two fully connected layers.\n    Adagrad(lr=0.00001, epsilon=1e-08, decay=0.0)\n    model = Sequential()\n    model.add(Dense(input_dim=np.array(a).shape[1], units=128, kernel_initializer='normal', bias_initializer='zeros'))\n    model.add(Activation('relu'))\n    model.add(Dense(units=1))\n    model.add(Activation('sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='Adagrad', metrics=['accuracy'])\n    model.fit(np.array(a), np.array(b), epochs=10, verbose=2, validation_split=0.2)\n    score = model.evaluate(np.array(c),np.array(d), verbose=0)\n    print('\\nLoss, Accuracy:\\n', score)\n    #kerasModelOne = model  \n    #return kerasModelOne\nprint(\"Before Data Scaling:\\n\")\nrunSimpleKeras(X_trainOriginal,Y_trainOriginal,X_testOriginal,Y_testOriginal)\nprint(\"After Data Scaling:\\n\")\nrunSimpleKeras(X_trainScaled,Y_trainScaled,X_testScaled,Y_testScaled)\nprint(\"After PCA:\\n\")\nrunSimpleKeras(X_trainScaledPCA,Y_trainScaledPCA,X_testScaledPCA,Y_testScaledPCA)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e43b7cb188b0dcbfa7c5e1557b54ae1e3e011357","_cell_guid":"9b5ada0f-6508-488b-85e7-d25f3f4ccf11"},"cell_type":"markdown","source":"Next I will try a voting classifier where we combine the best two models: SVM and MLPC.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"817068f4950f98f7a314af7358b09dfc18db2025","_cell_guid":"71b865db-6fce-482b-820f-b4ee49bda6e5","trusted":true},"cell_type":"code","source":"def runVotingClassifier(a,b,c,d):\n    \"\"\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n    http://scikit-learn.org/stable/modules/ensemble.html#voting-classifier\"\"\"\n    #global votingC, mean, stdev # eventually I should get rid of these global variables and use classes instead.  in this case i need these variables for the submission function.\n    votingC = VotingClassifier(estimators=[('SVM', SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)), ('MLPC', MLPC(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n       hidden_layer_sizes=(100,), learning_rate='constant',\n       learning_rate_init=0.001, max_iter=2000, momentum=0.9,\n       nesterovs_momentum=True, power_t=0.5, random_state=None,\n       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n       verbose=False, warm_start=False))], voting='hard')  \n    votingC = votingC.fit(a,b)   \n    kfold = model_selection.KFold(n_splits=10)\n    accuracy = model_selection.cross_val_score(votingC, a,b, cv=kfold, scoring='accuracy')\n    meanC = accuracy.mean() \n    stdevC = accuracy.std()\n    print('Ensemble Voting Classifier - Training set accuracy: %s (%s)' % (meanC, stdevC))\n    print('')\n    #return votingC, meanC, stdevC\n    prediction = votingC.predict(c)\n    cnf_matrix = confusion_matrix(d, prediction)\n    np.set_printoptions(precision=2)\n    class_names = dict_characters \n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names,title='Confusion matrix')\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n    plot_learning_curve(votingC, 'Learning Curve For Ensemble Voting Classifier', X_trainScaledPCA, Y_trainScaledPCA, (0.85,1), 10)\nprint(\"\\nAfter Data Scaling:\\n\")\nrunVotingClassifier(X_trainScaled, Y_trainScaled,  X_testScaled, Y_testScaled)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8db8a09c9f7d8f5ac7a857cefe0adbc0f4674998","_cell_guid":"445e802d-7884-4ca5-a204-e3f53ef752d3","trusted":true},"cell_type":"code","source":"print(\"\\nAfter PCA:\\n\")\nrunVotingClassifier(X_trainScaledPCA, Y_trainScaledPCA,  X_testScaledPCA, Y_testScaledPCA)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"486b7036350d2e8c84558f1ac6b52a421d9d1867","_cell_guid":"283a5bf9-fc1b-45e3-b1a1-3186e341a8e9"},"cell_type":"markdown","source":"My apologies to the small percentage of patients with malignant tumors who were told that their tumors were benign.  Besides that, we were consistently accurate.  We had our best results when we used SVM, MLPC, or a VotingClassifier where we combined both SVM and MLPC.  Furthermore, we had the best results when we used data scaling and we did not see much of a difference when we used PCA.  In the future, tools like this can be used to save time, cut costs, and increase the accuracy of imaging-based diagnostic approaches in the healthcare industry.","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}