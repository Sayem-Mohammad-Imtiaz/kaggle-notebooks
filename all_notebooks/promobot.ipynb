{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nimport time\nimport datetime\n\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_train.csv\", encoding='latin1')\ntest = pd.read_csv(\"/kaggle/input/covid-19-nlp-text-classification/Corona_NLP_test.csv\", encoding='latin1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[['OriginalTweet', 'Sentiment']]\ntest = test[['OriginalTweet', 'Sentiment']]\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.groupby('Sentiment').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_mapper = {\n    'Extremely Negative': 0,\n    'Negative': 1,\n    'Neutral': 2,\n    'Positive': 3,\n    'Extremely Positive': 4\n}\n\ntrain['Sentiment'] = train['Sentiment'].apply(lambda x: label_mapper[x])\ntest['Sentiment'] = test['Sentiment'].apply(lambda x: label_mapper[x])\n\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.utils import simple_preprocess\n\ndef preprocessing(X_train, X_test):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    html = re.compile(r'<.*?>')\n    number = re.compile(r'\\d+')\n    mention = re.compile(r'@\\w+')\n    tags = re.compile(r'#\\w+')\n    spaces = re.compile(r'\\s+')\n    \n    def cleaning_pipe(X):\n        out = []\n        \n        for x in X:\n            x = url.sub('url_token', x)\n            x = number.sub('num_token', x)\n            x = mention.sub('mention_token', x)\n            x = tags.sub('tag_token', x)\n            \n            x = html.sub(' ', x)\n            x = spaces.sub(' ', x)\n            \n            out.append(x)\n        \n        return out  \n        \n    X_train = [simple_preprocess(x) for x in cleaning_pipe(X_train)]\n    X_test = [simple_preprocess(x) for x in cleaning_pipe(X_test)]\n    \n    # any other steps\n    \n    with open('data.txt', 'w') as f:\n        for x in X_train:\n            f.write(' '.join(x) + '\\n')\n        \n        for x in X_test:\n            f.write(' '.join(x) + '\\n')\n    \n    return X_train, X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_texts_repr(texts, model):\n    global_pooling = lambda tokens: np.array([model[t] for t in tokens]).mean(axis=0)\n    return [global_pooling(t) for t in texts]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\ndef eval_clf(X_train, y_train, X_test, y_test, model, rs=50):\n    scaler = StandardScaler()\n    \n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.fit_transform(X_test)\n    \n    clf = model().fit(X_train, y_train)\n    \n    print(classification_report(y_train, clf.predict(X_train)))\n    print(classification_report(y_test, clf.predict(X_test)))\n    \n    print(confusion_matrix(y_test, clf.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nX_train = train['OriginalTweet']\ny_train = train['Sentiment']\nX_test = test['OriginalTweet']\ny_test = test['Sentiment']\n\nX_train, X_test = preprocessing(X_train, X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"emb_dim = 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gensim fasttext"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.fasttext import FastText","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ngensim_model = FastText(\n    size=emb_dim,\n    window=5, \n    min_count=5, \n    corpus_file='./data.txt',\n    iter=5\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\neval_clf(\n    X_train=get_texts_repr(X_train, gensim_model.wv),\n    y_train=y_train,\n    X_test=get_texts_repr(X_test, gensim_model.wv), \n    y_test=y_test,\n    model=LogisticRegression, \n    rs=50\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\neval_clf(\n    X_train=get_texts_repr(X_train, gensim_model.wv),\n    y_train=y_train,\n    X_test=get_texts_repr(X_test, gensim_model.wv), \n    y_test=y_test,\n    model=GaussianNB,\n    rs=50\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Facebook fasttext"},{"metadata":{"trusted":true},"cell_type":"code","source":"import fasttext","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"facebook_model = fasttext.train_unsupervised(\n    './data.txt', \n    model='cbow'\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\neval_clf(\n    X_train=get_texts_repr(X_train, facebook_model),\n    y_train=y_train,\n    X_test=get_texts_repr(X_test, facebook_model), \n    y_test=y_test,\n    model=LogisticRegression, \n    rs=50\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\neval_clf(\n    X_train=get_texts_repr(X_train, facebook_model),\n    y_train=y_train,\n    X_test=get_texts_repr(X_test, facebook_model),\n    y_test=y_test,\n    model=GaussianNB,\n    rs=50\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hugging face"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport random\n\nif torch.cuda.is_available():    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n    \nseed_val = 50\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_border = len(X_train)\n\nX = [' '.join(x) for x in X_train + X_test]\ny = [int(y) for y in y_train.tolist() + y_test.tolist()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import DistilBertTokenizer\n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nmax_len = 0\n\nfor x in X:\n    input_ids = tokenizer.encode(x, add_special_tokens=True)\n    max_len = max(max_len, len(input_ids))\n\nprint('Max sentence length: ', max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ninput_ids = []\nattention_masks = []\n\nfor x in X:\n    encoded_dict = tokenizer.encode_plus(\n        x,                      \n        add_special_tokens = True,\n        max_length = 256,\n        pad_to_max_length = True,\n        return_attention_mask = True,\n        return_tensors = 'pt',\n    )\n    \n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\nlabels = torch.tensor(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset, random_split\n\n# Combine the training inputs into a TensorDataset.\ntrain_dataset = TensorDataset(\n    input_ids[:train_test_border], \n    attention_masks[:train_test_border], \n    labels[:train_test_border]\n)\n\ntrain_size = int(0.9 * len(train_dataset))\nval_size = len(train_dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(\n    train_dataset, \n    [train_size, val_size]\n)\n\ntest_dataset = TensorDataset(\n    input_ids[train_test_border:], \n    attention_masks[train_test_border:], \n    labels[train_test_border:]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\nbatch_size = 32\n\ntrain_dataloader = DataLoader(\n            train_dataset,\n            sampler=RandomSampler(train_dataset),\n            batch_size=batch_size\n        )\n\nvalidation_dataloader = DataLoader(\n            val_dataset, \n            sampler=SequentialSampler(val_dataset),\n            batch_size=batch_size\n        )\n\ntest_dataloader = DataLoader(\n            test_dataset, \n            sampler=SequentialSampler(test_dataset),\n            batch_size=batch_size\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification, AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\",\n    num_labels = 5,\n    output_attentions = False,\n    output_hidden_states = False,\n)\n\nmodel.cuda()\n\noptimizer = AdamW(\n    model.parameters(),\n    lr = 2e-5, \n    eps = 1e-8\n)\n\nepochs = 4\n\ntotal_steps = len(train_dataloader) * epochs\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps = 0,\n    num_training_steps = total_steps\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# utils\n\nfrom sklearn.metrics import classification_report\n\ndef update_classification_report(preds, labels, report):\n    predicted_y = np.argmax(preds, axis=1).flatten()\n    true_y = labels.flatten()\n    \n    metrics = classification_report(true_y, predicted_y, output_dict=True)\n    \n    report['precision'] += metrics['macro avg']['precision']\n    report['f1-score']  += metrics['macro avg']['f1-score']\n    report['accuracy']  += metrics['accuracy']\n    report['recall']    += metrics['macro avg']['recall']\n    \n    return report\n\n\ndef format_time(elapsed):\n    \"\"\"Takes a time in seconds and returns a string hh:mm:ss\"\"\"\n    return str(datetime.timedelta(seconds=int(round((elapsed)))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\n\ntraining_stats = []\n\n# total training time for all epochs\ntotal_t0 = time.time() \n\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #                  Train\n    # ========================================\n    \n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n\n    t0 = time.time() # epoch time tracking\n\n    total_train_loss = 0\n\n    model.train() # set train mode\n\n    for step, batch in enumerate(train_dataloader):\n        if step % 50 == 0 and not step == 0:\n            elapsed = format_time(time.time() - t0) # elapsed time in minutes.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        model.zero_grad()        \n\n        loss, _ = model(\n            b_input_ids, \n            attention_mask=b_input_mask, \n            labels=b_labels\n        )\n\n        total_train_loss += loss.item()\n\n        # calc gradients\n        loss.backward() \n\n        # avoid exploding gradients problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) \n\n        # Update params\n        optimizer.step()\n\n        # Update the lr\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    \n    print(\"\")\n\n    t0 = time.time()\n\n    model.eval()\n\n    # Tracking variables \n    report = {\n        'precision': 0,\n        'f1-score':  0,\n        'accuracy':  0,\n        'recall':    0\n    }\n    \n    total_eval_loss = 0\n\n    for batch in validation_dataloader:\n        \n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        with torch.no_grad():        \n            loss, preds = model(\n                b_input_ids, \n                attention_mask=b_input_mask,\n                labels=b_labels\n            )\n\n        total_eval_loss += loss.item()\n\n        preds = preds.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # callculate metrics         \n        report = update_classification_report(preds, label_ids, report)\n    \n    for k, v in report.items():\n        report[k] = v / len(validation_dataloader)\n        print(\"  {1}: {0:.2f}\".format(report[k], k))\n\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Training Time': training_time,\n            'Validation Time': validation_time,\n            **report\n        }\n    )\n\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(data=training_stats).set_index('epoch')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\n\npredictions , true_labels = [], []\n\nfor batch in test_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    b_input_ids, b_input_mask, b_labels = batch\n\n    with torch.no_grad():\n        outputs = model(\n            b_input_ids,\n            attention_mask=b_input_mask\n        )\n\n    preds = outputs[0]\n\n    preds = preds.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    # Store predictions and true labels\n    predictions.append(preds)\n    true_labels.append(label_ids)\n\npredicted_y = np.concatenate([np.argmax(x, axis=1) for x in predictions], axis=0)\ntrue_y = np.concatenate(true_labels, axis=0)\n\nprint(classification_report(true_y, predicted_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.concatenate(true_labels, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train/validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = \"\"\"{\"id\": 1, \"text\": \"чем занимаешься по жизни я вот бизнесмен\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[31, 40, \"activity\"]]}\n{\"id\": 2, \"text\": \"а я вот учу детей работаю с начальными классами к свадьбе готовлюсь\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[8, 17, \"activity\"]]}\n{\"id\": 3, \"text\": \"я люблю есть арбуз любишь готовить\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[8, 18, \"hobby\"], [26, 34, \"hobby\"]]}\n{\"id\": 4, \"text\": \"люблю готовить пасту у меня классно получается\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[6, 20, \"hobby\"]]}\n{\"id\": 5, \"text\": \"хочу быть психологом в газпроме а кем ты работаешь\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[10, 20, \"activity\"]]}\n{\"id\": 6, \"text\": \"не поверишь я психолог в партии роста\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[14, 22, \"activity\"]]}\n{\"id\": 7, \"text\": \"а у меня мама домохозяйка поэтому редко бываю дома одна в питере а ты\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[58, 64, \"place\"], [14, 25, \"activity\"]]}\n{\"id\": 8, \"text\": \"а я в ростове-на-дону на 130-м шоссе в придорожном кафе\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[6, 21, \"place\"]]}\n{\"id\": 9, \"text\": \"кто по профессии ты расскажи о себе, контракты перебираешь\", \"meta\": {}, \"annotation_approver\": null, \"labels\": []}\n{\"id\": 10, \"text\": \"лошадей люблю у нас были давно 10 лошадей вот следил за ними отцу помогал с тех пор так подрабатываю иногда как с трудоустройством в норвегии\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[133, 141, \"place\"], [0, 7, \"hobby\"]]}\n{\"id\": 11, \"text\": \"живу в норвегии с 11 лет сейчас мне 21 уехали с родителями отцу надо было по работе остались периодически бываем в россии конечно\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[7, 15, \"place\"], [115, 121, \"place\"]]}\n{\"id\": 12, \"text\": \"круто я бы тоже поехал люблю путешествовать думаю посетить страну когда последний раз был в россии\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[92, 98, \"place\"], [29, 43, \"hobby\"]]}\n{\"id\": 13, \"text\": \"летом 2017 давно скучаю по россии ты из города на неве\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[27, 33, \"place\"]]}\n{\"id\": 14, \"text\": \"я из тульской области чем увлекаешься\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[5, 21, \"place\"]]}\n{\"id\": 15, \"text\": \"добрый день кирилл сто кило приятно, потому что вешу 250 фунтов познакомится\", \"meta\": {}, \"annotation_approver\": null, \"labels\": []}\n{\"id\": 16, \"text\": \"чем занимаетесь кирилл английскую музыку любите и английский язык наверно знаете\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[23, 40, \"hobby\"], [50, 65, \"hobby\"]]}\n{\"id\": 17, \"text\": \"в декрете за жену сижу у меня три дочери - вот слежу, по спектаклям и в кино не хожу\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[0, 9, \"activity\"], [57, 67, \"hobby\"], [70, 76, \"hobby\"]]}\n{\"id\": 18, \"text\": \"а я само леты испытываю сейчас много новых разработок так что постоянно в стрессе\", \"meta\": {}, \"annotation_approver\": null, \"labels\": []}\n{\"id\": 19, \"text\": \"на самом деле втягиваешься в домашнюю работу я кулинар - люблю готовить - вся семья довольна но конечно хочется обратно на работу все же\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[63, 71, \"hobby\"], [123, 129, \"activity\"], [47, 54, \"activity\"]]}\n{\"id\": 20, \"text\": \"мне бы так расслабиться как вы но детей пока нет все с женой только хотим завести а кем работали до декрета\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[100, 107, \"activity\"]]}\n{\"id\": 21, \"text\": \"сходите в бассейн - хорошо расслабляет я хорошо плаваю рекомендую\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[10, 17, \"hobby\"]]}\n{\"id\": 22, \"text\": \"а я пою хорошо это мое расслабление мечтаю поучаствовать в шоу голос\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[4, 7, \"hobby\"], [59, 68, \"hobby\"]]}\n{\"id\": 23, \"text\": \"представляешь летчик - испытатель в шоу бизнесе\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[14, 33, \"activity\"]]}\n{\"id\": 24, \"text\": \"я помогаю людям правильно одеваться сейчас просто только онлайн могу это делать из - за декрета а так очень люблю шопинг\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[88, 95, \"activity\"], [114, 120, \"hobby\"], [2, 35, \"activity\"]]}\n{\"id\": 25, \"text\": \"хорошо поющий лётчик это находка особенно во время восстания\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[14, 20, \"activity\"]]}\n{\"id\": 26, \"text\": \"так сейчас многие онлайн из дома работают это удобно особенно для тех кто в декрете\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[76, 83, \"activity\"]]}\n{\"id\": 27, \"text\": \"да так и получается так мечтаю поехать в париж опять но думаю скоро реализую мечту\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[31, 46, \"hobby\"]]}\n{\"id\": 28, \"text\": \"пробовался в песеных конкурсах караоке например, далеко добираться, сначала на поезде, потом на автобусе или на машине, и в конце на пароме\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[31, 38, \"hobby\"]]}\n{\"id\": 29, \"text\": \"может дашь пару советов по стилю что сейчас у мужиков в моде а то я постоянно в форме и редко выхожу в люди а так джинсы и пиджак моя основная домашняя одежда\", \"meta\": {}, \"annotation_approver\": null, \"labels\": []}\n{\"id\": 30, \"text\": \"ну на работе участвую в самодеятельности метные конкурсы так сказать но нигде на больших сценах не выступал меня уже достали на корпоративах просить спеть\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[149, 154, \"hobby\"]]}\n{\"id\": 31, \"text\": \"хотя лепс тоже с ресторанов начинал не люблю лепса\", \"meta\": {}, \"annotation_approver\": null, \"labels\": []}\n{\"id\": 32, \"text\": \"отель 2 суток бесплатно третий день 270 ₽ далее 70 ₽ сутки скидка получается 75%\", \"meta\": {}, \"annotation_approver\": null, \"labels\": []}\n{\"id\": 33, \"text\": \"не надо tele2 не надо мне sim-карту она у вас за евро\", \"meta\": {}, \"annotation_approver\": null, \"labels\": []}\n{\"id\": 34, \"text\": \"а я встречаюсь с парнем надеюсь все серьёзно он у меня первый и единственный\", \"meta\": {}, \"annotation_approver\": null, \"labels\": []}\n{\"id\": 35, \"text\": \"свободное время посещаю животным в приюте собак ведь они такие умные\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[16, 41, \"hobby\"]]}\n{\"id\": 36, \"text\": \"у тебя есть родные братья или сестры\", \"meta\": {}, \"annotation_approver\": null, \"labels\": []}\n{\"id\": 37, \"text\": \"сейчас реп слушаю а так из рока skillet nirvana слот ну и другие а ты а из репа всего по не многу очень люблю сёрфинг а ты пробовала\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[7, 17, \"hobby\"], [110, 117, \"hobby\"]]}\n{\"id\": 38, \"text\": \"я программистом работаю программы пишу под 1 с\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[2, 15, \"activity\"], [24, 46, \"activity\"]]}\n{\"id\": 39, \"text\": \"очень классно бухгалтерия я фрилансер\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[28, 37, \"activity\"]]}\n{\"id\": 40, \"text\": \"бухгалтер классно я тоже дома работаю\", \"meta\": {}, \"annotation_approver\": null, \"labels\": []}\n{\"id\": 41, \"text\": \"не фрилансер имела в виду 1с под бухгалтерию крутые программы\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[3, 12, \"activity\"]]}\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport math\n\ndef split_train_test_for_ner(samples, train_size=0.8):\n    \"\"\"\n    split input samples according to train_size and labels distribution\n    \"\"\"\n    \n    label2samples = {'no_label': set()}\n    \n    for i, sample_as_str in enumerate(samples):\n        sample = json.loads(sample_as_str)\n        labels = sample['labels']\n        \n        if len(labels) == 0:\n            label2samples['no_label'].add(i)\n        else:    \n            for label_data in labels:\n                label_name = label_data[2]\n\n                if label_name not in label2samples:\n                    label2samples[label_name] = set()\n                \n                label2samples[label_name].add(i)\n    \n    train_indexes = {x for x in range(len(samples))}\n    test_indexes = set()\n    \n    for label, samples_id in label2samples.items():\n        samples_id = samples_id - test_indexes     # remove selected indexes\n        samples_id = list(samples_id)\n        \n        n_samples = len(samples_id)                # amount of samples that contain label\n        n_test = math.ceil(n_samples * (1 - train_size)) # required samples for test\n        \n        for i in range(n_test):\n            idx = np.random.randint(n_samples)     # select random sample\n            sample_id = samples_id.pop(idx)  \n            n_samples = n_samples - 1\n            \n            test_indexes.add(sample_id)\n    \n    \n    return list(train_indexes - test_indexes), list(test_indexes)\n\ndataset_to_split = dataset.split('\\n')\ntrain, test = split_train_test_for_ner(dataset_to_split)     \n\nprint(train)\nprint(test)\n\nprint(len(test) / len(dataset_to_split))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}