{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Credit Card Fraud Analysis - Predictive Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Credit Card Fraud Detection with Machine Learning is a process of data investigation and the development of a model that will provide the best results in revealing and preventing fraudulent transactions. This is achieved through bringing together all meaningful features of card users’ transactions, such as Date, User Zone, Product Category, Amount, Provider, Client’s Behavioral Patterns, etc.\n\nThe Credit Card Fraud Detection Problem includes modeling past credit card transactions with the knowledge of the ones that turned out to be fraud. This model is then used to identify whether a new transaction is fraudulent or not. Our aim here is to detect 100% of the fraudulent transactions while minimizing the incorrect fraud classifications.\n\nThe purpose of this data analysis is therefore to identify potential fraudulent credit card transactions.\n\nI order to detect these anomalies and propose a prediction model, I would use machine learning-based techniques such as: Desicion Tree Classification, Random Forest classification method, Logistic Regregression etc.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Machine Learning-based Fraud Detection:\n\n* Detecting fraud automatically\n* Real-time streaming\n* Less time needed for verification methods\n* Identifying hidden correlations in data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data\nfrom https://www.kaggle.com/mlg-ulb/creditcardfraud\n\nThe dataset contains transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have **492** frauds out of **284,807** transactions. The dataset is highly unbalanced, the positive class (frauds) account for **0.172%** of all transactions.\n\nThe dataset consists of numerical values from the **28** \"Principal Component Analysis (PCA)\" transformed features, namely V1 to V28. Furthermore, there is no metadata about the original features provided, so pre-analysis or feature study could not be done.\nThe 'Time' and 'Amount' features are not transformed data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Import Standard Packages:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#packages\n%matplotlib inline \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set(style=\"ticks\", color_codes=True)\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection  import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\nfrom sklearn.pipeline import make_pipeline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Load the dataset:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"CRdf = pd.read_csv('../input/creditcardfraud/creditcard.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset size\nCRdf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display 5 first rows\nCRdf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#display 5 last rows\nCRdf.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the variables types","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#variables types\nCRdf.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check for missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def NA_val(data):\n    \n    missing = data.isna().sum()\n    missing = missing[missing>0]\n    missing_perc = missing/CRdf.shape[0]*100\n    na = pd.DataFrame([missing, missing_perc], index = ['missing_num', 'missing_perc']).T\n    NA_val = na.sort_values(by = 'missing_perc', ascending = False)\n    NA_val = round(NA_val,2)\n\n    return NA_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NA_val=NA_val(CRdf)\nNA_val","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good! there is no missing value in the dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Exploratory Data Analysis\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(CRdf.shape)\nprint(CRdf.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Column Names\", CRdf.columns) #Here I am using both the print function","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"CRdf.hist(figsize=(20,20))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# designate target variable name\ntargetVar = 'Class'\n#print(targetVar)\ntargetSeries = CRdf[targetVar] #notice one column is considered a series in pandas\n#print(targetSeries)\n#remove target from current location and insert in column number 0\ndel CRdf[targetVar]\nCRdf.insert(0, targetVar, targetSeries)\n#reprint dataframe and see target is in position 0\nCRdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Basic bar chart since the target is binominal\ngroupby = CRdf.groupby(targetVar)\ntargetEDA=groupby[targetVar].aggregate(len)\nprint(targetEDA)\n\nlabels = [\"Normal\", \"Fraud\"]\nplt.figure()\ntargetEDA.plot(kind='bar', grid=True, color='orange')\nplt.axhline(0, color='k')\nplt.title(\"Transaction Class Distribution\")\nplt.xticks(range(2), labels)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The fraud (Class 1) frequency is too low to see; to remind the number of fraudulent transactions is 492 frauds.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate fraud rate\nnb_customers = len(CRdf.index)\nprint('There are a total of %s customers in the dataset among which %s anomaly (or fraud).' \n      %(nb_customers, CRdf[CRdf['Class'] == 1].shape[0]))\nCR_NB = CRdf['Class'].value_counts()[1]\nFraudRate = float(CR_NB) / nb_customers\nprint('The Attrition rate is {:.2f}%'.format(FraudRate*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How different are the amount of money used in different transaction classes?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"normal_transactions = CRdf[CRdf['Class'] == 0]\nfraud_transactions = CRdf[CRdf['Class'] == 1]\nnormal_transactions.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normal_transactions.Amount.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fraud_transactions.Amount.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (11,3))\nplt.subplot(1,2,1)\nplt.scatter(normal_transactions.Time, normal_transactions.Amount)\nplt.title('Normal transactions')\nplt.xlabel('Time in seconds'); \nplt.ylabel('Amount')\nplt.subplot(1,2,2)\nplt.scatter(fraud_transactions.Time, fraud_transactions.Amount)\nplt.title('Fraud transactions')\nplt.xlabel('Time in seconds'); \nplt.ylabel('Amount')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The observation of the graphs above shows that the time of transactions does not really matters. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Drop the variable Time:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"CRdf.drop([\"Time\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation:\n\nBy plotting a correlation matrix, we have a very nice overview of how the features are related to one another.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation matrix\ncorr = CRdf.corr()\n\n#plot using seaborn library\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(17, 11))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(round(corr,2), annot=True, mask=mask, cmap=cmap, vmax=.3,\n                linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above correlation matrix shows that none of the V1 to V28 PCA components have any correlation to each other, however the target variable \"Class\" has some form positive and negative correlations with the V components, but it has no correlation with Time and Amount. There is no risk of collinarity between our variables.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3. Machine Learning Models\n\n\nMachine learning algorithms have hyperparameters that allow you to tailor the behavior of the algorithm to your specific dataset.\nHyperparameters are different from parameters, which are the internal coefficients or weights for a model found by the learning algorithm. Unlike parameters, hyperparameters are specified by the practitioner when configuring the model.\nTypically, it is challenging to know what values to use for the hyperparameters of a given algorithm on a given dataset, therefore it is common to use random or grid search strategies for different hyperparameter values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Create a training and test set with a split 70/30:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'Class'\npredictors = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split dataset into testing and training\n# column location 1 to end of dataframe are the features.\n# column location 0 is the target\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    CRdf.iloc[:,1:].values, CRdf.iloc[:,0].values, test_size=0.30, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(features_test.shape)\nprint(features_train.shape)\nprint(target_test.shape)\nprint(target_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. 1. Decision Tree Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#decision tree. Call up my model and name it clf\n#clf is a notation used by many people for classifier\nfrom sklearn import tree \ndt_clf = tree.DecisionTreeClassifier()\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(dt_clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train model\ndt_model = dt_clf.fit(features_train, target_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict clf DT model again test data\ntarget_pred_dt = dt_model.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Decision Tree Accuracy Score\", accuracy_score(target_test, target_pred_dt))\nprint(classification_report(target_test, target_pred_dt, target_names = [\"Class = no\", \"Class = yes\"]))\nprint(confusion_matrix(target_test, target_pred_dt))\n\n#extracting true_positives, false_positives, true_negatives, false_negatives\ntn, fp, fn, tp = confusion_matrix(target_test, target_pred_dt).ravel()\nprint(\"True Negatives: \",tn)\nprint(\"False Positives: \",fp)\nprint(\"False Negatives: \",fn)\nprint(\"True Positives: \",tp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Decision Tree classifier's accuracy score is 99.92%, which is a excellent score.\n\nPrecision: How often is the classifier correct with its positive predictions? Precision = True Positives/(True Positives + False Positives). \n\nRecall: How well does the classifier predict positive cases? Recall = True Positives/(True Positives + False Negatives). Yes, recall is the same as the sensitivity rate. \n\nF1-score is a function of Precision and Recall. It is needed when you want to seek a balance between Precision and Recall.\n\nOur classifier correctly identifies 77% of fraudulent transactions (Recall). Also, The DT classifier is 79% correct when it predicts \"fraud\" (Precision).\n\n* We can further deepen our analysis by trying to improve the recall score (sensitivity). As we know, this metric is very important in detecting anomalies.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Features importance:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = pd.DataFrame({'Feature': predictors, 'Feature importance': dt_model.feature_importances_})\nimp = imp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=imp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **In order to tune the different methods I am going to create a tuning function based on GridSearch:**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Run grid search, get the prediction array and print the accuracy and best combination\ndef fit_and_pred_grid_classifier(clf, param_grid, X_train, X_test, y_train, y_test, scoring = \"recall\", folds = 10):\n\n    \n    gs = GridSearchCV(estimator = clf, param_grid = param_grid, cv = folds, scoring = scoring, n_jobs = -1, verbose = 0)\n    gs = gs.fit(X_train, y_train)\n\n    best_score = gs.best_score_\n    best_parameters = gs.best_params_\n\n    # Get the prediction array\n    grid_search_pred = gs.predict(X_test)\n    \n    \n    # summarize results\n    print(\"Best \" +  scoring + \" score: %f using %s\" % (best_score, best_parameters))\n    means = gs.cv_results_['mean_test_score']\n    stds = gs.cv_results_['std_test_score']\n    params = gs.cv_results_['params']\n    \n    for mean, stdev,param in zip(means, stds, params):\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n    return grid_search_pred, grid_search_pred\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's call the tuning for DT classifier. \n\nThe first parameter to tune is max_depth. This indicates how deep the tree can be. The deeper the tree, the more splits it has and it captures more information about the data. We fit a decision tree with depths ranging from 1 to 32. Other hyper parameters we can also tune are \"max_features\" and the \"criterion\" type.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\"max_depth\": [3,7,11],\n              \"max_features\": [3,9,15],\n             \"criterion\": [\"gini\", \"entropy\"]\n             }\nimport time\nstart = time.time()\n\n# Run grid search, print the results and get the prediction array and model\ngs_pred_dt, dt_grid = fit_and_pred_grid_classifier(dt_clf, param_grid, features_train,\n                                                   features_test, target_train, target_test)\n\nend = time.time()\nprint(\"Time to run\", round(end-start), \"seconds\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We reach the best recall score, about 77% by using inputs {'criterion': 'entropy', 'max_depth': 3, 'max_features': 15}.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### *Confusion matrix:*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate set\ncm_dt = confusion_matrix(target_test, gs_pred_dt)#correlation matrix\n\nprint(classification_report(target_test, gs_pred_dt,target_names = [\"Class = no\", \"Class = yes\"])) \nprint(\"Recall: \" + str(round(recall_score(target_test, gs_pred_dt), 4) * 100) + \"%\") \n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cm_dt.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm_dt.flatten()/np.sum(cm_dt)]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in\n          zip(group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cm_dt, annot=labels, fmt='', cmap='Oranges')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results presented above show us that:\n\nWe now correctly predicted  **117** entries as \"Fraudulent\" and increase the recall score to 80% from 77%. This a little bit better than previously, with default parameters.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3. 2. Random Forest Model\n\n\nRandom Forest is a variant of Bagging where only a randomly chosen subset of features are considered to split at each node. Each node is split on the \"best\" of the given subset of features. The random forest model has  less variance than the decision tree.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n#Build\nrf_clf = RandomForestClassifier(max_features='auto', random_state=123)\nprint(rf_clf)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train set\nrf_model = rf_clf.fit(features_train, target_train)\n\n#Validation set - prediction\ntarget_pred_rf = rf_clf.predict(features_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Random Forest classifier Accuracy Score\", accuracy_score(target_test, target_pred_rf))\nprint(classification_report(target_test, target_pred_rf, target_names = [\"Class = no\", \"Class = yes\"]))\nprint(confusion_matrix(target_test, target_pred_rf))\n\n#extracting true_positives, false_positives, true_negatives, false_negatives\ntn, fp, fn, tp = confusion_matrix(target_test, target_pred_rf).ravel()\nprint(\"True Negatives: \",tn)\nprint(\"False Positives: \",fp)\nprint(\"False Negatives: \",fn)\nprint(\"True Positives: \",tp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By using default paramenters the Random Forest classifier is 94% corret when identifying \"fraud transaction\" (precision) and the model also has a good recall score (75%). As the DT model, the RF model did with a very good accuracy.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*Features importance:*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = pd.DataFrame({'Feature': predictors, 'Feature importance': rf_clf.feature_importances_})\nimp = imp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=imp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. 3. Extremely Randomized Trees (Extra Trees)\n\nExtra Trees is like Random Forest, in that it builds multiple trees and splits nodes using random subsets of features, but with two key differences: it does not bootstrap observations (meaning it samples without replacement), and nodes are split on random splits, not best splits. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nclf_xdt = ExtraTreesClassifier(n_estimators= 100, n_jobs=-1, random_state=123)\nprint(clf_xdt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train data\nmodel_xdt = clf_xdt.fit(features_train, target_train)\n\n#validation set\ntarget_predicted=clf_xdt.predict(features_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Extra Trees Accuracy\", accuracy_score(target_test,target_predicted))\ntarget_names = [\"Class = no\", \"Class = yes\"]\nprint(classification_report(target_test, target_predicted,target_names=target_names))\nprint(confusion_matrix(target_test, target_predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This classifier correctly identifies 75% of \"Fraud\" (recall). This classifier is also 97% correct when it predicts an fraudulent case (precision). ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf_xdt.feature_importances_})\nimp = imp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=imp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Tuning:** The most important parameter is the number of random features to sample at each split point (max_features).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# use a full grid over all parametersimport time\nimport time\nparam_grid = {\"max_features\": [7,11,15]}\nstart = time.time()\n\n# run grid search\nimport time\nstart = time.time()\n\n# run grid search\nxdt_gs_pred, xdt_grid = fit_and_pred_grid_classifier(clf_xdt, param_grid, features_train, \n                                                     features_test, target_train, target_test, scoring='recall')\nend = time.time()\nprint(\"Time to run\", round(end-start), \"seconds\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well done! We increase the recall score from 75% to 79.45%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix*:*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#with the validation set\ncm_xdt = confusion_matrix(target_test, xdt_gs_pred)#confusion matrix\n\nprint(classification_report(target_test, xdt_gs_pred,target_names = [\"Class = no\", \"Class = yes\"])) \nprint(\"Recall: \" + str(round(recall_score(target_test, xdt_gs_pred),4) * 100) + \"%\") \n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cm_xdt.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm_xdt.flatten()/np.sum(cm_xdt)]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in\n          zip(group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cm_xdt, annot=labels, fmt='', cmap='Oranges')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The classifier correctly predicted 85290 (99.82%) entries as \"Normal transactions (No Fraudulents)\" and 112 entries as \"Fraudulents\" (0.13%). This method incorrectly predicted 35 entries as \"No Fraudulents\" and 6 as \"Fraudulents\". We slightly improve the recall score from 75 to 76%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3. 4. Stochastic Gradient Descent Classifier","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's first normalize features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalize features\nscaler = StandardScaler()  \n#Train\nscaler.fit(features_train)  \n#Validate\nfeatures_train_norm = scaler.transform(features_train)  \n# apply same transformation to test data\nfeatures_test_norm = scaler.transform(features_test) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd_linear_svm_clf = SGDClassifier(random_state=0)\nprint(sgd_linear_svm_clf )\n\n#Train data\nmodel_sgd = sgd_linear_svm_clf.fit(features_train_norm, target_train)\n\n#test data\ntarget_pred_sgd = sgd_linear_svm_clf.predict(features_test_norm)\nprint(\"Accuracy\", accuracy_score(target_test, target_pred_sgd))\ntarget_names = [\"Class = no\", \"Class = yes\"]\nprint(classification_report(target_test, target_pred_sgd, target_names=target_names))\nprint(confusion_matrix(target_test, target_pred_sgd))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Good accuracy 99.90%. Our classifier correctly identifies 55% of fraud transactions (Recall). The Stochastic Gradient Descent  classifier is 87% correct when it predicts \"Fraud\" (Precision).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Tuning...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nstart = time.time()\nparam_grid = {'alpha': [0.0001,0.01,0.1]\n             }\n# Run grid search, print the results and get the prediction array and model\ngsd_gs_pred, gsd_grid = fit_and_pred_grid_classifier(sgd_linear_svm_clf, param_grid, features_train_norm, \n                                                     features_test_norm, target_train, target_test)\nend = time.time()\nprint(\"Time to run\", round(end-start), \"seconds\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We reach the best recall score (56.85%)  for alpha value equals to 0.0001.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Confusion matrix:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate set\ncm_gsd = confusion_matrix(target_test, gsd_gs_pred)#confusion matrix\n\nprint(classification_report(target_test, gsd_gs_pred,target_names = [\"Class = no\", \"Class = yes\"])) \nprint(\"Recall: \" + str(round(recall_score(target_test, gsd_gs_pred),4) * 100) + \"%\") \n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cm_gsd.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm_gsd.flatten()/np.sum(cm_gsd)]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in\n          zip(group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cm_gsd, annot=labels, fmt='', cmap='Oranges')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. 5. Logit Regression Model\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# building logistic regression classifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.ticker\n\nlogit = LogisticRegression()\n\n#Call up the model to see the parameters you can tune (and their default setting)\nprint(logit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train model\nlogit_model = logit.fit(features_train_norm, target_train)\n\n#validate set\nlogit_predicted=logit.predict(features_test_norm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Logistic classifier Accuracy Score\", accuracy_score(target_test, logit_predicted))\nprint(classification_report(target_test, logit_predicted))\nprint(confusion_matrix(target_test, logit_predicted))\n\n#extracting true_positives, false_positives, true_negatives, false_negatives\ntn, fp, fn, tp = confusion_matrix(target_test, logit_predicted).ravel()\nprint(\"True Negatives: \",tn)\nprint(\"False Positives: \",fp)\nprint(\"False Negatives: \",fn)\nprint(\"True Positives: \",tp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The logit model also did it with a good 99.92% accuracy. It correctly identifies 62% of fraud transactions (Recall), and this method is 88% correct when it predicts a fraud transaction (Precision).\n\nThe logit model correctly predicted 85284 entries as \"Normal transactions (No Fraudulents)\" and 91 entries as \"Fraudulent\".\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'penalty' : ['l1', 'l2'],\n    'C' : np.logspace(-1, 1, 10),\n    'solver' : ['liblinear']}\n\n# Run grid search, print the results and get the prediction array and model\nlogit_target_pred, logit_grid = fit_and_pred_grid_classifier(logit, param_grid, features_train_norm, \n                                                     features_test_norm, target_train, target_test)\nend = time.time()\nprint(\"Time to run\", round(end-start), \"seconds\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate set\ncm_logit = confusion_matrix(target_test, logit_target_pred)#confusion matrix\n\nprint(classification_report(target_test, logit_target_pred,target_names=['Class = no','Class = yes'])) \nprint(\"Recall: \" + str(round(recall_score(target_test, logit_target_pred),4) * 100) + \"%\") \n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cm_logit.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm_logit.flatten()/np.sum(cm_logit)]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in\n          zip(group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cm_logit, annot=labels, fmt='', cmap='Oranges')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The logistic regression classier correctly predicted 85284entries as \"Normal transactions (No Fraudulents)\" and 91 entries as \"Fraudulents\". It also incorrectly predicted 56 entries as \"No Fraudulents\" and 0.01% (12 entries) as \"Fraudulents\". The recall score change to 61.9%.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3. 6.  ROC Curves","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.naive_bayes import GaussianNB\n\n# Instantiate the classfiers and make a list\nclassifiers = [make_pipeline(StandardScaler(), LogisticRegression()),\n               tree.DecisionTreeClassifier(),\n               RandomForestClassifier(max_features='auto', n_estimators=100), \n               ExtraTreesClassifier(n_estimators= 100),\n               ]\n\n# Define a result table as a DataFrame\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    model = cls.fit(features_train, target_train)\n    target_predicted = model.predict_proba(features_test)[::,1]\n    \n    fpr, tpr, _ = roc_curve(target_test,  target_predicted)\n    auc = roc_auc_score(target_test, target_predicted)\n    \n    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'auc':auc}, ignore_index=True)\n\n# Set name of the classifiers as index labels\nresult_table.set_index('classifiers', inplace=True)\n\n\nfig = plt.figure(figsize=(8,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In general we obtained very good scores for all models. The logistic regression model leads with a auc score equals to 96.9%.  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4.  Conclusion\n\nWe have first explorated the dataset, by understanding features and the relationship between each to other. In the second part We modeled the data set to achieve about 99.9% accuracy for fraud detection according to the different supervized ML methods implemented. Such models will intially capture all the frauds, but will rigorously classify non-frauds as fradulent as well.\n\nSince all algorithms performed with high accuracy, it was interesting to look at other metrics, especially the recall score. By tuning our models we managed to increase the recall score. This comes out with good results, however at the cost of computational expense.\n\nSo, overall, the Extremely Randomized Trees model (Extra Trees) were much more successful in determining fraudulent transactions. With a high accuracy (99.95%), a good recall score equal to 79.45% and a goog precision (97%), this model seems to be the best candidate to detect fraudulent transactions. It is followed by the Random Forest model. The gradient descent classifier has the fewest recall score.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}