{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\n\nHai Kagglers, today i'm going to show you simple tutorial on how to handle regression problem while producing high accuracy models as well. Models i;m using today is Gradient Boosting, XGBoost, and Light GBM. Let's do it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import Modules","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read csv file\n\ndf = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quick Look\n\nLet's take a look at our dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation\n\nCor_heat = df.corr()\nplt.figure(figsize=(16,16))\nsns.heatmap(Cor_heat, cmap = \"RdBu_r\", vmax=0.9, square=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Lets see what most important features we have\n\nIF = Cor_heat['price'].sort_values(ascending=False).head(10).to_frame()\nIF.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the data above we know that this dataset contain 21613 row with no missing value. The target column is price and the rest is the feature. This time i'm not gonna use date and id column, so let's drop it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the Data\n\nFeature_data = df.drop(['price','date', 'id'], axis=1)\nTarget_data = df['price']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next is we need to check the target column skewness to make sure we can get a high accuration models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check Target Data Skewness\n\nprint('Skew Value : ' + str(Target_data.skew()))\nsns.distplot(Target_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4.02 ??\n\nThat's teribble, let's use a few transformation technique to make sure we can best skew value (the nearest value to 0).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform target\n\nfrom scipy.special import inv_boxcox\nfrom scipy.stats import boxcox","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(16,16))\n\n# log 1 Transform\nax = f.add_subplot(221)\nL1p = np.log1p(Target_data)\nsns.distplot(L1p,color='b',ax=ax)\nax.set_title('skew value Log 1 transform: ' + str(np.log1p(Target_data).skew()))\n\n# Square Log Transform\nax = f.add_subplot(222)\nSRT = np.sqrt(Target_data)\nsns.distplot(SRT,color='c',ax=ax)\nax.set_title('Skew Value Square Transform: ' + str(np.sqrt(Target_data).skew()))\n\n# Log Transform\nax = f.add_subplot(223)\nLT = np.log(Target_data)\nsns.distplot(LT, color='r',ax=ax)\nax.set_title('Skew value Log Transform: ' + str(np.log(Target_data).skew()))\n\n# Box Cox Transform\nax = f.add_subplot(224)\nBCT,fitted_lambda = boxcox(Target_data,lmbda=None)\nsns.distplot(BCT,color='g',ax=ax)\nax.set_title('Skew Value Box Cox Transform: ' + str(pd.Series(BCT).skew()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best skew value obtain by applying box cox transformation, let's use this as our target column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Target_data = BCT","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Models\n\nIt's time to make the models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'm using 5 fold in this cross val score\n\ndef rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, Feature_data, Target_data, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initiate Models\n## Gradient Boosting\n\nGB = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## XGBoost\n\nXGB = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=220,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## LightGBM\n\nLGB = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=320,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation\n\nLet's see which models has the lowest RMSE value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = [GB, XGB, LGB]\n\nfor x in model:    \n    score = rmse_cv(x).mean()\n    print('RMSE Score with ' + str(x.__class__.__name__) + ' : ' + str(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Gradient Boosting own the lowest score with 0.0084, this make Gradient Boosting the best model.\nLet's perform comparison to see the distribution between actual and predicted data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Predicted_data = []\n\nfor x in model:\n    result = cross_val_predict(x, Feature_data, Target_data, cv=5)\n    Predicted_data.append(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(15,5))\n\n# Gradient Boosting\nax = f.add_subplot(131)\nsns.distplot(Target_data, hist=False, label=\"Actual Values\")\nsns.distplot(Predicted_data[0], hist=False, label=\"Predicted Values\")\nax.set_title('Distribution Comaprison with Gradient Boosting')\n\n# XGBoost\nax = f.add_subplot(132)\nsns.distplot(Target_data, hist=False, label=\"Actual Values\")\nsns.distplot(Predicted_data[1], hist=False, label=\"Predicted Values\")\nax.set_title('Distribution Comaprison with XGBoost')\n\n# LightGBM\nax = f.add_subplot(133)\nsns.distplot(Target_data, hist=False, label=\"Actual Values\")\nsns.distplot(Predicted_data[2], hist=False, label=\"Predicted Values\")\nax.set_title('Distribution Comaprison with LightGBM')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here Gradient Boosting distibution is almost alike, which is good. on the other side XGBoost perform quite bad this time.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## End\n\nThat is how we handle regression problem, thank you.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}