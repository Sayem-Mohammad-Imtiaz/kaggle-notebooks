{"cells":[{"metadata":{"_uuid":"885af7e4-f28c-412c-8096-cc4e91c9261f","_cell_guid":"64f78a2b-bcee-463f-aec4-815d803a1dd5","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# coding: utf-8\n\n# In[82]:\n\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport seaborn as sns\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\n# In[70]:\n\n\ndf = pd.read_csv(r'\\Users\\Pedro\\Documents\\high_diamond_ranked_10min.csv')\n\n\n# In[71]:\n\n\ndf.head()\n\n\n# In[72]:\n\n\n# check all the variables\ndf.info()\n\n\n# In[73]:\n\n\n\nfig, ax = plt.subplots(figsize=(15,15)) \nsns.heatmap(df.corr(), \n        xticklabels=df.corr().columns,\n        yticklabels=df.corr().columns, ax=ax)\n\n\n# In[74]:\n\n\n# from the correlation matrix we see that gold, kills, assists, gold difference, cs, mosters and toers \n# from the same team are highly correlated. \n# So for our first model, from the mention above we will only use both teams total gold\n# since we will be trying to predict the result for the blue team we will exclude red team first blood \n# we will exclude total experience because its highly correlated with average level which we will use \n\n\n# In[75]:\n\n\ndf_model = df[['blueWins', 'blueWardsPlaced', 'blueWardsDestroyed', 'blueTotalGold', 'blueAvgLevel', 'redWardsPlaced',\n              'redWardsDestroyed', 'redTotalGold', 'redAvgLevel']]\n\n\n# In[76]:\n\n\n# df_model.corr() \n# the calculations show a correlation of 0.61 between total gold and average level but still I want to keep those\n\n\n# In[80]:\n\n\nX = df_model[['blueWardsPlaced', 'blueWardsDestroyed', 'blueTotalGold', 'blueAvgLevel', 'redWardsPlaced',\n              'redWardsDestroyed', 'redTotalGold', 'redAvgLevel']]\nY = df_model[['blueWins']]\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,train_size=0.75, random_state=69)\n\n\n# In[128]:\n\n\nmodel_1 = LogisticRegression(solver='liblinear')\n\n\n# In[129]:\n\n\nmodel_1.fit(X_train,Y_train)\n\n\n# In[130]:\n\n\nmodel_1.predict(X_test)\n\n\n# In[132]:\n\n\n#model_1.predict_proba(X_test)\n\n\n# In[133]:\n\n\nmodel_1.score(X_test,Y_test)\n\n\n# In[134]:\n\n\n# so now we need to add variables possibly some that we thought we would be needing\n# At this point we are using the forward method (we keep adding variables until our model score gets better)\n# Later we maybe we need to remove some turning into a step wise\n# so where are we at the moment? we decided that to predict the outcome for the blue team we needed the TotalGold, WardsPlaced \n# and destroyed by the opposing team and both teams average levels\n# from my current knowledge of the game objectives such as epic monsters such as dragons and heralds give the team that gets them\n# a big advantage so thats what we will include\n\n\n# In[135]:\n\n\ndf_model2 = df[['blueWins', 'blueWardsPlaced', 'blueWardsDestroyed', 'blueDragons', 'blueHeralds', \n                'blueTotalGold', 'blueAvgLevel', 'redWardsPlaced','redWardsDestroyed', 'redTotalGold', 'redAvgLevel']]\n\n\n# In[136]:\n\n\nX2 = df_model2[['blueWardsPlaced', 'blueWardsDestroyed', 'blueDragons', 'blueHeralds', \n                'blueTotalGold', 'blueAvgLevel', 'redWardsPlaced','redWardsDestroyed', 'redTotalGold', 'redAvgLevel']]\nY2 = df_model2[['blueWins']]\n\n\n# In[137]:\n\n\nX2_train, X2_test, Y2_train, Y2_test = train_test_split(X2,Y2,train_size=0.75, random_state=69)\n\n\n# In[138]:\n\n\nmodel_2 = LogisticRegression(solver='liblinear')\n\n\n# \n\n# In[139]:\n\n\nmodel_2.fit(X2_train,Y2_train)\n\n\n# In[140]:\n\n\nmodel_2.predict(X2_test)\n\n\n# In[141]:\n\n\nmodel_2.score(X2_test,Y2_test)\n\n\n# In[142]:\n\n\n# with more of information our model improved\n# now ill include towers destroyed \n# if the model doesnt improve, I'll work with different train, test size groups since\n# we have a lot of data. At the moment I'm doing 75% train, 25% test\n\n\n# In[156]:\n\n\ndf_model3 = df[['blueWins', 'blueWardsPlaced', 'blueWardsDestroyed', 'blueDragons', 'blueHeralds', 'blueTowersDestroyed',\n                'blueTotalGold', 'blueAvgLevel', 'redWardsPlaced','redWardsDestroyed', 'redTowersDestroyed',\n                'redTotalGold', 'redAvgLevel']]\n\nX3 = df_model3[['blueWardsPlaced', 'blueWardsDestroyed', 'blueDragons', 'blueHeralds', 'blueTowersDestroyed',\n                'blueTotalGold', 'blueAvgLevel', 'redWardsPlaced','redWardsDestroyed', 'redTowersDestroyed',\n                'redTotalGold', 'redAvgLevel']]\n\nY3 = df_model3[['blueWins']]\n\n\n# In[174]:\n\n\nX3_train, X3_test, Y3_train, Y3_test = train_test_split(X3,Y3,train_size=0.75, random_state=69)\n\n\n# In[175]:\n\n\nmodel_3=LogisticRegression(solver='liblinear')\n\n\n# In[176]:\n\n\nmodel_3.fit(X3_train,Y3_train)\n\n\n# In[177]:\n\n\nmodel_3.predict(X3_test)\n\n\n# In[178]:\n\n\nmodel_3.score(X3_test, Y3_test)\n\n\n# In[ ]:\n\n\n# so after adding 2 new variable of towers destroyed the model we obtained has the same quality as the previous one, without \n# this variables so for this data set turrets destroyed have no influence on the response \n\n\n# In[182]:\n\n\nwin = df['blueWins']\nrtd = df['redTowersDestroyed']\nbtd = df['blueTowersDestroyed']\n\nwin.corr(rtd)\n\n\n# In[183]:\n\n\nwin.corr(btd)\n\n\n# In[184]:\n\n\n# observing the correlation between bluewins and towers destroyed and the previous results for the models 2 and 3 \n# we can conclude in fact that turrets destroyed have next to no impact on the outcome of the game\n\n# keep the current model, model 2, and work around test, train sets size\n\n# for the next experiments we will have train sizes of 80%, 85%, 90% and 95% and we will keep the model with the best score \n\n\n# In[186]:\n\n\nX21_train, X21_test, Y21_train, Y21_test = train_test_split(X2,Y2,train_size=0.80, random_state=69)\nmodel_21 = LogisticRegression(solver='liblinear')\nmodel_21.fit(X21_train,Y21_train)\nmodel_21.score(X21_test,Y21_test)\n\n\n# In[187]:\n\n\nX22_train, X22_test, Y22_train, Y22_test = train_test_split(X2,Y2,train_size=0.85, random_state=69)\nmodel_22 = LogisticRegression(solver='liblinear')\nmodel_22.fit(X22_train,Y22_train)\nmodel_22.score(X22_test,Y22_test)\n\n\n# In[189]:\n\n\nX23_train, X23_test, Y23_train, Y23_test = train_test_split(X2,Y2,train_size=0.90, random_state=69)\nmodel_23 = LogisticRegression(solver='liblinear')\nmodel_23.fit(X23_train,Y23_train)\nmodel_23.score(X23_test,Y23_test)\n\n\n# In[190]:\n\n\nX24_train, X24_test, Y24_train, Y24_test = train_test_split(X2,Y2,train_size=0.95, random_state=69)\nmodel_24 = LogisticRegression(solver='liblinear')\nmodel_24.fit(X24_train,Y24_train)\nmodel_24.score(X24_test,Y24_test)\n\n\n# In[197]:\n\n\n# Our conclusion is we should use model_2 with a training set of 90% of the data_set\n# Our data size is very big and probably is the case where too much information produces to much noise with a score of 75,8%\n# this model doesn't have a convincing performance\n# The variables were almost handpicked\n\n# so now I'll check a second time the correlation matrix and check for variables that have at least 0.9 in absolute value with\n# the variable blueWins\n\ndf.corr()['blueWins']\n\n\n# In[198]:\n\n\n# looking at this correlations I will try another model using the variables which have higher correlation whether\n# positive or negative with blueWins \n# given the interval of correlations we will use the variables with 30% or higher and -30% or lower\n# blueGoldDifference (this time we'll use this variable instead of blueTotalGold because both of these are highly \n# correlated and blueDGoldDifference has a higher correlation with blueWins), blueExperienceDifference (same thought as \n# blueTotalGold) and finally redDeaths\n\n\n# In[201]:\n\n\ndf_model4 = df[['blueWins', 'blueGoldDiff', 'blueExperienceDiff', 'redDeaths']]\n\nX4 = df_model4[['blueGoldDiff', 'blueExperienceDiff', 'redDeaths']]\nY4 = df_model4[['blueWins']]\n\n\n# In[210]:\n\n\nX4_train, X4_test, Y4_train, Y4_test = train_test_split(X4,Y4,train_size=0.99, random_state=69)\nmodel_4 = LogisticRegression(solver='liblinear')\nmodel_4.fit(X4_train,Y4_train)\nmodel_4.score(X4_test,Y4_test)\n\n\n# In[213]:\n\n\n# some conclusion\n# the goal for this project was to find what variables explain better the outcome for blueWins\n# none of the models obtained were satisfying because of scores between 70 and 80% in the last model. \n# the dataset is too big (40 variables + thousands of entries) which means a lot of variance and a lot of noise \n# this being the reason I didnt start with the complete model\n# a lot of variables show high correlation with each other which makes the task of picking variables somewhat harder \n# Finally if I had to choose a model to make predictions with I would use the last one.\n\n\n# In[ ]:","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}