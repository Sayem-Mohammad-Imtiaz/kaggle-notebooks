{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Assessing the Economic Impact of Wildires"},{"metadata":{},"cell_type":"markdown","source":"## Loading Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd #pandas - for data manipulation\nimport datetime as dt\nfrom dateutil import parser\nnew_data = pd.read_csv('/kaggle/input/wildfire-satellite-data/fire_nrt_M6_156000.csv') #load new data (June 2020->present)\nold_data = pd.read_csv('/kaggle/input/wildfire-satellite-data/fire_archive_M6_156000.csv') #load old data (Sep 2010->June 2020)\nfire_data = pd.concat([old_data.drop('type',axis=1), new_data]) #concatenate old and new data\nfire_data = fire_data.reset_index().drop('index',axis=1)\nfire_data = fire_data[fire_data.satellite != \"Aqua\"]\nfire_data = fire_data.sample(frac=0.1)\nfire_data = fire_data.reset_index().drop(\"index\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Viewing data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Shape of data: {fire_data.shape}\")\nfire_data.rename(columns={\"acq_date\":\"Date\"}, inplace=True)\nfire_data[\"WEI Value\"] = 0\nfire_data['month'] = fire_data['Date'].apply(lambda x:int(x.split('-')[1]))\nfire_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading weekly economic index data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"wei = pd.read_excel('/kaggle/input/weekly-economic-index-wei-federal-reserve-bank/Weekly Economic Index.xlsx')\nwei.drop('WEI as of 7/28/2020',axis=1,inplace=True)\nwei = wei.set_index(\"Date\")\nwei.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merging Data Sets "},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nfor index in tqdm(range(len(fire_data))):\n    fire_date = (fire_data[\"Date\"][index]) \n    fire_date = parser.parse(fire_date)\n    min_wei_date_value = wei.iloc[wei.index.get_loc(fire_date,method='nearest')][\"WEI\"]\n    fire_data.loc[index, \"WEI Value\"] = min_wei_date_value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple Satellite Data Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"Checklist:\n- [X] Remove arbitary `instrument` variable\n- [X] Convert `daynight` variable into numerical"},{"metadata":{"trusted":true},"cell_type":"code","source":"fire_data['daynight'] = fire_data['daynight'].map({'D':0,'N':1})\nfire_data.drop('instrument', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking back with our data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"fire_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction Code"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = fire_data[['latitude','longitude','month','brightness','scan','track',\n               'acq_time','bright_t31','daynight','frp', 'confidence']]\ny = fire_data['WEI Value']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Neural Network Approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from keras.models import Sequential\n# from keras.layers import Dense, Dropout, BatchNormalization\n# model = Sequential()\n# model.add(Dense(32,input_shape=(11,),activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Dense(64,activation='relu'))\n# model.add(Dense(64,activation='relu'))\n# model.add(Dense(64,activation='relu'))\n# model.add(Dense(64,activation='relu'))\n# model.add(Dense(32,activation='relu'))\n# model.add(Dropout(0.2))\n# model.add(Dense(1,activation='linear'))\n# model.compile(optimizer='adam',metrics=['mae'],loss='mse')\n# model.fit(X_train, y_train, epochs=20)\nprint(\"Fail\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.metrics import mean_absolute_error as mae\n# model = RandomForestRegressor(n_estimators = 200, max_depth = 15)\n# model.fit(X_train, y_train)\n# mae(model.predict(X_train), y_train)\n# print(f\"Train MAE: {mae(model.predict(X_train), y_train)}\")\n# print(f\"Test MAE: {mae(model.predict(X_test), y_test)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Find best hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestRegressor\n# parameters = {'n_estimators':[300,500,1000], 'max_depth':[5, 10, 20, 50, 100]}\n# from sklearn.model_selection import GridSearchCV\n# model2 = RandomForestRegressor()\n# clf = GridSearchCV(model2, parameters, cv=3)\n# clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Gradient Boosting Approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    from sklearn.ensemble import GradientBoostingRegressor\n    parameters = {'n_estimators':[100,500,1000], 'max_depth':[3, 5, 15, 50],\n                  \"learning_rate\":[0.05,0.1,0.2]}\n    from sklearn.model_selection import GridSearchCV\n    model2 = GradientBoostingRegressor()\n    clf = GridSearchCV(model2, parameters, n_jobs=-1, cv=3)\n    clf.fit(X_train, y_train)#, verbose=True)\nexcept:\n    from sklearn.ensemble import GradientBoostingRegressor\n    parameters = {'n_estimators':[100,300,500,1000], 'max_depth':[3, 5, 15, 50],\n                  \"learning_rate\":[0.05,0.1,0.2]}\n    from sklearn.model_selection import GridSearchCV\n    model2 = GradientBoostingRegressor()\n    clf = GridSearchCV(model2, parameters, cv=3)\n    clf.fit(X_train, y_train)#, verbose=True)\n\n\n# from sklearn.ensemble import GradientBoostingRegressor\n# model1 = GradientBoostingRegressor(n_estimators = 400, learning_rate=0.1,\n#                                   max_depth = 10, random_state = 0, loss = 'ls')\n# model1.fit(X_train, y_train)\n# print(f\"Train MAE: {mae(model1.predict(X_train), y_train)}\")\n# print(f\"Test MAE: {mae(model1.predict(X_test), y_test)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error as mae\nprint(f\"Train MAE: {mae(clf.predict(X_train), y_train)}\")\nprint(f\"Test MAE: {mae(clf.predict(X_test), y_test)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explanations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import shap\n# explainer = shap.TreeExplainer(model1)\n# shap_values = explainer.shap_values(X_test)\n# shap.summary_plot(shap_values, X_test)#, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import shap\n# explainer = shap.TreeExplainer(model1)\n# shap_values = explainer.shap_values(X_test)\n# shap.summary_plot(shap_values, X_test, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import eli5\n# from eli5.sklearn import PermutationImportance\n# perm = PermutationImportance(model1, random_state=1).fit(X_test, y_test)\n# eli5.show_weights(perm, feature_names = X_test.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from pdpbox import pdp, info_plots\n# import matplotlib.pyplot as plt\n# base_features = X.columns.values.tolist()\n# for column in X.columns:\n#     feat_name = column\n#     pdp_dist = pdp.pdp_isolate(model=model1, dataset=X_test, model_features=base_features, feature=feat_name)\n#     pdp.pdp_plot(pdp_dist, feat_name)\n#     plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# WORK IN PROGRESS"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}