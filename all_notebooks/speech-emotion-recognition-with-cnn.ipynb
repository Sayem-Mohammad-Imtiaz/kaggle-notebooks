{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import libraries \nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport glob \nfrom sklearn.metrics import confusion_matrix\nimport IPython.display as ipd  # To play sound in the notebook\nimport os\nimport sys\nimport warnings\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\nTESS = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nRAV = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nSAVEE = \"/kaggle/input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\nCREMA = \"/kaggle/input/cremad/AudioWAV/\"\n\n# Run one example \ndir_list = os.listdir(SAVEE)\ndir_list[0:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the data location for SAVEE\ndir_list = os.listdir(SAVEE)\n\n# parse the filename to get the emotions\nemotion=[]\npath = []\nfor i in dir_list:\n    if i[-8:-6]=='_a':\n        emotion.append('male_angry')\n    elif i[-8:-6]=='_d':\n        emotion.append('male_disgust')\n    elif i[-8:-6]=='_f':\n        emotion.append('male_fear')\n    elif i[-8:-6]=='_h':\n        emotion.append('male_happy')\n    elif i[-8:-6]=='_n':\n        emotion.append('male_neutral')\n    elif i[-8:-6]=='sa':\n        emotion.append('male_sad')\n    elif i[-8:-6]=='su':\n        emotion.append('male_surprise')\n    else:\n        emotion.append('male_error') \n    path.append(SAVEE + i)\n\n# Now check out the label count distribution \nSAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\nSAVEE_df['source'] = 'SAVEE'\nSAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\nSAVEE_df.labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the well known Librosa library for this task \nfname = SAVEE + 'DC_f11.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets play a happy track\nfname = SAVEE + 'DC_h11.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\nRAV_df.labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pick a fearful track\nfname = RAV + 'Actor_14/03-01-06-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pick a happy track\nfname = RAV + 'Actor_14/03-01-03-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_list = os.listdir(TESS)\ndir_list.sort()\ndir_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = []\nemotion = []\n\nfor i in dir_list:\n    fname = os.listdir(TESS + i)\n    for f in fname:\n        if i == 'OAF_angry' or i == 'YAF_angry':\n            emotion.append('female_angry')\n        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n            emotion.append('female_disgust')\n        elif i == 'OAF_Fear' or i == 'YAF_fear':\n            emotion.append('female_fear')\n        elif i == 'OAF_happy' or i == 'YAF_happy':\n            emotion.append('female_happy')\n        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n            emotion.append('female_neutral')                                \n        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n            emotion.append('female_surprise')               \n        elif i == 'OAF_Sad' or i == 'YAF_sad':\n            emotion.append('female_sad')\n        else:\n            emotion.append('Unknown')\n        path.append(TESS + i + \"/\" + f)\n\nTESS_df = pd.DataFrame(emotion, columns = ['labels'])\nTESS_df['source'] = 'TESS'\nTESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nTESS_df.labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets play a fearful track \nfname = TESS + 'YAF_fear/YAF_dog_fear.wav' \n\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets play a happy track \nfname =  TESS + 'YAF_happy/YAF_dog_happy.wav' \n\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_list = os.listdir(CREMA)\ndir_list.sort()\nprint(dir_list[0:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gender = []\nemotion = []\npath = []\nfemale = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n\nfor i in dir_list: \n    part = i.split('_')\n    if int(part[0]) in female:\n        temp = 'female'\n    else:\n        temp = 'male'\n    gender.append(temp)\n    if part[2] == 'SAD' and temp == 'male':\n        emotion.append('male_sad')\n    elif part[2] == 'ANG' and temp == 'male':\n        emotion.append('male_angry')\n    elif part[2] == 'DIS' and temp == 'male':\n        emotion.append('male_disgust')\n    elif part[2] == 'FEA' and temp == 'male':\n        emotion.append('male_fear')\n    elif part[2] == 'HAP' and temp == 'male':\n        emotion.append('male_happy')\n    elif part[2] == 'NEU' and temp == 'male':\n        emotion.append('male_neutral')\n    elif part[2] == 'SAD' and temp == 'female':\n        emotion.append('female_sad')\n    elif part[2] == 'ANG' and temp == 'female':\n        emotion.append('female_angry')\n    elif part[2] == 'DIS' and temp == 'female':\n        emotion.append('female_disgust')\n    elif part[2] == 'FEA' and temp == 'female':\n        emotion.append('female_fear')\n    elif part[2] == 'HAP' and temp == 'female':\n        emotion.append('female_happy')\n    elif part[2] == 'NEU' and temp == 'female':\n        emotion.append('female_neutral')\n    else:\n        emotion.append('Unknown')\n    path.append(CREMA + i)\n    \nCREMA_df = pd.DataFrame(emotion, columns = ['labels'])\nCREMA_df['source'] = 'CREMA'\nCREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nCREMA_df.labels.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the well known Librosa library for this task \nfname = CREMA + '1012_IEO_HAP_HI.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A fearful track\nfname = CREMA + '1012_IEO_FEA_HI.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([SAVEE_df, RAV_df, TESS_df, CREMA_df], axis = 0)\nprint(df.labels.value_counts())\ndf.head()\ndf.to_csv(\"Data_path.csv\",index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import our libraries\nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport os\nimport IPython.display as ipd  # To play sound in the notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source - RAVDESS; Gender - Female; Emotion - Angry \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_08/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source - RAVDESS; Gender - Male; Emotion - Angry \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_09/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source - RAVDESS; Gender - Female; Emotion - Happy \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_12/03-01-03-01-02-01-12.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source - RAVDESS; Gender - Male; Emotion - Happy \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_11/03-01-03-01-02-02-11.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.waveplot(X, sr=sample_rate)\nplt.title('Audio sampled at 44100 hrz')\n\n# MFCC\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source - RAVDESS; Gender - Female; Emotion - Angry \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_08/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Source - RAVDESS; Gender - Male; Emotion - Angry \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_09/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# audio wave\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source - RAVDESS; Gender - Female; Emotion - happy \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_12/03-01-03-01-02-01-12.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Source - RAVDESS; Gender - Male; Emotion - happy \npath = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/Actor_11/03-01-03-01-02-02-11.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# Plot the two audio waves together\nplt.figure(figsize=(20, 15))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing required libraries \n# Keras\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Other  \nimport librosa\nimport librosa.display\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob \nimport os\nimport pickle\nimport IPython.display as ipd  # To play sound in the notebook","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets pick up the meta-data that we got from our first part of the Kernel\nref = pd.read_csv(\"/kaggle/input/data-pathcsv/Data_path.csv\")\nref.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# Note this takes a couple of minutes (~10 mins) as we're iterating over 4 datasets \ndf = pd.DataFrame(columns=['feature'])\n\n# loop feature extraction over the entire dataset\ncounter=0\nfor index,path in enumerate(ref.path):\n    X, sample_rate = librosa.load(path\n                                  , res_type='kaiser_fast'\n                                  ,duration=2.5\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n    sample_rate = np.array(sample_rate)\n    \n    # mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=sample_rate, \n                                        n_mfcc=13),\n                    axis=0)\n    df.loc[counter] = [mfccs]\n    counter=counter+1   \n\n# Check a few records to make sure its processed successfully\nprint(len(df))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now extract the mean bands to its own feature columns\ndf = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\ndf[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# replace NA with 0\ndf=df.fillna(0)\nprint(df.shape)\ndf[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n                                                    , df.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n# Lets see how the data present itself before normalisation \nX_train[150:160]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lts do data normalization \nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)/std\nX_test = (X_test - mean)/std\n\n# Check the dataset now \nX_train[150:160]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets few preparation steps to get it into the correct format for Keras \nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\nprint(X_train.shape)\nprint(lb.classes_)\n#print(y_train[0:10])\n#print(y_test[0:10])\n\n# Pickel the lb object for future use \nfilename = 'labels'\noutfile = open(filename,'wb')\npickle.dump(lb,outfile)\noutfile.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New model\nmodel = Sequential()\nmodel.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(256, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(14)) # Target class number\nmodel.add(Activation('softmax'))\n# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n# opt = keras.optimizers.Adam(lr=0.0001)\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\nmodel_history=model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save model and weights\nmodel_name = 'Emotion_Model.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Save model and weights at %s ' % model_path)\n\n# Save the model to disk\nmodel_json = model.to_json()\nwith open(\"model_json.json\", \"w\") as json_file:\n    json_file.write(model_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading json and model architecture \njson_file = open('model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = loaded_model.predict(X_test, \n                         batch_size=16, \n                         verbose=1)\n\npreds=preds.argmax(axis=1)\npreds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions \npreds = preds.astype(int).flatten()\npreds = (lb.inverse_transform((preds)))\npreds = pd.DataFrame({'predictedvalues': preds})\n\n# Actual labels\nactual=y_test.argmax(axis=1)\nactual = actual.astype(int).flatten()\nactual = (lb.inverse_transform((actual)))\nactual = pd.DataFrame({'actualvalues': actual})\n\n# Lets combined both of them into a single dataframe\nfinaldf = actual.join(preds)\nfinaldf[170:180]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Write out the predictions to disk\nfinaldf.to_csv('Predictions.csv', index=False)\nfinaldf.groupby('predictedvalues').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the confusion matrix heat map plot\ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n        \n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n        \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Gender recode function\ndef gender(row):\n    if row == 'female_disgust' or 'female_fear' or 'female_happy' or 'female_sad' or 'female_surprise' or 'female_neutral':\n        return 'female'\n    elif row == 'male_angry' or 'male_fear' or 'male_happy' or 'male_sad' or 'male_surprise' or 'male_neutral' or 'male_disgust':\n        return 'male'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the predictions file \nfinaldf = pd.read_csv(\"Predictions.csv\")\nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \n\n# Confusion matrix \nc = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\nprint(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification report \nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modidf = finaldf\nmodidf['actualvalues'] = finaldf.actualvalues.replace({'female_angry':'female'\n                                       , 'female_disgust':'female'\n                                       , 'female_fear':'female'\n                                       , 'female_happy':'female'\n                                       , 'female_sad':'female'\n                                       , 'female_surprise':'female'\n                                       , 'female_neutral':'female'\n                                       , 'male_angry':'male'\n                                       , 'male_fear':'male'\n                                       , 'male_happy':'male'\n                                       , 'male_sad':'male'\n                                       , 'male_surprise':'male'\n                                       , 'male_neutral':'male'\n                                       , 'male_disgust':'male'\n                                      })\n\nmodidf['predictedvalues'] = finaldf.predictedvalues.replace({'female_angry':'female'\n                                       , 'female_disgust':'female'\n                                       , 'female_fear':'female'\n                                       , 'female_happy':'female'\n                                       , 'female_sad':'female'\n                                       , 'female_surprise':'female'\n                                       , 'female_neutral':'female'\n                                       , 'male_angry':'male'\n                                       , 'male_fear':'male'\n                                       , 'male_happy':'male'\n                                       , 'male_sad':'male'\n                                       , 'male_surprise':'male'\n                                       , 'male_neutral':'male'\n                                       , 'male_disgust':'male'\n                                      })\n\nclasses = modidf.actualvalues.unique()  \nclasses.sort() \n\n# Confusion matrix \nc = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\nprint(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification report \nclasses = modidf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"modidf = pd.read_csv(\"Predictions.csv\")\nmodidf['actualvalues'] = modidf.actualvalues.replace({'female_angry':'angry'\n                                       , 'female_disgust':'disgust'\n                                       , 'female_fear':'fear'\n                                       , 'female_happy':'happy'\n                                       , 'female_sad':'sad'\n                                       , 'female_surprise':'surprise'\n                                       , 'female_neutral':'neutral'\n                                       , 'male_angry':'angry'\n                                       , 'male_fear':'fear'\n                                       , 'male_happy':'happy'\n                                       , 'male_sad':'sad'\n                                       , 'male_surprise':'surprise'\n                                       , 'male_neutral':'neutral'\n                                       , 'male_disgust':'disgust'\n                                      })\n\nmodidf['predictedvalues'] = modidf.predictedvalues.replace({'female_angry':'angry'\n                                       , 'female_disgust':'disgust'\n                                       , 'female_fear':'fear'\n                                       , 'female_happy':'happy'\n                                       , 'female_sad':'sad'\n                                       , 'female_surprise':'surprise'\n                                       , 'female_neutral':'neutral'\n                                       , 'male_angry':'angry'\n                                       , 'male_fear':'fear'\n                                       , 'male_happy':'happy'\n                                       , 'male_sad':'sad'\n                                       , 'male_surprise':'surprise'\n                                       , 'male_neutral':'neutral'\n                                       , 'male_disgust':'disgust'\n                                      })\n\nclasses = modidf.actualvalues.unique() \nclasses.sort() \n\n# Confusion matrix \nc = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\nprint(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Classification report \nclasses = modidf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential, Model, model_from_json\nimport matplotlib.pyplot as plt\nimport keras \nimport pickle\nimport wave  # !pip install wave\nimport os\nimport pandas as pd\nimport numpy as np\nimport sys\nimport warnings\nimport librosa\nimport librosa.display\nimport IPython.display as ipd  # To play sound in the notebook\n\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data, sampling_rate = librosa.load('/kaggle/input/happy-audio/Liza-happy-v3.wav')\nipd.Audio('/kaggle/input/happy-audio/Liza-happy-v3.wav')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading json and model architecture \njson_file = open('/kaggle/input/saved-model/model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"/kaggle/input/saved-model/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n\n# the optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading json and model architecture \njson_file = open('/kaggle/input/saved-model/model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"/kaggle/input/saved-model/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n\n# the optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}