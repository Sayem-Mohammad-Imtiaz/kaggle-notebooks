{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nimport pandas as pd # data processing,\nimport tensorflow as tf\nfrom keras.preprocessing.image import load_img,img_to_array,array_to_img\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,BatchNormalization,Activation\nfrom keras.optimizers import RMSprop,Adam,SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom tensorflow import keras\nimport os\nfrom keras.utils.np_utils import to_categorical \nfrom keras.models import Model\nimport cv2\nfrom tensorflow import Tensor\nfrom keras.layers import Input, Conv2D, ReLU, BatchNormalization,\\\n                                    Add, AveragePooling2D, Flatten, Dense\nfrom keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n\nfrom keras.applications import ResNet101,InceptionResNetV2,NASNetLarge,ResNet152,ResNet152V2,Xception,MobileNetV2\n\n\nfrom sklearn.utils import shuffle\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"scrolled":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-13T10:36:50.284736Z","iopub.execute_input":"2021-07-13T10:36:50.285096Z","iopub.status.idle":"2021-07-13T10:36:55.237732Z","shell.execute_reply.started":"2021-07-13T10:36:50.285021Z","shell.execute_reply":"2021-07-13T10:36:55.235632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_csv=pd.read_csv(\"../input/staj-veri-concat/staj veri/male_female.csv\")\ndata_csv[\"path\"]=data_csv[\"file_name\"].map(lambda x: os.path.join(\"../input/staj-veri-concat/staj veri/Reshaped\", \"{}\".format(x))) # paths of images\ny=data_csv[\"gender\"]\ny=to_categorical(y,num_classes=2)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T10:36:55.239371Z","iopub.execute_input":"2021-07-13T10:36:55.239709Z","iopub.status.idle":"2021-07-13T10:36:55.26102Z","shell.execute_reply.started":"2021-07-13T10:36:55.239672Z","shell.execute_reply":"2021-07-13T10:36:55.26021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images=[]\nfor path in data_csv[\"path\"]:\n    image=cv2.imread(path)\n    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    image=cv2.resize(image,(224,224))\n    images.append(image)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T10:36:55.264021Z","iopub.execute_input":"2021-07-13T10:36:55.26428Z","iopub.status.idle":"2021-07-13T10:36:57.073503Z","shell.execute_reply.started":"2021-07-13T10:36:55.264255Z","shell.execute_reply":"2021-07-13T10:36:57.072466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalizasyon i≈ülemi\nimages=np.array(images)   \nimages=images/255","metadata":{"execution":{"iopub.status.busy":"2021-07-13T10:36:57.075187Z","iopub.execute_input":"2021-07-13T10:36:57.075511Z","iopub.status.idle":"2021-07-13T10:36:57.299198Z","shell.execute_reply.started":"2021-07-13T10:36:57.075479Z","shell.execute_reply":"2021-07-13T10:36:57.298339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(images,y,random_state=42,test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T10:36:57.30044Z","iopub.execute_input":"2021-07-13T10:36:57.30094Z","iopub.status.idle":"2021-07-13T10:36:57.466698Z","shell.execute_reply.started":"2021-07-13T10:36:57.300901Z","shell.execute_reply":"2021-07-13T10:36:57.465832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n        horizontal_flip=True,\n        vertical_flip=True,\n        rotation_range=360\n        )\n\ndatagen.fit(x_train)\n\ncheckopointer=tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"./weihts.h5\",\n    monitor=\"val_accuracy\",\n    verbose=1,\n    save_best_only=True,\n    save_freq=\"epoch\"\n)\n\nmodel=Sequential()\n\ninputShape = (224, 224, 3)\n\nmodel.add(Conv2D(64, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\", input_shape=inputShape))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n          \nmodel.add(Conv2D(64, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n          \nmodel.add(Conv2D(128, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n          \nmodel.add(Conv2D(256, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n          \nmodel.add(Conv2D(256, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n\nmodel.add(Conv2D(256, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n          \nmodel.add(Conv2D(256, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n          \nmodel.add(Conv2D(512, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n          \nmodel.add(Conv2D(512, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n\nmodel.add(Conv2D(512, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n          \nmodel.add(Conv2D(512, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n          \nmodel.add(Conv2D(512, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n          \nmodel.add(Conv2D(512, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n\nmodel.add(Conv2D(512, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n          \nmodel.add(Conv2D(512, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n\nmodel.add(Conv2D(512, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n          \nmodel.add(Conv2D(512, (3, 3), padding=\"same\",strides=(1,1), activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Activation(\"relu\"))\n\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(2,activation='sigmoid'))\n\nmodel.summary()\n\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.0005),metrics=['accuracy'])\n\nbatch_size=25\nepochs=250\nhistory=model.fit(datagen.flow(x_train, y_train, \n                               batch_size=batch_size),\n                               steps_per_epoch=len(x_train) / batch_size, \n                               epochs=epochs,\n                               validation_data=(x_test, y_test),\n                               callbacks=[checkopointer])\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-13T10:36:57.468019Z","iopub.execute_input":"2021-07-13T10:36:57.468407Z","iopub.status.idle":"2021-07-13T11:04:40.760912Z","shell.execute_reply.started":"2021-07-13T10:36:57.468365Z","shell.execute_reply":"2021-07-13T11:04:40.759384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_loss=[]\nfor loss in history.history['loss']:\n    if loss<3:\n        history_loss.append(loss)\n    else:\n        history_loss.append(4)\n        \nhistory_val_loss=[]\nfor loss in history.history['val_loss']:\n    if loss<3:\n        history_val_loss.append(loss)\n    else:\n        history_val_loss.append(4)\n\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_loss)\nplt.plot(history_val_loss)\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T11:04:40.765143Z","iopub.execute_input":"2021-07-13T11:04:40.765516Z","iopub.status.idle":"2021-07-13T11:04:41.090735Z","shell.execute_reply.started":"2021-07-13T11:04:40.765475Z","shell.execute_reply":"2021-07-13T11:04:41.089975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%Corrolation table\n\nloaded_model = tf.keras.models.load_model('./weihts.h5')\n\n# confusion matrix\nimport seaborn as sns\n# Predict the values from the validation dataset\nY_pred = loaded_model.predict(x_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T11:04:41.092839Z","iopub.execute_input":"2021-07-13T11:04:41.09317Z","iopub.status.idle":"2021-07-13T11:04:42.790116Z","shell.execute_reply.started":"2021-07-13T11:04:41.093134Z","shell.execute_reply":"2021-07-13T11:04:42.789314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}