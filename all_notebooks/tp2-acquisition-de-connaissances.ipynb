{"cells":[{"metadata":{"_uuid":"993f14d7e5ef2503e7529d35e789e4bff124fde8"},"cell_type":"markdown","source":"# Quelques mots sur notre jeu de données :\n\nNous avons choisi le set de données  Heart Disease UCI. Il est composé de 303 exemples et de 14 attributs (pression artérielle, localisation de la douleur...) numériques (sauf le dernier, qui est binaire), et le but et de deviner à partir des 13 premiers attributs si le patient a une maladie cardiaque ou non."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\n\n#Tout les imports - certains ne sont pas utilisés\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.neural_network import MLPClassifier\n\nimport os\nds = open(\"../input/heart.csv\")\n\n# Initialisation des données\n\nNB_EXEMPLES = 303\nNB_ATTR = 13\n\n# On fait deux tableaux de la taille de nos données initialisés à 0\nX = np.zeros((NB_EXEMPLES, NB_ATTR))\nY = np.zeros(NB_EXEMPLES)\n\n# on remplit nos tableaux avec nos données\nfor i,l in enumerate(ds):\n    if i == 0: continue # on enlève la ligne descriptive\n    t_l = l.rstrip('\\r\\n').split(',')\n    for j,c in enumerate(t_l[0:13]):\n        X[i-1,j] = float(c)\n    Y[i-1] = float (t_l[-1])\n\n# On définit les ensembles d'entrainement et de test\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,shuffle=True)\n\n# On normalise les attributs pour leur donner le meme poids\nscale = MinMaxScaler()\nx_train = scale.fit_transform(x_train)\nx_test = scale.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7021db644ae1e4495bc4fc38ef32ce07b547255"},"cell_type":"markdown","source":"**Pour commencer, nous avons fait un classifieur Random Forest**\nOn a testé différentes valeurs pour chacuns des parametres, à savoir le nombre d'arbres dans la forêt, et la profondeur de chaque arbre.\nLes résultats du Random Forest sont très bons généralement, avec des f-mesures supérieures à 80%. Habituellement, le nombre d'arbres est assez élevé, mais la profondeur est généralement de 2."},{"metadata":{"trusted":true,"_uuid":"92bf2461ede53e768e9f9226a8b0bf260081623e"},"cell_type":"code","source":"# Random Forest\n# On définit le classifieur : ici un random forest\nrf = RandomForestClassifier(random_state=0)\n# On donne les valeurs testées pour chacun des paramètres\nparam_rf={'n_estimators':[10,100,200,300,400], 'max_depth':[1,2,3,4,5]}\n# On créé un classifieur qui teste toutes les combinaisons définies ci-dessus\n# En plus de cela, le paramètre cv permet la cross-validation, et n_jobs parralellise les calculs.\nclf = GridSearchCV(estimator=rf, param_grid=param_rf, cv=10, n_jobs=4, verbose=0, scoring='accuracy')\n\nprint (\"Learn...\")\nclf.fit(x_train,y_train)\npredictions = clf.predict(x_test)\nprint (\"best n is:\", clf.best_estimator_.n_estimators)\nprint (\"best depth is:\", clf.best_estimator_.max_depth)\nprint(classification_report(y_test, predictions,digits=4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea3211129150c8b8586f3a5a5054d582f9331ea1"},"cell_type":"markdown","source":"**Classifieur Bayésien Naïf**\nOn a tenté un apprentissage bayésien naïf : c-a-d avec l'hypothèse que chacun de nos attributs soient indépendants les uns envers les autres. On fait aussi l'hypothèse que chaque attribut numérique suit une répartition gaussienne.\nMalgré une hypothèse naïve discutable, le classifieur montre de bons résultats autour de 80% de f-mesure."},{"metadata":{"trusted":true,"_uuid":"5bed369e6f681dad89e168e31d2f0f74f54f570f"},"cell_type":"code","source":"# Bayesian\n# On définit un classifieur Bayesien\nbay=GaussianNB()\n# On donne les valeurs testées pour le lissage : ici 20 valeurs comprises entre 10e-20 et 10e-1\nparam_b = {'var_smoothing':np.logspace(-20, -1, num=20)}\nclf = GridSearchCV(estimator=bay, param_grid=param_b, cv=10, n_jobs=4, verbose=0, scoring='accuracy')\n\nprint (\"Learn...\")\nclf.fit(x_train,y_train);\npredictions = clf.predict(x_test)\nprint (\"best smoothing is:\", clf.best_estimator_.var_smoothing)\nprint(classification_report(y_test, predictions,digits=4))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d95f77df8dcfde0d537487f7f6a46c2112279e6a"},"cell_type":"markdown","source":"**Machine à vecteurs de support**\nIci, on projete nos exemples dans un espace à 14 dimensions ou plus, avec un noyau de transformation gaussien (le RBF). Le but est de trouver un hyperplan séparateur optimal pour toutes nos données. On a testé différentes combinaisons de paramètres pour le RBF. Les résultats ici sont similaires ou meilleurs au Random Forest. Par contre le temps de calcul est assez long pour trouver le séparateur optimal.\n"},{"metadata":{"trusted":true,"_uuid":"aa169b1a26f1438732de5be3f57cc4a5fdfac5e1"},"cell_type":"code","source":"#SVM\n# On définit un classifieur de machine à vecteurs de support\nk = 'rbf' # noyau gaussien\nsvm = SVC(kernel=k,verbose=False) \n\n# On donne les valeurs testées pour chacun des paramètres\nparam_svm = {'gamma':np.logspace(-10, 10, num=10),'C':np.logspace(-10, 10, num=10)}\nclf = GridSearchCV(estimator=svm, param_grid=param_svm,cv=10,n_jobs=4,verbose=0,scoring='accuracy')\n\nprint (\"Learn...\")\nclf.fit(x_train,y_train);\npredictions = clf.predict(x_test)\nprint (\"best gamma is:\", clf.best_estimator_.gamma)\nprint (\"best C     is:\", clf.best_estimator_.C)\nprint(classification_report(y_test, predictions,digits=4))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bef46e47eb72fd4e8cd4871a029514720d76ff7b"},"cell_type":"markdown","source":"**K plus proches voisins**\nOn teste ici un apprentissage fainéant : nos exemples sont projetés dans un espace à 13 dimensions, et les exemples tests sont projetés à leur tour dans cet espace, et ils sont étiquetés en fonction des k plus proches voisins au sens de la distance de minkowski. Ce classifieur est très rapide, mais présente des résultats généralement en deçà des classifieurs précédents (entre 75 et 80% de f-mesure)"},{"metadata":{"trusted":true,"_uuid":"793325d77a81e6ffe16dcf6cb0bd2150a5014bd3"},"cell_type":"code","source":"#knn\n# On définit un classifieur aux k plus proches voisins\nknn = KNeighborsClassifier()\n\n# On mets dans un tableau les valeurs testées pour les plus proches voisins : entre 1 et 60\ntab = []\nfor i in range (1,60):\n    tab.append(i)\n\n# On donne les valeurs testées pour chacun des paramètres\nparam_knn = {'n_neighbors':tab, 'algorithm' : ['ball_tree', 'kd_tree', 'brute']}\nclf = GridSearchCV(estimator=knn, param_grid=param_knn, cv=10,n_jobs=4,verbose=0,scoring='accuracy')\n\nprint (\"Learn...\")\nclf.fit(x_train,y_train)\nprint (\"best k is:\", clf.best_estimator_.n_neighbors)\nprint (\"best algorithm is:\", clf.best_estimator_.algorithm)\npredictions = clf.predict(x_test)\nprint(classification_report(y_test, predictions,digits=4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8dde79794ae331b99b3e214ca0069c8b5a84907b"},"cell_type":"markdown","source":"**Le réseau de neurones**\nPour classifier les données, nous avons aussi utilisé un réseau de neurones. Nous avons mis une vingtaine de couches cachées, pour que les fonctions non linéaires puissent être exploitées au maximum, ainsi qu'une quinzaine de neurones par couche, car de toute façon nous n'avons que 14 attributs, et plus de neurones par couche ne seront pas utiles. \nLes résultats sont généralement supérieurs au Random Forest (entre 80 et 85% de f-mesure). Les fonctions d'activations qui semblent les meilleures pour ce problème sont le tanh ainsi que le RELU (deux fonctions non linéaires), et adam semble être le solveur le plus optimal.\n"},{"metadata":{"trusted":true,"_uuid":"f0ef567fe1d518b420d0242ae2256bcbce346ace"},"cell_type":"code","source":"# neural network\n# On créé un réseau de neurones\nnn = MLPClassifier(hidden_layer_sizes=(15, 20)) \n\n# On définit plusieurs fonctions d'activation, et plusieurs optimisateurs\nparam_nn = {'activation':['identity', 'logistic', 'tanh', 'relu'], 'solver' : ['lbfgs', 'sgd', 'adam']}\nclf = GridSearchCV(estimator=nn, param_grid=param_nn, cv=10,n_jobs=4,verbose=0,scoring='accuracy')\n\nprint (\"Learn...\")\nclf.fit(x_train,y_train);\npredictions = clf.predict(x_test)\nprint (\"best activation function is:\", clf.best_estimator_.activation)\nprint (\"best solver is:\", clf.best_estimator_.solver)\nprint(classification_report(y_test, predictions,digits=4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7c15b601c04ddfba80b07cd8f1a520efe758375"},"cell_type":"markdown","source":"**En conclusion**\nOn peut dire que les méthodes utilisées sont  plus fiables que l'aléatoire pur. Les méthodes proposées avaient de très bons résultats sur notre jeu de données. Mention spéciale pour le réseau de neurones, qui aligne de meilleurs résultats comparés aux autres classifieurs. On peut noter tout de même que le Random Forest donne de bons résultats comparé au temps de calcul."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}