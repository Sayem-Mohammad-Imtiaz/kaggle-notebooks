{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-28T23:39:53.508431Z","iopub.execute_input":"2021-05-28T23:39:53.508897Z","iopub.status.idle":"2021-05-28T23:39:53.523194Z","shell.execute_reply.started":"2021-05-28T23:39:53.508811Z","shell.execute_reply":"2021-05-28T23:39:53.521941Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This article covers two fundamental techniques of feature selection:\nFilter Methods and Wrapper Methods, as well as how EDA would guide decision making in feature selection.\n  *Please visit [Feature Selection and EDA](https://towardsdatascience.com/feature-selection-and-eda-in-python-c6c4eb1058a3?source=post_stats_page-------------------------------------) for detailed code walkthrough*.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:39:53.525014Z","iopub.execute_input":"2021-05-28T23:39:53.525322Z","iopub.status.idle":"2021-05-28T23:39:54.52608Z","shell.execute_reply.started":"2021-05-28T23:39:53.525293Z","shell.execute_reply":"2021-05-28T23:39:54.524688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import dataset\ndf = pd.read_csv(\"../input/credit-card-customers/BankChurners.csv\")\ndf = df.drop([\"CLIENTNUM\",\"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\", \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\"], axis = 1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:39:54.530559Z","iopub.execute_input":"2021-05-28T23:39:54.530882Z","iopub.status.idle":"2021-05-28T23:39:54.64219Z","shell.execute_reply.started":"2021-05-28T23:39:54.530854Z","shell.execute_reply":"2021-05-28T23:39:54.641091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# describe data\ndf.describe(include = \"all\")","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:39:54.644594Z","iopub.execute_input":"2021-05-28T23:39:54.645048Z","iopub.status.idle":"2021-05-28T23:39:54.758717Z","shell.execute_reply.started":"2021-05-28T23:39:54.645007Z","shell.execute_reply":"2021-05-28T23:39:54.75728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing values\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:39:54.760319Z","iopub.execute_input":"2021-05-28T23:39:54.760748Z","iopub.status.idle":"2021-05-28T23:39:54.778645Z","shell.execute_reply.started":"2021-05-28T23:39:54.760707Z","shell.execute_reply":"2021-05-28T23:39:54.777219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n* univariate analysis\n* correlation analysis\n* bivariate analysis","metadata":{}},{"cell_type":"code","source":"# populate list of numerical and categorical variables\nnum_list = []\ncat_list = []\n\nfor column in df:\n    if is_numeric_dtype(df[column]):\n        num_list.append(column)\n    elif is_string_dtype(df[column]):\n        cat_list.append(column)\n        \n\nprint(\"numeric:\", num_list)\nprint(\"categorical:\", cat_list)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:39:54.7804Z","iopub.execute_input":"2021-05-28T23:39:54.780835Z","iopub.status.idle":"2021-05-28T23:39:54.788608Z","shell.execute_reply.started":"2021-05-28T23:39:54.780792Z","shell.execute_reply":"2021-05-28T23:39:54.78757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# univariate analysis\nfor column in df:\n    plt.figure(column, figsize = (6,6))\n    plt.title(column)\n    if is_numeric_dtype(df[column]):\n        df[column].plot(kind = 'hist')\n    elif is_string_dtype(df[column]):\n        # show only the TOP 10 value count in each categorical data\n        df[column].value_counts()[:10].plot(kind = 'bar')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-28T23:39:54.789995Z","iopub.execute_input":"2021-05-28T23:39:54.790281Z","iopub.status.idle":"2021-05-28T23:39:58.402199Z","shell.execute_reply.started":"2021-05-28T23:39:54.790253Z","shell.execute_reply":"2021-05-28T23:39:58.401204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# correlation analysis\nplt.figure(figsize = (20,20))\ncorrelation = df.corr()\nsns.heatmap(correlation, cmap = \"GnBu\", annot = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:39:58.403682Z","iopub.execute_input":"2021-05-28T23:39:58.403996Z","iopub.status.idle":"2021-05-28T23:39:59.825853Z","shell.execute_reply.started":"2021-05-28T23:39:58.403965Z","shell.execute_reply":"2021-05-28T23:39:59.824475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns=[\"Avg_Open_To_Buy\", \"Total_Trans_Ct\", \"Customer_Age\"])","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:39:59.827688Z","iopub.execute_input":"2021-05-28T23:39:59.828116Z","iopub.status.idle":"2021-05-28T23:39:59.837445Z","shell.execute_reply.started":"2021-05-28T23:39:59.82807Z","shell.execute_reply":"2021-05-28T23:39:59.835708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grouped bar chart\nfor i in range(0, len(cat_list)):\n    primary_cat = cat_list[i]\n    plt.figure (figsize = (8,8))\n    chart = sns.countplot(\n        data = df,\n        x= primary_cat, \n        hue= \"Attrition_Flag\",\n        palette = 'GnBu',\n    )","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:55:36.080718Z","iopub.execute_input":"2021-05-28T23:55:36.081139Z","iopub.status.idle":"2021-05-28T23:55:37.212064Z","shell.execute_reply.started":"2021-05-28T23:55:36.081105Z","shell.execute_reply":"2021-05-28T23:55:37.210839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# box plot\nfor j in range(0, len(num_list)):\n    cat = \"Attrition_Flag\"\n    num = num_list[j]\n    plt.figure (figsize = (5,5))\n    sns.boxplot( x = cat, y = num, data = df, palette = \"GnBu\")","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:40:00.680911Z","iopub.execute_input":"2021-05-28T23:40:00.6813Z","iopub.status.idle":"2021-05-28T23:40:02.566044Z","shell.execute_reply.started":"2021-05-28T23:40:00.681264Z","shell.execute_reply":"2021-05-28T23:40:02.565121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Filter Methods\n* **chi square, anova and mutual information**\n* how does the accuracy changes by chosen score functions and number of variables?\n* how does each score function rate each features?","metadata":{}},{"cell_type":"code","source":"# encode columns\nfrom sklearn.preprocessing import LabelEncoder\n\nfor i in cat_list:\n    df[i] = LabelEncoder().fit_transform(df[i])","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:41:34.08314Z","iopub.execute_input":"2021-05-28T23:41:34.083617Z","iopub.status.idle":"2021-05-28T23:41:34.294827Z","shell.execute_reply.started":"2021-05-28T23:41:34.083569Z","shell.execute_reply":"2021-05-28T23:41:34.293493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature scaling\nfrom mlxtend.preprocessing import minmax_scaling\ndf_scaled = minmax_scaling(df, columns = df.columns.values)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:41:34.296735Z","iopub.execute_input":"2021-05-28T23:41:34.297163Z","iopub.status.idle":"2021-05-28T23:41:34.342419Z","shell.execute_reply.started":"2021-05-28T23:41:34.297118Z","shell.execute_reply":"2021-05-28T23:41:34.34151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny = df_scaled[\"Attrition_Flag\"]\nX = df_scaled.iloc[:, 1:]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\nprint(X.columns.values)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:41:34.344072Z","iopub.execute_input":"2021-05-28T23:41:34.3445Z","iopub.status.idle":"2021-05-28T23:41:34.428417Z","shell.execute_reply.started":"2021-05-28T23:41:34.344456Z","shell.execute_reply":"2021-05-28T23:41:34.42714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nreg = LogisticRegression()\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\n\nprint(X_train.shape)\n\nprint(metrics.accuracy_score(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:41:34.429943Z","iopub.execute_input":"2021-05-28T23:41:34.430242Z","iopub.status.idle":"2021-05-28T23:41:34.632482Z","shell.execute_reply.started":"2021-05-28T23:41:34.430214Z","shell.execute_reply":"2021-05-28T23:41:34.631356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\n# define feature selection - filter methods function\ndef feature_selection(variable_counts,score_function):\n    selection_model = SelectKBest(score_func=score_function, k=variable_counts) # create a selection model based on the score function\n    selection = selection_model.fit(X_train, y_train) # fit the selection to the data\n    features_selected = X_train.columns[selection.get_support()] # get the selected variables\n    X_train_selected = selection_model.fit_transform(X_train, y_train) # transform the trainig data based on selected features\n    \n    # calculate the accuracy of prediction based on selected features\n    reg = LogisticRegression()\n    reg.fit(X_train_selected, y_train)\n    y_pred = reg.predict(X_test[features_selected])\n    \n    return metrics.accuracy_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:41:34.634085Z","iopub.execute_input":"2021-05-28T23:41:34.634743Z","iopub.status.idle":"2021-05-28T23:41:34.74483Z","shell.execute_reply.started":"2021-05-28T23:41:34.634695Z","shell.execute_reply":"2021-05-28T23:41:34.743603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create accuracy chart\naccuracy_df = pd.DataFrame({\"features count\": [], \"accuracy\": [], \"score function\": []})\nfunction_list = [chi2, f_classif,mutual_info_classif]\nfunction_name = [\"chi square\", \"anova\", \"mutual information\"]\n\nfor j in range(len(function_list)): \n    func = function_list[j]\n    func_name = function_name[j]\n    for i in range(1, len(df.columns) - 1):\n        accuracy = feature_selection(i, func)\n        new_record = {\"features count\": round(i), \"accuracy\": round(accuracy, 3), \"score function\": func_name}\n        accuracy_df = accuracy_df.append(new_record, ignore_index = True)\n\nprint(accuracy_df)\nplt.figure(figsize = (10, 10))\nsns.lineplot(data = accuracy_df, x = 'features count', y = 'accuracy', hue = 'score function', palette = \"GnBu\")","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:41:34.746916Z","iopub.execute_input":"2021-05-28T23:41:34.747872Z","iopub.status.idle":"2021-05-28T23:41:57.191247Z","shell.execute_reply.started":"2021-05-28T23:41:34.747821Z","shell.execute_reply":"2021-05-28T23:41:57.190378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfor i in range(len(function_list)):\n    function = function_list[i]\n    name = function_name[i]\n    selection_model = SelectKBest(score_func = function, k = 8).fit(X_train, y_train)\n    feature_score = pd.DataFrame({\"features\": X_train.columns.values, \"scores\": selection_model.scores_})\n    print(feature_score.sort_values(by = ['scores'], ascending = False))\n    print(feature_score.plot(x = \"features\", kind = 'bar', title = name))","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:41:57.192591Z","iopub.execute_input":"2021-05-28T23:41:57.192885Z","iopub.status.idle":"2021-05-28T23:41:58.610564Z","shell.execute_reply.started":"2021-05-28T23:41:57.192856Z","shell.execute_reply":"2021-05-28T23:41:58.609235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wrapper Method\n* **forward selection and backward elimination**\n* how does the accuracy change by the number of variables?","metadata":{}},{"cell_type":"code","source":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\n# forward selection\naccuracy_df = pd.DataFrame({\"features count\": [], \"accuracy\": [], \"score function\": []})\nfor i in range(1, len(df.columns) - 1):\n    sfs = SFS(LogisticRegression(),\n              k_features = i,\n              forward = True, \n              scoring = 'accuracy')\n    \n    X_train_selected = sfs.fit_transform(X_train, y_train)\n    reg = LogisticRegression()\n    reg.fit(X_train_selected, y_train)\n    y_pred = reg.predict(X_test[list(sfs.k_feature_names_)])\n    sfs_accuracy = metrics.accuracy_score(y_test, y_pred)\n    \n    new_record = {\"features count\": round(i), \"accuracy\": round(sfs_accuracy, 3), \"score function\": \"forward selection\"}\n    accuracy_df = accuracy_df.append(new_record, ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:41:58.6124Z","iopub.execute_input":"2021-05-28T23:41:58.612859Z","iopub.status.idle":"2021-05-28T23:46:06.669076Z","shell.execute_reply.started":"2021-05-28T23:41:58.612815Z","shell.execute_reply":"2021-05-28T23:46:06.667826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# backward elimination\nfor i in range(1, len(df.columns) - 1):\n    sbs = SFS(LogisticRegression(),\n              k_features = i,\n              forward = False,\n              # floating = False,\n              scoring = 'accuracy')\n\n    X_train_selected = sbs.fit_transform(X_train, y_train)\n    reg = LogisticRegression()\n    reg.fit(X_train_selected, y_train)\n    y_pred = reg.predict(X_test[list(sbs.k_feature_names_)])\n    sbs_accuracy = metrics.accuracy_score(y_test, y_pred)\n\n    new_record = {\"features count\": round(i), \"accuracy\": round(sbs_accuracy, 3), \"score function\": \"backward elimination\"}\n    accuracy_df = accuracy_df.append(new_record, ignore_index = True)\n    \n    \nprint(accuracy_df)\nplt.figure(figsize = (10, 10))\nsns.lineplot(data = accuracy_df, x = 'features count', y = 'accuracy', hue = 'score function', palette = \"GnBu\")","metadata":{"execution":{"iopub.status.busy":"2021-05-28T23:46:06.675371Z","iopub.execute_input":"2021-05-28T23:46:06.676192Z","iopub.status.idle":"2021-05-28T23:54:06.654817Z","shell.execute_reply.started":"2021-05-28T23:46:06.676145Z","shell.execute_reply":"2021-05-28T23:54:06.653699Z"},"trusted":true},"execution_count":null,"outputs":[]}]}