{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Retail Dataset"},{"metadata":{},"cell_type":"markdown","source":"This dataset contains lot of historical sales data. It was extracted from a Brazilian top retailer and has many SKUs and many stores. The data was transformed to protect the identity of the retailer."},{"metadata":{},"cell_type":"markdown","source":"## 1. Load "},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport calendar\nimport datetime  \nfrom datetime import datetime as dt\nfrom pandas import DataFrame, Series\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail = pd.read_csv('mock_kaggle.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail['date'] = pd.to_datetime(retail['date'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"## 2. Explorer Dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"retail.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail['dow'] = retail['date'].dt.dayofweek\nretail['day_of_week'] = retail['date'].dt.day_name()\n#retail['weeknum'] = retail['date'].dt.week\nretail['weeknum'] = retail['date'].dt.strftime('%W')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### - Day of Week Analysis"},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_dow = retail.groupby(['day_of_week','dow']).mean()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"retail['sale']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_dow","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_dow.sort_values(\"dow\", axis = 0, ascending = True, \n                 inplace = True, na_position ='last') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_dow","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = retail_dow.drop(['stock'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df[\"sale\"].plot(label=\"Average sale\",  title = \"Relationship between price(t_avg) and sales(t_avg) - day of week\", legend=True, figsize=(13,7))\ndf[\"price\"].plot(label=\"Average price\", legend=True, secondary_y=True)\n## df.plot(figsize=(12,8),secondary_y=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## According to above dual weekly graph, we could find there is an inverse relationship between sales and price"},{"metadata":{"trusted":false},"cell_type":"code","source":"retail","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail['year'] = retail['date'].dt.year","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_dow2 = retail.groupby(['year','weeknum']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_dow2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df2 = retail_dow2.drop(['stock','dow'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df2[\"sale\"].plot(label=\"Average sale\",  title = \"Relationship between price(avg) and sales(avg) - weekly \", legend=True, figsize=(13,7))\ndf2[\"price\"].plot(label=\"Average price\", legend=True, secondary_y=True)\n## df.plot(figsize=(12,8),secondary_y=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"# 3. Moving Average"},{"metadata":{"trusted":false},"cell_type":"code","source":"retail['7-day'] = retail['price'].rolling(7).mean()\nretail['14-day'] = retail['price'].rolling(14).mean()\nretail['21-day'] = retail['price'].rolling(21).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"with plt.style.context('ggplot'):\n    plt.figure(figsize = (20,10))\n    plt.plot(retail.price[-120:], label = 'Real Price')\n    plt.plot(retail['7-day'][-120:], label = '7 Day Moving Average')\n    plt.plot(retail['14-day'][-120:], label = '14 Day Moving Average')\n    plt.plot(retail['21-day'][-120:], label = '21 Day Moving Average')\n    plt.legend(loc =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail['7-day'] = retail['sale'].rolling(7).mean()\nretail['14-day'] = retail['sale'].rolling(14).mean()\nretail['21-day'] = retail['sale'].rolling(21).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"with plt.style.context('ggplot'):\n    plt.figure(figsize = (20,10))\n    plt.plot(retail.sale[-120:], label = 'Real Price')\n    plt.plot(retail['7-day'][-120:], label = '7 Day Moving Average')\n    plt.plot(retail['14-day'][-120:], label = '14 Day Moving Average')\n    plt.plot(retail['21-day'][-120:], label = '21 Day Moving Average')\n    plt.legend(loc =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dicky Fuller Test"},{"metadata":{},"cell_type":"markdown","source":"#### Check for stationarity using the Dicky Fuller Test for all three datasets\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\nresult1 = adfuller(retail['price'])\nprint(result1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"# 4.1 Simple Exponential Smoothing "},{"metadata":{"trusted":false},"cell_type":"code","source":"def simple_exp_smooth(d,extra_periods,alpha):  \n\n\n  d = np.array(d)  # Transform the input into a numpy array  \n\n  cols = len(d)  # Historical period length  \n\n  d = np.append(d,[np.nan]*extra_periods)  # Append np.nan into the demand array to cover future periods  \n\n\n\n  f = np.full(cols+extra_periods,np.nan)  # Forecast array  \n\n  f[0] = d[0]  # initialization of first forecast  \n\n    \n\n  # Create all the t+1 forecasts until end of historical period  \n\n  for t in range(1,cols+1):  \n\n    f[t] = alpha*d[t-1]+(1-alpha)*f[t-1]  \n    f[cols+1:] = f[t]  # Forecast for all extra periods  \n\n\n  df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Error\":f-d}) \n  return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2 Double Exponential Smoothing"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Double Exponential Smoothing\ndef double_exp_smooth(d,extra_periods,alpha,beta):  \n\n    d = np.array(d)  # Transform the input into a numpy array  \n    cols = len(d)  # Historical period length  \n    d = np.append(d,[np.nan]*extra_periods)  # Append np.nan into the demand array to cover future periods  \n# Creation of the level, trend, and forecast arrays\n    f= np.full(cols+extra_periods,np.nan)  # Forecast array  \n    a = np.full(cols+extra_periods,np.nan)\n    b = np.full(cols+extra_periods,np.nan)\n# Level and trend initialization \n    a[0] = d[0]\n    b[0] = d[1] -d[0]\n\n    \n  # Create all the t+1 forecasts until end of historical period  \n\n    for t in range(1,cols):  \n        f[t] = a[t-1]+ b[t-1]\n        a[t] = alpha*d[t] + (1-alpha)*(a[t-1] + b[t-1])\n        b[t] = beta*(a[t] - a[t-1]) + (1-beta)* b[t-1]\n\n# Forecast for all extra periods  \n    for t in range(cols, cols+extra_periods):\n        f[t] = a[t-1] + b[t-1]\n        a[t] = f[t]\n        b[t] = b[t-1]\n\n\n    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Level\": a, \"Trend\": b, \"Error\":f-d}) \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df1=simple_exp_smooth(retail['sale'],7,1)\ndf1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# For KPI Calculation\nMAE = df1[\"Error\"].abs().mean()  \nprint(\"MAE:\",round(MAE,2)) \nRMSE = np.sqrt((df1[\"Error\"]**2).mean())\nprint(\"RMSE:\",round(RMSE,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#For Plotting\ndf1.index.name = \"Periods\"\ndf1[[\"Demand\",\"Forecast\"]].plot(figsize=(20,8),title=\"Simple exponential smoothing\",style=[\"-\",\"--\"])  \nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df11=double_exp_smooth(retail['sale'],7,1,0.05)\ndf11.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"MAE = df11[\"Error\"].abs().mean()  \nprint(\"MAE:\",round(MAE,2)) \nRMSE = np.sqrt((df11[\"Error\"]**2).mean())\nprint(\"RMSE:\",round(RMSE,2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#For Plotting\ndf11.index.name = \"Periods\"\ndf11[[\"Demand\",\"Forecast\"]].plot(figsize=(20,8),title=\"Double exponential smoothing\",style=[\"-\",\"--\"])  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>\n<blank>\n<blank>"},{"metadata":{},"cell_type":"markdown","source":"# 5. Machine Learning"},{"metadata":{"trusted":false},"cell_type":"code","source":"retail","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predict based on weekly average"},{"metadata":{"trusted":false},"cell_type":"code","source":"retail[\"year_week\"] = retail[\"year\"].map(str) + '-' + retail[\"weeknum\"].map(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_ml = retail.groupby([\"year_week\"]).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_ml","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_ml = retail_ml.drop(['stock','year','7-day','14-day','21-day','dow','price'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_ml.sort_values(\"year_week\", axis = 0, ascending = True, \n                 inplace = True, na_position ='last') \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_ml","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_lstm = retail_ml","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_ml = pd.DataFrame(retail_ml)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_ml","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_ml = df_ml.sort_values(by = ['year_week'], ascending = [True])\nretail_ml = retail_ml.sort_values(by = ['year_week'], ascending = [True])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create 53 weeks of lag values to predict current observation\n# Shift of 53 weeks in this case\nfor i in range(53,0,-1):\n    df_ml[['t-'+str(i)]] = retail_ml.shift(i)\nprint(df_ml)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_ml","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail.to_csv('output.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_ml = df_ml.drop(['sale'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_ml","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a new subsetted dataframe, removing Nans from first 12 rows\ndf_ml2 = df_ml[53:]\nprint(df_ml2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_ml2.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Split Data into dependent(target) and independent(features) variables\n\nretail_ml = df_ml2.values\n# Lagged variables (features) and original time series data (target)\nX2 = retail_ml[:,0:]  # slice all rows and start with column 0 and go up to but not including the last column\ny2 = retail_ml[:,0:]  # slice all rows and last column, essentially separating out 't' column","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Columns t-1 to t-12, which are the lagged variables\nX2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Column t, which is the original time series\n# Give first 10 values of target variable, time series\ny2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We are using 80-20 and 70-30 split."},{"metadata":{"trusted":false},"cell_type":"code","source":"# Target(Y) Train-Test split\n\nY2 = y2\ntraintarget_size = int(len(Y2) * 0.80)   # Set split\nprint(traintarget_size)\ntrain_target, test_target = Y2[:traintarget_size], Y2[traintarget_size:len(Y2)]\n\nprint('Observations for Target: %d' % (len(Y2)))\nprint('Training Observations for Target: %d' % (len(train_target)))\nprint('Testing Observations for Target: %d' % (len(test_target)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y2[traintarget_size:len(Y2)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Features(X) Train-Test split\n\ntrainfeature_size = int(len(X2) * 0.80)\ntrain_feature, test_feature = X2[:trainfeature_size], X2[trainfeature_size:len(X2)]\nprint('Observations for feature: %d' % (len(X2)))\nprint('Training Observations for feature: %d' % (len(train_feature)))\nprint('Testing Observations for feature: %d' % (len(test_feature)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_feature","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Benchmark Model"},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n  \nreg = LinearRegression() # Create a linear regression object\n  \nreg = reg.fit(train_feature, train_target) # Fit it to the training data\n  \n# Create two predictions for the training and test sets\ntrain_prediction = reg.predict(train_feature)\ntest_prediction = reg.predict(test_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compute the MAE for both the training and test sets\n\nMAE_train=np.mean(abs(train_target-train_prediction))/np.mean(train_target)\nprint(\"Tree on train set MAE%:\", round(MAE_train*100,3))\n\nMAE_test=np.mean(abs(test_target-test_prediction))/np.mean(test_target)\nprint(\"Tree on test set MAE%:\", round(MAE_test*100,3))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Decision Tree Regression Model\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Create a decision tree regression model with default arguments\ndecision_tree_retail = DecisionTreeRegressor()  # max_depth not set\n\n# Fit the model to the training features and targets\ndecision_tree_retail.fit(train_feature, train_target)\n\n# Check the score on train and test\nprint(decision_tree_retail.score(train_feature, train_target))\nprint(decision_tree_retail.score(test_feature,test_target))  # predictions are horrible if negative value, no relationship if 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Find Best Max Depth\n\n# Loop through a few different max depths and check the performance\n# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n\nfor d in [2, 3,4, 5,7,8,10]:\n    # Create the tree and fit it\n    decision_tree_retail = DecisionTreeRegressor(max_depth=d)\n    decision_tree_retail.fit(train_feature, train_target)\n\n    # Print out the scores on train and test\n    print('max_depth=', str(d))\n    print(decision_tree_retail.score(train_feature, train_target))\n    print(decision_tree_retail.score(test_feature, test_target), '\\n')  # You want the test score to be positive\n    \n# R-square for train and test scores are below. \n;","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### max_depth = 10 "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot predicted against actual values\n\nfrom matplotlib import pyplot as plt\n\n# Use the best max_depth \ndecision_tree_retail = DecisionTreeRegressor(max_depth=5) # Fill in best max depth score here\ndecision_tree_retail.fit(train_feature, train_target)\n\n# Predict values for train and test\ntrain_prediction = decision_tree_retail.predict(train_feature)\n\nMAE_train=np.mean(abs(train_target-train_prediction))/np.mean(train_target)\nprint(\"Tree on train set MAE%:\", round(MAE_train*100,1))\n\n\ntest_prediction = decision_tree_retail.predict(test_feature)\n\nMAE_test=np.mean(abs(test_target-test_prediction))/np.mean(test_target)\nprint(\"Tree on test set MAE%:\", round(MAE_test*100,1))\n\n# Scatter the predictions vs actual values, orange is predicted\nplt.scatter(train_prediction, train_target, label='train')  # blue \nplt.scatter(test_prediction, test_target, label='test')  # orange\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Model"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Random Forest Model\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create the random forest model and fit to the training data\nrfr = RandomForestRegressor(n_estimators=200)\nrfr.fit(train_feature, train_target)\n\n# Look at the R^2 scores on train and test\nprint(rfr.score(train_feature, train_target))\nprint(rfr.score(test_feature, test_target))  # Try to attain a positive value","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import ParameterGrid\nimport numpy as np\n\n# Create a dictionary of hyperparameters to search\n# n_estimators is the number of trees in the forest. The larger the better, but also takes longer it will take to compute. \n# Run grid search\n#grid = {'n_estimators': [200], 'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10], 'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'random_state': [13]}\ngrid = {'n_estimators': [200], 'max_depth': [10], 'max_features': [3], 'random_state': [13]}\ntest_scores = []\n\n# Loop through the parameter grid, set the hyperparameters, and save the scores\nfor g in ParameterGrid(grid):\n    rfr.set_params(**g)  # ** is \"unpacking\" the dictionary\n    rfr.fit(train_feature, train_target)\n    test_scores.append(rfr.score(test_feature, test_target))\n\n# Find best hyperparameters from the test score and print\nbest_idx = np.argmax(test_scores)\nprint(test_scores[best_idx], ParameterGrid(grid)[best_idx])  \n\n# The best test score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We use max_depth : 10"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Use the best hyperparameters from before to fit a random forest model\nrfr = RandomForestRegressor(n_estimators=200, max_depth=8, max_features = 3, random_state=13)\nrfr.fit(train_feature, train_target)\n\n# Make predictions with our model\ntrain_prediction = rfr.predict(train_feature)\ntest_prediction = rfr.predict(test_feature)\n\n# Create a scatter plot with train and test actual vs predictions\nplt.scatter(train_target, train_prediction, label='train')\nplt.scatter(test_target, test_prediction, label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compute the MAE for both the training and test sets\n\nMAE_train=np.mean(abs(train_target-train_prediction))/np.mean(train_target)\nprint(\"Tree on train set MAE%:\", round(MAE_train*100,1))\n\nMAE_test=np.mean(abs(test_target-test_prediction))/np.mean(test_target)\nprint(\"Tree on test set MAE%:\", round(MAE_test*100,1))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Random Forest Feature Importance\n# get column names\ndf_ml.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_ml","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get feature importances from our random forest model\nimportances = rfr.feature_importances_\n\n# Get the index of importances from greatest importance to least\nsorted_index = np.argsort(importances)[::-1]\nx = range(len(importances))\n\n# Create tick labels \nfeature_names = ['t-53','t-52','t-51','t-50','t-49','t-48','t-47','t-46','t-45','t-44','t-43','t-42','t-41','t-40','t-39','t-38','t-37','t-36','t-35','t-34','t-33','t-32','t-31','t-30','t-29','t-28','t-27','t-26','t-25','t-24','t-23','t-22','t-21','t-20','t-19','t-18','t-17','t-16','t-15','t-14','t-13','t-12','t-11','t-10','t-9','t-8','t-7','t-6','t-5','t-4','t-3','t-2','t-1']\nlabels = np.array(feature_names)[sorted_index]\nplt.figure(figsize=(15, 3))\nplt.bar(x, importances[sorted_index], tick_label=labels, width = 0.4)\n\n# Rotate tick labels to vertical\n\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"importances","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"## 70:30"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Target(Y) Train-Test split\n\nY3 = y2\ntraintarget_size = int(len(Y3) * 0.7)   # Set split\nprint(traintarget_size)\ntrain_target, test_target = Y3[:traintarget_size], Y3[traintarget_size:len(Y2)]\n\nprint('Observations for Target: %d' % (len(Y3)))\nprint('Training Observations for Target: %d' % (len(train_target)))\nprint('Testing Observations for Target: %d' % (len(test_target)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Features(X) Train-Test split\n\ntrainfeature_size = int(len(X2) * 0.7)\ntrain_feature, test_feature = X2[:trainfeature_size], X2[trainfeature_size:len(X2)]\nprint('Observations for feature: %d' % (len(X2)))\nprint('Training Observations for feature: %d' % (len(train_feature)))\nprint('Testing Observations for feature: %d' % (len(test_feature)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n  \nreg = LinearRegression() # Create a linear regression object\n  \nreg = reg.fit(train_feature, train_target) # Fit it to the training data\n  \n# Create two predictions for the training and test sets\ntrain_prediction = reg.predict(train_feature)\ntest_prediction = reg.predict(test_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compute the MAE for both the training and test sets\n\nMAE_train=np.mean(abs(train_target-train_prediction))/np.mean(train_target)\nprint(\"Tree on train set MAE%:\", round(MAE_train*100,3))\n\nMAE_test=np.mean(abs(test_target-test_prediction))/np.mean(test_target)\nprint(\"Tree on test set MAE%:\", round(MAE_test*100,3))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Decision Tree Regression Model\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Create a decision tree regression model with default arguments\ndecision_tree_avocado = DecisionTreeRegressor()  # max_depth not set\n\n# Fit the model to the training features and targets\ndecision_tree_avocado.fit(train_feature, train_target)\n\n# Check the score on train and test\nprint(decision_tree_avocado.score(train_feature, train_target))\nprint(decision_tree_avocado.score(test_feature,test_target))  # predictions are horrible if negative value, no relationship if 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Find Best Max Depth\n\n# Loop through a few different max depths and check the performance\n# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n\nfor d in [2, 3,4, 5,7,8,10]:\n    # Create the tree and fit it\n    decision_tree_avocado = DecisionTreeRegressor(max_depth=d)\n    decision_tree_avocado.fit(train_feature, train_target)\n\n    # Print out the scores on train and test\n    print('max_depth=', str(d))\n    print(decision_tree_avocado.score(train_feature, train_target))\n    print(decision_tree_avocado.score(test_feature, test_target), '\\n')  # You want the test score to be positive\n    \n# R-square for train and test scores are below. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot predicted against actual values\n\nfrom matplotlib import pyplot as plt\n\n# Use the best max_depth \ndecision_tree_avocado = DecisionTreeRegressor(max_depth=5) # Fill in best max depth score here\ndecision_tree_avocado.fit(train_feature, train_target)\n\n# Predict values for train and test\ntrain_prediction = decision_tree_avocado.predict(train_feature)\n\nMAE_train=np.mean(abs(train_target-train_prediction))/np.mean(train_target)\nprint(\"Tree on train set MAE%:\", round(MAE_train*100,1))\n\n\ntest_prediction = decision_tree_avocado.predict(test_feature)\n\nMAE_test=np.mean(abs(test_target-test_prediction))/np.mean(test_target)\nprint(\"Tree on test set MAE%:\", round(MAE_test*100,1))\n\n# Scatter the predictions vs actual values, orange is predicted\nplt.scatter(train_prediction, train_target, label='train')  # blue \nplt.scatter(test_prediction, test_target, label='test')  \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forrest"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import ParameterGrid\nimport numpy as np\n\n# Create a dictionary of hyperparameters to search\n# n_estimators is the number of trees in the forest. The larger the better, but also takes longer it will take to compute. \n# Run grid search\n#grid = {'n_estimators': [200], 'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10], 'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'random_state': [13]}\ngrid = {'n_estimators': [200], 'max_depth': [8], 'max_features': [3], 'random_state': [13]}\ntest_scores = []\n\n# Loop through the parameter grid, set the hyperparameters, and save the scores\nfor g in ParameterGrid(grid):\n    rfr.set_params(**g)  # ** is \"unpacking\" the dictionary\n    rfr.fit(train_feature, train_target)\n    test_scores.append(rfr.score(test_feature, test_target))\n\n# Find best hyperparameters from the test score and print\nbest_idx = np.argmax(test_scores)\nprint(test_scores[best_idx], ParameterGrid(grid)[best_idx])  \n\n# The best test score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Compute the MAE for both the training and test sets\n\nMAE_train=np.mean(abs(train_target-train_prediction))/np.mean(train_target)\nprint(\"Tree on train set MAE%:\", round(MAE_train*100,1))\n\nMAE_test=np.mean(abs(test_target-test_prediction))/np.mean(test_target)\nprint(\"Tree on test set MAE%:\", round(MAE_test*100,1))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<blank>\n    <blank>\n        <blank>"},{"metadata":{},"cell_type":"markdown","source":"# 6. LSTM"},{"metadata":{"trusted":false},"cell_type":"code","source":"import math\nimport matplotlib.pyplot as plt\nfrom statsmodels.tools.eval_measures import rmse\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport warnings \nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_lstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_lstm['year_week'] = retail_lstm.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_lstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_lstm.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_lstm.index = retail_lstm.year_week.str.replace('-', '')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_lstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_lstm.rename(index={'year_week': 'year_weeks'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_lstm = retail_lstm.drop(['year_week'], axis=1) \n# (['year_week'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"retail_lstm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train, test = retail_lstm[:-12],retail_lstm[-12:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"scalar =MinMaxScaler()\nscalar.fit(train)\ntrain = scalar.transform(train)\ntest = scalar.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_input = 12\nn_features = 1\ngenerator = TimeseriesGenerator(train, train, n_input, batch_size =6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Epoch Start"},{"metadata":{},"cell_type":"markdown","source":"# Epoch 50"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = Sequential()\n# Adding the input layer and LSTM layer\nmodel.add(LSTM(200, activation= 'relu', input_shape =(n_input, n_features)))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1))\nmodel.compile(optimizer ='adam', loss='mse')\nhistory = model.fit_generator(generator, epochs = 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_list =[]\nbatch = train[-n_input:].reshape(1, n_input, n_features)\nfor i in range(n_input):\n    pred_list.append(model.predict(batch)[0])\n    batch = np.append(batch[:, 1:,:], [[pred_list[i]]], axis=1)\n\npred_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(history.history['loss'])\nplt.title('model loss - Epoch 50')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_predict= pd.DataFrame(scalar.inverse_transform(pred_list), index= retail_lstm[-n_input:].index, columns =['Predictions'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test = pd.concat([retail_lstm, df_predict], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.xlabel(\"Week\")\nplt.ylabel(\"Retail Sales\")\nplt.title(\"Retail Sales - Epoch 250\")\nplt.plot(df_test.index, df_test['sale'])\nplt.plot(df_test.index, df_test['Predictions'], color ='r')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RMSE (Epoch 50)"},{"metadata":{"trusted":false},"cell_type":"code","source":"testScore = math.sqrt(mean_squared_error(df_test.iloc[-12:,0].values, df_test.iloc[-12:,1].values))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"## Epoch 100"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = Sequential()\n# Adding the input layer and LSTM layer\nmodel.add(LSTM(200, activation= 'relu', input_shape =(n_input, n_features)))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1))\nmodel.compile(optimizer ='adam', loss='mse')\nhistory = model.fit_generator(generator, epochs =100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_list =[]\nbatch = train[-n_input:].reshape(1, n_input, n_features)\nfor i in range(n_input):\n    pred_list.append(model.predict(batch)[0])\n    batch = np.append(batch[:, 1:,:], [[pred_list[i]]], axis=1)\n\npred_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Loss"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(history.history['loss'])\nplt.title('model loss - Epoch 100)')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_predict= pd.DataFrame(scalar.inverse_transform(pred_list), index= retail_lstm[-n_input:].index, columns =['Predictions'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test = pd.concat([retail_lstm, df_predict], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.xlabel(\"Week\")\nplt.ylabel(\"Retail Sales\")\nplt.title(\"Retail Sales - Epoch 100\")\nplt.plot(df_test.index, df_test['sale'])\nplt.plot(df_test.index, df_test['Predictions'], color ='r')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test.iloc[-12:,1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test.iloc[-12:,0].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RMSE (Epoch 100)"},{"metadata":{"trusted":false},"cell_type":"code","source":"testScore = math.sqrt(mean_squared_error(df_test.iloc[-12:,0].values, df_test.iloc[-12:,1].values))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Epoch 150"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = Sequential()\n# Adding the input layer and LSTM layer\nmodel.add(LSTM(200, activation= 'relu', input_shape =(n_input, n_features)))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1))\nmodel.compile(optimizer ='adam', loss='mse')\nhistory = model.fit_generator(generator, epochs =150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_list =[]\nbatch = train[-n_input:].reshape(1, n_input, n_features)\nfor i in range(n_input):\n    pred_list.append(model.predict(batch)[0])\n    batch = np.append(batch[:, 1:,:], [[pred_list[i]]], axis=1)\n\npred_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Loss - Epoch 150"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(history.history['loss'])\nplt.title('model loss - Epoch 150')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_predict= pd.DataFrame(scalar.inverse_transform(pred_list), index= retail_lstm[-n_input:].index, columns =['Predictions'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test = pd.concat([retail_lstm, df_predict], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.xlabel(\"Week\")\nplt.ylabel(\"Retail Sales\")\nplt.title(\"Retail Sales - Epoch 150\")\nplt.plot(df_test.index, df_test['sale'])\nplt.plot(df_test.index, df_test['Predictions'], color ='r')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RMSE (Epoch 150)"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"testScore = math.sqrt(mean_squared_error(df_test.iloc[-12:,0].values, df_test.iloc[-12:,1].values))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Epoch 200"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = Sequential()\n# Adding the input layer and LSTM layer\nmodel.add(LSTM(200, activation= 'relu', input_shape =(n_input, n_features)))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1))\nmodel.compile(optimizer ='adam', loss='mse')\nhistory=model.fit_generator(generator, epochs =200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_list =[]\nbatch = train[-n_input:].reshape(1, n_input, n_features)\nfor i in range(n_input):\n    pred_list.append(model.predict(batch)[0])\n    batch = np.append(batch[:, 1:,:], [[pred_list[i]]], axis=1)\n\npred_list\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Loss - Epoch 200"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(history.history['loss'])\nplt.title('model loss - Epoch 200')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_predict= pd.DataFrame(scalar.inverse_transform(pred_list), index= retail_lstm[-n_input:].index, columns =['Predictions'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test = pd.concat([retail_lstm, df_predict], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_predict.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.xlabel(\"Week\")\nplt.ylabel(\"Retail Sales\")\nplt.title(\"Retail Sales - Epoch 200\")\nplt.plot(df_test.index, df_test['sale'])\nplt.plot(df_test.index, df_test['Predictions'], color ='r')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RMSE (Epoch 200)"},{"metadata":{"trusted":false},"cell_type":"code","source":"testScore = math.sqrt(mean_squared_error(df_test.iloc[-12:,0].values, df_test.iloc[-12:,1].values))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Epoch 250"},{"metadata":{"trusted":false},"cell_type":"code","source":"model = Sequential()\n# Adding the input layer and LSTM layer\nmodel.add(LSTM(200, activation= 'relu', input_shape =(n_input, n_features)))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(1))\nmodel.compile(optimizer ='adam', loss='mse')\nhistory = model.fit_generator(generator, epochs =250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_list =[]\nbatch = train[-n_input:].reshape(1, n_input, n_features)\nfor i in range(n_input):\n    pred_list.append(model.predict(batch)[0])\n    batch = np.append(batch[:, 1:,:], [[pred_list[i]]], axis=1)\n\npred_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.plot(history.history['loss'])\nplt.title('model loss - Epoch 250')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_predict= pd.DataFrame(scalar.inverse_transform(pred_list), index= retail_lstm[-n_input:].index, columns =['Predictions'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test = pd.concat([retail_lstm, df_predict], axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(20,5))\nplt.xlabel(\"Week\")\nplt.ylabel(\"Retail Sales\")\nplt.title(\"Retail Sales - Epoch 250\")\nplt.plot(df_test.index, df_test['sale'])\nplt.plot(df_test.index, df_test['Predictions'], color ='r')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RMSE (Epoch 250)"},{"metadata":{"trusted":false},"cell_type":"code","source":"testScore = math.sqrt(mean_squared_error(df_test.iloc[-12:,0].values, df_test.iloc[-12:,1].values))\nprint('Test Score: %.2f RMSE' % (testScore))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<blank>"},{"metadata":{},"cell_type":"markdown","source":"<blank>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}