{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nfrom skimage import io, transform\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom IPython.display import display\n\n# Filter harmless warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TEST YOUR VERSION OF PILLOW\n# Run this cell. If you see a picture of a cat you're all set!\nwith Image.open('/kaggle/input/hackerearth-dl-challengeautotag-images-of-gala/dataset/Train Images/image8247.jpg') as im:\n    display(im)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain=pd.read_csv('/kaggle/input/hackerearth-dl-challengeautotag-images-of-gala/dataset/train.csv')\ndftest=pd.read_csv('/kaggle/input/hackerearth-dl-challengeautotag-images-of-gala/dataset/test.csv')\ntarget_map={'Food':0, 'misc':1, 'Attire':2, 'Decorationandsignage':3}\n# train\ndftrain['Class'].unique()\ndftrain['Class']=dftrain['Class'].map(target_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftrain[:5000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=dftrain[:5000]\ntest=dftrain[5000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start by creating a list\nimg_sizes = []\nrejected = []\n\nfor item in train.Image:\n    try:\n        with Image.open('/kaggle/input/hackerearth-dl-challengeautotag-images-of-gala/dataset/Train Images/'+item) as img:\n            img_sizes.append(img.size)\n    except:\n        rejected.append(item)\n        \nprint(f'Images:  {len(img_sizes)}')\nprint(f'Rejects: {len(rejected)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_img = pd.DataFrame(img_sizes)\n\n# Run summary statistics on image widths\ndf_img[0].describe(),df_img[1].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This tells us the shortest width is 80, the shortest height is 20, the largest width and height are 80 and 235, and that most images have more than 90 pixels per side. This is useful for deciding on an input size. We'll see in the next section that 224x224 will work well for our purposes (we'll take advantage of some pre-trained models that use this size!)"},{"metadata":{},"cell_type":"markdown","source":"## Image Preprocessing\nAny network we define requires consistent input data. That is, the incoming image files need to have the same number of channels (3 for red/green/blue), the same depth per channel (0-255), and the same height and width. This last requirement can be tricky. How do we transform an 800x450 pixel image into one that is 224x224? In the theory lectures we covered the following:\n* <a href='https://en.wikipedia.org/wiki/Aspect_ratio_(image)'><strong>aspect ratio</strong></a>: the ratio of width to height (16:9, 1:1, etc.) An 800x450 pixel image has an aspect ration of 16:9. We can change the aspect ratio of an image by cropping it, by stretching/squeezing it, or by some combination of the two. In both cases we lose some information contained in the original. Let's say we crop 175 pixels from the left and right sides of our 800x450 image, resulting in one that's 450x450.\n* <strong>scale</strong>: Once we've attained the proper aspect ratio we may need to scale an image up or down to fit our input parameters. There are several libraries we can use to scale a 450x450 image down to 224x224 with minimal loss.\n* <a href=''><strong>normalization</strong></a>: when images are converted to tensors, the [0,255] rgb channels are loaded into range [0,1]. We can then normalize them using the generally accepted values of mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]. For the curious, these values were obtained by the PyTorch team using a random 10,000 sample of <a href='http://www.image-net.org/'>ImageNet</a> images. There's a good discussion of this <a href='https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/22'>here</a>, and the original source code can be found <a href='https://github.com/soumith/imagenet-multiGPU.torch/blob/master/donkey.lua#L154'>here</a>."},{"metadata":{},"cell_type":"markdown","source":"## Transformations\nBefore defining our Convolutional Network, let's look at a sample image and perform various transformations on it to see their effect."},{"metadata":{"trusted":true},"cell_type":"code","source":"img1=Image.open('/kaggle/input/hackerearth-dl-challengeautotag-images-of-gala/dataset/Train Images/image9233.jpg')\nprint(img1.size)\ndisplay(img1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is how jupyter displays the original .jpg image. Note that size is given as (width, height).<br>\nLet's look at a single pixel:"},{"metadata":{"trusted":true},"cell_type":"code","source":"r, g, b = img1.getpixel((0, 0))\nprint(r,g,b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The pixel at position [0,0] (upper left) of the source image has an rgb value of (90,95,98). This corresponds to <font style=\"background-color:rgb(90,95,98)\">this color </font><br>\nGreat! Now let's look at some specific transformations.\n### <a href='https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor'><tt>transforms.ToTensor()</tt></a>\nConverts a PIL Image or numpy.ndarray (HxWxC) in the range [0, 255] to a <tt>torch.FloatTensor</tt> of shape (CxHxW) in the range [0.0, 1.0]"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor()\n])\nim = transform(img1)\nprint(im.shape)\nplt.imshow(np.transpose(im.numpy(), (1, 2, 0)));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is the same image converted to a tensor and displayed using matplotlib. Note that the torch dimensions follow [channel, height, width]<br><br>\nPyTorch automatically loads the [0,255] pixel channels to [0,1]:<br><br>\n$\\frac{242}{255}=0.94\\quad\\frac{229}{255}=0.89\\quad\\frac{213}{255}=0.83$\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"im[:,0,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a href='https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Resize'><tt>transforms.Resize(<em>size</em>)</tt></a>\nIf size is a sequence like (h, w), the output size will be matched to this. If size is an integer, the smaller edge of the image will be matched to this number.<br>i.e, if height > width, then the image will be rescaled to (size * height / width, size)"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize(224), \n    transforms.ToTensor()\n])\nim = transform(img1)\nprint(im.shape)\nplt.imshow(np.transpose(im.numpy(), (1, 2, 0)));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### <a href='https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.CenterCrop'><tt>transforms.CenterCrop(<em>size</em>)</tt></a>\nIf size is an integer instead of sequence like (h, w), a square crop of (size, size) is made.\n\n\n## Other affine transformations\nAn <a href='https://en.wikipedia.org/wiki/Affine_transformation'><em>affine</em></a> transformation is one that preserves points and straight lines. Examples include rotation, reflection, and scaling. For instance, we can double the effective size of our training set simply by flipping the images.\n### <a href='https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomHorizontalFlip'><tt>transforms.RandomHorizontalFlip(<em>p=0.5</em>)</tt></a>\nHorizontally flip the given PIL image randomly with a given probability.\n\n### Scaling is done using <a href='https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Resize'><tt>transforms.Resize(<em>size</em>)</tt></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=1),  # normally we'd set p=0.5\n    transforms.RandomRotation(30),\n    transforms.Resize(224),\n    transforms.CenterCrop(224), \n    transforms.ToTensor()\n])\nim = transform(img1)\nprint(im.shape)\nplt.imshow(np.transpose(im.numpy(), (1, 2, 0)));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"___\n## Normalization\nOnce the image has been loaded into a tensor, we can perform normalization on it. This serves to make convergence happen quicker during training. The values are somewhat arbitrary - you can use a mean of 0.5 and a standard deviation of 0.5 to convert a range of [0,1] to [-1,1], for example.<br>However, <a href='https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/22'>research has shown</a> that mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225] work well in practice.\n\n### <a href='https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize'><tt>transforms.Normalize(<em>mean, std</em>)</tt></a>\nGiven mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input tensor\n### $\\quad\\textrm {input[channel]} = \\frac{\\textrm{input[channel] - mean[channel]}}{\\textrm {std[channel]}}$"},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\nim = transform(img1)\nprint(im.shape)\nplt.imshow(np.transpose(im.numpy(), (1, 2, 0)));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Recall that before normalization, the upper-leftmost tensor had pixel values of <tt>[0.3529, 0.3725, 0.3843]</tt>.<br>\nWith normalization we subtract the channel mean from the input channel, then divide by the channel std.<br><br>\n$\\frac{(0.3529-0.485)}{0.229}=-0.5767\\quad\\frac{(0.3725-0.456)}{0.224}=-0.3725\\quad\\frac{(0.3843-0.406)}{0.225}=-0.0964$<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# After normalization:\nim[:,0,0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optional: De-normalize the images\nTo see the image back in its true colors, we can apply an inverse-transform to the tensor being displayed."},{"metadata":{"trusted":true},"cell_type":"code","source":"inv_normalize = transforms.Normalize(\n    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n    std=[1/0.229, 1/0.224, 1/0.225]\n)\nim_inv = inv_normalize(im)\nplt.figure(figsize=(12,4))\nplt.imshow(np.transpose(im_inv.numpy(), (1, 2, 0)));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,4))\nplt.imshow(np.transpose(im.numpy(), (1, 2, 0)));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define transforms\nIn the previous section we looked at a variety of transforms available for data augmentation (rotate, flip, etc.) and normalization.<br>\nHere we'll combine the ones we want, including the <a href='https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/22'>recommended normalization parameters</a> for mean and std per channel."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transform = transforms.Compose([\n        transforms.RandomRotation(10),      # rotate +/- 10 degrees\n        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n        transforms.Resize(224),             # resize shortest side to 224 pixels\n        transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])\n\ntest_transform = transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prepare train and test sets, loaders\n\n### Custom data set loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomDataset(Dataset):\n\n    def __init__(self, imgz,labels=None, root_dir='', transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.images = imgz\n        self.labels = labels\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.root_dir,self.images[idx])\n        image = Image.open(img_name).convert('RGB')\n#         plt.imshow(image)\n        if self.transform:\n            image = self.transform(image)\n        if self.labels is not None:\n            return image, self.labels[idx]\n        else:\n            return image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = CustomDataset(imgz=train['Image'].values,labels=train['Class'].values,root_dir='/kaggle/input/hackerearth-dl-challengeautotag-images-of-gala/dataset/Train Images/',transform=train_transform)\ntest_data = CustomDataset(imgz=test['Image'].values,labels=test['Class'].values,root_dir='/kaggle/input/hackerearth-dl-challengeautotag-images-of-gala/dataset/Train Images/', transform=test_transform)\ntotal_test_data = CustomDataset(imgz=dftest['Image'].values,root_dir='/kaggle/input/hackerearth-dl-challengeautotag-images-of-gala/dataset/Test Images/', transform=test_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntorch.manual_seed(42)\ntrain_loader = DataLoader(train_data, batch_size=4, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=4, shuffle=True)\ntotal_test_loader = DataLoader(total_test_data, batch_size=4, shuffle=False)\n\nclass_names = train.Class.unique()\n\nprint(class_names)\nprint(f'Training images available: {len(train_data)}')\nprint(f'Testing images available:  {len(test_data)}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Total Train and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"total_train_data = CustomDataset(imgz=dftrain['Image'].values,labels=dftrain['Class'].values,root_dir='/kaggle/input/hackerearth-dl-challengeautotag-images-of-gala/dataset/Train Images/',transform=train_transform)\ntotal_train_loader = DataLoader(total_train_data, batch_size=4, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display a batch of images\nTo verify that the training loader selects cat and dog images at random, let's show a batch of loaded images.<br>\nRecall that imshow clips pixel values <0, so the resulting display lacks contrast. We'll apply a quick inverse transform to the input tensor so that images show their \"true\" colors."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.__getitem__(2644)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grab the first batch of 10 images\n\ntarget_map_inv={0:'Food', 1:'misc', 2:'Attire', 3:'Decorationandsignage'}\nfrom torchvision.utils import make_grid\nfor images,labels in train_loader: \n    break\n\n# Print the labels\nprint('Label:', labels.numpy())\n\nprint('Class:', *np.array([target_map_inv[i.tolist()] for i in labels]))\n\nim = make_grid(images,nrow=8)  # the default nrow is 8\n\n# Inverse normalize the images\ninv_normalize = transforms.Normalize(\n    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n    std=[1/0.229, 1/0.224, 1/0.225]\n)\nim_inv = inv_normalize(im)\n\n# Print the images\nplt.figure(figsize=(12,4))\nplt.imshow(np.transpose(im_inv.numpy(), (1, 2, 0)));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define the model\nWe'll start by using a model similar to the one we applied to the CIFAR-10 dataset, except that here we have a binary classification (2 output channels, not 10). Also, we'll add another set of convolution/pooling layers."},{"metadata":{},"cell_type":"markdown","source":"Simple CNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvolutionalNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 3, 1)\n        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n        self.fc1 = nn.Linear(54*54*16, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 4)\n\n    def forward(self, X):\n        X = F.relu(self.conv1(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = F.relu(self.conv2(X))\n        X = F.max_pool2d(X, 2, 2)\n        X = X.view(-1, 54*54*16)\n        X = F.relu(self.fc1(X))\n        X = F.relu(self.fc2(X))\n        X = self.fc3(X)\n        return F.log_softmax(X, dim=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-info\"><strong>Why <tt>(54x54x16)</tt>?</strong><br>\nWith 224 pixels per side, the kernels and pooling layers result in $\\;(((224-2)/2)-2)/2 = 54.5\\;$ which rounds down to 54 pixels per side.</div>"},{"metadata":{},"cell_type":"markdown","source":"### Instantiate the model, define loss and optimization functions\nWe're going to call our model \"CNNmodel\" to differentiate it from an \"AlexNetmodel\" we'll use later."},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.manual_seed(101)\nCNNmodel = ConvolutionalNetwork()\ncriterion = nn.CrossEntropyLoss()\n\nif torch.cuda.is_available():\n    CNNmodel = CNNmodel.cuda()\n    criterion = criterion.cuda()\n    \n    \n# optimizer = torch.optim.Adam(CNNmodel.parameters(), lr=0.001)\noptimizer = torch.optim.SGD(CNNmodel.parameters(), lr=0.001, momentum=0.9)\nCNNmodel","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at the trainable parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_parameters(model):\n    params = [p.numel() for p in model.parameters() if p.requires_grad]\n    for item in params:\n        print(f'{item:>8}')\n    print(f'________\\n{sum(params):>8}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_parameters(CNNmodel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape,test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef test_model(net, testloader):\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for b, (X_test, y_test) in enumerate(test_loader):\n            if torch.cuda.is_available():\n                X_test = X_test.cuda()\n                y_test = y_test.cuda()\n            \n            inputs, labels = X_test, y_test\n            outputs = net(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n \n    print('Accuracy of the network on test images: %0.3f %%' % (100 * correct / total))\ndef train_model(net, trainloader,epochs):\n    for epoch in range(epochs): # no. of epochs\n        running_loss = 0\n#         exp_lr_scheduler.step()\n        for b, (X_train, y_train) in enumerate(train_loader):\n            # data pixels and labels to GPU if available\n            b+=1\n            \n            if torch.cuda.is_available():\n                X_train = X_train.cuda()\n                y_train = y_train.cuda()\n            inputs, labels = X_train, y_train\n            # set the parameter gradients to zero\n            optimizer.zero_grad()\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            # propagate the loss backward\n            loss.backward()\n            # update the gradients\n            optimizer.step()\n \n            running_loss += loss.item()\n        print('[Epoch %d] loss: %.3f' %\n                      (epoch + 1, running_loss/len(trainloader)))\n        \n \n    print('Done Training')\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate model performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(train_losses, label='training loss')\n# plt.plot(test_losses, label='validation loss')\n# plt.title('Loss at the end of each epoch')\n# plt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot([t for t in train_correct], label='training accuracy')\n# plt.plot([t for t in test_correct], label='validation accuracy')\n# plt.title('Accuracy at the end of each epoch')\n# plt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(test_correct)\n# print(f'Test accuracy: {test_correct[-1].item()*100/3000:.3f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Download a pretrained model\nTorchvision has a number of proven models available through <a href='https://pytorch.org/docs/stable/torchvision/models.html#classification'><tt><strong>torchvision.models</strong></tt></a>:\n<ul>\n<li><a href=\"https://arxiv.org/abs/1404.5997\">AlexNet</a></li>\n<li><a href=\"https://arxiv.org/abs/1409.1556\">VGG</a></li>\n<li><a href=\"https://arxiv.org/abs/1512.03385\">ResNet</a></li>\n<li><a href=\"https://arxiv.org/abs/1602.07360\">SqueezeNet</a></li>\n<li><a href=\"https://arxiv.org/abs/1608.06993\">DenseNet</a></li>\n<li><a href=\"https://arxiv.org/abs/1512.00567\">Inception</a></li>\n<li><a href=\"https://arxiv.org/abs/1409.4842\">GoogLeNet</a></li>\n<li><a href=\"https://arxiv.org/abs/1807.11164\">ShuffleNet</a></li>\n<li><a href=\"https://arxiv.org/abs/1801.04381\">MobileNet</a></li>\n<li><a href=\"https://arxiv.org/abs/1611.05431\">ResNeXt</a></li>\n</ul>\nThese have all been trained on the <a href='http://www.image-net.org/'>ImageNet</a> database of images. Our only task is to reduce the output of the fully connected layers from (typically) 1000 categories to just 2.\n\nTo access the models, you can construct a model with random weights by calling its constructor:<br>\n<pre>resnet18 = models.resnet18()</pre>\nYou can also obtain a pre-trained model by passing pretrained=True:<br>\n<pre>resnet18 = models.resnet18(pretrained=True)</pre>\nAll pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].\n\nFeel free to investigate the different models available. Each one will be downloaded to a cache directory the first time they're accessed - from then on they'll be available locally.\n\nFor its simplicity and effectiveness, we'll use AlexNet:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision import datasets, transforms, models # add models to the list\nAlexNetmodel = models.alexnet(pretrained=True)\nAlexNetmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for param in AlexNetmodel.parameters():\n    param.requires_grad = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.manual_seed(42)\n# AlexNetmodel.fc = nn.Sequential(nn.Linear(9216, 1024),\nAlexNetmodel.classifier = nn.Sequential(nn.Linear(9216, 1024),\n                                 nn.ReLU(),\n                                 nn.Dropout(0.4),\n                                 nn.Linear(1024, 4),\n                                 nn.LogSoftmax(dim=1))\n\n# for param in AlexNetmodel.fc.parameters():\n#     param.requires_grad = True\nAlexNetmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These are the TRAINABLE parameters:\ncount_parameters(AlexNetmodel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.optim import lr_scheduler\n\n\ncriterion = nn.CrossEntropyLoss()\nif torch.cuda.is_available():\n    AlexNetmodel = AlexNetmodel.cuda()\n    criterion = criterion.cuda()\n    \n# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n    \noptimizer = torch.optim.Adam(AlexNetmodel.classifier.parameters(), lr=0.0001)\n# optimizer = torch.optim.SGD(AlexNetmodel.classifier.parameters(), lr=0.0001, momentum=0.9)\n# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and Validates model performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model(AlexNetmodel,train_loader,5)\ntest_model(AlexNetmodel,test_loader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict on Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediciton(net, data_loader):\n    test_pred = torch.LongTensor()\n    for i, data in enumerate(data_loader):\n        if torch.cuda.is_available():\n            data = data.cuda()\n        output = net(data)\n        pred = output.cpu().data.max(1, keepdim=True)[1]\n        test_pred = torch.cat((test_pred, pred), dim=0)\n    \n    return test_pred\n\n\n\ntrain_model(AlexNetmodel,total_train_loader,15)\n\ntest_pred = prediciton(AlexNetmodel, total_test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred.numpy().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftest['Class']=test_pred.numpy()\ndftest.Class=dftest.Class.map(target_map_inv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftest.to_csv('s6_alexnet.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dftest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img1=Image.open('/kaggle/input/hackerearth-dl-challengeautotag-images-of-gala/dataset/Test Images/image3442.jpg')\nprint(img1.size)\ndisplay(img1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}