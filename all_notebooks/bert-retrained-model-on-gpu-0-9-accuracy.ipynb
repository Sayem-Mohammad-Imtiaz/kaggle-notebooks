{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## How quickly train BERT model to solve binary classification task","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nimport transformers\nimport pandas as pd \nimport numpy as np\nfrom transformers import get_linear_schedule_with_warmup\nfrom transformers import AdamW\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import logging\nlogging.set_verbosity_warning()","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:26:01.051317Z","iopub.execute_input":"2021-08-07T13:26:01.051721Z","iopub.status.idle":"2021-08-07T13:26:07.736203Z","shell.execute_reply.started":"2021-08-07T13:26:01.051635Z","shell.execute_reply":"2021-08-07T13:26:07.735367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this is the maximum number of tokens in the sentence\nMAX_LEN = 512\n# batch sizes for pytorch dataloaders\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 2\nEPOCHS = 10\nTRAINING_FILE = \"../input/imdb-sentiment-10k-reviews-binary-classification/imdb_10K_sentimnets_reviews.csv\"","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:26:19.007013Z","iopub.execute_input":"2021-08-07T13:26:19.007354Z","iopub.status.idle":"2021-08-07T13:26:19.011704Z","shell.execute_reply.started":"2021-08-07T13:26:19.007325Z","shell.execute_reply":"2021-08-07T13:26:19.010559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load pretained BERT model\n#### More models and docs here https://huggingface.co/models","metadata":{}},{"cell_type":"code","source":"TOKENIZER = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:26:40.500333Z","iopub.execute_input":"2021-08-07T13:26:40.500732Z","iopub.status.idle":"2021-08-07T13:26:48.867719Z","shell.execute_reply.started":"2021-08-07T13:26:40.500697Z","shell.execute_reply":"2021-08-07T13:26:48.866822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Class that helps load data for Pytorch\n##### more here https://pytorch.org/docs/stable/data.html#dataset-types","metadata":{}},{"cell_type":"code","source":"class BertDataSetPytorch:\n    def __init__(self, review, sentiment):\n        self.review = review\n        self.sentiment = sentiment\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n    def __len__(self):\n        return len(self.review)\n    def __getitem__(self, item):\n        review = str(self.review[item])\n        review = \" \".join(review.split())\n        # Tokenize with padding and max lenght of the sentence\n        inputs = self.tokenizer.encode_plus(review, None, add_special_tokens=True, truncation=True, max_length=self.max_len, pad_to_max_length=True)\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        \n        return {\"ids\": torch.tensor(ids, dtype=torch.long),\n                \"mask\": torch.tensor(mask, dtype=torch.long),\n                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n                \"targets\": torch.tensor(self.sentiment[item], dtype=torch.float)}\n# Test\ntest_data_ = pd.read_csv(TRAINING_FILE, nrows=3) # just 3 reviews\ntest_bert_class_ = BertDataSetPytorch(test_data_.review, test_data_.sentiment)\nlen(test_bert_class_[0]['ids'])\nprint(f'The number of tokens in the sentence is {MAX_LEN}')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:32:21.036188Z","iopub.execute_input":"2021-08-07T13:32:21.036524Z","iopub.status.idle":"2021-08-07T13:32:21.059083Z","shell.execute_reply.started":"2021-08-07T13:32:21.036493Z","shell.execute_reply":"2021-08-07T13:32:21.058022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Class for BERT model\n#### more here \"Approaching (Almost) Any Machine Learning Problem\" https://github.com/abhishekkrthakur/approachingalmost, by p.256","metadata":{}},{"cell_type":"code","source":"class BertPretrainedUncased(nn.Module):\n    def __init__(self):\n        super(BertPretrainedUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(\"bert-base-uncased\", return_dict=False) # Here we load from pretrained\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 1)\n    def forward(self, ids, mask, token_type_ids):\n        ids, results = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        drop_out = self.drop(results)\n        out_layer = self.out(drop_out)\n        return out_layer\n# test model and print # params\ntest_bert_model_ = BertPretrainedUncased()\nprint(f'number of parameters {test_bert_model_.bert.num_parameters()}')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:36:03.00183Z","iopub.execute_input":"2021-08-07T13:36:03.002169Z","iopub.status.idle":"2021-08-07T13:36:46.943506Z","shell.execute_reply.started":"2021-08-07T13:36:03.002139Z","shell.execute_reply":"2021-08-07T13:36:46.942274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss Function","metadata":{}},{"cell_type":"code","source":"def loss(results, targets):\n    return nn.BCEWithLogitsLoss()(results, targets.view(-1,1))\n\n# test loss with random results \ninp_ = torch.randn(3, requires_grad=True).view(-1,1)\nout_ = torch.empty(3).random_(2)\nff_ = loss(inp_, out_)\nprint(ff_)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:37:22.489346Z","iopub.execute_input":"2021-08-07T13:37:22.489689Z","iopub.status.idle":"2021-08-07T13:37:22.56999Z","shell.execute_reply.started":"2021-08-07T13:37:22.489657Z","shell.execute_reply":"2021-08-07T13:37:22.568691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train and validate functions","metadata":{}},{"cell_type":"code","source":"def train(data_loader, model, optimizer, device, scheduler):\n    model.train()\n    for data in data_loader:\n        # prepare data        \n        ids = data[\"ids\"]\n        token_type_ids = data[\"token_type_ids\"]\n        mask = data[\"mask\"]\n        targets = data[\"targets\"]\n        # to device\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        # grads to zero\n        optimizer.zero_grad()\n        # get the model result\n        out = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n        # loss\n        ls = loss(out, targets)\n        # backword gradients\n        ls.backward()\n        # steps of optimizer and scheduler\n        optimizer.step()\n        scheduler.step()\n        \ndef validate(data_loader, model, device):\n    model.eval()\n    target_list = []\n    output_list = []\n    with torch.no_grad():\n        for data in data_loader:\n            # prepare data        \n            ids = data[\"ids\"]\n            token_type_ids = data[\"token_type_ids\"]\n            mask = data[\"mask\"]\n            targets_ = data[\"targets\"]\n            # to device\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets_.to(device, dtype=torch.float)\n            # get results of current model\n            outs = model(ids, mask, token_type_ids)\n            # convert targets to cpu and extend the final list\n            targets__ = targets.cpu().detach()\n            target_list.extend(targets__.numpy().tolist())\n            # convert outs to cpu\n            out_ = torch.sigmoid(outs).cpu().detach()\n            output_list.extend(out_.numpy().tolist())\n    return output_list, target_list","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:40:38.423966Z","iopub.execute_input":"2021-08-07T13:40:38.424338Z","iopub.status.idle":"2021-08-07T13:40:38.436161Z","shell.execute_reply.started":"2021-08-07T13:40:38.424282Z","shell.execute_reply":"2021-08-07T13:40:38.435249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepate and split data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(TRAINING_FILE).fillna('none')\n# We need to stratify (not distributed well)\ndata_train, data_valid = model_selection.train_test_split(data, test_size=0.10, random_state=11, stratify=data.sentiment.values)\ndata_train.reset_index(drop=True)\ndata_valid.reset_index(drop=True)\n\nprint(len(data_train), len(data_valid))\ndata_train.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:42:01.803909Z","iopub.execute_input":"2021-08-07T13:42:01.804234Z","iopub.status.idle":"2021-08-07T13:42:02.224525Z","shell.execute_reply.started":"2021-08-07T13:42:01.8042Z","shell.execute_reply":"2021-08-07T13:42:02.223632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conver data into tensors after being tokenezed","metadata":{}},{"cell_type":"code","source":"train_data_set = BertDataSetPytorch(data_train.review.values, data_train.sentiment.values)\ntest_data_set = BertDataSetPytorch(data_valid.review.values, data_valid.sentiment.values)\n\nprint('params of the data tokenized')\nprint()\nprint(train_data_set.tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:43:31.368452Z","iopub.execute_input":"2021-08-07T13:43:31.36879Z","iopub.status.idle":"2021-08-07T13:43:31.3772Z","shell.execute_reply.started":"2021-08-07T13:43:31.36876Z","shell.execute_reply":"2021-08-07T13:43:31.3762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load torch dataloader","metadata":{}},{"cell_type":"code","source":"train_data_loader = torch.utils.data.DataLoader(train_data_set, batch_size=TRAIN_BATCH_SIZE, num_workers=2)\ntest_data_loader = torch.utils.data.DataLoader(test_data_set, batch_size=VALID_BATCH_SIZE, num_workers=1)\n\nprint(train_data_loader.dataset.review[0], 'AND REVIEW IS ', train_data_loader.dataset.sentiment[0]) # negative?","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:44:08.840807Z","iopub.execute_input":"2021-08-07T13:44:08.841143Z","iopub.status.idle":"2021-08-07T13:44:08.846943Z","shell.execute_reply.started":"2021-08-07T13:44:08.841115Z","shell.execute_reply":"2021-08-07T13:44:08.846054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Set number of training steps","metadata":{}},{"cell_type":"code","source":"num_train_steps = int(len(data_train) / TRAIN_BATCH_SIZE * EPOCHS)\nprint(num_train_steps)","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:44:53.70295Z","iopub.execute_input":"2021-08-07T13:44:53.70329Z","iopub.status.idle":"2021-08-07T13:44:53.707646Z","shell.execute_reply.started":"2021-08-07T13:44:53.70326Z","shell.execute_reply":"2021-08-07T13:44:53.706839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare model and set paraments space\n#### more here \"Approaching (Almost) Any Machine Learning Problem\" https://github.com/abhishekkrthakur/approachingalmost, by p.270","metadata":{}},{"cell_type":"code","source":"# cuda if exits \nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\nprint(device)    \n# set model\nMODEL = BertPretrainedUncased()\n# send model to device \nMODEL.to(device)\n# params of the model\nparam_optimizer = list(MODEL.named_parameters())\n# get names of paraments to search \nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n# params space to optimize \noptimizer_parameters = [{\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\\\n                         \"weight_decay\": 0.001,},\n                        {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\\\n                         \"weight_decay\": 0.0,},]\n# set optimizer\noptimizer = AdamW(optimizer_parameters, lr=3e-5)\n# set scheduler to stop\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n\nprint(optimizer_parameters[0]['params'][0])","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:46:54.163317Z","iopub.execute_input":"2021-08-07T13:46:54.163658Z","iopub.status.idle":"2021-08-07T13:47:03.331344Z","shell.execute_reply.started":"2021-08-07T13:46:54.163626Z","shell.execute_reply":"2021-08-07T13:47:03.32985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run model with params ","metadata":{}},{"cell_type":"code","source":"accuracy = 0 # set initial \nsave_model = True # if we need to save model\n\nfor epoch in range(EPOCHS):\n    # train\n    print('start train')\n    train(train_data_loader, MODEL, optimizer, device, scheduler)\n    # evaluate\n    print('start validate')\n    outputs, targets = validate(test_data_loader, MODEL, device)\n    # all that less 0.5 is negative\n    outputs = np.array(outputs) >= 0.5\n    acc = accuracy_score(outputs, targets)\n    print(f'accuracy for {epoch} is {acc}')\n    if acc > accuracy:\n        accuracy = acc\n        if save_model:\n            torch.save(MODEL.state_dict(), './model_bert.bin')\n            \nprint(f'best accuraccy is {acc}')","metadata":{"execution":{"iopub.status.busy":"2021-08-07T13:53:19.26449Z","iopub.execute_input":"2021-08-07T13:53:19.264838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}