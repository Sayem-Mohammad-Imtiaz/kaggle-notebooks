{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this report, I am going to load the dataset, explore it, and use Multiple Linear Regression, SVR, K Nearest Neighbor Regression, Random Forest Regression and XGBoost to do the prediction, finally apply the Adjusted R^2 to measure the models."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Import the libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom learntools.core import *\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load and understand the dataset\nThe dataset records houses sold which range from May 2014 to May 2015. It consists of 19 home features, 1 house ID, and 1 dependent variable which is the price."},{"metadata":{"trusted":true},"cell_type":"code","source":"kc_data = pd.read_csv('../input/kc_house_data.csv')\nkc_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Information about the dataset\nkc_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistical summary of the dataset\nkc_data.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore the dataset by visualization\nUsually in this step I can find the characteristics of dataset through various visualization techniques. I drew a correlation matrix heat map to depict the different degrees of correlation among the variables. As to price, high positively correlated features include sqft_living, grade, sqft_above, and sqft_living15. There are two negatively correlated features id and zipcode, and they have a very low correlation with price as well."},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.offline as ply\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nfrom plotly import tools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style= \"whitegrid\")\n\ncorr_mat = kc_data.corr()\nplt.figure(figsize=(30,15))\nsns.heatmap(corr_mat, cmap = 'BrBG', linecolor = 'white', linewidth = 1, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With distribution plot of price, I can see that most of the prices are under 1 million with few outliers, some even close to 8 million."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.distplot(kc_data['price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this subplot, y axis is price and x axis are sqft_living, bedrooms, bathrooms, grade, yr_built and lat. My first assumption for Linear Regression is that the features of the dataset have a linear relationship with those dependent variables. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = go.Scattergl(x=kc_data['sqft_living'], y=kc_data['price'], mode='markers', name='sqft_living')\nfig2 = go.Scattergl(x=kc_data['bedrooms'], y=kc_data['price'], mode = 'markers', name = 'bedrooms')\nfig3 = go.Scattergl(x=kc_data['bathrooms'], y=kc_data['price'],mode = 'markers', name = 'bathrooms')\nfig4 = go.Scattergl(x=kc_data['grade'], y=kc_data['price'],mode = 'markers', name = 'grade')\nfig5 = go.Scattergl(x=kc_data['yr_built'], y=kc_data['price'],mode = 'markers', name = 'yr_built')\nfig6 = go.Scattergl(x=kc_data['lat'], y=kc_data['price'],mode = 'markers', name = 'lat')\nfig = tools.make_subplots(rows=2, cols=3, subplot_titles=('sqft_living vs Price', 'bedrooms vs Price',\n'bathrooms vs Price', 'grade vs Price', 'yr_built vs price', 'lat vs price'))\nfig.append_trace(fig1, 1, 1)\nfig.append_trace(fig2, 1, 2)\nfig.append_trace(fig3, 1, 3)\nfig.append_trace(fig4, 2, 1)\nfig.append_trace(fig5, 2, 2)\nfig.append_trace(fig6, 2, 3)\nfig['layout'].update(height=800, width=800, title='Price Subplots')\nply.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing\nFrom output of kc_data.info() above, I can tell the dataset does not have null values. But as we can see from the visualization I need to remove some outliers with "},{"metadata":{"trusted":true},"cell_type":"code","source":"kc_df = kc_data.drop(kc_data[kc_data[\"bedrooms\"]>10].index )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create y and X, split data for training and testing "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ny = kc_df.price\nfeatures = ['bedrooms', 'bathrooms', 'sqft_living', 'grade', 'yr_built', 'lat']\nX = kc_df[features]\nX_train , X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nkc_lrmodel = LinearRegression()\nkc_lrmodel.fit(X_train, y_train)\n# Predicting the Test set results\ny_lrpred = kc_lrmodel.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVR"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\n\nkc_svrmodel = SVR(kernel='rbf')\nkc_svrmodel.fit(X_train, y_train)\ny_svrpred = kc_svrmodel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K Nearest Neighbor Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nkc_knnmodel = KNeighborsRegressor(n_neighbors=1)\nkc_knnmodel.fit(X_train,y_train)\ny_knnpred = kc_knnmodel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nkc_rfmodel = RandomForestRegressor(n_estimators=20, random_state = 0)\nkc_rfmodel.fit(X_train, y_train)\ny_rfpred = kc_rfmodel.predict(X_test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nkc_xgbmodel = XGBRegressor()\nkc_xgbmodel.fit(X_train, y_train)\ny_xgbpred = kc_xgbmodel.predict(X_test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Measuring the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate Adjusted R Squared Value\nfrom sklearn import metrics\nlr_R = metrics.r2_score(y_test,y_lrpred)\nlr_a_R = 1 - (1-lr_R)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\nprint('Adjusted R Squared Value for Linear Regression: ', round(lr_a_R, 3) )\n\nsvr_R = metrics.r2_score(y_test,y_svrpred)\nsvr_a_R = 1 - (1-svr_R)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\nprint('Adjusted R Squared Value for SVR: ', round(svr_a_R, 3) )\n\nrf_R = metrics.r2_score(y_test,y_rfpred)\nrf_a_R = 1 - (1-rf_R)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\nprint('Adjusted R Squared Value for Random Forest: ', round(rf_a_R, 3) )\n\nknn_R = metrics.r2_score(y_test,y_knnpred)\nknn_a_R = 1 - (1-knn_R)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\nprint('Adjusted R Squared Value for KNN: ', round(knn_a_R, 3) )\n\nxgb_R = metrics.r2_score(y_test,y_xgbpred)\nxgb_a_R = 1 - (1-xgb_R)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\nprint('Adjusted R Squared Value for XGBoost: ', round(xgb_a_R, 3) )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost got the best score! sencond place was Random Forest."},{"metadata":{},"cell_type":"markdown","source":"### A real sample prediction\n2015 I bought a house in Redmond, can't wait to try my models! Guess which model will win?"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['bedrooms', 'bathrooms', 'sqft_living', 'grade', 'yr_built', 'lat']\nsample = pd.DataFrame([[4, 3.25, 3360, 10, 1994, 47.70]],\n                        columns = columns )\ncustomer = sc_X.transform(sample)\n\nlrpredictor = kc_lrmodel.predict(customer)\nprint('Prediction by Linear Regression is', lrpredictor)\n\nsvrpredictor = kc_svrmodel.predict(customer)\nprint('Prediction by SVR is',svrpredictor)\n\nrfpredictor = kc_rfmodel.predict(customer)\nprint('Prediction by Random Forest is', rfpredictor)\n\nknnpredictor = kc_knnmodel.predict(customer)\nprint('Prediction by KNN is',knnpredictor)\n\nxgbpredictor = kc_xgbmodel.predict(customer)\nprint('Prediction by XGBoost is',xgbpredictor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I bought the house on 840000, Random Forest win this single case. Real world is much more complicated, will keep exploring!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}