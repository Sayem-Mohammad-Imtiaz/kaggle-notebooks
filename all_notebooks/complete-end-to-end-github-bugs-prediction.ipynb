{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Complete END to END GitHub Bugs Prediction**"},{"metadata":{},"cell_type":"markdown","source":"![Minion](https://www.aismartz.com/blog/wp-content/uploads/2019/11/Electronic-Design-Automation-data-science-model.jpg)"},{"metadata":{},"cell_type":"markdown","source":"## **Overview Of The Problem**\nIn this problem, we are challenging the machine learning community to come up with an algorithm that can predict the bugs, features, and questions based on GitHub titles and the text body. With text data, there can be a lot of challenges especially when the dataset is big.  \nWith text data, there can be a lot of challenges especially when the dataset is big. Analyzing such a dataset requires a lot to be taken into account mainly due to the preprocessing involved to represent raw text and make them machine-understandable. Usually, we stem and lemmatize the raw information and then represent it using TF-IDF, Word Embeddings, etc.\nHowever, provided the state-of-the-art NLP models such as Transformer based **BERT** models one can skip the manual feature engineering like **TF-IDF** and **Count Vectorizers**. In this short span of time, we would encourage you to leverage the ImageNet moment (Transfer Learning) in NLP using various pre-trained models.\n\n\n# <center><span style=\"color:blue\"> **Table of content** </span></center>\n\n1. <span style=\"color:purple\"> **Dataset Description**</span>.\n2. <span style=\"color:purple\"> **Attribute Description**</span>.\n3. <span style=\"color:purple\"> **Loading Dataset**</span>.\n4. <span style=\"color:purple\"> **Loading Libraries**</span>.\n5. <span style=\"color:purple\"> **Columns Exploration**</span>.\n6. <span style=\"color:purple\"> **Exploring Shape of Dataset**</span>.\n7. <span style=\"color:purple\"> **Statistical Analysis-I**</span>.\n\n       \n"},{"metadata":{},"cell_type":"markdown","source":"## **Dataset Description**\n- Train.json - 150000 rows x 3 columns (Includes label Column as Target variable)\n- Test.json - 30000 rows x 2 columns\n- Train_extra.json - 300000 rows x 3 columns (Includes label Column as Target variable)\n- Provided solely for training purposes, can be appended in the train.json for training the    model\n- Sample Submission.csv - Please check the Evaluation section for more details on how to generate a valid submission"},{"metadata":{},"cell_type":"markdown","source":"## **Attribute Description:** \n* Title - the title of the GitHub bug, feature, question\n* Body - the body of the GitHub bug, feature, question\n* Label - Represents various classes of Labels\n\n    1. Bug - 0\n    2. Feature - 1\n    3. Question - 2\n\n"},{"metadata":{},"cell_type":"markdown","source":"## **Loading Libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\nfrom collections import Counter,defaultdict\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly import tools\npy.init_notebook_mode(connected=True)\nimport re\nimport seaborn as sns\nimport string\nfrom unidecode import unidecode\nfrom wordcloud import WordCloud, STOPWORDS\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing,metrics,manifold\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict\nfrom imblearn.over_sampling import ADASYN,SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport collections\nimport matplotlib.patches as mpatches\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\nfrom sklearn.preprocessing import RobustScaler\nimport xgboost\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\nfrom collections import Counter\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Loading Dataset**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df= pd.read_json(\"/kaggle/input/github-bugs-prediction/embold_train.json\").reset_index(drop=True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ## **Columns Exploration**\n\nThe \"tiltle, and body\" column contains the textual information(input features) and the \"label\" column contains the output labels. The task of any classifier is to correctly predict;\n        -  Bug - 0\n        -  Feature - 1\n        -  Question - 2\nThat given any \"label\" or textual column. Hence we have to apply our data cleaning, transformation steps to the \"title and body\" column."},{"metadata":{},"cell_type":"markdown","source":"- ## **Exploring Shape of dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_df),train_df.index.shape[-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Assess the shape of the data\nprint(\"The Shape of the Dataset\".format(),train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of data points : ', train_df.shape[0])\nprint('Number of features : ', train_df.shape[1])\nprint('Features : ', train_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking null values\ntrain_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking values of label column\ntrain_df['label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![Analysis](https://learn.g2.com/hs-fs/hubfs/What%20is%20Statistical%20Analysis.png?width=900&name=What%20is%20Statistical%20Analysis.png)"},{"metadata":{},"cell_type":"markdown","source":"- ### **Statistical Analysis-I**\nThis is the start of the analysis phase where we will first check the amount of data present in either of the labels. We will follow this up with some pictorial representations related to the words and frequency mappings."},{"metadata":{"trusted":true},"cell_type":"code","source":"#The describe() function is used to generate descriptive statistics that summarize the central tendency, \n#dispersion and shape of a datasetâ€™s distribution, excluding NaN values.\ntrain_df.describe(include=['O'])  #include=['O'] for Object ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#include='all' its include all details\ntrain_df.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking percentage \n(train_df.label.value_counts(normalize=True).sort_index())*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Hypothesize**\n* Title columns have some duplicate. As 149677 is unique out of 150000\n        \"add unit tests\" most repeative sentence in title\n* Body have no duplicate.\n* Label have three catergories\n        0    44.551333 %\n        1    46.070667 %\n        2     9.378000 %\n         "},{"metadata":{},"cell_type":"markdown","source":"### **Pictorial representation of Label**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Count of all Labels\n\ncount=train_df['label'].value_counts()\nprint('Total Counts of all sets\\n', count)\n\nprint(\"==============\")\n#Creating a function to plot the counts using matplotlib\ndef plot_counts(count_Bug,count_Feature,count_Question):\n    plt.rcParams['figure.figsize']=(6,6)\n    plt.bar(0,count_Bug,width=0.6,label='Bugs',color='Green')\n    plt.legend()\n    plt.bar(1,count_Feature,width=0.6,label='Features',color='Red')\n    plt.legend()\n    plt.bar(2,count_Question,width=0.6,label='Questions',color='Black')\n    plt.legend()\n    plt.ylabel('Count of Label')\n    plt.xlabel('Types of Label')\n    plt.show()\n    \ncount_Bug=train_df[train_df['label']== 0]\ncount_Feature=train_df[train_df['label']==1]\ncount_Question=train_df[train_df['label']==2]\nplot_counts(len(count_Bug),len(count_Feature),len(count_Question))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ## **Hypothesize**\nHere you can we balance between Bugs and Features class."},{"metadata":{},"cell_type":"markdown","source":"- ## **Filtering train_data based on each unique label category** \n        1. Bug - 0\n        2. Feature - 1\n        3. Question - 2\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_Bug=train_df[train_df['label']== 0]\ncount_Feature=train_df[train_df['label']== 1]\ncount_Question=train_df[train_df['label']== 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_body_Bug = count_Bug.body.str.split().apply(lambda w : len(w)).sort_values(ascending=True)\ncount_body_Feature = count_Feature.body.str.split().apply(lambda w : len(w)).sort_values(ascending=True)\ncount_body_Question = count_Question.body.str.split().apply(lambda w : len(w)).sort_values(ascending=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pictorial representation of Word in each Label Catergory"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a Generic count_plot function with Seaborn\n\ndef plot_count_dist(count_Bug,count_Feature,count_Question,title_1,title_2,title_3,subtitle):\n    fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(18,6))\n    sns.distplot(count_Bug,ax=ax1,color='r')\n    ax1.set_title(title_1)\n    sns.distplot(count_Feature,ax=ax2,color='g')\n    ax2.set_title(title_2)\n    sns.distplot(count_Question,ax=ax3,color='b')\n    ax3.set_title(title_3)\n    fig.suptitle(subtitle)\n    plt.show()\n    \nplot_count_dist(count_body_Bug,count_body_Feature,count_body_Question,'Bug','Feature','Question','Body Data Word Count Aanalysis')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- ### **Generating WordCloud across different label categories.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a generic function for generating WordCloud across different label categories.\n\ndef generate_wordcloud(df,col,i,label):\n    \n    data = df[df.label == i][col].values\n    \n    wc = WordCloud(stopwords=STOPWORDS, background_color='black',\n                   max_words=10000, min_font_size=6, min_word_length=1)\n    wc.generate(' '.join(data))\n    \n    plt.figure(figsize=(15,15))\n    plt.title('WordCloud for {}'.format(label), fontsize = 24)\n    plt.imshow(wc)\n    plt.axis(\"off\")\n    plt.show()\n    \n\n\nlabels = np.unique(train_df.label.values)\nlabel_names = ['Bug','Feature','Question']\nfor i in labels:\n    generate_wordcloud(train_df,'body',i,label_names[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}