{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport keras\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(keras.__version__)\n\nprint(\"{} paintings\".format(len(os.listdir(\"../input/resized/resized\"))))","execution_count":7,"outputs":[{"output_type":"stream","text":"2.2.4\n8355 paintings\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Building an artist classifier for vector encoding\nWe use the [Xception](https://arxiv.org/abs/1610.02357v2) model with the imagenet weights as a base model without the top layers.\nWe add a dense vector layer with 1024 dimensions and a classification layer with 50 dimensions for the 50 classes/artists that are given in the dataset.\n\nThis model can than be used to extract vector representations for images.\nWe want to use these vector representations to build a simple similarity search for images.\nSee https://www.kaggle.com/roccoli/painting-similarity/"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/artists.csv')\ndf.head()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"   id    ...    paintings\n0   0    ...          193\n1   1    ...           88\n2   2    ...           70\n3   3    ...           73\n4   4    ...          194\n\n[5 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n      <th>years</th>\n      <th>genre</th>\n      <th>nationality</th>\n      <th>bio</th>\n      <th>wikipedia</th>\n      <th>paintings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Amedeo Modigliani</td>\n      <td>1884 - 1920</td>\n      <td>Expressionism</td>\n      <td>Italian</td>\n      <td>Amedeo Clemente Modigliani (Italian pronunciat...</td>\n      <td>http://en.wikipedia.org/wiki/Amedeo_Modigliani</td>\n      <td>193</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Vasiliy Kandinskiy</td>\n      <td>1866 - 1944</td>\n      <td>Expressionism,Abstractionism</td>\n      <td>Russian</td>\n      <td>Wassily Wassilyevich Kandinsky (Russian: Васи́...</td>\n      <td>http://en.wikipedia.org/wiki/Wassily_Kandinsky</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Diego Rivera</td>\n      <td>1886 - 1957</td>\n      <td>Social Realism,Muralism</td>\n      <td>Mexican</td>\n      <td>Diego María de la Concepción Juan Nepomuceno E...</td>\n      <td>http://en.wikipedia.org/wiki/Diego_Rivera</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Claude Monet</td>\n      <td>1840 - 1926</td>\n      <td>Impressionism</td>\n      <td>French</td>\n      <td>Oscar-Claude Monet (; French: [klod mɔnɛ]; 14 ...</td>\n      <td>http://en.wikipedia.org/wiki/Claude_Monet</td>\n      <td>73</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Rene Magritte</td>\n      <td>1898 - 1967</td>\n      <td>Surrealism,Impressionism</td>\n      <td>Belgian</td>\n      <td>René François Ghislain Magritte (French: [ʁəne...</td>\n      <td>http://en.wikipedia.org/wiki/René_Magritte</td>\n      <td>194</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.transform import resize\nfrom keras.preprocessing import image\n\nimg_dir = '../input/images/images/'\nfiles = os.listdir(img_dir)\nimg_size = (300,300)\ninput_shape = [*img_size, 3]\n\ndef load_img(path):\n    img = image.load_img(path=path, target_size=img_size)\n    return np.asarray(img, dtype=\"int32\" )/255","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datagen = keras.preprocessing.image.ImageDataGenerator(\n    featurewise_center=False,\n    featurewise_std_normalization=False,\n    rescale=1.0/255,\n    rotation_range=0,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    validation_split=0.1)\n\ntrain_generator = datagen.flow_from_directory(\n        img_dir,\n        target_size=img_size,\n        batch_size=12,\n        class_mode='categorical',\n        subset=\"training\")\nvalidation_generator = datagen.flow_from_directory(\n        img_dir,\n        target_size=img_size,\n        batch_size=12,\n        class_mode='categorical',\n        subset=\"validation\")\n\nnum_classes = len(train_generator.class_indices)","execution_count":10,"outputs":[{"output_type":"stream","text":"Found 7625 images belonging to 50 classes.\nFound 821 images belonging to 50 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(train_generator.classes)\nplt.hist(validation_generator.classes)","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"(array([103.,  40.,  96.,  67.,  51.,  46.,  91., 104.,  81., 142.]),\n array([ 0. ,  4.9,  9.8, 14.7, 19.6, 24.5, 29.4, 34.3, 39.2, 44.1, 49. ]),\n <a list of 10 Patch objects>)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.xception import Xception\nfrom keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Dense\n\nbase_model = Xception(include_top=False, weights='imagenet')\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu', name=\"dense_1024\")(x)\npredictions = Dense(num_classes, activation='softmax', name=\"predictions\")(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)","execution_count":12,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n83689472/83683744 [==============================] - 1s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, layer in enumerate(base_model.layers):\n    print(i, layer.name)","execution_count":13,"outputs":[{"output_type":"stream","text":"0 input_1\n1 block1_conv1\n2 block1_conv1_bn\n3 block1_conv1_act\n4 block1_conv2\n5 block1_conv2_bn\n6 block1_conv2_act\n7 block2_sepconv1\n8 block2_sepconv1_bn\n9 block2_sepconv2_act\n10 block2_sepconv2\n11 block2_sepconv2_bn\n12 conv2d_1\n13 block2_pool\n14 batch_normalization_1\n15 add_1\n16 block3_sepconv1_act\n17 block3_sepconv1\n18 block3_sepconv1_bn\n19 block3_sepconv2_act\n20 block3_sepconv2\n21 block3_sepconv2_bn\n22 conv2d_2\n23 block3_pool\n24 batch_normalization_2\n25 add_2\n26 block4_sepconv1_act\n27 block4_sepconv1\n28 block4_sepconv1_bn\n29 block4_sepconv2_act\n30 block4_sepconv2\n31 block4_sepconv2_bn\n32 conv2d_3\n33 block4_pool\n34 batch_normalization_3\n35 add_3\n36 block5_sepconv1_act\n37 block5_sepconv1\n38 block5_sepconv1_bn\n39 block5_sepconv2_act\n40 block5_sepconv2\n41 block5_sepconv2_bn\n42 block5_sepconv3_act\n43 block5_sepconv3\n44 block5_sepconv3_bn\n45 add_4\n46 block6_sepconv1_act\n47 block6_sepconv1\n48 block6_sepconv1_bn\n49 block6_sepconv2_act\n50 block6_sepconv2\n51 block6_sepconv2_bn\n52 block6_sepconv3_act\n53 block6_sepconv3\n54 block6_sepconv3_bn\n55 add_5\n56 block7_sepconv1_act\n57 block7_sepconv1\n58 block7_sepconv1_bn\n59 block7_sepconv2_act\n60 block7_sepconv2\n61 block7_sepconv2_bn\n62 block7_sepconv3_act\n63 block7_sepconv3\n64 block7_sepconv3_bn\n65 add_6\n66 block8_sepconv1_act\n67 block8_sepconv1\n68 block8_sepconv1_bn\n69 block8_sepconv2_act\n70 block8_sepconv2\n71 block8_sepconv2_bn\n72 block8_sepconv3_act\n73 block8_sepconv3\n74 block8_sepconv3_bn\n75 add_7\n76 block9_sepconv1_act\n77 block9_sepconv1\n78 block9_sepconv1_bn\n79 block9_sepconv2_act\n80 block9_sepconv2\n81 block9_sepconv2_bn\n82 block9_sepconv3_act\n83 block9_sepconv3\n84 block9_sepconv3_bn\n85 add_8\n86 block10_sepconv1_act\n87 block10_sepconv1\n88 block10_sepconv1_bn\n89 block10_sepconv2_act\n90 block10_sepconv2\n91 block10_sepconv2_bn\n92 block10_sepconv3_act\n93 block10_sepconv3\n94 block10_sepconv3_bn\n95 add_9\n96 block11_sepconv1_act\n97 block11_sepconv1\n98 block11_sepconv1_bn\n99 block11_sepconv2_act\n100 block11_sepconv2\n101 block11_sepconv2_bn\n102 block11_sepconv3_act\n103 block11_sepconv3\n104 block11_sepconv3_bn\n105 add_10\n106 block12_sepconv1_act\n107 block12_sepconv1\n108 block12_sepconv1_bn\n109 block12_sepconv2_act\n110 block12_sepconv2\n111 block12_sepconv2_bn\n112 block12_sepconv3_act\n113 block12_sepconv3\n114 block12_sepconv3_bn\n115 add_11\n116 block13_sepconv1_act\n117 block13_sepconv1\n118 block13_sepconv1_bn\n119 block13_sepconv2_act\n120 block13_sepconv2\n121 block13_sepconv2_bn\n122 conv2d_4\n123 block13_pool\n124 batch_normalization_4\n125 add_12\n126 block14_sepconv1\n127 block14_sepconv1_bn\n128 block14_sepconv1_act\n129 block14_sepconv2\n130 block14_sepconv2_bn\n131 block14_sepconv2_act\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# freeze some layers, only train the last blocks\nfor layer in base_model.layers[:31]:\n    layer.trainable = True\nfor layer in base_model.layers[86:]:\n    layer.trainable = True","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom keras.callbacks import *\nfrom keras.optimizers import *\n\ncallbacks = [\n    EarlyStopping(monitor='val_loss', patience=6),\n    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.2, min_lr=0.001)\n]\n\nmodel.compile(optimizer=SGD(lr=0.01),\n              loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.fit_generator(\n    train_generator, \n    callbacks=callbacks, \n    validation_data=validation_generator, \n    epochs=16,\n    steps_per_epoch=500,\n    validation_steps=80,\n    workers=2)","execution_count":15,"outputs":[{"output_type":"stream","text":"WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nEpoch 1/14\n500/500 [==============================] - 235s 471ms/step - loss: 3.2617 - acc: 0.1982 - val_loss: 2.6980 - val_acc: 0.3473\nEpoch 2/14\n500/500 [==============================] - 217s 434ms/step - loss: 2.3281 - acc: 0.4210 - val_loss: 1.9262 - val_acc: 0.5110\nEpoch 3/14\n500/500 [==============================] - 217s 434ms/step - loss: 1.7066 - acc: 0.5578 - val_loss: 1.4889 - val_acc: 0.6044\nEpoch 4/14\n500/500 [==============================] - 215s 430ms/step - loss: 1.3319 - acc: 0.6466 - val_loss: 1.2096 - val_acc: 0.6716\nEpoch 5/14\n500/500 [==============================] - 216s 433ms/step - loss: 1.0592 - acc: 0.7085 - val_loss: 1.1117 - val_acc: 0.6988\nEpoch 6/14\n500/500 [==============================] - 216s 431ms/step - loss: 0.8296 - acc: 0.7692 - val_loss: 0.9575 - val_acc: 0.7261\nEpoch 7/14\n500/500 [==============================] - 214s 427ms/step - loss: 0.6741 - acc: 0.8167 - val_loss: 0.9126 - val_acc: 0.7463\nEpoch 8/14\n500/500 [==============================] - 216s 432ms/step - loss: 0.5824 - acc: 0.8380 - val_loss: 0.8684 - val_acc: 0.7482\nEpoch 9/14\n500/500 [==============================] - 215s 430ms/step - loss: 0.4813 - acc: 0.8675 - val_loss: 0.8137 - val_acc: 0.7681\nEpoch 10/14\n500/500 [==============================] - 216s 431ms/step - loss: 0.3957 - acc: 0.8948 - val_loss: 0.7598 - val_acc: 0.7891\nEpoch 11/14\n500/500 [==============================] - 216s 431ms/step - loss: 0.3028 - acc: 0.9225 - val_loss: 0.7956 - val_acc: 0.7671\nEpoch 12/14\n500/500 [==============================] - 214s 429ms/step - loss: 0.2598 - acc: 0.9345 - val_loss: 0.7736 - val_acc: 0.7849\nEpoch 13/14\n500/500 [==============================] - 220s 441ms/step - loss: 0.2196 - acc: 0.9443 - val_loss: 0.7926 - val_acc: 0.7822\nEpoch 14/14\n500/500 [==============================] - 221s 441ms/step - loss: 0.1950 - acc: 0.9480 - val_loss: 0.7835 - val_acc: 0.7754\nCPU times: user 1h 23min 12s, sys: 15min 50s, total: 1h 39min 2s\nWall time: 51min 8s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('best_model.h5')\n\nmodel.evaluate_generator(datagen.flow_from_directory(\n        img_dir,\n        target_size=img_size,\n        batch_size=16,\n        class_mode='categorical'), steps=100)","execution_count":16,"outputs":[{"output_type":"stream","text":"Found 8446 images belonging to 50 classes.\n","name":"stdout"},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"[0.22784342849627137, 0.94375]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.resnet50 import preprocess_input, decode_predictions\nfrom keras.models import Model\n\nclass ImgVectorizer:\n\n    def __init__(self, model, dense_layer_name):\n        self.intermediate_layer_model = Model(\n            inputs=model.input, \n            outputs=model.get_layer(dense_layer_name).output\n        )\n\n    def to_vector(self, imgs):\n        \"\"\" Gets a vector embedding from an image\n        :param image_path: path to image on filesystem\n        :returns: numpy ndarray\n        \"\"\"\n        batch = np.array(imgs)\n        intermediate_output = self.intermediate_layer_model.predict(batch)\n        return intermediate_output\n    \nvectorizer = ImgVectorizer(model=model, dense_layer_name=\"dense_1024\")","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def batched(batch_size, iterable):\n    batch = []\n    for item in iterable:\n        batch.append(item)\n        if len(batch) == batch_size:\n            yield batch\n            batch = []\n    if len(batch) > 0:\n        yield batch","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nimport re\n\np = re.compile(\".*/images/(?P<name>.*)/(?P<img>.*)\\.jpg\")\n\ndef img_dict_generator():\n    for file in glob.glob(img_dir + '*/*'):\n        match = p.match(file)\n        if match:\n            artist = p.match(file).group(\"name\")\n            yield {'artist': artist, 'file': file, 'vector': []}\n        \ndf = pd.DataFrame(img_dict_generator())","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\ndef add_vector_field(dataframe):\n    dataframe['file'].values\n\nfor indexes in batched(16, df.index):\n    df_batch = df.loc[indexes]\n    imgs = [load_img(file) for file in df_batch['file'].values]\n    vectors = vectorizer.to_vector(imgs)\n    df_batch['vector'] = [tuple(list) for list in vectors]\n    df.update(df_batch)","execution_count":20,"outputs":[{"output_type":"stream","text":"CPU times: user 2min 35s, sys: 36.3 s, total: 3min 11s\nWall time: 3min 8s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.to_hdf('df_with_vectors.h5', key=\"vectors\")\ndf.to_csv('df_with_vectors.csv')","execution_count":21,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py:1996: PerformanceWarning: \nyour performance may suffer as PyTables will pickle object types that it cannot\nmap directly to c-types [inferred_type->mixed,key->block0_values] [items->['artist', 'file', 'vector']]\n\n  return pytables.to_hdf(path_or_buf, key, self, **kwargs)\n","name":"stderr"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}