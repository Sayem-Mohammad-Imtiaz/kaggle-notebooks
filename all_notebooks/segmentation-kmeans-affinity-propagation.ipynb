{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#In this project I will segment bank customers based on their Credit Score, Age and Number of banking products that they own.\n#I will use KMeans segmentation and affinity propagation to identify the correct number of clusters\n#Besides this, I will use different visualization technique to better understand if Gender or Geography have any influence on credit score.\n\n##Result: Affinity Propagation seem to indicate a larger number of cluster as being more suited, vs. K-means. \n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, AffinityPropagation\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dp = pd.read_csv('../input/bank-customer-segmentation-kmeans-affinity/Bank Customer Segmentation.csv')\ndp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We drop the first 3 coloumns as they all refer to the same customer.\ndp = dp.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis = 1)\ndp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dp.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dp.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dp.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dp.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique values in each categorical column:\")\nfor col in dp.select_dtypes(include=[object]):\n    print(col,\":\", dp[col].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scatters(data, h=None, pal=None):\n    fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(8,8))\n    sns.scatterplot(x=\"CreditScore\",y=\"NumOfProducts\", hue=h, palette=pal, data=data, ax=ax1)\n    sns.scatterplot(x=\"Age\",y=\"CreditScore\", hue=h, palette=pal, data=data, ax=ax2)\n    sns.scatterplot(x=\"Age\",y=\"NumOfProducts\", hue=h, palette=pal, data=data, ax=ax3)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatters(dp, h=\"Gender\")\n#Gender does not seem to make any differences in terms of credit score, no. of products or age.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us check the linear correlation between credit score and no. of products.\nimport scipy.stats as stats\nr1 = sns.jointplot(x=\"CreditScore\",y=\"NumOfProducts\", data=dp, kind=\"reg\", height=8)\nplt.show()\n#A small tendency exists of having a lower credit score as you increase the number of products.\n#The logic is that as you take more banking credits or products and become more indebted to the bank, your overall suitability for another new credit decreases.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x=\"CreditScore\",y=\"NumOfProducts\", hue=\"Gender\", data=dp, palette=\"Set1\", aspect=2)\nplt.show()\n#Gender does not have any impact on this relationship between no. of products and gender.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x=\"CreditScore\",y=\"NumOfProducts\", hue=\"Geography\", data=dp, palette=\"Set1\", aspect=2)\nplt.show()\n#Only customers from Germany seem to differentiate as they have a stronger relationship between no. of products and gender.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Kernel Density Estimation: I`m using KDE here in order to better evaluate the probability density of our variiables.\n#This is a more powerful technique, compared to normal histograms, to better understand the data.\nsns.jointplot(x=\"CreditScore\",y=\"NumOfProducts\", data=dp, kind=\"kde\", space=0, color=\"g\",  height=8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#By grouping the customers, it can be observed that most credits were given for customers with 1 or 2 products.\nn_credits = dp.groupby(\"NumOfProducts\")[\"Age\"].count().rename(\"Count\").reset_index()\nn_credits.sort_values(by=[\"Count\"], ascending=False, inplace=True)\n\nplt.figure(figsize=(10,6))\nbar = sns.barplot(x=\"NumOfProducts\",y=\"Count\",data=n_credits)\nbar.set_xticklabels(bar.get_xticklabels(), rotation=60)\nplt.ylabel(\"Number of granted credits\")\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us firther exploire our variables by using different boxplots.\ndef boxes(x,y,h,r=45):\n    fig, ax = plt.subplots(figsize=(10,6))\n    box = sns.boxplot(x=x,y=y, hue=h, data=dp)\n    box.set_xticklabels(box.get_xticklabels(), rotation=r)\n    fig.subplots_adjust(bottom=0.2)\n    plt.tight_layout()\nboxes(\"NumOfProducts\",\"CreditScore\",\"Gender\")\n#Insight: It can be observed that as the number of products increase so does the credit score differences between males and females.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxes(\"NumOfProducts\",\"CreditScore\",\"Geography\")\n\n#Insight: It can be observed that as the number of products increase so does the credit score differences between different geographic locations.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3D representation for creditscore and no of products.\nfrom mpl_toolkits.mplot3d import Axes3D \nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(dp[\"CreditScore\"], dp[\"NumOfProducts\"], dp[\"Age\"])\nax.set_xlabel(\"CreditScore\")\nax.set_ylabel(\"NumOfProducts\")\nax.set_zlabel(\"Age\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Clustering with KMeans.\n##I will a subset containing only numerical variables (Age, CreditScore, Number of products).\n##Then I will create distribution histograms, standardize the variables through logarithmic transformation and then rescale them as required by k-means clustering.\nselected_cols = [\"Age\",\"CreditScore\", \"NumOfProducts\"]\ncluster_data = dp.loc[:,selected_cols]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def distributions(df):\n    fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize=(8,8))\n    sns.distplot(df[\"Age\"], ax=ax1)\n    sns.distplot(df[\"CreditScore\"], ax=ax2)\n    sns.distplot(df[\"NumOfProducts\"], ax=ax3)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distributions(cluster_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cluster_log = np.log(cluster_data)\ndistributions(cluster_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ncluster_scaled = scaler.fit_transform(cluster_log)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Among the most important factors when evaluating clusters is their internal consistency, called inertia.\n#Inertia is the element which needs to be controlled in order to have sensible and relevant output clusters.\nclusters_range = [2,3,4,5,6,7,8,9,10,11,12,13,14]\ninertias =[]\n\nfor c in clusters_range:\n    kmeans = KMeans(n_clusters=c, random_state=0).fit(cluster_scaled)\n    inertias.append(kmeans.inertia_)\n\nplt.figure()\nplt.plot(clusters_range,inertias, marker='o')\n##Insight: As the number of clusters increase inertia decreases.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Silhouette analysis: This is used to determine the optimal distance between each cluster centroid.\n#This method helps to identify the number of clusters by looking at the silouhette scores. \nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nclusters_range = range(2,15)\nrandom_range = range(0,20)\nresults =[]\nfor c in clusters_range:\n    for r in random_range:\n        clusterer = KMeans(n_clusters=c, random_state=r)\n        cluster_labels = clusterer.fit_predict(cluster_scaled)\n        silhouette_avg = silhouette_score(cluster_scaled, cluster_labels)\n        #print(\"For n_clusters =\", c,\" and seed =\", r,  \"\\nThe average silhouette_score is :\", silhouette_avg)\n        results.append([c,r,silhouette_avg])\n\nresult = pd.DataFrame(results, columns=[\"n_clusters\",\"seed\",\"silhouette_score\"])\npivot_km = pd.pivot_table(result, index=\"n_clusters\", columns=\"seed\",values=\"silhouette_score\")\n\nplt.figure(figsize=(15,6))\nsns.heatmap(pivot_km, annot=True, linewidths=.5, fmt='.3f', cmap=sns.cm.rocket_r)\nplt.tight_layout()\n##Insight: 4 Clusters have the highest scores together with 13 and 14 clusters. \n##Insight: As such, I will try with 4 clusters to obtain more insights.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_sel = KMeans(n_clusters=4, random_state=1).fit(cluster_scaled)\nlabels = pd.DataFrame(kmeans_sel.labels_)\nclustered_data = cluster_data.assign(Cluster=labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scatters(clustered_data, 'Cluster')\n##Insight: The scatterplots seem to indicate that 3 clusters separate clearly, not 4 clusters. \n##Insight: As such I will run the analysis again on 3 clusters.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans_sel = KMeans(n_clusters=3, random_state=1).fit(cluster_scaled)\nlabels = pd.DataFrame(kmeans_sel.labels_)\nclustered_data = cluster_data.assign(Cluster=labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_km = clustered_data.groupby(['Cluster']).mean().round(1)\ngrouped_km\n##Insight: Cluster 0: younger customers, average credit score, high no. of products\n##Insight: Cluster 1: middle-aged customers, low credit score, low no. of products\n##Insight: Cluster 2: younger customers, high credit score, low no. of products","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Clustering with Affinity Propagation.\n##This technique is very important as it segments the data without the need to specify the number of clusters.\npreferences = np.arange(-30,-200,-10)\nclusters = []\n\nfor p in preferences:\n    af = AffinityPropagation(preference=p, damping=0.6, max_iter=400, verbose=False).fit(cluster_scaled)\n    labels_af = pd.DataFrame(af.labels_)\n    clusters.append(len(af.cluster_centers_indices_))\n\nplt.figure(figsize=(10,7))\nplt.xlabel(\"Preference\")\nplt.ylabel(\"Number of clusters\")\nplt.plot(preferences,clusters, marker='o')\n\n##Insight: As the preferences parameter goes down so does the number of clusters.\n##Insight: The minimum is attained for 5 clusters at -160. As such I will check the 5 clusters option","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"af = AffinityPropagation(preference=-160, damping=0.6, verbose=False).fit(cluster_scaled)\nlabels_af = pd.DataFrame(af.labels_)\nn_clusters_ = len(af.cluster_centers_indices_)\n\nclustered_data_af = cluster_data.assign(Cluster=labels_af)\nscatters(clustered_data_af,'Cluster')\n\ngrouped_af = clustered_data_af.groupby(['Cluster']).mean().round(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_af = clustered_data_af.groupby(['Cluster']).mean().round(1)\ngrouped_af\n##Insight: Cluster 0: middle-aged customers, average credit score, high no. of products\n##Insight: Cluster 1: younger customers, low credit score, average no. of products\n##Insight: Cluster 2: younger customers, high credit score, low no. of products\n##Insight: Cluster 3: middle-aged customers, low credit score, low no. of products\n##Insight: Cluster 4: older customers, high credit score, low no. of products","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}