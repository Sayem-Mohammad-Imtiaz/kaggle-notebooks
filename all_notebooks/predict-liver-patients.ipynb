{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Liver patients prediction\n### A prediction /classification model should take into account at least three factors: \n### 1. Purpose 2. Dataset characteristics. 3. Computanional & run time resurces. \n\n#### Starting with point #1, the goal I aim at is to get the **highest recall for positive diagnosis**, for not missing any patient. Nevertheless, I will also try to get a good **precision for healthy people**. \n#### As for point #2, the Liver patients dataset is **small and imbalnced**.\n\n#### Sorry, I don't like oversampling, such as **smote** method. I suspect that the model will learn the smote pattern, and not the real generalization that I expect from a good ML algorithm.  \n#### Because it is a small Dataset, I tried **NearMiss** undersampling method.\n#### Additional improvments I got with 1. a **costum loss** function, 2. not using NearMiss and 3. increasing number of epochs. \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-22T10:11:38.778489Z","iopub.execute_input":"2021-07-22T10:11:38.779281Z","iopub.status.idle":"2021-07-22T10:11:38.797871Z","shell.execute_reply.started":"2021-07-22T10:11:38.7792Z","shell.execute_reply":"2021-07-22T10:11:38.79694Z"}}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport torch\nimport math\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:54.346202Z","iopub.execute_input":"2021-08-01T23:42:54.346605Z","iopub.status.idle":"2021-08-01T23:42:54.353067Z","shell.execute_reply.started":"2021-08-01T23:42:54.346574Z","shell.execute_reply":"2021-08-01T23:42:54.351597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Some data preparation... ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/indian-liver-patient-records/indian_liver_patient.csv')\nprint (df['Dataset'])\ndf.dropna(inplace=True)\n\ndf.Gender.replace({'Male': 1, 'Female': 2}, inplace=True)\ndf.Dataset.replace({2: 0}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:54.355331Z","iopub.execute_input":"2021-08-01T23:42:54.355993Z","iopub.status.idle":"2021-08-01T23:42:54.381228Z","shell.execute_reply.started":"2021-08-01T23:42:54.355945Z","shell.execute_reply":"2021-08-01T23:42:54.380073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Lets **normalize** the data: ","metadata":{}},{"cell_type":"code","source":"# Create a minimum and maximum processor object\nfrom sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\n\n# Create an object to transform the data to fit minmax processor\ndf_scaled = min_max_scaler.fit_transform(df)\n\n# Run the normalizer on the dataframe\ndf_normalized = pd.DataFrame(df_scaled)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:54.383138Z","iopub.execute_input":"2021-08-01T23:42:54.383412Z","iopub.status.idle":"2021-08-01T23:42:54.392863Z","shell.execute_reply.started":"2021-08-01T23:42:54.383387Z","shell.execute_reply":"2021-08-01T23:42:54.391829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Let's set aside 20% of the data for **validation**.","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(df_normalized, test_size=0.2, random_state=1)\n\nlabels = train[10]\ntrain = train.drop(10, axis=1)\n\nlabelsTest = test[10]\ntest = test.drop(10, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:54.394494Z","iopub.execute_input":"2021-08-01T23:42:54.39477Z","iopub.status.idle":"2021-08-01T23:42:54.408765Z","shell.execute_reply.started":"2021-08-01T23:42:54.394747Z","shell.execute_reply":"2021-08-01T23:42:54.408002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Shifting to pytorch","metadata":{}},{"cell_type":"code","source":"x = torch.tensor(train.values.astype(np.float32))\ny = torch.tensor(labels.values.astype(np.float32))\n\nx_test = torch.tensor(test.values.astype(np.float32))\ny_test = torch.tensor(labelsTest.values.astype(np.float32))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:54.410142Z","iopub.execute_input":"2021-08-01T23:42:54.410713Z","iopub.status.idle":"2021-08-01T23:42:54.42121Z","shell.execute_reply.started":"2021-08-01T23:42:54.41067Z","shell.execute_reply":"2021-08-01T23:42:54.420431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Because the dataset is so tiny, it suffers from the Biasâ€“variance tradeoff. \n### Therefore, it will be hard to do **hyperparameters tuning**. For this purpose I use seed:","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(123)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:54.42245Z","iopub.execute_input":"2021-08-01T23:42:54.42303Z","iopub.status.idle":"2021-08-01T23:42:54.433632Z","shell.execute_reply.started":"2021-08-01T23:42:54.422988Z","shell.execute_reply":"2021-08-01T23:42:54.432417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **NearMiss**\n### To deal with this small and imbalnced dataset, I will start with NearMiss.\n### By using the NearMiss method, we undersample the frequent class. In NearMiss method, the chosen samples have similar characteristics to those of the rare class. We assume that the clasifier will need to \"work\" harder to find good generalization princepeles, and it will be easier to clasify the droped samples.","metadata":{}},{"cell_type":"code","source":"from imblearn.under_sampling import NearMiss\nnr = NearMiss()\nx_near, y_near = nr.fit_resample(x, y)\n\nx_nm = torch.from_numpy(x_near)\ny_nm = torch.from_numpy(y_near)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:54.436109Z","iopub.execute_input":"2021-08-01T23:42:54.436553Z","iopub.status.idle":"2021-08-01T23:42:54.448388Z","shell.execute_reply.started":"2021-08-01T23:42:54.436509Z","shell.execute_reply":"2021-08-01T23:42:54.447374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's verify that the classes are balanced","metadata":{}},{"cell_type":"code","source":"negative_imbalanced=0\npositive_imbalanced=0\nfor i in range(y_nm.shape[0]):\n    if y_nm[i] == 0:\n        negative_imbalanced += 1\n    if y_nm[i] == 1:\n        positive_imbalanced += 1\nprint ('Number of negative samples after balance: ', negative_imbalanced)\nprint ('Number of positive samples after balance: ', positive_imbalanced)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:54.450188Z","iopub.execute_input":"2021-08-01T23:42:54.450613Z","iopub.status.idle":"2021-08-01T23:42:54.472345Z","shell.execute_reply.started":"2021-08-01T23:42:54.450569Z","shell.execute_reply":"2021-08-01T23:42:54.471606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The model: ","metadata":{}},{"cell_type":"code","source":"model = torch.nn.Sequential(\n    torch.nn.Linear(10, 50, bias=True),\n    torch.nn.Dropout(0.4),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20, bias=True),\n    torch.nn.Dropout(0.3),\n    torch.nn.LeakyReLU(),\n    torch.nn.Linear(20, 1, bias=True),\n    torch.nn.Sigmoid()\n)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:54.473902Z","iopub.execute_input":"2021-08-01T23:42:54.474307Z","iopub.status.idle":"2021-08-01T23:42:54.480646Z","shell.execute_reply.started":"2021-08-01T23:42:54.474279Z","shell.execute_reply":"2021-08-01T23:42:54.479928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### I will use conventional configuration (Binary Cross Entropy, Adam, ReLU)","metadata":{}},{"cell_type":"code","source":"criterion = torch.nn.BCELoss()\nlearning_rate = 1e-3\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:54.481827Z","iopub.execute_input":"2021-08-01T23:42:54.482366Z","iopub.status.idle":"2021-08-01T23:42:54.489651Z","shell.execute_reply.started":"2021-08-01T23:42:54.482335Z","shell.execute_reply":"2021-08-01T23:42:54.489043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lets run!","metadata":{}},{"cell_type":"code","source":"Losses = []\nepochs = 500\n\nfor epoch in range(epochs):\n    y_pred = model(x_nm)    \n    y_nm = torch.reshape(y_nm, (-1, 1))\n    loss = criterion(y_pred, y_nm)\n    #collecting the losses... \n    Losses.append(loss)\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:54.605559Z","iopub.execute_input":"2021-08-01T23:42:54.605984Z","iopub.status.idle":"2021-08-01T23:42:55.566429Z","shell.execute_reply.started":"2021-08-01T23:42:54.605943Z","shell.execute_reply":"2021-08-01T23:42:55.565266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's see the loss","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(Losses)\nplt.title('Test loss trend')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:55.568349Z","iopub.execute_input":"2021-08-01T23:42:55.56865Z","iopub.status.idle":"2021-08-01T23:42:55.730157Z","shell.execute_reply.started":"2021-08-01T23:42:55.56862Z","shell.execute_reply":"2021-08-01T23:42:55.728773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### let's see the recall","metadata":{}},{"cell_type":"code","source":"# Get the model predictions for the testset:\ntest_pred = model(x_test)\n\n# Lets round the test prediction to yes/no. (This means: 1 or 0).\ny_out = torch.round(test_pred)\n\n# mach the two vectors dimantion:\ny_test = torch.reshape(y_test, (-1, 1))\n\n# By the following vector we can extract F/P, F/N, T/P, T/N. \nF_positive, T_positive, F_negative, T_negative = 0, 0, 0, 0\nfor i in range(y_test.shape[0]):\n    if y_out[i] == 0:\n        if y_test[i] == 0:\n            T_negative += 1\n        else:\n            F_negative += 1\n    if y_out[i] == 1:\n        if y_test[i] == 1:\n            T_positive += 1\n        if y_test[i] == 0:\n            F_positive += 1\n\nrecall = T_positive / (F_negative + T_positive)\nprint('The percentage of recall is: ', int(100*recall))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:55.733068Z","iopub.execute_input":"2021-08-01T23:42:55.733505Z","iopub.status.idle":"2021-08-01T23:42:55.758529Z","shell.execute_reply.started":"2021-08-01T23:42:55.73346Z","shell.execute_reply":"2021-08-01T23:42:55.75739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **SKlearn** is surely much more elegant **!!**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nLabel = y_test.detach().numpy()\nPrediction = y_out.detach().numpy()\n\nprint('Classification report for NearMiss: \\n', classification_report(Label, Prediction))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:55.760288Z","iopub.execute_input":"2021-08-01T23:42:55.760575Z","iopub.status.idle":"2021-08-01T23:42:55.771398Z","shell.execute_reply.started":"2021-08-01T23:42:55.760547Z","shell.execute_reply":"2021-08-01T23:42:55.770626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The precision is better then other notbooks I saw here, but this result does not match **my main goal**: high recall for a Liver patients prediction. \n### To reach this goal, I will try to **manipulate the loss function** a little bit: ","metadata":{}},{"cell_type":"code","source":"epsilon = 10**-10\ndef my_loss(output, target, alpha):\n    loss = alpha * target * torch.abs(torch.log(output + epsilon)) + torch.abs((1 - target) * torch.log(1 - output + epsilon))\n    loss = torch.mean(loss)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:55.772815Z","iopub.execute_input":"2021-08-01T23:42:55.773355Z","iopub.status.idle":"2021-08-01T23:42:55.778711Z","shell.execute_reply.started":"2021-08-01T23:42:55.773311Z","shell.execute_reply":"2021-08-01T23:42:55.777966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Redefine the model. \n##### (I define a function for quick reuse.)","metadata":{}},{"cell_type":"code","source":"def run(x, y, x_test, y_test, alpha, epochs):\n    Losses = []\n    for epoch in range(epochs):\n        y_pred = model(x)    \n        y = torch.reshape(y, (-1, 1))\n        loss = my_loss(y_pred, y, alpha)\n        #collecting the losses... \n        Losses.append(loss)\n        # Zero gradients, perform a backward pass, and update the weights.\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Get the model predictions for the testset:\n    test_pred = model(x_test)\n\n    # Lets round the test prediction to yes/no. (This means: 1 or 0).\n    y_out = torch.round(test_pred)\n\n    # mach the two vectors dimantion:\n    y_test = torch.reshape(y_test, (-1, 1))\n\n    plt.plot(Losses)\n    plt.title('Test loss trend')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.show()\n\n    Label = y_test.detach().numpy()\n    Prediction = y_out.detach().numpy()\n    print('Classification report for alpha =', alpha, ': \\n', classification_report(Label, Prediction))","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:55.780141Z","iopub.execute_input":"2021-08-01T23:42:55.780442Z","iopub.status.idle":"2021-08-01T23:42:55.79197Z","shell.execute_reply.started":"2021-08-01T23:42:55.780407Z","shell.execute_reply":"2021-08-01T23:42:55.791169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run(x_nm, y_nm, x_test, y_test, 0.5, 500)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:55.793349Z","iopub.execute_input":"2021-08-01T23:42:55.793844Z","iopub.status.idle":"2021-08-01T23:42:56.989504Z","shell.execute_reply.started":"2021-08-01T23:42:55.793813Z","shell.execute_reply":"2021-08-01T23:42:56.988517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Alpha=0.5 compensating the imbalance of the data, and give greater precision. But the recall is poor.. ### Let's see the recall for higher alpha.","metadata":{}},{"cell_type":"code","source":"run(x_nm, y_nm, x_test, y_test, 30, 500)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:56.990987Z","iopub.execute_input":"2021-08-01T23:42:56.991274Z","iopub.status.idle":"2021-08-01T23:42:58.223943Z","shell.execute_reply.started":"2021-08-01T23:42:56.991246Z","shell.execute_reply":"2021-08-01T23:42:58.223125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adding a 4th layer to the model didn't give me a signicent improvement.\n### I also tryied to add epochs.\n#### (Pytorch-lightning has an early stopping, but it did not work properly at the time I made this kernal). ","metadata":{}},{"cell_type":"code","source":"run(x, y, x_test, y_test,1.4, 1000)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:42:58.225867Z","iopub.execute_input":"2021-08-01T23:42:58.226159Z","iopub.status.idle":"2021-08-01T23:43:00.556128Z","shell.execute_reply.started":"2021-08-01T23:42:58.226131Z","shell.execute_reply":"2021-08-01T23:43:00.55524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run(x, y, x_test, y_test, 1, 500)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T23:43:00.557413Z","iopub.execute_input":"2021-08-01T23:43:00.557687Z","iopub.status.idle":"2021-08-01T23:43:01.918298Z","shell.execute_reply.started":"2021-08-01T23:43:00.557659Z","shell.execute_reply":"2021-08-01T23:43:01.91741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## I'm not sure why imbalanced loss function gave better results compare to NearMiss.  \n## Please let me know if you have some insights and/or improvement sugestions. \n### *I assume it relates to the {small} size of this dataset.* \n### Thanks for reading,\n### Ziv","metadata":{}}]}