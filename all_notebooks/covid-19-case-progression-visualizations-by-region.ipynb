{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport scipy as sp\nimport matplotlib as mpl\nmpl.rc('figure', max_open_warning =1000)\nfrom matplotlib import pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib import style\nimport pandas as pd\npd.set_option(\"display.max_rows\", 1000)\nfrom bs4 import BeautifulSoup \nimport requests\nimport re\nimport os\nimport logging\nfrom datetime import datetime as dt\nimport itertools as itr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Acquire Data\n\nThe following code snippet (you can expand the cell if you're curious) shows how the data was collected from John's Hopkin's University Center for Computer Studies Github repository.  It should be noted that it doesn't run on Kaggle (perhaps as a bandwidth limiter), but does on a home computer"},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"def getFileList(homepage):\n    \"\"\"Use BeautifulSoup to fetch a file list from the GitHub page stored in homepage\"\"\"\n    source = requests.get(homepage).text\n    soup = BeautifulSoup(source, 'lxml')\n    files = []\n    for markup in soup.find_all('a', class_='js-navigation-open'):\n        arr = markup.get_text().split('\\n')\n        if re.search('\\d\\d\\-\\d\\d\\-\\d\\d\\d\\d.csv',arr[0]):\n            files.append(arr[0])\n    return files\n\ndef exDate(sDat, sFormat='%m-%d-%Y'):\n    \"\"\"remove the extension from a file name formated in sFormat and return a datetime object\"\"\"\n    return dt.strptime(sDat[0:-4], sFormat)\n\ndef strfdat(datIn, sFormat = '%m-%d-%Y'): \n    \"\"\"return a filename with a .csv extension and a date formatted string name\"\"\"\n    return datIn.strftime(sFormat)+'.csv'\n\ndef trimFileList(fetchList, datCutOff, sFormat = '%m-%d-%Y'):\n    \"\"\"Pop any files of the form %m-%d-%Y.csv out of fetchList if they occur on or before any of the files in dirList\"\"\"\n    fetchFile = fetchList\n    retList = []\n    for n in range(len(fetchFile)):\n        if dt.strptime(fetchFile[n].split('.')[0], sFormat) > datCutOff: \n            retList.append(fetchFile[n])\n    return retList\n\ndef removeUndated(fileList, sFormat = '%m-%d-%Y'):\n    undatedRemoved = fileList\n    n = 0\n    while n <= len(undatedRemoved):\n        try:\n            n += 1\n            dt.strptime(undatedRemoved[n-1].split('.')[0], sFormat)\n        except ValueError:\n            pop = undatedRemoved.pop(n-1)\n            n -= 1\n        except IndexError:\n            break\n    return undatedRemoved\n\ndef sortFileList(fileList):\n    fl = np.array(list(map(exDate, fileList)))\n    fl = np.sort(fl)\n    #Convert sorted arrray back to an array of file names\n    fl = list(map(strfdat, fl))\n    return fl\n\ndef grabDataFromURL(urlroot, filename):\n    url = urlroot+'/'+filename\n    logger.info('grabDataFromURL() - Grabbing and adding data from file at ' + url)\n    #Check the date and grab the data according the the correct schema\n    if dt.strptime(filename.split('.')[0], '%m-%d-%Y') < dt.strptime(\"03-01-2020\", '%m-%d-%Y'):\n        df2 = pd.read_csv(filepath_or_buffer = url, sep = ',', index_col= False, header = 0,\\\n                         names = ['province', 'country', 'recorded', 'confirmed', 'deaths', 'recovered'])\n    elif dt.strptime(filename.split('.')[0], '%m-%d-%Y') < dt.strptime(\"03-22-2020\", '%m-%d-%Y'):\n        df2 = pd.read_csv(filepath_or_buffer = url, sep = ',', index_col= False, header = 0,\\\n                names = ['province', 'country', 'recorded', 'confirmed', 'deaths', 'recovered', 'lat', 'lon'])\n    elif dt.strptime(filename.split('.')[0], '%m-%d-%Y') < dt.strptime(\"05-29-2020\", '%m-%d-%Y'):\n        df2 = pd.read_csv(filepath_or_buffer = url, sep = ',', index_col= False, header = 0,\\\n                names = ['fips', 'city', 'province', 'country', 'recorded', 'lat', 'lon', 'confirmed', 'deaths', 'recovered', 'active', 'combined_key'])        \n    else:\n        df2 = pd.read_csv(filepath_or_buffer = url, sep = ',', index_col= False, header = 0,\\\n                names = ['fips', 'city', 'province', 'country', 'recorded', 'lat', 'lon', 'confirmed', 'deaths', 'recovered', 'active', 'combined_key', 'incidence_rate', 'case-fatality_ratio'])\n    #log what's been grabbed\n    logger.info('grabDataFromURL() - Data grabbed is \\n' + str(df2.values) + '\\n')\n    return df2                    \n\n#define variables\nhomepage = 'https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports'\nurlroot = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports'    \nlocalpath = 'C:\\\\Users\\\\Sean\\\\Documents\\\\Dev\\\\Covid-19\\\\'\n    \n#Create logger\nLOG_FORMAT = \"%(levelname)s %(asctime)s - %(message)s\"\nlogging.basicConfig(filename = localpath+'Covid-19.log',\n                   level = logging.DEBUG,\n                   format = LOG_FORMAT,\n                   filemode = 'w')\nlogger = logging.getLogger()\n\n# Execution\ntry: \n    fl = getFileList(homepage)\n    # Convert fl to to numpy series of dates to prepare for descending sort\n    ldir = os.listdir(localpath)\n    # Remove the files from ldir that don't conform to a date\n    ldir = removeUndated(ldir)    \n    ldir = sortFileList(ldir)\n    # Remove files from fl that have already been grabbed and sit in local files\n    if len(ldir) > 0:\n        fl = trimFileList(fl, exDate(ldir[-1]))\n    # Sort the remaining files in the file list\n    fl = sortFileList(fl)         \n    # Prepare empty dataframe for population\n    df = pd.DataFrame()\n    if len(ldir) > 0:\n        df = pd.read_csv(localpath+ldir[-1])        \n    for n in fl:\n        df2 = grabDataFromURL(urlroot, n)\n        df2.insert(0, 'file', n.split('.')[0])\n        df = df.append(df2)\n    # Drop columns like \"Unnamed:*\" that are getting created in the Dataframe.append() process\n    rexp = re.compile('^Unnamed:')\n    for n in list(df.columns):\n        if rexp.match(n):\n            df.drop(n, axis =1, inplace = True)    \n    # save the accumulated file locally\n    if len(fl) > 0:\n        if fl[-1] not in os.listdir(localpath):\n            df.to_csv(localpath+'\\\\'+fl[-1])        \n    logger.info('pull_data() - output \\n'+str(df.values))\n    \nfinally:\n    logging.shutdown()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning Data\n\nThe following code snippet (you can expand the cell if you're curious) illustrates how the data that was collected was cleaned.\n"},{"metadata":{"trusted":false,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"def add_column(sName, df, fFunc):def add_column(sName, df, fFunc):\n    df2 = df.copy()\n    newCol = []\n    for n in range(0, df.shape[0]):\n        newCol.append(fFunc(df2, n))\n    df2[sName] = newCol\n    return df2    \n    \ndef get_mDate(df, n):\n    datestr = df['file'][n]\n    datestrrec = df['recorded'][n]\n    dtime = dt.strptime(datestrrec, getDateFormatString(datestr, datestrrec))\n    return mdates.date2num(dtime)\n\n# def get_mDate(df,n):\n#     dTime = dt.strptime(df['file'][n], '%m-%d-%Y')\n#     return np.floor(mdates.date2num(dTime))\n\ndef get_Active(dftemp, n):\n#     import pdb; pdb.set_trace()\n    confirmed = dftemp['confirmed'][n]\n    recovered = dftemp['recovered'][n]\n    deaths = dftemp['deaths'][n]\n    rtypes = ['float', 'int', 'numpy.float64']\n#     if type(confirmed) not in rtypes or type(recovered) not in rtypes or type(deaths) not in rtypes:\n#         print('Wrong data type(s) detected.')\n#         print('[confirmed] is %s of type %s.' %(confirmed, type(confirmed)))\n#         print('[recovered] is %s of type %s.' %(recovered, type(recovered)))\n#         print('[deaths] is %s of type %s.' %(deaths, type(deaths)))\n#         print('[n] is %d, and [file] is %s' %(n, df['file'][n]))\n#         return 0.0\n#     else:\n    return (confirmed - recovered - deaths)\n\ndef get_Resolved(df, n):\n    return (df['recovered'][n]+df['deaths'][n])\n\ndef splDateStr(sDtime):\n    sRecParts = re.split('[,-/:T .]', sDtime)\n    for n in range(len(sRecParts)):\n        sRecParts[n] = int(sRecParts[n])\n    return sRecParts\n    \ndef datInt(sDateRef):\n    ldat = splDateStr(sDateRef)\n    return ldat[1]+ldat[0]*31+ldat[2]*366\n            \ndef compareDates(sDateRef, sDate):\n    \"\"\"Takes two date strings, the reference datestring [sDateRef] formatted as mm-dd-YYYY as a later date \n    that is reasonably close to the inspected datestring [sDate], and returns the most probable order for sDate\"\"\"\n    lRef = splDateStr(sDate)\n    a = str(lRef[0])\n    b = str(lRef[1])\n    c = str(lRef[2])\n    nDate1 = datInt(sDate)\n    nDate2 = datInt(str(b)+'-'+str(a)+'-'+str(c))\n    nDate3 = datInt(str(b)+'-'+str(c)+'-'+str(a))\n    refDat = splDateStr(sDateRef)\n    sDateRef2 = str(refDat[0])+'-'+str(refDat[1])+'-'+str(refDat[2]%100)\n    nDateRef = datInt(sDateRef)\n    nDateRef2 = datInt(sDateRef2)\n    lDiffs = np.array([nDateRef-nDate1, nDateRef-nDate2, nDateRef-nDate3,\n                       nDateRef2 - nDate1, nDateRef2-nDate2, nDateRef2-nDate3])\n    mask = lDiffs > -3\n    bestMatch = lDiffs[mask].min()\n    if abs(lDiffs[0])==abs(bestMatch):\n        return 'mdY'\n    elif abs(lDiffs[1])==abs(bestMatch):\n        return 'dmY'\n    elif abs(lDiffs[2])==abs(bestMatch):\n        return 'Ymd'\n    elif abs(lDiffs[3])==abs(bestMatch):\n        return 'mdy'\n    elif abs(lDiffs[4])==abs(bestMatch):\n        return 'dmy'\n    elif abs(lDiffs[5])==abs(bestMatch):\n        return 'ymd'\n    else:\n        return None\n\ndef getDateFormatString(sDateRef, sDate):\n    lookup = memoDictDt2Fmt.get(sDate, 'None')\n    if  lookup == 'None':\n        sep = re.split('\\d+', sDate)\n        sSeq = compareDates(sDateRef, sDate)\n        sFormat = '%'+sSeq[0]+sep[1]+'%'+sSeq[1]+sep[2]+'%'+sSeq[2]+sep[3]\n        nParts = len(re.split('[,-/:T .]', sDate))\n        if nParts == 5:\n            sFormat = sFormat+\"%H\"+sep[4]+'%M'\n        elif nParts == 6:\n            sFormat = sFormat+\"%H\"+sep[4]+'%M'+sep[5]+'%S'\n        elif nParts == 7:\n            sFormat = sFormat+\"%H\"+sep[4]+'%M'+sep[5]+'%S'+sep[6]+'%f'\n        memoDictDt2Fmt[sDate]=sFormat\n        return sFormat\n    else:\n        return lookup\n\ndf2 = df.copy()\n\ndf2.reset_index(inplace=True, drop = True)\ndf2['deaths'] = df2['deaths'].fillna(0.0)\ndf2['confirmed']= df2['confirmed'].fillna(0.0)\ndf2['recovered']= df2['recovered'].fillna(0.0)\ndf2 = add_column('calc_active', df2, get_Active)\ndf2 = add_column('resolved', df2, get_Resolved)\nmemoDictDt2Fmt = {}\ndf2 = add_column('mDate', df2, get_mDate)\n# df2['flmDate'] = np.floor(df2['mDate'])\n# df2.drop_duplicates(subset = ['flmDate', 'country', 'province'], inplace = True)\n\n# Strip string values\ndf2['country']=list(map(lambda n: n.strip(), df2['country']))\ndf2['province']=df2['province'].fillna('')\ndf2['city']=df2['city'].fillna('')\n# df2['province']=list(map(lambda n: n.strip(), df2['province']))\n# df2['admin2']=list(map(lambda n: n.strip(), df2['admin2']))\n#Clarifications needed as some countries are referred to by more than one identifier\nreplaceDict = {'Mainland China':'China', 'Bahamas, The':'Bahamas',\n               'The Bahamas': 'Bahamas', 'Taipei and environs':'Taiwan',\n              'Gambia, The': 'Gambia', 'The Gambia':'Gambia', 'Republic of Ireland':'Ireland',\n              'UK':'United Kingdom', 'Congo (Brazzaville)':'Republic of the Congo',\n              'Congo (Kinshasa)':'Democratic Republic of the Congo'}\ndf2.replace(replaceDict, inplace=True)\n\n# df2['country'].replace(to_replace='Hong Kong', value = 'China', inplace=True)\n# logger.info('transform_data() - returning df\\n'+str(df.values))\n\ndropList = {'province': ['Unassigned Location, WA', 'Unknown Location, MA', 'Recovered', 'None', 'Unknown',\n                        'Diamond Princess', 'Grand Princess', 'Grand Princess Cruise Ship', 'From Diamond Princess',\n                        'Lackland, TX (From Diamond Princess)', 'Omaha, NE (From Diamond Princess)',\n                        'Travis, CA (From Diamond Princess)', 'Unassigned Location (From Diamond Princess)'],\n           'country': ['Others', 'Russian Federation', 'Viet Nam', 'Cruise Ship', 'MS Zaandam', 'Diamond Princess']}\nfor rgn in dropList:\n    for item in dropList[rgn]:\n        df2.drop(df2[df2[rgn] == item].index, axis = 0, inplace = True)\n\n#change [province, country] pairs\ncorrDict = {('Chicago','US'):['Chicago, IL','US'], ('Macau', 'Macau'):['Macau', 'China'],\n            ('','Taiwan*'):['','Taiwan'], ('Macau', 'Macao SAR'):['Macau SAR', 'China'],\n            ('', 'Macao SAR'):['Macau SAR','China'], ('Denmark','Denmark'):['Mainland','Denmark'],\n            ('France','France'):['Mainland', 'France'], ('','Mayotte'):['Mayotte', 'France'],\n            ('','Reunion'):['Reunion', 'France'], ('','Saint Martin'):['Saint Martin', 'France'],            \n            ('', \"\"\"\"('St. Martin',)\"\"\"):['Saint Martin', 'France'],\n            ('','Saint Barthelemy'):['Saint Barthelemy', 'France'],\n            ('Mexico','Mexico'):['','Mexico'],\n            ('Netherlands', 'Netherlands'):['Mainland','Netherlands'], \n            ('Taiwan','Taiwan'):['','Taiwan'],\n            ('United Kingdom','United Kindgom'):['England','United Kingdom'], \n            ('', 'North Ireland'):['Northern Irelend','United Kingdom'], \n            ('', 'Hong Kong'):['Hong Kong SAR', 'China'], \n            ('Hong Kong', 'China'):['Hong Kong SAR', 'China'],\n            ('Hong Kong', 'Hong Kong'):['Hong Kong SAR', 'China'],\n            ('Hong Kong','Hong Kong SAR'):['Hong Kong SAR', 'China'],\n            ('','Hong Kong SAR'):['Hong Kong SAR', 'China'], \n            ('','Iran (Islamic Republic of)'):['','Iran'],\n            ('','French Guiana'):['French Guiana','France'], \n            ('','Channel Islands'):['Channel Islands', 'United Kingdom'],\n            ('','Cayman Islands'):['Cayman Islands', 'United Kingdom'],\n            ('','Korea, South'):['', 'Republic of Korea'],\n            ('','South Korea'):['','Republic of Korea'], \n            ('','Republic of Moldova'):['','Moldova'],\n            ('','occupied Palestinian territory'):['','Palestine'],\n            ('','West Bank and Gaza'):['','Palestine']   }\nfor rgn in corrDict:\n    sProv, sCtry = rgn\n    sNewProv, sNewCtry = corrDict[rgn]\n    flt = (df2['province'] == sProv) & (df2['country'] == sCtry)\n    df2.loc[flt,'province'] = df2.loc[flt,'province'].replace(to_replace = sProv, value = sNewProv)\n    df2.loc[flt,'country'] = df2.loc[flt,'country'].replace(to_replace = sCtry, value = sNewCtry)\n\n# drop 4 rows for US, US only because they don't fit into part of a larger sequence\nflt = (df2['country'] == 'US') & (df2['province'] == 'US')\ndf2.drop(df2[flt].index, axis = 0, inplace = True)\ndf2.to_csv(localpath+'\\\\Cleaned_Data.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getMortRate(df):\n    deaths = df['deaths'].to_numpy()[-1]\n    cases = df['confirmed'].to_numpy()[-1]\n    lastdate = df.index.to_numpy()[-1] - DaysToResolve\n    df2 = df[df.index <= lastdate]\n    if df2.shape[0]>0:\n        cases = df2['confirmed'].to_numpy()[-1]\n    if deaths > cases:\n        deaths = cases\n    if cases != 0:\n        return np.abs(100*deaths/cases)\n    else:\n        return np.NaN\n\ndef PlotCases(df, sTitle):\n    fontsz = (12,15)\n    fig, ax = plt.subplots(1, 1, figsize=(16,9))\n    ax.plot(df['mDate'], df['confirmed'],label='All Cases', color='b')\n    ax.plot(df['mDate'], df['active'], label = 'Active Cases', color = 'orange')\n    ax.plot(df['mDate'], df['resolved'], label = 'Resolved Cases', color='g')\n    ax.set_ylim(0, max(ax.get_ylim())*1.1)\n    ax2 = ax.twinx()\n    ax2.fill_between(df['mDate'], 0, df['deaths'], label = 'Deaths', color='r', alpha = 0.2)\n    if (df['deaths'].max() == np.NaN) or (df['deaths'].max() == 0):\n        deaths = 5.0\n    else:\n        deaths = df['deaths'].max()  \n    ax2.set_ylim(0, 2*deaths)\n    ax2.grid(False)\n    xMortRate = getMortRate(df)\n    if xMortRate != np.nan and xMortRate > 0.0:\n        xPos = (df['mDate'].to_numpy()[-1]+df['mDate'].to_numpy()[0])/2\n        yPos = (max(plt.ylim())/10)\n        plt.text(xPos, yPos, 'Mortality Rate: '+\"{:.2f}\".format(xMortRate)+'%',\\\n                verticalalignment = 'bottom', horizontalalignment = 'center',\\\n                color = 'k', fontsize=fontsz[0], alpha=0.6)\n    leg = ax.legend(loc=2,ncol=1,prop={'size':fontsz[0]})\n    leg.get_frame().set_alpha(0.4)\n    leg = ax2.legend(loc=6,ncol=1,prop={'size':fontsz[0]})\n    leg.get_frame().set_alpha(0.4)\n    ax.grid()\n    ax.xaxis.set_major_formatter(mdates.DateFormatter('1 %b %y'))\n    ax.xaxis.set_major_locator(mdates.MonthLocator())\n    ax.xaxis.set_minor_locator(mdates.WeekdayLocator(byweekday=6))\n    plt.title(sTitle, size=fontsz[1])\n    fig.autofmt_xdate()\n    plt.show()\n\ndef getPlotData(df, country, province = '', city = '', ):\n    if len(city) > 0:\n        flt = (df['country']==country) & (df['province']==province) & (df['city']==city)\n        dfint = df[flt].loc[:,['mDate', 'confirmed', 'calc_active', 'deaths', 'resolved']]\n        dfint['mDate']=np.floor(dfint.loc[:, 'mDate'])\n        dfint.drop_duplicates(subset = 'mDate', inplace = True)\n        return dfint.groupby('mDate').agg(\\\n                    confirmed = ('confirmed', ['first']),\\\n                    active = ('calc_active', 'first'),\\\n                    deaths = ('deaths', 'first'),\\\n                    resolved = ('resolved', 'sum'))\n    elif (len(province) >0):\n        flt = ((df['country']==country) & (df['province']==province))\n        dfint = df[flt].loc[:,['city', 'mDate', 'confirmed', 'calc_active', 'deaths', 'resolved']]\n        dfint['mDate'] = np.floor(dfint.loc[:, 'mDate'])\n        dfint.drop_duplicates(subset = ['mDate', 'city'], inplace = True)\n        return dfint.groupby('mDate').agg(\\\n                    confirmed = ('confirmed', 'sum'),\\\n                    active = ('calc_active', 'sum'),\\\n                    deaths = ('deaths', 'sum'),\\\n                    resolved = ('resolved', 'sum'))\n    else:\n        flt = (df['country']==country)\n        dfint = df[flt].loc[:,['province', 'city', 'mDate', 'confirmed', 'calc_active', 'deaths', 'resolved']]\n        dfint['mDate'] = np.floor(dfint.loc[:, 'mDate'])\n        dfint.drop_duplicates(subset = ['mDate', 'province', 'city'], inplace = True)\n        return dfint.groupby('mDate').agg(\\\n                    confirmed = ('confirmed', 'sum'),\\\n                    active = ('calc_active', 'sum'),\\\n                    deaths = ('deaths', 'sum'),\\\n                    resolved = ('resolved', 'sum'))\n\nDaysToResolve = 18\nurl = '../input/pull-datacsv/Cleaned_Data.csv'\n\ndf3 = pd.read_csv(url)\n\nfor country in df3.groupby('country'):\n    sCountry = country[0].strip()\n    df4 = getPlotData(df3, country = sCountry)\n    df4 = df4.reset_index()\n    if df4.shape[0]>30:\n        PlotCases(df4, sTitle= sCountry)\n    for province in df3[df3['country']==country[0]].groupby('province'):\n        sProvince = province[0].strip()\n        if sProvince != '':\n            df4 = getPlotData(df3, country= sCountry, province = sProvince)\n            df4 = df4.reset_index()\n            if df4.shape[0]>30:\n                PlotCases(df4, sTitle = sProvince+', '+sCountry)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysing Regional Data\n\nFirst, create a data frame that can be used to store information gathered for each region."},{"metadata":{"trusted":true},"cell_type":"code","source":"def populateRegions(dfIn):\n    dfReturn = dfIn.loc[:,['province', 'country']]\n#    Some countries in the list are only represented by their provinces, so it is necessary to append a list of countries\n    dfAppend = dfIn.loc[:, ['country']]\n    dfAppend['province'] = np.NaN\n    dfReturn = dfReturn.append(dfAppend)\n#     dfReturn['country']=list(map(lambda name: name.strip(), dfReturn['country']))\n#     dfReturn['province']=list(map(lambda name: name.strip(), dfReturn['province']))\n    dfReturn = dfReturn.drop_duplicates().sort_values(by= ['country', 'province'], ascending=True)\n    dfReturn.reset_index(inplace = True, drop = True)\n    return dfReturn\n\ndfRegionData = populateRegions(df3)\ndfRegionData","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I want to evaluate how long, on average, it takes for patients in each region to achieve 'resolved' status from the time they are recorded as having been infected.  in this case $n_{resolved} = n_{deaths} + n_{recovered}$ and is representative of the R in the S.I.R. model for epidemic progression.  To make this calculation, we know that 100% of cases will resolve, and so we simply compute the average difference between the date $n_{cases}$ reached a certain level and the date at which $n_{resolved}$ reached that same level."},{"metadata":{"trusted":true},"cell_type":"code","source":"def getDays2Resolve(df, country, province = ''):\n    dfSeries = getPlotData(df, country, province)\n    dfSeries.reset_index(inplace = True)\n    #cycle down from the end of the series locating the date of the \n    #   lowest number of confirmed that is greater than resolved\n    resolvedays = []\n    for n in dfSeries.index[-1::-1]:\n        res = dfSeries['resolved'][n]\n        flt = dfSeries['confirmed']>=res\n        if dfSeries[flt]['mDate'].shape[0]>0:\n            dtconfirmed = dfSeries[flt]['mDate'].min()\n            resolvedays.append(dfSeries['mDate'][n]-dtconfirmed)\n    nresolve = np.array(resolvedays)\n    if len(nresolve)>0:\n        return nresolve.mean(), nresolve.std(), len(nresolve)\n    else:\n        return np.NaN, np.NaN, np.NaN\n\n# print(getDays2Resolve(df3, 'Canada', 'Alberta'))\ndaysToRecover = []\nfor n in dfRegionData.index:\n    prov = str(dfRegionData['province'][n])\n    if prov != 'nan' and len(prov)>0:\n        daysToRecover.append(getDays2Resolve(df3, dfRegionData['country'][n], dfRegionData['province'][n]))\n    else:\n        daysToRecover.append(getDays2Resolve(df3, country = dfRegionData['country'][n]))\nnparr = np.array(daysToRecover)\ndfRegionData['resolveDaysMean'] = nparr[:,0]\ndfRegionData['resolveDaysStd'] = nparr[:,1]\ndfRegionData['resolveDaysCount'] = nparr[:,2]\ndfRegionData","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Perform a scatter plot that shows the time to resolve on the x-axis and the standard deviation as a percentage of the time to resolve on the y-axis.  By looking for a cluster near the bottom of the plot, we will have identified the most probable value for the time to resolve if this could indeed be considered a property of the Covid-19 disease.  To help weed out poorer-quality data, I choose to restrict the scatter plot to regions where the count of data points is higher than 30."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfPlotData = dfRegionData\nflt = (dfPlotData['resolveDaysMean']>0.0) & (dfPlotData['resolveDaysStd']>0.0) & (dfPlotData['resolveDaysCount']>=30)\ndfPlotData = dfPlotData[flt]\ndfPlotData['ratio']=list(map(lambda x,y:x/y*100, dfPlotData['resolveDaysStd'],dfPlotData['resolveDaysMean']))\nflt = dfPlotData['ratio']<140.0\ndfPlotData = dfPlotData[flt]\nfig = plt.figure(dpi=150, figsize = (8,8))\n\ngs = fig.add_gridspec(2, 2,  width_ratios=(7, 2), height_ratios=(2, 7),\n                      left=0.1, right=0.9, bottom=0.1, top=0.9,\n                      wspace=0.05, hspace=0.05)\n\nax = fig.add_subplot(gs[1, 0])\nax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\nax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\nax.scatter(dfPlotData.loc[:,('resolveDaysMean')],dfPlotData.loc[:,('ratio')], s = 1)\nax2 = ax_histx.hist(dfPlotData['resolveDaysMean'], bins = 100, rwidth = 0.8)\nax3 = ax_histy.hist(dfPlotData['ratio'], bins = 100, rwidth =0.8, orientation = 'horizontal')\nax.grid(True)\nax_histx.grid(True)\nax_histy.grid(True)\nax.set_ylabel('Std. Dev. as a Percentage')\nax.set_xlabel('Days to Resolve')\nax_histx.set_title('Days to Resolve Covid-19 by Region')\nfont_dict = {'family':'serif',\\\n                'color':'darkred',\\\n                'size':8}\nax.text(40,80,'One region somewhere on earth represented by one marker.', fontdict = font_dict, ha = 'center', wrap = True)\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph indicates a cluster of points that is elongated about a diagonal.  I interpret this as regions that report longer recovery times have greater variation in recovery times.  Even without the graph, it stands to reason that this would be true.  Several regions don't conform to this cluster, and it would seem these are the regions where there are problems with the data (in collection, reporting, or conveyance) such as with the arm that extends upward close to the [days to resolve] = 0 axis, or that these are regions that struggle with dealing with Covid-19 most acutely.  Either way, it's clear that 7 is not the correct number, but a number closer to 20 is acceptable.  This would increase the results for the calculations of mortality rates."},{"metadata":{},"cell_type":"markdown","source":"### Fitting curves"},{"metadata":{},"cell_type":"markdown","source":"I first wish to fit an exponential curve to the leading edge of an epidemic in each reagion.  This will take on the form $$cases_{confirmed} = e^{k(t-t_0)}$$  A fit of this curve will provide $t_0$ as the start date of the pandemic for each region and $e^k$, the initial daily growth rate.  Note: the medical community defines the start date as the date at which 100 cases are confirmed.  However, this function defines a t_0 at the time where the first case would have been confirmed if the series conformed perfectly to an exponential progression.\n\nTo accomplish this, I can use linear regression to find the best fit to the series of confirmed cases $\\{{c_0, c_1, c_2,...,c_n\\}}$ after it is transformed using a logarithm $b_i = ln(c_i)$ to create another series $\\{b_0, b_1, b_2,...,b_n\\}$ that will conform to the linear progression $k\\left(t-t_0\\right)$.  After the best fit is found, it is a simple matter to reverse the transform to compare with the original series.\n\nI wish to find the longest sustained segment of $\\{c_n\\}$ that can be said to fit exponential growth.  I observe the best fit will occur near the segment of the series $\\{c_n\\}$ where the daily growth rate, calculated as $r_d = \\frac{c_{n+1}}{c_n\\times(d_{n+1}-d_n)}$ is highest, and so I can eliminate much of the computation by choosing to eliminate the range of dates where the daily growth rate is in its 30th percentile or less.\n\nAlso, recall the formulae for linear regression:\n- slope: $$k = \\frac {n \\left(\\sum c_it_i \\right) - \\sum t_i \\sum c_i}{n \\left( \\sum t_i^2 \\right) - \\left( \\sum t_i \\right)^2}$$\n- y-interecept: $$kt_0 = \\frac{ \\left( \\sum c_i \\right) \\left( \\sum t_i^2 \\right) - \\sum t_i \\sum c_it_i}{n \\left( \\sum t_i^2 \\right) - \\left( \\sum t_i \\right)^2}$$\n- correlation coefficient $$r = \\frac{\\sum \\left( t_i - \\bar{t} \\right) \\left( c_i - \\bar{c} \\right)}{\\sqrt{\\sum \\left( t_i - \\bar{t} \\right)^2 \\sum \\left( c_i - \\bar{c} \\right)^2 }}$$"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"def linreg(df):\n    \"\"\"Returns linear regression data on first two columns of the indexed pandas Dataframe passed to it\"\"\"  \n#     print(df)\n    n = df.shape[0]\n    sumxy, sumx, sumx2, sumy, sumy2, sumvarx, sumvary, sumvarxvary, sumvarx2, sumvary2 = 10 * [0.0, ]\n    meanx, meany = df.mean()[0], df.mean()[1]\n    for i in range(n):\n        x = df.iloc[i,0]\n        y = df.iloc[i,1]\n#         print(x)\n        sumxy += x * y\n        sumx += x\n        sumx2 += x ** 2\n        sumy += y\n        sumy2 += y ** 2\n        sumvarx += x-meanx\n        sumvary += y-meany\n        sumvarxvary += sumvarx*sumvary\n        sumvarx2 += np.square(sumvarx)\n        sumvary2 += np.square(sumvary)\n    m = (n*sumxy-sumx*sumy)/(n*sumx2 - sumx**2)\n    b = (sumy*sumx2-sumx*sumxy)/(n*sumx2 - sumx**2)\n    r = (sumvarxvary)/(sumvarx2 * sumvary2) ** 0.5\n    return m, b, r\n\ndef get_Series(df, country, province = ''):\n    dfint = getPlotData(df, country, province)\n    # First, enumerate the date, value pairs by creating a dataframe with an index \n    dfint.reset_index(inplace = True)\n    return dfint.loc[:, ['mDate', 'confirmed']]\n\ndef get_BestFitExp(df, country, province = ''):\n    dfint = get_Series(df, country, province)\n    #append daily growth rates\n    growth = [0,]\n    for n in range(dfint.shape[0]-1):\n        timedif = dfint.iloc[n+1,0]-dfint.iloc[n,0]\n        if timedif > 0:\n            growth.append(dfint.iloc[n+1,1]/(dfint.iloc[n,1]*timedif))\n        else:\n            growth.append(np.NaN)\n    dfint['dgrowth'] = growth\n#     print(dfint)\n    #determine range of greatest interest to reduce calculation time\n    dfsorted = dfint.sort_values(by='dgrowth', ascending = False)\n    dfsorted.reset_index(inplace=True)\n#     print(dfsorted)\n    cutoff = 0.1\n    growthcutoff = dfsorted.loc[np.floor(dfsorted.shape[0]*cutoff), 'dgrowth']\n#     print('Growth cut-off value is %s' % growthcutoff)\n    flt = dfint['dgrowth'] >= growthcutoff\n    dfsorted = dfint[flt]\n#     print(dfsorted)\n    highDate = dfsorted.agg({'mDate': 'max'})[0]\n    lowDate = dfsorted.agg({'mDate': 'min'})[0]    \n    #insert logarithm of 'confirmed'\n    def lbind(value, cutoff):\n        if value < cutoff:\n            return cutoff\n        else:\n            return value\n    dfint.insert(1, 'lnconfirmed', list(map(lambda value: np.log(lbind(value, np.exp(-100))), dfint['confirmed'])))\n    #iterate through the series trimming leading and trailing dates, accepting each trim if r increases as a result\n    #Grab the index of lowDate and highDate\n    lowIndex = dfint.loc[dfint['mDate'] == lowDate].index[0]\n    highIndex = dfint[(dfint['mDate'] == highDate)].index[0]\n#     print('Starting lowIndex = %s and starting highIndex = %s' % (lowIndex, highIndex))\n    #Define the minimum distance between lowIndex and highIndex\n    minDist = 7\n    maxIter = 100\n    stopChecking = False\n    bestR = linreg(dfint.loc[lowIndex:(highIndex+1), ['mDate', 'lnconfirmed']])  \n    while (highIndex - lowIndex > minDist) & (not stopChecking) & (maxIter > 0):\n        maxIter -= 1\n        bestR = linreg(dfint.loc[lowIndex:(highIndex+1), ['mDate', 'lnconfirmed']])        \n        if highIndex - lowIndex > minDist:\n            rLowInside = linreg(dfint.loc[(lowIndex+1):(highIndex+1), ['mDate', 'lnconfirmed']])\n            rHighInside = linreg(dfint.loc[(lowIndex):(highIndex), ['mDate', 'lnconfirmed']])\n        else:\n            rLowInside = (0, 0, 0)\n            rHighInside = (0, 0, 0)\n        if highIndex < (dfint.shape[0]-1):\n            rHighOutside = linreg(dfint.loc[(lowIndex):(highIndex+2), ['mDate', 'lnconfirmed']])\n        else:\n            rHighOutside = (0,0,0)\n        if lowIndex > 0:\n            rLowOutside = linreg(dfint.loc[(lowIndex-1):(highIndex+1), ['mDate', 'lnconfirmed']])\n        else:\n            rLowOutside = (0,0,0)\n        bestmove = np.max([(bestR[0]*bestR[2]), (rLowInside[0]*rLowInside[2]), \\\n                            (rHighInside[0]*rHighInside[2]), (rHighOutside[0]*rHighOutside[2]),\\\n                           (rLowOutside[0]*rLowOutside[2])])\n        if bestmove == (bestR[0]*bestR[2]): #Stay put, most optimal solution has been achieved\n            stopChecking = True\n        elif bestmove == (rLowInside[0]*rLowInside[2]): #Increase lowIndex\n            lowIndex += 1\n#             print('Increasing lowIndex to %s' % lowIndex)\n        elif bestmove == (rHighInside[0]*rHighInside[2]): #Decrease highIndex\n            highIndex -= 1\n#             print('Decreasing highIndex to %s' % highIndex)\n        elif bestmove == (rHighOutside[0]*rHighOutside[2]): #Increase highIndex\n            highIndex += 1\n#             print('Increasing highIndex to %s' % highIndex)\n        elif bestmove == (rLowOutside[0]*rLowOutside[2]): #Decrease lowIndex\n            lowIndex -= 1\n#             print('Decreasing lowIndex to %s' % lowIndex)\n    bestR = list(bestR)\n    bestR.append(lowIndex)\n    bestR.append(highIndex)\n    return bestR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now commit our findings of exponential curve fitting for all regions"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"dfRegionData.style.background_gradient(cmap='Greens')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Curve Approximation\n\nA logistic curve has a similar morphology to the S.I.R. model of infected cases, \n![logistic_curve_shape.png](attachment:logistic_curve_shape.png)\nand is described analytically, \n$$ cases_{confirmed} \\left( t \\right) = \\frac {S_\\infty} {1 + e ^ {-k \\left( t - t_m \\right)}} $$ \n\nwhere $S_\\infty$ is the number of people projected to become infected, $k$ is a growth factor (not equal, but relatable to the growth factor in the best-fit exponential curve), and $t_m$ is the median time of the regional epidemic.\n\nWhile it might be possible to introduce a transform to the standard logistic function $$\\frac {1} {1 + e ^ {-k \\left( t - t_m \\right)}} $$ of two variables $ k $ and $ t_m $ to render it amenable to linear regression, it will not be possible to use linear regression to fit a curve to this function that possesses three independent variables.  Furthermore, a quick survey of machine learning libraries did not reveal an algorithm that works with a numerator other than 1.  \n\nInstead, I write a class to handle the type of regression that is needed.","attachments":{"logistic_curve_shape.png":{"image/png":"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"}}}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}