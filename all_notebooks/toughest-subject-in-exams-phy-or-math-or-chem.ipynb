{"cells":[{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"!pip install --upgrade seaborn","execution_count":null,"outputs":[]},{"metadata":{"id":"pQEnETXtSGR1","trusted":true},"cell_type":"code","source":"import gc\nimport re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.ticker as ticker\n\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm_notebook as tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"id":"eNHy6BlsgRcp","outputId":"d6e2bc9c-faf7-4816-a86c-fec904f46663","_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"nltk.download(\"stopwords\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def func(pct, allvalues):\n    absolute = int(pct /100.*np.sum(allvalues))\n    return \"{:.1f}%\".format(pct)\n    pass\n\n\ndef remove_single_word_num(sent):\n    '''\n    Remove numbers and words of single length such as \"x + 23 y - abc\" will become \"+ - abc\"\n    '''\n    dummy_list = []\n  \n    for token in sent.split():\n        if (not token.isdigit()) and ((token.isalpha() and len(token)>1) or (not token.isalnum())):\n            dummy_list.append(token)   \n  \n    return ' '.join(dummy_list)\n\n\ndef insert_spaces(sentence):\n    '''\n    Add a space around special characters, number and digits. So \"2x+y -1/3x\" becomes: \"2 x + y - 1 / 3 x\"\n    '''\n    dummy_list = []\n    splitted_sent = list(sentence)\n    \n    for i in range(len(splitted_sent)-1):\n        dummy_list.append(splitted_sent[i])\n        \n        if splitted_sent[i].isalpha(): # if it is an alphabet\n            if splitted_sent[i+1].isdigit() or (not splitted_sent[i+1].isalnum()):\n                dummy_list.append(' ')\n    \n        elif splitted_sent[i].isdigit(): # if it is a number\n            if splitted_sent[i+1].isalpha() or (not splitted_sent[i+1].isalnum()):\n                dummy_list.append(' ')\n        \n        elif (not splitted_sent[i].isalnum()) and (splitted_sent[i] not in [' ','\\\\']): # if it is a special char but not ' ' already\n            if splitted_sent[i+1].isalnum():\n                dummy_list.append(' ')\n        \n    dummy_list.append(splitted_sent[-1])\n  \n    return ''.join(dummy_list)\n\n\ndef preprocess(a):\n    # convert the characters into lower case\n    a = a.lower()\n\n    # remomve newline character\n    a = re.sub(\"\\\\n\", \" \", a)\n\n    # remove the pattern [ whatever here ]. Use { } or  ( ) in place of [ ] in regex\n    a = re.sub(r\"\\[(.*?)\\]\",' ',a)\n\n    # remove Questions beginners Q5. 5. question 5. \n    a = re.sub(r\"^[\\w]+(\\s|\\.)(\\s|\\d+(\\.*(\\d+|\\s)))\\s*\", \" \", a)\n\n    # remove MathPix markdown starting from \\( and ending at \\) while preserving data inside \\text { preserve this }\n    a = re.sub(r'\\s*\\\\+\\((.*?)\\\\+\\)', lambda x: \" \".join(re.findall(r'\\\\[a-z]{3,}\\s*{([^{}]*)}', x.group(1))), repr(a))\n\n    # remove options from questions i.e character bounded by () given there is no spacing inside ()\n    a = re.sub(r\"\\s*\\([^)\\s]*\\)\\s*\", \" \", a)\n\n    # remove any repeating special character (more than one times) except \\(){}[] and space.  So it'll remove .. ,, ___ +++ etc\n    a = re.sub(r\"([^a-zA-Z0-9\\\\ (){}\\]\\[])\\1{1,}\",' ',a)\n\n    # remove data inside {} -> at max 2 characters {q.}, {5.}\n    a = re.sub(r\"{.{0,2}}\", \" \", a)\n\n    # Insert spaces among spec chars, digits and nums  and then remove every single len alphabet and number\n    a = remove_single_word_num(insert_spaces(a))\n\n    # remove whatever comes after \\\\ double slashes except space \n    a = re.sub(r\"(\\\\[^ ]+)\",' ',a)\n\n    #remove every special characcter\n    a = re.sub(r'(\\W)|([_])',' ',a)\n\n    # remomve newline character\n    a = re.sub(\"\\\\n\", \" \", a)\n\n    # remove repeated space if there is any\n    a = re.sub(r\"\\s+\", \" \", a)\n  \n    return a\n\n\ndef remove_stopword(x):\n    stopwords_new = stopwords.words('english')\n    return [y for y in x if y not in stopwords_new]\n\ndef count_special_characters(df, col):\n    \n    pass\n\ndef common_tokens(data, col, top_most=50, title=None, return_temp=False, is_top=True):\n     \n    top = Counter([item for sublist in data[col] for item in sublist])\n    if not is_top:\n        temp = pd.DataFrame(top.most_common()[:-top_most:-1])\n    else:\n        temp = pd.DataFrame(top.most_common(top_most))\n    temp.columns = ['Common_words','count']\n    display(temp.style.background_gradient(cmap='Blues'))\n    \n    def plot_barchart(title=None):\n        fig = px.bar(temp, x=\"count\", y=\"Common_words\", title=title, orientation='h', \n                 width=700, height=700, color='Common_words')\n        fig.show()\n        pass\n    \n    # plot_barchart(title)\n    if return_temp:\n        return temp\n    \n    del temp, top\n    gc.collect()\n    \n    pass\n\ndef plot_wordcloud(text, mask=None, max_words=250, max_font_size=100, figure_size=(24.0,16.0), color = 'black',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=400, \n                    height=200)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    pass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start with reading the data."},{"metadata":{"id":"Tg0M3CakVWNs","outputId":"2eb22661-144a-43ae-f6d0-72430b462523","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/iitjee-neet-aims-students-questions-data/subjects-questions.csv\")\ndf = df[df[\"Subject\"] != \"English\"].reset_index(drop=True)\ndf","execution_count":null,"outputs":[]},{"metadata":{"id":"i_4kUhhYjke0"},"cell_type":"markdown","source":"# Check for Null values"},{"metadata":{"id":"Pw-TwPskaxhb","outputId":"982e9653-3891-4b6f-87dd-ded1816171c2","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.isna().sum().to_frame().rename(columns={0:\"NaN_Count\"}).style.background_gradient(cmap=\"Wistia\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, there are 3 nan values `eng` column which is supposed to be the question description itself. Hence let's remove those 3 samples and move ahead."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(subset=[\"eng\"], inplace=True)\ndf.reset_index(inplace=True, drop=True)\ndf.isna().sum().to_frame().rename(columns={0:\"NaN_Count\"}).style.background_gradient(cmap=\"Wistia\")","execution_count":null,"outputs":[]},{"metadata":{"id":"5sHPctNZYOke"},"cell_type":"markdown","source":"Our target is to classify whether the description belongs to one of the categories `Physics`, `Maths`, `Chemistry` or `Bio`. Let's visualise the distribution of our target."},{"metadata":{"id":"cY51_bzjjzfl"},"cell_type":"markdown","source":"# Number of samples per Subject"},{"metadata":{"id":"8uoWPmGBV5F2","outputId":"39f62269-11a7-4627-c1ec-168db2552dcd","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"target_Count = df['Subject'].value_counts().to_frame()\ntarget_Count.style.background_gradient(cmap=\"BrBG\")","execution_count":null,"outputs":[]},{"metadata":{"id":"-YEzUkWlZjtK"},"cell_type":"markdown","source":"As far as this much of data is concerned, this dataset is pretty imbalanced with very less samples in 2 classes i.e. `English` and `Bio`. and compratively more samples in another 3 classes, i.e `Physics`, `Chemistry` and `Math`. So, students face more difficulties in `Physics`, then `Chemistry` and then `Math` respectively, as they are seeking for help in these subjects. Let's have a pie chart for better visualization."},{"metadata":{"id":"qJQdAjDXYv0y","outputId":"5be24040-8cea-43c6-8d20-861850e6f505","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"target_Count.reset_index(inplace=True)\ntarget_Count.columns = [\"Subject\", \"pct\"]\ntarget_Count.loc[:, \"pct\"] /= len(df)\n\n\nplt.figure(figsize=(6, 10))\nwegdes, texts, autotexts = plt.pie(target_Count['pct'],\n                                  autopct=lambda pct: func(pct, target_Count['pct']),\n                                  explode=(0.05, 0.05, 0.05, 0.05),\n                                  labels=target_Count[\"Subject\"],\n                                  shadow=True,\n                                  startangle=45,\n                                  wedgeprops={\"linewidth\":1, \"edgecolor\":\"black\"},\n                                  textprops= dict(color=\"black\"))\nplt.legend(wegdes, target_Count[\"Subject\"],\n          title=\"Subjects\",\n          loc=\"center\",\n          bbox_to_anchor=(1, 0, 0, 0))\nplt.setp(autotexts, size=14, weight=\"bold\")\nplt.title(\"Subject Percentage distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# free up space\ndel target_Count\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"id":"fQXTbrV7dw3B"},"cell_type":"markdown","source":"The pie chart shows $96.5\\%$ doubts are from `PCM` only, with toughest among them being `Physics`. Now, let's analyse our text data. i.e `eng` column for better understading of our data."},{"metadata":{"id":"p_N-GpGcj54x"},"cell_type":"markdown","source":"# Length of the Questions/ Description"},{"metadata":{"id":"zKnLe1y2bkxI","outputId":"f5f34042-ddf6-445d-ca54-9645c1ae0c0e","trusted":true},"cell_type":"code","source":"# let's start with the length of the description.\ndf[\"length_eng\"] = df['eng'].apply(lambda x: len(x.split()))\nsns.distplot(df[\"length_eng\"], color=\"red\", bins=25)\nplt.title(\"Length of the description\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oukaie, we can say that the length data is left skewed. Before going more statistical, let's look for outliers."},{"metadata":{"id":"uUguLWoSgtrI"},"cell_type":"markdown","source":"Let's see if there lies any outliers by plotting the box plot."},{"metadata":{"id":"SnJlzRergtE4","outputId":"66ae0113-e458-47e9-ab5b-e38da0def32a","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(16, 10))\nsns.boxplot(x=\"Subject\", y=\"length_eng\", data=df, ax=ax1)\nsns.violinplot(x=\"Subject\", y=\"length_eng\", data=df, ax=ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"y7J2qAaHli55"},"cell_type":"markdown","source":"There are outliers, too much outliers in our length column. Well, the takeaways from these plots would be:\n\n- `Physics` questions are lengthier and `Biology` questions are shorter.\n- `Biology` seems to have lesser number of outliers which indicates, most of the biology question's length fall near their median value, only a few outlies. Variance is less.\n- `Physics` seems to have highest variance with lowest median.\n- `Chemistry` though has the longest description, yet the not so large variance indicates it has quite a significant number of samples roaming arond the median value. And same goes for `Math`, with a lower *longest Description* value."},{"metadata":{"id":"G50ukzjLj_YE"},"cell_type":"markdown","source":"# Statistical Analysis of Length"},{"metadata":{"trusted":true},"cell_type":"code","source":"pct_25 = lambda x: np.percentile(x, 25)\npct_75 = lambda x: np.percentile(x, 75)\npct_75.__name__ = \"75%\"\npct_25.__name__ = \"25%\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.pivot_table(\"length_eng\", \"Subject\", aggfunc=[\"count\", \"min\", pct_25, \"mean\", \"median\", pct_75, \"max\", \"std\", \"var\"]).style.background_gradient(cmap=\"plasma\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Note*: Above analysis are obtained provided no preprocessing is applied yet."},{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"color:red\">If you find this notebook interesting and well written, don't forget to upvote :)</h1>"},{"metadata":{},"cell_type":"markdown","source":"Well, here comes anoter crucial obeservation I think we should drop before movind ahead. Let me show."},{"metadata":{"id":"RI5ig2IxkhfY"},"cell_type":"markdown","source":"# Analysing most common words in the dataset and per subject"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for _ in range(5):\n    print(df.loc[np.random.randint(0, len(df)), \"eng\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, most of the questions contain options such as `(a)`, `(b)`, `(i)` etc. Ofcourse, we will find such options in every subject. As our aim to build a classifier to classify that a given question or description belongs to a particular Subject, keeping these tokens will only bring redudant similarity, so let's remove them and apply some basic preprocessing to move ahead with our exploration."},{"metadata":{},"cell_type":"markdown","source":"Let's prepare our preprocessing function and see the effects.   \n*NOTE*: The functions can be found under the Helper functions section :)"},{"metadata":{"id":"Iy-KhacoX53-","outputId":"986736ef-0a77-4507-d5ce-eaa47d0b6126","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"for _ in range(25):\n    print(\"\\n\")\n    i = np.random.randint(0, len(df))\n    x, y = df.loc[i, \"eng\"], df.loc[np.random.randint(0, len(df)), \"Subject\"]\n    print(x)\n    print()\n    print(y, \"\\t\", i, \"\\n\")\n    print(preprocess(x))","execution_count":null,"outputs":[]},{"metadata":{"id":"UtYors0pfmwt"},"cell_type":"markdown","source":"Seems like there are too many stop words, but we will see, because assuming stopwords based on `English` literature might not work in our case."},{"metadata":{},"cell_type":"markdown","source":"# Comparison with After and Before Processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"processed_eng\"] = df[\"eng\"].apply(lambda x: preprocess(x))\ndisplay(df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Without removing stopwords"},{"metadata":{},"cell_type":"markdown","source":"#### Most common words without any preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"token_list\"] = df[\"eng\"].apply(lambda x: x.split())\ncommon_tokens(df, col=\"token_list\", top_most=2000)  # experiment with stopwords(removing/not removing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the unexpected tokens as we haven't applied preprocessing yet. Let's apply our preprocessing function and look for the most common words."},{"metadata":{},"cell_type":"markdown","source":"#### Least common words without any preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"common_tokens(df, col=\"token_list\", top_most=2000, is_top=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most common words with Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"token_list_processed\"] = df[\"processed_eng\"].apply(lambda x: x.split())\nmost_2000 = common_tokens(df, col=\"token_list_processed\", top_most=2000, return_temp=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Least common words with Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"least_2000 = common_tokens(df, col=\"token_list_processed\", top_most=2000, is_top=False, return_temp=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### With Stopwords removal"},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"token_list\"] = df[\"token_list\"].apply(lambda x: remove_stopword(x))\ndf[\"token_list_processed\"] = df[\"token_list_processed\"].apply(lambda x: remove_stopword(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most common words without any preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"common_tokens(df, col=\"token_list\", top_most=2000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Least common words without any preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"common_tokens(df, col=\"token_list\", top_most=2000, is_top=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most common words with preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"most_2000_sr = common_tokens(df, col=\"token_list_processed\", top_most=2000, return_temp=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Least Common words with preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"least_2000_sr = common_tokens(df, col=\"token_list_processed\", top_most=2000, return_temp=True, is_top=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well, it seems pretty clearer now."},{"metadata":{},"cell_type":"markdown","source":"# Category wise Data"},{"metadata":{"id":"Cg-x4WrLgJ-J","trusted":true},"cell_type":"code","source":"Phy = df[df['Subject']=='Physics']\nMath = df[df['Subject']=='Maths']\nChem = df[df['Subject']=='Chemistry']\nBio = df[df['Subject']=='Biology']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Phy.name = \"Phy\"\nChem.name = \"Chem\"\nBio.name = \"Bio\"\nMath.name = \"Math\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most Common Words in Physics"},{"metadata":{"id":"DsrJ0MYNgblL","outputId":"2c259e19-2587-4347-d377-e63a3907e323","trusted":true},"cell_type":"code","source":"top_phy = common_tokens(Phy, col=\"token_list_processed\", top_most=2000, return_temp=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Least common words in Physics"},{"metadata":{"trusted":true},"cell_type":"code","source":"least_Phy = common_tokens(Phy, col=\"token_list_processed\", top_most=2000, return_temp=True, is_top=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most common words in Chemistry"},{"metadata":{"id":"MmEuohNBhZVz","outputId":"bfa8350c-030b-48b8-c41e-f13ea6b36075","trusted":true},"cell_type":"code","source":"top_chem = common_tokens(Chem, col=\"token_list_processed\", top_most=2000, return_temp=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Least common words in Chemistry"},{"metadata":{"trusted":true},"cell_type":"code","source":"least_Chem = common_tokens(Chem, col=\"token_list_processed\", top_most=2000, return_temp=True, is_top=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most common words in Biology"},{"metadata":{"id":"uhHlgbZqhnZn","outputId":"f313aafd-0097-4e5a-9185-40ed53af6ebe","trusted":true},"cell_type":"code","source":"top_bio = common_tokens(Bio, col=\"token_list_processed\", top_most=2000, return_temp=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Least common words in Biology"},{"metadata":{"trusted":true},"cell_type":"code","source":"least_Bio = common_tokens(Bio, col=\"token_list_processed\", top_most=2000, return_temp=True, is_top=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Most common words in Maths"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_math = common_tokens(Math, col=\"token_list_processed\", top_most=2000, return_temp=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Least common words in Maths"},{"metadata":{"trusted":true},"cell_type":"code","source":"least_math = common_tokens(Math, col=\"token_list_processed\", top_most=2000, return_temp=True, is_top=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What can be our takeaways from here?\n\n- After stop words removal, still we can notice some irrelevant data being appeared as most common words.\n- In most of the cases, these are (b), (a_number), Q., etc. These can be assumed as the option number of the questions. (The same has been discussed with Vedant, and the suggestions on removing such instances are implemented.)\n\n*NOTE: Theses updates have been made in this version. I have removed all such ocurrences those might hamper the model peformance and cause unnecessary similarity between target classes.*"},{"metadata":{},"cell_type":"markdown","source":"# Most common words across classes"},{"metadata":{},"cell_type":"markdown","source":"Let's check out what will happen if we analyse inter-category questions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have dataframes for each subject as well as on the whole set\ncommon_words = pd.concat([most_2000.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"most_common\"}),\n           most_2000_sr.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"most_common_sr\"}),\n           least_2000.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"least_common\"}),\n           least_2000_sr.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"least_common_sr\"}),\n           top_phy.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"most_phy\"}),\n           least_Phy.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"least_phy\"}),\n           top_chem.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"most_chem\"}),\n           least_Chem.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"least_chem\"}),\n           top_bio.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"most_bio\"}),\n           least_Bio.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"least_bio\"}),\n           top_math.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"most_math\"}),\n           least_math.drop(\"count\", axis=1).rename(columns={\"Common_words\": \"least_math\"})], axis=1)\ndisplay(common_words.head())\ncommon_words.to_csv(\"common-words-analysis.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysing Special characters"},{"metadata":{"trusted":true},"cell_type":"code","source":"special_dict = Counter([item for sublist in df[\"eng\"] for item in sublist if not item.isalnum()])\nspecial_dict = pd.DataFrame(special_dict.most_common())\nspecial_dict_pp = Counter([item for sublist in df[\"processed_eng\"] for item in sublist if not item.isalnum()])\nspecial_dict_pp = pd.DataFrame(special_dict_pp.most_common())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"special_dict.columns = [\"Special_tokens\", \"count\"]\nspecial_dict_pp.columns = [\"Special_tokens\", \"count\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for ddf in [Phy, Chem, Bio, Math]:\n    \n    special_ = Counter([item for sublist in ddf[\"eng\"] for item in sublist if not item.isalnum()])\n    special_ = pd.DataFrame(special_.most_common())\n    special_.columns = [\"Special_tokens\", ddf.name]\n\n    special_dict = pd.merge(special_dict, special_, how=\"left\", on=\"Special_tokens\")\n    special_dict[ddf.name] = special_dict[ddf.name].fillna(0).astype(\"int64\")\n    \nspecial_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(special_dict.style.background_gradient(cmap=\"twilight_shifted\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"special_dict.to_csv(\"special-characters-analysis.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"_qaZWqyAkphx"},"cell_type":"markdown","source":"# Time for Wordcloud"},{"metadata":{"id":"td9zpDgmiJKj","outputId":"1db1e155-f9fb-4bce-a767-32ea81d6ac01","trusted":true},"cell_type":"code","source":"plot_wordcloud(Phy.processed_eng, color='black', max_font_size=100, title_size=30, title=\"WordCloud of Physics\")","execution_count":null,"outputs":[]},{"metadata":{"id":"5vduV9-HiynZ","outputId":"9062d76e-b59e-4e25-b8dd-b4efb499adc0","trusted":true},"cell_type":"code","source":"plot_wordcloud(Chem.processed_eng, color='black', max_font_size=100, title_size=30, title=\"WordCloud of Chemistry\")","execution_count":null,"outputs":[]},{"metadata":{"id":"JMkggKa9jA7R","outputId":"b06a6525-2418-463e-f824-0c4654e80ab1","trusted":true},"cell_type":"code","source":"plot_wordcloud(Math.processed_eng, color='black', max_font_size=100, title_size=30, title=\"WordCloud of Maths\")","execution_count":null,"outputs":[]},{"metadata":{"id":"XoG-altSjGks","outputId":"e48dd1b4-9efc-45aa-89b7-22df9cf9a1d0","trusted":true},"cell_type":"code","source":"plot_wordcloud(Bio.processed_eng, color='black', max_font_size=100, title_size=30, title=\"WordCloud of Biology\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}