{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-27T01:47:46.211436Z","iopub.execute_input":"2021-07-27T01:47:46.211856Z","iopub.status.idle":"2021-07-27T01:47:46.233566Z","shell.execute_reply.started":"2021-07-27T01:47:46.211775Z","shell.execute_reply":"2021-07-27T01:47:46.232451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Credit Card customers**\n\n**Predict Churning customers**","metadata":{}},{"cell_type":"markdown","source":"Problem :\n\nA manager at the bank is disturbed with more and more customers leaving their credit card services. They would really appreciate if one could predict for them who is gonna get churned so they can proactively go to the customer to provide them better services and turn customers decisions in the opposite direction. We have only 16.07% of customers who have churned. Thus, it's a bit difficult to train our model to predict churning customers.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv('/kaggle/input/credit-card-customers/BankChurners.csv')\ndf_data = pd.DataFrame(data)\ndf_data","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:10:42.987012Z","iopub.execute_input":"2021-07-27T07:10:42.987374Z","iopub.status.idle":"2021-07-27T07:10:43.101643Z","shell.execute_reply.started":"2021-07-27T07:10:42.987345Z","shell.execute_reply":"2021-07-27T07:10:43.100682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Understanding the Data","metadata":{}},{"cell_type":"markdown","source":"> Based on the note from the data source, I decided to ignore the last 2 columns of the data, so the first thing I did was I had to delete it so it wouldn't interfere when analyzing the overall data","metadata":{}},{"cell_type":"code","source":"#Deleting the last 2 columns\ndf_data1 = df_data.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1'])\ndf_data1.drop('Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2', inplace=True, axis=1)\ndf_data1.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:10:48.368199Z","iopub.execute_input":"2021-07-27T07:10:48.368586Z","iopub.status.idle":"2021-07-27T07:10:48.408093Z","shell.execute_reply.started":"2021-07-27T07:10:48.368549Z","shell.execute_reply":"2021-07-27T07:10:48.406963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data1.dtypes","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:10:51.917497Z","iopub.execute_input":"2021-07-27T07:10:51.917879Z","iopub.status.idle":"2021-07-27T07:10:51.926243Z","shell.execute_reply.started":"2021-07-27T07:10:51.917844Z","shell.execute_reply":"2021-07-27T07:10:51.925304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Grouping columns by data type","metadata":{}},{"cell_type":"code","source":"numerical_columns = [\"Customer_Age\", \n                     \"Months_on_book\", \n                     \"Credit_Limit\", \n                     \"Total_Revolving_Bal\", \n                     \"Avg_Open_To_Buy\", \n                     \"Total_Amt_Chng_Q4_Q1\", \n                     \"Total_Trans_Amt\", \n                     \"Total_Trans_Ct\", \n                     \"Total_Ct_Chng_Q4_Q1\", \n                     \"Avg_Utilization_Ratio\"]\n\ncategorical_columns = ['Attrition_Flag',\n                       'Gender',\n                       'Education_Level',\n                       'Marital_Status',\n                       'Income_Category',\n                       'Card_Category']\n\ndiscrete_columns = ['CLIENTNUM',\n                    'Dependent_count',\n                    'Total_Relationship_Count',\n                    'Months_Inactive_12_mon',\n                    'Contacts_Count_12_mon']","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:10:56.600534Z","iopub.execute_input":"2021-07-27T07:10:56.600929Z","iopub.status.idle":"2021-07-27T07:10:56.607723Z","shell.execute_reply.started":"2021-07-27T07:10:56.600893Z","shell.execute_reply":"2021-07-27T07:10:56.606485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Identify the Data**","metadata":{}},{"cell_type":"code","source":"df_data1.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:11:01.376444Z","iopub.execute_input":"2021-07-27T07:11:01.376836Z","iopub.status.idle":"2021-07-27T07:11:01.401341Z","shell.execute_reply.started":"2021-07-27T07:11:01.376801Z","shell.execute_reply":"2021-07-27T07:11:01.400118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data1.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:11:04.748516Z","iopub.execute_input":"2021-07-27T07:11:04.748913Z","iopub.status.idle":"2021-07-27T07:11:04.77097Z","shell.execute_reply.started":"2021-07-27T07:11:04.748879Z","shell.execute_reply":"2021-07-27T07:11:04.769725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because there are no null values and duplicated values found, it can be said that this data is clean, and can be continued for data analysis by looking at the distribution of the data.","metadata":{}},{"cell_type":"code","source":"# Numerical Value \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport matplotlib.pyplot as plt\ndef histo(data,x):\n    sns.histplot(data=df_data1,x=x)\n    \nfor col in numerical_columns:\n    histo(df_data1, x=col)\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:13:10.608998Z","iopub.execute_input":"2021-07-27T07:13:10.609377Z","iopub.status.idle":"2021-07-27T07:13:13.60749Z","shell.execute_reply.started":"2021-07-27T07:13:10.609345Z","shell.execute_reply":"2021-07-27T07:13:13.606769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Discrete Value \nfor col in discrete_columns:\n    histo(data, x=col)\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:13:22.244369Z","iopub.execute_input":"2021-07-27T07:13:22.244748Z","iopub.status.idle":"2021-07-27T07:13:23.404712Z","shell.execute_reply.started":"2021-07-27T07:13:22.244716Z","shell.execute_reply":"2021-07-27T07:13:23.403671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categorical Value \ndef prop(categorical_columns):\n    proportion = pd.DataFrame(data[categorical_columns].value_counts())\n    proportion['proportion(%)'] = data[categorical_columns].value_counts(normalize=True)\n    proportion['proportion(%)'].plot(kind='bar')\n\nfor colu in categorical_columns:\n    print('Percentage of',colu)\n    prop(colu)\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:13:28.998504Z","iopub.execute_input":"2021-07-27T07:13:28.998902Z","iopub.status.idle":"2021-07-27T07:13:29.88211Z","shell.execute_reply.started":"2021-07-27T07:13:28.998869Z","shell.execute_reply":"2021-07-27T07:13:29.88088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Analyze the Data**","metadata":{}},{"cell_type":"markdown","source":"> **Analyze the data better focus on attrition_Flag column to dig more information about churning customers**\n\n> The first analysis is to look for outliers values, because these values must be determined to see the pattern of distribution so that we can study the data better.\n\n> To find outliers, it is necessary to pay attention to using a boxplot chart","metadata":{}},{"cell_type":"code","source":"def boxplot(data,x,y):\n    sns.boxplot(data=data, y=y, x=x)\n\nfor num in numerical_columns:\n    boxplot(df_data1,num,'Attrition_Flag')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:16:50.463597Z","iopub.execute_input":"2021-07-27T07:16:50.463968Z","iopub.status.idle":"2021-07-27T07:16:52.153391Z","shell.execute_reply.started":"2021-07-27T07:16:50.463938Z","shell.execute_reply":"2021-07-27T07:16:52.152351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> To see it in another way, you can use a line plot like the one below","metadata":{}},{"cell_type":"code","source":"# Measures of Dispersion\nexisting_customer_df = df_data1[df_data1['Attrition_Flag'] == \"Existing Customer\"]\nattried_customer_df = df_data1[df_data1['Attrition_Flag'] == \"Attrited Customer\"]\n\ndef plot(column, dataset1, dataset2, label1, label2):\n    sns.kdeplot(dataset1[column], color='blue', label=label1).set(xlim=(0, existing_customer_df[column].max()))\n    sns.kdeplot(dataset2[column], color='red', label=label2).set(xlim=(0, attried_customer_df[column].max()))\n    plt.legend()\n    plt.show()\n\nfor col in numerical_columns:\n    mean_existing = existing_customer_df[col].mean()\n    mean_attried = attried_customer_df[col].mean()\n    print(\"The average\",col, \"of Existing Customer: {:.2f}\".format(round(mean_existing,2)))\n    print(\"The average\",col, \"of Attrited Customer: {:.2f}\".format(round(mean_attried,2)))\n    plot(col, existing_customer_df,attried_customer_df, \"Existing Customer\",'Attrition Customer')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:25:57.84785Z","iopub.execute_input":"2021-07-27T07:25:57.848217Z","iopub.status.idle":"2021-07-27T07:26:00.449437Z","shell.execute_reply.started":"2021-07-27T07:25:57.848188Z","shell.execute_reply":"2021-07-27T07:26:00.448584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Then the analysis continues by seeing how the level of correlation between all columns","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,11))\nsns.heatmap(df_data1.corr(), annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:21:42.185891Z","iopub.execute_input":"2021-07-27T07:21:42.186255Z","iopub.status.idle":"2021-07-27T07:21:43.883359Z","shell.execute_reply.started":"2021-07-27T07:21:42.186225Z","shell.execute_reply":"2021-07-27T07:21:43.882557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hipothesis** : Seeing from the correlation value between columns a and b, which is 1, we are curious.\ndoes the data in the two columns have the same value? or just do have similarities? For that we use the conjecture that the average of credit limit and avg of avg_open_to_buy are the same\n\n**Parameters** :\n\nx1 (array_like, 1-D or 2-D) = first of the two independent samples\n\nx2 (array_like, 1-D or 2-D) = second of the two independent samples\n\nvalue (float) = \n*   In the one sample case, value is the mean of x1 under the Null hypothesis. \n*   In the two sample case, value is the difference between mean of x1 and mean of x2 under the Null hypothesis.\n","metadata":{}},{"cell_type":"code","source":"# import library\nimport pandas as pd\nimport numpy as np\n\nimport scipy.stats as stats\nfrom statsmodels.stats.weightstats import ztest\nimport matplotlib.pyplot as plt\nimport math","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:26:13.821003Z","iopub.execute_input":"2021-07-27T07:26:13.821355Z","iopub.status.idle":"2021-07-27T07:26:13.826147Z","shell.execute_reply.started":"2021-07-27T07:26:13.821324Z","shell.execute_reply":"2021-07-27T07:26:13.825367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Credit Limit\ncredit_limit_mean = data['Credit_Limit'].mean()\nsample_data_credit_limit = data['Credit_Limit'].sample(n=50)\n\n# Avg Open To Buy\navg_open_to_buy_mean = data['Avg_Open_To_Buy'].mean()\nsample_data_avg_open_to_buy = data['Avg_Open_To_Buy'].sample(n=50)\n\n# Difference between mean of x1 and mean of x2  \ndifference = credit_limit_mean - avg_open_to_buy_mean\nprint('Difference between population mean: {:.2f}'.format(difference))\n\n# set the value\nconfidence_level = 0.95\nalpha = 1-confidence_level\n\n# ztest hypothesis\nztest_Score, pvalue = ztest(x1=sample_data_credit_limit, x2=sample_data_avg_open_to_buy, value=difference)\ndisplay(ztest(x1=sample_data_credit_limit, x2=sample_data_avg_open_to_buy, value=difference))\n\n# test\nif alpha < pvalue:\n  print(\"Reject the hypothesis\")\n  print(\"Because the pvalue {:.2f} > {:.2f}\".format(pvalue, alpha))\nelse:\n  print(\"Accept the hypothesis\")\n  print(\"Because the pvalue {:.2f} < {:.2f}\".format(pvalue, alpha))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T07:26:28.195642Z","iopub.execute_input":"2021-07-27T07:26:28.195998Z","iopub.status.idle":"2021-07-27T07:26:28.21109Z","shell.execute_reply.started":"2021-07-27T07:26:28.195969Z","shell.execute_reply":"2021-07-27T07:26:28.210025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Data Pre-Processing**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Develop Model**","metadata":{}}]}