{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Lead Scoring Case Study","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Problem Statement\n\nX Education offers online courses to industry professionals. On a daily basis,professionals who are interested in the courses land on their website and browse for courses. Though X Education acquires a lot of lead, their conversion rate(lead to purchase) is poor. The company want to idenitfy potential leads so the sales team focus on communicating to the potential clients.\n\nThe company want to develop a model to improve the identification of hot leads","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Objective\n\nDevelop a regression model for identification of lead with a sensitivity of ~80%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## The analysis Structure is as follows:\n- Read the data\n- Inspect and Clean the data\n- Prepare the data for modelling\n- Model development and model optimization\n- Identifying the optimal probability cutoff\n- Checking model on test data\n- Final Model and generation of lead Score ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Importing Libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\npd.set_option('display.max_rows', 999)\npd.set_option('display.max_columns', 999)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\nimport statsmodels.api as sm\n\nfrom sklearn import metrics\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Importing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing the dataset\nlocation = '/kaggle/input//leadscore/Leads.csv'\nlead_data = pd.read_csv(location)\n\n# Inspecting the original dataFrame.\nlead_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Cleaning and Data Inspection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Understanding the provided dataset","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Number of rows and columns\nlead_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understanding the data types and missing values in the features\nlead_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Understanding the statisctics of the numerical columns\nlead_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Data Cleaning\n<ol>\n<li> Handling the 'select' values in the categorical columns.\n<li> Identifying the percentage of missing value in the columns.\n<li> Dropping the columns with high missing values.\n<li> Handling skewed data distribution.\n<li> Treating the varing unique categories. \n<li> Dropping rows and columns with high missing value\n</ol>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.1 Handling the 'select' value in the categorical columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identifying number of select values across each column\n# lead_data[lead_data[list(lead_data.columns)]=='Select'].notnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# checking missing value across each column\nlead_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing 'select' with null value\nlead_data.replace('Select', np.nan, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if 'select' value are replaced or not\n# lead_data[lead_data[list(lead_data.columns)]=='Select'].notnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# checking if the 'select' value converted is traslated as null\nlead_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identifying number of select values across each categorical variable\n# lead_data[lead_data[list(lead_data.columns)]=='Select'].notnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.2 Identifying the percentage of missing value in the columns","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# calculating the missing percentage of the missing values \nround(100.0 * lead_data.isnull().sum()/len(lead_data), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### There are a few columns with a lot of missing values. Hence, we will drop columns with more than 40% missing values\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.3 Dropping the columns with high missing values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dropping the columns with more than 40% missing value\n- How did you hear about X Education\n- Lead Quality  \n- Lead Profile\n- Asymmetrique Activity Index\n- Asymmetrique Profile Index\n- Asymmetrique Activity Score   \n- Asymmetrique Profile Score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the column\nlead_df = lead_data.drop(columns = ['How did you hear about X Education','Lead Quality','Lead Profile','Asymmetrique Activity Index','Asymmetrique Profile Index','Asymmetrique Activity Score','Asymmetrique Profile Score'], axis=1)\nlead_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Original lead dataframe\nlead_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cleaned/ Updated Dataframe\nlead_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.4 Creating list of categorical and numerical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a list of numerical feature\nnumerical_columns = ['Lead Number', 'Converted','TotalVisits', 'Total Time Spent on Website','Page Views Per Visit']\nnumerical_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a list of categorical columns\ncategorical_columns = []\nfor i in list(lead_df.columns):\n    if i not in numerical_columns:\n        categorical_columns.append(i)\ncategorical_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.5 Checking the categorical columns having skewed distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in categorical_columns[1:]:\n    plt.figure(figsize=(20,8))\n    ax = (pd.Series(lead_df[i]).value_counts(normalize=True, sort=False)*100).plot.bar()\n    ax.set(ylabel=\"Percent\")\n    plt.title(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Based on the above bar chart, we can see the following categorical columns are heavily skewed (~100%) towards one category:\n- Do Not Call\n- What matters most to you in choosing a course\n- Search\n- Magazine\n- Newspaper Article\n- X Education Forums\n- Newspaper\n- Digital Advertisement\n- Through Recommendations\n- Receive More Updates About Our Courses\n- Update me on Supply Chain Content\n- Get updates on DM Content\n- I agree to pay the amount through cheque","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"skewed_columns= [\n    'Do Not Call',\n    'What matters most to you in choosing a course',\n    'Search',\n    'Magazine',\n    'Newspaper Article',\n    'X Education Forums',\n    'Newspaper',\n    'Digital Advertisement',\n    'Through Recommendations',\n    'Receive More Updates About Our Courses',\n    'Update me on Supply Chain Content',\n    'Get updates on DM Content',\n    'I agree to pay the amount through cheque'\n]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"lead_df = lead_df.drop(columns= skewed_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# After dropping the skewed data.\nlead_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.6 Checking the unique categories in the categorical columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = []\nfor i in list(lead_df.columns):\n    if i not in numerical_columns:\n        categorical_columns.append(i)\ncategorical_columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding number of unique categories in each columns \nlead_df[categorical_columns].nunique(dropna=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a few columns with high number of categories. For all such columns, we will check percentage records in each category and may combine low contibution categories into one","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.6.1 Checking the unique categories in the Lead Origin column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Lead Origin'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['Lead Origin'].replace(['Lead Import','Quick Add Form'], 'others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Lead Origin'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.6.2 Checking the unique categories in the Lead Source column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Lead Source'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_source = pd.DataFrame(lead_df['Lead Source'].value_counts(normalize = True)).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['Lead Source'].replace(list(lead_source[lead_source['Lead Source']<0.05]['index']), 'others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Lead Source'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.6.3 Checking the unique categories in the Last Activity column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Last Activity'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_activity = pd.DataFrame(lead_df['Last Activity'].value_counts(normalize = True)).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['Last Activity'].replace(list(last_activity[last_activity['Last Activity']<0.05]['index']), 'others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Last Activity'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.6.4 Checking the unique categories in the Country column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Country'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"country = pd.DataFrame(lead_df['Country'].value_counts(normalize = True)).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['Country'].replace(list(country[country['Country']<0.01]['index']), 'others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"round(lead_df['Country'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.6.5 Checking the unique categories in the Specialization column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Specialization'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"specialization = pd.DataFrame(lead_df['Specialization'].value_counts(normalize = True)).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['Specialization'].replace(list(specialization[specialization['Specialization']<0.05]['index']), 'others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Specialization'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.6.6 Checking the unique categories in the What is your current occupation column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['What is your current occupation'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"occupation = pd.DataFrame(lead_df['What is your current occupation'].value_counts(normalize = True)).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['What is your current occupation'].replace(list(occupation[occupation['What is your current occupation']<0.01]['index']), 'others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['What is your current occupation'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.6.7 Checking the unique categories in the Tags column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Tags'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags = pd.DataFrame(lead_df['Tags'].value_counts(normalize = True)).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['Tags'].replace(list(tags[tags['Tags']<0.03]['index']), 'others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Tags'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.6.8 Checking the unique categories in the City column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['City'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"city = pd.DataFrame(lead_df['City'].value_counts(normalize = True)).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['City'].replace(list(city[city['City']<0.01]['index']), 'others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['City'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.6.9 Checking the unique categories in the Last Notable Activity  column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Last Notable Activity'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"last_activity = pd.DataFrame(lead_df['Last Notable Activity'].value_counts(normalize = True)).reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['Last Notable Activity'].replace(list(last_activity[last_activity['Last Notable Activity']<0.05]['index']), 'others', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(lead_df['Last Notable Activity'].value_counts(normalize = True)*100.0, 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the updated number of unique categories in each columns \nlead_df[categorical_columns[1:]].nunique(dropna=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.7 Handling the columns with lower missing value\nIn this step, we will adopt two techniques:\n1) Drop the rows with a high number of missing values(>5)\n2) Treat the missing values of individual column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for the no. of null values per column\nround(100.0 * lead_df.isnull().sum()/len(lead_df), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 3.2.7.1 Dropping rows with high missing value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['missing_row_count'] = lead_df.isnull().sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['missing_row_count'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"The total no. of rows with high missing values\",len(lead_df) - len(lead_df[lead_df['missing_row_count']<5]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df = lead_df[lead_df['missing_row_count']<5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Updated dataset\nlead_df.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"round(100.0 * lead_df.isnull().sum()/len(lead_df), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 3.2.7.2 Treating columns with high missing value","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"###### 3.2.7.2.1 Treating Lead Source","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['Lead Source'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The value for lead source feature is  spread across multiple categories. Hence, imputation is difficult. Further number of rows with missing value is small. We will just drop the missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df = lead_df[~pd.isnull(lead_df['Lead Source'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(100.0 * lead_df.isnull().sum()/len(lead_df), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### 3.2.7.2.2 Treating TotalVisits","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"lead_df['TotalVisits'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y =lead_df['TotalVisits'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The value of TotalVisits feature is broadly distributed. Hence, imputation is difficult. Further number of rows with missing value is small. We will just drop the missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df = lead_df[~pd.isnull(lead_df['TotalVisits'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"round(100.0 * lead_df.isnull().sum()/len(lead_df), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### 3.2.7.2.3 Treating Country column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['Country'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As majority of the value in Country Category is India, we will replace null with India","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['Country'].fillna('India', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(100.0 * lead_df.isnull().sum()/len(lead_df), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### 3.2.7.2.4 Treating Specialization column","execution_count":null},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"lead_df['Specialization'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no clear mode in for this feature. Hence, imputation by central tendency value will not be correct. Further, it may be an important prediction feature. Hence, we will replace it by missing value and will later check if it helps in prediction ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['Specialization'].fillna('Missing Value', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### 3.2.7.2.5 Treating What is your current occupation column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['What is your current occupation'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As majority of the value in What is your current occupation is Unemployed, we will replace null with Unemployed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['What is your current occupation'].fillna('Unemployed', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### 3.2.7.2.6 Treating Tags column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df['Tags'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no clear mode in this column. Further, this feature is collected when a call is made to user. However, this feature is not required for our prediction. Hence, we will drop it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.drop(columns=['Tags'], inplace= True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### 3.2.7.2.7 Treating City column","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"lead_df['City'].value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no clear mode in for this feature. Hence, imputation by central tendency value will not be correct. \nFurther, adding Mumbai and null values are equal to 6752 and rest has 12 % values which are not giving any information. Hence, we will drop this columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.drop(columns='City', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"round(100.0 * lead_df.isnull().sum()/len(lead_df), 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.drop(columns='missing_row_count', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 EDA for numerical columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 3.3.1 Univariate Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in numerical_columns[1:]:\n    sns.boxplot(y=lead_df[i])\n    plt.title(i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As boxplots show the presence of outliers in TotalVisits and Page Views Per Visit, we have to do outlier treatment. We will use winsorizing/capping for outlier treatment","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 3.3.2 Outlier treatment for TotalVisits","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"q4 = lead_df['TotalVisits'].quantile(0.95)\nq4\nlead_df['TotalVisits'][lead_df['TotalVisits']>=q4] = q4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y=lead_df['TotalVisits'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.3.2 Outlier treatment for Page Views Per Visit","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"q4 = lead_df['Page Views Per Visit'].quantile(0.95)\nq4\nlead_df['Page Views Per Visit'][lead_df['Page Views Per Visit']>=q4] = q4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y=lead_df['Page Views Per Visit'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.3.3 Bi-variate Analysis","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Heatmap for understanding correlation\nplt.figure (figsize= (25,20))\nsns.heatmap (lead_df[numerical_columns[1:]].corr(), annot =True, cmap ='YlGnBu')\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\nplt.yticks(rotation = 0)\nplt.show()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Total visits and Page Views Per Visit have high correlation and we might need to choose one. However, we will assess this later in the model develop phase and will rely on VIF","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Pairplot for relation between the features\nsns.pairplot(lead_df[numerical_columns[1:]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4 Understanding Data lost in data cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round(100.0 * len(lead_df)/len(lead_data), 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Various data cleaning steps led to the removal of 11% of the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4. Data Preparation for Modelling","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Converting Binary categorical variable to Numerical Variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are two columns which need to be converted\n- Do Not Email\n- A free copy of Mastering The Interview","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"varlist =  ['Do Not Email', 'A free copy of Mastering The Interview']\nlead_df[varlist]=lead_df[varlist].apply(lambda x :x.map({'Yes': 1, \"No\": 0}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Creating Dummy Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = [\n    'Lead Origin',\n    'Lead Source',\n    'Last Activity',\n    'Country',\n    'What is your current occupation',\n    'A free copy of Mastering The Interview',\n    'Last Notable Activity'\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_model_df = lead_df.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy_var = pd.get_dummies(lead_model_df[categorical_features], drop_first=True)\n\n# Adding the results to the master dataframe\nlead_model_df = pd.concat([lead_model_df, dummy_var], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_model_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_model_df['Specialization'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy1 = pd.get_dummies(lead_model_df['Specialization'])\ndummy_var_1 = dummy1.drop(['Missing Value'], 1)\n# Adding the results to the master dataframe\nlead_model_df = pd.concat([lead_model_df,dummy_var_1], axis=1)\nlead_model_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_model_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the repeated categorical columns \nlead_model_df = lead_model_df.drop(columns = [ 'Lead Origin','Lead Source','Last Activity','Country','What is your current occupation','A free copy of Mastering The Interview','Last Notable Activity','Specialization'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_model_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_model_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Train Test Split ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting feature variable to X\nX = lead_model_df.drop(['Prospect ID','Lead Number','Converted'], axis=1)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting response variable to y\ny = lead_model_df['Converted']\n\ny.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Feature Scaling\n- Using MinMax scaling (Normalisation) - Compressing the data between 0-1.\n\n     #### Normalisation: (x- xmin/ xmax- xmin)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = MinMaxScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Checking the Conversion Rate\nconversion = (sum(lead_df['Converted'])/len(lead_df['Converted'].index))*100\nconversion","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is some data imbalance but it is not very high","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5. Model Development","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Understanding correlation among features","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Validating Multi Colinearity\nplt.figure(figsize=(30,10))\nsns.heatmap(X_train.corr(),annot = True, cmap=\"RdYlGn\",linewidth =1)\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\nplt.yticks(rotation = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There is some correlation between features and hence, we might need to drop a few feature using VIF","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Variable Selection Using RFE","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression()\nrfe = RFE(logreg, 15)           \nrfe = rfe.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe.support_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identifying features suggested by RFE\ncol = X_train.columns[rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# features outside top 15 as per RFE\nX_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Model Building ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 5.3.1 Model-1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm1 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm1.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Creating a dataframe with the actual churn flag and the predicted probabilities","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'converted':y_train.values, 'converted_prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0. We will later optimize the optimal probability cut-off using ROC curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"    y_train_pred_final['predicted'] = y_train_pred_final['converted_prob'].map(lambda x: 1 if x > 0.5 else 0)\n\n    # Let's see the head\n    y_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final['converted'], y_train_pred_final['predicted'])\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(\"The overall accuracy of the model1 is\", round(100.0 * metrics.accuracy_score(y_train_pred_final['converted'], y_train_pred_final['predicted']), 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking VIFs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As some of the feature have VIF >5, we will drop the feature and then create a new model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 5.3.2 Model-2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col = col.drop('Page Views Per Visit', 1)\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'converted':y_train.values, 'converted_prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['predicted'] = y_train_pred_final['converted_prob'].map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final['converted'], y_train_pred_final['predicted'] )\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(\"The overall accuracy of the model2 is\",round(100.0 * metrics.accuracy_score(y_train_pred_final['converted'], y_train_pred_final['predicted']), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As the Marketing Management feature is giving p-value >0.05, we will drop the feature and create a next model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 5.3.3 Model-3","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col = col.drop('Marketing Management', 1)\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final = pd.DataFrame({'converted':y_train.values, 'converted_prob':y_train_pred})\ny_train_pred_final['LeadID'] = y_train.index\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['predicted'] = y_train_pred_final['converted_prob'].map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['predicted']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final['converted'], y_train_pred_final['predicted'] )\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nprint(\"The overall accuracy of the model3 is\",round(100.0 * metrics.accuracy_score(y_train_pred_final['converted'], y_train_pred_final['predicted']), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model-3/ Final Model seems to be the best model as all coeffecients have stat-sig value and none of the features have VIF >5. Further, there is no major change in accuracy as we move from model-1 to model-3. The number of features used is 12","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 6. Confusion Matrix","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negative predictive value\nprint (TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The confusion matrix shows that the chosen cut off probability value is not optimal as it is not giving the desired specificity. We will use ROC curves to find optimal cut off value","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 7. ROC curve\n- ROC Curves shows the tradeoff between the True Positive Rate (TPR) and the False Positive Rate (FPR).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final['converted'], y_train_pred_final['converted_prob'], drop_intermediate = False )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_train_pred_final['converted'], y_train_pred_final['converted_prob'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"_**As the ROC curve is more towards the upper-left corner of the graph, the proposed model(model3) could be considered a good model**_","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating columns with different probability cutoffs \nnumbers = [float(x)/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final['converted_prob'].map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final['converted'], y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])/total1\n    \n    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Optimal Probability Cutoff","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From the curve above, 0.35 is the optimum point to take it as a cutoff probability.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred_final['final_predicted'] = y_train_pred_final['converted_prob'].map( lambda x: 1 if x > 0.35 else 0)\n\ny_train_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final['converted'], y_train_pred_final['final_predicted'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_train_pred_final['converted'], y_train_pred_final['final_predicted'])\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP,TN,FP,FN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive predictive value \nprint (TP / float(TP+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negative predictive value\nprint (TN / float(TN + FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### At cut off proabibility of 0.35, we get the desired sensitivity with acceptable accuracy and specificity. We will choose 0.35 as the cut off probability and will check model performance on test data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 9: Making predictions on the test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test[col]\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_sm = sm.add_constant(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = res.predict(X_test_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_pred_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)\ny_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting LeadID to index\ny_test_df['LeadID'] = y_test_df.index\ny_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'converted_prob'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the head of y_pred_final\ny_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final['final_predicted'] = y_pred_final['converted_prob'].map(lambda x: 1 if x > 0.35 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9.1 Confusion matrix on test data","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our logistic regression model\nTP / float(TP+FN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\nTN / float(TN+FP)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The sensitivity on the test data is within 5% range of the train data. Hence, we can finalize the model3 as the final model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 10. Final model and score variable calculation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Based on sensitivity, specificity, and accuracy, we can conclde Model 3 with cut off probability of 0.35 as the recommended model for lead idenitification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 10.1 score variable calculation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_score_variable = y_train_pred_final[['LeadID','converted','final_predicted','converted_prob']] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_score_variable['converted_prob'] = round(y_train_score_variable['converted_prob']*100.0, 2)\ny_train_score_variable.rename(columns = {'converted_prob':'Lead Score'}).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_score_variable = y_pred_final[['LeadID','Converted','final_predicted','converted_prob']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_score_variable['converted_prob'] = round(y_test_score_variable['converted_prob']*100.0, 2)\ny_test_score_variable.rename(columns = {'converted_prob':'Lead Score'}).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}