{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Customer Classification**\n\nWe have a dataset consisting of Bank Customer information, so we build a classifier which will tell us if a customer will exit the bank or not."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We start by encoding the categorical values:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Multiple Columns Label Encoder\nfrom sklearn.preprocessing import LabelEncoder\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns \n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X):\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data Preprocessing**\nafter, we pass to the preprocessing phase, in this part, we separate training data and test, and we standardize the data with the *MinMaxScaler* function instead of the *StandardScaler* method because the range of values must be between 0 and 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nChurn_Modelling = pd.read_csv(\"../input/bank-customer-churn-modeling/Churn_Modelling.csv\")\nX = Churn_Modelling.iloc[:,3:-1]\ny = Churn_Modelling.iloc[:,-1]\nX = MultiColumnLabelEncoder(columns = ['Geography','Gender']).fit_transform(pd.DataFrame(X))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nsc = MinMaxScaler(feature_range=(0,1))\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Making the RNN (LSTM)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LSTM Implementation\nimport keras\nfrom subprocess import check_output\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nimport time\ntrainX = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\ntestX = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A hurestic tip is that the amount of nodes (dimensions) in your hidden layer should be the average of your input and output layers, which means that since we have **11** dimensions and we are looking for a binary output, we calculate this to be  **(11+1)รท2=6** .\n\n**The breakdown of the inputs for the output layer is as follows:**\n\n**optimizer:** *adam* The algorithm we want to use to find the optimal set of weights in the neural networks. Adam is a very efficeint variation of Stochastic Gradient Descent.\n\n**loss:** *binary_crossentropy* This is the loss function used within adam. This should be the logarthmic loss. If our dependent (output variable) is Binary, it is binary_crossentropy. If Categorical, then it is called categorical_crossentropy\n\n**metrics:** *[accuracy]* The accuracy metrics which will be evaluated(minimized) by the model. Used as accuracy criteria to imporve model performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from numpy import newaxis\nmodel = Sequential()\n\nmodel.add(LSTM(input_shape=(1,10),units=6,return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(32,return_sequences=True))\nmodel.add(LSTM(32))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(activation=\"sigmoid\", units=1))\n\nstart = time.time()\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\nprint ('compilation time : ', time.time() - start)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fitting the RNN**\n\nThis is where we will be fitting the RNN to our training set.\n\nThe breakdown of the inputs for compiling is as follows:\n**trainX** The independent variable portion of the data which needs to be fitted with the model.\n\n**y_train** The output portion of the data which the model needs to produce after fitting.\n\n**batch_size:** How often we want to back-propogate the error values so that individual node weights can be adjusted.\n\n**epochs:** The number of times we want to run the entire test data over again to tune the weights. This is like the fuel of the algorithm.\n\n**validation_split:** 0.1 The fraction of data to use for validation data."},{"metadata":{"trusted":true},"cell_type":"code","source":"history=model.fit(trainX,y_train,batch_size=500,epochs=1000,validation_split=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The output network should converge to an accuracy of around 86%"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainPredict = model.predict(trainX)\nprint(trainPredict)\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Testing the RNN**\nPredicting the Test set results\nThis shows the probability of a customer leaving given the testing data. Each row in X_test corresponds to a row in Y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(np.array(history.history['accuracy']) * 100)\nplt.plot(np.array(history.history['val_accuracy']) * 100)\nplt.ylabel('accuracy')\nplt.xlabel('epochs')\nplt.legend(['train', 'validation'])\nplt.title('Accuracy over epochs')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(testX)\nprint(y_pred[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To use the confusion Matrix, we need to convert the probabilities that a customer will leave the bank into the form true or false. So we will use the cutoff value 0.5 to indicate whether they are likely to exit or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = (y_pred > 0.5).astype(int)\nprint(y_pred[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Making the Confusion Matrix**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Significance of the confusion matrix value:**\n\nThis means that we should have about  *(1547+184)=1731*  correct classifications out of our total testing data size of  2000 . This means that our accuracy for this trial was  *1731รท2000=0.8655* , which matches the classifier's prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (((cm[0][0]+cm[1][1])*100)/(len(y_test)), '% of testing data was classified correctly')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}