{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NOTEBOOK OBJECTIVE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This notebook outlines the ML model using k-Nearest Neighbors algorithm. We will \n# 1. Load data, Explore data\n# 2. Visualise the dataset\n# 3. Determine the number of neighbours\n# 4. Predict the colour of the fruit - apple, mandarin, orange, lemon","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importing the key Python libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The following set of commands will load the necessary Python libraries\n\n# for linear algebra, random number capabilities\nimport numpy as np\n\n# for data manipulation, analysis and reading our dataset\nimport pandas as pd\n\n# for plotting and visualising the data\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. IMPORT & EXPLORE DATA"},{"metadata":{},"cell_type":"markdown","source":"### Load the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now that we have imported the necessary libraries, we will now use the panda command to load our dataset, which in the CSV format. You can also load CSV, TXT etc.\n# The file below is loaded from the same folder where the notebook is saved, and hence no file path is provided\nfruit = pd.read_csv('fruit_data_with_colours.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Explore the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now that the dataset is loaded, let's check the data and it's features using the head command\nfruit.head()\n\n# head function in python with no arguments gets the first five rows of data, and tail function the last 5\n# head function with specified N arguments e.g. N = 10, gets the first 10 rows of data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://)"},{"metadata":{},"cell_type":"markdown","source":"### Determine the number of pieces of fruits(rows) and attributes(columns)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(fruits.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Determine the fruits within the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The fruit_data_with_colours.csv has total of seven columns which contains the information about fruits. \n# The information is nothing but features such as fruit_subtype, mass, width, height, colour_score\n# In the table, using the head() function, we can only view apple and mandarin\n\nprint(fruits['fruit_name'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Determine the count of fruits within the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# As the next step, need to determine how many fruits are present in our data\n\nfruit['fruit_name'].value_counts()\n\n# The result is count of each of the data within the unique fruit_name column\n# We determined that there are four fruits, namely - apple, orange, lemon, mandarin, and each with count of entries","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. DATA VISUALISATION"},{"metadata":{},"cell_type":"markdown","source":"### Visualise the data"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Since we know the type of fruit and count, we will visualise it using a simple bar graph\n\n# Seaborn is a data visualization library in Python based on matplotlib\nimport seaborn as sns\n\nsns.countplot(fruits['fruit_name'], label=\"Count\", palette=\"Set3\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualise using Bloxplot (to assess the distribution)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fruits.drop('fruit_label', axis=1).plot(kind='box', subplots=True, layout=(2,2), \n                                        sharex=False, sharey=False, figsize=(10,10), \n                                        color ='c', patch_artist=True)\npl.suptitle(\"Box Plot for each input variable\")\nplt.savefig('fruits_boxplot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualise using Histogram (to understand the distribution)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# PyLab is a module that belongs to the Python mathematics library Matplotlib. \n# PyLab combines the numerical module numpy with the graphical plotting module pyplot\nimport pylab as pl\n\n# To create a histogram, we will use pandas hist() method.\nfruit.drop('fruit_label', axis=1).hist(bins=30, figsize=(10,10), color = \"c\", ec = \"m\", lw=0)\npl.suptitle(\"Histogram for each numeric input variable\")\nplt.savefig('fruits_histogram')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Gaussian distribution is also commonly called the \"normal distribution\" and is often described as a \"bell-shaped curve\".\n# Colour_Score and Height seem to be closer to the Gaussian distribution","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualise using Scatter matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\nfrom matplotlib import cm\n\ncmap = cm.get_cmap('gnuplot')\ndf = pd.DataFrame(np.random.randn(1000, 4), columns=['mass', 'width', 'height', 'color_score'])\nscatter_matrix(df, alpha=0.2, cmap = cmap, figsize=(10,10), marker = '.', s=30, hist_kwds={'bins':10}, range_padding=0.05, color = 'm')\nplt.suptitle('Scatter-matrix for each input variable')\nplt.savefig('fruit_scatter_matrix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. K-Nearest Neighbors"},{"metadata":{},"cell_type":"markdown","source":"### Build the KNN classifier model to determine K"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, import the KNeighborsClassifier module\n# details about the module here: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# In order to understand the model performance, divide the dataset into a training set and a test set.\n# The split is done by using the function train_test_split()\n# details here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the dataset into two different datasets\n# X for the independent features such as mass, width, height\n# Y for the dependent feature i.e. fruit name\nX = fruit[['mass','width','height','color_score']]\nY = fruit['fruit_name']\n\n# Now split the dataset X into two separate sets — X_train and X_test \n# Similarly, split the dataset Y into two sets — y_train and y_test\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n\n# Notice the use of test_size. This parameter decides the size of the data that has to be split as the test dataset\n# In the above case it is 0.2, which means that the dataset will be split 20% as the test dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's look at the statistical summary using describe() method"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Invoke the classifier and Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now create a KNN classifier for making predictions\nknn = KNeighborsClassifier()\n\n# Train the model using the training sets\nknn.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Note the output above that by default the n_neighbors = 5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the accuracy of the model for K=5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model Accuracy, how often is the classifier correct?\n# Accuracy can be computed by comparing actual test set values and predicted values.\n# The score function is simply a utility function for a default metric to be used within some algorithms of scikit-learn\nknn.score(X_test, y_test)\nprint(\"Accuracy for K=5 : \", knn.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Well, we got a classification rate of 58.33%, which is good\n# Now, let's increase the number of neighbors in the model and observe the accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the accuracy of the model for K=6"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 6)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\nprint(\"Accuracy for K=6 : \", knn.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We got a classification rate of 66.66%, which is even better","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the accuracy of the model for K=7"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 7)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\nprint(\"Accuracy for K=7 : \", knn.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ok, we got a classification rate of 66.66%, which is same as K=6","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Evaluate the accuracy of the model for K=8"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 8)\nknn.fit(X_train, y_train)\nknn.score(X_test, y_test)\nprint(\"Accuracy for K=8 : \", knn.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Great, we got a classification rate of 41.67%, so we decide the number of neighbors to be 7","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data visualisation: Find the most appropriate K by plotting the accuracy for the various neighbours in a graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"neighbours = np.arange(1,10)\ntraining_accuracy = np.empty(len(neighbours))\ntesting_accuracy = np.empty(len(neighbours))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(len(neighbours)):\n    knn = KNeighborsClassifier(n_neighbors = i+1)\n    knn.fit(X_train,y_train)\n    training_accuracy[i] = knn.score(X_train,y_train)\n    testing_accuracy[i] = knn.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('KNN - Accuracy for various neighbors')\nplt.plot(neighbours, testing_accuracy, label = 'Testing Accuracy', color ='c')\nplt.plot(neighbours, training_accuracy, label = 'Training accuracy', color ='m')\nplt.legend()\nplt.xlabel('No. of neighbours')\nplt.ylabel('Accuracy')\nplt.show()\nplt.savefig('knn - accuracy vs no of neighbours')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From the above graph, we can say that the best fit value for K is either 6 of 7. Going by the accuracy, consider 7","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion"},{"metadata":{"trusted":true},"cell_type":"code","source":"# As a result, we can say that using KNN algorithm with K=7, \n# we can estimate the \"Colour\" of a fruit from its \"Mass\", \"Width\", \"Height\",\"Color_Code\" values with 66.67% accuracy","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Plot the Decision Boundary of the k-NN Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.cm as cm\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\nimport matplotlib.patches as mpatches\nfrom sklearn import neighbors, datasets\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features. We could avoid this ugly slicing by using a two-dim dataset\ny = iris.target\nh = .02  # step size in the mesh\n\nn_neighbors = 7\n\n# Create color maps\ncmap_light = ListedColormap(['#FFFACD', '#7FFFD4', '#87CEFA'])\ncmap_bold = ListedColormap(['#FF0000', '#228B22', '#0000FF'])\n\nfor weights in ['distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\nplt.show()\nplt.savefig('classification chart')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### References: "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Li, Susan (2017), https://towardsdatascience.com/solving-a-simple-classification-problem-with-python-fruits-lovers-edition-d20ab6b071d2\n# Navlani, Avinash (2018), https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn\n# https://stackoverflow.com/questions/45075638/graph-k-nn-decision-boundaries-in-matplotlib/45076236\n# Various others","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}