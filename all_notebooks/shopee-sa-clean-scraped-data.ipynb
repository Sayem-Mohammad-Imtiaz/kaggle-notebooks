{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Setting package umum \nimport pandas as pd\nimport pandas_profiling as pp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm\nimport time\nimport tensorflow as tf\n%matplotlib inline\n\nfrom matplotlib.pylab import rcParams\n# For every plotting cell use this\n# grid = gridspec.GridSpec(n_row,n_col)\n# ax = plt.subplot(grid[i])\n# fig, axes = plt.subplots()\nrcParams['figure.figsize'] = [10,5]\nplt.style.use('fivethirtyeight') \nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm import tqdm\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 50)\npd.options.display.float_format = '{:.5f}'.format\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load dataset\ndf_scrape = pd.read_csv('/kaggle/input/shopee-reviews/shopee_reviews.csv')\ndf_test = pd.read_csv('/kaggle/input/shopee-sa-cleaning/test_cleaned.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Remove unwanted rows\ndf_scrape = df_scrape[df_scrape['label']!='label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Change label to numeric\ndf_scrape['label'] = df_scrape['label'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Change it into competition format\ndf_scrape = df_scrape[['text','label']]\ndf_scrape.columns = ['review','rating']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Function to sampling to mimic test dataset distribution\ndef dataset_sampling(dataset) :\n    \n    df = df_scrape.copy()\n\n    # Cut class 5 observation\n    df_no_c5 = df[df['rating']!=5]\n    df_c5 = df[df['rating']==5]\n    n_to_sample = (len(df_no_c5)*0.35) / (1-0.35)\n\n    df_c5_sampled = df_c5.sample(n=int(np.round(n_to_sample)))\n    df = pd.concat([df_no_c5, df_c5_sampled]).sample(frac=1)\n    \n    # Cut class 3 observation\n    df_no_c3 = df[df['rating']!=3]\n    df_c3 = df[df['rating']==3]\n    n_to_sample = (len(df_no_c3)*0.06) / (1-0.06)\n\n    df_c3_sampled = df_c3.sample(n=int(np.round(n_to_sample)))\n    df = pd.concat([df_no_c3, df_c3_sampled]).sample(frac=1)\n    \n    # Cut class 2 observation\n    df_no_c2 = df[df['rating']!=2]\n    df_c2 = df[df['rating']==2]\n    n_to_sample = (len(df_no_c3)*0.02) / (1-0.02)\n\n    df_c2_sampled = df_c2.sample(n=int(np.round(n_to_sample)))\n    df = pd.concat([df_no_c2, df_c2_sampled]).sample(frac=1)\n\n    return df.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Sampling dataset\ndf_scrape = dataset_sampling(df_scrape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Function to quick clean\nimport string\nimport re\nimport emoji  \n\ndef remove_punctuation(text) :\n    no_punct = ''.join([c for c in text if c not in string.punctuation])\n    \n    return no_punct\n\ndef encode_emoticon(text) :\n    \n    text = re.sub(r':\\(', 'dislike', text)\n    text = re.sub(r': \\(\\(', 'dislike', text)\n    text = re.sub(r':, \\(', 'dislike', text)\n    text = re.sub(r':\\)', 'smile', text)\n    text = re.sub(r';\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)\\)\\)\\)', 'smile', text)\n    text = re.sub(r'=\\)\\)\\)\\)', 'smile', text)\n    \n    return text\n\ndef quick_clean_data(dataset, var) :\n    \n    df = dataset.copy()\n    \n    # Lowercase\n    df[var] = df[var].str.lower()\n    \n    # Strip whitespaces\n    df[var] = df[var].str.strip()\n    \n    # Remove punctuation\n    df[var] = df.apply(lambda x : remove_punctuation(x[var]), axis=1)\n    \n    # Remove double whitespaces\n    df[var] = df.apply(lambda x : \" \".join(x[var].split()), axis=1)\n    \n    # Change emoticon to text\n    df[var] = df.apply(lambda x : encode_emoticon(x[var]), axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Decode emoji\nimport emoji  \n\ndef find_emoji(text) :\n    \n    # Change emoji to text\n    text = emoji.demojize(text).replace(\":\", \" \")\n    \n    # Delete repeated emoji\n    tokenizer = text.split()\n    repeated_list = []\n    \n    for word in tokenizer:\n        if word not in repeated_list:\n            repeated_list.append(word)\n    \n    text = ' '.join(text for text in repeated_list)\n    text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n    \n    return text\n\ndef encode_emoji(dataset, var) :\n    \n    df = dataset.copy()\n    \n    # Get index for rows with emoji\n    list_idx = []\n    for idx, review in enumerate(df[var]):\n        if any(char in emoji.UNICODE_EMOJI for char in review):\n            list_idx.append(idx)\n            \n    print('Percentage of dataset with emoji :',len(list_idx)/len(df)*100)\n            \n    # Encode emoji\n    df.loc[list_idx, var] = df.loc[list_idx, var].apply(find_emoji)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Clear repeated words\n# Indralin ways - dealing with bahasa text\ndef find_repeated_char(text) :\n\n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n    \n    return text\n\ndef delete_repeated_char(dataset, var) :\n    \n    df = dataset.copy()\n    \n    # Get index for rows with repeated char\n    list_idx = []\n    for idx, review in enumerate(df[var]):\n        if re.match(r'\\w*(\\w)\\1+', review):\n            list_idx.append(idx)\n            \n    print('Percentage of dataset with repeated char :',len(list_idx)/len(df)*100)\n    \n    # Delete repeated char\n    df[var] = df[var].apply(find_repeated_char)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Load english stop words\nfrom nltk.corpus import stopwords\n\ndef remove_stop_words(text) :\n    \n    # List of stop words\n    en_stop_words = stopwords.words('english')\n    \n    # Remove stop words \n    text = ' '.join([c for c in text.split() if c not in en_stop_words])    \n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Decode unicode\nfrom unicodedata import normalize\n\ndef unicode_char(text) :\n    text = normalize('NFD', text).encode('ascii', 'ignore')\n    text = text.decode('UTF-8')\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Remove duplicated row\ndef remove_duplicates(dataset, var) :\n    \n    df = dataset.copy()\n    \n    # Remove rows with inconsistent rating\n    df_mean_rating = df.groupby(var).mean().reset_index()\n    df_problem_review = df_mean_rating[df_mean_rating['rating'] % 1 != 0]\n    review_to_exclude = list(df_problem_review[var])\n    df = df[~df[var].isin(review_to_exclude)]\n    \n    # Remove duplicate row with consistent rating\n    df = df.drop_duplicates(subset=[var])\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Compile all preprocessing\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\ndef compile_cleaning(df, var) :\n    \n    df = quick_clean_data(df, var)\n    df = encode_emoji(df, var)\n    df = delete_repeated_char(df, var)\n    df[var] = df.progress_apply(lambda x : remove_stop_words(x[var]), axis=1)\n    df[var] = df.progress_apply(lambda x : unicode_char(x[var]), axis=1)\n    df = remove_duplicates(df, var)\n    \n    return df\n\ndf_scrape = compile_cleaning(df_scrape, 'review')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Save dataset\ndf_scrape.to_csv('train_cleaned_scraped.csv', index=False)\ndf_test.to_csv('test_cleaned.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}