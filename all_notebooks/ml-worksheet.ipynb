{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/data.csv\")\ndata.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.iloc[:40,:]\nplt.scatter(data.Finishing, data.Penalties, color = \"green\")\nplt.title(\"Finishing vs Penalties for Top 20 Players\")\nplt.xlabel(\"Finishing Score\")\nplt.ylabel(\"Penalties Score\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear Regression","metadata":{}},{"cell_type":"code","source":"linear_reg = LinearRegression()\nx = data.Finishing.values.reshape(-1,1)\ny = data.Penalties.values.reshape(-1,1)\nlinear_reg.fit(x,y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b0 = linear_reg.predict([[0]])\nprint(\"b0 :\", b0)\nb1 = linear_reg.coef_\nprint(\"b1 :\", b1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**y = b0 + b1*x**\n<bn>I have fitted real values to equation.\n<bn>**Penalty Score = 26.69 + 0.59*Finishing Score**","metadata":{}},{"cell_type":"code","source":"array = np.array(x)\ny_head = linear_reg.predict(array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(array,y_head, color = \"brown\")\nplt.scatter(x,y, color = 'green')\nplt.title(\"Finishing vs Penalties for Top 20 Players With Prediction Line(Linear Regression)\")\nplt.xlabel(\"Finishing Score\")\nplt.ylabel(\"Penalties Score\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### R_Square Score","metadata":{}},{"cell_type":"code","source":"print(\"R_Square Score is :\", r2_score(y,y_head))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Polynomial Regression","metadata":{}},{"cell_type":"markdown","source":"The linear regression model doesn't fit all data samples therefore I need to create a new parameter for finding best regression model.","metadata":{}},{"cell_type":"code","source":"polynomial_reg = PolynomialFeatures(degree = 2)\nx_polynomial = polynomial_reg.fit_transform(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linear_reg2 = LinearRegression()\nlinear_reg2.fit(x_polynomial, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Created a new Linear Regression model with 'x^2' and 'y' parameters.","metadata":{}},{"cell_type":"code","source":"y_head2 = linear_reg2.predict(x_polynomial)\nplt.scatter(x,y, color =\"Green\")\nplt.plot(x,y_head2, color = 'Brown')\nplt.title(\"Finishing vs Penalty Score Polynomial Linear Regression\")\nplt.xlabel(\"Finishing Score\")\nplt.ylabel(\"Penalty Score\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### R_Square Score","metadata":{}},{"cell_type":"code","source":"print(\"r2 score is :\", r2_score(y,y_head2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nx_ = x[:15].reshape(-1,1)\ny_ = y[:15].reshape(-1,1)\nd_tree = DecisionTreeRegressor()\nd_tree.fit(x_,y_)\ny_head_dtree = d_tree.predict(x_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(x_, y_, color = 'green')\nplt.plot(x_,y_head_dtree, color = 'brown' )\nplt.title(\"Finishing vs Penalty Score Decision Tree Regression with Top 15 samples\")\nplt.xlabel(\"Finishing Score\")\nplt.ylabel(\"Penalty Score\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### R_Square Score","metadata":{}},{"cell_type":"code","source":"print(\"r_square score is: \",r2_score(y_,y_head_dtree))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrandom_forest = RandomForestRegressor(n_estimators = 100, random_state = 42)\nrandom_forest.fit(x_,y_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#x__ = np.arange(min(x_), max(x_), .01).reshape(-1,1)\ny_head_rf = random_forest.predict(x_)\nplt.scatter(x_,y_, color = 'green')\nplt.plot(x_, y_head_rf, color = 'brown')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### R_Square Score\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import r2_score\nprint(\"r score :\", r2_score(y_,y_head_rf))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Logistic Regression","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/data.csv\")\ndata = data.iloc[:100,:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()\ny_data = data[\"Preferred Foot\"]\ny_data = [1 if each == 'Right' else 0 for each in y_data]\ny_data = pd.DataFrame(y_data)\ny_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_overall = data[\"Overall\"]\nlist_longpassing = data[\"LongPassing\"]\nlist_curve = data[\"Curve\"]\nlist_vision = data[\"Vision\"]\nlist_label = [\"Overall\", \"LongPassing\",\"Curve\", \"Vision\"]\nlist_column = [list_overall, list_longpassing, list_curve, list_vision]\nzipped = list(zip(list_label, list_column))\ndata_dict = dict(zipped)\nx_data = pd.DataFrame(data_dict)\nx_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_normalized = (x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data)).values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test  = train_test_split(x_normalized, y_data, test_size = 0.2, random_state = 42)\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T, y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T, y_test.T)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KNN Algorithm ","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/data.csv\")\ndata = data.iloc[:100,:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_columns = data.columns\nlist_columns = [each.replace(\" \", \"\") for each in list_columns]\ndata.columns = list_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Left = data[data.PreferredFoot == 'Left']\nRight = data[data.PreferredFoot == 'Right']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(Left.Overall,Left.Curve, color = 'green', label = 'Left')\nplt.scatter(Right.Overall,Right.Curve, color = 'red', label = 'Rigth')\nplt.legend()\nplt.xlabel(\"Overall\")\nplt.ylabel(\"Curve\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preferred_foot = [1 if i == 'Left' else 0 for i in data.PreferredFoot]\ny = pd.DataFrame(preferred_foot)\n#normalization\nx = (x_data - np.min(x_data))/(np.max(x_data) - np.min(x_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state = 42)                                         ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) # k value\nknn.fit(x_train, y_train)\nprediction = knn.predict(x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"{}nn  score : {}\".format(3, knn.score(x_test, y_test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find k value:\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test, y_test))\n    \nplt.plot(range(1,15), score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Support Vector Machine","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svm.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy of SVM alghorithm: \", svm.score(x_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Naive Bayes Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy of Naive Bayes Algorithm : \",nb.score(x_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy of Decision Tree Classification Algorithm : \", dt.score(x_test,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest Classification","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100, random_state = 42)#count of d.trees, randomize id \nrf.fit(x_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy of Random Forest Classification Algorithm : \", rf.score(x_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation Classification Models - Confusion Matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100, random_state = 42)#count of d.trees, randomize id \nrf.fit(x_train, y_train)\n\ny_prediction = rf.predict(x_test)\ny_true = y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true, y_prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualization\n\nf, ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidths = 0.5, linecolor = 'Red', fmt = '.0f', ax = ax)\nplt.xlabel(\"y_prediction\")\nplt.ylabel(\"y_true\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"True Negative = 18\n<bn>True Positive = 1\n<bn>False Negative = 6\n<bn>False Positive = 5","metadata":{}},{"cell_type":"markdown","source":"# Clustering Algorithms","metadata":{}},{"cell_type":"markdown","source":"### K Means Algorithm","metadata":{}},{"cell_type":"markdown","source":"* Assign a K value \n* Assign centroids randomly\n* Compare each data sample distance to given centroids\n* Calculate a new centroid point(mean value of data samples) until centroid will not change.","metadata":{}},{"cell_type":"markdown","source":"#### How to assign K Value ?\n* WCSS calculates. (WCSS = Within-Cluster Sum of Squares)\n* Draw a plot with 'K' and 'WCSS' values\n* Select the elbow point of plot for 'K' value\n","metadata":{}},{"cell_type":"code","source":"#creating dataset\n\n#class1\nx1 = np.random.normal(25,5,100)#ortalama, sigma değeri(25+sigma, 25-sigma),  1000 tane sayı\ny1 = np.random.normal(25,5,100)\n#class2\nx2 = np.random.normal(55,5,100)\ny2 = np.random.normal(60,5,100)\n#class3\nx3 = np.random.normal(55,5,100)\ny3 = np.random.normal(15,5,100)\n\nx = np.concatenate((x1,x2,x3), axis = 0)\ny = np.concatenate((y1,y2,y3), axis = 0)\ndictionary = {'x':x, 'y':y}\n\ndata = pd.DataFrame(dictionary)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(x1,y1)\nplt.scatter(x2,y2)\nplt.scatter(x3,y3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters = k)\n    kmeans.fit(data)\n    wcss.append(kmeans.inertia_) # calculate wcss for each k values\n    \nplt.plot(range(1,15), wcss)\nplt.xlabel(\"number of k value\")\nplt.ylabel(\"wcss\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Our elbow point is k = 3","metadata":{}},{"cell_type":"code","source":"kmeans2 = KMeans(n_clusters = 3)\nclusters = kmeans2.fit_predict(data)\n\ndata[\"label\"] = clusters\nplt.scatter(data.x[data.label == 0], data.y[data.label == 0], color = 'red')\nplt.scatter(data.x[data.label == 1], data.y[data.label == 1], color = 'green')\nplt.scatter(data.x[data.label == 2], data.y[data.label == 2], color = 'blue')\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:, 1], color = 'purple')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hierarchical Clustering","metadata":{}},{"cell_type":"markdown","source":"* Make a cluster each data sample\n* Find the closest samples and put them into a new cluster\n* Find the closest clusters and put them into a new cluster\n* Remake the 3rd step.","metadata":{}},{"cell_type":"markdown","source":"#### Distance \n* Euclidean distance (distance between two different points)\n* Euclidean distance between two different clusters\n    * Distance for the closest sample to other cluster\n    * Distance for the farthest sample to other cluster\n    * Distance for mean value of each cluster samples\n    * Distance for centroids","metadata":{}},{"cell_type":"code","source":"#%%  create dataset\n\n#class1\nx1 = np.random.normal(25,5,100)\ny1 = np.random.normal(25,5,100)\n#class2\nx2 = np.random.normal(55,5,100)\ny2 = np.random.normal(60,5,100)\n#class3\nx3 = np.random.normal(55,5,100)\ny3 = np.random.normal(15,5,100)\n\nx = np.concatenate((x1,x2,x3), axis = 0)\ny = np.concatenate((y1,y2,y3), axis = 0)\ndictionary = {'x':x, 'y':y}\n\ndata = pd.DataFrame(dictionary)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(x1,y1)\nplt.scatter(x2,y2)\nplt.scatter(x3,y3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%dendrogram\nfrom scipy.cluster.hierarchy import linkage,dendrogram\n\nmerg = linkage(data,method = \"ward\")#wcss\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidian distance\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### How to use the Dendrogram\n* Find the longest distance on Dendrogram(the not splitted with a horizontal line)\n* After finding the longest distance, we can see the cluster's count with drawing a horizontal line on it.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering #tümevarım kümeleme\n\nhierarchical_cluster = AgglomerativeClustering(n_clusters = 3, affinity=\"euclidean\", linkage = \"ward\")\ncluster = hierarchical_cluster.fit_predict(data)\n\n\ndata[\"label\"] = cluster \n\nplt.scatter(data.x[data.label == 0], data.y[data.label == 0 ], color = 'red')\nplt.scatter(data.x[data.label == 1], data.y[data.label == 1 ], color = 'green')\nplt.scatter(data.x[data.label == 2], data.y[data.label == 2 ], color = 'blue')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}