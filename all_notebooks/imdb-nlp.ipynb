{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 自然言語処理 ー 感情分析","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## 1.　自然言語処理の6ステップ","metadata":{}},{"cell_type":"markdown","source":"#### ① 自然言語（文章）を用意\n　吾輩は猫である。","metadata":{}},{"cell_type":"markdown","source":"#### ② 単語ごとに区切る（形態素解析）\n　吾輩　／　は　／　猫　／　で　／　ある　／　。","metadata":{}},{"cell_type":"markdown","source":"#### ③ 助詞「「てにをは」などを削除する（データのクレンジング）\n　吾輩　／　猫　　　※それ以外は削除","metadata":{}},{"cell_type":"markdown","source":"#### ④ 単語を原形（running→run など）に戻す（トークン化）\n　今回はなし","metadata":{}},{"cell_type":"markdown","source":"#### ⑤ 各語句の頻出度を求める（BoW）\n　吾輩：１　猫：1","metadata":{}},{"cell_type":"markdown","source":"#### ⑥ 各語句の重み（TF-IDF）を調べ、特徴語を決める\n　吾輩：xxx　、猫：xxxx　　　→　特徴語は「猫」","metadata":{}},{"cell_type":"markdown","source":"#### ⑦ 他の文章との区別（分類）を行う（ロジスティック回帰）\n　夏目漱石「吾輩は猫である」の特徴語：猫<br>\n 　森鴎外「舞姫」の特徴語：姫\n  \n　どちらかの小説の一文を投入する　→　この小説は「夏目漱石」の小説です","metadata":{}},{"cell_type":"markdown","source":"## 2.　IMDbデータセットの取得<br>","metadata":{}},{"cell_type":"markdown","source":"今回は、以下のデータセットを使用する。\n\n・[IMDb](http://ai.stanford.edu/~amaas/data/sentiment/)：映画レビューのデータセット","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport sys\nimport tarfile\nimport time\nimport pyprind\nimport numpy as np\nimport pandas as pd\nimport nltk\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-08-03T23:43:43.367391Z","iopub.execute_input":"2021-08-03T23:43:43.367726Z","iopub.status.idle":"2021-08-03T23:43:44.677981Z","shell.execute_reply.started":"2021-08-03T23:43:43.367645Z","shell.execute_reply":"2021-08-03T23:43:44.677142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/movie-datacsv/movie_data.csv')\nprint(df.shape)\n\npd.set_option(\"display.max_colwidth\", 150)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:44.67949Z","iopub.execute_input":"2021-08-03T23:43:44.679857Z","iopub.status.idle":"2021-08-03T23:43:46.434825Z","shell.execute_reply.started":"2021-08-03T23:43:44.679821Z","shell.execute_reply":"2021-08-03T23:43:46.433847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.　BoWベクトル","metadata":{}},{"cell_type":"markdown","source":"文を数値化（ベクトル化）する方法として、**Bag of Words（BOW）**がある。","metadata":{}},{"cell_type":"markdown","source":"### 3.1　単語を特徴ベクトルに変換する","metadata":{}},{"cell_type":"code","source":"docs = np.array(['The sun is shining',\n                 'The weather is sweet',\n                 'The sun is shining, the weather is sweet, and one and one is two'])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.43671Z","iopub.execute_input":"2021-08-03T23:43:46.437073Z","iopub.status.idle":"2021-08-03T23:43:46.441723Z","shell.execute_reply.started":"2021-08-03T23:43:46.437035Z","shell.execute_reply":"2021-08-03T23:43:46.440503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"scikit-learnに実装されているCountVectorizer()で、BoWベクトルをカウントする。","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:18:04.16365Z","iopub.execute_input":"2021-07-30T11:18:04.164292Z","iopub.status.idle":"2021-07-30T11:18:04.174157Z","shell.execute_reply.started":"2021-07-30T11:18:04.164237Z","shell.execute_reply":"2021-07-30T11:18:04.171547Z"}}},{"cell_type":"code","source":"count = CountVectorizer()\nbag = count.fit_transform(docs)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.443669Z","iopub.execute_input":"2021-08-03T23:43:46.444082Z","iopub.status.idle":"2021-08-03T23:43:46.454693Z","shell.execute_reply.started":"2021-08-03T23:43:46.444041Z","shell.execute_reply":"2021-08-03T23:43:46.453645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"上記によって、次の3つの文章におけるBoWモデルが構築された。<br>\nここで、各単語に番号（インデックス）を付け、それぞれ確認をする。","metadata":{}},{"cell_type":"code","source":"print('単語:インデックス', count.vocabulary_)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.456131Z","iopub.execute_input":"2021-08-03T23:43:46.456548Z","iopub.status.idle":"2021-08-03T23:43:46.46318Z","shell.execute_reply.started":"2021-08-03T23:43:46.456507Z","shell.execute_reply":"2021-08-03T23:43:46.462079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3文目の「the」の頻出度を調べてみる。<br>\n0から数えるとインデックスは2となり、最後には単語を入れる。","metadata":{}},{"cell_type":"code","source":"print('頻出度：', bag.toarray()[2][count.vocabulary_['the']])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.464889Z","iopub.execute_input":"2021-08-03T23:43:46.465429Z","iopub.status.idle":"2021-08-03T23:43:46.473276Z","shell.execute_reply.started":"2021-08-03T23:43:46.465275Z","shell.execute_reply":"2021-08-03T23:43:46.472079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"次に、特徴ベクトルを出力する。","metadata":{"execution":{"iopub.status.busy":"2021-07-30T11:19:54.863996Z","iopub.execute_input":"2021-07-30T11:19:54.864599Z","iopub.status.idle":"2021-07-30T11:19:54.873052Z","shell.execute_reply.started":"2021-07-30T11:19:54.864555Z","shell.execute_reply":"2021-07-30T11:19:54.871126Z"}}},{"cell_type":"code","source":"pd.DataFrame(bag.toarray())","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.474955Z","iopub.execute_input":"2021-08-03T23:43:46.475357Z","iopub.status.idle":"2021-08-03T23:43:46.491065Z","shell.execute_reply.started":"2021-08-03T23:43:46.475317Z","shell.execute_reply":"2021-08-03T23:43:46.490102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"行は1～3文目、列は各単語のインデックス番号を前提に表示されている。<br>\n分かりやすいように、表示し直す。","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(bag.toarray(),\n             index=['1文目', '2文目', '3文目'],\n             columns=['and', 'is', 'one', 'shining', 'sun', 'sweet', 'the', 'two', 'weather'])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.493692Z","iopub.execute_input":"2021-08-03T23:43:46.494073Z","iopub.status.idle":"2021-08-03T23:43:46.506369Z","shell.execute_reply.started":"2021-08-03T23:43:46.494033Z","shell.execute_reply":"2021-08-03T23:43:46.505108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2　TF-IDFを使って単語の関連性を評価する","metadata":{}},{"cell_type":"markdown","source":"$\\rm{TF}$：対象の単語が、ある文書中にどれだけの頻度で出現しているか（頻度）<br>\n$\\rm{IDF}$：対象の単語が含まれる文が、文章全体にどれだけの頻度で出現していないか（珍しさ）<br>\n$\\rm{TF-IDF}$：Term Frequency – Inverse Document Frequency の略（重みづけの指標）","metadata":{}},{"cell_type":"markdown","source":"例えば、小説一冊が、下記で構成されているとする。\n\n・$D$ 個の文<br>\n・$N$ 個の単語\n\n小説に単語 $x$ が $n$ 回現れるならば\n$$\\begin{eqnarray}\n{\\rm TF}=\\frac{n}{N}\n\\end{eqnarray}$$\n小説に単語 $x$ を含む文が $d$ 個あるならば\n$$\\begin{eqnarray}\n{\\rm IDF}=-\\log_{10}\\frac{d}{D}=\\log_{10}\\frac{D}{d}\n\\end{eqnarray}$$\nよって、次のように求まる。\n$$\\begin{eqnarray}\n{\\rm TF-IDF}=\\frac{n}{N}\\log_{10}\\frac{D}{d}\n\\end{eqnarray}$$","metadata":{}},{"cell_type":"markdown","source":"例題\n\n太宰治の作品「一歩前進二歩退却」から一部を抜粋したものである。<br>\nTF-IDFを求めなさい。\n\n-----\n\n　作家は、いよいよ窮屈である。何せ、眼光紙背に徹する読者ばかりを<br>\n相手にしているのだから、うっかりできない。あんまり緊張して、ついには<br>\n机のまえに端座したまま、そのまま、沈黙は金、という格言を底知れず肯定している。<br>\nそんなあわれな作家さえ出て来ぬともかぎらない。<br>\n　謙譲を、作家のみ要求し、作家は大いに恐縮し、卑屈なほどへりくだって<br>\nそうして読者は旦那である。作家の私生活、底の底まで剥ごうとする。<br>\n失敗である。安売りにしていいのは作品である。作家の人間までを売ってはいない。<br>\n謙譲は、読者にこそ之を要求したい。\n\n-----\n\n下記をカウントした。<br>\n<br>\n単語数 $N$：150<br>\n単語の種類：48<br>\n文数 $D$：10\n\n次に「作家」という単語 $x$ に着目する。\n\n出現回数 $n$：6<br>\n含まれる文数 $d$：5\n\nよって、次のように求まる。<br>\n\n$$\\begin{eqnarray}\n{\\rm TF}=\\frac{n}{N}=\\frac{6}{150}=0.04\n\\end{eqnarray}$$\n\n$$\\begin{eqnarray}\n{\\rm IDF}=\\log_{10}\\frac{D}{d}=\\log_{10}\\frac{10}{5}=0.301\n\\end{eqnarray}$$\n\n$$\\begin{eqnarray}\n{\\rm TF-IDF}=0.0120\n\\end{eqnarray}$$<br>\n<br>\n実際の自然言語処理では、これを48種類の単語すべてに行っていく。","metadata":{}},{"cell_type":"markdown","source":"### 3.3　scikit-learnでTF-IDFを実装する","metadata":{}},{"cell_type":"markdown","source":"scikit-learnには、TfidTransformerという関数が実装されている。<br>\nこれは、fit_transformメソッドのCountVectorizerから「生の単語の出現頻度」を入力として受け取り、TF-IDFに変換する。","metadata":{}},{"cell_type":"code","source":"tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\ntfidf_vec = tfidf.fit_transform(count.fit_transform(docs)).toarray()\n\npd.DataFrame(tfidf_vec.round(2))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.508845Z","iopub.execute_input":"2021-08-03T23:43:46.509303Z","iopub.status.idle":"2021-08-03T23:43:46.533093Z","shell.execute_reply.started":"2021-08-03T23:43:46.509253Z","shell.execute_reply":"2021-08-03T23:43:46.532108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"smooth_idf：Truleがデフォルト<br>\nuse_idf：idf()の使用有無（idf()関数による重み付けを行うかどうか）<br>\nnorm：正則化の指定（デフォルトはなし、'l2'指定で単語ベクトルの長さが1になるよう正規化）<br>\ntoarray()：行列出力\n\nそれぞれの文や単語とも紐づけて可視化すると、以下のようになる。","metadata":{}},{"cell_type":"code","source":"tfidf_df = pd.DataFrame(tfidf_vec.round(2),\n                        index=['1文目', '2文目', '3文目'],\n                        columns=['and', 'is', 'one', 'shining', 'sun', 'sweet', 'the', 'two', 'weather'])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.534365Z","iopub.execute_input":"2021-08-03T23:43:46.534742Z","iopub.status.idle":"2021-08-03T23:43:46.541201Z","shell.execute_reply.started":"2021-08-03T23:43:46.534705Z","shell.execute_reply":"2021-08-03T23:43:46.539982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ここで、3つの文章を改めて見てみる。\n\n・The sun is shining<br>\n・The weather is sweet<br>\n・The sun is shining, the weather is sweet, and one and one is two","metadata":{}},{"cell_type":"markdown","source":"「is」は3つの文章で使われているため、特徴語ではなく、どの文法に必要な語と判断される。<br>\nTF-IDFが（0.45）とそれほど大きくないことからも考えられる。\n\n一方で、「one」は3つ目の文章だけで2回使用されているので<br>\nTF-IDFは（0.5）と少しだけ大きくなっている。","metadata":{}},{"cell_type":"markdown","source":"### 3.4　scikit-learnにおけるTF-IDFの定義式","metadata":{}},{"cell_type":"markdown","source":"また各々の特徴ベクトルのTF-IDFを「手動」で計算すると、上記の計算と合わないことが分かる。<br>\nscikit-learnについて、厳密には下記の式で定義される。","metadata":{}},{"cell_type":"markdown","source":"$$\\begin{eqnarray}\n{\\rm tf}(t,d)=N\n\\end{eqnarray}$$\n\n$$\\begin{eqnarray}\n{\\rm idf}(t,d) = \\log\\frac{1 + n_d}{1 + {\\rm idf}(d, t)}+1\n\\end{eqnarray}$$\n\n$$\\begin{eqnarray}\n{\\rm tf-idf}(t,d)={\\rm tf(t,d)}\\times {\\rm idf(t,d)}\n\\end{eqnarray}$$","metadata":{}},{"cell_type":"markdown","source":"$N$：1つの文章の中にある、単語$t$の出現回数","metadata":{"execution":{"iopub.status.busy":"2021-08-03T04:42:06.952413Z","iopub.execute_input":"2021-08-03T04:42:06.952735Z","iopub.status.idle":"2021-08-03T04:42:06.95828Z","shell.execute_reply.started":"2021-08-03T04:42:06.952705Z","shell.execute_reply":"2021-08-03T04:42:06.956989Z"}}},{"cell_type":"markdown","source":"TF-IDFの正規化については、次式のように定義される。","metadata":{}},{"cell_type":"markdown","source":"$$v_{\\text{norm}} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_{1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$","metadata":{"execution":{"iopub.status.busy":"2021-08-03T04:37:33.858295Z","iopub.execute_input":"2021-08-03T04:37:33.85862Z","iopub.status.idle":"2021-08-03T04:37:33.864372Z","shell.execute_reply.started":"2021-08-03T04:37:33.858592Z","shell.execute_reply":"2021-08-03T04:37:33.863153Z"}}},{"cell_type":"markdown","source":"ここで、3つ目の文章について、改めて確認をする。\n\n・The sun is shining, the weather is sweet, and one and one is two\n\n今後は「is」に着目して、考えていくことにする。","metadata":{}},{"cell_type":"markdown","source":"### 3.5　scikit-learnの計算方法を手動で行う","metadata":{}},{"cell_type":"markdown","source":"TFについて、scikit-learnでは、単語「is」の出現回数がそのままの値となる。<br>\n3つ目の文章において、「is」は3個あるため、次のようになる。","metadata":{}},{"cell_type":"markdown","source":"$$\\text{tf}(\"is\")= 3$$\n\n$$\\text{idf}(\"is\", d3) = log \\frac{1+3}{1+3}+1 = 0+1=1$$\n\nよってTF-IDFは、次の値となる。\n\n$$\\text{tf-idf}(\"is\",d3)= 3 \\times 1 = 3$$","metadata":{}},{"cell_type":"markdown","source":"これらの計算を3つ目の文章の全ての単語に行った後で<br>\n先ほど説明した正規化の式に従って、TF-IDFのL2正規化を行う。","metadata":{}},{"cell_type":"markdown","source":"$$\\text{tf-idf}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$\n\n$$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$\n\n$$\\Rightarrow \\text{tf-idf}_{norm}(\"is\", d3) = 0.45$$","metadata":{}},{"cell_type":"code","source":"tfidf_df.loc['3文目', 'is']","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.542632Z","iopub.execute_input":"2021-08-03T23:43:46.54333Z","iopub.status.idle":"2021-08-03T23:43:46.554816Z","shell.execute_reply.started":"2021-08-03T23:43:46.543269Z","shell.execute_reply":"2021-08-03T23:43:46.553491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"これにより、手動での計算結果とscikit-learnの計算結果の一致が証明できた。<br>\n自然言語処理では、このTF・IDF・TF-IDFをもとに、特徴ベクトル（特徴語、重み）を決定する。","metadata":{}},{"cell_type":"markdown","source":"## 4.　クレンジング","metadata":{}},{"cell_type":"markdown","source":"まず、映画レビューデータセットの1つ目の文章から、最後の50文字を出力する。","metadata":{}},{"cell_type":"code","source":"df.loc[0, 'review'][-50:]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.556476Z","iopub.execute_input":"2021-08-03T23:43:46.556951Z","iopub.status.idle":"2021-08-03T23:43:46.568474Z","shell.execute_reply.started":"2021-08-03T23:43:46.556906Z","shell.execute_reply":"2021-08-03T23:43:46.567176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"出力結果を見てみると、不要な句読点や非英字文字が多い。<br>\n感情分析に役に立ちそうな顔文字要素のある記号「　：)　」のみ残し、それ以外はすべて削除する。<br>\nその際、今回はPythonの正規表現ライブラリを使用する。","metadata":{}},{"cell_type":"code","source":"def preprocessor(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.570165Z","iopub.execute_input":"2021-08-03T23:43:46.57062Z","iopub.status.idle":"2021-08-03T23:43:46.578461Z","shell.execute_reply.started":"2021-08-03T23:43:46.570575Z","shell.execute_reply":"2021-08-03T23:43:46.577412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"re.sub：正規表現で指定した文字列を置換する<br>\nstr.lower(): すべての文字を小文字に変換\n\n2行目：正規表現（<[^>]*>）を使用し、HTMLマークアップを削除<br>\n3行目：顔文字を検索し、emoticonsに格納<br>\n4行目：正規表現[\\w]+を使って単語の一部でない文字を削除、小文字に変換し、emoticonsを加え、顔文字内の「-」を消去","metadata":{}},{"cell_type":"code","source":"print('変換前:', df.loc[0, 'review'][-50:])\nprint('変換後:', preprocessor(df.loc[0, 'review'][-50:]))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.580018Z","iopub.execute_input":"2021-08-03T23:43:46.580849Z","iopub.status.idle":"2021-08-03T23:43:46.590128Z","shell.execute_reply.started":"2021-08-03T23:43:46.580773Z","shell.execute_reply":"2021-08-03T23:43:46.588878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(preprocessor(\"</a>This :) is :( a test :-)!\"))\nprint(preprocessor(\"!\\/.i like ;.::python/:]/];/]\"))\nprint(preprocessor('machine\\::lear\\[:::nig];@[/]'))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.591655Z","iopub.execute_input":"2021-08-03T23:43:46.592014Z","iopub.status.idle":"2021-08-03T23:43:46.600346Z","shell.execute_reply.started":"2021-08-03T23:43:46.591986Z","shell.execute_reply":"2021-08-03T23:43:46.599212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"再度、映画レビューデータセットの1行目を表示する。","metadata":{}},{"cell_type":"code","source":"print(df.shape)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.60205Z","iopub.execute_input":"2021-08-03T23:43:46.602496Z","iopub.status.idle":"2021-08-03T23:43:46.619738Z","shell.execute_reply.started":"2021-08-03T23:43:46.602459Z","shell.execute_reply":"2021-08-03T23:43:46.618432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"これに、先ほど作成したpreprocessor関数を適用する。<br>\n出力結果を確認すると、余計な文字や記号がなくなっていることが分かる。","metadata":{}},{"cell_type":"code","source":"df['review'] = df['review'].apply(preprocessor)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:46.621142Z","iopub.execute_input":"2021-08-03T23:43:46.621718Z","iopub.status.idle":"2021-08-03T23:43:56.126563Z","shell.execute_reply.started":"2021-08-03T23:43:46.62168Z","shell.execute_reply":"2021-08-03T23:43:56.125763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.　トークン化","metadata":{}},{"cell_type":"markdown","source":"トークン化：文章を個々の単語に分割したり、変換したりすること<br>\nワードステミング：単語を原形に変換すること　（例）running →　run　など\n\nワードステミングは、PorterStemmerによって開発され、Porter stemmingアルゴリズムとも呼ばれる。<br>\nNLTKライブラリに実装されている。","metadata":{}},{"cell_type":"code","source":"porter = PorterStemmer()\n\ndef tokenizer(text):\n    return text.split()\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:56.127834Z","iopub.execute_input":"2021-08-03T23:43:56.128177Z","iopub.status.idle":"2021-08-03T23:43:56.133631Z","shell.execute_reply.started":"2021-08-03T23:43:56.128146Z","shell.execute_reply":"2021-08-03T23:43:56.132505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"split()：文字を空白で分割<br>\ntokenizer_porter(text)：split()で分割した単語をfor文で1個ずつ取り出し、ワードステミングを実行\n\nここで、トークン化とトークン化＋ワードワードステミングの結果を比較する。","metadata":{}},{"cell_type":"code","source":"print(tokenizer('runners like running and thus they run'))\nprint(tokenizer_porter('runners like running and thus they run'))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:56.135832Z","iopub.execute_input":"2021-08-03T23:43:56.136624Z","iopub.status.idle":"2021-08-03T23:43:56.144841Z","shell.execute_reply.started":"2021-08-03T23:43:56.13658Z","shell.execute_reply":"2021-08-03T23:43:56.143757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"また、ストップワードの除去も行う。<br>\nこちらも、NLTKライブラリで実行することができる。\n\nまず、nltkに登録されているストップワードをダウンロードする。<br>\nその後、ストップワードでないものにtokenizer_porter()を適用し、単語wを抽出する。","metadata":{}},{"cell_type":"code","source":"nltk.download('stopwords')\n\nstop = stopwords.words('english')\n[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:56.146286Z","iopub.execute_input":"2021-08-03T23:43:56.146957Z","iopub.status.idle":"2021-08-03T23:43:56.23316Z","shell.execute_reply.started":"2021-08-03T23:43:56.146914Z","shell.execute_reply":"2021-08-03T23:43:56.232294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.　ロジスティック回帰による文書の分類","metadata":{}},{"cell_type":"markdown","source":"ロジスティック回帰を用いて、下記に分類する。\n\n1：肯定的なレビュー（面白かった など）<br>\n0：否定的なレビュー（つまなかった など）\n\nまず、クレンジングしたテキストのDataframeを、訓練データとテストデータに分割する。","metadata":{}},{"cell_type":"code","source":"print(df.shape)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:56.234384Z","iopub.execute_input":"2021-08-03T23:43:56.234697Z","iopub.status.idle":"2021-08-03T23:43:56.249235Z","shell.execute_reply.started":"2021-08-03T23:43:56.234664Z","shell.execute_reply":"2021-08-03T23:43:56.247838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = df.loc[:25000, 'review'].values\ny_train = df.loc[:25000, 'sentiment'].values\nX_test = df.loc[25000:, 'review'].values\ny_test = df.loc[25000:, 'sentiment'].values","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:56.250756Z","iopub.execute_input":"2021-08-03T23:43:56.251154Z","iopub.status.idle":"2021-08-03T23:43:56.25782Z","shell.execute_reply.started":"2021-08-03T23:43:56.251116Z","shell.execute_reply":"2021-08-03T23:43:56.256674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"次に、GridSearchCVを使って、最適なパラメーター集合を求める。<br>\nここでは、5分割交差検証を使用する。","metadata":{}},{"cell_type":"code","source":"param_grid = [\n              {'vect__ngram_range': [(1, 1)],\n               'vect__stop_words': [stop, None],\n               'vect__tokenizer': [tokenizer, tokenizer_porter],\n               'clf__penalty': ['l1', 'l2'],\n               'clf__C': [1.0, 10.0, 100.0]},\n              \n              {'vect__ngram_range': [(1, 1)],\n               'vect__stop_words': [stop, None],\n               'vect__tokenizer': [tokenizer, tokenizer_porter],\n               'vect__use_idf':[False],\n               'vect__norm':[None],\n               'clf__penalty': ['l1', 'l2'],\n               'clf__C': [1.0, 10.0, 100.0]},\n    \n              ]","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:56.262831Z","iopub.execute_input":"2021-08-03T23:43:56.263177Z","iopub.status.idle":"2021-08-03T23:43:56.269918Z","shell.execute_reply.started":"2021-08-03T23:43:56.263149Z","shell.execute_reply":"2021-08-03T23:43:56.268828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pipelineで、TfidfVectorizer()とLogisticRegression()をセットにする。","metadata":{}},{"cell_type":"code","source":"tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\nlogreg = LogisticRegression(random_state=0)\n\nlr_tfidf = Pipeline([('vect', tfidf), ('clf', logreg)])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:56.272379Z","iopub.execute_input":"2021-08-03T23:43:56.272674Z","iopub.status.idle":"2021-08-03T23:43:56.279717Z","shell.execute_reply.started":"2021-08-03T23:43:56.272639Z","shell.execute_reply":"2021-08-03T23:43:56.278854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n                           scoring='accuracy',\n                           cv=5,\n                           verbose=1,\n                           n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:56.281127Z","iopub.execute_input":"2021-08-03T23:43:56.281711Z","iopub.status.idle":"2021-08-03T23:43:56.288117Z","shell.execute_reply.started":"2021-08-03T23:43:56.281661Z","shell.execute_reply":"2021-08-03T23:43:56.287348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"scoring：評価指標（初期値はNone、分類なら正解率、回帰ならMSE など）<br>\nverbose：途中経過を詳細に出力（初期値はTrue）<br>\nn_jobs：並列実行数（初期値は1）","metadata":{}},{"cell_type":"code","source":"if 'TRAVIS' in os.environ:\n    gs_lr_tfidf.verbose=2\n\n    X_train = df.loc[:250, 'review'].values\n    y_train = df.loc[:250, 'sentiment'].values\n    X_test = df.loc[25000:25250, 'review'].values\n    y_test = df.loc[25000:25250, 'sentiment'].values","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:43:56.289534Z","iopub.execute_input":"2021-08-03T23:43:56.289934Z","iopub.status.idle":"2021-08-03T23:43:56.298093Z","shell.execute_reply.started":"2021-08-03T23:43:56.289901Z","shell.execute_reply":"2021-08-03T23:43:56.297131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"verbose: 0は標準出力にログを出力しない、1はログをプログレスバーで標準出力，2はエポックごとに1行のログを出力","metadata":{}},{"cell_type":"code","source":"gs_lr_tfidf.fit(X_train, y_train)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-03T23:43:56.299522Z","iopub.execute_input":"2021-08-03T23:43:56.300084Z","iopub.status.idle":"2021-08-03T23:44:07.197868Z","shell.execute_reply.started":"2021-08-03T23:43:56.300045Z","shell.execute_reply":"2021-08-03T23:44:07.195116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"グリッドサーチが完了した後、性能指標が最も高くなるパラメータセットを出力する。","metadata":{}},{"cell_type":"code","source":"print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-03T23:44:07.198881Z","iopub.status.idle":"2021-08-03T23:44:07.199242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(gs_lr_tfidf.best_estimator_)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-08-03T23:44:07.200275Z","iopub.status.idle":"2021-08-03T23:44:07.2008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:44:07.202239Z","iopub.status.idle":"2021-08-03T23:44:07.202806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"よって、「C=10のときに、ロジスティック回帰とTF-IDFを組み合わせることで、性能指標が最も高くなる」ことが分かった。","metadata":{}},{"cell_type":"code","source":"clf = gs_lr_tfidf.best_estimator_\n\ny_pred = clf.predict(X_test)\nprint(y_pred[:10])","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:44:07.204333Z","iopub.status.idle":"2021-08-03T23:44:07.204906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(X_test, columns=['review'])\ndf['prediction'] = y_pred\ndf['answer'] = y_test\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:44:07.206167Z","iopub.status.idle":"2021-08-03T23:44:07.206714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Test Accuracy: %.3f' % clf.score(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2021-08-03T23:44:07.208036Z","iopub.status.idle":"2021-08-03T23:44:07.20858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"今回作成した機械学習モデルでは、映画レビューのデータセットを投入したときに<br>\n各レビューが肯定的か否定的なのかを、約90%の精度で予測できることが分かった。","metadata":{}}]}