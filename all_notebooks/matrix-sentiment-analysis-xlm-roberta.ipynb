{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/shopee-sentiment-analysis/train.csv')\ntest_df = pd.read_csv('../input/shopee-sentiment-analysis/test.csv')\nprint(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Check for duplicate reviews**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dup_df = train_df[train_df['review'].duplicated()]\nprint(f'No. of duplicate reviews on train data: {dup_df.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check if the duplicate reviews have the same rating","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dup_df['check'] = dup_df.apply(lambda x: str(x.review) + str(x.rating), axis = 1)\nprint(dup_df['check'].duplicated().sum(),'of duplicate reviews have the same rating')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since most of the duplicate reviews have the same rating, we could choose to drop them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop_duplicates(subset = 'review', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Rating statistics**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['rating'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_df = train_df.groupby(['rating']).count()\ncount_df.drop(['review_id'], axis = 1, inplace = True)\ncount_df['percentage'] = 100 * count_df['review']  / count_df['review'].sum()\ncount_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['rating'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Length statistics**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\n\ndef count_len(text):\n    return len(word_tokenize(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['len'] = train_df['review'].apply(count_len)\ntest_df['len'] = test_df['review'].apply(count_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['len'].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['len'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the histogram, a length of 100 is enough to cover most (more than 90 pecent) of the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Extend train data with publicly, freely available external data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# kaggle.com/liuhh02/test-labelled\n# old test leak labelled\ntest_labelled = pd.read_csv('../input/test-labelled/test_labelled.csv')\ntest_labelled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dup_testlab = test_labelled[test_labelled['review'].duplicated()]\nprint(f'No. of duplicate reviews: {dup_testlab.shape[0]}')\ndup_testlab['check'] = dup_testlab.apply(lambda x: str(x.review) + str(x.rating), axis = 1)\nprint(dup_testlab['check'].duplicated().sum(),'of duplicate reviews have the same rating')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labelled.drop_duplicates(subset = 'review', inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# kaggle.com/shymammoth/shopee-reviews\n# scraped shopee reviews\nscraped_reviews = pd.read_csv('../input/shopee-reviews/shopee_reviews.csv')\nscraped_reviews","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Change column names for merging","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scraped_reviews.rename(columns = {'text': 'review', 'label': 'rating'}, inplace = True)\nscraped_reviews.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scraped_reviews['rating'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scraped_reviews = scraped_reviews[scraped_reviews['rating'] != 'label']\nscraped_reviews['rating'] = scraped_reviews['rating'].astype(int)\nscraped_reviews['rating'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dup_scraped = scraped_reviews[scraped_reviews['review'].duplicated()]\nprint(f'No. of duplicate reviews: {dup_scraped.shape[0]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.append(test_labelled, ignore_index = True)\ntrain_df = train_df.append(scraped_reviews, ignore_index = True)\ntrain_df = train_df.sample(frac = 1).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Final check for duplicates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dup_train = train_df[train_df['review'].duplicated()]\nprint(f'No. of duplicate reviews on train data: {dup_train.shape[0]}')\ndup_train['check'] = dup_train.apply(lambda x: str(x.review) + str(x.rating), axis = 1)\nprint(dup_train['check'].duplicated().sum(),'of duplicate reviews have the same rating')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['rating'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Trim train set by half to minimize training time","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(train_df[train_df['rating'] == 5].sample(frac = .6).index)\nprint(train_df['rating'].value_counts())\nprint(train_df.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data cleaning**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Join train and test for cleaning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding column 'rating' to test dataset\ntest_df['rating'] = -1 # flag to separate train and test\n\n# joining train and test datasets\nreviews = pd.concat([train_df, test_df], ignore_index = True)\nreviews","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk import FreqDist\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\n\nstemmer = SnowballStemmer('english')\nlemma = WordNetLemmatizer()\n\nfrom string import punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_review(review_col):\n    review_corpus=[]\n    \n    for i in range(0, len(review_col)):\n        review = str(review_col[i])\n        review = re.sub('[^a-zA-Z]', ' ', review)\n        review = [lemma.lemmatize(w) for w in word_tokenize(str(review).lower())]\n        review = ' '.join(review)\n        \n        review_corpus.append(review)\n        \n    return review_corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import emoji  \n\nhave_emoji_train_idx = []\n\nfor idx, review in enumerate(reviews['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_train_idx.append(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def emoji_cleaning(text):\n    \n    # change emoji to text\n    text = emoji.demojize(text).replace(\":\", \" \")\n    \n    # delete repeated emoji\n    tokenizer = text.split()\n    repeated_list = []\n    \n    for word in tokenizer:\n        if word not in repeated_list:\n            repeated_list.append(word)\n    \n    text = ' '.join(text for text in repeated_list)\n    text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# emoji_cleaning\nreviews.loc[have_emoji_train_idx, 'review'] = reviews.loc[have_emoji_train_idx, 'review'].apply(emoji_cleaning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def review_cleaning(text):\n    \n    text = text.lower()\n    text = re.sub(r'\\n', '', text)\n    \n#     text = text.replace(\"n't\", ' not')\n    \n    # change emoticon to text\n    text = re.sub(r':\\(', 'dislike', text)\n    text = re.sub(r': \\(\\(', 'dislike', text)\n    text = re.sub(r':, \\(', 'dislike', text)\n    text = re.sub(r':\\)', 'smile', text)\n    text = re.sub(r';\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)\\)\\)\\)', 'smile', text)\n    text = re.sub(r'=\\)\\)\\)\\)', 'smile', text)\n    \n#     # delete punctuation\n#     text = re.sub('[^a-z0-9 ]', ' ', text)\n    \n    tokenizer = text.split()\n    \n    return ' '.join([text for text in tokenizer])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews['review'] = reviews['review'].apply(review_cleaning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"repeated_rows_train = []\n\nfor idx, review in enumerate(reviews['review']):\n    if re.match(r'\\w*(\\w)\\1+', review):\n        repeated_rows_train.append(idx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def delete_repeated_char(text):\n    \n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews.loc[repeated_rows_train, 'review'] = reviews.loc[repeated_rows_train, 'review'].apply(delete_repeated_char)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recover_shortened_words(text):\n    \n    text = re.sub(r'\\bapaa\\b', 'apa', text)\n    \n    text = re.sub(r'\\bbsk\\b', 'besok', text)\n    text = re.sub(r'\\bbrngnya\\b', 'barangnya', text)\n    text = re.sub(r'\\bbrp\\b', 'berapa', text)\n    text = re.sub(r'\\bbgt\\b', 'banget', text)\n    text = re.sub(r'\\bbngt\\b', 'banget', text)\n    text = re.sub(r'\\bgini\\b', 'begini', text)\n    text = re.sub(r'\\bbrg\\b', 'barang', text)\n    \n    text = re.sub(r'\\bdtg\\b', 'datang', text)\n    text = re.sub(r'\\bd\\b', 'di', text)\n    text = re.sub(r'\\bsdh\\b', 'sudah', text)\n    text = re.sub(r'\\bdri\\b', 'dari', text)\n    text = re.sub(r'\\bdsni\\b', 'disini', text)\n    \n    text = re.sub(r'\\bgk\\b', 'gak', text)\n    \n    text = re.sub(r'\\bhrs\\b', 'harus', text)\n    \n    text = re.sub(r'\\bjd\\b', 'jadi', text)\n    text = re.sub(r'\\bjg\\b', 'juga', text)\n    text = re.sub(r'\\bjgn\\b', 'jangan', text)\n    \n    text = re.sub(r'\\blg\\b', 'lagi', text)\n    text = re.sub(r'\\blgi\\b', 'lagi', text)\n    text = re.sub(r'\\blbh\\b', 'lebih', text)\n    text = re.sub(r'\\blbih\\b', 'lebih', text)\n    \n    text = re.sub(r'\\bmksh\\b', 'makasih', text)\n    text = re.sub(r'\\bmna\\b', 'mana', text)\n    \n    text = re.sub(r'\\borg\\b', 'orang', text)\n    \n    text = re.sub(r'\\bpjg\\b', 'panjang', text)\n    \n    text = re.sub(r'\\bka\\b', 'kakak', text)\n    text = re.sub(r'\\bkk\\b', 'kakak', text)\n    text = re.sub(r'\\bklo\\b', 'kalau', text)\n    text = re.sub(r'\\bkmrn\\b', 'kemarin', text)\n    text = re.sub(r'\\bkmrin\\b', 'kemarin', text)\n    text = re.sub(r'\\bknp\\b', 'kenapa', text)\n    text = re.sub(r'\\bkcil\\b', 'kecil', text)\n    \n    text = re.sub(r'\\bgmn\\b', 'gimana', text)\n    text = re.sub(r'\\bgmna\\b', 'gimana', text)\n    \n    text = re.sub(r'\\btp\\b', 'tapi', text)\n    text = re.sub(r'\\btq\\b', 'thanks', text)\n    text = re.sub(r'\\btks\\b', 'thanks', text)\n    text = re.sub(r'\\btlg\\b', 'tolong', text)\n    text = re.sub(r'\\bgk\\b', 'tidak', text)\n    text = re.sub(r'\\bgak\\b', 'tidak', text)\n    text = re.sub(r'\\bgpp\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bgapapa\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bga\\b', 'tidak', text)\n    text = re.sub(r'\\btgl\\b', 'tanggal', text)\n    text = re.sub(r'\\btggl\\b', 'tanggal', text)\n    text = re.sub(r'\\bgamau\\b', 'tidak mau', text)\n    \n    text = re.sub(r'\\bsy\\b', 'saya', text)\n    text = re.sub(r'\\bsis\\b', 'sister', text)\n    text = re.sub(r'\\bsdgkan\\b', 'sedangkan', text)\n    text = re.sub(r'\\bmdh2n\\b', 'semoga', text)\n    text = re.sub(r'\\bsmoga\\b', 'semoga', text)\n    text = re.sub(r'\\bsmpai\\b', 'sampai', text)\n    text = re.sub(r'\\bnympe\\b', 'sampai', text)\n    text = re.sub(r'\\bdah\\b', 'sudah', text)\n    \n    text = re.sub(r'\\bberkali2\\b', 'repeated', text)\n    \n    text = re.sub(r'\\byg\\b', 'yang', text)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nreviews['review'] = reviews['review'].apply(recover_shortened_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaning round 2, lemmatization\nreviews['review'] = clean_review(reviews['review'].values)\nreviews","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Separating train and test sets**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = reviews[reviews.rating != -1]\ntrain_df.drop(['review_id', 'len'], axis = 1, inplace = True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = reviews[reviews.rating == -1]\ntest_df.drop(['rating', 'len'], axis = 1, inplace = True)\ntest_df['review_id'] = test_df['review_id'].astype(int)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save to file: clean extended train and test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[['review', 'rating']].to_csv('clean_extended_train.csv', index = False)\ntest_df[['review_id', 'review']].to_csv('clean_test_up.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import clean extended train and test","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# train_df = pd.read_csv('../input/shopee-code-league-2020-sentiment-analysis/clean_extended_train.csv').fillna('')\n# test_df = pd.read_csv('../input/shopee-code-league-2020-sentiment-analysis/clean_test_up.csv').fillna('')\n# print(train_df.shape, test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\n\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\nprint('Using Tensorflow version:', tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n             texts, \n             return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=True,\n             max_length=maxlen)\n    \n    return np.array(enc_di['input_ids'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(5, activation='softmax')(cls_token) # 5 ratings to predict\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 4\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMODEL = 'jplu/tf-xlm-roberta-large' # bert-base-multilingual-uncased","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Change range of rating from (1 to 5) to (0 to 4)**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# since keras takes 0 as the reference, our category should start from 0 not 1\nrating_mapper_encode = {1: 0,\n                        2: 1,\n                        3: 2,\n                        4: 3,\n                        5: 4}\n\n# convert back to original rating after prediction later\nrating_mapper_decode = {0: 1,\n                        1: 2,\n                        2: 3,\n                        3: 4,\n                        4: 5}\n\ntrain_df['rating'] = train_df['rating'].map(rating_mapper_encode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\n# convert to one-hot-encoding-labels\ntrain_labels = to_categorical(train_df['rating'], num_classes=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train_df['review'],\n                                                  train_labels,\n                                                  stratify=train_labels,\n                                                  test_size=0.1,\n                                                  random_state=1111)\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"MAX_LEN = 104 # chosen from EDA\n\nX_train = regular_encode(X_train.values, tokenizer, maxlen=MAX_LEN)\nX_val = regular_encode(X_val.values, tokenizer, maxlen=MAX_LEN)\nX_test = regular_encode(test_df['review'].values, tokenizer, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train, y_train))\n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val, y_val))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(X_test)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\n\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"n_steps = X_train.shape[0] // BATCH_SIZE\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.style.use('fivethirtyeight')\n\n# Get training and test loss histories\ntraining_loss = train_history.history['loss']\ntest_loss = train_history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred = model.predict(test_dataset, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# for ensemble\nnp.save('xlm-roberta', pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_sentiment = np.argmax(pred, axis=1)\n\nprint(pred_sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"submission = pd.DataFrame({'review_id': test_df['review_id'],\n                           'rating': pred_sentiment})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Convert rating values back to (1 - 5)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"submission['rating'] = submission['rating'].map(rating_mapper_decode)\nsubmission.to_csv('submission.csv', index=False)\nsubmission['rating'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Thanks to:**\n* indralin/text-processing-augmentation-tpu-baseline-0-4544\n* garyongguanjie/eda-sentiment-shopee","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}