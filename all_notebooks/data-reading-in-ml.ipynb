{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot\n\n# from pandas import read_csv\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-14T04:20:50.815211Z","iopub.execute_input":"2021-07-14T04:20:50.815615Z","iopub.status.idle":"2021-07-14T04:20:50.848255Z","shell.execute_reply.started":"2021-07-14T04:20:50.815533Z","shell.execute_reply":"2021-07-14T04:20:50.847311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read the data set... read_csv(path)\npath = '../input/iriscsv/Iris.csv'\nheadernames = ['sr.no','SepLen','SepWid','PetLen','PetWid','TypeofFlower']\ndata = pd.read_csv(path, names=headernames)  # custom headers\n#read the data from CSV and store it in the dataFrame\ndata1 = pd.read_csv(path, header=0)  #header indicates the row number to be included for getting attribute names from dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:50.849598Z","iopub.execute_input":"2021-07-14T04:20:50.849878Z","iopub.status.idle":"2021-07-14T04:20:50.958304Z","shell.execute_reply.started":"2021-07-14T04:20:50.849852Z","shell.execute_reply":"2021-07-14T04:20:50.957504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking into the data\ndata1","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:50.959971Z","iopub.execute_input":"2021-07-14T04:20:50.960266Z","iopub.status.idle":"2021-07-14T04:20:50.993586Z","shell.execute_reply.started":"2021-07-14T04:20:50.960226Z","shell.execute_reply":"2021-07-14T04:20:50.992743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting insights of dataset\ndata1.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:50.994871Z","iopub.execute_input":"2021-07-14T04:20:50.995142Z","iopub.status.idle":"2021-07-14T04:20:51.018679Z","shell.execute_reply.started":"2021-07-14T04:20:50.995116Z","shell.execute_reply":"2021-07-14T04:20:51.016966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to know th shape of data (rows and columns)\ndata1.shape","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:51.020408Z","iopub.execute_input":"2021-07-14T04:20:51.020823Z","iopub.status.idle":"2021-07-14T04:20:51.027721Z","shell.execute_reply.started":"2021-07-14T04:20:51.020779Z","shell.execute_reply":"2021-07-14T04:20:51.026635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#looking at Top five...\n\ndata1.head()   # arguments are deciding no. of rows. by default it is 5 \n\n# similary we can use tail() to get rows from bottom","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:51.029456Z","iopub.execute_input":"2021-07-14T04:20:51.029866Z","iopub.status.idle":"2021-07-14T04:20:51.04889Z","shell.execute_reply.started":"2021-07-14T04:20:51.029825Z","shell.execute_reply":"2021-07-14T04:20:51.047613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to know the datatypes of each attribute in dataset\nprint(data1.dtypes)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:51.050462Z","iopub.execute_input":"2021-07-14T04:20:51.051098Z","iopub.status.idle":"2021-07-14T04:20:51.063563Z","shell.execute_reply.started":"2021-07-14T04:20:51.051063Z","shell.execute_reply":"2021-07-14T04:20:51.062446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to get statistical summary of the data...\npd.set_option('display.width', 100)\npd.set_option('precision', 2)\ndata1.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:51.065549Z","iopub.execute_input":"2021-07-14T04:20:51.065836Z","iopub.status.idle":"2021-07-14T04:20:51.101815Z","shell.execute_reply.started":"2021-07-14T04:20:51.065808Z","shell.execute_reply":"2021-07-14T04:20:51.101127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to find the unique/distinct values of attribute \ncount_class = data1.groupby('Species').size()\nprint(count_class)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:51.103176Z","iopub.execute_input":"2021-07-14T04:20:51.103652Z","iopub.status.idle":"2021-07-14T04:20:51.10969Z","shell.execute_reply.started":"2021-07-14T04:20:51.103622Z","shell.execute_reply":"2021-07-14T04:20:51.108789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to understand relationship among attributes we use correlation\ncorrelations = data1.corr(method='pearson')\nprint(correlations)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:51.110804Z","iopub.execute_input":"2021-07-14T04:20:51.111323Z","iopub.status.idle":"2021-07-14T04:20:51.127758Z","shell.execute_reply.started":"2021-07-14T04:20:51.111292Z","shell.execute_reply":"2021-07-14T04:20:51.126609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here given given data set is having major positive correlation co-efficeints which indicates positive relationship among input attributes. Sepal length is highly positivly correlated with petallength. Petal length and Petal width are the highest correlated attributes.","metadata":{}},{"cell_type":"code","source":"# to find the skewness of the given data\nprint(data1.skew())","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:51.129182Z","iopub.execute_input":"2021-07-14T04:20:51.129507Z","iopub.status.idle":"2021-07-14T04:20:51.146544Z","shell.execute_reply.started":"2021-07-14T04:20:51.129478Z","shell.execute_reply":"2021-07-14T04:20:51.145535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the symmetricity perspective we can see that sepallength and sepalwidth are positivly skewed data where petallength and petalwidth are negetively skewed data","metadata":{}},{"cell_type":"code","source":"# to observe the attributes graphically with histogram\ndata1.hist()\npyplot.show() # to show the graph","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:51.14795Z","iopub.execute_input":"2021-07-14T04:20:51.148447Z","iopub.status.idle":"2021-07-14T04:20:51.752596Z","shell.execute_reply.started":"2021-07-14T04:20:51.148411Z","shell.execute_reply":"2021-07-14T04:20:51.751308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# density plot\ndata1.plot(kind='line', subplots=True, layout=(3,2), sharex=False)\n# kind -- type of plot we want to draw\n#subplots -true/false ----> True - allow us to draw subplots\n#layout - subplots alignment\n#sharex ,sharey --- axis sharing\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:51.754604Z","iopub.execute_input":"2021-07-14T04:20:51.754893Z","iopub.status.idle":"2021-07-14T04:20:52.334973Z","shell.execute_reply.started":"2021-07-14T04:20:51.754866Z","shell.execute_reply":"2021-07-14T04:20:52.33387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Box and Whisker Plots --> to understand the skeness in the attribute data...\ndata1.plot(kind='box', subplots=True, layout=(3,2), sharex=False)\n# kind -- type of plot we want to draw\n#subplots -true/false ----> True - allow us to draw subplots\n#layout - subplots alignment\n#sharex ,sharey --- axis sharing\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:52.337161Z","iopub.execute_input":"2021-07-14T04:20:52.337799Z","iopub.status.idle":"2021-07-14T04:20:52.795697Z","shell.execute_reply.started":"2021-07-14T04:20:52.337753Z","shell.execute_reply":"2021-07-14T04:20:52.794688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:52.797007Z","iopub.execute_input":"2021-07-14T04:20:52.797618Z","iopub.status.idle":"2021-07-14T04:20:52.827161Z","shell.execute_reply.started":"2021-07-14T04:20:52.797573Z","shell.execute_reply":"2021-07-14T04:20:52.826056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TO check/visaulize individual attribure\ndata1['SepalLengthCm'].plot(kind='density')\n\n#data1['PetalLengthCm'].plot(kind='box')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:52.828585Z","iopub.execute_input":"2021-07-14T04:20:52.829225Z","iopub.status.idle":"2021-07-14T04:20:54.026524Z","shell.execute_reply.started":"2021-07-14T04:20:52.829178Z","shell.execute_reply":"2021-07-14T04:20:54.02559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the correlation matrix plot\n\nfig = pyplot.figure()  # instance of a figure\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nnames = ['SL','SW','PL','PW']\nfig.colorbar(cax)\nticks = np.arange(0,4,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names)\nax.set_yticklabels(names)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:54.02773Z","iopub.execute_input":"2021-07-14T04:20:54.02802Z","iopub.status.idle":"2021-07-14T04:20:54.321333Z","shell.execute_reply.started":"2021-07-14T04:20:54.02799Z","shell.execute_reply":"2021-07-14T04:20:54.320274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n\nsns.heatmap(correlations, annot=True)\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:54.323029Z","iopub.execute_input":"2021-07-14T04:20:54.323734Z","iopub.status.idle":"2021-07-14T04:20:54.812558Z","shell.execute_reply.started":"2021-07-14T04:20:54.323686Z","shell.execute_reply":"2021-07-14T04:20:54.811718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# showing Scatter plot\n\nsns.pairplot(data1)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:54.813644Z","iopub.execute_input":"2021-07-14T04:20:54.814085Z","iopub.status.idle":"2021-07-14T04:20:59.958294Z","shell.execute_reply.started":"2021-07-14T04:20:54.814053Z","shell.execute_reply":"2021-07-14T04:20:59.957346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{}},{"cell_type":"markdown","source":"**1. Scaling**","metadata":{}},{"cell_type":"code","source":"# using MinMaxScaler from sklearn->preprocessing->MinMaxScaler()\n\nfrom sklearn import preprocessing\ndata1 = pd.read_csv(path, header=0)\n#print(data1)\n\n#to select specific column from dataframe use [[ ]] double square brackets.\ndata2 = data1[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n# print(data2)\n\narray = data2.values\nprint(array[:10])\n\n#creating the scaler function with MinMaxScaler\ndata_scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n\n#applying scaler to the data using fit_transform\ndata_rescaled = data_scaler.fit_transform(array)\n\n#setting precision of the scaled values\nnp.set_printoptions(precision=2)\nprint(data_rescaled[:10])","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:20:59.95966Z","iopub.execute_input":"2021-07-14T04:20:59.959972Z","iopub.status.idle":"2021-07-14T04:21:00.103162Z","shell.execute_reply.started":"2021-07-14T04:20:59.959942Z","shell.execute_reply":"2021-07-14T04:21:00.102094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. Noramlization**","metadata":{}},{"cell_type":"markdown","source":"**1. L1 Normalization**\n* It may be defined as the normalization technique that modifies the dataset values in a way that in each row the sum of the absolute values will always be up to 1. It is also called Least Absolute Deviations.","metadata":{}},{"cell_type":"code","source":"# using Normalizer from sklearn->preprocessing->Noramlizer()->L1,L2,....\n\nfrom sklearn.preprocessing import Normalizer\n\ndata1 = pd.read_csv(path, header=0)\n#print(data1)\n\n#to select specific column from dataframe use [[ ]] double square brackets.\ndata2 = data1[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n# print(data2)\n\narray = data2.values\nprint('-----------Original Data-----------')\nprint(array[:10])\nprint('-----------Original Data-----------')\n#creating and Fitting the normalizer function with L1 Normalization\ndata_noramlize = Normalizer(norm='l1').fit(array)\n\n#applying Normalizer to the data using transform\ndata_normalized = data_noramlize.transform(array)\n\n#setting precision of the scaled values\nnp.set_printoptions(precision=2)\nprint('-----------Normalized Data-----------')\nprint(data_normalized[:10])\nprint('-----------Normalized Data-----------')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:28:50.634981Z","iopub.execute_input":"2021-07-14T04:28:50.635366Z","iopub.status.idle":"2021-07-14T04:28:50.654955Z","shell.execute_reply.started":"2021-07-14T04:28:50.635333Z","shell.execute_reply":"2021-07-14T04:28:50.653952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2. L2 Normalization**\n* It may be defined as the normalization technique that modifies the dataset values in a way that in each row the sum of the squares will always be up to 1. It is also called least squares.","metadata":{}},{"cell_type":"code","source":"# using Normalizer from sklearn->preprocessing->Noramlizer()->L1,L2,....\n\nfrom sklearn.preprocessing import Normalizer\n\ndata1 = pd.read_csv(path, header=0)\n#print(data1)\n\n#to select specific column from dataframe use [[ ]] double square brackets.\ndata2 = data1[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n# print(data2)\n\narray = data2.values\nprint('-----------Original Data-----------')\nprint(array[:10])\nprint('-----------Original Data-----------')\n#creating and Fitting the normalizer function with L2 Normalization\ndata_noramlize = Normalizer(norm='l2').fit(array)\n\n#applying Normalizer to the data using transform\ndata_normalized = data_noramlize.transform(array)\n\n#setting precision of the scaled values\nnp.set_printoptions(precision=2)\nprint('-----------Normalized Data-----------')\nprint(data_normalized[:10])\nprint('-----------Normalized Data-----------')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:32:53.629215Z","iopub.execute_input":"2021-07-14T04:32:53.629602Z","iopub.status.idle":"2021-07-14T04:32:53.646715Z","shell.execute_reply.started":"2021-07-14T04:32:53.629567Z","shell.execute_reply":"2021-07-14T04:32:53.645314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. Binaraization**\n* As the name suggests, this is the technique with the help of which we can make our data binary. We can use a binary threshold for making our data binary. The values above that threshold value will be converted to 1 and below that threshold will be converted to 0. For example, if we choose threshold value = 0.5, then the dataset value above it will become 1 and below this will become 0. That is why we can call it binarizing the data or thresholding the data. This technique is useful when we have probabilities in our dataset and want to convert them into crisp values.\n\n* We can binarize the data with the help of Binarizer class of scikit-learn Python library.","metadata":{}},{"cell_type":"code","source":"# using Normalizer from sklearn->preprocessing->Bianrizer()\n\nfrom sklearn.preprocessing import Binarizer\n\ndata1 = pd.read_csv(path, header=0)\n#print(data1)\n\n#to select specific column from dataframe use [[ ]] double square brackets.\ndata2 = data1[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n# print(data2)\n\narray = data2.values\nprint('-----------Original Data-----------')\nprint(array[:10])\nprint('-----------Original Data-----------')\n#creating and Fitting the Binarizer function\ndata_binarize = Binarizer(threshold=3).fit(array)\n\n#applying Binarizer to the data using transform\ndata_binarized = data_binarize.transform(array)\n\n#setting precision of the binarized values\nnp.set_printoptions(precision=2)\nprint('-----------Binarized Data-----------')\nprint(data_binarized[:10])\nprint('-----------Binarized Data-----------')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:43:19.610589Z","iopub.execute_input":"2021-07-14T04:43:19.611162Z","iopub.status.idle":"2021-07-14T04:43:19.631105Z","shell.execute_reply.started":"2021-07-14T04:43:19.611104Z","shell.execute_reply":"2021-07-14T04:43:19.630056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4. Standardization**\n* Another useful data preprocessing technique which is basically used to transform the data attributes with a Gaussian distribution. It differs the mean and SD (Standard Deviation) to a standard Gaussian distribution with a mean of 0 and a SD of 1. This technique is useful in ML algorithms like linear regression, logistic regression that assumes a Gaussian distribution in input dataset and produce better results with rescaled data. We can standardize the data (mean = 0 and SD =1) with the help of StandardScaler class of scikit-learn Python library.","metadata":{}},{"cell_type":"code","source":"# using StandardScaler from sklearn->preprocessing->StandardScaler()\n\nfrom sklearn.preprocessing import StandardScaler\n\ndata1 = pd.read_csv(path, header=0)\n#print(data1)\n\n#to select specific column from dataframe use [[ ]] double square brackets.\ndata2 = data1[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n# print(data2)\n\narray = data2.values\nprint('-----------Original Data-----------')\nprint(array[:10])\nprint('-----------Original Data-----------')\n#creating and Fitting the StandardScaler function\ndata_StandardScaler = StandardScaler().fit(array)\n\n#applying StandardScaler to the data using transform\ndata_rescaled = data_StandardScaler.transform(array)\n\n#setting precision of the scaled values\nnp.set_printoptions(precision=2)\nprint('-----------Rescaled Data-----------')\nprint(data_rescaled[:10])\nprint('-----------Rescaled Data-----------')\n\n#converting array to dataframe\ndf = pd.DataFrame(data = data_rescaled)\n\n#plotting attribure after Standard scaling\ndf[[1]].plot(kind='line')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:59:14.701142Z","iopub.execute_input":"2021-07-14T04:59:14.70152Z","iopub.status.idle":"2021-07-14T04:59:14.910054Z","shell.execute_reply.started":"2021-07-14T04:59:14.701487Z","shell.execute_reply":"2021-07-14T04:59:14.908621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Data Labeling\n* We discussed the importance of good fata for ML algorithms as well as some techniques to pre-process the data before sending it to ML algorithms. One more aspect in this regard is data labeling. It is also very important to send the data to ML algorithms having proper labeling.\n\n* **Label Encoding**\nMost of the sklearn functions expect that the data with number labels rather than word labels. Hence, we need to convert such labels into number labels. This process is called label encoding.","metadata":{}},{"cell_type":"code","source":"#importing the preprocessing library\nfrom sklearn import preprocessing\n\n# distinct values of attribure in word form is passed as input_labels..\ninput_labels = ['male','female']\n\n#creating the Labelencoder from the input labels.\nencoder = preprocessing.LabelEncoder()\nencoder.fit(input_labels)\n\n# attribute values to be transformed are passed in transform function\ntest_labels = ['female','female','male','female','male']\nencoded_values = encoder.transform(test_labels)\nprint(\"\\nLabels =\", test_labels)\nprint(\"Encoded values =\", list(encoded_values))\n\n#getting original word labels by applying decoding\ndecoded_list = encoder.inverse_transform(encoded_values)\nprint(\"\\nDecoded labels =\", list(decoded_list))","metadata":{"execution":{"iopub.status.busy":"2021-07-14T05:15:49.838436Z","iopub.execute_input":"2021-07-14T05:15:49.838816Z","iopub.status.idle":"2021-07-14T05:15:49.847887Z","shell.execute_reply.started":"2021-07-14T05:15:49.838781Z","shell.execute_reply":"2021-07-14T05:15:49.846694Z"},"trusted":true},"execution_count":null,"outputs":[]}]}