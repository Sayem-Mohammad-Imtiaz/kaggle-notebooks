{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font color='darkblue' size=7 style=\"font-family:garamond;\">Leaving job prediction & EDA</font>"},{"metadata":{},"cell_type":"markdown","source":"Welcome to this notebook, where today we will be performing EDA and predicting whether a person will leave their job."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"http://adcengineers.com/wp-content/uploads/2016/06/careers.jpg\" width=\"1000px\">"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import boxcox\nfrom collections import Counter\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.svm import LinearSVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='purple' size=6 style=\"font-family:garamond;\">Gathering data</font>"},{"metadata":{},"cell_type":"markdown","source":"The first step is to acquire our dataset and put it in an 'X' variable. We extract the 'target' feature out of X and assign a variable 'y' to it."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"X = pd.read_csv('../input/hr-analytics-job-change-of-data-scientists/aug_train.csv')\ny = X['target']\n\nX = X.drop(['enrollee_id', 'target'], axis=1)\nX['major_discipline'][X['major_discipline']=='Business Degree'] = 'Business'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='darkblue' size=7 style=\"font-family:garamond;\">Categorical feature visualisation</font>"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/1000/1*WACiczYwdWTnJ94mnizS4Q.jpeg\" width=\"400px\">"},{"metadata":{},"cell_type":"markdown","source":"<font color='purple' size=6 style=\"font-family:garamond;\">Pie charts</font>"},{"metadata":{},"cell_type":"markdown","source":"The first part of our EDA is displaying the different categorical features in our dataset. We take three columns: '**experience**', '**company_size**' and '**last_new_job**' and plot them out using pie charts."},{"metadata":{},"cell_type":"markdown","source":"As seen below, we visualise 'experience' using a pie chart, which tells us how many years the candidate has been working. More than 17% of the people have been working for more than 20 years, 7.46% have worked for 5 years and 7.32% have had 4 years."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\ncount = Counter(X['experience'])\nplt.pie(count.values(), labels=count.keys(), labeldistance=0.75, autopct=lambda p:f'{p:.2f}%',\n       explode=[0]*14+[0.12]+[0.06]+[0]*7, shadow=True, rotatelabels=0.75)\nplt.title('Work experience (in years)', fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 'company_size' tells us the range of how many employees were in the companies. In this column, a lot of our samples are missing (almost two fifths!), as indicated by the 'nan'. Though, the largest group out of the recorded samples is 50-99, followed by 100-500 (13.42%) and 10,000+ (10.54%)."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\ncount = Counter(X['company_size'])\nplt.pie(count.values(), labels=count.keys(), labeldistance=0.75, autopct=lambda p:f'{p:.2f}%',\n       shadow=True, explode=[0.075]+[0]*8, rotatelabels=0.75)\nplt.title('Company size (number of people)', fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final feature which we will visualise using a pie chart is the 'last_new_job'. This tells us how many years have passed between the last and the current job. Over 40% of people have had one year between their previous and current job, then 17.17% have had a gap of more than four years and for 15.14% of the candidates, two years have passed."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10, 10))\ncount = Counter(X['last_new_job'])\nplt.pie(count.values(), labels=count.keys(), labeldistance=0.75, autopct=lambda p:f'{p:.2f}%',\n       explode=[0.05]+[0]*6, shadow=True)\nplt.title('Number of years between last and current job', fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='purple' size=6 style=\"font-family:garamond;\">Bar graphs</font>"},{"metadata":{},"cell_type":"markdown","source":"Subsequently, we now take a look at six different columns: '**gender**', '**relevent_experience**', '**enrolled_university**', '**education_level**', '**major_discipline**', '**company_type**'."},{"metadata":{},"cell_type":"markdown","source":"For the **gender** column, we can see that there are significantly more men than any other gender. All of the non-male samples combined make up of less than one half of the amount of males in the dataset."},{"metadata":{},"cell_type":"markdown","source":"The amount of people who have had **relevent experience** is more than twice the amount who have not."},{"metadata":{},"cell_type":"markdown","source":"The vast majority of people (13817 candidates) have not **enrolled** to any university, followed by 3757 candidates at a full time course and 1198 at a part time course."},{"metadata":{},"cell_type":"markdown","source":"As for the **education level**, 60% are graduates, 23% have a master's and 11% only finished high school."},{"metadata":{},"cell_type":"markdown","source":"In the **major discipline**, a substantial amount of people are involved in STEM, as only 24% are involved in any other discipline and 76% are in STEM."},{"metadata":{},"cell_type":"markdown","source":"More than half of candidates (51%) were involved in a Pvt Ltd **company type**, while 32% of the samples are missing and 5% were in a funded startup."},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [['gender', 'relevent_experience'], ['enrolled_university', 'education_level'], \n ['major_discipline', 'company_type']]\ncolours = ['red', 'green', 'blue', 'purple', 'orange', 'skyblue']\n\nfig, axes = plt.subplots(3, 2, figsize=(15, 17))\nfig.tight_layout(h_pad=3)\n\nfor i in names:\n    for name, ax in zip(i, axes[names.index(i)]):\n        col = X[name].fillna('NaN')\n        count = Counter(col)\n        count = pd.Series(count).sort_values(ascending=False)\n        bars = ax.bar(count.keys(), count, color=colours[list(axes.flatten()).index(ax)])\n        for bar in bars:\n            label = count[list(bars).index(bar)]\n            ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(), label, ha='center', \n                     va='bottom', fontsize=15)  \n        ax.set_title(name, fontsize=15)\n        ax.xaxis.label.set_size(50)\n        plt.xticks(rotation='vertical')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='darkblue' size=7 style=\"font-family:garamond;\">Data Transformation</font>"},{"metadata":{},"cell_type":"markdown","source":"Now we move onto transforming our data in certain ways so that it can be inputted into a model for predictions."},{"metadata":{},"cell_type":"markdown","source":"<font color='purple' size=6 style=\"font-family:garamond;\">Missing data</font>"},{"metadata":{},"cell_type":"markdown","source":"We start off by replacing the null values in 'experience', 'company_size' and 'last_new_job' with -1's. This is because these columns are ordinal (ordered) and need to have a replacement for any missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X['experience'] = X['experience'].fillna('-1')\nX['company_size'] = X['company_size'].fillna('-1')\nX['last_new_job'] = X['last_new_job'].fillna('-1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='purple' size=6 style=\"font-family:garamond;\">Categorical to numerical</font>"},{"metadata":{},"cell_type":"markdown","source":"Furthermore, we will transform our categorical columns to numerical ones. We use a LabelEncoder for the ordinal (ordered) features and a get_dummies/One Hot Encoder for the nominal (unordered) features."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = ['gender', 'relevent_experience', 'enrolled_university', 'education_level', \n            'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job']\nordinal = ['experience', 'company_size', 'last_new_job']\n\nfor col in cat_cols:\n    if col in ordinal:\n        le = LabelEncoder()\n        X[col] = le.fit_transform(X[col])\n    else:\n        dummies = pd.get_dummies(X[col])\n        for d_col in dummies:\n            X[col+' '+d_col] = dummies[d_col]\n        X = X.drop(col, axis=1)\n                           \nX['city'] = [int(city[5:]) for city in X['city']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='purple' size=6 style=\"font-family:garamond;\">Log and Box Cox</font>"},{"metadata":{},"cell_type":"markdown","source":"Afterwards, we plot the distribution of the 'city', 'city_development_index', 'experience', 'company_size', 'last_new_job','training_hours' using histograms and box plots. Also, we check how they fare when transformed using a log transform and a box cox."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in X.columns[:6]:\n    normal = X[col]\n    transforms = [[normal, 'Normal', 'lightblue'], \n                  [(normal+1).transform(np.log), 'Log Transform', 'lightgreen'], \n                  [boxcox(normal+1)[0], 'Box Cox', 'pink']]\n    fig, axes = plt.subplots(2, len(transforms), figsize=(20, 12))\n    \n    for ax in axes[0]:\n        transform = transforms[list(axes[0]).index(ax)]\n        pd.DataFrame(transform[0]).hist(ax=ax, color=transform[2])\n        ax.set_title('')\n        ax.set_xlabel(transform[1], fontsize=15)\n        \n        deciles = pd.Series(transform[0]).quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9])\n        for pos in np.array(deciles).reshape(1, -1)[0]:\n            handle = ax.axvline(pos, color='darkblue', linewidth=1)\n        ax.legend([handle], ['deciles'])\n        \n    for ax in axes[1]:\n        transform = transforms[list(axes[1]).index(ax)]\n        ax.boxplot(transform[0])\n        ax.set_title('')\n        ax.set_xlabel(transform[1], fontsize=15)\n        \n    axes[0][int(np.floor(len(transforms)/2))].set_title(col, pad=25, fontsize=30)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our conclusions are that the 'city' is best left alone, the 'city_development_index', 'experience' and 'training_hours' features should be transformed using box cox and the 'company_size' and 'last_new_job' work best with a log transform."},{"metadata":{"trusted":true},"cell_type":"code","source":"X['city'] = X['city']\nX['city_development_index'] = boxcox(X['city_development_index']+1)[0]\nX['experience'] = boxcox(X['experience']+1)[0]\nX['company_size'] = (X['company_size']+1).transform(np.log)\nX['last_new_job'] = (X['last_new_job']+1).transform(np.log)\nX['training_hours'] = boxcox(X['training_hours']+1)[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='purple' size=6 style=\"font-family:garamond;\">Binning</font>"},{"metadata":{},"cell_type":"markdown","source":"Another useful technique to change the data is binning, which reduces the amount of unique classes in the columns to a specified amount by grouping certain ranges together."},{"metadata":{},"cell_type":"markdown","source":"The variables we choose to bin are 'city' and 'training_hours'. The graph below shows us not just their compared distribution, but also how many different classes they have - 20, as shown by the maximum value on the x axis."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(13, 8))\nfor name in ['city', 'training_hours']:\n    col = X[name]\n    X[name] = np.digitize(col, np.arange(col.min(), col.max(), (col.max()-col.min())/20))\n    X[name].hist(alpha=0.65, legend=True)\n    plt.title('Binned Distribution', fontsize=20)\n        \nplt.show()\nX = X.drop(['city', 'training_hours'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='purple' size=6 style=\"font-family:garamond;\">Resampling</font>"},{"metadata":{},"cell_type":"markdown","source":"The distribution of y has a lot more samples in '0' than in '1'. Therefore, we will use the SMOTE class to resample our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"count = Counter(y)\nplt.bar(['1', '0'], count.values(), color='blue')\nplt.title('Distribution of y')\nplt.xlabel('Class')\nplt.ylabel('Number of samples')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smote = SMOTE()\nX, y = smote.fit_resample(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the number of samples per class in y are equal, so the predictor can have a higher accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"count = Counter(y)\nplt.bar(['1', '0'], count.values(), color='blue')\nplt.title('Distribution of y')\nplt.xlabel('Class')\nplt.ylabel('Number of samples')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='purple' size=6 style=\"font-family:garamond;\">Splitting sets and sampling</font>"},{"metadata":{},"cell_type":"markdown","source":"An extremely common practice among people who wish to predict data is to split the X and y into train and test sets, which is what we do in the cell below. The test set has 20% of data, while the train has 80%."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we use a Standard Scaler to scale our X train and test data so that it can be more useful to our classifier."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='purple' size=6 style=\"font-family:garamond;\">Dimensionality reduction</font>"},{"metadata":{},"cell_type":"markdown","source":"Another very useful piece of data cleaning that we will perform is dimensionality reduction, in which we remove useless features so that our model can have greater accuracy."},{"metadata":{},"cell_type":"markdown","source":"The correlation of the columns in our data is shown below using a heatmap. There don't seem to be any strong connections between them that jump out to us, which is a good sign because if they were correlated then that would mean that the probability of us having a useless feature would be greater."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(X.corr())\nplt.title('Correlation of features', fontsize=15, pad=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the next cell, we use a PCA to determine how much each feature contributes to our dataset. The first column has 12% of explained variance ratio, and we can also see that 25 other variables also have some contribution."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=29).fit(X_train)\nevr = pca.explained_variance_ratio_\nplt.bar(range(len(evr)), evr)\nplt.title('Explained variance ratio for features')\nplt.xlabel('Features')\nplt.ylabel('Explained variance ratio')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use a PCA to pull out the best 26 components and apply it to our X train and test."},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=26)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='darkblue' size=7 style=\"font-family:garamond;\">Creating a classifier</font>"},{"metadata":{},"cell_type":"markdown","source":"The final step is to create a machine learning classifier which will accurately predict our data."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"http://www.rgitaa.com/wp-content/uploads/2018/12/2.png\" width=\"500\">"},{"metadata":{},"cell_type":"markdown","source":"Here, we test four predictors: XGBoost, Random Forest, Linear SVC and SGD Classifier. We use the model, cross validation and ROC AUC scores in order to evaluate which model performs the best."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [['XGBoost', XGBClassifier()], ['Random Forest', RandomForestClassifier()], \n               ['Linear SVC', LinearSVC(dual=False)], ['SGD', SGDClassifier()]]\nscores = []\ncross_vals = []\nroc_aucs = []\n\nfor classifier in classifiers:\n    name = classifier[0]\n    model = classifier[1]\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    score = model.score(X_test, y_test)\n    cross_val = cross_val_score(model, X_test, y_test).mean()\n    roc_auc = roc_auc_score(y_test, y_pred)\n    \n    scores.append(score)\n    cross_vals.append(cross_val)\n    roc_aucs.append(roc_auc)\n    \n    print(name)\n    print('model score:    ', score)\n    print('cross val score:', cross_val)\n    print('ROC AUC score:  ', roc_auc)\n    if classifier != classifiers[-1]:\n        print('')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that the ensemble algorithms: XGBoost and Random Forest perform the best on our dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(18, 7))\nmetrics = [scores, cross_vals, roc_aucs]\nmetric_names = ['model score', 'cross validation score', 'ROC AUC score']\nnames = ['XGBoost', 'Random Forest', 'Linear SVC', 'SGD']\ncolours = ['red', 'lightgreen', 'blue']\n\nfor metric in metrics:\n    index = metrics.index(metric)\n    ax = axes.flatten()[index]\n    bars = ax.bar(names, metric, color=colours[index])\n    for bar in bars:\n        label = str(metric[list(bars).index(bar)])[:4]\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), label, ha='center', \n                va='bottom', fontsize=16)\n    ax.set_title(metric_names[index], fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color='darkblue' size=5 style=\"font-family:georgia\">Thank you for reading my notebook.</font>"},{"metadata":{},"cell_type":"markdown","source":"<font color='darkblue' size=5 style=\"font-family:georgia;\">If you enjoyed this notebook and found it helpful, please upvote it and give feedback as it would help me make more of these.</font>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}