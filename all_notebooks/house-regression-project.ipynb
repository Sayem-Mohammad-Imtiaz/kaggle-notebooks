{"cells":[{"metadata":{},"cell_type":"markdown","source":"# King County House Price Regression Project"},{"metadata":{},"cell_type":"markdown","source":"Coded by Luna McBride"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split #Split the data into train and test\nfrom sklearn.linear_model import LinearRegression #Add in our linear regression\nfrom sklearn.preprocessing import StandardScaler #Test out scaling\nfrom sklearn.neural_network import MLPRegressor #Add a multilayer perceptron to test regression ability\nfrom sklearn import svm #Add a support vector machine to test regression ability\nfrom sklearn.tree import DecisionTreeRegressor #Add a single tree regressor to test regression ability\nfrom sklearn.ensemble import RandomForestRegressor #Add a forest regressor to test regression ability\nfrom sklearn.ensemble import ExtraTreesRegressor #Add even more trees to test regression ability\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"house = pd.read_csv(\"../input/kc-housesales-data/kc_house_data.csv\") #Read in the houses dataset\nhouse.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Check for nulls"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(house.count())\nprint(house.isnull().any())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values. It is time to fix it up some more."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Fix up some variables"},{"metadata":{},"cell_type":"markdown","source":"## Change dates into an age"},{"metadata":{"trusted":true},"cell_type":"code","source":"house[\"age\"] = pd.DatetimeIndex(house['date']).year - house[\"yr_built\"] #Get the age of the building\nprint(house['date'][1], \" - \", house[\"yr_built\"][1], \" = \", house[\"age\"][1]) #Print the equation as a sanity check","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Remove year renovated"},{"metadata":{"trusted":true},"cell_type":"code","source":"house.drop(columns = [\"yr_renovated\"], inplace = True) #Drop the year renovated field\nhouse.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop ID and Date, as they are not helpful here"},{"metadata":{"trusted":true},"cell_type":"code","source":"house.drop(columns = [\"id\", \"date\"], inplace = True) #Drop the ID and Date Fields\nhouse.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Drop the first living and lot columns"},{"metadata":{},"cell_type":"markdown","source":"More updated versions exist in the sqft_living/lot 15 variables, and thus they are better representation of the current lot."},{"metadata":{"trusted":true},"cell_type":"code","source":"house.drop(columns = [\"sqft_living\", \"sqft_lot\"], inplace = True) #Drop the old sqft lot and living Fields\nhouse.head() #Take a peek at the dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Split the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"price = np.array(house[\"price\"].copy().astype(int)) #Set price to be the prices of the houses\nprice = np.log(price) #Log the price\nprint(price) #Print the prices","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I tried various scaling functions to try to get this to increase the the linear regression score, but I searched the internet for more options and, strangely, just taking the log worked a whole lot better."},{"metadata":{"trusted":true},"cell_type":"code","source":"characteristics = house.drop(\"price\", axis = 1) #Get every other feature of our dataframe except price\nchara = pd.get_dummies(characteristics) #Get the dummies for easier model training\nscale = StandardScaler() #Add a standard scaler to scale our data for easier use later\nscale.fit(chara) #Fit the scaler with our characteristics\nchara = scale.transform(chara) #Transform the data with our scaler\n\nprint(chara) #Print the scaled data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"charaTrain, charaTest, priceTrain, priceTest = train_test_split(chara, price, test_size = 0.3) #Split the data into train and test\nprint(priceTest) #Print one of the price splits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Train the linear regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"regression = LinearRegression() #Open a linear regression model\nregression.fit(charaTrain, priceTrain) #Fit the regression model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(regression.score(charaTest, priceTest)) #Print the accuracy of the model\nprint(regression.coef_) #Print the model coefficients","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Test other regressions"},{"metadata":{"trusted":true},"cell_type":"code","source":"neural = MLPRegressor(hidden_layer_sizes = (3,100), random_state=1, max_iter=500) #Build a neural network to test regression\nneural.fit(charaTrain, priceTrain) #Fit the network with the train set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr =  svm.SVR() #Get a support vector regressor to test ability\nsvr.fit(charaTrain, priceTrain) #Fit the regresso","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeRegressor() #Build a tree\ntree.fit(charaTrain, priceTrain) #Fit the tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forest = RandomForestRegressor() #Build a whole forest of trees\nforest.fit(charaTrain, priceTrain) #Fit the forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"forestBig = ExtraTreesRegressor() #Build a more random forest\nforestBig.fit(charaTrain, priceTrain) #Fit the more random forest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{},"cell_type":"markdown","source":"# Print Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Print the accuracies of all the models\nprint(\"Linear Regression Accuracy: \", regression.score(charaTest, priceTest))\nprint(\"Neural Network Accuracy: \", neural.score(charaTest, priceTest))\nprint(\"Support Vector Accuracy: \", svr.score(charaTest, priceTest))\nprint(\"Single Tree Accuracy: \", tree.score(charaTest, priceTest))\nprint(\"Random Forest Accuracy: \",forest.score(charaTest, priceTest))\nprint(\"Even more Random Forest Accuracy: \", forestBig.score(charaTest, priceTest))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to the scores, the strongest regressors for this data were the forest, reaching above 88% accuracy. These were followed close behind by the neural network and SVM regressions, which were both above 84% accuracy. Trailing far at the end were the Linear Regression and the single tree, surprisingly. There was a difference of about 7% accuracy between the Linear Regression and its next closest neighbor, the SVM (77 vs 84). This shows that while the linear regression is the typical go-to in settings like statistics, there are better options out there depending on the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes = characteristics.columns #Get the tested attributes\nattributes = list(zip(attributes, regression.coef_)) #Zip the attributes together with their coefficient\nsortAtt = sorted(attributes, key = lambda x: x[1], reverse = True) #Sort the zipped attributes by their coefficients\n\nprint(\"According to the Linear Regression, the most important factors for pricing are: \") #Start printing the most important labels\ni=0 #Counter variable so only the top five are printed\n\n#For each attribute in the sorted attributes\nfor label, coef in sortAtt:\n    if i<5: #If there has not been five printed yet\n        print(label) #Print the label as an important factor\n    i += 1 #Increase i by 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"attributes = characteristics.columns #Get the tested attributes\nattributes = list(zip(attributes, forest.feature_importances_)) #Zip the attributes together with their coefficient\nsortAtt = sorted(attributes, key = lambda x: x[1], reverse = True) #Sort the zipped attributes by their coefficients\n\nprint(\"According to the Random Forest (most accurate), the most important factors for pricing are: \") #Start printing the most important labels\ni=0 #Counter variable so only the top five are printed\n\n#For each attribute in the sorted attributes\nfor label, coef in sortAtt:\n    if i<5: #If there has not been five printed yet\n        print(label) #Print the label as an important factor\n    i += 1 #Increase i by 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the listings of most important features contributing to the price, the Linear Regression and far more accurate Random Forest gave widely different results. Both models produced grade as one of the most important. Grade is how King County grades houses, so it makes sense houses in King County will rely on their grade to price houses. They both also have latitude and size (not counting the basement), but the utilization in the list is very different.\n\nThe Forest ranked location (latitude and longitude) and above-ground sizing (the size of the house and the size not counting the basement) as the most important metrics. This makes sense, as different areas tend to have higher or lower prices and size does tend to be an important factor in considering a house.\n\nThe Linear regression ranked the age a whole lot higher than anything else, which is a bit odd, considering it was not even on the Forest's list. There is also latitude and size above ground, which tells me the Linear regression was trying to come to a similar conclusion about space and location that the forest did, it just put higher emphasis on age. Price emphasis by age can make sense, but I feel like that only matters highly with the very old houses. This intuition is also shown in the forest model, as I had to change the iterator to 9 in order to even show age, thus it is ranking it as the 9th most important factor."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}