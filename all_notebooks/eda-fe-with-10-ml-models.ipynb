{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Used Car Data Price Prediction\n\nBuild of the 10+ most popular ML regression models to predict the price of used Car in Indian Market.\n\nLet's design the flow of computation:\n1. Supervised, Unsupervised, Reinforcement Learning? Ans. Supervised because provided with labeled data.\n2. Classification, Regression, something else? Ans. Univariate Multiple Regression because we need to predict a single variable, selling price on the basis of multiple features.\n3. Batch learning or online learning techniques? Ans. Plain Batch learning as there is no continous inflow of data apart from provided once.\n4. Performance Measure? Ans. Root Mean Square Error(RMSE), a typical performance measure for regression problems. Will also consider, using Mean Absolute Error(MAE).\n5. Hypothesis: Selling price of car will be more if less driven, of premium brand, sold by first owner, of latest year with automatic transmission."},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n\n## Table of Contents\n\n1. [Import libraries & dataset](#1)\n1. [EDA](#2)\n1. [Preparing to modeling](#3)\n1. [ML models](#4)\n    -  [Linear Regression](#4.1)\n    -  [Ridge Regression](#4.2)\n    -  [K Neighbors Regressor](#4.3)\n    -  [SVR](#4.4)\n    -  [Stochastic Gradient Descent](#4.5)\n    -  [Decision Tree Regressor](#4.6)\n    -  [Random Forest](#4.7)\n    -  [Gradient Boosting](#4.8)\n    -  [XG Boost](#4.9)\n    -  [ExtraTreesRegressor](#4.10)\n    -  [VotingRegressor](#4.11)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Import libraries & dataset <a class=\"anchor\" id=\"1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"car_details = pd.read_csv('../input/vehicle-dataset-from-cardekho/CAR DETAILS FROM CAR DEKHO.csv')\ncar_details.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, we have details of car with second hand selling price for a particular year"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the count of rows for each column\ncar_details.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. EDA <a class=\"anchor\" id=\"2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# A good library to get basic EDA done\nimport seaborn as sns\nprint(car_details.describe())\n\nsns.distplot(car_details.selling_price, color = 'blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(car_details.iloc[:,1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(car_details)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This gives a sense of correlation among variables with integer values. As, price of car is maximum with less driven and sold in latest year"},{"metadata":{"trusted":true},"cell_type":"code","source":"# car_details[\"is_duplicate\"]= car_details.duplicated()\n# car_details[\"is_duplicate\"].value_counts()\n#car_details.drop(car_details[car_details['is_duplicate'] == True].index, inplace = True)\n#By default, for each set of duplicated values, the first occurrence is set on False and all others on True.\n#value_counts is a Series method rather than a DataFrame method\n#car_details[(car_details.name == 'Hyundai Verna SX') & (car_details.year == 2007)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are duplicate rows and let's drop them out"},{"metadata":{},"cell_type":"markdown","source":"The or and and python statements require truth-values. For pandas these are considered ambiguous so you should use \"bitwise\" | (or) or & (and) operations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"car_details.drop_duplicates(inplace = True)\ncar_details1 = car_details.reset_index()\nlen(car_details)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car_details1.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Categorical Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_variables = car_details1.select_dtypes(exclude=[\"number\"]).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical plots in the shape of Violin\nfor i in categorical_variables:\n    if(len(car_details1[i].unique())<10 and len(car_details1[i].unique())>0):\n        sns.catplot(x=\"selling_price\", y=i,kind=\"violin\", split=True, data=car_details1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pair plot for integer variables\nsns.pairplot(car_details1.iloc[:,2:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car_details1['name'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to many car models with less data associated. I will consider cars on the basis of their Company & car name."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Taking company name and parent model name, not focused on exact model type\nfor i in range(len(car_details1)):\n    car_details1.loc[i,'name_model'] = ' '.join(car_details1.loc[i,'name'].split()[:2]) #split and join the string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car_details1.name_model.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop models whose count is less than 5\ncounts = car_details1['name_model'].value_counts()\n\ncar_details1 = car_details1[~car_details1['name_model'].isin(counts[counts < 5].index)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_variables\n# We will not encode the name variable. As it will result to more 188 columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#one-hot encoding, adding dummy-variables\ncar_details2 = pd.get_dummies(car_details1, columns=categorical_variables[1:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car_details2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now, drop the columns not required for further analysis\ncar_details2.drop(columns = ['index','name'], inplace = True)\ncar_details2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label encoding of model names\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(list(car_details2.name_model))\ncar_details2.name_model = le.transform(list(car_details2.name_model))\ncar_details2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"car_details2.info()\n#Now all the columns are in numeric format","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's figure out the heat map among the features\nplt.figure(figsize=(24,24)) # For a appropriate size\n\nsns.heatmap(car_details2.corr(),annot=True,cmap='summer') #Annotation = enables value of each box visible","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.pairplot(car_details2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For models from Sklearn\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain = pd.DataFrame(scaler.fit_transform(car_details2), columns = car_details2.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Preparing to modeling <a class=\"anchor\" id=\"3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train.selling_price\nfeatures = train.drop(columns = ['selling_price'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, target, stratify = features.name_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Linear Regression <a class=\"anchor\" id=\"4.1\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Regression\nfrom sklearn.linear_model import LinearRegression\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)\nlinreg.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nnp.mean(cross_val_score(LinearRegression(), X_train, y_train, cv=10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Ridge Regression <a class=\"anchor\" id=\"4.2\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'alpha': np.logspace(-3, 3, 13)}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = GridSearchCV(Ridge(), param_grid, cv=10, return_train_score=True, iid=False)\ngrid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(cross_val_score(Ridge(), X_train, y_train, cv=10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 K Neighbors Regressor <a class=\"anchor\" id=\"4.3\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nneighbors = range(1, 30, 2)\n\ntraining_scores = []\ntest_scores = []\nfor n_neighbors in neighbors:\n    knn = KNeighborsRegressor(n_neighbors=n_neighbors).fit(X_train, y_train)\n    training_scores.append(knn.score(X_train, y_train))\n    test_scores.append(knn.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.plot(neighbors, training_scores, label=\"training scores\")\nplt.plot(neighbors, test_scores, label=\"test scores\")\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsRegressor(n_neighbors=7)\nscore = cross_val_score(knn, X_train, y_train, cv=10)\nprint(f\"best cross-validation score: {np.max(score):.3}\")\n\nknn.fit(X_train, y_train)\nprint(f\"test-set score: {knn.score(X_test, y_test):.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.4 SVR <a class=\"anchor\" id=\"4.4\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\nsvr = SVR()\nsvr.fit(X_train, y_train)\nprint(f\"test-set score: {svr.score(X_test, y_test):.3f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr1 = SVR(kernel='poly')\nsvr1.fit(X_train, y_train)\nprint(f\"test-set score: {svr1.score(X_test, y_test):.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5 Stochastic Gradient Descent <a class=\"anchor\" id=\"4.5\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\nsgd = SGDRegressor()\nsgd.fit(X_train, y_train)\nprint(f\"test-set score: {sgd.score(X_test, y_test):.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.6 Decision Tree Regressor <a class=\"anchor\" id=\"4.6\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\ndtr.fit(X_train, y_train)\nprint(f\"test-set score: {dtr.score(X_test, y_test):.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.7 Random Forest <a class=\"anchor\" id=\"4.7\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)\nprint(f\"test-set score: {rfr.score(X_test, y_test):.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.8 Gradient Boosting <a class=\"anchor\" id=\"4.8\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor()\ngbr.fit(X_train, y_train)\nprint(f\"test-set score: {gbr.score(X_test, y_test):.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.9 XG Boost <a class=\"anchor\" id=\"4.9\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb = XGBRegressor()\nxgb.fit(X_train,y_train)\nprint(f\"test-set score: {xgb.score(X_test, y_test):.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.10 Extra Tree Regressor <a class=\"anchor\" id=\"5.7\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\netr = ExtraTreesRegressor()\netr.fit(X_train, y_train)\nprint(f\"test-set score: {etr.score(X_test, y_test):.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.11 Voting Regressor <a class=\"anchor\" id=\"4.11\"></a>\n\n[Back to Table of Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nvr = VotingRegressor(estimators=[('rfr', rfr), ('gbr', gbr), ('xgb', xgb)])\nvr.fit(X_train,y_train)\nprint(f\"test-set score: {vr.score(X_test, y_test):.3f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nfor clf in (rfr, gbr, xgb, vr):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__,  mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}