{"cells":[{"metadata":{},"cell_type":"markdown","source":"# KnightHack 4 ~ Climate Change Tweets Sentiment Analysis\n## A Survey of Different Models to do Sentiment Analysis\n\nby: [John Muchovej](john.muchovej.com)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom pathlib import Path\ndata = {}\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        data[filename] = Path(dirname) / filename\n\n# Any results you write to the current directory are saved as output.\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from IPython.display import (\n    Markdown as md,\n    Latex,\n    HTML,\n)\nfrom tqdm.auto import tqdm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Cursory Analysis\n\nWe'll start out by loading up the Twitter Sentiment Data and doing a bit of exploration to get a feel for what's going on with the data."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"tweets = pd.read_csv(data[\"twitter_sentiment_data.csv\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"display(tweets.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`pd.DataFrame.shape` returns a tuple of (# rows, # columns, ...). This tells us that we have ~44K Tweets (or rows) and each Tweet has 3 features (or columns)."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"value_counts = tweets[\"sentiment\"].value_counts()\nvalue_counts.name = \"Raw Number\"\n\nvalue_normd = tweets[\"sentiment\"].value_counts(normalize=True)\nvalue_normd.name = \"Percentage\"\n\ndisplay(pd.concat([value_counts, value_normd], axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`pd.DataFrame[\"column\"].value_counts` returns an enumeration over all the unique values and how many times that value appears in the `pd.DataFrame`."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"display(tweets.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`pd.DataFrame.head` gives us the first 5 (by default) rows of the `tweets` DataFrame. This gives us a preview of the kinds of data we have in `tweets`."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis)\n\n**Before we pick up on our analysis, let's make a copy of the `pd.DataFrame` so we can feed `tweets` into our models later.**\n\nWe're going to start an Exploratory Data Analysis **(EDA)**. The first step of any Machine Learning project is to develop an understanding of your data, as that will help with model selection later on."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from copy import deepcopy\neda = deepcopy(tweets)\n# tqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First up, I have a strong aversion to keeping track of numeric keys. So let's replace all numeric values with the appropriate **labels**, given by the dataset."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sentiment_num2name = {\n    -1: \"Anti\",\n     0: \"Neutral\",\n     1: \"Pro\",\n     2: \"News\",\n}\neda[\"sentiment\"] = eda[\"sentiment\"].apply(lambda num: sentiment_num2name[num])\neda.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom matplotlib import style\n\nimport seaborn as sns\n\nsns.set(font_scale=1.5)\nstyle.use(\"seaborn-poster\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(20, 10), dpi=100)\n\nsns.countplot(eda[\"sentiment\"], ax=axes[0])\nlabels = list(sentiment_num2name.values())\n\naxes[1].pie(eda[\"sentiment\"].value_counts(),\n            labels=labels,\n            autopct=\"%1.0f%%\",\n            startangle=90,\n            explode=tuple([0.1] * len(labels)))\n\nfig.suptitle(\"Distribution of Tweets\", fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, since Twitter uses Hashtags almost like a summarization feature (at least in the sense of highlighting core ideas). So let's look at some of top hashtags for each of the classes of `sentiment`. We'll then make \"word clouds\" to visualize their prominence."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import re\nimport nltk\nimport itertools","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"top15 = {}\n\nby_sentiment = eda.groupby(\"sentiment\")\nfor sentiment, group in tqdm(by_sentiment):\n    hashtags = group[\"message\"].apply(lambda tweet: re.findall(r\"#(\\w+)\", tweet))\n    hashtags = itertools.chain(*hashtags)\n    hashtags = [ht.lower() for ht in hashtags]\n    \n    frequency = nltk.FreqDist(hashtags)\n    \n    df_hashtags = pd.DataFrame({\n        \"hashtags\": list(frequency.keys()),\n        \"counts\": list(frequency.values()),\n    })\n    top15_htags = df_hashtags.nlargest(15, columns=[\"counts\"])\n    \n    top15[sentiment] = top15_htags.reset_index(drop=True)\n\ndisplay(pd.concat(top15, axis=1).head(n=10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(28, 20))\ncounter = 0\n\nfor sentiment, top in top15.items():\n    sns.barplot(data=top, y=\"hashtags\", x=\"counts\", palette=\"Blues_d\", ax=axes[counter // 2, counter % 2])\n    axes[counter // 2, counter % 2].set_title(f\"Most frequent Hashtags by {sentiment} (Visually)\", fontsize=25)\n    counter += 1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n- The most popular hashtags are, broadly, **climate** and **climatechange**. Which is expected, given the topic; but also, among the top 3 are relating to **trump** and his campaign slogan **maga**.\n- The **BeforeTheFlood** hashtag refers to a 2016 documentary where Leonardo DiCaprio met with scientists, activists, and word leaders to discuss the dangers of climate and and possible solutions.\n- **COP22**, **ParisAgreement**, and **Trump** in the **Pro** `sentiment` are likely related to the formal process Trump's administrastion began to exit the Paris Agreements, where north of 200 nations pledged to reduce greenhour gas emissions, assist developing nations, and assist [poor] nations struggling with the consequences of a warming Earth.\n- Interestingly, **auspol** (short for Australian Politics) made the shortlist of the **Pro** `sentiment`. This is likeyl attributed to an assessment published quantifying the role of climate change in Australian brushfires and their increaseed risk of occuring."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def cleaner(tweet):\n    tweet = tweet.lower()\n    \n    to_del = [\n        r\"@[\\w]*\",  # strip account mentions\n        r\"http(s?):\\/\\/.*\\/\\w*\",  # strip URLs\n        r\"#\\w*\",  # strip hashtags\n        r\"\\d+\",  # delete numeric values\n        r\"U+FFFD\",  # remove the \"character note present\" diamond\n    ]\n    for key in to_del:\n        tweet = re.sub(key, \"\", tweet)\n    \n    # strip punctuation and special characters\n    tweet = re.sub(r\"[,.;':@#?!\\&/$]+\\ *\", \" \", tweet)\n    # strip excess white-space\n    tweet = re.sub(r\"\\s\\s+\", \" \", tweet)\n    \n    return tweet.lstrip(\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"eda[\"message\"] = eda[\"message\"].apply(cleaner)\neda.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords, wordnet  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def lemmatizer(df):\n    df[\"length\"] = df[\"message\"].str.len()\n    df[\"tokenized\"] = df[\"message\"].apply(word_tokenize)\n    df[\"parts-of-speech\"] = df[\"tokenized\"].apply(nltk.tag.pos_tag)\n    \n    def str2wordnet(tag):\n        conversion = {\"J\": wordnet.ADJ, \"V\": wordnet.VERB, \"N\": wordnet.NOUN, \"R\": wordnet.ADV}\n        try:\n            return conversion[tag[0].upper()]\n        except KeyError:\n            return wordnet.NOUN\n    \n    wnl = WordNetLemmatizer()\n    df[\"parts-of-speech\"] = df[\"parts-of-speech\"].apply(\n        lambda tokens: [(word, str2wordnet(tag)) for word, tag in tokens]\n    )\n    df[\"lemmatized\"] = df[\"parts-of-speech\"].apply(\n        lambda tokens: [wnl.lemmatize(word, tag) for word, tag in tokens]\n    )\n    df[\"lemmatized\"] = df[\"lemmatized\"].apply(lambda tokens: \" \".join(map(str, tokens)))\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"eda = lemmatizer(eda)\neda.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 15))\nsns.boxplot(x=\"sentiment\", y=\"length\", data=eda, palette=(\"Blues_d\"))\nplt.title(\"Tweet Length Distribution for each Sentiment\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Buzzwords\n\nBelow, we'll compute the frequency of words for each `sentiment`. Following that, we'll build `WordCloud`s to visualize these words.\n\n`WordCloud`s convey importance through opacity, so the more translucent a word, the less frequently it appears."},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"frequency = {}\n\nby_sentiment = eda.groupby(\"sentiment\")\nfor sentiment, group in tqdm(by_sentiment):\n    cv = CountVectorizer(stop_words=\"english\")\n    words = cv.fit_transform(group[\"lemmatized\"])\n    \n    n_words = words.sum(axis=0)\n    word_freq = [(word, n_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n    word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\n    \n    freq = pd.DataFrame(word_freq, columns=[\"word\", \"freq\"])\n    \n    frequency[sentiment] = freq.head(n=25)\n\nto_view = pd.concat(frequency, axis=1).head(n=25)\ndisplay(to_view)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we've computing the frequency, let's generate and plot the WordClouds for each `sentiment`."},{"metadata":{"trusted":true},"cell_type":"code","source":"words = {sentiment: \" \".join(frequency[sentiment][\"word\"].values) for sentiment in sentiment_num2name.values()}\n\ncmaps = {\n    \"Anti\": (\"Reds\", 110),\n    \"Pro\" : (\"Greens\", 73),\n    \"News\": (\"Blues\", 0),\n    \"Neutral\": (\"Oranges\", 10),\n}\n\nfrom wordcloud import WordCloud\n\nwordclouds = {}\nfor sentiment, (cmap, rand) in tqdm(cmaps.items()):\n    wordclouds[sentiment] = WordCloud(\n        width=800, height=500, random_state=rand,\n        max_font_size=110, background_color=\"white\",\n        colormap=cmap\n    ).generate(words[sentiment])\n    \nfig, axes = plt.subplots(2, 2, figsize=(28, 20))\ncounter = 0\n\nfor sentiment, wordcloud in wordclouds.items():\n    axes[counter // 2, counter % 2].imshow(wordcloud)\n    axes[counter // 2, counter % 2].set_title(sentiment, fontsize=25)\n    counter += 1\n    \nfor ax in fig.axes:\n    plt.sca(ax)\n    plt.axis(\"off\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n- The top 3 buzzwords are **climate**, **change**, and **rt** (retweet). This seems to indicate that a lot of the same information is being shared/viewed – this applies across all `sentiments`. While we can't conclude that's a result of the \"filter bubble\", it certainly seems like that might be a latent (hidden) cause.\n- Interestingly, **trump** occurs across all cases. This may not be surprising given his presidency during the timeframe the Tweets were recorded – this is something that likely warrants further investigation especially along the axes of **Neutral** and **Pro**.\n- Words like **real**, **believe**, **think**, and **fight** occur quite frequently in the **Pro** `sentiment`. Interestingly, both the **Pro** and **Anti** sentiment seem to be saying **science** and **scientist**, which seems indicative that both sides believe their quoting accurate, reproduced, research. \n- Take a look at the table above, you'll see the **http** actually shows up in the **Pro** `sentiment` quite frequently. This would imply that links are being shared alongside the Tweets quite frequently. Contrast that with the other `sentiment`s – particularly, **News**. Why might this be the case?"},{"metadata":{},"cell_type":"markdown","source":"## Some Crude Entity Extraction\n\nSo, this is an entire field of NLP – entity extraction. We're going to use `spacy`, a pretty great NLP library. We're to extract the following:\n- People\n- Geopolitical Regions\n- Organizations\n\nFor this particular dataset, we're looking to these factors as there's probably some causal relationship between them. Importantly, this might not tell us how these Tweeters would land on the spectrum of support, but it can tell us the most highly focused organizations, geopolitical regions, and influencers/people in advocating for/against \"Human-driven Climate Change\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nspacy_en = spacy.load('en')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def crude_entities(tweet):\n    as_words = tweet.apply(spacy_en)\n    \n    def by_label(words, label):\n        filtered = [word.text for word in words.ents if word.label_ == label]\n        return filtered\n    \n    def get_top(label, n=10):\n        thing = as_words.apply(lambda x: by_label(x, label))\n        flattened = itertools.chain(*thing.values)\n        \n        counter = Counter(flattened)\n        topN = counter.most_common(n)\n        \n        topN_things = [thing for thing, _ in topN]\n        \n        return thing\n    \n    entities = pd.DataFrame()\n    entities[\"people\"] = get_top(\"PERSON\", n=10)\n    entities[\"geopolitics\"] = get_top(\"GPE\", n=10)\n    entities[\"organizations\"] = get_top(\"ORG\")\n    \n    return entities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"entities = {}\n\nby_sentiment = eda.groupby(\"sentiment\")\n\nfor sentiment, group in tqdm(by_sentiment):\n    entities[sentiment] = crude_entities(group[\"lemmatized\"])\n    \ndisplay(pd.concat(entities, axis=1).head(n=10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling!\n\nTime to build our sentiment classifiers! We'll be \"vectorizing\" our text data before passing it through to our model. We need to vectorize our data for similar reasons to why we have ASCII and Unicode. Machines don't understand letters and words, but numeric values are their bread-and-butter.\n\nWe'll start out by looking at 5 models:\n- Random Forests\n- Naïve Bayes\n- K-Nearest Neighbors\n- Logistic Regression\n- Support Vector Machines (Linear SVC)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer \nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\n# Building classification models\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Model evaluation\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Your professors don't give you test answers, there's a reason\n\nAs with every Supervised Learning task, we need to split our data into (at least) Training and Validation sets. Typically, data will be given to you as a `Training` and `Testing` sets; but in our case, we have one massive CSV, so we need to make that split ourselves.\n\nThese splits allow us to train our model, but also give us the ability to test it's performance on data it _shouldn't have seen_. (This is a problem known as \"data leakage\" – try to avoid it.)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_all = tweets[\"message\"]\ny_all = tweets[\"sentiment\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.25, random_state=1337)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, random_state=1337)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**What is TFIDF?** Essentially, it assigns word frequency scores. These scores _try_ to highlight words of greater interest – you can get at this idea by looking at in-document frequency vs across-document frequency. The `TFIDFVectorizer` will tokenize the documents, learn the vocabulary and \"inverse document frequency wegihtings\", and allow you to encode new documents."},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer()\ntfidf.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following functions `train`, `grade`, and `train_and_grade` are helper functiosn to make life easier and practice DRY."},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(tfidf, model, train_data, train_labels, test_data):\n    model.fit(tfidf.transform(train_data), train_labels)\n    preds = model.predict(tfidf.transform(test_data))\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def grade(model, preds, test_labels):\n    print(metrics.classification_report(test_labels, preds))\n    \n    cm = confusion_matrix(test_labels, preds)\n    cm_normd = cm / cm.sum(axis=1).reshape(-1, 1)\n    \n    heatmap_kwargs = dict(\n        cmap=\"YlGnBu\",\n        xticklabels=model.classes_,\n        yticklabels=model.classes_,\n        vmin=0.,\n        vmax=1.,\n        annot=True,\n        annot_kws={\"size\": 10},\n    )\n    \n    sns.heatmap(cm_normd, **heatmap_kwargs)\n    \n    plt.title(f\"{model.__class__.__name__} Classification\")\n    plt.ylabel(\"Ground-truth labels\")\n    plt.xlabel(\"Predicted labels\")\n    plt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_and_grade(tfidf, model, train_data, train_labels, test_data, test_labels):\n    preds = train(tfidf, model, train_data, train_labels, test_data)\n    grade(model, preds, test_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forests\n\nRandom Forests are a tree-based Machine Learning algorithm that leverages the power of multiple Decision Trees. Decision trees work essentially like `if-elif-else` control flow, but the metric for each decision boundary is \"information gain\". The Forest component is pretty lackluster, you're taking a bunch of Decision Trees and \"planting them together\" to build a Forest.\n\nA visual representation of Random Forests:\n\n![](https://kevintshoemaker.github.io/NRES-746/rf.png)\n\nRetrieved from [here](https://kevintshoemaker.github.io/NRES-746)."},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(max_depth=5, n_estimators=100)\ntrain_and_grade(tfidf, rf, X_train, y_train, X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n- From the Confusion Matrix above, you can see that the model strictly predicts the **Pro** `sentiment`. This is likely due to the balance of data, but since we haven't tested that, we can't quite conclude that.\n- Looking at the Precision/Recall/F1 Score, for the **Anti**, **Neutral**, and **News** `sentiment`s, you'll see they're all 0.\n- Tree-based methods are prone to overfitting on imbalanced data, like what we have. However, we could potentially re-sample so the training data has a more uniform spread of each `sentiment` to test if that's truly the problem with our `RandomForestClassifier`.\n- Our overall F1 score is 0.52, which if you recall from our earlier visualizations, matches the %-age of **Pro** `sentiment` Tweets."},{"metadata":{},"cell_type":"markdown","source":"## Naïve Bayes\n\nNaïve Bayes leverages Bayes Theorem to make classifications. This assumes that independent variables are statistically independent from one another.\n\n$$P(A | B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n\nLet's break this down:\n- $P(A|B)$ is the posterior – our prediction of the likelihood of $A$, given we've observed $B$\n- $P(A)$ is the likelihood of $A$\n- $P(B|A)$ is the likelihood of $B$, given we've observed $A$\n- $P(B)$ is the likelihood of $B$\n\nSo, in summary, we're taking a known, $P(B|A)$ combining it with the liklihood that $A$ even happens, then we're \"re-normalizing it\" in terms of $B$.\n\n### Naïve Bayes' 3 Classification Methods\n- **Gaussian**: often used in classfication tasks and *assumes* a Normal Distribution (the \"bell curve\")\n- **Bernoulli**: a \"binomial\" model – this is useful if you have a binary classification (e.g. `True`/`False`)\n- **Multinomial**: used for discrete counts. In our case, instead of looking at \"is the word in the document\" (a Bernoulli view), we instead cound the frequency of the word in the document."},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB()\ntrain_and_grade(tfidf, nb, X_train, y_train, X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n- An improvement of Random Forests, but it still performs pretty poorly.\n- It still classifies most Tweets with the **Pro** `sentiment`.\n- Precision, Accuracy, and F1 Scores, though, have signifcantly improved across the other `sentiments`.\n- While Naïve Bayes performs better, it's performance is likely hampered by the balance of data we have."},{"metadata":{},"cell_type":"markdown","source":"## K-Nearest Neighbors\n\nKNN uses \"feature similarity\" to predict the values of new data points. Basically, it looks at the $K$ nearest points of the data point given, and computes a similarity between them.\n\nYou can compute the similarity with a variety of measures, e.g. Euclidean, Manhattan (good for Continuous), and Hamming (good for Categorial) distances.\n\n![](https://adrianromano.com/wp-content/uploads/2019/02/A-typical-example-of-a-KNN-classification-for-a-two-class-problem-ie-the-pink-and.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=2)\ntrain_and_grade(tfidf, knn, X_train, y_train, X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n- KNN improves over both Naïve Bayes and Random Forests.\n- It still leans **Pro** on classification, but you'll notice that it actually has greater diversity in classification, overall."},{"metadata":{},"cell_type":"markdown","source":"## [Multinomial] Logistic Regression (Classification)\n\nMultinomial Logistic Regression is a generalization of Logistic Regression, so that it can handle multiple classes. Typically Logistic Regression does well when you linearly separate the classes in question. Like Naïve Bayes and Random Forests, it's very sensitive to the class balance."},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg = LogisticRegression(C=1, class_weight=\"balanced\", max_iter=1000)\ntrain_and_grade(tfidf, logreg, X_train, y_train, X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Observations:\n\n- Logistic Regression does quite well, especially compared to the previous models.\n- The Precision, Recall, and F1-scores of all non-**Pro** classes is still trending upwards, which is good."},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machines (Linear SVC)\n\nWith SVMs we plot our data in $n$-dimensional space ($n$ is the number of features) so that each feature is created as a coordinate on an axis. The goal of SVMs to create the best decision boundary between all the features (this gets hard to visualize past $n=3$. This decision boundary is also called the hyperplane.\n\nSVM typically uses extreme points/vectors to help in creating the Hyperplane. These vectors are called \"Support Vectors\". Peep the image below for an idea of what's going on.\n\n![](https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_lsvc = LinearSVC(class_weight=\"balanced\")\ntrain_and_grade(tfidf, svm_lsvc, X_train, y_train, X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Observations:\n\n- SVM is able to quite successfully classify Tweets.\n- Based on the CM above, you can see there are pretty clear boundaries across all the `sentiments`.\n- Interestly, the SVM seems more confused about what should be classified as **Pro** than even Logistic Regression.\n- The trade-off of classifying **Pro** tweets, though, still leads to gains in properly classifying the majority of our data correctly."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}