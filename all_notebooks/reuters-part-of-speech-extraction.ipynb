{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Part-of-Speech\n\nFor various NLP tasks it is useful to extract information about the Part-of-Speech structure of the text. In this notebook we will compute some statistics about trigrams, like the probability of a Noun following a Verb and a Preposition."},{"metadata":{},"cell_type":"markdown","source":"## Data Processing\n\nFirst we will read data from [the Reuters dataset](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/XDB74W&version=2.0)."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\n\nreuters = pd.read_csv('../input/fake-news-data/reuters-newswire-2017.v5.csv')\ndata = reuters.drop(['publish_time'], axis=1).rename(columns={'headline_text': 'headline'})\nheadlines = data['headline']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then we are going to define the special start/end tokens."},{"metadata":{"trusted":true},"cell_type":"code","source":"START = '÷'\nEND = '■'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part-of-Speech Extraction"},{"metadata":{},"cell_type":"markdown","source":"We are going to import the required libraries, and define a SpaCy language model trained on web data."},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import defaultdict\nimport spacy, nltk\nnlp = spacy.load('en_core_web_sm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The information we extract will be based on trigrams of the text. Specifically, we will compute the Part-of-Speech, POS tags and Dependencies of each headline using SpaCy. We also need to include information about the start and end tokens. To accomplish this, we will append start and end tokens as prefixes and postfixes respectively to each parsed headline. Finally, we will extract trigrams on each headline for all three types of labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"trigrams_pos = defaultdict(lambda: defaultdict(int))\ntrigrams_tag = defaultdict(lambda: defaultdict(int))\ntrigrams_dep = defaultdict(lambda: defaultdict(int))\nfor h in headlines:\n    parsed = nlp(h)\n    pos = [START, START] + [doc.pos_ for doc in parsed] + [END, END]\n    tag = [START, START] + [doc.tag_ for doc in parsed] + [END, END]\n    dep = [START, START] + [doc.dep_ for doc in parsed] + [END, END]\n    for i in range(2, len(pos)):\n        trigrams_pos[(pos[i-2], pos[i-1])][pos[i]] += 1\n        trigrams_tag[(tag[i-2], tag[i-1])][tag[i]] += 1\n        trigrams_dep[(dep[i-2], dep[i-1])][dep[i]] += 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we are going to convert the counts to probabilities and store them in a dictionary."},{"metadata":{"trusted":true},"cell_type":"code","source":"trigrams_pos = {k: {k_: v_/sum(v.values()) for k_, v_ in v.items()} for k, v in trigrams_pos.items()}\ntrigrams_tag = {k: {k_: v_/sum(v.values()) for k_, v_ in v.items()} for k, v in trigrams_tag.items()}\ntrigrams_dep = {k: {k_: v_/sum(v.values()) for k_, v_ in v.items()} for k, v in trigrams_dep.items()}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To check what we are storing, we will print the probabilities of part-of-speech labels that follow a Noun and a Verb."},{"metadata":{"trusted":true},"cell_type":"code","source":"trigrams_pos[('NOUN', 'VERB')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to `pickle` the dictionaries so that we can re-use them in other Notebooks."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\npickle.dump(trigrams_pos, open(\"trigrams_pos.pkl\", \"wb\"))\npickle.dump(trigrams_tag, open(\"trigrams_tag.pkl\", \"wb\"))\npickle.dump(trigrams_dep, open(\"trigrams_dep.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In a lot of use-cases we are going to need a map from words to their POS tags, so we are going to build them here with the use of some pre-trained tokenizers."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer_1000 = pickle.load(open('../input/tokenization/tokenizer_1000.pkl', 'rb'))\ntokenizer_1500 = pickle.load(open('../input/tokenization/tokenizer_1500.pkl', 'rb'))\ntokenizer_2000 = pickle.load(open('../input/tokenization/tokenizer_2000.pkl', 'rb'))\ntokenizer_2500 = pickle.load(open('../input/tokenization/tokenizer_2500.pkl', 'rb'))\ntokenizer_2750 = pickle.load(open('../input/tokenization/tokenizer_2750.pkl', 'rb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_map_1000, tag_map_1000, dep_map_1000 = defaultdict(int), defaultdict(int), defaultdict(int)\nfor k in tokenizer_1000.word_index.keys():\n    tokens = nlp(k)\n    pos_map_1000[k] = tokens[0].pos_\n    tag_map_1000[k] = tokens[0].tag_\n    dep_map_1000[k] = tokens[0].dep_\n\npos_map_1000[START], pos_map_1000[END] = START, END\ntag_map_1000[START], tag_map_1000[END] = START, END\ndep_map_1000[START], dep_map_1000[END] = START, END","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_map_1500, tag_map_1500, dep_map_1500 = defaultdict(int), defaultdict(int), defaultdict(int)\nfor k in tokenizer_1500.word_index.keys():\n    tokens = nlp(k)\n    pos_map_1500[k] = tokens[0].pos_\n    tag_map_1500[k] = tokens[0].tag_\n    dep_map_1500[k] = tokens[0].dep_\n\npos_map_1500[START], pos_map_1500[END] = START, END\ntag_map_1500[START], tag_map_1500[END] = START, END\ndep_map_1500[START], dep_map_1500[END] = START, END","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_map_2000, tag_map_2000, dep_map_2000 = defaultdict(int), defaultdict(int), defaultdict(int)\nfor k in tokenizer_2000.word_index.keys():\n    tokens = nlp(k)\n    pos_map_2000[k] = tokens[0].pos_\n    tag_map_2000[k] = tokens[0].tag_\n    dep_map_2000[k] = tokens[0].dep_\n\npos_map_2000[START], pos_map_2000[END] = START, END\ntag_map_2000[START], tag_map_2000[END] = START, END\ndep_map_2000[START], dep_map_2000[END] = START, END","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_map_2500, tag_map_2500, dep_map_2500 = defaultdict(int), defaultdict(int), defaultdict(int)\nfor k in tokenizer_2500.word_index.keys():\n    tokens = nlp(k)\n    pos_map_2500[k] = tokens[0].pos_\n    tag_map_2500[k] = tokens[0].tag_\n    dep_map_2500[k] = tokens[0].dep_\n\npos_map_2500[START], pos_map_2500[END] = START, END\ntag_map_2500[START], tag_map_2500[END] = START, END\ndep_map_2500[START], dep_map_2500[END] = START, END","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_map_2750, tag_map_2750, dep_map_2750 = defaultdict(int), defaultdict(int), defaultdict(int)\nfor k in tokenizer_2750.word_index.keys():\n    tokens = nlp(k)\n    pos_map_2750[k] = tokens[0].pos_\n    tag_map_2750[k] = tokens[0].tag_\n    dep_map_2750[k] = tokens[0].dep_\n\npos_map_2750[START], pos_map_2750[END] = START, END\ntag_map_2750[START], tag_map_2750[END] = START, END\ndep_map_2750[START], dep_map_2750[END] = START, END","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And, finally, we need to pickle all these maps for future use."},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(dict(pos_map_1000), open(\"pos_map_1000.pkl\", \"wb\"))\npickle.dump(dict(tag_map_1000), open(\"tag_map_1000.pkl\", \"wb\"))\npickle.dump(dict(dep_map_1000), open(\"dep_map_1000.pkl\", \"wb\"))\n\npickle.dump(dict(pos_map_1500), open(\"pos_map_1500.pkl\", \"wb\"))\npickle.dump(dict(tag_map_1500), open(\"tag_map_1500.pkl\", \"wb\"))\npickle.dump(dict(dep_map_1500), open(\"dep_map_1500.pkl\", \"wb\"))\n\npickle.dump(dict(pos_map_2000), open(\"pos_map_2000.pkl\", \"wb\"))\npickle.dump(dict(tag_map_2000), open(\"tag_map_2000.pkl\", \"wb\"))\npickle.dump(dict(dep_map_2000), open(\"dep_map_2000.pkl\", \"wb\"))\n\npickle.dump(dict(pos_map_2500), open(\"pos_map_2500.pkl\", \"wb\"))\npickle.dump(dict(tag_map_2500), open(\"tag_map_2500.pkl\", \"wb\"))\npickle.dump(dict(dep_map_2500), open(\"dep_map_2500.pkl\", \"wb\"))\n\npickle.dump(dict(pos_map_2750), open(\"pos_map_2750.pkl\", \"wb\"))\npickle.dump(dict(tag_map_2750), open(\"tag_map_2750.pkl\", \"wb\"))\npickle.dump(dict(dep_map_2750), open(\"dep_map_2750.pkl\", \"wb\"))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}