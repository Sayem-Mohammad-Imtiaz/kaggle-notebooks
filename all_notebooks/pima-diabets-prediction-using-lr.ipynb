{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\ndiabetes_data = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\nprint(diabetes_data['Outcome'].value_counts())\ndiabetes_data.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diabetes_data.info( )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> ****Make Evaluation Function -> return confusion matrix, accuracy, precision, recall, f1 score, roc_auc","metadata":{}},{"cell_type":"code","source":"def get_clf_eval(y_test, pred=None, pred_proba=None):\n    confusion = confusion_matrix( y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test , pred)\n    recall = recall_score(y_test , pred)\n    f1 = f1_score(y_test,pred)\n    # ROC-AUC 추가 \n    roc_auc = roc_auc_score(y_test, pred_proba)\n    print('Confusion Matrix')\n    print(confusion)\n    # ROC-AUC print 추가\n    print('Accuracy: {0:.4f}, Precision: {1:.4f}, Recall: {2:.4f},\\\n    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def precision_recall_curve_plot(y_test=None, pred_proba_c1=None):\n    # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출. \n    precisions, recalls, thresholds = precision_recall_curve( y_test, pred_proba_c1)\n    \n    # X축을 threshold값으로, Y축은 정밀도, 재현율 값으로 각각 Plot 수행. 정밀도는 점선으로 표시\n    plt.figure(figsize=(8,6))\n    threshold_boundary = thresholds.shape[0]\n    plt.plot(thresholds, precisions[0:threshold_boundary], linestyle='--', label='precision')\n    plt.plot(thresholds, recalls[0:threshold_boundary],label='recall')\n    \n    # threshold 값 X 축의 Scale을 0.1 단위로 변경\n    start, end = plt.xlim()\n    plt.xticks(np.round(np.arange(start, end, 0.1),2))\n    \n    # x축, y축 label과 legend, 그리고 grid 설정\n    plt.xlabel('Threshold value'); plt.ylabel('Precision and Recall value')\n    plt.legend(); plt.grid()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Logistic Regression model","metadata":{}},{"cell_type":"code","source":"# 피처 데이터 세트 X, 레이블 데이터 세트 y를 추출. \n# 맨 끝이 Outcome 컬럼으로 레이블 값임. 컬럼 위치 -1을 이용해 추출 \nX = diabetes_data.iloc[:, :-1]\ny = diabetes_data.iloc[:, -1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 156, stratify=y)\n\n# 로지스틱 회귀로 학습,예측 및 평가 수행. \nlr_clf = LogisticRegression()\nlr_clf.fit(X_train , y_train)\npred = lr_clf.predict(X_test)\n# roc_auc_score 수정에 따른 추가\npred_proba = lr_clf.predict_proba(X_test)[:, 1]\n\nget_clf_eval(y_test , pred, pred_proba)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_proba_c1 = lr_clf.predict_proba(X_test)[:, 1]\nprecision_recall_curve_plot(y_test, pred_proba_c1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diabetes_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 0값을 검사할 피처명 리스트 객체 설정\nzero_features = ['Glucose', 'BloodPressure','SkinThickness','Insulin','BMI']\n\n# 전체 데이터 건수\ntotal_count = diabetes_data['Glucose'].count()\n\n# 피처별로 반복 하면서 데이터 값이 0 인 데이터 건수 추출하고, 퍼센트 계산\nfor feature in zero_features:\n    zero_count = diabetes_data[diabetes_data[feature] == 0][feature].count()\n    print('{0} 0 건수는 {1}, 퍼센트는 {2:.2f} %'.format(feature, zero_count, 100*zero_count/total_count))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# zero_features 리스트 내부에 저장된 개별 피처들에 대해서 0값을 평균 값으로 대체\ndiabetes_data[zero_features]=diabetes_data[zero_features].replace(0, diabetes_data[zero_features].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = diabetes_data.iloc[:, :-1]\ny = diabetes_data.iloc[:, -1]\n\n# StandardScaler 클래스를 이용해 피처 데이터 세트에 일괄적으로 스케일링 적용\nscaler = StandardScaler( )\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state = 156, stratify=y)\n\n# 로지스틱 회귀로 학습, 예측 및 평가 수행. \nlr_clf = LogisticRegression()\nlr_clf.fit(X_train , y_train)\npred = lr_clf.predict(X_test)\n# roc_auc_score 수정에 따른 추가\npred_proba = lr_clf.predict_proba(X_test)[:, 1]\nget_clf_eval(y_test , pred, pred_proba)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import Binarizer\n\ndef get_eval_by_threshold(y_test , pred_proba_c1, thresholds):\n    # thresholds 리스트 객체내의 값을 차례로 iteration하면서 Evaluation 수행.\n    for custom_threshold in thresholds:\n        binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) \n        custom_predict = binarizer.transform(pred_proba_c1)\n        print('임곗값:',custom_threshold)\n        # roc_auc_score 관련 수정\n        get_clf_eval(y_test , custom_predict, pred_proba_c1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholds = [0.3 , 0.33 ,0.36,0.39, 0.42 , 0.45 ,0.48, 0.50]\npred_proba = lr_clf.predict_proba(X_test)\nget_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 임곗값를 0.50로 설정한 Binarizer 생성\nbinarizer = Binarizer(threshold=0.48)\n\n# 위에서 구한 lr_clf의 predict_proba() 예측 확률 array에서 1에 해당하는 컬럼값을 Binarizer변환. \npred_th_048 = binarizer.fit_transform(pred_proba[:, 1].reshape(-1,1)) \n\n# roc_auc_score 관련 수정\nget_clf_eval(y_test , pred_th_048, pred_proba[:, 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}