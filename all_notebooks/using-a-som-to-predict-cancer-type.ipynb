{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Using a SOM to predict cancer type"},{"metadata":{"trusted":true},"cell_type":"code","source":"#First, we import the modules that we're going to use\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nnp.random.seed(0)\n\nprint('Setup complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Paste the SOM code I wrote previously\n\n#This function is the learning rate\n# @param t expects an int or float value\n# @param max_eta expects a float value as the maximum learning rate\ndef eta(max_eta, t):\n    return max_eta/t\n\n#Function for the Gaussian sigma\ndef sigma(max_sigma, t):\n    return max_sigma/t\n\n# @params x,w_m expect 1D arrays\n# @out returns euclidean distance of x-w_m\ndef Euclidean_dist(x,w_m):\n    d_vec = x - w_m\n    D = np.sqrt(np.dot(d_vec,d_vec))\n    return D\n\n# @param sigma_0 expects the max_sigma of the network\n# @param t expects a given iteration step\n# @param D expects the values of the output nodes\n# @param d_min expects the index of the BMU\ndef h(sigma_0,t,D,d_min):\n    I = np.ones(np.shape(D))\n    d = np.power((D-I*D[d_min]), 2)\n    coef = -1/(2*sigma(sigma_0, t)**2)\n    result = np.exp(d*coef)\n    return result\n    \n    \nclass SOM(object):\n    def __init__(self, sigma, eta, size):\n        #parameters\n        self.sigma = sigma\n        self.eta = eta\n        # @param size expects a two tuple with the input\n        # and output number of neurons\n        self.inputLayerSize = size[0]   # X1, X2, X3, ...\n        self.outputLayerSize = size[1]  # Y1, Y2, Y3, ...\n        \n        # build weights of each layer, set to random values\n        self.W1 = \\\n                np.random.rand(self.inputLayerSize, self.outputLayerSize)\n                \n    #This function finds the Best Matching Unit (BMU)\n    # @param x expects the input layer\n    # @param W expects the weight matrix     \n    def find_nodes_and_BMU(self,x):\n        #Calculate distances\n        d = []\n        for m in range(self.outputLayerSize):\n            d.append(Euclidean_dist(x,self.W1[:,m]))\n        D = np.array(d, dtype=float)\n        m = np.amin(D)\n        #Return BMU\n        return D, d.index(m)\n    \n    \n    #This function corrects the weights in W1\n    def update(self,x,t):\n        D, d_min = self.find_nodes_and_BMU(x)\n        m = self.outputLayerSize\n        H = np.ones((m,m))*h(self.sigma, t, D, d_min)\n        X = x.reshape((self.inputLayerSize,1))*np.ones((self.inputLayerSize,m))\n        new_W1 = self.W1 + eta(self.eta,t)*np.matmul((X-self.W1), H)\n        self.W1 = new_W1\n        \n    def save_weights(self):\n        # save this in order to reproduce our cool network\n        np.savetxt(\"weights.txt\", self.W1, fmt=\"%s\")\n    \n    #This function predicts an output based on an input x and trained weights\n    #Returns the index of the BMU\n    def predict_output(self,x):\n        D, d_min = self.find_nodes_and_BMU(x)\n        return d_min\n    \n    \n    \n    #Now lets define the training process\n    def train(self, data):\n        # @param data expects an ndarray or DataFrame\n        if str(type(data)) == '<class \\'numpy.ndarray\\'>':\n            n = data.shape[0]\n            for i in range(n):\n                self.update(data[i,:],i+1)\n                \n        elif str(type(data)) == '<class \\'pandas.core.frame.DataFrame\\'>':\n            n = data.shape[0]\n            new_data = data.to_numpy()\n            for i in range(n):\n                self.update(new_data[i,:],i+1)\n            \n        \n    def predict(self, data):\n        # @param data expects an ndarray or a DataFrame\n        if str(type(data)) == '<class \\'numpy.ndarray\\'>':\n            n = data.shape[0]\n            out_values = np.zeros((n,1))\n            for i in range(n):\n                output = self.predict_output(data[i,:])\n                out_values[i,0] = output\n            result = np.hstack((data,out_values))\n            return result\n                \n        elif str(type(data)) == '<class \\'pandas.core.frame.DataFrame\\'>':           \n            n = data.shape[0]\n            new_data = data.to_numpy()\n            out_values = np.zeros((n,1))\n            for i in range(n):\n                output = self.predict_output(new_data[i,:])\n                out_values[i,0] = output\n            np_result = np.hstack((new_data,out_values))\n            result = pd.DataFrame(data = np_result)\n            return result\n        \nprint('SOM setup complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Then we import the data\ndata_path = '../input/breast-cancer-wisconsin-data/data.csv'\n\ndata = pd.read_csv(data_path)\ndata = data.drop(columns='Unnamed: 32')\n\nb_data = data.copy().loc[data.diagnosis == 'B']\nm_data = data.copy().loc[data.diagnosis == 'M']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Size of the samples"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Benign tumors sample size: ' + str(b_data.shape[0]))\nprint('\\nMalignant tumors sample size: ' +str(m_data.shape[0]))\n\nprint('\\nTotal data sample size: ' + str(data.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data features"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Data features: ')\nfor i in range(data.shape[1]):\n    print('- ' + data.columns[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are actually 10 unique features apart from the 'Diagnosis' condition, with three different measured metrics (mean, se, worst)."},{"metadata":{},"cell_type":"markdown","source":"Let's see if we can use the Id column as an index:"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data.id.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are as many different Ids as there are rows, so we can use this column as the new index"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.set_index('id')\nb_data = b_data.set_index('id')\nm_data = m_data.set_index('id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualization of the data\n\nFirst we present the first rows of both datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"b_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try to find insights by visualizing some features of the datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot():\n    global fig\n    fig = plt.figure(figsize=(13,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot()\nsns.distplot(b_data['radius_mean'], label='Benign')\nsns.distplot(m_data['radius_mean'], label='Malignant')\nplt.legend()\nplt.title('Radius Histogram for Benign and Malignant Tumors')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As an example, we can see that the average radius (mean) of the malignant tumors is bigger than the radius of their benign counterparts. So we could include this feature in the input of the SOM."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot()\nsns.distplot(b_data['texture_mean'], label='Benign')\nsns.distplot(m_data['texture_mean'], label='Malignant')\nplt.legend()\nplt.title('Texture Histogram for Benign and Malignant Tumors')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Radius and Compactness data:"},{"metadata":{},"cell_type":"markdown","source":"With a scatter plot, we can see if there's a relationship between the radius of a tumor and its compactness (mean):"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot()\nsns.scatterplot(data=data, x='radius_mean', y='compactness_mean', hue='diagnosis')\nplt.title('Radius (mean) vs. Compactness (mean)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot()\nsns.regplot(data=data, x='radius_mean', y='compactness_mean')\nplt.title('Linear Regression of Radius (mean) vs Compactness (mean)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check if there are relationships that involve the other measures of Radius and Compactness (se and worst):"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plot()\nsns.regplot(data=b_data, y='radius_mean', x='radius_se', fit_reg=False, label='Benign')\nsns.regplot(data=m_data, y='radius_mean', x='radius_se', logx=True, label='Malignant')\nplt.legend()\nplt.title('Radius (se) vs Radius (mean)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trf_data_1 = data.copy()\ntrf_data_1['radius'] = trf_data_1['radius_mean'] * trf_data_1['radius_se']\ntrf_data_1['compactness'] = trf_data_1['compactness_mean'] * trf_data_1['compactness_se']\ntrf_data_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot()\nsns.scatterplot(x='radius', y='compactness', data=trf_data_1, hue='diagnosis')\nplt.title('Trf1 Radius vs Trf1 Compactness')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try with another transformation. In this case we're going to normalize the data and divide the mean by the se:"},{"metadata":{"trusted":true},"cell_type":"code","source":"trf_data_2 = data.copy()\ntrf_data_2['radius'] = data['radius_mean'] * (1/data['radius_mean'].max())\ntrf_data_2['compactness'] = data['compactness_mean'] * (1/data['compactness_mean'].max())\n#trf_data_2['Compactness (mean)'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot()\nsns.scatterplot(x='radius', y='compactness', data=trf_data_2, hue='diagnosis')\nplt.title('Trf2 Radius vs Trf2 Compactness')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fractal dimension of a tumor:"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"plot()\nsns.distplot(b_data['fractal_dimension_mean'], label='Benign')\nsns.distplot(m_data['fractal_dimension_mean'], label='Malignant')\nplt.legend()\nplt.title('Fractal Dimension (mean) Histogram')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It doesn't seem to be a good indicator of the type of tumor."},{"metadata":{},"cell_type":"markdown","source":"## Predicting the cancer type using a Self Organizing Map"},{"metadata":{},"cell_type":"markdown","source":"First, we're going to make a train-test split of the data:\n\n(WARNING: Changing the random seed RS may require you to manually re-map the SOM results to the Benign-Malignant classification)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nRS = 0 #General random state\ntarget = 'diagnosis'\nall_features = list(data.columns)\nall_features.remove(target)\n\nX_train, X_test, y_train, y_test = train_test_split(data[all_features], data[target], random_state=RS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Success metrics\n\nWe're going to use the same validation metrics for every attempt so that they can be compared cuantitatively. In this case those metrics are the Mean Absolute Error (MAE), the Mean Squared Error (MSE) and the Accuracy Score."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pipeline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build, train the SOM and predict the output\ndef run_SOM(sigma,eta,size,train_data,test_data):\n    np.random.seed(RS)\n    som = SOM(sigma,eta,size)\n    som.train(train_data)\n    preds = som.predict(test_data)\n    \n    retrieve_cols = {0: 0}\n    cols = test_data.columns.to_list()\n    for i in range(len(cols)):\n        retrieve_cols[i] = cols[i]\n    retrieve_cols[len(cols)] = 'Predictions'\n    preds = preds.rename(axis=1, mapper=retrieve_cols)\n    \n    return preds\n\n# After the predictions are obtained, we have to format the validation\n# data so that we can score the model\ndef format_data(p,B,M):\n    if p == 'B':\n        return B\n    elif p == 'M':\n        return M\n\n# Finally, we use the metrics to see how well the model performed\ndef score_model(val_data, predictions, model_name):\n    mae = mean_absolute_error(val_data, predictions)\n    print('MAE Score for %s: ' % model_name)\n    print(mae)\n\n    msd = mean_squared_error(val_data, predictions)\n    print('\\nMSD Score for %s: ' % model_name)\n    print(msd)\n    \n    acc_score = accuracy_score(val_data, predictions)\n    print('\\nAccuracy Score for %s: ' % model_name)\n    print(acc_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Attempt 1:\n\nWe can try to use the radius and the compactness of a tumor to guess its diagnosis."},{"metadata":{"trusted":true},"cell_type":"code","source":"SOM_fts_1_1 = ['radius_mean', 'compactness_mean', 'radius_se', 'compactness_se',\n             'radius_worst', 'compactness_worst']\n\nX_train_1_1, X_test_1_1 = X_train[SOM_fts_1_1], X_test[SOM_fts_1_1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size_1_1 = (6,2)\n\npreds_1_1 = run_SOM(3,1,size_1_1,X_train_1_1,X_test_1_1)\npreds_1_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far so good, now we need to map the Bs and Ms in the y_test series to a 1 or a 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"mapped_test = y_test.apply(func=format_data,args=(1,0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapped_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're now ready to estimate the accuracy of the model with the success metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"score_model(mapped_test, preds_1_1.Predictions, 'SOM 1.1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This looks pretty good!"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#Let's see how well does a Random Forest work in comparisson\nfrom sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier(n_estimators=100, random_state=RS)\nRF.fit(X_train_1_1, y_train.apply(func=format_data, args=(1,0)))\nRF_preds = RF.predict(X_test_1_1)\nscore_model(mapped_test, RF_preds, 'Random Forest')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Althoug a random forest did better."},{"metadata":{},"cell_type":"markdown","source":"Now we can try to use the normalized data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"SOM_fts_1_2 = ['radius', 'compactness']\n\nX_train_1_2, X_test_1_2 = X_train.copy(), X_test.copy()\n\nX_train_1_2['radius'] = X_train['radius_mean'] * (1/data['radius_mean'].max())\nX_train_1_2['compactness'] = X_train['compactness_mean'] * (1/data['compactness_mean'].max())\n\nX_test_1_2['radius'] = X_test['radius_mean'] * (1/data['radius_mean'].max())\nX_test_1_2['compactness'] = X_test['compactness_mean'] * (1/data['compactness_mean'].max())\n\nprint(X_train_1_2[['radius_mean','radius','compactness_mean','compactness']].head())\nprint(X_test_1_2[['radius_mean','radius','compactness_mean','compactness']].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let's select the features we want\nX_train_1_2, X_test_1_2 = X_train_1_2[SOM_fts_1_2], X_test_1_2[SOM_fts_1_2]\n#Now, we define the new model we want to test\nsize_1_2 = (2,2)\n\npreds_1_2 = run_SOM(3,1,size_1_2,X_train_1_2,X_test_1_2)\npreds_1_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapped_test = y_test.apply(func=format_data, args=(0,1))\nscore_model(mapped_test, preds_1_2.Predictions, 'SOM 1.2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's did slightly better than the SOM 1.1."},{"metadata":{},"cell_type":"markdown","source":"### Attempt 2:\n\nIn this attempt we're going to use every feature present in the data."},{"metadata":{"trusted":true},"cell_type":"code","source":"SOM_fts_2_1 = all_features\nX_train_2_1, X_test_2_1 = X_train, X_test\nsize_2_1 = (len(SOM_fts_2_1),2)\n\npreds_2_1 = run_SOM(3,1,size_2_1,X_train_2_1,X_test_2_1)\npreds_2_1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapped_test = y_test.apply(func=format_data, args=(0,1))\nscore_model(mapped_test, preds_2_1.Predictions, 'SOM 2.1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This has been the best version so far. The accuracy of a SOM seems to increase with the amount of features we input."},{"metadata":{},"cell_type":"markdown","source":"We saw that the fractal dimension (mean) feature didn't show a correlation with the tumor diagnosis. Let's see what happens if we remove it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"SOM_fts_2_2 = all_features.copy()\nSOM_fts_2_2.remove('fractal_dimension_mean')\n\nX_train_2_2, X_test_2_2 = X_train[SOM_fts_2_2], X_test[SOM_fts_2_2]\nsize_2_2 = (len(SOM_fts_2_2),2)\n\npreds_2_2 = run_SOM(3,1,size_2_2,X_train_2_2,X_test_2_2)\npreds_2_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mapped_test = y_test.apply(func=format_data, args=(0,1))\nscore_model(mapped_test, preds_2_2.Predictions, 'SOM 2.2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same score as SOM 2.1."},{"metadata":{},"cell_type":"markdown","source":"### Attempt 3\nIn this case, we'll try to find the optimal parameters for the SOM. Asuming there's a global minima for the accuracy of a SOM based on the succes metrics, first we're going to find the best value for the **sigma** parameter, and then for the **eta** parameter.\n\nWe'll be using SOM 2.1 as the model."},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_3_1, X_test_3_1 = X_train.copy(), X_test.copy()\nsize_3_1 = size_2_1\nmapped_test = y_test.apply(func=format_data, args=(0,1))\n#Let's search for the best value of sigma\nfor i in [1,2,3,4,5]:\n    print('Sigma = %d' % i)\n    preds = run_SOM(i,1,size_3_1, X_train_3_1, X_test_3_1)\n    score_model(mapped_test, preds.Predictions, 'SOM 3.1')\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The sigma value doesn't seem to affect the results. Let's try with the **eta** parameter:"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train_3_2, X_test_3_2 = X_train.copy(), X_test.copy()\nsize_3_2 = size_2_1\nmapped_test = y_test.apply(func=format_data, args=(0,1))\n#Let's search for the best value of sigma\nfor i in [0.1,0.2,0.5,1,2]:\n    print('Eta = %.1f' % i)\n    preds = run_SOM(3,i,size_3_2, X_train_3_2, X_test_3_2)\n    score_model(mapped_test, preds.Predictions, 'SOM 3.2')\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems to converge to the a score that's very close to the one obtained for SOM 2.1, but this difference could be caused by the small size of the sample. We will asume we already had near optimal values for the parameters."},{"metadata":{},"cell_type":"markdown","source":"Finally, we'll do a cross-validation of the SOM 2.1 to get an average score, given the small size of the sample."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\n\ndef cross_val_SOM(sigma, eta, size, X, y, n_splits=5):\n    np.random.seed(RS)\n    # First let's generate the data split\n    group_kfold = GroupKFold(n_splits=n_splits)\n    groups = np.random.randint(low=0, high=n_splits, size=(X.shape[0]), dtype='int64')\n    \n    scores = []\n    \n    for train_index, test_index in group_kfold.split(X, y, groups):\n        #Generate the split\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        #Build, train and apply the SOM\n        predictions = run_SOM(sigma,eta,size,X_train,X_test)\n        \n        B, M = 1, 0\n        mapped_test = y_test.apply(func=format_data, args=(B,M))\n        \n        #Get the score\n        score = accuracy_score(mapped_test, predictions.Predictions)\n        if score <= 0.5:\n            score = 1 - score    \n        scores.append(score)\n        \n    #Turn the scores to a panda.Series\n    scores_pd = pd.Series(data=scores)\n    \n    print('Mean score: ' + str(scores_pd.mean()))\n    print('Max score: ' + str(scores_pd.max()))\n    print('Min score: ' + str(scores_pd.min()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_SOM(2, 1, size_2_1, X=data[all_features], y=data[target], n_splits=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cross_val_SOM(2, 2, size_2_1, X=data[all_features], y=data[target], n_splits=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SOM 2.1 seems to have a better mean score than the alternative."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}