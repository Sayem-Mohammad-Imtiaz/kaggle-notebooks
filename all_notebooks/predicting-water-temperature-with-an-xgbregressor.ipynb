{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split #will be used to generate a train, validation, and test set\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom mlxtend.preprocessing import minmax_scaling\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's look at our data! We'll be focusing this first test entirely on the data in bottle.csv to simplify things. FYI: Our goal is going to be to try and predict \"T_degC\" using compositional elements in the water. Basically, anything that is \"in\" the water is a valid independent variable for the purposes of this test. Other things, such as quality measurements, time indicators, and depth have been dropped (you'll see this happen later).\n\nWe're going to be using all of the relevant compositional data from 1949 to 2016 as our training & validation set. I retrieved a set of data from 2017 to 2019 from CalCOFI's website to use as a test set."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"X_full = pd.read_csv('../input/calcofi/bottle.csv', low_memory=False)\nX_full.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_full = pd.read_csv('../input/2017-to-2019-bottle/2017 to 2019 bottle.csv', low_memory=False)\nX_test_full.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Wow, that's a good amount of data. Let's try to get a sense of how much it is usable."},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values_count = X_full.isnull().sum()\ntotal_cells = np.product(X_full.shape)\ntotal_missing = missing_values_count.sum()\n\npercent_missing = (total_missing/total_cells * 100)\nprint(\"Total cells:\", total_cells)\nprint(\"Total cells missing a value:\", total_missing)\nprint(percent_missing)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yikes -- almost half of our cells are empty!"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_full.dropna(axis=0, subset=['T_degC'], inplace=True)\nX_test_full.dropna(axis=0, subset=['T_degC'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = X_full.T_degC\ny_test = X_test_full.T_degC\n#X_full.drop(['T_degC', 'IncTim', 'LightP', 'Depth_ID', 'R_Depth', 'R_TEMP', 'R_POTEMP', 'R_SALINITY', 'R_SIGMA', 'R_SVA', 'R_DYNHT', 'R_O2', 'R_O2Sat', 'R_SIO3', 'R_PO4', 'R_NO3', 'R_NO2', 'R_NH4', 'R_CHLA', 'R_PHAEO', 'R_PRES', 'R_SAMP', 'DIC1', 'DIC2', 'TA1', 'TA2', 'pH2', 'pH1', 'MeanAq', 'MeanAp', 'MeanAs', 'DarkAq', 'DarkAp', 'DarkAs', 'DIC Quality Comment'], axis=1, inplace=True)\nX_full.drop(['T_degC'], axis=1, inplace=True) #drop our dependent variable\nX_full.drop(['T_prec', 'T_qual', 'R_TEMP', 'R_POTEMP'], axis=1, inplace=True) #drop all temperature-based variables\nX_full.drop(['Sta_ID', 'Depth_ID', 'DIC Quality Comment', 'S_prec', 'S_qual', 'P_qual', 'O_qual', 'SThtaq', 'O2Satq', 'Chlqua', 'Phaqua', 'PO4q', 'SiO3qu', 'NO2q', 'NO3q', 'NH3q', 'C14A1p', 'C14A1q', 'C14A2p', 'C14A2q', 'DarkAp', 'DarkAq', 'MeanAp', 'MeanAq'], axis=1, inplace=True) #drop precision and quality metrics\nX_full.drop(['R_Depth', 'R_SALINITY', 'R_SIGMA', 'R_SVA', 'R_DYNHT', 'R_O2', 'R_O2Sat', 'R_SIO3', 'R_PO4', 'R_NO3', 'R_NO2', 'R_NH4', 'R_CHLA', 'R_PHAEO', 'R_SAMP'], axis=1, inplace=True) #drop R_ columns\nX_full.drop(['IncTim', 'Cst_Cnt', 'Btl_Cnt'], axis=1, inplace=True) #dropping date-time related factors\nX_full.drop(['Depthm', 'BtlNum', 'RecInd'], axis=1, inplace=True)\nX_full.drop(['R_PRES'], axis=1, inplace=True)\n\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, train_size=0.8, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_cols = [cname for cname in X_train_full.columns if\n                   X_train_full[cname].dtype == 'object']\nnumerical_cols = [cname for cname in X_train_full.columns if \n                 X_train_full[cname].dtype in ['int64', 'float64']]\n\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\nprint(X_train.shape)\nprint(X_test.shape)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='most_frequent')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define model\nn_estimators = [600]\nmax_depth = [6]\nlearning_rate = [0.125]\n\nmodel = XGBRegressor(n_estimators=300, max_depth=5, learning_rate=0.135, random_state=0)\n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth, learning_rate = learning_rate)\ngridF = GridSearchCV(model, hyperF, cv = 2, verbose = 1, n_jobs = -1)\n\ngrid_test = Pipeline(steps=[('preprocessor', preprocessor),\n                           ('model', gridF)\n                           ])\n\n# Preprocessing of training data, fit model \ngrid_test.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = grid_test.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds))\nprint(grid_test.named_steps[\"model\"].best_params_)\n#0.081 MAE with learning_rate = 0.125, max_depth = 6, n_estimators = 600","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\n\nfeature_importance = grid_test.named_steps[\"model\"].best_estimator_.get_booster().get_score(importance_type=\"gain\")\n\nplot_importance(feature_importance)\nplt.show()\nprint(len(feature_importance))\nprint(feature_importance)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = {}\n\nfor col, score in zip(X_train.columns, feature_importance):\n    results[col] = score\n    \ns = pd.Series(results, name='Feature number')\ns.index.name = 'Feature name'\ns = s.reset_index()\n\ns2 = pd.Series(feature_importance, name='F-score')\ns2.index.name = 'Feature number'\ns2 = s2.reset_index()\n\ns = pd.DataFrame(s)\ns2 = pd.DataFrame(s2)\n\nmerged = pd.merge(left=s, right=s2, left_on='Feature number', right_on='Feature number').sort_values(by=['F-score'], ascending=False)\nprint(merged)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_test = grid_test.predict(X_test)\nprint(y_test)\nprint(preds_test)\n\nprint('MAE for 2017 to 2019 test:', mean_absolute_error(y_test, preds_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}