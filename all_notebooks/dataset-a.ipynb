{"cells":[{"metadata":{},"cell_type":"markdown","source":"*Script*:\nimplémentation des techniques de la machine learning pour la prédiction du taux de Churn pour je jeux de données Telecom Costumer\n\n*Description* :\nCe script est dédie à la comparaison entres des algorithmes de classification supervisée et les algorithmes à base d'ensemble pour la prédiction de taux de Churn.\nil structuré selon les étapes ci-dessous :\n1 - Importation des bibliothèques.\n2 - Chargement du jeu de données.\n3 - Suppression des variables inutiles.\n4 - Traitement des valeurs manquantes et les valeurs erronées.\n6 - Transformation des types des variables Objet en des types numériques.\n7 - Calcul des coefficients de corrélation entre les différentes variables du jeu de données et la suppréssion d'elles ayant un coéfficient supérieur ou égal à 0,95\n8 - Séparation entres les variables prédictives et la variables à prédire.\n9 - Normalisation des variables prédictives.\n10- Appliquer la technique de la validation croisée pour l'implémentation des algorithmes sélectionnés pour l'étude comparative.\n11- Calcul des métriques Accuracy et temps d'execution pour chaque algorithme\n12- Calcul de la métrique AUC\n13- Affichage de différent graphiques \n14- Tracage de la courbe ROC pour tous les algorithmes de l'étude comparative.\n\n*Version*:\nGODIAL KHALID : Juillet 2019 Script Original\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importation des bibliothèques\nimport pandas as pd \nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder \nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import model_selection\nfrom catboost import CatBoostClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc\nfrom scipy import interp\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Chargement du jeu de données\ntry:\n    ds1=pd.read_csv(\"../input/Telecom_customer churn.csv\")\n    print(\"dataset is loaded\")\nexcept:\n    print(\"Dataset could not be loaded. Is the dataset missing?\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Traitement des valeurs manquantes \nds1=ds1.fillna(method='ffill')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Suppression des variables inutiles\nds1.pop('Customer_ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transformation des types des variables Objet en des types numériques.\nlabelencoder= LabelEncoder()\ncolum_Object=ds1.select_dtypes(include=['object'])\nfor c in colum_Object:\n    ds1[c] = labelencoder.fit_transform(ds1[c])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds1=ds1.drop(columns=['churn']).assign(churn=ds1['churn'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ds1['churn'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calcul des coefficients de corrélation entre les différentes variables du jeu de données \n# et la suppréssion d'elles ayant un coéfficient supérieur ou égal à 0,95\n\n#creation de la matrice de corrélation\ncorr_matrix = ds1.corr().abs()\n\n# sélection du triangle de la matrice de corrélation\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# cherche les indices des variables ayant un coéfficient de corrélation >= 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n#suppression d'une variable ayant un coéfficient de corrélation >=0.95\nfor c in to_drop:\n    ds1.pop(c)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d=pd.DataFrame({'exclut':to_drop})\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds1.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Séparation entres les variables prédictives et la variables à prédire.\nX=ds1.iloc[:,:-1].values\ny=ds1.iloc[:,ds1.columns.size-1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalisation des variables prédictives.\nsc_X = StandardScaler()\nX = sc_X.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tableau contient l'abréviation et l'instance de chaque classifieur\nclassifieurs = []\nclassifieurs.append(('LR', LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)))\nclassifieurs.append(('CART', DecisionTreeClassifier()))\nclassifieurs.append(('LSVM',  CalibratedClassifierCV(base_estimator=LinearSVC(penalty='l2', dual=False), cv=10)))\nclassifieurs.append(('RF', RandomForestClassifier(n_estimators=300)))\nclassifieurs.append(('ADAB', AdaBoostClassifier(n_estimators=300, )))\nclassifieurs.append(('XGB', XGBClassifier( n_estimators=300, n_jobs=-1)))\nclassifieurs.append(('LGBM', LGBMClassifier()))\nclassifieurs.append(('CATB', CatBoostClassifier()))\n\n#Tableau contient les Accuracy de chaque classifieur\nresults = []\n#Tableau contient le nom de chaque classifieur\nnames = []\n#Tableau contient le temps d'execution pour chaque classifieur\ntimes =[]\n#Tableau contient l'Accuracy moyenne pour chaque classifieur\nresults_mean=[]\n#Tableau contient l'écart-type moyenne pour chaque classifieur\nresults_std=[]\n\n# Métrique d'évaluation a calculée\nscoring = 'accuracy'\n\nfor name, model in classifieurs:\n    kfold = model_selection.KFold(n_splits=10, random_state=0)\n    start = time.time()\n    cv_results = model_selection.cross_val_score(model, X, y, cv=10, scoring='accuracy')\n    end = time.time()\n    t=(end-start)/10\n    times.append(t)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (+/- %f) : %f s\" % (name, cv_results.mean(), cv_results.std(),t)\n    results_mean.append(cv_results.mean())\n    results_std.append(cv_results.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# implémentation dex réseaux des neurones\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\ncvscores = []\nstart = time.time()\nfor train, test in kfold.split(X, y):\n    classifieur = Sequential()\n    classifieur.add(Dense((ds1.columns.size-1)*2, input_dim=ds1.columns.size-1, activation='relu'))\n    classifieur.add(Dense((ds1.columns.size-1), activation='relu'))\n    classifieur.add(Dense(1, activation='sigmoid'))\n    classifieur.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    classifieur.fit(X[train], y[train], epochs=10, batch_size=10, verbose=0)\n    scores = classifieur.evaluate(X[test], y[test],   verbose=0)\n    print(\"%s: %.2f%%\" % (classifieur.metrics_names[1], scores[1]*100))\n    cvscores.append(scores[1])\nend = time.time()\nt=(end-start)/10\ntimes.append(t)\nresults.append(cvscores)\nresults_mean.append(np.mean(cvscores))\nresults_std.append(np.std(cvscores))\nnames.append('DL')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=list(map(list, zip(*results)))\nr = pd.DataFrame(results,columns=names)\nr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=list(map(list, zip(*results)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nk_range = range(1, 11)\nplt.style.use('seaborn-whitegrid')\nfor i in range(0,8):\n    plt.plot(k_range, results[i],marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4, linestyle='dashed')\n    plt.xlabel('K-fold')\n    plt.ylabel('Cross-Validated Accuracy')\n    plt.title(names[i])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1, figsize = (8,5) )\nplt.style.use('seaborn-whitegrid')\nplt.bar(names,times, color='red',width=0.1)\nplt.xlabel('Classifieurs')\nplt.ylabel('Temps execution en s')\nplt.show()\nplt.figure(1, figsize = (8,5) )\n\nplt.errorbar(names, results_mean, yerr=results_std, fmt='ro', ecolor='blue',elinewidth=5, capsize=10)\nplt.xlabel('Classifieurs')\nplt.ylabel('Accuracy avec std')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calcul de AUC\nresults_Roc_Auc = []\nnames_Roc_Auc = []\nresults_Roc_Auc_mean=[]\nresults_Roc_Auc_std=[]\nnames_Roc_Auc=[]\nscoring = 'roc_auc'\nfor name, model in classifieurs:\n    kfold = model_selection.KFold(n_splits=10, random_state=0)\n    cv_results_Roc_Auc = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n    results.append(cv_results_Roc_Auc)\n    names_Roc_Auc.append(name)\n    msg = \"%s: %.2f (+/- %f)\" % (name, cv_results_Roc_Auc.mean(),cv_results_Roc_Auc.std())\n    results_Roc_Auc_mean.append(cv_results_Roc_Auc.mean())\n    results_Roc_Auc_std.append(cv_results_Roc_Auc.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calcul de AUC pour Deep learning\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\ncvscores_Roc_Auc = []\nfor train, test in kfold.split(X, y):\n    classifieur = Sequential()\n    classifieur.add(Dense((ds1.columns.size-1)*2, input_dim=ds1.columns.size-1, activation='relu'))\n    classifieur.add(Dense(ds1.columns.size-1, activation='relu'))\n    classifieur.add(Dense(1, activation='sigmoid'))\n    classifieur.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    classifieur.fit(X[train], y[train], epochs=10, batch_size=10, verbose=0)\n    scores = classifieur.evaluate(X[test], y[test],   verbose=0)\n    y_pred = classifieur.predict_proba(X[test])\n    cvscores_Roc_Auc.append(roc_auc_score(y[test], y_pred))\nresults_Roc_Auc_mean.append(np.mean(cvscores_Roc_Auc))\nresults_Roc_Auc_std.append(np.std(cvscores_Roc_Auc))\nnames_Roc_Auc.append('DL')\nmsg = \"DL: %.2f (+/- %f)\" % (np.mean(cvscores_Roc_Auc),np.std(cvscores_Roc_Auc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(1, figsize = (8,5) )\nplt.style.use('seaborn-whitegrid')\nplt.errorbar(names, results_Roc_Auc_mean, yerr=results_Roc_Auc_std, fmt='ro', ecolor='blue',elinewidth=5, capsize=10)\nplt.xlabel('Classifieurs')\nplt.ylabel('AUC avec std')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AC_AUC = pd.DataFrame({'Accuracy':results_mean,'AUC':results_Roc_Auc_mean}, index=names)\nAC_AUC.plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = pd.DataFrame({'Temps':times,'Accuracy':results_mean,'AUC':results_Roc_Auc_mean}, index=names)\ndf_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# traçage des courbes ROC pour tous les classifieurs\n\nclassifieurs = []\nclassifieurs.append(('LR', LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)))\nclassifieurs.append(('CART', DecisionTreeClassifier()))\nclassifieurs.append(('LSVM',  CalibratedClassifierCV(base_estimator=LinearSVC(penalty='l2', dual=False), cv=10)))\nclassifieurs.append(('RF', RandomForestClassifier(n_estimators=300)))\nclassifieurs.append(('ADAB', AdaBoostClassifier(n_estimators=300, )))\nclassifieurs.append(('XGB', XGBClassifier( n_estimators=300, n_jobs=-1)))\nclassifieurs.append(('LGBM', LGBMClassifier()))\nclassifieurs.append(('CATB', CatBoostClassifier()))\n\n# plot arrows\nfig1 = plt.figure(figsize=[12,12])\nax1 = fig1.add_subplot(111,aspect = 'equal')\n\ntprs = []\naucs = []\nmean_fpr = np.linspace(0,1,100)\nc=['blue','green','red','cyan','magenta','yellow','violet','grey']\nj=0\nfor name, model in classifieurs:\n    kfold = model_selection.KFold(n_splits=10, random_state=0)\n    cv=kfold\n    i = 1\n    for train,test in cv.split(X,y):\n        prediction = model.fit(X[train],y[train]).predict_proba(X[test])\n        fpr, tpr, t = roc_curve(y[test], prediction[:, 1])\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        i= i+1\n    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_auc = auc(mean_fpr, mean_tpr)\n    plt.plot(mean_fpr, mean_tpr, color=c[j],label=r'%s (AUC = %0.2f )' % (name,mean_auc),lw=2, alpha=1)\n    j=j+1\n    \nfor train, test in kfold.split(X, y):\n    classifieur = Sequential()\n    classifieur.add(Dense((ds1.columns.size-1)*2, input_dim=ds1.columns.size-1, activation='relu'))\n    classifieur.add(Dense(ds1.columns.size-1, activation='relu'))\n    classifieur.add(Dense(1, activation='sigmoid'))\n    classifieur.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    classifieur.fit(X[train], y[train], epochs=10, batch_size=3, verbose=0)\n    prediction = classifieur.predict_proba(X[test])\n    fpr, tpr, t = roc_curve(y[test], prediction)\n    tprs.append(interp(mean_fpr, fpr, tpr))\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\nplt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\nmean_tpr = np.mean(tprs, axis=0)\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='orange',label=r'DL (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n\nplt.xlabel('Taux Faux Positif')\nplt.ylabel('Taux Vrai Positif')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}