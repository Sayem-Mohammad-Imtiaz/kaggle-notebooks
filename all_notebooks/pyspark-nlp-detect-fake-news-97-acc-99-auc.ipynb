{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP for detect fake news using PySpark"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nsc= SparkContext(master= 'local', appName= 'Fake and real news')\nss= SparkSession(sc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Read data set  \nReading data using the SparkSession.read.csv method causes structural errors for the data file. So read the data using pandas.read_csv method and convert to Spark Dataframe.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Funtion for conver Pandas Dataframe to Spark Dataframe\nfrom pyspark.sql.types import StringType, StructField, StructType\ndef read_data(path):\n  schema= StructType(\n      [StructField('title',StringType(),True),\n      StructField('text',StringType(),True),\n      StructField('subject',StringType(),True),\n      StructField('date',StringType(),True)])\n  pd_df= pd.read_csv(path)\n  sp_df= ss.createDataFrame(pd_df, schema= schema)\n  return sp_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read data set\npath_true= '/kaggle/input/fake-and-real-news-dataset/True.csv'\npath_fake= '/kaggle/input/fake-and-real-news-dataset/Fake.csv'\ntrue_df= read_data(path_true)\nfake_df= read_data(path_fake)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Check the data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of news true\ntrue_df.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of news fake\nfake_df.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenate 2 data sets into one and shuffle data set\nfrom pyspark.sql.functions import lit, rand\ndata= true_df.withColumn('fake', lit(0)).union(fake_df.withColumn('fake', lit(1))).orderBy(rand())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check again\ndata.groupBy('fake').count().show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the values of the subject column\ndata.select('subject').distinct().show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.show(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Create objects for processing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.feature import SQLTransformer, RegexTokenizer, StopWordsRemover, CountVectorizer, Imputer, IDF\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nStopWordsRemover.loadDefaultStopWords('english')\n\n# 0. Extract tokens from title\ntitle_tokenizer= RegexTokenizer(inputCol= 'title', outputCol= 'title_words',\n                                pattern= '\\\\W', toLowercase= True)\n# 1. Remove stop words from title\ntitle_sw_remover= StopWordsRemover(inputCol= 'title_words', outputCol= 'title_sw_removed')\n# 2. Compute Term frequency from title\ntitle_count_vectorizer= CountVectorizer(inputCol= 'title_sw_removed', outputCol= 'tf_title')\n# 3. Compute Term frequency-inverse document frequency from title\ntitle_tfidf= IDF(inputCol= 'tf_title', outputCol= 'tf_idf_title')\n# 4. Extract tokens from text\ntext_tokenizer= RegexTokenizer(inputCol= 'text', outputCol= 'text_words',\n                                pattern= '\\\\W', toLowercase= True)\n# 5. Remove stop words from text\ntext_sw_remover= StopWordsRemover(inputCol= 'text_words', outputCol= 'text_sw_removed')\n# 6. Compute Term frequency from text\ntext_count_vectorizer= CountVectorizer(inputCol= 'text_sw_removed', outputCol= 'tf_text')\n# 7. Compute Term frequency-inverse document frequency text\ntext_tfidf= IDF(inputCol= 'tf_text', outputCol= 'tf_idf_text')\n# 8. StringIndexer subject\nsubject_str_indexer= StringIndexer(inputCol= 'subject', outputCol= 'subject_idx')\n# 9. VectorAssembler\nvec_assembler= VectorAssembler(inputCols=['tf_idf_title', 'tf_idf_text', 'subject_idx'], outputCol= 'features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Create object for Random Forest Classifier model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.classification import RandomForestClassifier\n# 10 Random Forest Classifier\nrf= RandomForestClassifier(featuresCol= 'features', labelCol= 'fake', predictionCol= 'fake_predict', maxDepth= 7, numTrees= 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Create Pipeline for processing and fitting data to model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml import Pipeline\nrf_pipe= Pipeline(stages=[title_tokenizer, # 0\n                title_sw_remover, # 1\n                title_count_vectorizer, # 2\n                title_tfidf, # 3\n                text_tokenizer, # 4\n                text_sw_remover, # 5\n                text_count_vectorizer, # 6\n                text_tfidf, # 7\n                subject_str_indexer, # 8\n                vec_assembler, # 9\n                rf]) # 10 model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Splitting the dataset into the training set and test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test= data.randomSplit([0.8, 0.2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Fitting model"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_model= rf_pipe.fit(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Evaluate classification model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for evaluating classification model\nfrom pyspark.ml.evaluation import  MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n\naccuracy= MulticlassClassificationEvaluator(labelCol= 'fake', predictionCol= 'fake_predict', metricName= 'accuracy')\nf1= MulticlassClassificationEvaluator(labelCol= 'fake', predictionCol= 'fake_predict', metricName= 'f1')\nareaUnderROC= BinaryClassificationEvaluator(labelCol= 'fake', metricName= 'areaUnderROC')\n\ndef classification_evaluator(data_result):\n    data_result.crosstab(col1= 'fake_predict', col2= 'fake').show()\n    print('accuracy:' ,accuracy.evaluate(data_result))\n    print('f1:' ,f1.evaluate(data_result))\n    print('areaUnderROC:' ,areaUnderROC.evaluate(data_result))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.1 Evaluation of final model fit on the training data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict on training data set\nrf_train_result= rf_model.transform(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_evaluator(rf_train_result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.2 evaluation of final model fit on the test data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict on test data set\nrf_test_result= rf_model.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_evaluator(rf_test_result)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}