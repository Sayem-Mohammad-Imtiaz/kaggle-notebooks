{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_PATH = f'../input/train.csv'\ndata = pd.read_csv(DATA_PATH)\n\ndata = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\ndata.head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5af480d297e898b9264383c6e081a6d04ff95879"},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import check_array\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"Encode categorical features as a numeric array.\n    The input to this transformer should be an array-like of integers or\n    strings, denoting the values taken on by categorical (discrete) features.\n    The features can be encoded using a one-hot (aka one-of-K or dummy)\n    encoding scheme (``encoding='onehot'``, the default) or converted\n    to ordinal integers (``encoding='ordinal'``).\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n    Parameters\n    ----------\n    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n        The type of encoding to use (default is 'onehot'):\n        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n          (or also called 'dummy' encoding). This creates a binary column for\n          each category and returns a sparse matrix.\n        - 'onehot-dense': the same as 'onehot' but returns a dense array\n          instead of a sparse matrix.\n        - 'ordinal': encode the features as ordinal integers. This results in\n          a single column of integers (0 to n_categories - 1) per feature.\n    categories : 'auto' or a list of lists/arrays of values.\n        Categories (unique values) per feature:\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories must be sorted and should not mix\n          strings and numeric values.\n        The used categories can be found in the ``categories_`` attribute.\n    dtype : number type, default np.float64\n        Desired dtype of output.\n    handle_unknown : 'error' (default) or 'ignore'\n        Whether to raise an error or ignore if a unknown categorical feature is\n        present during transform (default is to raise). When this parameter\n        is set to 'ignore' and an unknown category is encountered during\n        transform, the resulting one-hot encoded columns for this feature\n        will be all zeros. In the inverse transform, an unknown category\n        will be denoted as None.\n        Ignoring unknown categories is not supported for\n        ``encoding='ordinal'``.\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting\n        (in order corresponding with output of ``transform``).\n    Examples\n    --------\n    Given a dataset with two features, we let the encoder find the unique\n    values per feature and transform the data to a binary one-hot encoding.\n    >>> from sklearn.preprocessing import CategoricalEncoder\n    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n    >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n    >>> enc.fit(X)\n    ... # doctest: +ELLIPSIS\n    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n              encoding='onehot', handle_unknown='ignore')\n    >>> enc.categories_\n    [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n    >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\n    array([[1., 0., 1., 0., 0.],\n           [0., 1., 0., 0., 0.]])\n    >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n    array([['Male', 1],\n           [None, 2]], dtype=object)\n    See also\n    --------\n    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n      integer ordinal features. The ``OneHotEncoder assumes`` that input\n      features take on values in the range ``[0, max(feature)]`` instead of\n      using the unique values.\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    \"\"\"\n\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        if self.categories != 'auto':\n            for cats in self.categories:\n                if not np.all(np.sort(cats) == np.array(cats)):\n                    raise ValueError(\"Unsorted categories are not yet \"\n                                     \"supported\")\n\n        X_temp = check_array(X, dtype=None)\n        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n            X = check_array(X, dtype=np.object)\n        else:\n            X = X_temp\n\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                if self.handle_unknown == 'error':\n                    valid_mask = np.in1d(Xi, self.categories[i])\n                    if not np.all(valid_mask):\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(self.categories[i])\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using specified encoding scheme.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X_temp = check_array(X, dtype=None)\n        if not hasattr(X, 'dtype') and np.issubdtype(X_temp.dtype, np.str_):\n            X = check_array(X, dtype=np.object)\n        else:\n            X = X_temp\n\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            Xi = X[:, i]\n            valid_mask = np.in1d(Xi, self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    Xi = Xi.copy()\n                    Xi[~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(Xi)\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        feature_indices = np.cumsum(n_values)\n\n        indices = (X_int + feature_indices[:-1]).ravel()[mask]\n        indptr = X_mask.sum(axis=1).cumsum()\n        indptr = np.insert(indptr, 0, 0)\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csr_matrix((data, indices, indptr),\n                                shape=(n_samples, feature_indices[-1]),\n                                dtype=self.dtype)\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out\n\n    def inverse_transform(self, X):\n        \"\"\"Convert back the data to the original representation.\n        In case unknown categories are encountered (all zero's in the\n        one-hot encoding), ``None`` is used to represent this category.\n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n            The transformed data.\n        Returns\n        -------\n        X_tr : array-like, shape [n_samples, n_features]\n            Inverse transformed array.\n        \"\"\"\n        check_is_fitted(self, 'categories_')\n        X = check_array(X, accept_sparse='csr')\n\n        n_samples, _ = X.shape\n        n_features = len(self.categories_)\n        n_transformed_features = sum([len(cats) for cats in self.categories_])\n\n        # validate shape of passed X\n        msg = (\"Shape of the passed X data is not correct. Expected {0} \"\n               \"columns, got {1}.\")\n        if self.encoding == 'ordinal' and X.shape[1] != n_features:\n            raise ValueError(msg.format(n_features, X.shape[1]))\n        elif (self.encoding.startswith('onehot')\n                and X.shape[1] != n_transformed_features):\n            raise ValueError(msg.format(n_transformed_features, X.shape[1]))\n\n        # create resulting array of appropriate dtype\n        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])\n        X_tr = np.empty((n_samples, n_features), dtype=dt)\n\n        if self.encoding == 'ordinal':\n            for i in range(n_features):\n                labels = X[:, i].astype('int64')\n                X_tr[:, i] = self.categories_[i][labels]\n\n        else:  # encoding == 'onehot' / 'onehot-dense'\n            j = 0\n            found_unknown = {}\n\n            for i in range(n_features):\n                n_categories = len(self.categories_[i])\n                sub = X[:, j:j + n_categories]\n\n                # for sparse X argmax returns 2D matrix, ensure 1D array\n                labels = np.asarray(_argmax(sub, axis=1)).flatten()\n                X_tr[:, i] = self.categories_[i][labels]\n\n                if self.handle_unknown == 'ignore':\n                    # ignored unknown categories: we have a row of all zero's\n                    unknown = np.asarray(sub.sum(axis=1) == 0).flatten()\n                    if unknown.any():\n                        found_unknown[i] = unknown\n\n                j += n_categories\n\n            # if ignored are found: potentially need to upcast result to\n            # insert None values\n            if found_unknown:\n                if X_tr.dtype != object:\n                    X_tr = X_tr.astype(object)\n\n                for idx, mask in found_unknown.items():\n                    X_tr[mask, idx] = None\n\n        return X_tr\n    \nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, *attrs):\n        self.attrs = list(attrs)\n    \n    def fit(self, X, *args, **kwargs):\n        return self\n    \n    def transform(self, X, *args, **kwargs):\n        return X[self.attrs]\n    \n    @classmethod\n    def _get_param_names(cls):\n        return ['attrs']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80ab60285ca19ee0d0eee1ff56a12b54de61dba5"},"cell_type":"code","source":"data['family'] = (data.Parch + data.SibSp)\ndata = data.drop(['Parch', 'SibSp'], axis=1)\n\ndata.Age.fillna(data.Age.mean(), inplace=True)\ndata['age_class'] = (data.Age // 5)\n\ndata.Embarked.fillna('S', inplace=True)\n\ndata.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f1375193b3fb7a013fdf8a5cd2bb946cc49a13e","collapsed":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline, make_union, Pipeline, FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\n\nsex = Pipeline([\n    ('selector', DataFrameSelector('Sex')),\n    ('enc', CategoricalEncoder(encoding='ordinal')),\n    ('scaler', StandardScaler())\n])\npclass = Pipeline([\n    ('selector', DataFrameSelector('Pclass')),\n    ('scaler', StandardScaler())\n])\nfamily = Pipeline([\n    ('selector', DataFrameSelector('family')),\n    ('scaler', StandardScaler())\n])\nage = Pipeline([\n    ('selector', DataFrameSelector('age_class')),\n    ('scaler', StandardScaler())\n])\npipe = FeatureUnion(transformer_list=[\n    ('sex', sex),\n    ('pclass', pclass),\n    ('family', family),\n    ('age', age)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"304342cfe36a02dca7cfe591cc97ec99d060fe77"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = data.copy()\nY = data[['Survived']]\n\nX_train, X_test, y_train, y_test = train_test_split(data, Y, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5b773db20d73a922530b155766ef993d4d316c9","scrolled":false},"cell_type":"code","source":"prep_data = pipe.fit_transform(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a797fe52818af041552e65d91a22df7bce9a03aa"},"cell_type":"code","source":"n_inputs = 4\nn_hidden1=300\nn_hidden2=600\nn_hidden3=200\nn_output = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"235838e026eca3c6dad2dd88e5f5ed02a510a6e9","collapsed":true},"cell_type":"code","source":"X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\ny = tf.placeholder(tf.int64, shape=(None), name='y')\n\nwith tf.name_scope('dnn'):\n    hidden1 = tf.layers.dense(X, n_hidden1, name='hidden1', activation=tf.nn.relu)\n    hidden2 = tf.layers.dense(hidden1, n_hidden2, name='hidden2', activation=tf.nn.relu)\n    hidden3 = tf.layers.dense(hidden2, n_hidden3, name='hidden3', activation=tf.nn.relu)\n    logits = tf.layers.dense(hidden3, n_output, name='output')\n    \nwith tf.name_scope('loss'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    loss = tf.reduce_mean(xentropy, name='loss')\n    \nlearning_rate = 0.01\nwith tf.name_scope('train'):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)\n    \nwith tf.name_scope('eval'):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd2abc213710d45dab4223e5f83e00fcb1317949","scrolled":false},"cell_type":"code","source":"n_epochs = 200\nbatch_size = 50\nsubbatch_size = 10\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\n\nn_data = prep_data.shape[0]\nn_batches = n_data // batch_size\nlast_acc_val = 0\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        for iteration in range(n_batches):\n            sb = 0\n            if iteration:\n               sb = subbatch_size\n            \n            start_n = iteration*batch_size\n            x_batch = prep_data[start_n-sb:start_n+batch_size+sb]\n            y_batch = y_train[start_n-sb:start_n+batch_size+sb]['Survived'].values\n            sess.run(training_op, feed_dict = {X: x_batch, y: y_batch})\n        \n        acc_train = accuracy.eval(feed_dict={X: x_batch, y: y_batch})\n        loss_val = loss.eval(feed_dict={X: x_batch, y: y_batch})\n        \n        test_data = pipe.fit_transform(X_test, y_test)\n        acc_val = accuracy.eval(feed_dict={X: test_data, y: y_test['Survived'].values})\n        \n        print(epoch, 'Правильность при обучении:', acc_train, 'Правильность при проверке:', acc_val, 'loss', loss_val)\n        \n        if acc_val > last_acc_val:\n            last_acc_val = acc_val\n            print(epoch, 'Сохранение модели на:', acc_val)\n            save_path = saver.save(sess, './titanic_dnn.ckpt')\n            \n    Z = logits.eval(feed_dict={X: test_data, y: y_test['Survived'].values})\n    y_prep = np.argmax(Z, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5737fb881eb9cc89bfa407709ed3a8d738b9ab08"},"cell_type":"code","source":"with tf.Session() as sess:\n    saver.restore(sess, './titanic_dnn.ckpt')\n    test_data = pipe.fit_transform(X_test, y_test)\n    Z = logits.eval(feed_dict={X: test_data, y: y_test['Survived'].values})\n#     print(Z, y_test['Survived'].values)\n    y_prep = np.argmax(Z, axis=1)\n    \n# pred\nfrom sklearn.metrics import roc_curve, roc_auc_score, mean_squared_error\n\nmse = mean_squared_error(y_test.Survived, y_prep)\nrmse = np.sqrt(mse)\n\nroc_auc_score(y_test.Survived, y_prep), rmse","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}