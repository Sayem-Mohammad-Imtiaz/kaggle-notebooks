{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":7.795557,"end_time":"2021-05-29T21:26:37.327281","exception":false,"start_time":"2021-05-29T21:26:29.531724","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:33.705194Z","iopub.execute_input":"2021-05-30T04:54:33.705653Z","iopub.status.idle":"2021-05-30T04:54:33.715946Z","shell.execute_reply.started":"2021-05-30T04:54:33.705571Z","shell.execute_reply":"2021-05-30T04:54:33.7151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nimport time","metadata":{"papermill":{"duration":4.821015,"end_time":"2021-05-29T21:26:42.163194","exception":false,"start_time":"2021-05-29T21:26:37.342179","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:33.720393Z","iopub.execute_input":"2021-05-30T04:54:33.720781Z","iopub.status.idle":"2021-05-30T04:54:38.755183Z","shell.execute_reply.started":"2021-05-30T04:54:33.720753Z","shell.execute_reply":"2021-05-30T04:54:38.75436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class InstanceNormalization(tf.keras.layers.Layer):\n  \"\"\"Instance Normalization Layer (https://arxiv.org/abs/1607.08022).\"\"\"\n\n  def __init__(self, epsilon=1e-5):\n    super(InstanceNormalization, self).__init__()\n    self.epsilon = epsilon\n\n  def build(self, input_shape):\n    self.scale = self.add_weight(\n        name='scale',\n        shape=input_shape[-1:],\n        initializer=tf.random_normal_initializer(1., 0.02),\n        trainable=True)\n\n    self.offset = self.add_weight(\n        name='offset',\n        shape=input_shape[-1:],\n        initializer='zeros',\n        trainable=True)\n\n  def call(self, x):\n    mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n    inv = tf.math.rsqrt(variance + self.epsilon)\n    normalized = (x - mean) * inv\n    return self.scale * normalized + self.offset\n\n\ndef downsample(filters, size, norm_type='batchnorm', apply_norm=True):\n  \"\"\"Downsamples an input.\n  Conv2D => Batchnorm => LeakyRelu\n  Args:\n    filters: number of filters\n    size: filter size\n    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n    apply_norm: If True, adds the batchnorm layer\n  Returns:\n    Downsample Sequential Model\n  \"\"\"\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(\n      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n  if apply_norm:\n    if norm_type.lower() == 'batchnorm':\n      result.add(tf.keras.layers.BatchNormalization())\n    elif norm_type.lower() == 'instancenorm':\n      result.add(InstanceNormalization())\n\n  result.add(tf.keras.layers.LeakyReLU())\n\n  return result\n\n\ndef upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n  \"\"\"Upsamples an input.\n  Conv2DTranspose => Batchnorm => Dropout => Relu\n  Args:\n    filters: number of filters\n    size: filter size\n    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n    apply_dropout: If True, adds the dropout layer\n  Returns:\n    Upsample Sequential Model\n  \"\"\"\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(\n      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n  if norm_type.lower() == 'batchnorm':\n    result.add(tf.keras.layers.BatchNormalization())\n  elif norm_type.lower() == 'instancenorm':\n    result.add(InstanceNormalization())\n\n  if apply_dropout:\n    result.add(tf.keras.layers.Dropout(0.5))\n\n  result.add(tf.keras.layers.ReLU())\n\n  return result\n\n\ndef unet_generator(output_channels, norm_type='batchnorm'):\n  \"\"\"Modified u-net generator model (https://arxiv.org/abs/1611.07004).\n  Args:\n    output_channels: Output channels\n    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n  Returns:\n    Generator model\n  \"\"\"\n\n  down_stack = [\n      downsample(64, 4, norm_type, apply_norm=False),  # (bs, 128, 128, 64)\n      downsample(128, 4, norm_type),  # (bs, 64, 64, 128)\n      downsample(256, 4, norm_type),  # (bs, 32, 32, 256)\n      downsample(512, 4, norm_type),  # (bs, 16, 16, 512)\n      downsample(512, 4, norm_type),  # (bs, 8, 8, 512)\n      downsample(512, 4, norm_type),  # (bs, 4, 4, 512)\n      downsample(512, 4, norm_type),  # (bs, 2, 2, 512)\n      downsample(512, 4, norm_type),  # (bs, 1, 1, 512)\n  ]\n\n  up_stack = [\n      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 2, 2, 1024)\n      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 4, 4, 1024)\n      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 8, 8, 1024)\n      upsample(512, 4, norm_type),  # (bs, 16, 16, 1024)\n      upsample(256, 4, norm_type),  # (bs, 32, 32, 512)\n      upsample(128, 4, norm_type),  # (bs, 64, 64, 256)\n      upsample(64, 4, norm_type),  # (bs, 128, 128, 128)\n  ]\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n  last = tf.keras.layers.Conv2DTranspose(\n      output_channels, 4, strides=2,\n      padding='same', kernel_initializer=initializer,\n      activation='tanh')  # (bs, 256, 256, 3)\n\n  concat = tf.keras.layers.Concatenate()\n\n  inputs = tf.keras.layers.Input(shape=[256, 256, 1])\n  x = inputs\n\n  # Downsampling through the model\n  skips = []\n  for down in down_stack:\n    x = down(x)\n    skips.append(x)\n\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    x = concat([x, skip])\n\n  x = last(x)\n\n  return tf.keras.Model(inputs=inputs, outputs=x)\n\n\ndef discriminator(norm_type='batchnorm', target=True):\n  \"\"\"PatchGan discriminator model (https://arxiv.org/abs/1611.07004).\n  Args:\n    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n    target: Bool, indicating whether target image is an input or not.\n  Returns:\n    Discriminator model\n  \"\"\"\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  inp = tf.keras.layers.Input(shape=[256, 256, 1], name='input_image')\n  x = inp\n\n  if target:\n    tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n    x = tf.keras.layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)\n\n  down1 = downsample(64, 4, norm_type, False)(x)  # (bs, 128, 128, 64)\n  down2 = downsample(128, 4, norm_type)(down1)  # (bs, 64, 64, 128)\n  down3 = downsample(256, 4, norm_type)(down2)  # (bs, 32, 32, 256)\n\n  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n  conv = tf.keras.layers.Conv2D(\n      512, 4, strides=1, kernel_initializer=initializer,\n      use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n\n  if norm_type.lower() == 'batchnorm':\n    norm1 = tf.keras.layers.BatchNormalization()(conv)\n  elif norm_type.lower() == 'instancenorm':\n    norm1 = InstanceNormalization()(conv)\n\n  leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n\n  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n\n  last = tf.keras.layers.Conv2D(\n      1, 4, strides=1,\n      kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n\n  if target:\n    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n  else:\n    return tf.keras.Model(inputs=inp, outputs=last)","metadata":{"papermill":{"duration":0.044123,"end_time":"2021-05-29T21:26:42.221159","exception":false,"start_time":"2021-05-29T21:26:42.177036","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:38.756767Z","iopub.execute_input":"2021-05-30T04:54:38.757086Z","iopub.status.idle":"2021-05-30T04:54:38.785942Z","shell.execute_reply.started":"2021-05-30T04:54:38.757052Z","shell.execute_reply":"2021-05-30T04:54:38.785073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'\nfiles = [os.path.join(path + file) for file in os.listdir(path)]\nfiles = files[:5000]","metadata":{"papermill":{"duration":0.035554,"end_time":"2021-05-29T21:26:42.269789","exception":false,"start_time":"2021-05-29T21:26:42.234235","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:38.788214Z","iopub.execute_input":"2021-05-30T04:54:38.788597Z","iopub.status.idle":"2021-05-30T04:54:41.386136Z","shell.execute_reply.started":"2021-05-30T04:54:38.788562Z","shell.execute_reply":"2021-05-30T04:54:41.385313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load(image):\n    image = tf.io.read_file(image)\n    color_image = tf.image.decode_jpeg(image, channels=3)\n    color_image = tf.image.resize(color_image, [256,256])\n    bw_image = tf.image.rgb_to_grayscale(color_image)\n    color_image = tf.cast(color_image, tf.float32)\n    bw_image = tf.cast(bw_image, tf.float32)\n    color_image = color_image/127.5 - 1\n    bw_image = bw_image/ 127.5 - 1\n    return bw_image, color_image","metadata":{"papermill":{"duration":0.020671,"end_time":"2021-05-29T21:26:42.30387","exception":false,"start_time":"2021-05-29T21:26:42.283199","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:41.38857Z","iopub.execute_input":"2021-05-30T04:54:41.389157Z","iopub.status.idle":"2021-05-30T04:54:41.395048Z","shell.execute_reply.started":"2021-05-30T04:54:41.389117Z","shell.execute_reply":"2021-05-30T04:54:41.394236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((files))\ntrain_dataset = train_dataset.map(load).shuffle(1000).batch(1)","metadata":{"papermill":{"duration":1.917093,"end_time":"2021-05-29T21:26:44.234794","exception":false,"start_time":"2021-05-29T21:26:42.317701","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:41.397726Z","iopub.execute_input":"2021-05-30T04:54:41.39797Z","iopub.status.idle":"2021-05-30T04:54:43.161587Z","shell.execute_reply.started":"2021-05-30T04:54:41.397939Z","shell.execute_reply":"2021-05-30T04:54:43.160771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator = unet_generator(output_channels=3, norm_type=\"batchnorm\")\ngenerator.summary()","metadata":{"papermill":{"duration":0.946272,"end_time":"2021-05-29T21:26:45.195179","exception":false,"start_time":"2021-05-29T21:26:44.248907","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:43.163598Z","iopub.execute_input":"2021-05-30T04:54:43.16384Z","iopub.status.idle":"2021-05-30T04:54:43.91559Z","shell.execute_reply.started":"2021-05-30T04:54:43.163816Z","shell.execute_reply":"2021-05-30T04:54:43.914785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator = discriminator(norm_type=\"batchnorm\")\ndiscriminator.summary()","metadata":{"papermill":{"duration":0.117144,"end_time":"2021-05-29T21:26:45.326475","exception":false,"start_time":"2021-05-29T21:26:45.209331","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:43.918326Z","iopub.execute_input":"2021-05-30T04:54:43.918568Z","iopub.status.idle":"2021-05-30T04:54:44.021905Z","shell.execute_reply.started":"2021-05-30T04:54:43.918543Z","shell.execute_reply":"2021-05-30T04:54:44.021147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LAMBDA = 100\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef generator_loss(disc_generated_output, gen_output, target):\n    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n\n    # mean absolute error\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n\n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n\n    return total_gen_loss, gan_loss, l1_loss","metadata":{"papermill":{"duration":0.022555,"end_time":"2021-05-29T21:26:45.363473","exception":false,"start_time":"2021-05-29T21:26:45.340918","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:44.024581Z","iopub.execute_input":"2021-05-30T04:54:44.02491Z","iopub.status.idle":"2021-05-30T04:54:44.029847Z","shell.execute_reply.started":"2021-05-30T04:54:44.024876Z","shell.execute_reply":"2021-05-30T04:54:44.028871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discriminator_loss(disc_real_output, disc_generated_output):\n    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n\n    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n\n    total_disc_loss = real_loss + generated_loss\n\n    return total_disc_loss","metadata":{"papermill":{"duration":0.021552,"end_time":"2021-05-29T21:26:45.399145","exception":false,"start_time":"2021-05-29T21:26:45.377593","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:44.031471Z","iopub.execute_input":"2021-05-30T04:54:44.032212Z","iopub.status.idle":"2021-05-30T04:54:44.038897Z","shell.execute_reply.started":"2021-05-30T04:54:44.032176Z","shell.execute_reply":"2021-05-30T04:54:44.038058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"papermill":{"duration":0.021475,"end_time":"2021-05-29T21:26:45.434853","exception":false,"start_time":"2021-05-29T21:26:45.413378","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:44.041789Z","iopub.execute_input":"2021-05-30T04:54:44.04206Z","iopub.status.idle":"2021-05-30T04:54:44.048107Z","shell.execute_reply.started":"2021-05-30T04:54:44.042037Z","shell.execute_reply":"2021-05-30T04:54:44.047263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_path = \"outputs\"\nif not os.path.exists(output_path):\n    os.mkdir(output_path)\n    \ndef generate_images(model, test_input, tar, epoch):\n    prediction = model(test_input, training=True)\n    display_list = [test_input[0] * 0.5 + 0.5, tar[0]*0.5+0.5, prediction[0] * 0.5 + 0.5]\n    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n    fig, axes = plt.subplots(1,3, figsize = (15,6))\n    for i in range(3):\n        if display_list[i].shape[2] == 1:\n            axes[i].imshow(display_list[i], cmap = \"gray\")\n        else:\n            axes[i].imshow(display_list[i])\n        # getting the pixel values between [0, 1] to plot it.\n    plt.axis('off')\n    plt.savefig(f\"{output_path}/epoch_{epoch}.jpg\")","metadata":{"papermill":{"duration":0.024142,"end_time":"2021-05-29T21:26:45.473297","exception":false,"start_time":"2021-05-29T21:26:45.449155","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:44.051017Z","iopub.execute_input":"2021-05-30T04:54:44.051331Z","iopub.status.idle":"2021-05-30T04:54:44.059179Z","shell.execute_reply.started":"2021-05-30T04:54:44.051288Z","shell.execute_reply":"2021-05-30T04:54:44.058171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color, bw = next(iter(train_dataset))\ncolor.shape, bw.shape","metadata":{"papermill":{"duration":8.576722,"end_time":"2021-05-29T21:26:54.064152","exception":false,"start_time":"2021-05-29T21:26:45.48743","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:44.060534Z","iopub.execute_input":"2021-05-30T04:54:44.060918Z","iopub.status.idle":"2021-05-30T04:54:50.029729Z","shell.execute_reply.started":"2021-05-30T04:54:44.060883Z","shell.execute_reply":"2021-05-30T04:54:50.028944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_images(generator, color, bw, 0)","metadata":{"papermill":{"duration":4.270604,"end_time":"2021-05-29T21:26:58.349501","exception":false,"start_time":"2021-05-29T21:26:54.078897","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:50.031012Z","iopub.execute_input":"2021-05-30T04:54:50.031568Z","iopub.status.idle":"2021-05-30T04:54:53.984245Z","shell.execute_reply.started":"2021-05-30T04:54:50.031528Z","shell.execute_reply":"2021-05-30T04:54:53.983424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(input_image, target, epoch):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        gen_output = generator(input_image, training=True)\n\n        disc_real_output = discriminator([input_image, target], training=True)\n        disc_generated_output = discriminator([input_image, gen_output], training=True)\n\n        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n    generator_gradients = gen_tape.gradient(gen_total_loss,\n                                          generator.trainable_variables)\n    discriminator_gradients = disc_tape.gradient(disc_loss,\n                                               discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(generator_gradients,\n                                          generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n                                              discriminator.trainable_variables))\n    return gen_total_loss, disc_loss","metadata":{"papermill":{"duration":0.033179,"end_time":"2021-05-29T21:26:58.406639","exception":false,"start_time":"2021-05-29T21:26:58.37346","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:53.985422Z","iopub.execute_input":"2021-05-30T04:54:53.985734Z","iopub.status.idle":"2021-05-30T04:54:53.994703Z","shell.execute_reply.started":"2021-05-30T04:54:53.985698Z","shell.execute_reply":"2021-05-30T04:54:53.993901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit(train_ds, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n        for example_input, example_target in train_ds.take(1):\n            generate_images(generator, example_input, example_target, epoch)\n        print(\"Epoch: \", epoch+1)\n\n        # Train\n        for n, (input_image, target) in train_ds.enumerate():\n            gen_loss, disc_loss = train_step(input_image, target, epoch)\n\n        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                                            time.time()-start))\n        print(\"Epoch {} Gen loss: {:.2f} Disc loss: {:.2f}\".format(epoch+1, gen_loss, disc_loss))\n        generator.save_weights(\"generator.h5\")\n        discriminator.save_weights(\"discriminator.h5\")","metadata":{"papermill":{"duration":0.032397,"end_time":"2021-05-29T21:26:58.462158","exception":false,"start_time":"2021-05-29T21:26:58.429761","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:53.996267Z","iopub.execute_input":"2021-05-30T04:54:53.996884Z","iopub.status.idle":"2021-05-30T04:54:54.007352Z","shell.execute_reply.started":"2021-05-30T04:54:53.996844Z","shell.execute_reply":"2021-05-30T04:54:54.006508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 30\nfit(train_dataset, epochs)","metadata":{"papermill":{"duration":11296.576028,"end_time":"2021-05-30T00:35:15.061447","exception":false,"start_time":"2021-05-29T21:26:58.485419","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2021-05-30T04:54:54.008815Z","iopub.execute_input":"2021-05-30T04:54:54.009179Z","iopub.status.idle":"2021-05-30T04:55:06.953166Z","shell.execute_reply.started":"2021-05-30T04:54:54.009143Z","shell.execute_reply":"2021-05-30T04:55:06.951197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.335816,"end_time":"2021-05-30T00:35:15.735613","exception":false,"start_time":"2021-05-30T00:35:15.399797","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}