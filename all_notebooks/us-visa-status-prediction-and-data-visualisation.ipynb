{"metadata":{"kernelspec":{"language":"python","name":"python2","display_name":"Python 2"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":2},"name":"python","version":"2.7.12","pygments_lexer":"ipython2","file_extension":".py"}},"cells":[{"cell_type":"code","metadata":{"collapsed":true},"source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import grid_search\nfrom random import seed\nfrom random import randrange\n\nfrom sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVR\n\nimport xgboost\nimport pickle\n%matplotlib inline","outputs":[],"execution_count":66},{"cell_type":"markdown","metadata":{},"source":"### Reading the raw data into a dataframe"},{"cell_type":"code","metadata":{"collapsed":true},"source":"#decision_date and case_recieved_date are read as dates\ndf = pd.read_csv('us_perm_visas.csv', low_memory = False, parse_dates=['decision_date', 'case_received_date'],\\\n                 index_col = ['case_no'])","outputs":[],"execution_count":2},{"cell_type":"code","metadata":{"collapsed":true},"source":"#Removing all withdrawn applications\ndf = df[df.case_status != 'Withdrawn']\n\n#Combining certified-expired and certified applications\ndf.loc[df.case_status == 'Certified-Expired', 'case_status'] = 'Certified'\n\n#Now only two labels, Certified and Denied, which are stored in the column case_status","outputs":[],"execution_count":3},{"cell_type":"code","metadata":{"collapsed":true},"source":"#Convering the date to just the year\ndf['year'] = df['case_received_date'].dt.year","outputs":[],"execution_count":4},{"cell_type":"code","metadata":{"collapsed":true},"source":"#Dropping all empty columns\ndf = df.dropna(axis=1, how='all');\n\n#Dropping all empty rows\ndf = df.dropna(axis=0, how='all');","outputs":[],"execution_count":5},{"cell_type":"code","metadata":{},"source":"#Dropping all rows with any missing values\ndf_nona = df.dropna(axis=0, how='any');\n\ndf_nona.info()\n\n#This shows that dropping rows with empty values is not an option","outputs":[],"execution_count":6},{"cell_type":"markdown","metadata":{},"source":"### Visualisation of the Unprocessed Data"},{"cell_type":"code","metadata":{"collapsed":true},"source":"pd.set_option('display.max_colwidth', -1)","outputs":[],"execution_count":7},{"cell_type":"code","metadata":{"collapsed":true},"source":"i = 0\ncount_nonnan = [];\nct = [];\n\nfor col in df.columns:\n    i+=1\n    count_nonnan.append(df[col].count())\n    ct.append(i)","outputs":[],"execution_count":8},{"cell_type":"code","metadata":{},"source":"plt.scatter(count_nonnan, ct)\nplt.ylabel('Feature Number', fontsize=16)\nplt.xlabel('Number of non-empty entries', fontsize=16)\n\n#This is a scatterplot to show the number of non-empty values per feature","outputs":[],"execution_count":9},{"cell_type":"code","metadata":{},"source":"#Total Number of data points of each class\ndf['case_status'].value_counts().plot(kind = 'bar')","outputs":[],"execution_count":10},{"cell_type":"code","metadata":{},"source":"print('There are', df['country_of_citizenship'].value_counts().count(), \\\n      'different countries of citizenship in this dataset')\ndf['country_of_citizenship'].value_counts().head(10).plot(kind = 'barh',color = 'maroon', alpha=0.7)\n\n#Graph showing the number of applicants from the countried with the 10 highest number of applicants and the total number of countries","outputs":[],"execution_count":11},{"cell_type":"code","metadata":{},"source":"#Graph showing the number of Certified and Denied visa applications for the top 10 countries by number of visa applicants\n\nsns.set_style(\"whitegrid\")\nfig, ax = plt.subplots()\nfig.set_size_inches(11.7, 8.27)\nsns.countplot(ax = ax, x='country_of_citizenship', data = df, hue='case_status', \\\n              order=df.country_of_citizenship.value_counts().iloc[:10].index,alpha = 0.5)#, palette=pkmn_type_colors)","outputs":[],"execution_count":12},{"cell_type":"markdown","metadata":{},"source":"### Feature Selection"},{"cell_type":"code","metadata":{},"source":"#Displaying percentage of non-null values for each feature\n\ni = 0;\nfor col in df.columns:\n    i = i+1;\n    print (i-1, col, 100*df[col].count()/len(df['case_status']))","outputs":[],"execution_count":13},{"cell_type":"code","metadata":{},"source":"#Finding relation between case_received_date and decision_date\ndf[['case_received_date', 'decision_date']]\n\n#Since they are almost the same, and the decision_date is 100% filled, we can drop case_recieved_date","outputs":[],"execution_count":14},{"cell_type":"code","metadata":{},"source":"#Indices of selected features\na = [2,3,7,8,9,11,15,17,18,22,23,77,90,96,98,101];\n\n#These features were selected based on the number of empty values and relevance to the visa application decision\n\ndf2 = df.iloc[:,a];\ndf2 = df2.dropna(thresh=0.5*len(df), axis=1);\ndf2.info()\n\n#selected features are displayed","outputs":[],"execution_count":15},{"cell_type":"markdown","metadata":{},"source":"### Selected Features"},{"cell_type":"code","metadata":{"collapsed":true},"source":"import matplotlib.colors as mcolors\n\ndef make_colormap(seq):\n    \"\"\"Return a LinearSegmentedColormap\n    seq: a sequence of floats and RGB-tuples. The floats should be increasing\n    and in the interval (0,1).\n    \"\"\"\n    seq = [(None,) * 3, 0.0] + list(seq) + [1.0, (None,) * 3]\n    cdict = {'red': [], 'green': [], 'blue': []}\n    for i, item in enumerate(seq):\n        if isinstance(item, float):\n            r1, g1, b1 = seq[i - 1]\n            r2, g2, b2 = seq[i + 1]\n            cdict['red'].append([item, r1, r2])\n            cdict['green'].append([item, g1, g2])\n            cdict['blue'].append([item, b1, b2])\n\n    return mcolors.LinearSegmentedColormap('CustomMap', cdict)\n\nc = mcolors.ColorConverter().to_rgb\nrvb = make_colormap(\n[c('red'), 0.125, c('red'), c('orange'), 0.25, c('orange'),c('green'),0.5, c('green'),0.7, c('green'), c('blue'), 0.75, c('blue')])\n\nN = 60\nx = np.arange(N).astype(float)\n#y = np.random.uniform(0, 5, size=(N,))\n\n#plt.bar(x,y, color=rvb(x/N),alpha = 0.5)\n#plt.show()\n","outputs":[],"execution_count":16},{"cell_type":"code","metadata":{},"source":"\n#Displaying percentage of non-null values for each selected feature\nobjects = []\ncounts = []\ni = 0;\nfor col in df2.columns:\n    i = i+1;\n    objects.append(col)\n    counts.append(100*df2[col].count()/len(df2['case_status']))\n    \ny_pos = np.arange(len(objects))\nplt.figure(figsize=(10, 10))  # width:20, height:3\nplt.barh(y_pos, counts, align='edge', alpha=0.5, color = rvb(x/30))\nplt.yticks(y_pos, objects)\nplt.xlabel('Percentage Filled')\nplt.title('Selected Features')\n \nplt.show()\n\n#Features with more than 90% non-empty values can be filled in using the mean/mode\n#Features with lesser than 90% non-empty values, other methods are used","outputs":[],"execution_count":17},{"cell_type":"markdown","metadata":{},"source":"### Data Cleaning"},{"cell_type":"code","metadata":{"collapsed":true},"source":"#Assigning Labels to Case Status\ndf2.loc[df.case_status == 'Certified', 'case_status'] = 1\ndf2.loc[df.case_status == 'Denied', 'case_status'] = 0\n\n#Calculating pay based on unit of pay and wage\ndi = {\"Year\": 1, \"yr\": 1, \"Hour\": 2080, \"hr\": 2080, \"Week\": 52, \"wk\": 52, \"Bi-Weekly\": 26,\"bi\": 26, \"Month\": 12, \"mth\": 12}\ndf2['pw_unit_of_pay_9089'].replace(di, inplace=True)\n\n#Changing datatype of pay to float\ndf2['pw_amount_9089'] = df2['pw_amount_9089'].str.replace(',', '')\ndf2[['pw_amount_9089', 'pw_unit_of_pay_9089']] = df2[['pw_amount_9089', 'pw_unit_of_pay_9089']].astype(float)\ndf2['pw_amount_9089_new'] = df2['pw_amount_9089']*df2['pw_unit_of_pay_9089']\ndf2['pw_amount_9089_new']=df2['pw_amount_9089_new'].fillna((df2['pw_amount_9089_new'].mean()))\n\n#Dropping redundant pay related features\ndf2 = df2.drop('pw_amount_9089', 1)\ndf2 = df2.drop('pw_unit_of_pay_9089', 1)","outputs":[],"execution_count":18},{"cell_type":"code","metadata":{"collapsed":true},"source":"#Extracting year from decision date and converting to an integer value\ndf2['decision_date'] = df2['decision_date'].dt.year\ndf2['decision_date'] = df2['decision_date'].astype(int)\ndf2['decision_date']=df2['decision_date'].fillna((df2['decision_date'].mean()))","outputs":[],"execution_count":19},{"cell_type":"code","metadata":{"collapsed":true},"source":"#Since there are too many unknown values, it is treated as a new value\ndf2['agent_state'].fillna('Unknown');\n\n#Mapping from state name to short-form\nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY',\n    'Northern Mariana Islands':'MP', \n    'Palau': 'PW', \n    'Puerto Rico': 'PR', \n    'Virgin Islands': 'VI', \n    'District of Columbia': 'DC'\n}\n\n#Capitalizing Keys\nus_state_abbrev = {k.upper(): v for k, v in us_state_abbrev.items()}\ndf2['agent_state'].replace(us_state_abbrev, inplace=True)\ndf2.agent_state = df2.agent_state.astype(str)\n","outputs":[],"execution_count":20},{"cell_type":"code","metadata":{"collapsed":true},"source":"#Similarly for Employer State\ndf2['employer_state'] = df2['employer_state'].fillna(df2['employer_state'].mode()[0]);\n\n#Capitalizing Keys\nus_state_abbrev = {k.upper(): v for k, v in us_state_abbrev.items()}\ndf2['employer_state'].replace(us_state_abbrev, inplace=True)\ndf2.employer_state = df2.employer_state.astype(str)\n\n\n#Similarly for Job info work State\ndf2['job_info_work_state'] = df2['job_info_work_state'].fillna(df2['job_info_work_state'].mode()[0]);\n\n#Capitalizing Keys\nus_state_abbrev = {k.upper(): v for k, v in us_state_abbrev.items()}\ndf2['job_info_work_state'].replace(us_state_abbrev, inplace=True)\ndf2.job_info_work_state = df2.job_info_work_state.astype(str)\n\n","outputs":[],"execution_count":21},{"cell_type":"code","metadata":{},"source":"#df2['employer_postal_code'] = df2['employer_postal_code'].str.replace('.','')\n#df2['employer_postal_code'] = df2['employer_postal_code'].str.replace('-','')\n#df2['employer_postal_code'] = df2['employer_postal_code'].astype(str).str[0:4]\n#empty_emp_index = df2['employer_postal_code'] == \"nan\"\n#df2['employer_postal_code'][empty_emp_index] = '0000';\n#not_num_values = df2['employer_postal_code'].str.contains('[a-zA-Z]')\n#df2['employer_postal_code'][not_num_values] = '0000';\n#df2['employer_postal_code'] = df2['employer_postal_code'].astype(int)\ndf2['pw_soc_code'] = df2['pw_soc_code'].str.replace('.','')\ndf2['pw_soc_code'] = df2['pw_soc_code'].str.replace('-','')\ndf2['pw_soc_code'] = df2['pw_soc_code'].astype(str).str[0:6]\ndf2['pw_soc_code'].value_counts()\nempty_soc_index = df2['pw_soc_code'] == \"nan\"\ndf2['pw_soc_code'][empty_soc_index] = df2['pw_soc_code'].mode()[0];\n\nempty_soc_index = df2['pw_soc_code'] == \"None\"\ndf2['pw_soc_code'][empty_soc_index] = df2['pw_soc_code'].mode()[0];","outputs":[],"execution_count":22},{"cell_type":"code","metadata":{"collapsed":true},"source":"df2['pw_soc_code'] = df2['pw_soc_code'].astype(float)\n\ndf2['pw_soc_code'] = df2['pw_soc_code'].astype(int)\n\ndf2['case_status'] = df2['case_status'].astype(int)","outputs":[],"execution_count":23},{"cell_type":"code","metadata":{},"source":"#Displaying percentage of non-null values for each selected feature\n\ni = 0;\nfor col in df2.columns:\n    i = i+1;\n    print (i-1, col, 100*df2[col].count()/len(df2['case_status']))","outputs":[],"execution_count":24},{"cell_type":"code","metadata":{"collapsed":true},"source":"#Replace the values with the mode or mean\ndf2['class_of_admission']=df2['class_of_admission'].fillna((df2['class_of_admission'].mode()[0]))\n\ndf2['country_of_citizenship']=df2['country_of_citizenship'].fillna((df2['country_of_citizenship'].mode()[0]))\n\n#df2['employer_city']=df2['employer_city'].fillna((df2['employer_city'].mode()[0]))\n\ndf2['employer_name']=df2['employer_name'].fillna((df2['employer_name'].mode()[0]))\ndf2['employer_name']=df2['employer_name'].astype(str).str.upper()\n\n\n#df2['job_info_work_city']=df2['job_info_work_city'].astype(str).str.upper()\n#df2['job_info_work_city']=df2['job_info_work_city'].fillna((df2['job_info_work_city'].mode()[0].upper()))\n\n\ndf2['pw_source_name_9089']=df2['pw_source_name_9089'].fillna((df2['pw_source_name_9089'].mode()[0]))\n\n\ndf2['pw_soc_code'] = df2['pw_soc_code'].astype(str)\ndf2['pw_soc_code'] = df2['pw_soc_code'].str.replace('-', '')\ndf2['pw_soc_code'] = df2['pw_soc_code'].str[0:6]\n\ndf2['pw_soc_code']=df2['pw_soc_code'].fillna((df2['pw_soc_code'].mode()[0]))\n\ndf2['employer_yr_estab']=df2['employer_yr_estab'].fillna(1700.0)\ndf2['employer_yr_estab'] = df2['employer_yr_estab'].astype(int)\ndf2['employer_yr_estab'].value_counts()\ndf2.loc[df['employer_yr_estab'] < 1700, 'employer_yr_estab'] = 1700\n\ndf2['employer_country']=df2['employer_country'].fillna((df2['employer_country'].mode()[0]))\n\ndf2['employer_num_employees']=df2['employer_num_employees'].fillna((df2['employer_num_employees'].mean()))\n","outputs":[],"execution_count":25},{"cell_type":"code","metadata":{},"source":"#Displaying percentage of non-null values for each selected feature\n\ni = 0;\nfor col in df2.columns:\n    i = i+1;\n    print (i-1, col, 100*df2[col].count()/len(df2['case_status']))","outputs":[],"execution_count":26},{"cell_type":"markdown","metadata":{},"source":"### Visualizing the filled features"},{"cell_type":"code","metadata":{},"source":"df2['pw_amount_9089_new'].head(25).plot(kind='hist',color = 'purple')\n","outputs":[],"execution_count":27},{"cell_type":"code","metadata":{},"source":"df2['employer_state'].value_counts().head(15).plot(kind='barh',color= rvb(x/20),alpha = 0.8)","outputs":[],"execution_count":28},{"cell_type":"code","metadata":{},"source":"df2['class_of_admission'].value_counts().head(15).plot(kind='barh',alpha = 0.9, color ='pink')","outputs":[],"execution_count":29},{"cell_type":"markdown","metadata":{},"source":"### Converting all data to Categories"},{"cell_type":"code","metadata":{},"source":"converter_dict = {}\n#Categorising agent firm name\ndf2.agent_firm_name = df2.agent_firm_name.astype(str)\n\nle_agent_state = preprocessing.LabelEncoder()\nle_agent_state.fit(df2['agent_state'])\ndf2['agent_state'] = le_agent_state.transform(df2['agent_state'])\nconverter_dict[\"agent_state\"] = le_agent_state\n\nle_employer_state = preprocessing.LabelEncoder()\nle_employer_state.fit(df2['employer_state'])\ndf2['employer_state'] = le_employer_state.transform(df2['employer_state'])\nconverter_dict[\"employer_state\"] = le_employer_state\n\nle_agent_firm_name = preprocessing.LabelEncoder()\nle_agent_firm_name.fit(df2['agent_firm_name'])\ndf2['agent_firm_name'] = le_agent_firm_name.transform(df2['agent_firm_name'])\nconverter_dict[\"agent_firm_name\"] = le_agent_firm_name\n\nle_class_of_admission = preprocessing.LabelEncoder()\nle_class_of_admission.fit(df2['class_of_admission'])\ndf2['class_of_admission'] = le_class_of_admission.transform(df2['class_of_admission'])\nconverter_dict[\"class_of_admission\"] = le_class_of_admission\n\nle_country_of_citizenship = preprocessing.LabelEncoder()\nle_country_of_citizenship.fit(df2['country_of_citizenship'])\ndf2['country_of_citizenship'] = le_country_of_citizenship.transform(df2['country_of_citizenship'])\nconverter_dict[\"country_of_citizenship\"] = le_country_of_citizenship\n\n#employer_city\n#le_employer_city = preprocessing.LabelEncoder()\n#le_employer_city.fit(df2['employer_city'])\n#df2['employer_city'] = le_employer_city.transform(df2['employer_city'])\n#converter_dict[\"employer_city\"] = le_employer_city\n\n#employer_country\nle_employer_country = preprocessing.LabelEncoder()\nle_employer_country.fit(df2['employer_country'])\ndf2['employer_country'] = le_employer_country.transform(df2['employer_country'])\nconverter_dict[\"employer_country\"] = le_employer_country\n\n#employer_name\nle_employer_name = preprocessing.LabelEncoder()\nle_employer_name.fit(df2['employer_name'])\ndf2['employer_name'] = le_employer_name.transform(df2['employer_name'])\nconverter_dict[\"employer_name\"] = le_employer_name\n\n#job_info_work_ciity\n#le_job_info_work_city = preprocessing.LabelEncoder()\n#le_job_info_work_city.fit(df2['job_info_work_city'])\n#df2['job_info_work_city'] = le_job_info_work_city.transform(df2['job_info_work_city'])\n#converter_dict[\"job_info_work_city\"] = le_job_info_work_city\n\n#job_info_work_state\nle_job_info_work_state = preprocessing.LabelEncoder()\nle_job_info_work_state.fit(df2['job_info_work_state'])\ndf2['job_info_work_state'] = le_job_info_work_state.transform(df2['job_info_work_state'])\nconverter_dict[\"job_info_work_state\"] = le_job_info_work_state\n\n#pw_source_name_9089\nle_pw_source_name_9089 = preprocessing.LabelEncoder()\nle_pw_source_name_9089.fit(df2['pw_source_name_9089'])\ndf2['pw_source_name_9089'] = le_pw_source_name_9089.transform(df2['pw_source_name_9089'])\nconverter_dict[\"pw_source_name_9089\"] = le_pw_source_name_9089\n\nprint converter_dict\n\nf = open(\"saved_mapping\",\"wb\")\npickle.dump(converter_dict, f)\nf.close()","outputs":[],"execution_count":30},{"cell_type":"markdown","metadata":{},"source":"## Application Decision Prediction"},{"cell_type":"markdown","metadata":{},"source":"### Random Forest Classifier"},{"cell_type":"code","metadata":{},"source":"X = df2.loc[:, df2.columns != 'case_status']\nY = df2.case_status\n\nf = open(\"data_d2\",\"wb\")\npickle.dump(df2, f)\nf.close()\n\ndf2.iloc[0]","outputs":[],"execution_count":31},{"cell_type":"code","metadata":{},"source":"X.shape","outputs":[],"execution_count":32},{"cell_type":"code","metadata":{},"source":"Y.shape","outputs":[],"execution_count":33},{"cell_type":"code","metadata":{"collapsed":true},"source":"#X_try = [[1,1,3,2],[2,3,4,3]]\n#Y_try = [1,0]","outputs":[],"execution_count":34},{"cell_type":"code","metadata":{"collapsed":true},"source":"#estimator = SVR(kernel=\"linear\")\n#selector = RFE(estimator, 1, step=2,verbose=True)\n#selector = selector.fit(X_try, Y_try)\n#selector.support_\n#selector.ranking_","outputs":[],"execution_count":35},{"cell_type":"code","metadata":{"collapsed":true},"source":"#estimator = SVR(kernel=\"linear\")\n#selector = RFE(estimator, 5, step=16,verbose=True)\n#selector = selector.fit(X[0:1000], Y[0:1000])\n#selector.support_\n#selector.ranking_","outputs":[],"execution_count":36},{"cell_type":"code","metadata":{"collapsed":true},"source":"skf = StratifiedKFold(n_splits = 5)","outputs":[],"execution_count":37},{"cell_type":"code","metadata":{"collapsed":true},"source":"#Parameters for random forest. To perform hyper parameter optimisation a list of multiple elements can be entered\n#and the optimal value in that list will be picked using grid search\ndef parameter_set_random_forest(n_estimators = [10], criterion = ['gini'], max_depth = [None],\\\n                                min_samples_split = [2], min_samples_leaf = [1], min_weight_fraction_leaf = [0.0],\\\n                                max_features = ['auto'], max_leaf_nodes = [None], bootstrap = [True],\\\n                                oob_score = [False], random_state = [None], verbose = [0],warm_start = [False],\\\n                                class_weight = [None]):\n    \n    parameters_random_forest = {}\n    parameters_random_forest['criterion'] = criterion\n    parameters_random_forest['n_estimators'] = n_estimators\n    parameters_random_forest['max_depth'] = max_depth\n    parameters_random_forest['min_samples_split'] = min_samples_split\n    parameters_random_forest['min_samples_leaf'] = min_samples_leaf\n    parameters_random_forest['min_weight_fraction_leaf'] = min_weight_fraction_leaf\n    parameters_random_forest['max_features'] = max_features\n    parameters_random_forest['random_state'] = random_state\n    parameters_random_forest['max_leaf_nodes'] = max_leaf_nodes\n    parameters_random_forest['class_weight'] = class_weight\n    parameters_random_forest['bootstrap'] = bootstrap\n    parameters_random_forest['oob_score'] = oob_score\n    parameters_random_forest['warm_start'] = warm_start\n    \n    return parameters_random_forest","outputs":[],"execution_count":38},{"cell_type":"code","metadata":{},"source":"models = []\nfor train_index, test_index in skf.split(X, Y):\n    x_train = X.iloc[train_index,:]\n    x_test = X.iloc[test_index,:]\n    y_train = Y.iloc[train_index]\n    y_test = Y.iloc[test_index]\n    print(\"TRAIN:\", x_train.shape, \"TEST:\", x_test.shape)\n    random_forest_model = RandomForestClassifier()\n    parameters_random_forest = parameter_set_random_forest(n_estimators=[10,20,30,40],max_depth=[35,50,75,100,None])\n    model_gs = grid_search.GridSearchCV(random_forest_model, parameters_random_forest, scoring = 'roc_auc')\n    model_gs.fit(x_train,y_train)\n    predictions = model_gs.predict_proba(x_test)[:,1]\n    train_acc = roc_auc_score(y_train, model_gs.predict_proba(x_train)[:,1])\n    test_acc = roc_auc_score(y_test, predictions)\n    print (\"Train Accuracy :: \", roc_auc_score(y_train, model_gs.predict_proba(x_train)[:,1]))\n    print (\"Test Accuracy  :: \", roc_auc_score(y_test, predictions))\n    models.append((model_gs,x_train,y_train,x_test,y_test,train_acc,test_acc))","outputs":[],"execution_count":39},{"cell_type":"code","metadata":{"collapsed":true},"source":"","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"collapsed":true},"source":"model_out = open(\"saved_models_2\",\"wb\")\npickle.dump(models, model_out)\nmodel_out.close()","outputs":[],"execution_count":40},{"cell_type":"code","metadata":{"collapsed":true},"source":"model_in = open(\"saved_models_2\",\"rb\")\nmodels_load  = pickle.load(model_in)\nmodel_in.close()","outputs":[],"execution_count":41},{"cell_type":"code","metadata":{},"source":"df2.head()","outputs":[],"execution_count":44},{"cell_type":"code","metadata":{},"source":"models_load","outputs":[],"execution_count":43},{"cell_type":"markdown","metadata":{},"source":"### Decision Tree Classifier"},{"cell_type":"code","metadata":{"collapsed":true},"source":"# Convert string column to float\ndef str_column_to_float(dataset, column):\n    for row in dataset:\n        row[column] = float(row[column].strip())\n\n# Split a dataset into k folds\ndef cross_validation_split(dataset, n_folds):\n    dataset_split = list()\n    dataset_copy = list(dataset)\n    #print(len(dataset_copy), n_folds)\n    fold_size = int(len(dataset) / n_folds)\n    for i in range(n_folds):\n        fold = list()\n        while len(fold) < fold_size:\n            index = randrange(len(dataset_copy))\n            #print(len(dataset_copy))\n            #if(index == 0):\n             #   break;\n            #index = index - 1\n            fold.append(dataset_copy.pop(index))\n        dataset_split.append(fold)\n    return dataset_split\n\n# Calculate accuracy percentage\ndef accuracy_metric(actual, predicted):\n    correct = 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i]:\n            correct += 1\n    return correct / float(len(actual)) * 100.0\n\n# Evaluate an algorithm using a cross validation split\ndef evaluate_algorithm(dataset, algorithm, n_folds, *args):\n    folds = cross_validation_split(dataset, n_folds)\n    scores = list()\n    for fold in folds:\n        train_set = list(folds)\n        train_set.remove(fold)\n        train_set = sum(train_set, [])\n        test_set = list()\n        for row in fold:\n            row_copy = list(row)\n            test_set.append(row_copy)\n            row_copy[-1] = None\n        predicted = algorithm(train_set, test_set, *args)\n        actual = [row[-1] for row in fold]\n        accuracy = accuracy_metric(actual, predicted)\n        scores.append(accuracy)\n    return scores\n\n# Split a dataset based on an attribute and an attribute value\ndef test_split(index, value, dataset):\n    left, right = list(), list()\n    for row in dataset:\n        if row[index] < value:\n            left.append(row)    \n        else:\n            right.append(row)\n    return left, right\n\n# Calculate the Gini index for a split dataset\ndef gini_index(groups, classes):\n    # count all samples at split point\n    n_instances = float(sum([len(group) for group in groups]))\n    # sum weighted Gini index for each group\n    gini = 0.0\n    for group in groups:\n        size = float(len(group))\n        # avoid divide by zero\n        if size == 0:\n            continue\n        score = 0.0\n        # score the group based on the score for each class\n        for class_val in classes:\n            p = [row[-1] for row in group].count(class_val) / size\n            score += p * p\n        # weight the group score by its relative size\n        gini += (1.0 - score) * (size / n_instances)\n    return gini\n\n# Select the best split point for a dataset\ndef get_split(dataset):\n    class_values = list(set(row[-1] for row in dataset))\n    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n    for index in range(len(dataset[0])-1):\n        for row in dataset:\n            groups = test_split(index, row[index], dataset)\n            gini = gini_index(groups, class_values)\n            if gini < b_score:\n                b_index, b_value, b_score, b_groups = index, row[index], gini, groups\n    return {'index':b_index, 'value':b_value, 'groups':b_groups}\n\n# Create a terminal node value\ndef to_terminal(group):\n    outcomes = [row[-1] for row in group]\n    return max(set(outcomes), key=outcomes.count)\n\n# Create child splits for a node or make terminal\ndef split(node, max_depth, min_size, depth):\n    left, right = node['groups']\n    del(node['groups'])\n    # check for a no split\n    if not left or not right:\n        node['left'] = node['right'] = to_terminal(left + right)\n        return\n    # check for max depth\n    if depth >= max_depth:\n        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n        return\n    # process left child\n    if len(left) <= min_size:\n        node['left'] = to_terminal(left)\n    else:\n        node['left'] = get_split(left)\n        split(node['left'], max_depth, min_size, depth+1)\n    # process right child\n    if len(right) <= min_size:\n        node['right'] = to_terminal(right)\n    else:\n        node['right'] = get_split(right)\n        split(node['right'], max_depth, min_size, depth+1)\n\n# Build a decision tree\ndef build_tree(train, max_depth, min_size):\n    root = get_split(train)\n    split(root, max_depth, min_size, 1)\n    return root\n\n# Make a prediction with a decision tree\ndef predict(node, row):\n    if row[node['index']] < node['value']:\n        if isinstance(node['left'], dict):\n            return predict(node['left'], row)\n        else:\n            return node['left']\n    else:\n        if isinstance(node['right'], dict):\n            return predict(node['right'], row)\n        else:\n            return node['right']\n\n# Classification and Regression Tree Algorithm\ndef decision_tree(train, test, max_depth, min_size):\n    tree = build_tree(train, max_depth, min_size)\n    predictions = list()\n    for row in test:\n        prediction = predict(tree, row)\n        predictions.append(prediction)\n    return(predictions)\n\nseed(1)\n# load and prepare data\ndataset = df2.loc[:, df2.columns != 'case_status']\ndataset['case_status'] = df2.loc[:, df2.columns == 'case_status']\ndataset = dataset.values.tolist()[1:6000]\nn_folds = 5\nmax_depth = 5\nmin_size = 10\nscores = evaluate_algorithm(dataset, decision_tree, n_folds, max_depth, min_size)\nprint('Scores: %s' % scores)\nprint('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{},"source":"Y.value_counts()","outputs":[],"execution_count":46}],"nbformat_minor":1,"nbformat":4}