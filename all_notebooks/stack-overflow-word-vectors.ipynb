{"cells":[{"metadata":{"_uuid":"1c1d9246a826785ad74accf0bacaeaf2f44a7d4c","_cell_guid":"b8fe67d9-06b6-4dff-84f3-8d90ddf13f7e"},"cell_type":"markdown","source":"This is an implementation of \"word vectors\" based on Chris Moody's blog post: http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/  \n\nI've forked Alex Klibisz's notebook https://www.kaggle.com/alexklibisz\n\nI'm using StackOverflow's Data Dump hosted on [archive.org](https://archive.org/download/stackexchange/).\n\nThe data available in this notebook is a derivation of stackexchange-Posts.7z where I extracted each post's tags.\n\nI've incorporated some streaming methods that probably aren't necessary with Kaggle's 16GB RAM instances but were necessary for my laptop to run\n\n\n\n**Scroll to the bottom if you just want to see some nearest-neighbor examples.**\n\n**Changes**\n\n| Date | Change           |\n| ------------- |:-------------:|\n|7/16/2018      | Include tag creation dates in probabilities|\n|7/16/2018      | Change data source||\n|7/25/2018 | Normalize probabilities by considering tag creation date\n|7/25/2018 | Remove low probability tags\n|8/7/2018| Fix NameError, Add Output\n\n\n \n  "},{"metadata":{"trusted":true,"_uuid":"ff25a93ff2415f4d474697f87b5fb65e66add7a3"},"cell_type":"code","source":"import re\nimport os\nfrom itertools import combinations\nfrom collections import Counter, defaultdict, OrderedDict\nfrom math import log\nfrom scipy.sparse import csc_matrix\nfrom scipy.sparse.linalg import svds\nimport numpy as np\nfrom gensim.utils import smart_open\nimport datetime\n\nfp = \"../input/posts_data.csv\"\n\ntag_search = re.compile(r\"([^<>]+)\")\n\nfor i, line in enumerate(smart_open(fp)):\n    if i == 0:\n        headers = line.decode().replace(\"\\r\\n\", \"\").split(\",\")\n    elif i > 0:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc46866a4428359aabd7dbf6e3c91a56dd039d10"},"cell_type":"code","source":"class SmartCSV(object):\n    \n    def __init__(self, fp, headers):\n        self.fp = fp\n        self.headers = headers\n        \n    def __iter__(self):\n        for i, line in enumerate(smart_open(self.fp)):\n            if i == 0:\n                continue\n            line_to_dict = line.decode().replace(\"\\r\\n\", \"\").split(\",\")\n            line_to_dict = {k: v for k, v in zip(self.headers, line_to_dict)}\n            yield line_to_dict\n            \n    def __getitem__(self, index):\n        for i, line in enumerate(smart_open(self.fp)):\n            if i == index:\n                line_to_dict = line.decode().replace(\"\\r\\n\", \"\").split(\",\")\n                line_to_dict = {k: v for k, v in zip(self.headers, line_to_dict)}\n                return line_to_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bea2b58dfc74a210a051506e80d6715cd7cc174"},"cell_type":"code","source":"smart_csv = SmartCSV(fp, headers=headers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26e1ad47b7410b32fbe9ab7c5bcfb224a52b8ace"},"cell_type":"code","source":"class TagData(object):\n    \n    def __init__(self, smart_csv=smart_csv):\n        self.smart_csv = smart_csv\n        self.tag2id = defaultdict()\n        self.tag2id.default_factory = self.tag2id.__len__\n        self.id2tag = {}\n        self.tag2created = {}\n        self.tag_counts = Counter()\n        self.date2idx = defaultdict()\n        self.date2idx.default_factory = self.date2idx.__len__\n        self.rows = []\n        \n    def fit(self):\n        for line in self.smart_csv:\n            line_tags_raw = line.get('Tags')\n            if not line_tags_raw:\n                continue\n            line_tags = tag_search.findall(line_tags_raw)\n            if not line_tags:\n                continue\n            # Creating id2tag\n            for tag in line_tags:\n                _ = self.tag2id[tag]\n                \n            tag_ids = list(filter(lambda x: x is not None, [self.tag2id[x] for x in line_tags]))\n            \n            # Counting occurences\n            self.tag_counts.update(tag_ids)\n            \n            \n            # Tracking new tags and their creation date\n            post_date = datetime.datetime.strptime(line['CreationDate'].split(\"T\")[0], \"%Y-%m-%d\")\n            new_tags = list(filter(lambda x: x not in self.tag2created, tag_ids))\n            for nt in new_tags:\n                self.tag2created[nt] = post_date\n                \n            # Use date2idx to find which row to extend\n            row_index = self.date2idx[post_date]\n            \n            try:\n                _ = self.rows[row_index]\n            except IndexError:\n                self.rows.append([])\n                \n            self.rows[row_index].append(tag_ids)\n            \n        self.id2tag = {i: tag for tag, i in self.tag2id.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00eb69451665eee16db57e1183f699150c1ba2ee"},"cell_type":"code","source":"tag_data = TagData(smart_csv)\n%time tag_data.fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b11bdf3c798593095d2da045411e87355cc50c35"},"cell_type":"code","source":"%%time\n# We have the number of times a tag appears available at tag_data.tag_counts\n\n# We calculate the probability that a tag appears in a given post\n\n# For each tag we divide its count by the number of tags that occur ON OR AFTER its created date\n\noccurences = np.array(list(tag_data.tag_counts.values()))\n\n# Working backwards from the last date...\nsegmented_counter = Counter()\nsegmented_dict = {}\nsegment_i = len(tag_data.rows) - 1\nfor day in reversed(tag_data.rows):\n    for post in day:\n        segmented_counter.update(post)\n    segmented_dict[segment_i] = sum(segmented_counter.values())\n    segment_i -= 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"310342caeedb311089b592d45738d4d44c123480"},"cell_type":"code","source":"# Building word_sums\nword_sums = []\n\n# For each tag we:\nfor tag in list(tag_data.tag_counts.keys()):\n    # Get the tag created date\n    created_date = tag_data.tag2created[tag]\n    # Get the corresponding index for the creation date\n    date_idx = tag_data.date2idx[created_date]\n    # Lookup the accumulated counts from segemented_dict\n    accum_counts = segmented_dict[date_idx]\n    word_sums.append(accum_counts)\n\nword_sums = np.array(word_sums)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c0c16ab1e4264d6688a0065f2a5fecf73c74bda"},"cell_type":"code","source":"probabilities = occurences / word_sums\n\ndel word_sums, occurences, segmented_dict, segmented_counter","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fc30b806bb29c70f48cf0af01380f17b36732dc"},"cell_type":"code","source":"cxp = {k: v for k, v in zip(list(tag_data.tag_counts.keys()), probabilities)}\n\ndel probabilities","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1b80085fb139bf9908bcf222a6532284fc377a9"},"cell_type":"code","source":"import pandas as pd\ndf = pd.DataFrame.from_dict(list(cxp.items()))\n\nimport math\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\ndf[1] = df[1].apply(lambda x: math.log(x))\ndf[1].plot(kind='kde', figsize=(12, 8))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c1aab7b99cb6b3797aad66087aa2e1438e4cf82"},"cell_type":"code","source":"df = df.loc[df[1]>-15]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7cab517b24ab88615bae213899126c834ed0829"},"cell_type":"code","source":"del cxp\n\ncxp = {int(k): v for k, v in df.values}\n\ndel df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b414af232f6821366a56385b206579fcdbf8485"},"cell_type":"code","source":"# we want to use cxp's keys and drop others from tag_data\n\ntag_data.tag2id = dict(tag_data.tag2id)\n\nfor k in list(tag_data.id2tag.keys()):\n    if k not in cxp:\n        # get the tag2id before deleting\n        tag_name = tag_data.id2tag[k]\n        del tag_data.id2tag[k]\n        del tag_data.tag2id[tag_name]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1325e5f5c6ce1612a4ab887cf99e84d969d99418"},"cell_type":"code","source":"def stream_combo_tags(f=smart_csv, idx=tag_data.tag2id):\n    for line in f:\n        line_tags_raw = line.get('Tags')\n        if not line_tags_raw:\n            continue\n        line_tags = tag_search.findall(line_tags_raw)\n        line_tags = [idx.get(tag) for tag in line_tags]\n        line_tags = list(filter(lambda x: x is not None, line_tags))\n        if not line_tags or len(line_tags)<2:\n            continue\n        \n        for x, y in set(map(tuple, map(sorted, combinations(line_tags, 2)))):\n            yield x, y\n\nsc = stream_combo_tags()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"13ac3eaefdd464e3d69ddea11010c613d4e91f2b"},"cell_type":"code","source":"%time bigram_counts = Counter(sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a8c4da60c83ee525bbedbdb7be95b56ccac1bb9"},"cell_type":"code","source":"# Perform a similar operation as above\n\n# After summing a tag pair, we want to divide by the total # of occurences of bigrams ON OR AFTER the created date\n# of the newer tag\n\n# Working backwards from the last date...\n\nsegmented_counter_combos = Counter()\nsegmented_dict_combos = {}\nsegment_i_combos = len(tag_data.rows) - 1\n\nfor day in reversed(tag_data.rows):\n    for post in day:\n        post = list(filter(lambda x: x in tag_data.id2tag, post))\n        post_combos = list(map(sorted, combinations(post, 2)))\n        for combo in post_combos:\n            segmented_counter_combos.update(combo)\n    segmented_dict_combos[segment_i_combos] = sum(segmented_counter_combos.values())\n    segment_i_combos -= 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f3d1f03ddd1c1db3b9028618c4ba7e70ea62814"},"cell_type":"code","source":"cxyp = {}\n\nfor bigram_pair, v in list(bigram_counts.items()):\n    newest_tag_date = max([tag_data.tag2created[bigram_pair[0]], tag_data.tag2created[bigram_pair[1]]])\n    # Get the row corresponding to date\n    row_idx = tag_data.date2idx[newest_tag_date]\n    # Get total counts from date on\n    total_counts = segmented_dict_combos[row_idx]\n    cxyp[bigram_pair] = v / total_counts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3293bc52c54b5f570c44a30fb886682f8382fb77"},"cell_type":"code","source":"for x, y in map(sorted, combinations(['c', 'haskell', 'javascript', 'flask', 'python', 'django'], 2)):\n    x_id, y_id = tag_data.tag2id[x], tag_data.tag2id[y]\n    xy_prob = cxyp.get((x_id, y_id), 0)\n    print(\"Probability of {} and {} occurring together : {:.4%}\".format(x, y, xy_prob))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fac6ecd3e07740a7cb3916eedd951bd2b7b3fc24"},"cell_type":"code","source":"# Note that the frequency of a tags occurence is directly correlated with its skipgram probability\n# We want to normalize -> PMI\n# Probability of skipgram (a, b) / probability(a) * probability (b)\n\nfor x, y in map(sorted, combinations(['c', 'haskell', 'clojure', 'sql', 'python', 'django'], 2)):\n    x_id, y_id = sorted([tag_data.tag2id[x], tag_data.tag2id[y]])\n    \n    # We need 3 values - skipgram probability, x's probability and y's probability\n    x_prob, y_prob, xy_prob = cxp[x_id], cxp[y_id], cxyp.get((x_id, y_id), 0)\n    if xy_prob == 0:\n        print(\"{} and {} did not occur together\".format(x, y))\n        continue\n    pmi_value = log(xy_prob / (x_prob * y_prob))\n    print(\"PMI of {} and {} : {:.8}\".format(x, y, pmi_value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08d3dc8de652b7b4b08057ed25bf9d04b8a02082"},"cell_type":"code","source":"pmi_samples = Counter()\ndata, rows, cols = [], [], []\nfor (x, y), n in cxyp.items():\n    rows.append(x)\n    cols.append(y)\n    x_prob, y_prob, xy_prob = cxp[x_id], cxp[y_id], cxyp.get((x_id, y_id), 0)\n    pmi_value = log(xy_prob / (x_prob * y_prob))\n    data.append(pmi_value)\nPMI = csc_matrix((data, (rows, cols)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9182aad6f8b07556caee3884b81ea9da26a200f"},"cell_type":"code","source":"from sklearn.utils.extmath import svd_flip","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11886558716bee3e5d5da63e0dee4a5d81f2f54d"},"cell_type":"markdown","source":"scikit-learn uses a function, svd_flip for: \n```\nSign correction to ensure deterministic output from SVD.\nAdjusts the columns of u and the rows of v such that the loadings in the\ncolumns in u that are largest in absolute value are always positive.\n    ```\n   \n  \n"},{"metadata":{"trusted":true,"_uuid":"c0c99e0ad34379b55c0d85d2fc6d96fc5dfacd8e"},"cell_type":"code","source":"# Copied From sklearn.decomposition.truncated_svd.fit_transform\n\nU, Sigma, VT = svds(PMI, k=25)\n# svds doesn't abide by scipy.linalg.svd/randomized_svd\n# conventions, so reverse its outputs.\nSigma = Sigma[::-1]\nU, VT = svd_flip(U[:, ::-1], VT[::-1])\nU *= Sigma\n\n# Note: sklearn does not return normalized \nnorms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))\nU /= norms\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"accbc076ac2950e0de6c53153b4950ee40f08c6e"},"cell_type":"code","source":"for x in ['python', 'java', 'flask', 'clojure', 'sql', 'sqlalchemy', 'javascript']:\n    dd = np.dot(U, U[tag_data.tag2id[x]]) # Cosine similarity for this unigram against all others.\n    sims = np.argsort(-1 * dd)[:25]\n    readable = [(tag_data.id2tag[n], \"{:n}\".format(dd[n]), tag_data.tag_counts[n]) for n in sims if tag_data.tag2id[x] != n]\n    print(\"{} : {}\".format(x, tag_data.tag_counts[tag_data.tag2id[x]]))\n    print(\"-\" * 10)\n    s = ''\n    for n in readable:\n        s += \"({}, {}, {}), \".format(n[0], n[1], n[2])\n    print(s)\n    print()\n    print(\"=\" * 80)\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5be81eff9ee5c0500843a78c83897160ad0530a"},"cell_type":"code","source":"# Copied From sklearn.decomposition.truncated_svd.fit_transform\n\nU, Sigma, VT = svds(PMI, k=100)\n# svds doesn't abide by scipy.linalg.svd/randomized_svd\n# conventions, so reverse its outputs.\nSigma = Sigma[::-1]\nU, VT = svd_flip(U[:, ::-1], VT[::-1])\nU *= Sigma\n\n# Note: sklearn does not return normalized \nnorms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))\nU /= norms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"528c9cd000d92839a75023b627a3cc51b1350feb"},"cell_type":"code","source":"for x in ['python', 'java', 'flask', 'clojure', 'sql', 'sqlalchemy', 'javascript']:\n    dd = np.dot(U, U[tag_data.tag2id[x]]) # Cosine similarity for this unigram against all others.\n    sims = np.argsort(-1 * dd)[:25]\n    readable = [(tag_data.id2tag[n], \"{:n}\".format(dd[n]), tag_data.tag_counts[n]) for n in sims if tag_data.tag2id[x] != n]\n    print(\"{} : {}\".format(x, tag_data.tag_counts[tag_data.tag2id[x]]))\n    print(\"-\" * 10)\n    s = ''\n    for n in readable:\n        s += \"({}, {}, {}), \".format(n[0], n[1], n[2])\n    print(s)\n    print()\n    print(\"=\" * 80)\n    print()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}