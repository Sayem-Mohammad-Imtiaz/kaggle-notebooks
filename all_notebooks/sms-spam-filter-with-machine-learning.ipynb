{"cells":[{"metadata":{"_uuid":"f8bf8eec-c2ee-455f-8905-6331cacde774","_cell_guid":"0e35972f-bc22-41dc-8b91-c0c619485891","trusted":true},"cell_type":"markdown","source":"# SMS SPAM FILTER\n"},{"metadata":{"_uuid":"8d98f9e4-827c-4a02-9f9c-064e395cf118","_cell_guid":"c9c7275c-dd3f-43c1-bef4-3233e014b761","trusted":true},"cell_type":"markdown","source":"There are 5,572 Sms writtten in English. 4825 is ham sms and 747 is spam\n"},{"metadata":{"_uuid":"d684bbb8-7f9e-4deb-928c-d2649d97a5ba","_cell_guid":"82271a54-df44-4e0b-b1a9-d7cc88bf3d77","trusted":true},"cell_type":"markdown","source":"****Text preprocessing****\n\nChange column names (v1 to label and v2 to text) and create a new column. The name of the new column is 'copy' and it is the exact copy of the text column. The main purpose of creating an exact copy of the text column is to see the difference between processed and unprocessed data. We need to encode the class labels in the text column because of the binary classes. (spam = 1 , ham = 0)"},{"metadata":{"_uuid":"ee702fa9-6b31-440c-8131-eddccf26aef3","_cell_guid":"59ae217b-f8f7-4716-9eec-6c2d66258632","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\n\ndf = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1', usecols = [0, 1])\n\ndf.rename(columns = {'v1':'label','v2':'text'},inplace=True)\n\ndf['class'] = df.label.map({'ham':0, 'spam':1})\n\ndf['copy'] = df.text\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1649bd02-4204-451f-b5a2-6a38f24ead8c","_cell_guid":"87ad15d5-a5fa-4ae6-9446-fe376f99b3b8","trusted":true},"cell_type":"markdown","source":"Let's see what we got"},{"metadata":{"_uuid":"4d46e278-9087-436f-a47d-a0f9c04d6ca2","_cell_guid":"89819e42-af2a-449e-acca-db5938827b4f","trusted":true},"cell_type":"code","source":"df.info()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"There is no null object which is good."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nWe need to replace the some words and/or numbers with spesific strings in order to extract meaningful features from dataset. \n\nThese are the few steps of what we are doing.\n\n1. Replaced email adresses with 'emailaddr'\n2. Replaced website names with httpaddr etc\n\nSome words in the English language, while necessary, don't contribute much to the meaning of a phrase. These words, such as \"when\", \"had\", \"those\" or \"before\", are called **stop_words** and should be filtered out.\n\nYou can check what are the all words in the list of stop_words from [here](https://gist.github.com/sebleier/554280)\n\n**Why we are do these things?**\n\nLet's think of programming languages as babies. At the very beginning, babies know nothing about what you are trying to do. Later you give them some information and rules to understand their environment and their own purposes. In this way, babies can grow through our teaching and reflects it. Programming languages are exactly like the babies, they demand some information (inputs) from you, for the processing operation. Thus, you should describe every nuance in a proper way otherwise it's gonna be insufficient.\nFor instance\n\n*  *\"I feel exhausted.\"* \n*  *\"i FEEL eXhAusted!\"* \n\nRefers to same meaning. But in programming languages like python, its not the same thing at all. From python perspective, these are two different sentences.That's why we use lower() function and remove all punctuation since ***exhausted.*** and ***exhausted!*** refers to same word.\n\n\n\n\nIt's likely the corpus contains words with various suffixes such as \"distribute\", \"distributing\", \"distributor\" or \"distribution\". We can replace these four words with just \"distribut\" via a step called stemming.\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\n\nporter = nltk.PorterStemmer() #\"distribute\", \"distributing\", \"distributor\" or \"distribution\".\nstop_words = nltk.corpus.stopwords.words('english')\n\ndef clean_text(string):\n    message = re.sub(r'\\b[\\w\\-.]+?@\\w+?\\.\\w{2,4}\\b', 'emailaddr', string)\n    message = re.sub(r'(http[s]?\\S+)|(\\w+\\.[A-Za-z]{2,4}\\S*)', 'httpaddr', #Replace URLs with 'httpaddr'\n                     message)\n    message = re.sub(r'£|\\$', 'money', message) #Replace money symbols with 'moneysymb'\n    message = re.sub(\n        r'\\b(\\+\\d{1,2}\\s)?\\d?[\\-(.]?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b', #Replace phone numbers with 'phonenumbr'\n        'phonenumbr', message)\n    message = re.sub(r'\\d+(\\.\\d+)?', 'numbr', message)  #Replace numbers with 'numbr'\n    message = re.sub(r'[^\\w\\d\\s]', ' ', message)\n    message = re.sub(r'\\s+', ' ', message)\n    message = re.sub(r'^\\s+|\\s+?$', '', message.lower())\n    return ' '.join(\n    porter.stem(term) \n    for term in message.split()\n    if term not in set(stop_words)\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's test it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_text(\"going to vacation!!! 5734 I have ,.$ £\")\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool! Clean_text() function changed some words to spesific strings\n\n* going -> go (PorterStemmer)\n* to -> removed (because it is in the stop_words list)\n* vacation -> vacat ( PorterStemmer)\n* !!! -> removed\n* 5734 -> numbr\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"textCopy = df['text'].copy()\ntextCopy = textCopy.apply(clean_text)\ndf[\"copy\"] = textCopy\ndf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n\nvectorizer = CountVectorizer() \n\nvectorizer.fit(textCopy)\n\ntf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n\nngrams = vectorizer.fit_transform(textCopy).toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Graphs**\n\nIn this part, we want to check what is the most 10 common words with how many times words are used in spam and ham texts.\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nspam_df = df[df['class'] == 1] #create sub-dataframe of spam text\nham_df = df[df['class'] == 0] #sub-dataframe of ham text\nspam_df['copy'] = spam_df['copy'].map(clean_text)\nham_df['copy'] = ham_df['copy'].map(clean_text)\nspam_df['new_column'] = spam_df['copy'].apply(lambda x: Counter(x.split(' ')))\nforspam=Counter(\" \".join(spam_df['copy']).split()).most_common(10)\nforham=Counter(\" \".join(ham_df['copy']).split()).most_common(10)\n\nspamfeat=[]\nfor i in range(len(forspam)):\n    spamcounter=forspam[i][0]\n    spamfeat.append(spamcounter)\n\nspamnumber=[]\nfor i in range(len(forspam)):\n    spamcounter=forspam[i][1]\n    spamnumber.append(spamcounter)\n\nhamnumber=[]\nfor i in range(len(forham)):\n    spamcounter=forham[i][1]\n    hamnumber.append(spamcounter)\n    \nhamfeat=[]\nfor i in range(len(forham)):\n    spamcounter=forham[i][0]\n    hamfeat.append(spamcounter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\n\nfig, (ax,ax1) = plt.subplots(1,2,figsize = (25, 8))\nsns.barplot(x = spamfeat, y=spamnumber, ax = ax)\nax.set_ylabel('count', fontsize = 15)\nax.set_xlabel('word',fontsize = 15)\nax.tick_params(labelsize=12)\nax.set_title('spam top 10 words', fontsize = 15)\n\nsns.barplot(x = hamfeat, y = hamnumber, ax = ax1)\nax1.set_ylabel('count', fontsize = 15)\nax1.set_xlabel('word',fontsize = 15)\nax1.tick_params(labelsize=15)\nax1.set_title('ham top 10 words', fontsize = 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\n\nforS=\" \".join(spam_df['copy'])\nforH=\" \".join(ham_df['copy'])\nspam_word_cloud = WordCloud(width = 600, height = 400, background_color = 'white').generate(forS)\nham_word_cloud = WordCloud(width = 600, height = 400,background_color = 'white').generate(forH)\n\nfig, (ax, ax2) = plt.subplots(1,2, figsize = (18,8))\nax.imshow(spam_word_cloud)\nax.axis('off')\nax.set_title('spam word cloud', fontsize = 20)\nax2.imshow(ham_word_cloud)\nax2.axis('off')\nax2.set_title('ham word cloud', fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\n\nn_folds = 5\ndef f1_cv(model):\n    kf = KFold(n_folds, shuffle = True, random_state = 29).get_n_splits(ngrams)\n    f1 = cross_val_score(model, ngrams, df[\"class\"], scoring = 'f1', cv = kf )\n    return (f1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\n\n\nx_train, x_test, y_train, y_test = train_test_split(ngrams, df['class'].values, test_size=.40)\n\nclfs = {\n    'Decision_tree': DecisionTreeClassifier(),\n    'gradient_descent': SGDClassifier(),\n    'Naive_bayes': GaussianNB(),\n    'Logistic_Regression': LogisticRegression()\n}\n\nfor clf_name in clfs.keys():\n    print(\"Training\",clf_name,\"classifier\")\n    clf = clfs[clf_name]\n    clf.fit(x_train, y_train)\n    y_predict = clf.predict(x_test)\n    print(classification_report(y_test, y_predict))\n    print()\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}