{"cells":[{"metadata":{},"cell_type":"markdown","source":"[Google Colaboratory Verson](https://colab.research.google.com/drive/1djAv8JBVP8TMuXidwU7J-bFn-U92McL2)\n## Modules & Functions","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings; warnings.filterwarnings('ignore')\nimport numpy as np,pylab as pl,pandas as pd\nimport sys,h5py,urllib,zipfile\nimport tensorflow as tf\nimport tensorflow_hub as th\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def prepro(x_train,y_train,x_test,y_test,n_class):\n    n=int(len(x_test)/2)\n    x_train=x_train.astype('float32')/255\n    x_test=x_test.astype('float32')/255\n    y_train=y_train.astype('int32')\n    y_test=y_test.astype('int32')    \n    x_valid,y_valid=x_test[:n],y_test[:n]\n    x_test,y_test=x_test[n:],y_test[n:]\n    cy_train=tf.keras.utils.to_categorical(y_train,n_class) \n    cy_valid=tf.keras.utils.to_categorical(y_valid,n_class)\n    cy_test=tf.keras.utils.to_categorical(y_test,n_class)\n    df=pd.DataFrame([[x_train.shape,x_valid.shape,x_test.shape],\n                     [y_train.shape,y_valid.shape,y_test.shape],\n                     [cy_train.shape,cy_valid.shape,cy_test.shape]],\n                    columns=['train','valid','test'],\n                    index=['images','labels','encoded labels'])\n    display(df)\n    return [[x_train,x_valid,x_test],\n            [y_train,y_valid,y_test],\n            [cy_train,cy_valid,cy_test]]\ndef display_10img(X,y,s):\n    fig,ax=pl.subplots(figsize=(10,3),nrows=2,ncols=5,\n                       sharex=True,sharey=True)\n    ax=ax.flatten()\n    for i in range(10):\n        ax[i].imshow(X[i].reshape(s,s,3),cmap=pl.cm.Pastel1)\n        ax[i].set_title(y[i])\n    ax[0].set_xticks([]); ax[0].set_yticks([])\n    pl.tight_layout()\ndef display_resize(x_train,x_valid,x_test,\n                   y_valid,cy_valid,pixels):\n    x_train=tf.image.resize(x_train,[pixels,pixels])\n    x_valid=tf.image.resize(x_valid,[pixels,pixels])\n    x_test=tf.image.resize(x_test,[pixels,pixels])\n    img=x_valid[1]\n    lbl='one example of resized images \\nlabel: '+\\\n     str(y_valid[1][0])+'=>'+str(cy_valid[1])+\\\n     '\\nshape: '+str(img.shape)\n    pl.imshow(img); pl.title(lbl)\n    return [x_train,x_valid,x_test]\ndef cb(fw):\n    early_stopping=tf.keras.callbacks\\\n    .EarlyStopping(monitor='val_loss',patience=20,verbose=2)\n    checkpointer=tf.keras.callbacks\\\n    .ModelCheckpoint(filepath=fw,save_best_only=True,verbose=2)\n    lr_reduction=tf.keras.callbacks\\\n    .ReduceLROnPlateau(monitor='val_loss',verbose=2,\n                       patience=5,factor=.8)\n    return [checkpointer,early_stopping,lr_reduction]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"(x_train1,y_train1),(x_test1,y_test1)=\\\ntf.keras.datasets.cifar10.load_data()\n[[x_train1,x_valid1,x_test1],\n [y_train1,y_valid1,y_test1],\n [cy_train1,cy_valid1,cy_test1]]=\\\nprepro(x_train1,y_train1,x_test1,y_test1,10)\ndisplay_10img(x_test1,y_test1,32)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fpath='../input/classification-of-handwritten-letters/'\nf=h5py.File(fpath+'LetterColorImages_123.h5','r') \nkeys=list(f.keys()); print(keys)\nimages=np.array(f[keys[1]])\nlabels=np.array(f[keys[2]]).reshape(-1,1)-1\nx_train2,x_test2,y_train2,y_test2=\\\ntrain_test_split(images,labels,test_size=.2,random_state=1)\ndel images,labels\n[[x_train2,x_valid2,x_test2],\n [y_train2,y_valid2,y_test2],\n [cy_train2,cy_valid2,cy_test2]]=\\\nprepro(x_train2,y_train2,x_test2,y_test2,33)\ndisplay_10img(x_test2,y_test2,32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple MLP Building & Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_accuracy(predictions,labels):\n    return (100.0*np.sum(np.argmax(predictions,1)==\\\n           np.argmax(labels,1))/predictions.shape[0])\ndef mlp(x,weights,biases):\n    layer1=tf.add(tf.matmul(x,weights['W1']),biases['b1'])\n    layer2=tf.add(tf.matmul(layer1,weights['W2']),biases['b2'])\n    return tf.matmul(layer2,weights['out'])+biases['out']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def nn_train(x_train,cy_train,x_test,cy_test,\n             lr,epochs,hidden1,hidden2,batch_size,\n             display_step,n_inputs,n_classes):\n    graph=tf.Graph()\n    with graph.as_default():\n        weights={'W1':tf.Variable(\\\n        tf.compat.v1.random_normal([n_inputs,hidden1])),\n        'W2':tf.Variable(\\\n        tf.compat.v1.random_normal([hidden1,hidden2])),\n        'out':tf.Variable(\\\n        tf.compat.v1.random_normal([hidden2,n_classes]))}\n        biases={'b1':tf.Variable(\\\n        tf.compat.v1.random_normal([hidden1])),\n        'b2':tf.Variable(\\\n        tf.compat.v1.random_normal([hidden2])),\n        'out':tf.Variable(\\\n        tf.compat.v1.random_normal([n_classes]))}\n        X=tf.compat.v1.placeholder(\"float32\",[None,n_inputs])\n        y=tf.compat.v1.placeholder(\"int32\",[None,n_classes])\n        vX=tf.constant(x_test.reshape(-1,n_inputs))\n        logits=mlp(X,weights,biases)\n        vlogits=mlp(vX,weights,biases)\n        loss=tf.reduce_mean(\\\n        tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y))\n        optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=lr)\n        train_opt=optimizer.minimize(loss)\n        train_predictions=tf.nn.softmax(logits)\n        test_predictions=tf.nn.softmax(vlogits)\n    with tf.compat.v1.Session(graph=graph) as sess:\n        tf.compat.v1.global_variables_initializer().run()\n        for epoch in range(epochs):\n            avg_loss=0.; avg_acc=0.\n            total_batch=int(x_train.shape[0]/batch_size)\n            for i in range(total_batch):\n                offset=(i*batch_size)%(x_train.shape[0]-batch_size)\n                batch_X=x_train.reshape(-1,32*32*3)[offset:(offset+batch_size)]\n                batch_y=cy_train[offset:(offset+batch_size)]\n                _,l,batch_py=\\\n                sess.run([train_opt,loss,train_predictions],\n                         feed_dict={X:batch_X,y:batch_y})\n                avg_loss+=l/total_batch\n                avg_acc+=cat_accuracy(batch_py,batch_y)/total_batch\n            if epoch%display_step==0:\n                print(\"Epoch: %04d\"%(epoch+1),\n                      \"loss={:.9f}\".format(avg_loss),\n                      \"accuracy={:.3f}\".format(avg_acc))\n        print(\"Test accuracy: %.3f%%\"%\\\n        cat_accuracy(test_predictions.eval(),cy_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=.001; epochs=15\nhidden1=512; hidden2=256\nbatch_size=128; display_step=1 \nn_inputs=32*32*3; n_classes=10\ntf.compat.v1.reset_default_graph()\nnn_train(x_train1,cy_train1,x_test1,cy_test1,\n         lr,epochs,hidden1,hidden2,batch_size,\n         display_step,n_inputs,n_classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=.001; epochs=100; hidden1=512; hidden2=256\nbatch_size=128; display_step=10; \nn_inputs=32*32*3; n_classes=33\ntf.compat.v1.reset_default_graph()\nnn_train(x_train2,cy_train2,x_test2,cy_test2,\n         lr,epochs,hidden1,hidden2,batch_size,\n         display_step,n_inputs,n_classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Simple CNN Building & Training","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def cnn(x,n_classes,conv1,conv2,dense1):\n    W1=tf.Variable(tf.compat.v1\\\n         .truncated_normal([2,2,x.shape[3],conv1],stddev=.04))\n    b1=tf.Variable(tf.compat.v1\\\n         .constant(.04,shape=[conv1]))\n    x=tf.nn.conv2d(x,W1,strides=[1,5,5,1],padding='SAME')\n    x=tf.nn.relu(x+b1)\n    x=tf.nn.max_pool(x,ksize=[1,2,2,1], \n                     strides=[1,2,2,1],padding='SAME')\n    x=tf.nn.dropout(x,.2)\n    W2=tf.Variable(tf.compat.v1\\\n         .truncated_normal([2,2,x.shape[3],conv2],stddev=.04))\n    b2=tf.Variable(tf.compat.v1\\\n         .constant(.04,shape=[conv2]))\n    x=tf.nn.conv2d(x,W2,strides=[1,2,2,1],padding='SAME')\n    x=tf.nn.relu(x+b2)\n    x=tf.nn.max_pool(x,ksize=[1,2,2,1], \n                     strides=[1,2,2,1],padding='SAME')\n    x=tf.nn.dropout(x,.2)\n    x=tf.reshape(x,[-1,x.shape[1]*x.shape[2]*x.shape[3]])\n    W3=tf.Variable(tf.compat.v1.\\\n    truncated_normal([x.shape[1],dense1],stddev=.04))\n    b3=tf.Variable(tf.compat.v1.truncated_normal([dense1],stddev=.04))\n    x=tf.nn.relu(tf.add(tf.matmul(x,W3),b3)) \n    W=tf.Variable(tf.compat.v1.\\\n    truncated_normal([x.shape[1],n_classes],stddev=.04))\n    b=tf.Variable(tf.compat.v1.truncated_normal([n_classes],stddev=.04))\n    x=tf.add(tf.matmul(x,W),b)\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr=.0005; epochs=100; \nbatch_size=64; display_step=5 \nimg_size=32; n_classes=10\nsave_model_path='./img_class'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.compat.v1.reset_default_graph()\ngraph=tf.Graph()\nwith graph.as_default():\n    X=tf.compat.v1.\\\n    placeholder(\"float32\",[None,img_size,img_size,3],name='x')\n    y=tf.compat.v1.placeholder(\"int32\",[None,n_classes],name='y')\n    logits=cnn(X,n_classes,32,196,512)\n    logits=tf.compat.v1.identity(logits,name='logits')\n    loss=tf.reduce_mean(\\\n    tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y))\n    optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=lr)\n    train_opt=optimizer.minimize(loss)\n    train_predictions=tf.nn.softmax(logits)\n    correct_predictions=tf.equal(tf.argmax(train_predictions,1),\n                                 tf.argmax(y,1))\n    accuracy=tf.reduce_mean(tf.cast(correct_predictions,tf.float32),\n                            name='accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config=tf.compat.v1.ConfigProto()\nwith tf.compat.v1.Session(graph=graph,config=config) as sess:\n    tf.compat.v1.global_variables_initializer().run()\n    for epoch in range(epochs):\n        avg_loss=0.; avg_acc=0.\n        total_batch=int(x_train1.shape[0]/batch_size)\n        for i in range(total_batch):\n            offset=(i*batch_size)%(x_train1.shape[0]-batch_size)\n            batch_X=x_train1[offset:(offset+batch_size)]\n            batch_y=cy_train1[offset:(offset+batch_size)]\n            _,l,_,_,acc=sess.run([train_opt,loss,train_predictions,\n                                  correct_predictions,accuracy],\n                                  feed_dict={X:batch_X,y:batch_y})\n            avg_loss+=l/total_batch\n            avg_acc+=acc/total_batch\n        if epoch%display_step==0:\n            print(\"Epoch: %04d\"%(epoch+1),\n                  \"loss={:.9f}\".format(avg_loss),\n                  \"accuracy={:.3f}\".format(avg_acc))\n    saver=tf.compat.v1.train.Saver()\n    save_path=saver.save(sess,save_model_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loaded_graph=tf.Graph()\nconfig=tf.compat.v1.ConfigProto()\nwith tf.compat.v1.Session(graph=loaded_graph,config=config) as sess:\n    loader=tf.compat.v1.train.import_meta_graph(save_model_path+'.meta')\n    loader.restore(sess,save_model_path)\n    loaded_X=loaded_graph.get_tensor_by_name('x:0')\n    loaded_y=loaded_graph.get_tensor_by_name('y:0')\n    loaded_logits=loaded_graph.get_tensor_by_name('logits:0')\n    loaded_acc=loaded_graph.get_tensor_by_name('accuracy:0')\n    loss=tf.reduce_mean(\\\n    tf.nn.softmax_cross_entropy_with_logits(logits=loaded_logits,\n                                            labels=loaded_y))\n    avg_loss=0.; avg_acc=0.\n    total_batch=int(x_valid1.shape[0]/batch_size)\n    for i in range(total_batch):\n        offset=(i*batch_size)%(x_valid1.shape[0]-batch_size)\n        batch_X=x_valid1[offset:(offset+batch_size)]\n        batch_y=cy_valid1[offset:(offset+batch_size)]\n        l,acc=sess.run([loss,loaded_acc],\n                     feed_dict={loaded_X:batch_X,\n                                loaded_y:batch_y})\n        avg_loss+=l/total_batch\n        avg_acc+=acc/total_batch\n    print(\"test loss={:.9f}\".format(avg_loss),\n          \"test accuracy={:.3f}\".format(avg_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparing with Keras Applications","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def premodel(pix,den,mh,lbl):\n    model=tf.keras.Sequential([\n        tf.keras.layers.Input((pix,pix,3),\n                              name='input'),\n        th.KerasLayer(mh,trainable=True),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(den,activation='relu'),\n        tf.keras.layers.Dropout(rate=.5),\n        tf.keras.layers.Dense(lbl,activation='softmax')])\n    model.compile(optimizer='adam',metrics=['accuracy'],\n                  loss='categorical_crossentropy')\n    display(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[handle_base,pixels]=[\"mobilenet_v2_050_96\",96]\nmhandle=\"https://tfhub.dev/google/imagenet/{}/feature_vector/4\"\\\n.format(handle_base)\nfw='weights.best.hdf5'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[x_train1,x_valid1,x_test1]=\\\ndisplay_resize(x_train1,x_valid1,x_test1,\n               y_valid1,cy_valid1,pixels)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model=premodel(pixels,512,mhandle,10)\nhistory=model.fit(x=x_train1,y=cy_train1,batch_size=64,\n                  epochs=10,callbacks=cb(fw),\n                  validation_data=(x_valid1,cy_valid1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(fw)\nmodel.evaluate(x_test1,cy_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[x_train2,x_valid2,x_test2]=\\\ndisplay_resize(x_train2,x_valid2,x_test2,\n               y_valid2,cy_valid2,pixels)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"model=premodel(pixels,512,mhandle,33)\nhistory=model.fit(x=x_train2,y=cy_train2,batch_size=64,\n                  epochs=100,callbacks=cb(fw),\n                  validation_data=(x_valid2,cy_valid2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights(fw)\nmodel.evaluate(x_test2,cy_test2)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}