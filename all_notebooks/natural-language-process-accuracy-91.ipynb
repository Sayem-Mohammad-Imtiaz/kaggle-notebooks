{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Natural Language Process(NLP)\n**Target:** <br>\nDetermining the category of the video according to the video title."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/youtubevideodataset/Youtube Video Dataset.csv\")\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">** We need header and category columns. We can delete the remaining columns.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop([\"Videourl\",\"Description\"],axis=1)\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Let's check if there is a null value inside."},{"metadata":{"trusted":true},"cell_type":"code","source":"data is None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Category.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Travel Blog => 0\n* Science & Technology => 1\n* Food => 2\n* Art&Music => 3\n* manufacturing => 4\n* History => 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Category\"] = data[\"Category\"].map({\"travel blog\":0,\"Science&Technology\":1,\"Food\":2,\"Art&Music\":3,\"manufacturing\":4,\"History\":5})\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regular Expression"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk \nimport re\nfrom nltk.corpus import stopwords\n\ntitle_list = []\nfor title in data.Title:\n    title = re.sub(\"[^a-zA-Z]\",\" \", title)\n    title = title.lower()\n    title = nltk.word_tokenize(title)\n    lemma = nltk.WordNetLemmatizer()\n    title = [ lemma.lemmatize(word) for word in title]\n    title = \" \".join(title)\n    title_list.append(title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Bag Of Words"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nmax_features = 1000\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words=\"english\")\nspace_matrix = count_vectorizer.fit_transform(title_list).toarray() # 0-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data[\"Category\"].values\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = space_matrix\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.1,random_state=42)\nprint(\"x_train\",x_train.shape)\nprint(\"x_test\",x_test.shape)\nprint(\"y_train\",y_train.shape)\nprint(\"y_test\",y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive Bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\nprint(\"Accuracy => \", nb.score(x_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_words = count_vectorizer.get_feature_names()\nprint(\"Most used words: \",all_words[50:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nplt.subplots(figsize=(12,12))\nwordcloud=WordCloud(background_color=\"white\",width=1024,height=768).generate(\" \".join(all_words[100:]))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 10, random_state=42)\nrf.fit(x_train,y_train)\nprint(\"accuracy: \",rf.score(x_test,y_test)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#confussion matrix\ny_pred=rf.predict(x_test)\ny_true=y_test\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nnames=[\"travel blog\",\"Science&Technology\",\"Food\",\"Art&Music\",\"manufacturing\",\"History\"]\ncm=confusion_matrix(y_true,y_pred)\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nax.set_xticklabels(names,rotation=90)\nax.set_yticklabels(names,rotation=0)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}