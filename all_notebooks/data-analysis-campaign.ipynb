{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Feature Exploration\n\n## Importing packages and choosing work directory"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\nfrom collections import Counter\nimport os\nimport openpyxl # pip install openpyxl\n\n\nos.chdir(\"/kaggle/input/\") # Change this!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('/kaggle/input')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting up"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading database\ntrain_data = pd.read_excel(\"ml_project1_data.xlsx\")\n\n# Iteration\niteration = 1\n\n\n# Setting Seed\nseeds = [101, 2019, 555, 975, 369]\n\nseed = seeds[iteration]\n\n\n# Splitting the train and test dataset\n#train_data, test_data, train_label, test_label = train_test_split(database,\n#                                                                  database[\"Response\"],\n#                                                                 test_size=0.2,\n#                                                                  random_state=seed)\n\nprint(train_data.info())\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Exploring Data\n## 1.1 Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Receives a dataframe and returns a table with missing values count, perentage and flag for higher than 3%.\ndef missing_rep(df):\n    miss = df.isna().sum()\n    miss = miss[miss>0]\n    miss_p = miss/df.shape[0]\n    miss_t = miss_p>0.03\n\n    return pd.DataFrame({\"Missings\" : miss, \"Proportion of Missings\" : miss_p, \"Higher than 3%\" : miss_t})\n\n\n\nmissing_rep(train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These missings will be dealt with further into our exploration."},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Categorical Variables Analysis\n\n#### The average response rate is almost 15%."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"Response\"].mean() # Close to 15%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2.1 Descriptive table of categorial features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of categorical features:\nfeat_c = [\"Education\", \"Marital_Status\", \"Kidhome\", \"Teenhome\", \"AcceptedCmp1\", \"AcceptedCmp2\", \"AcceptedCmp3\", \"AcceptedCmp4\",\n         \"AcceptedCmp5\", \"Complain\"]\n\n\n\n# Receives dataframe, list of categorical features names(can be just 1), targer variable name,\n# number of minimu observations to be taken into account and threshold of discrimination ability and\n# returns a table with descriptive stats about these categorical features.\ndef cat_feat_describe(df, fc, target, n, thresh):\n\n    fl = []\n    \n    \n    \n    if (type(fc)==list):\n    \n        for feature in fc:\n            fl.append(df.groupby([feature]).agg({target : [\"count\", \"mean\"]}))    \n\n            fm = pd.concat(fl, keys=fc)\n\n            fm = pd.DataFrame({\"Number of observations\" : fm.iloc[:,0], \"Discrimination ability\" : fm.iloc[:,1],\n                                 \"More than n observations\" : fm.iloc[:,0]>n,\n                                 \"Higher discrimination ability than the Threshold\" : fm.iloc[:,1]>thresh,\n                                 \"Both True\" : ((fm.iloc[:,0]>n) & (fm.iloc[:,1]>thresh))})\n    else:\n        fm = (df.groupby(fc).agg({target : [\"count\", \"mean\"]}))\n        \n        fm = pd.DataFrame({\"Number of observations\" : fm.iloc[:,0], \"Discrimination ability\" : fm.iloc[:,1],\n                                 \"More than n observations\" : fm.iloc[:,0]>n,\n                                 \"Higher discrimination ability than the Threshold\" : fm.iloc[:,1]>thresh,\n                                 \"Both True\" : ((fm.iloc[:,0]>n) & (fm.iloc[:,1]>thresh))})\n        \n    return fm\n\n\n\n\n\n\nfeat_sum = cat_feat_describe(train_data, feat_c, \"Response\", 40, 0.15)\nfeat_sum","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### These states of these classes show both decent number of observations and a discrimination ability higher than 15%."},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_sum[feat_sum[\"Both True\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### States of classes with few observations\n\nWe can observe that there are three status of the categorial variable Marital_Status that hold few observations while not fitting into the common marital status we know. We should remove these observations."},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_sum[-feat_sum[\"More than n observations\"]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2.2 Visualization of Discrimination Ability of Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Takes a dataframe, list of categorical features names (can be 1), the target variable name\n# and the trheshold of discrimination ability and outputs all plots.\ndef cat_feat_plot(df, fc, target, thresh):\n\n    sns.set_style(\"whitegrid\")\n\n    if (type(fc)==str):\n        plot_df = cat_feat_describe(df, fc, target, 50, thresh).iloc[:,1]\n\n        plot_df = plot_df.sort_values(ascending=False)\n\n        ax = plot_df.plot.bar(color=\"black\", title=\"{}% Threshold Line on Discrimination Ability\".format(thresh*100), legend=False)\n        ax.set_ylabel(\"Discrimination Ability\", size = 10)\n        ax.axhline(y=thresh, color=\"red\")\n        ax.set_xticklabels(plot_df.index, rotation=50, size=10)\n\n\n        plt.show()\n        \n        \n        \n    else:\n        for feat in fc:\n            plot_df = cat_feat_describe(df, feat, target, 50, thresh).iloc[:,1]\n\n            plot_df = plot_df.sort_values(ascending=False)\n\n            \n            ax = plot_df.plot.bar(color=\"black\", title=\"{}% Threshold Line on Discrimination Ability\".format(thresh*100), legend=False)\n            ax.set_ylabel(\"Discrimination Ability\", size = 10)\n            ax.axhline(y=thresh, color=\"red\")\n            ax.set_xticklabels(plot_df.index, rotation=50, size=10)\n\n            plt.show()\n        \n        \n        \n        \n        \n    \ncat_feat_plot(train_data, feat_c, \"Response\", 0.15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that:\n\n- $Education$: PhD and Master are above the 15% Discrimination Ability threshold.\n- $Marital Status$: Only Married and Together are below the threshold.\n- $KidHome$ & $TeenHome$: Only the clients with zero children or teen show a high discrimination ability.\n- $Accepted Campaign$: All of the 5 previous campaigns show that, as expected, the clients which accepted the campaigns show discrimination ability above the threshold.\n- $Complain$: While both who does and does not registered complains show high discrimination ability, only the ones who did complain show a discrimination ability higher than the threshold."},{"metadata":{},"cell_type":"markdown","source":"## CHANGE"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_ = train_data.copy()\n\nlow_discriminability_cat = [\"Absurd\", \"Alone\", \"YOLO\", \"Married\", \"Together\"]\ndata_['Marital_Status'].loc[data_['Marital_Status'].isin(low_discriminability_cat)] = 'Other'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_.groupby(\"Marital_Status\").count().index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"low_discriminability_cat = [\"Graduation\", \"2n Cycle\", \"Basic\"]\ndata_['Education'].loc[data_['Education'].isin(low_discriminability_cat)] = 'Other'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_.groupby(\"Education\").count().index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_['NumberOff'] = data_['Kidhome'] + data_['Teenhome']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_feat_plot(df, fc, target, thresh):\n\n    sns.set_style(\"whitegrid\")\n\n    if (type(fc)==str):\n        plot_df = cat_feat_describe(df, fc, target, 50, thresh).iloc[:,1]\n\n        plot_df = plot_df.sort_values(ascending=False)\n\n        ax = plot_df.plot.bar(color=\"black\", title=\"{}% Threshold Line on Discrimination Ability\".format(thresh*100), legend=False)\n        ax.set_ylabel(\"Discrimination Ability\", size = 10)\n        ax.axhline(y=thresh, color=\"red\")\n        ax.set_xticklabels(plot_df.index, rotation=50, size=10)\n\n\n        plt.show()\n        \n        \n        \n    else:\n        for feat in fc:\n            plot_df = cat_feat_describe(df, feat, target, 50, thresh).iloc[:,1]\n\n            plot_df = plot_df.sort_values(ascending=False)\n\n            \n            ax = plot_df.plot.bar(color=\"black\", title=\"{}% Threshold Line on Discrimination Ability\".format(thresh*100), legend=False)\n            ax.set_ylabel(\"Discrimination Ability\", size = 10)\n            ax.axhline(y=thresh, color=\"red\")\n            ax.set_xticklabels(plot_df.index, rotation=50, size=10)\n\n            plt.show()\n        \n        \n        \nfeat_c.append(\"NumberOff\")\ncat_feat_plot(data_, feat_c, \"Response\", 0.15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"low_discriminability_cat = [\"1\", \"2\"]\ndata_['Teenhome'].loc[data_['Teenhome'].isin(low_discriminability_cat)] = '1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"low_discriminability_cat = [\"1\", \"2\"]\ndata_['Kidhome'].loc[data_['Kidhome'].isin(low_discriminability_cat)] = '1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"low_discriminability_cat = [\"1\", \"2\", \"3\"]\ndata_['NumberOff'].loc[data_['NumberOff'].isin(low_discriminability_cat)] = '1'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_feat_plot(df, fc, target, thresh):\n\n    sns.set_style(\"whitegrid\")\n\n    if (type(fc)==str):\n        plot_df = cat_feat_describe(df, fc, target, 50, thresh).iloc[:,1]\n\n        plot_df = plot_df.sort_values(ascending=False)\n\n        ax = plot_df.plot.bar(color=\"black\", title=\"{}% Threshold Line on Discrimination Ability\".format(thresh*100), legend=False)\n        ax.set_ylabel(\"Discrimination Ability\", size = 10)\n        ax.axhline(y=thresh, color=\"red\")\n        ax.set_xticklabels(plot_df.index, rotation=50, size=10)\n\n\n        plt.show()\n        \n        \n        \n    else:\n        for feat in fc:\n            plot_df = cat_feat_describe(df, feat, target, 50, thresh).iloc[:,1]\n\n            plot_df = plot_df.sort_values(ascending=False)\n\n            \n            ax = plot_df.plot.bar(color=\"black\", title=\"{}% Threshold Line on Discrimination Ability\".format(thresh*100), legend=False)\n            ax.set_ylabel(\"Discrimination Ability\", size = 10)\n            ax.axhline(y=thresh, color=\"red\")\n            ax.set_xticklabels(plot_df.index, rotation=50, size=10)\n\n            plt.show()\n        \n\ncat_feat_plot(data_, feat_c, \"Response\", 0.15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Numerical Variables Analysis\n\n### 1.3.1 Dealing with Dates - Dt_Customer into days since registration"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gets a series of dates and its format as parameters and returns a series of days since that date until today.\ndef days_since(dates_series, date_format):\n\n    n = len(dates_series)\n    \n    result = [0] * n\n\n    for i in range(n):\n        result[i] = (datetime.today()-datetime.strptime(dates_series[i], date_format)).days\n    \n    return result\n\n\n\ndata_[\"Days_Customer\"] = days_since(list(data_.Dt_Customer), \"%Y-%m-%d\")\n\ndata_ = data_.drop(columns=\"Dt_Customer\")\n\ndata_[\"Days_Customer\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_n = list(data_.columns)\n\nfeat_n = list(filter(lambda x: x not in feat_c, feat_n))\n\nfeat_n.remove(\"ID\") # Removing ID.\n# List of Numerical Variables\n\ndata_[feat_n].describe() # Describing only Numerical Variables","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3.2 Variables with zero variance (constants)"},{"metadata":{"trusted":true},"cell_type":"code","source":"std = data_[feat_n].describe().iloc[2,:]\n\nconst_lab = [std[std<0.05].index[0], std[std<0.05].index[1]]\n\nstd[std<0.05]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Removing constant variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_.drop(labels=const_lab, axis=1, inplace=True) # Now data_ does not contain zero variance variables.\n\n\nfeat_n = list(filter(lambda x: x not in const_lab, feat_n)) # Removing the names of the no varaiance variables\n                                                        # from the list of numerical features.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3.3 Inputing Missing Values\nInstead of a simple mean inputation, we employ the Linear Regression model from sklearn package to perform a prediction of what the missing values in Income would be considering all the other variables and use it to input the missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing all datasets to be used in the Linear Regression Model\nX = data_\ny = X[\"Income\"]\ny = y[-y.isna()]\nX = X.drop(columns=[\"ID\"])\n\n\nX[\"Marital_Status\"] = pd.Categorical(X[\"Marital_Status\"])\nX[\"Marital_Status\"] = X[\"Marital_Status\"].cat.codes\n\nX[\"Education\"] = pd.Categorical(X[\"Education\"])\nX[\"Education\"] = X[\"Education\"].cat.codes\n\nx_pred = X[X.Income.isna()]\nx_pred = x_pred.drop(columns=\"Income\")\n\nX = X[-X.Income.isna()]\nX = X.drop(columns=\"Income\")\n\n\n\n\n\n# Linear Regression Model\nreg = LinearRegression().fit(X, y)\n\n# Predictions\ny_pred = reg.predict(x_pred)\n\n# Store the predictions in the missing values\ndata_.loc[data_.Income.isna(), \"Income\"] = y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_rep(data_) # No more missings!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CHANGE"},{"metadata":{},"cell_type":"markdown","source":"The same was done to Year_birth since they were inconsistent."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_[(2019 - data_[\"Year_Birth\"])>=90]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing all datasets to be used in the Linear Regression Model\nX = data_\ny = X[(2019 - X[\"Year_Birth\"])<90].Year_Birth\nX = X.drop(columns=[\"ID\"])\n\nX[\"Marital_Status\"] = pd.Categorical(X[\"Marital_Status\"])\nX[\"Marital_Status\"] = X[\"Marital_Status\"].cat.codes\n\nX[\"Education\"] = pd.Categorical(X[\"Education\"])\nX[\"Education\"] = X[\"Education\"].cat.codes\n\nx_pred = X[(2019 - X[\"Year_Birth\"])>=90]\nx_pred = x_pred.drop(columns=\"Year_Birth\")\n\nX = X[(2019 - X[\"Year_Birth\"])<90]\nX = X.drop(columns=\"Year_Birth\")\n\n\n\n\n\n# Linear Regression Model\nreg = LinearRegression().fit(X, y)\n\n# Predictions\ny_pred = reg.predict(x_pred)\n\n# Store the predictions in the missing values\ndata_.loc[(2019 - data_[\"Year_Birth\"])>=90, \"Year_Birth\"] = y_pred.round()\ndata_[\"Year_Birth\"].astype('int')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3.4 Correlation Matrix between numerical variables\n\n\nWe employ the correlation matrix here in order to find variables that show high correlation between them so we could eliminate one or join them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# The function to \"zoom\" in the correlation matrix.\ndef magnify():\n    return [dict(selector=\"th\",\n                 props=[(\"font-size\", \"7pt\")]),\n            dict(selector=\"td\",\n                 props=[('padding', \"0em 0em\")]),\n            dict(selector=\"th:hover\",\n                 props=[(\"font-size\", \"12pt\")]),\n            dict(selector=\"tr:hover td:hover\",\n                 props=[('max-width', '200px'),\n                        ('font-size', '12pt')])]\n\n\n\n# Takes a dataframe and returns the correlation matrix while plotting the correlation matrix plot using hues of blue and red.\ndef corr_matrix(df):\n    # Compute the correlation matrix\n    corr = df.corr()\n\n    cmap = sns.diverging_palette(5, 250, as_cmap=True)\n\n\n    vis = corr.style.background_gradient(cmap, axis=1)\\\n            .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n            .set_caption(\"Hover to magify\")\\\n            .set_precision(2)\\\n            .set_table_styles(magnify())\n\n    return vis\n\n\n\nfeat_n_ = feat_n.copy()\nfeat_n.remove(\"Response\") # Removing the Targer variable from the list of numerical features to be analyzed by correlation.\ncorr_matrix(data_[feat_n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to find the maximum value in a correlation matrix and print the line and column it is located.\ndef max_matrix(matrix):\n\n    aux = 0\n    line = \"\"\n    col = \"\"\n    for l in matrix:\n        for i in matrix:\n            if ((matrix[l][i]<1) & (abs(matrix[l][i])>abs(aux))):\n                aux = matrix[l][i]\n                line = l\n                col = i\n\n\n    print(\"Max Value:\", aux,\n          \"\\nLine:\", line,\n          \"\\nColumn:\", col)\n\n\n    return aux\n\n\nmax_matrix(data_[feat_n_].corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The highest correlation we have is between number of Meat PRoducst purchased and number of Catalog Purchases (0,72).\n\nShould we do something about it?"},{"metadata":{},"cell_type":"markdown","source":"### 1.3.5 Visualization of Numerical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Can give from 1 to all of numeric features a feat_n.\n# Can give a categorical feature to be used (optional).\ndef num_feat_plot(df, feat_nlist, target, feat_clist = None):\n    \n    if(target in feat_nlist):\n        feat_nl = feat_nlist.copy()\n        feat_nl.remove(target)\n    \n    if (feat_clist==None):\n        \n        if (type(feat_nl)==str):\n            \n            sns.violinplot(data = df, y = feat_nl, x = target)\n            plt.show()\n            \n            \n        else:\n            for feat in feat_nl:\n                sns.violinplot(data = df, y = feat, x = target)\n                plt.show()\n        \n    else:\n        if (type(feat_nl)==str):\n            \n            sns.violinplot(data = data_, y = feat_nl, x = feat_clist, hue = target)\n            plt.show()\n            \n            \n        else:\n            for feat in feat_nl:\n                sns.violinplot(data = data_, y = feat, x = feat_clist, hue = target)\n                plt.show()\n                \n    return\n\nnum_feat_plot(data_, feat_n_, \"Response\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that Income holds many high outliers, while Year_Birth holds low outliers. Some of the number of purchases variables also show high outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_feat_plot(data_, feat_n_, \"Response\", \"Kidhome\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Outlier Detection\n\n### 1.4.1 Univariate Outliers\n\n#### Standard Deviation Cutoffs\n\nFirst we will use the Standard Deviation to find univariate outliers in numerical features. Every observation above 3 standard deviations of its own distribution will be considered an outlier.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function that is given a series and returns a list of same size indicating with boolean the outliers.\n# 3 Standard Deviations is the default cutoff.\ndef filter_by_std(series_, n_stdev=3.0, return_thresholds=False):\n    mean_, stdev_ = series_.mean(), series_.std()\n    cutoff = stdev_ * n_stdev\n    lower_bound, upper_bound = mean_ - cutoff, mean_ + cutoff\n    if return_thresholds:\n        return lower_bound, upper_bound\n    else:\n        return [True if i < lower_bound or i > upper_bound else False for i in series_]\n\n    \n# Function that is given a series and returns a list of same size indicating with boolean the outliers.\n# 1.5 Interquartile Range is the default cutoff.\ndef filter_by_iqr(series_, k=1.5, return_thresholds=False):\n    q25, q75 = np.percentile(series_, 25), np.percentile(series_, 75)\n    iqr = q75-q25\n    \n    cutoff = iqr*k\n    lower_bound, upper_bound = q25-cutoff, q75+cutoff\n    \n    if return_thresholds:\n        return lower_bound, upper_bound\n    else:\n        return [True if i < lower_bound or i > upper_bound else False for i in series_]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"outliers = pd.DataFrame()\n\nfor feat in feat_n:\n    outliers = outliers.append(data_[filter_by_std(data_[feat], n_stdev=3)])\n    \n\noutliers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Anomaly Detection with Isolation Forest\n\nThrough the use of Isolation Forest algorithm, from scikitlearn, we can calculate the anomaly score of each point of the numerical features and, then, search for outliers.\n\nRed areas indicate where the probability of observations existing there is low. The blue line, representing the anomaly score, tends to be similar to the distribution of the variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function that takes as parameters a dataframe, list of numerical features names, number of lines and columns of\n# the resulting multiplot and plots the anomaly plots of all numerical features.\ndef anom_plot(df, num_feat_list, l, c):\n    fig, axs = plt.subplots(l, c, figsize=(22, 12), facecolor='w', edgecolor='k')\n    axs = axs.ravel()\n\n    for i, column in enumerate(num_feat_list):\n        isolation_forest = IsolationForest(n_estimators=500, behaviour=\"new\", contamination=\"auto\")\n        isolation_forest.fit(df[column].values.reshape(-1,1))\n\n        xx = np.linspace(df[column].min(), df[column].max(), len(df)).reshape(-1,1)\n        anomaly_score = isolation_forest.decision_function(xx)\n        outlier = isolation_forest.predict(xx)\n    \n        axs[i].plot(xx, anomaly_score, label='anomaly score')\n        axs[i].fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score), \n                     where=outlier==-1, color='r', \n                     alpha=.4, label='outlier region')\n        axs[i].legend()\n        axs[i].set_title(column)\n        \n    return\n    \n    \n    \nanom_plot(data_, feat_n, 3, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4.2 Multivariate Outliers\n#### Mahalanobis Distance\nThe Euclidian distance is known to fail to find outliers when dealing with multi dimensional data. So we use the Mahalanobis Distance, because, since it uses the Eigenvalues of the variables instead of the original axis, it can make something similar to a feature scaling.\n\nBasically it calculates the distance of each point to the center of mass measured in standard deviations through the inverted covariance matrix.\n\nWith these functions we can find the rows which represent multivariate outliers. The only path to take from this is removing these rows, as opposed to inputing the extreme values, like when we are dealing with univariate outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple function to check if the matrix is positive definite (for example, it will return False if the matrix contains NaN).\ndef is_pos_def(A):\n    if np.allclose(A, A.T):\n        try:\n            np.linalg.cholesky(A)\n            return True\n        except np.linalg.LinAlgError:\n            return False\n    else:\n        return False \n\n    \n    \n    \n    \n    \n# The function to calculate the Mahalanobis Distance. Returns a list of distances.\ndef MahalanobisDist(data):\n    covariance_matrix = np.cov(data, rowvar=False)\n    if is_pos_def(covariance_matrix):\n        inv_covariance_matrix = np.linalg.inv(covariance_matrix)\n        if is_pos_def(inv_covariance_matrix):\n            vars_mean = []\n            for i in range(data.shape[0]):\n                vars_mean.append(list(data.mean(axis=0)))\n            diff = data - vars_mean\n            md = []\n            for i in range(len(diff)):\n                md.append(np.sqrt(diff[i].dot(inv_covariance_matrix).dot(diff[i])))\n            return md\n        else:\n            print(\"Error: Inverse of Covariance Matrix is not positive definite!\")\n    else:\n        print(\"Error: Covariance Matrix is not positive definite!\")\n\n        \n        \n        \n        \n# Function to detect multivariate outliers from the Mahalanobis Distances. Returns an array of indexes of the outliers.    \ndef MD_detectOutliers(data, extreme=False):\n    MD = MahalanobisDist(data)\n\n    std = np.std(MD)\n    k = 3. * std if extreme else 2. * std\n    m = np.mean(MD)\n    up_t = m + k\n    low_t = m - k\n    outliers = []\n    for i in range(len(MD)):\n        if (MD[i] >= up_t) or (MD[i] <= low_t):\n            outliers.append(i)  # index of the outlier\n    return np.array(outliers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we are creating an auxiliar dataframe and inputing NaN with the mean so we can calculate the Mahalanobis Distances.\ndata_aux = data_[feat_n_]\n\ndata_aux = data_aux.apply(lambda x: x.fillna(x.mean()), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run the function\noutliers_i = MD_detectOutliers(np.array(data_aux))\nlen(outliers_i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These observations were detected as multivariate outliers by the Mahalanobis Distance method.\n\nWe could simply remove these rows from the dataset, since only 33 outlier observations were detected.\n\nWhat should we do?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rows which were detected as multivariate outliers:\ndat = pd.DataFrame()\nfor i in outliers_i:\n    dat = dat.append(data_aux.iloc[i,:])\n    \ndat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Means of Ranking Value of Features\n\nWe need means of raking the value of features so we can compare them and check which ones are the best in order to feed them as input to the pipeline. In order to do that we want an algorithm that can give us some measure of discrimination ability of all features in regards to our Response feature.\n\nAfter we have one tool to measure the value, we can go ahead and create new features and compare them with the ones we already have.\n\n### 2.1 The $\\chi ^2$ Independence Test\n\nThe $\\chi ^2$ test for independence, despite having the null hypothesis of independence of variables, can also measure the degree of association between two variables, which means we can use it to check which feature is similar to our Response variable.\n\nIn order to use as input of the test a numerical feature, the binning strategy with 10 bins was employed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Is given as input a dataframe, a list of continuous features names, a list of categorical features names,\n# the name of the target feature and returns a dataframe with the discrimination ability of each feature and if\n# its p-value is lower than 0.05.\n# 10 is the default number of bins and uniform is the strategy used in the binning of continuous features.\ndef chisq_ranker(df, continuous_flist, categorical_flist, target, n_bins=10, binning_strategy=\"uniform\"):\n    chisq_dict = {}\n    if  continuous_flist:\n        bindisc = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', \n                               strategy=binning_strategy)\n        for feature in continuous_flist:            \n            feature_bin = bindisc.fit_transform(df[feature].values[:, np.newaxis])\n            feature_bin = pd.Series(feature_bin[:, 0], index=df.index)\n            cont_tab = pd.crosstab(feature_bin, df[target], margins = False)\n            chisq_dict[feature] = stats.chi2_contingency(cont_tab.values)[0:2] \n    if  categorical_flist:\n        for feature in categorical_flist:  \n            cont_tab = pd.crosstab(df[feature], df[target], margins = False)          \n            chisq_dict[feature] = stats.chi2_contingency(cont_tab.values)[0:2]       \n    \n    df_chi = pd.DataFrame(chisq_dict, index=[\"Chi-Squared\", \"p-value\"]).transpose()\n    df_chi.sort_values(\"Chi-Squared\", ascending=False, inplace=True)\n    df_chi[\"valid\"]=df_chi[\"p-value\"]<=0.05\n    \n    \n    return df_chi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now check the similarity of each feature with the Response, which, in turn, means we can check the discrimination ability of our features.\n\nIn the table we can see that, according to the test, the feature AcceptedCmp1 presents the higher discrimination ability, followed by the AcceptedCmp5 feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"chisq_ranker(data_, feat_n, feat_c, \"Response\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Decision Tree Algorithm for Measuring Feature Worth\nEven though the purpose of a DTA is not exactly measuring worth of features, we can use it that way. By employing the DecisionTreeClassifier from scikit-learn we can pull the features importances from the class."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function that receives a dataframe, list of feature names, name of target and DecisionTreeClassifier paramethers and\n# returns a df with all features with a worth higher than zero and plots it.\ndef dta_feat_worth(df, feat_list, target, max_depth, min_samples_split, min_samples_leaf, seed):\n    \n    # Preparing the Input Data for the DTA\n    X = data_.loc[:, feat_list].values\n    y = data_[target].values\n    \n    \n    # Run the estimation through DecisionTreeClassifier\n    dtree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4, min_samples_split=50, \n                                   min_samples_leaf = 20, random_state=seed)\n    # Fits the DTClassifier with our data\n    dtree = dtree.fit(X, y)\n    \n    \n    # Create a dictionary with the name of all features and its importances according to the DTA estimation\n    fi = dict(zip(columns, dtree.feature_importances_))\n    # Then creates a Dataframe with it\n    fidf = pd.DataFrame(fi, index=[\"worth\"])\n    # Transpose it because the way it is created it is on the other orientation\n    fidf_t = fidf.transpose().sort_values(by=\"worth\", ascending=False)\n     # Removes features with worth 0 and puts it into a df called worth_df\n    worth_df = fidf_t[fidf_t.worth>0]\n\n    # Uses seaborn to create a plot with the worth of features\n    plt.style.use('seaborn-whitegrid')\n    axes = worth_df.sort_values(by=\"worth\").plot.barh(y='worth', color='gray', legend=False)\n    axes.set_title(\"Worth of features\")\n    # Repositions legend\n    plt.legend(loc='lower right')\n    plt.show()\n    \n    return worth_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a feature list without categorical variables, ID and Response\ncolumns = data_.columns\ncolumns = columns.drop([\"ID\", \"Response\", \"Marital_Status\", \"Education\"])\n\n\ndta_feat_worth(data_, columns, \"Response\", 5, 100, 10, seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Feature Engineering\n\nNow, with a mean of comparing features according to their value, we can extract new features from the ones we already have in order to search for better features.\n\nWe will create all the new features and will compare and rank them afterwards, in the next section (4.)\n\n### 3.1 Power Transformations\n\nBy applying the Box-Cox transformations on all features, we can try and find the ones that best fit each one in terms of discrimination ability.\n\nBut first, we need to perform feature scaling on continuous variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Given a dataframe, numerical featue name list and a suffix for the name of the new variables returns a dataframe with the\n# original and new variables.\ndef MinMaxScaling(df, num_feat_list, suffix):\n\n    data_scaler = df[num_feat_list]\n\n    fscaler = MinMaxScaler()\n    scaled_d = fscaler.fit_transform(data_scaler.values)\n\n    colnames = [s + suffix for s in data_scaler.columns]\n\n    return pd.concat([df, pd.DataFrame(scaled_d, index=data_scaler.index, columns=colnames)], axis=1)\n\n\ndata_t = MinMaxScaling(data_, feat_n, \"_t\")\ndata_t.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Receives a dataframe consisting only of scaled features and the target, and the name of the target feature.\n# Returns both the dataframe with the features already transformed to the best transformation and a dictionary\n# containing the name of each feature with its best transformation name.\n\ndef power_transf(df, target_feat):\n\n    # define a set of transformations\n    trans_dict = {\"x\": lambda x: x, \"log\": np.log, \"sqrt\": np.sqrt, \n                  \"exp\": np.exp, \"**1/4\": lambda x: np.power(x, 0.25), \n                  \"**2\": lambda x: np.power(x, 2), \"**4\": lambda x: np.power(x, 4)}\n\n    target = target_feat\n    best_power_dict = {}\n    for feature in df.columns[:-1]:\n        max_test_value, max_trans, best_power_trans = 0, \"\", None\n        for trans_key, trans_value in trans_dict.items():\n            # apply transformation\n            feature_trans = trans_value(df[feature])\n            if trans_key == \"log\":\n                feature_trans.loc[np.isfinite(feature_trans)==False] = -50.\n\n            # bin feature\n            bindisc = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy=\"uniform\")\n            feature_bin = bindisc.fit_transform(feature_trans.values[:, np.newaxis])\n            feature_bin = pd.Series(feature_bin[:, 0], index=df.index)\n\n            # obtain contingency table\n            df_ = pd.DataFrame(data={feature: feature_bin, target: df[target]})\n            cont_tab = pd.crosstab(df_[feature], df_[target], margins = False)        \n\n            # compute Chi-Squared\n            chi_test_value = stats.chi2_contingency(cont_tab)[0]\n            if chi_test_value > max_test_value:\n                max_test_value, max_trans, best_power_trans = chi_test_value, trans_key, feature_trans      \n\n        best_power_dict[feature] = (max_test_value, max_trans, best_power_trans)\n        df[feature] = best_power_trans\n        \n    return df, best_power_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to create a dataframe containing only the scaled features with the Response.\ndf_pt = data_t.iloc[:,-15:]\ndf_pt[\"Response\"] = data_[\"Response\"]\n\n\n\ndata_pt, best_pt = power_transf(df_pt, \"Response\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best Power Transformation for each feature:\")\nfor key in best_pt:\n    print(\"\\t->\", key, best_pt[key][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing the old columns of scaled features with the features transformed according to the best transformation\ncoln = data_pt.columns[:-1]\ndata_t.drop(columns=coln, inplace=True)\ndata_t[coln] = data_pt[coln]\n\ndata_ = data_t\ndata_.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Business Oriented Transformations\n\nThere are some transformations to our variables that can generate new features. Now we focus on the creation of business oriented features, like RFM, Monetary, Frequency, Proportion of Gold Products, etc.\n\nThey are built as follows:\n#### 3.2.1 Numerical Transformations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of Monetary Units spent on gold products out of the total spent\naux = [0]* data_.shape[0]\n\n\nfor i in range(data_.shape[0]):\n    aux[i] = data_[\"MntGoldProds\"].iloc[i]/sum(data_[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:])\n    \n    \ndata_[\"PrpGoldProds\"] = aux\ndata_[\"PrpGoldProds\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of Accepted Campaigns out of the last 5 Campaigns\naux = [0]* data_.shape[0]\n\n\nfor i in range(data_.shape[0]):\n    aux[i] = sum(data_[['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2']].iloc[i,:])\n    \n    \ndata_[\"NmbAccCmps\"] = aux\ndata_[\"NmbAccCmps\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Accepted Campaigns out of the last 5 Campaigns\naux = [0]* data_.shape[0]\n\n\nfor i in range(data_.shape[0]):\n    aux[i] = sum(data_[['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2']].iloc[i,:])/5\n    \n    \ndata_[\"PrpAccCmps\"] = aux\ndata_[\"PrpAccCmps\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Wine out of the total spent\naux = [0]* data_.shape[0]\n\n\nfor i in range(data_.shape[0]):\n    aux[i] = float(data_[[\"MntWines\"]].iloc[i,:]/sum(data_[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \ndata_[\"PrpWines\"] = aux\ndata_[\"PrpWines\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Fruits out of the total spent\naux = [0]* data_.shape[0]\n\n\nfor i in range(data_.shape[0]):\n    aux[i] = float(data_[[\"MntFruits\"]].iloc[i,:]/sum(data_[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \ndata_[\"PrpFruits\"] = aux\ndata_[\"PrpFruits\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Meat out of the total spent\naux = [0]* data_.shape[0]\n\n\nfor i in range(data_.shape[0]):\n    aux[i] = float(data_[[\"MntMeatProducts\"]].iloc[i,:]/sum(data_[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \ndata_[\"PrpMeat\"] = aux\ndata_[\"PrpMeat\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Fish out of the total spent\naux = [0]* data_.shape[0]\n\n\nfor i in range(data_.shape[0]):\n    aux[i] = float(data_[[\"MntFishProducts\"]].iloc[i,:]/sum(data_[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \ndata_[\"PrpFish\"] = aux\ndata_[\"PrpFish\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Proportion of Monetary Units spent on Sweets out of the total spent\naux = [0]* data_.shape[0]\n\n\nfor i in range(data_.shape[0]):\n    aux[i] = float(data_[[\"MntSweetProducts\"]].iloc[i,:]/sum(data_[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:]))\n    \ndata_[\"PrpSweets\"] = aux\ndata_[\"PrpSweets\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Monetary\naux = [0]* data_.shape[0]\n\n\nfor i in range(data_.shape[0]):\n    aux[i] = sum(data_[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:])\n    \n    \ndata_[\"Mnt\"] = aux\ndata_[\"Mnt\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Buy Potential\naux = [0]* data_.shape[0]\n\n\nfor i in range(data_.shape[0]):\n    aux[i] = float(sum(data_[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts']].iloc[i,:])/((data_[[\"Income\"]].iloc[i,:])*2))\n    \n    \ndata_[\"BuyPot\"] = aux\ndata_[\"BuyPot\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Frequency\naux = [0]* data_.shape[0]\n\n\nfor i in range(data_.shape[0]):\n    aux[i] = sum(data_[['NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases']].iloc[i,:])\n    \n    \ndata_[\"Freq\"] = aux\ndata_[\"Freq\"].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating RFM feature using Recency, Freq and Mnt:\nfeature_list, n_bins = [\"Recency\", \"Freq\", \"Mnt\"], 5\nrfb_dict = {}\nfor feature in feature_list:\n    bindisc = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy=\"quantile\")\n    feature_bin = bindisc.fit_transform(data_[feature].values[:, np.newaxis])\n    feature_bin = pd.Series(feature_bin[:, 0], index=data_.index)\n    feature_bin += 1\n    \n    if feature == \"Recency\":\n        feature_bin = feature_bin.sub(5).abs() + 1\n    rfb_dict[feature+\"_bin\"] = feature_bin.astype(int).astype(str)\n\ndata_[\"RFM\"] = (rfb_dict['Recency_bin'] + rfb_dict['Freq_bin'] + rfb_dict['Mnt_bin']).astype(int)\ndata_.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.2 Merging Categories\nNow, regarding business oriented transformations we also need to merge categories without a high dicsimination ability in order to facilitate te ML algorithm we will employ.\n\nBy looking at the discrimination abilities of our features at point 1.2.2, we were able to define these strategies:\n- Education: Phd, Master, others;\n- Marital_Status: Single, Widow, Divorced, others;\n- Kidhome & Teenhome: will be merged into a binary feature indicating presence of offspring."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging Categories\n# in Marital_Status:  \"Single\" as 3, \"Widow\" as 2, \"Divorced\" as 1 and [\"Married\", \"Together\"] as 0\ndata_[\"Marital_Status_bin\"] = data_['Marital_Status'].apply(lambda x: 3 if x == \"Single\" else\n                                                            (2 if x == \"Widow\" else\n                                                             (1 if x == \"Divorced\" else 0))).astype(int)\n\n# in Education: \"Phd\" as 2, \"Master\" as 1 and ['Graduation', 'Basic', '2n Cycle'] as 0\ndata_[\"Education_bin\"] = data_['Education'].apply(lambda x: 2 if x == \"PhD\" else (1 if x == \"Master\" else 0)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_[\"Kidhome\"] = data_['Kidhome'].astype(int)\ndata_[\"Teenhome\"] = data_['Teenhome'].astype(int)\ndata_[\"NumberOff\"] = data_['NumberOff'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# HasOffsrping Feature\n\"\"\"\naux = [0]* data_.shape[0]\n\n\nfor i in range(data_.shape[0]):\n    if(int(data_[[\"Kidhome\"]].iloc[i,:])+int(data_[[\"Teenhome\"]].iloc[i,:])>0):\n        aux[i] = 1\n    else:\n        aux[i] = 0\n    \ndata_[\"HasOffspring\"] = aux\ndata_[\"HasOffspring\"].head()\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding these new categorical features to the list:\n#feat_c.append(\"Marital_Status_bin\")\n#feat_c.append(\"Education_bin\")\n#feat_c.append(\"HasOffspring\")\n\n# Our data now:\ndata_.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.2.3 Principal Component Analysis (PCA)\nPCA is a useful tool for dimensionality reduction. We will try to use it as a transformation tool, as it could create useful new features consisting of the principal components grouping the other variables. It could lead to a good summarization of the data or an excessive loss of information.\n\nFirst we will try an PCA with 2 components:"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = data_.columns\ncolumns = columns.drop([\"ID\", \"Response\", \"Marital_Status\", \"Education\"])\n\n\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(data_[columns])\n\ndata_[\"pc1_\"] = principalComponents[:,0]\ndata_[\"pc2_\"] = principalComponents[:,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then a PCA with 5 components:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=5)\nprincipalComponents = pca.fit_transform(data_[columns])\n\ndata_[\"pc1\"] = principalComponents[:,0]\ndata_[\"pc2\"] = principalComponents[:,1]\ndata_[\"pc3\"] = principalComponents[:,2]\ndata_[\"pc4\"] = principalComponents[:,3]\ndata_[\"pc5\"] = principalComponents[:,4]\n\ndata_.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Evaluating Feature Selection\nIn order to assess if the feature selection performed is appropriate we will use K-Means clustering to measure accuracy.\n\nFirst we will try to choose the best selection of features we have."},{"metadata":{"trusted":true},"cell_type":"code","source":"# New numerical feature list:\nfeat_n = data_.columns\nfeat_n = feat_n.drop(feat_c)\nfeat_n = feat_n.drop([\"ID\", \"Response\", \"PrpGoldProds\", \"NmbAccCmps\"])\nfeat_n = list(feat_n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chisq_ranker(data_, feat_n, feat_c, \"Response\").head(15) # First 15 ranked features according to chisq_ranker","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting a feature list without categorical variables, ID and Response\ncolumns = data_.columns\ncolumns = columns.drop([\"ID\", \"Response\", \"Marital_Status\", \"Education\"])\n\n\ndta_feat_worth(data_t, columns, \"Response\", 5, 100, 10, seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the tests above we can see some similarities but there is discrepancy in a sense that, in general, for the chisq_ranker the counting features are prioritized, while the dta_feat_worth test seems to give proportion features more value.\n\n\nWe will try to arrange a selection of features that incorporate the best mix of the results of both tests.\n\nThis attempt will be with this list of features:\n- PrpAccCmps\n- RFM\n- Mnt\n- PrpMeat\n- Days_Customer_t\n- MntWines_t\n- PrpGoldProds\n- Marital_Status_bin\n\n$Note$: Even though pc5 seems like a good choice of feature, we will not use any of the new PCA features here because it will have a weird interaction when we use PCA to plot the K-Means result in two dimensions."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_list = [\"PrpAccCmps\", \"RFM\", \"Mnt\", \"PrpMeat\", \"Days_Customer_t\",\n                     \"MntWines_t\", \"PrpGoldProds\", \"Marital_Status_bin\"]\n\nfeatures_df = data_t[feature_list]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we use a Standard Scaler to perform feature scaling on all selected features."},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_features = StandardScaler().fit_transform(features_df.values)\nscaled_features_df = pd.DataFrame(scaled_features, index=features_df.index, columns=features_df.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Setting up the K-Means model with $K=2$ as the target feature, Response, shows only two states (0 or 1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"K = 2\n\nkmeans = KMeans(n_clusters=K, max_iter=1000, random_state=12345, init = 'k-means++') # Set the number of clusters we want, the maximum number of iteration and a seed for the random state.\nkmeans.fit(scaled_features_df)\nlabels = kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now plot the results.\n\nIn a bidimensional space we can only compare two features a time.\n\nUse x and y to select the variable according to its position in the feature_list:"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting the resulting clusters:\n\nplt.figure(num=None, figsize=(7, 7), dpi=80, facecolor='w', edgecolor='k')\n\n\nx = 3 # Select the variable that goes into the X axis in the graph.\ny = 5 # Select the variable that goes into the Y axis in the graph.\nplt.scatter(scaled_features_df.iloc[:, x], scaled_features_df.iloc[:, y], c=labels, s=5, cmap='viridis')\n\n\n\n\ncenters_cv = kmeans.cluster_centers_\nplt.scatter(centers_cv[:, x], centers_cv[:, y], c='black', s=200, alpha=0.5);\nplt.xlabel(scaled_features_df.columns[x], fontsize=10)\nplt.ylabel(scaled_features_df.columns[y], fontsize=10)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In order to achieve a multidimensional behaviour evaluation of the K-Means model we will use Principal Component Analysis (PCA) to perform a dimension reduction from N to 2. PCA achieves this through an orthogonal transformation."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(num=None, figsize=(7, 7), dpi=80, facecolor='w', edgecolor='k')\n\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(scaled_features_df)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['Principal Component 1', 'Principal Component 2'])\n\n\nplt.scatter(principalDf.iloc[:, 0], principalDf.iloc[:, 1], c=labels, s=5, cmap='viridis')\nplt.xlabel(principalDf.columns[0], fontsize=10)\nplt.ylabel(principalDf.columns[1], fontsize=10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the balance of points assigned to each cluster. More points were assigned to cluster 0 than to cluster 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(data_t[\"Response\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the model assigned 1419 to one cluster and 367 to another, only 265 out of the total clients did respond positively to the campaign.\n\n367 is somewhat close to 265."},{"metadata":{},"cell_type":"markdown","source":"In order to measure it accuracy we count how many points the K-Means model assigned correctly and divide it by the lenght of our database:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sum(labels==data_t[\"Response\"])/len(data_t[\"Response\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"79% of Accuracy is a decent measure. We could tinker a little more with the feature selection or go on to the pipeline."},{"metadata":{},"cell_type":"markdown","source":"## 5. Balancing Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_complete = data_t.drop(columns=['ID', 'Education', 'Marital_Status'])\ndata_complete.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_columns = ['Marital_Status_bin','Education_bin']\ndata_complete = pd.get_dummies(data_t, prefix_sep=\"_\",\n                              columns=cat_columns)\ndata_complete = data_complete.drop(columns=['ID', 'Education', 'Marital_Status'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_complete.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data, test_data, train_label, test_label = train_test_split(data_complete,\n                                                                  data_complete[\"Response\"],\n                                                                  test_size=0.3,\n                                                                  random_state=seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\ndf_scaled = scaler.fit_transform(train_data)\ndf_scaled = pd.DataFrame(df_scaled, columns=[train_data.columns])\ndf_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_complete['Response'].value_counts().plot(kind='bar', title='Count (target)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop = ['Response','Education', 'Marital_Status']\nlabels = data_complete.columns[~data_complete.columns.isin(drop)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Undersampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler(random_state=0)\nX_resampled, y_resampled = rus.fit_resample(df_scaled[labels], df_scaled['Response'])\ndf_under = pd.DataFrame(X_resampled, columns=labels)\ndf_under['Response'] = y_resampled\ndf_under['Response'].value_counts().plot(kind='bar', title='Count (target)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_under.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Oversampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(ratio='minority', n_jobs=-1)\nX_sm, y_sm = smote.fit_resample(df_scaled[labels], df_scaled['Response'])\n\ndf_over = pd.DataFrame(X_sm, columns=labels)\ndf_over['Response'] = y_sm\n\ndf_over['Response'].value_counts().plot(kind='bar', title='Count (target)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_over.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Export Pipeline\nAnd when we are satisfied we can export our data as .xslx to be fed into our pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"#data_t.to_excel(\"train_dataset_seed1.xlsx\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Models to test:\n- Decision Tree\n- Logistic Regression\n- Random Forest\n- SVM\n- ANN"},{"metadata":{},"cell_type":"markdown","source":"## Deep Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(5, activation='relu', input_dim=66))\nmodel.add(layers.Dense(5, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_data, train_label, epochs=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(test_data, test_label)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}