{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Loading the data and exploring its shape and values\n\n","metadata":{}},{"cell_type":"markdown","source":"# 1. библиотеки","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.utils import resample\nimport itertools\n\nimport pywt\ntry:\n    import pathlib\nexcept ImportError:\n    import pathlib2 as pathlib\nimport scipy.signal as signal\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import f1_score\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout, InputLayer, LSTM, GRU, BatchNormalization, Bidirectional, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.optimizers import SGD, RMSprop\nfrom tensorflow.keras.utils import to_categorical","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2021-08-01T16:09:39.903465Z","iopub.execute_input":"2021-08-01T16:09:39.904474Z","iopub.status.idle":"2021-08-01T16:09:39.929598Z","shell.execute_reply.started":"2021-08-01T16:09:39.904408Z","shell.execute_reply":"2021-08-01T16:09:39.928169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. вспомогательные функции\n## 2.1 класс детекторов для нахождения пик","metadata":{}},{"cell_type":"code","source":"#класс детекторов дл нахождения пик\nclass Detectors:\n    \"\"\"ECG heartbeat detection algorithms\n    General useage instructions:\n    r_peaks = detectors.the_detector(ecg_in_samples)\n    The argument ecg_in_samples is a single channel ECG in volt\n    at the given sample rate.\n    \"\"\"\n    \n    def __init__(self, sampling_frequency):\n        \"\"\"\n        The constructor takes the sampling rate in Hz of the ECG data.\n        \"\"\"\n\n        self.fs = sampling_frequency\n        # this is set to a positive value for benchmarking\n        self.engzee_fake_delay = 0\n\n    def hamilton_detector(self, unfiltered_ecg):\n        \"\"\"\n        P.S. Hamilton, \n        Open Source ECG Analysis Software Documentation, E.P.Limited, 2002.\n        \"\"\"\n        \n        f1 = 8/self.fs\n        f2 = 16/self.fs\n\n        b, a = signal.butter(1, [f1*2, f2*2], btype='bandpass')\n\n        filtered_ecg = signal.lfilter(b, a, unfiltered_ecg)\n\n        diff = abs(np.diff(filtered_ecg))\n\n        b = np.ones(int(0.08*self.fs))\n        b = b/int(0.08*self.fs)\n        a = [1]\n\n        ma = signal.lfilter(b, a, diff)\n\n        ma[0:len(b)*2] = 0\n\n        n_pks = []\n        n_pks_ave = 0.0\n        s_pks = []\n        s_pks_ave = 0.0\n        QRS = [0]\n        RR = []\n        RR_ave = 0.0\n\n        th = 0.0\n\n        i=0\n        idx = []\n        peaks = []  \n\n        for i in range(len(ma)):\n\n            if i>0 and i<len(ma)-1:\n                if ma[i-1]<ma[i] and ma[i+1]<ma[i]:\n                    peak = i\n                    peaks.append(i)\n\n                    if ma[peak] > th and (peak-QRS[-1])>0.3*self.fs:        \n                        QRS.append(peak)\n                        idx.append(i)\n                        s_pks.append(ma[peak])\n                        if len(n_pks)>8:\n                            s_pks.pop(0)\n                        s_pks_ave = np.mean(s_pks)\n\n                        if RR_ave != 0.0:\n                            if QRS[-1]-QRS[-2] > 1.5*RR_ave:\n                                missed_peaks = peaks[idx[-2]+1:idx[-1]]\n                                for missed_peak in missed_peaks:\n                                    if missed_peak-peaks[idx[-2]]>int(0.360*self.fs) and ma[missed_peak]>0.5*th:\n                                        QRS.append(missed_peak)\n                                        QRS.sort()\n                                        break\n\n                        if len(QRS)>2:\n                            RR.append(QRS[-1]-QRS[-2])\n                            if len(RR)>8:\n                                RR.pop(0)\n                            RR_ave = int(np.mean(RR))\n\n                    else:\n                        n_pks.append(ma[peak])\n                        if len(n_pks)>8:\n                            n_pks.pop(0)\n                        n_pks_ave = np.mean(n_pks)\n\n                    th = n_pks_ave + 0.45*(s_pks_ave-n_pks_ave)\n\n                    i+=1\n\n        QRS.pop(0)\n\n        return QRS\n\n    \n    def christov_detector(self, unfiltered_ecg):\n        \"\"\"\n        Ivaylo I. Christov, \n        Real time electrocardiogram QRS detection using combined \n        adaptive threshold, BioMedical Engineering OnLine 2004, \n        vol. 3:28, 2004.\n        \"\"\"\n        total_taps = 0\n\n        b = np.ones(int(0.02*self.fs))\n        b = b/int(0.02*self.fs)\n        total_taps += len(b)\n        a = [1]\n\n        MA1 = signal.lfilter(b, a, unfiltered_ecg)\n\n        b = np.ones(int(0.028*self.fs))\n        b = b/int(0.028*self.fs)\n        total_taps += len(b)\n        a = [1]\n\n        MA2 = signal.lfilter(b, a, MA1)\n\n        Y = []\n        for i in range(1, len(MA2)-1):\n            \n            diff = abs(MA2[i+1]-MA2[i-1])\n\n            Y.append(diff)\n\n        b = np.ones(int(0.040*self.fs))\n        b = b/int(0.040*self.fs)\n        total_taps += len(b)\n        a = [1]\n\n        MA3 = signal.lfilter(b, a, Y)\n\n        MA3[0:total_taps] = 0\n\n        ms50 = int(0.05*self.fs)\n        ms200 = int(0.2*self.fs)\n        ms1200 = int(1.2*self.fs)\n        ms350 = int(0.35*self.fs)\n\n        M = 0\n        newM5 = 0\n        M_list = []\n        MM = []\n        M_slope = np.linspace(1.0, 0.6, ms1200-ms200)\n        F = 0\n        F_list = []\n        R = 0\n        RR = []\n        Rm = 0\n        R_list = []\n\n        MFR = 0\n        MFR_list = []\n\n        QRS = []\n\n        for i in range(len(MA3)):\n\n            # M\n            if i < 5*self.fs:\n                M = 0.6*np.max(MA3[:i+1])\n                MM.append(M)\n                if len(MM)>5:\n                    MM.pop(0)\n\n            elif QRS and i < QRS[-1]+ms200:\n                newM5 = 0.6*np.max(MA3[QRS[-1]:i])\n                if newM5>1.5*MM[-1]:\n                    newM5 = 1.1*MM[-1]\n\n            elif QRS and i == QRS[-1]+ms200:\n                if newM5==0:\n                    newM5 = MM[-1]\n                MM.append(newM5)\n                if len(MM)>5:\n                    MM.pop(0)    \n                M = np.mean(MM)    \n            \n            elif QRS and i > QRS[-1]+ms200 and i < QRS[-1]+ms1200:\n\n                M = np.mean(MM)*M_slope[i-(QRS[-1]+ms200)]\n\n            elif QRS and i > QRS[-1]+ms1200:\n                M = 0.6*np.mean(MM)\n\n            # F\n            if i > ms350:\n                F_section = MA3[i-ms350:i]\n                max_latest = np.max(F_section[-ms50:])\n                max_earliest = np.max(F_section[:ms50])\n                F = F + ((max_latest-max_earliest)/150.0)\n\n            # R\n            if QRS and i < QRS[-1]+int((2.0/3.0*Rm)):\n\n                R = 0\n\n            elif QRS and i > QRS[-1]+int((2.0/3.0*Rm)) and i < QRS[-1]+Rm:\n\n                dec = (M-np.mean(MM))/1.4\n                R = 0 + dec\n\n\n            MFR = M+F+R\n            M_list.append(M)\n            F_list.append(F)\n            R_list.append(R)\n            MFR_list.append(MFR)\n\n            if not QRS and MA3[i]>MFR:\n                QRS.append(i)\n            \n            elif QRS and i > QRS[-1]+ms200 and MA3[i]>MFR:\n                QRS.append(i)\n                if len(QRS)>2:\n                    RR.append(QRS[-1]-QRS[-2])\n                    if len(RR)>5:\n                        RR.pop(0)\n                    Rm = int(np.mean(RR))\n\n        QRS.pop(0)\n        \n        return QRS\n\n    \n    def engzee_detector(self, unfiltered_ecg):\n        \"\"\"\n        C. Zeelenberg, A single scan algorithm for QRS detection and\n        feature extraction, IEEE Comp. in Cardiology, vol. 6,\n        pp. 37-42, 1979 with modifications A. Lourenco, H. Silva,\n        P. Leite, R. Lourenco and A. Fred, “Real Time\n        Electrocardiogram Segmentation for Finger Based ECG\n        Biometrics”, BIOSIGNALS 2012, pp. 49-54, 2012.\n        \"\"\"\n                \n        f1 = 48/self.fs\n        f2 = 52/self.fs\n        b, a = signal.butter(4, [f1*2, f2*2], btype='bandstop')\n        filtered_ecg = signal.lfilter(b, a, unfiltered_ecg)\n\n        diff = np.zeros(len(filtered_ecg))\n        for i in range(4, len(diff)):\n            diff[i] = filtered_ecg[i]-filtered_ecg[i-4]\n\n        ci = [1,4,6,4,1]        \n        low_pass = signal.lfilter(ci, 1, diff)\n\n        low_pass[:int(0.2*self.fs)] = 0\n      \n        ms200 = int(0.2*self.fs)\n        ms1200 = int(1.2*self.fs)        \n        ms160 = int(0.16*self.fs)\n        neg_threshold = int(0.01*self.fs)\n\n        M = 0\n        M_list = []\n        neg_m = []\n        MM = []\n        M_slope = np.linspace(1.0, 0.6, ms1200-ms200)\n\n        QRS = []\n        r_peaks = []\n\n        counter = 0\n\n        thi_list = []\n        thi = False\n        thf_list = []\n        thf = False\n\n        for i in range(len(low_pass)):\n\n            # M\n            if i < 5*self.fs:\n                M = 0.6*np.max(low_pass[:i+1])\n                MM.append(M)\n                if len(MM)>5:\n                    MM.pop(0)\n\n            elif QRS and i < QRS[-1]+ms200:\n\n                newM5 = 0.6*np.max(low_pass[QRS[-1]:i])\n\n                if newM5>1.5*MM[-1]:\n                    newM5 = 1.1*MM[-1]\n\n            elif QRS and i == QRS[-1]+ms200:\n                MM.append(newM5)\n                if len(MM)>5:\n                    MM.pop(0)    \n                M = np.mean(MM)    \n            \n            elif QRS and i > QRS[-1]+ms200 and i < QRS[-1]+ms1200:\n\n                M = np.mean(MM)*M_slope[i-(QRS[-1]+ms200)]\n\n            elif QRS and i > QRS[-1]+ms1200:\n                M = 0.6*np.mean(MM)\n\n            M_list.append(M)\n            neg_m.append(-M)\n\n\n            if not QRS and low_pass[i]>M:\n                QRS.append(i)\n                thi_list.append(i)\n                thi = True\n            \n            elif QRS and i > QRS[-1]+ms200 and low_pass[i]>M:\n                QRS.append(i)\n                thi_list.append(i)\n                thi = True\n\n            if thi and i<thi_list[-1]+ms160:\n                if low_pass[i]<-M and low_pass[i-1]>-M:\n                    #thf_list.append(i)\n                    thf = True\n                    \n                if thf and low_pass[i]<-M:\n                    thf_list.append(i)\n                    counter += 1\n                \n                elif low_pass[i]>-M and thf:\n                    counter = 0\n                    thi = False\n                    thf = False\n            \n            elif thi and i>thi_list[-1]+ms160:\n                    counter = 0\n                    thi = False\n                    thf = False                                        \n            \n            if counter>neg_threshold:\n                unfiltered_section = unfiltered_ecg[thi_list[-1]-int(0.01*self.fs):i]\n                r_peaks.append(self.engzee_fake_delay+\n                               np.argmax(unfiltered_section)+thi_list[-1]-int(0.01*self.fs))\n                counter = 0\n                thi = False\n                thf = False\n\n        return r_peaks\n\n    \n    def matched_filter_detector(self, unfiltered_ecg, template_file = \"\"):\n        \"\"\"\n        FIR matched filter using template of QRS complex.\n        Template provided for 250Hz and 360Hz. Optionally provide your\n        own template file where every line has one sample.\n        Uses the Pan and Tompkins thresholding method.\n        \"\"\"\n        current_dir = pathlib.Path(__file__).resolve()\n\n        if len(template_file) > 1:\n            template = np.loadtxt(template_file)\n        else:\n            if self.fs == 250:\n                template_dir = current_dir.parent/'templates'/'template_250hz.csv'\n                template = np.loadtxt(template_dir)\n            elif self.fs == 360:\n                template_dir = current_dir.parent/'templates'/'template_360hz.csv'\n                template = np.loadtxt(template_dir)\n            else:\n                print('\\n!!No template for this frequency!!\\n')\n                return False\n\n        f0 = 0.1/self.fs\n        f1 = 48/self.fs\n\n        b, a = signal.butter(4, [f0*2, f1*2], btype='bandpass')\n\n        prefiltered_ecg = signal.lfilter(b, a, unfiltered_ecg)\n\n        matched_coeffs = template[::-1]  #time reversing template\n\n        detection = signal.lfilter(matched_coeffs, 1, prefiltered_ecg)  # matched filter FIR filtering\n        squared = detection*detection  # squaring matched filter output\n        squared[:len(template)] = 0\n\n        squared_peaks = panPeakDetect(squared, self.fs)\n  \n        return squared_peaks\n\n    \n    def swt_detector(self, unfiltered_ecg):\n        \"\"\"\n        Stationary Wavelet Transform \n        based on Vignesh Kalidas and Lakshman Tamil. \n        Real-time QRS detector using Stationary Wavelet Transform \n        for Automated ECG Analysis. \n        In: 2017 IEEE 17th International Conference on \n        Bioinformatics and Bioengineering (BIBE). \n        Uses the Pan and Tompkins thresolding.\n        \"\"\"\n        \n        swt_level=3\n        padding = -1\n        for i in range(1000):\n            if (len(unfiltered_ecg)+i)%2**swt_level == 0:\n                padding = i\n                break\n\n        if padding > 0:\n            unfiltered_ecg = np.pad(unfiltered_ecg, (0, padding), 'edge')\n        elif padding == -1:\n            print(\"Padding greater than 1000 required\\n\")    \n\n        swt_ecg = pywt.swt(unfiltered_ecg, 'db3', level=swt_level)\n        swt_ecg = np.array(swt_ecg)\n        swt_ecg = swt_ecg[0, 1, :]\n\n        squared = swt_ecg*swt_ecg\n\n        f1 = 0.01/self.fs\n        f2 = 10/self.fs\n\n        b, a = signal.butter(3, [f1*2, f2*2], btype='bandpass')\n        filtered_squared = signal.lfilter(b, a, squared)       \n\n        filt_peaks = panPeakDetect(filtered_squared, self.fs)\n        \n        return filt_peaks\n\n\n    def pan_tompkins_detector(self, unfiltered_ecg):\n        \"\"\"\n        Jiapu Pan and Willis J. Tompkins.\n        A Real-Time QRS Detection Algorithm. \n        In: IEEE Transactions on Biomedical Engineering \n        BME-32.3 (1985), pp. 230–236.\n        \"\"\"\n        \n        f1 = 5/self.fs\n        f2 = 15/self.fs\n\n        b, a = signal.butter(1, [f1*2, f2*2], btype='bandpass')\n\n        filtered_ecg = signal.lfilter(b, a, unfiltered_ecg)        \n\n        diff = np.diff(filtered_ecg) \n\n        squared = diff*diff\n\n        N = int(0.12*self.fs)\n        mwa = MWA(squared, N)\n        mwa[:int(0.2*self.fs)] = 0\n\n        mwa_peaks = panPeakDetect(mwa, self.fs)\n\n        return mwa_peaks\n\n\n    def two_average_detector(self, unfiltered_ecg):\n        \"\"\"\n        Elgendi, Mohamed & Jonkman, \n        Mirjam & De Boer, Friso. (2010).\n        Frequency Bands Effects on QRS Detection.\n        The 3rd International Conference on Bio-inspired Systems \n        and Signal Processing (BIOSIGNALS2010). 428-431.\n        \"\"\"\n        \n        f1 = 8/self.fs\n        f2 = 20/self.fs\n\n        b, a = signal.butter(2, [f1*2, f2*2], btype='bandpass')\n\n        filtered_ecg = signal.lfilter(b, a, unfiltered_ecg)\n\n        window1 = int(0.12*self.fs)\n        mwa_qrs = MWA(abs(filtered_ecg), window1)\n\n        window2 = int(0.6*self.fs)\n        mwa_beat = MWA(abs(filtered_ecg), window2)\n\n        blocks = np.zeros(len(unfiltered_ecg))\n        block_height = np.max(filtered_ecg)\n\n        for i in range(len(mwa_qrs)):\n            if mwa_qrs[i] > mwa_beat[i]:\n                blocks[i] = block_height\n            else:\n                blocks[i] = 0\n\n        QRS = []\n\n        for i in range(1, len(blocks)):\n            if blocks[i-1] == 0 and blocks[i] == block_height:\n                start = i\n            \n            elif blocks[i-1] == block_height and blocks[i] == 0:\n                end = i-1\n\n                if end-start>int(0.08*self.fs):\n                    detection = np.argmax(filtered_ecg[start:end+1])+start\n                    if QRS:\n                        if detection-QRS[-1]>int(0.3*self.fs):\n                            QRS.append(detection)\n                    else:\n                        QRS.append(detection)\n\n        return QRS\n\n\ndef MWA(input_array, window_size):\n\n    mwa = np.zeros(len(input_array))\n    for i in range(len(input_array)):\n        if i < window_size:\n            section = input_array[0:i]\n        else:\n            section = input_array[i-window_size:i]\n        \n        if i!=0:\n            mwa[i] = np.mean(section)\n        else:\n            mwa[i] = input_array[i]\n\n    return mwa\n\n\ndef normalise(input_array):\n\n    output_array = (input_array-np.min(input_array))/(np.max(input_array)-np.min(input_array))\n\n    return output_array\n\n\ndef panPeakDetect(detection, fs):    \n\n    min_distance = int(0.25*fs)\n\n    signal_peaks = [0]\n    noise_peaks = []\n\n    SPKI = 0.0\n    NPKI = 0.0\n\n    threshold_I1 = 0.0\n    threshold_I2 = 0.0\n\n    RR_missed = 0\n    index = 0\n    indexes = []\n\n    missed_peaks = []\n    peaks = []\n\n    for i in range(len(detection)):\n\n        if i>0 and i<len(detection)-1:\n            if detection[i-1]<detection[i] and detection[i+1]<detection[i]:\n                peak = i\n                peaks.append(i)\n\n                if detection[peak]>threshold_I1 and (peak-signal_peaks[-1])>0.3*fs:\n                        \n                    signal_peaks.append(peak)\n                    indexes.append(index)\n                    SPKI = 0.125*detection[signal_peaks[-1]] + 0.875*SPKI\n                    if RR_missed!=0:\n                        if signal_peaks[-1]-signal_peaks[-2]>RR_missed:\n                            missed_section_peaks = peaks[indexes[-2]+1:indexes[-1]]\n                            missed_section_peaks2 = []\n                            for missed_peak in missed_section_peaks:\n                                if missed_peak-signal_peaks[-2]>min_distance and signal_peaks[-1]-missed_peak>min_distance and detection[missed_peak]>threshold_I2:\n                                    missed_section_peaks2.append(missed_peak)\n\n                            if len(missed_section_peaks2)>0:           \n                                missed_peak = missed_section_peaks2[np.argmax(detection[missed_section_peaks2])]\n                                missed_peaks.append(missed_peak)\n                                signal_peaks.append(signal_peaks[-1])\n                                signal_peaks[-2] = missed_peak   \n\n                else:\n                    noise_peaks.append(peak)\n                    NPKI = 0.125*detection[noise_peaks[-1]] + 0.875*NPKI\n\n                threshold_I1 = NPKI + 0.25*(SPKI-NPKI)\n                threshold_I2 = 0.5*threshold_I1\n\n                if len(signal_peaks)>8:\n                    RR = np.diff(signal_peaks[-9:])\n                    RR_ave = int(np.mean(RR))\n                    RR_missed = int(1.66*RR_ave)\n\n                index = index+1      \n    \n    signal_peaks.pop(0)\n\n    return signal_peaks","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:09:43.658622Z","iopub.execute_input":"2021-08-01T16:09:43.659057Z","iopub.status.idle":"2021-08-01T16:09:43.754464Z","shell.execute_reply.started":"2021-08-01T16:09:43.659006Z","shell.execute_reply":"2021-08-01T16:09:43.753541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 функция выранного детектора","metadata":{}},{"cell_type":"code","source":"def detect(unfiltered_ecg, fs=250):\n    detectors = Detectors(fs)\n    r_peaks = []\n    # r_peaks_two_average = detectors.two_average_detector(unfiltered_ecg)\n    #r_peaks = detectors.matched_filter_detector(unfiltered_ecg,\"templates/template_250hz.csv\")\n    # r_peaks_swt = detectors.swt_detector(unfiltered_ecg)\n    # r_peaks_engzee = detectors.engzee_detector(unfiltered_ecg)\n    # r_peaks_christ = detectors.christov_detector(unfiltered_ecg)\n    # r_peaks_ham = detectors.hamilton_detector(unfiltered_ecg)\n    r_peaks = detectors.pan_tompkins_detector(unfiltered_ecg)\n    # r_peaks.append(sum(r_peaks_two_average, r_peaks_swt, r_peaks_engzee, \\\n    #               r_peaks_christ, r_peaks_ham, r_peaks_pan_tom)/(r_peaks_two_average, \\\n    #               r_peaks_swt, r_peaks_engzee, \\\n    #               r_peaks_christ, r_peaks_ham, r_peaks_pan_tom).count())\n\n    return r_peaks\ndef visualize(unfiltered_ecg, r_peaks):\n    plt.figure()\n    plt.plot(unfiltered_ecg)\n    plt.plot(r_peaks, unfiltered_ecg[r_peaks], 'ro')\n    plt.title('Detected R-peaks')\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:09:55.311641Z","iopub.execute_input":"2021-08-01T16:09:55.312378Z","iopub.status.idle":"2021-08-01T16:09:55.320549Z","shell.execute_reply.started":"2021-08-01T16:09:55.31233Z","shell.execute_reply":"2021-08-01T16:09:55.318286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Загрузка подготовленных датасетов","metadata":{}},{"cell_type":"code","source":"data_test = pd.read_csv('../input/test-ecg/new_test_df.csv')\ndata_train = pd.read_csv('../input/train-ecg/new_train_df.csv')\ndata_valid = pd.read_csv('../input/valid-ecg/new_valid_df.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:09:58.611083Z","iopub.execute_input":"2021-08-01T16:09:58.611546Z","iopub.status.idle":"2021-08-01T16:11:07.879024Z","shell.execute_reply.started":"2021-08-01T16:09:58.611497Z","shell.execute_reply":"2021-08-01T16:11:07.877968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# тренировочная, тестовая и валидационная выборки X\nX_cols = []\nfor col in data_train.columns.tolist():\n    if 'channel' in col:\n        X_cols.append(col)\ntrain_ptb = data_train[X_cols] \n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:07.880604Z","iopub.execute_input":"2021-08-01T16:11:07.881103Z","iopub.status.idle":"2021-08-01T16:11:08.409426Z","shell.execute_reply.started":"2021-08-01T16:11:07.88105Z","shell.execute_reply":"2021-08-01T16:11:08.408386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ptb = data_test[X_cols] ","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:08.411121Z","iopub.execute_input":"2021-08-01T16:11:08.411662Z","iopub.status.idle":"2021-08-01T16:11:08.463044Z","shell.execute_reply.started":"2021-08-01T16:11:08.41161Z","shell.execute_reply":"2021-08-01T16:11:08.462229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_ptb = data_valid[X_cols] ","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:08.464236Z","iopub.execute_input":"2021-08-01T16:11:08.464676Z","iopub.status.idle":"2021-08-01T16:11:08.505652Z","shell.execute_reply.started":"2021-08-01T16:11:08.464631Z","shell.execute_reply":"2021-08-01T16:11:08.504537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# таргеты будем выбирать по очереди\ny_variables = data_train.columns.tolist()[-5:]","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:16.927146Z","iopub.execute_input":"2021-08-01T16:11:16.927531Z","iopub.status.idle":"2021-08-01T16:11:16.932255Z","shell.execute_reply.started":"2021-08-01T16:11:16.927498Z","shell.execute_reply":"2021-08-01T16:11:16.931005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# тренировочная выборка y - синусовый ритм\nout_train_ptb = data_train[y_variables[0]]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:18.31673Z","iopub.execute_input":"2021-08-01T16:11:18.317164Z","iopub.status.idle":"2021-08-01T16:11:18.327055Z","shell.execute_reply.started":"2021-08-01T16:11:18.317126Z","shell.execute_reply":"2021-08-01T16:11:18.325769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_test_ptb = data_test[y_variables[0]]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:20.988725Z","iopub.execute_input":"2021-08-01T16:11:20.989227Z","iopub.status.idle":"2021-08-01T16:11:20.994543Z","shell.execute_reply.started":"2021-08-01T16:11:20.989181Z","shell.execute_reply":"2021-08-01T16:11:20.993326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_valid_ptb = data_valid[y_variables[0]]\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:22.687714Z","iopub.execute_input":"2021-08-01T16:11:22.688185Z","iopub.status.idle":"2021-08-01T16:11:22.693507Z","shell.execute_reply.started":"2021-08-01T16:11:22.688144Z","shell.execute_reply":"2021-08-01T16:11:22.692403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Normalizing the training, validation & test data \n# train_ptb = normalize(train_ptb, axis=0, norm='max')\nvalid_ptb = normalize(valid_ptb, axis=0, norm='max')\ntest_ptb = normalize(test_ptb, axis=0, norm='max')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:24.804056Z","iopub.execute_input":"2021-08-01T16:11:24.804545Z","iopub.status.idle":"2021-08-01T16:11:25.260075Z","shell.execute_reply.started":"2021-08-01T16:11:24.804501Z","shell.execute_reply":"2021-08-01T16:11:25.258657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reshaping the dataframe into a 3-D Numpy array (batch, Time Period, Value)\n# x_train_ptb = train_ptb.reshape(len(train_ptb),train_ptb.shape[1],1)\nx_valid_ptb = valid_ptb.reshape(len(valid_ptb),valid_ptb.shape[1],1)\nx_test_ptb = test_ptb.reshape(len(test_ptb),test_ptb.shape[1],1)\n\n# Converting the output into a categorical array\n# y_train_ptb = to_categorical(out_train_ptb)\ny_valid_ptb = to_categorical(out_valid_ptb)\ny_test_ptb = to_categorical(out_test_ptb)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:27.755614Z","iopub.execute_input":"2021-08-01T16:11:27.756017Z","iopub.status.idle":"2021-08-01T16:11:27.820387Z","shell.execute_reply.started":"2021-08-01T16:11:27.755983Z","shell.execute_reply":"2021-08-01T16:11:27.819305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# 4. Выбор таргета","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Таргет для модели - 1","metadata":{}},{"cell_type":"markdown","source":"## 4.2 Таргет для модели - 2 по конкретному ЭКГ","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Датафрейм с конкретным ЭКГ по указанным датасету и номеру ecg_id","metadata":{}},{"cell_type":"code","source":"# функция, возвращающая фрейм только по указанному экг\ndef get_unfiltered_ecg(df, ecg_id, cut_target_data=True):\n\n    ecg_unique = df[df['ecg_id']==ecg_id]\n    \n            \n    return ecg_unique[ecg_unique.columns[1:-5]] if cut_target_data else ecg_unique","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:32.227569Z","iopub.execute_input":"2021-08-01T16:11:32.228045Z","iopub.status.idle":"2021-08-01T16:11:32.234457Z","shell.execute_reply.started":"2021-08-01T16:11:32.227968Z","shell.execute_reply":"2021-08-01T16:11:32.232986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 Формируем датасет с пиками поканально по указанным датасету и ЭКГ","metadata":{}},{"cell_type":"code","source":"# функция, возвращающая словарь из пиков к конкретному экг\ndef get_peaks(df, ecg_id):\n    list_of_peaks = []\n    num_channels = []\n    ecg_unique = get_unfiltered_ecg(df, ecg_id)\n    ecg_data = ecg_unique.to_numpy()\n\n# подавать несколько каналов (пики попеременно)\n    for num_channel in range(ecg_data.shape[1]):\n        unfiltered_ecg = ecg_data[:,num_channel]\n        r_peaks = detect(unfiltered_ecg, 100)\n        # visualize(unfiltered_ecg, r_peaks)\n        list_of_peaks.append(r_peaks)\n        num_channels.append(f'channel-{num_channel}')\n\n        dict_peaks = dict(list(zip(num_channels, list_of_peaks)))\n#     dict_peaks_list[ecg_id_data] = dict_peaks\n\n#     peaks_data = pd.DataFrame(dict_peaks_list).T\n    return dict_peaks\n\n# функция, возвращающая фрейм из пиков для дальнейшего слияния с общими данными по этому экг\ndef pad_nan_peaks(dict_peaks, pad_value = 9999):\n    len_max = 0\n    for channel, list_peaks in dict_peaks.items():\n        if len_max < len(list_peaks):\n            len_max = len(list_peaks)\n    \n    for channel, list_peaks in dict_peaks.items():\n        dict_peaks[channel] = np.pad(list_peaks, (0, len_max-len(list_peaks)), 'constant', \n                 constant_values=pad_value)\n        \n    peaks_df = pd.DataFrame(dict_peaks)\n        \n    return peaks_df","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:36.107767Z","iopub.execute_input":"2021-08-01T16:11:36.108406Z","iopub.status.idle":"2021-08-01T16:11:36.120928Z","shell.execute_reply.started":"2021-08-01T16:11:36.108359Z","shell.execute_reply":"2021-08-01T16:11:36.119845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Формируем датасет с неотфильтрованными данными и пиками поканально по указанным датасету и ЭКГ","metadata":{}},{"cell_type":"code","source":"def merge_peaks_to_data(df, ecg_id):\n    ecg_unique = get_unfiltered_ecg(df, ecg_id)\n    peaks_df = pad_nan_peaks(get_peaks(df, ecg_id))\n    df_with_peaks = pd.concat([ecg_unique, peaks_df], ignore_index=True)\n    df_with_peaks['ecg_id'] = ecg_id\n    return df_with_peaks","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:39.216816Z","iopub.execute_input":"2021-08-01T16:11:39.217509Z","iopub.status.idle":"2021-08-01T16:11:39.222997Z","shell.execute_reply.started":"2021-08-01T16:11:39.217452Z","shell.execute_reply":"2021-08-01T16:11:39.222028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_with_peaks(df):\n    labels = []\n    for ecg_id in df['ecg_id'].unique():\n        if ecg_id == min(df['ecg_id'].unique()):\n            df_with_peaks = merge_peaks_to_data(df, ecg_id)\n            label = np.pad([], (0, df_with_peaks.shape[0]), 'constant', \n                 constant_values=ecg_id)\n            labels.append(label)\n\n        else:\n            df_with_peaks = pd.concat([df_with_peaks, merge_peaks_to_data(df, ecg_id)])\n            \n            label = np.pad([], (0, merge_peaks_to_data(df, ecg_id).shape[0]), 'constant', \n                 constant_values=ecg_id)\n            labels.append(label)\n            \n    return df_with_peaks\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:19:27.433223Z","iopub.execute_input":"2021-08-01T16:19:27.433626Z","iopub.status.idle":"2021-08-01T16:19:27.441538Z","shell.execute_reply.started":"2021-08-01T16:19:27.433593Z","shell.execute_reply":"2021-08-01T16:19:27.440687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# если y с пиками и надо определить длину таргета\ndef find_y(df_with_peaks, start_df):\n    ecg_id = df_with_peaks['ecg_id'][0]\n    y_value = start_df['sinus_rythm'].tolist()[0]\n    df_with_peaks['target'] = y_value\n    return df_with_peaks['target']","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:19:29.253063Z","iopub.execute_input":"2021-08-01T16:19:29.253739Z","iopub.status.idle":"2021-08-01T16:19:29.258623Z","shell.execute_reply.started":"2021-08-01T16:19:29.253681Z","shell.execute_reply":"2021-08-01T16:19:29.257738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_y(merge_peaks_to_data(data_train[:2000], 1), get_unfiltered_ecg(data_train[:2000], 1, False))","metadata":{"execution":{"iopub.status.busy":"2021-07-31T16:44:41.245662Z","iopub.execute_input":"2021-07-31T16:44:41.246004Z","iopub.status.idle":"2021-07-31T16:44:41.37924Z","shell.execute_reply.started":"2021-07-31T16:44:41.245975Z","shell.execute_reply":"2021-07-31T16:44:41.378401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_with_peaks = merge_peaks_to_data(data_valid, 17)\nstart_df = get_unfiltered_ecg(data_valid, 17, False)\n# y_valid_ptb = find_y(df_with_peaks, get_unfiltered_ecg(data_valid, 17, False))\necg_id = df_with_peaks['ecg_id'][0]\nstart_df['sinus_rythm'].tolist()[0]\n# df_with_peaks['target'] = y_value","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:19:34.457851Z","iopub.execute_input":"2021-08-01T16:19:34.458259Z","iopub.status.idle":"2021-08-01T16:19:34.887124Z","shell.execute_reply.started":"2021-08-01T16:19:34.458226Z","shell.execute_reply":"2021-08-01T16:19:34.885568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# достать сразу пики с основными с датасетами для входа в модель (тяжелые, не получается)\n\n# def X_Y_valid(data_valid):\n#     y_valid_list = []\n#     x_valid_list = []\n#     for ecg_id in data_valid['ecg_id'].unique():\n#         df_with_peaks = merge_peaks_to_data(data_valid, ecg_id)\n#         y_valid_ptb = find_y(df_with_peaks, get_unfiltered_ecg(data_valid, ecg_id, False))\n#         valid_ptb = normalize(df_with_peaks, axis=0, norm='max')\n#         x_valid_ptb = valid_ptb.reshape(len(valid_ptb),valid_ptb.shape[1],1)\n#         y_valid_list.append(y_valid_ptb)\n#         x_valid_list.append(x_valid_ptb)\n\n#         y_valid_ptb = list(itertools.chain(*y_valid_list))\n#         x_valid_ptb = list(itertools.chain(*x_valid_list))\n\n#     return y_valid_list, x_valid_list\n# y_valid_list, x_valid_list = X_Y_valid(data_valid)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:07:33.801376Z","iopub.execute_input":"2021-07-31T17:07:33.801751Z","iopub.status.idle":"2021-07-31T17:20:36.381005Z","shell.execute_reply.started":"2021-07-31T17:07:33.801716Z","shell.execute_reply":"2021-07-31T17:20:36.379966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# data_train.shape[1]","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:35:38.063452Z","iopub.execute_input":"2021-07-31T17:35:38.063782Z","iopub.status.idle":"2021-07-31T17:35:38.069047Z","shell.execute_reply.started":"2021-07-31T17:35:38.063753Z","shell.execute_reply":"2021-07-31T17:35:38.068078Z"}}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n\ndef build_conv1d_model(input_shape):\n    model = keras.models.Sequential()\n    \n    model.add(Conv1D(32,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(64,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(128,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(256,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(512,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(2, activation=\"softmax\"))\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(2,\"micro\")])\n    return model\n\ndef summary_model():\n    \n\n    model_conv1d_ptb= build_conv1d_model(input_shape=(12, 1))\n    model_conv1d_ptb.summary()\n    return model_conv1d_ptb\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:20:45.263005Z","iopub.execute_input":"2021-08-01T16:20:45.263582Z","iopub.status.idle":"2021-08-01T16:20:45.285981Z","shell.execute_reply.started":"2021-08-01T16:20:45.263532Z","shell.execute_reply":"2021-08-01T16:20:45.28488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def df_X_y_from_ecg_id(data_train, x_valid_ptb, y_valid_ptb):\n#     checkpoint_cb = ModelCheckpoint(\"conv1d_ptb.h5\", save_best_only=True)\n#     earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n#     model_conv1d_ptb = summary_model()\n    history_conv1d_ptb = None\n    \n    for ecg_id in data_train['ecg_id'].unique():\n        \n        df_with_peaks = merge_peaks_to_data(data_train, ecg_id)\n        y_train_ptb = find_y(df_with_peaks, get_unfiltered_ecg(data_train, ecg_id, False))\n        train_ptb = normalize(df_with_peaks, axis=0, norm='max')\n        x_train_ptb = train_ptb.reshape(len(train_ptb),train_ptb.shape[1],1)\n        \n        history_conv1d_ptb = (model_conv1d_ptb, checkpoint_cb, earlystop_cb,\n                              x_train_ptb, y_train_ptb, x_valid_ptb, y_valid_ptb)\n        print(ecg_id,sep='',end=\"\\r\",flush=True)\n\n    return model_conv1d_ptb\n","metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:11:48.720609Z","iopub.execute_input":"2021-08-01T16:11:48.721372Z","iopub.status.idle":"2021-08-01T16:11:48.729402Z","shell.execute_reply.started":"2021-08-01T16:11:48.721329Z","shell.execute_reply":"2021-08-01T16:11:48.727617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel_conv1d_ptb = df_X_y_from_ecg_id(data_train[:10000], x_valid_ptb, y_valid_ptb)\n# model_conv1d_ptb.load_weights(\"conv1d_ptb.h5\")\n# model_conv1d_ptb.evaluate(x_test_ptb,y_test_ptb)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:51:57.721915Z","iopub.execute_input":"2021-08-01T15:51:57.722291Z","iopub.status.idle":"2021-08-01T15:51:59.641424Z","shell.execute_reply.started":"2021-08-01T15:51:57.72226Z","shell.execute_reply":"2021-08-01T15:51:59.640533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_conv1d_ptb.load_weights(\"conv1d_ptb.h5\")\nmodel_conv1d_ptb.evaluate(x_test_ptb,y_test_ptb)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T15:52:00.832551Z","iopub.execute_input":"2021-08-01T15:52:00.832947Z","iopub.status.idle":"2021-08-01T15:52:00.962181Z","shell.execute_reply.started":"2021-08-01T15:52:00.832915Z","shell.execute_reply":"2021-08-01T15:52:00.959974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ncheckpoint_cb = ModelCheckpoint(\"conv1d_ptb.h5\", save_best_only=True)\nearlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\nmodel_conv1d_ptb = summary_model()\n\n# checkpoint_cb = ModelCheckpoint(\"conv1d_ptb.h5\", save_best_only=True)\n# earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n# model_conv1d_ptb= build_conv1d_model(input_shape=(x_train_ptb.shape[1], x_train_ptb.shape[2]))\n# model_conv1d_ptb.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_with_peaks(data_train[:2000])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-31T16:35:12.204636Z","iopub.execute_input":"2021-07-31T16:35:12.205088Z","iopub.status.idle":"2021-07-31T16:35:12.236637Z","shell.execute_reply.started":"2021-07-31T16:35:12.205058Z","shell.execute_reply":"2021-07-31T16:35:12.235632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Conv1D model for PTB\n\nCreating a model based on a series of Conv1D layers that are connected to another series of full connected dense layers","metadata":{"execution":{"iopub.status.busy":"2021-07-12T22:24:23.632402Z","iopub.execute_input":"2021-07-12T22:24:23.632815Z","iopub.status.idle":"2021-07-12T22:24:23.642339Z","shell.execute_reply.started":"2021-07-12T22:24:23.632724Z","shell.execute_reply":"2021-07-12T22:24:23.641119Z"}}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n\n#Function to build Convolutional 1D Networks\n# def build_conv1d_model_old (input_shape=(x_train_ptb.shape[1],1)):\n#     model = keras.models.Sequential()\n    \n#     model.add(Conv1D(32,7, padding='same', input_shape=input_shape))\n#     model.add(BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n#     model.add(MaxPool1D(5,padding='same'))\n\n#     model.add(Conv1D(64,7, padding='same'))\n#     model.add(BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n#     model.add(MaxPool1D(5,padding='same'))\n\n#     model.add(Conv1D(128,7, padding='same', input_shape=input_shape))\n#     model.add(BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n#     model.add(MaxPool1D(5,padding='same'))\n\n#     model.add(Conv1D(256,7, padding='same'))\n#     model.add(BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n#     model.add(MaxPool1D(5,padding='same'))\n\n#     model.add(Conv1D(512,7, padding='same', input_shape=input_shape))\n#     model.add(BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n#     model.add(MaxPool1D(5,padding='same'))\n\n#     model.add(Flatten())\n#     model.add(Dense(512, activation='relu'))\n#     model.add(Dropout(0.5))\n#     model.add(Dense(256, activation='relu'))\n#     model.add(Dropout(0.5))\n#     model.add(Dense(128, activation='relu'))\n#     model.add(Dense(64, activation='relu'))\n#     model.add(Dense(32, activation='relu'))\n#     model.add(Dense(2, activation=\"softmax\"))\n#     model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(2,\"micro\")])\n#     return model\ndef build_conv1d_model(input_shape):\n    model = keras.models.Sequential()\n    \n    model.add(Conv1D(32,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(64,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(128,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(256,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(512,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(2, activation=\"softmax\"))\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(2,\"micro\")])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:44:58.366Z","iopub.execute_input":"2021-07-31T17:44:58.36632Z","iopub.status.idle":"2021-07-31T17:44:58.379651Z","shell.execute_reply.started":"2021-07-31T17:44:58.366292Z","shell.execute_reply":"2021-07-31T17:44:58.378845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checkpoint_cb = ModelCheckpoint(\"conv1d_ptb.h5\", save_best_only=True)\n\n# earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n\n# model_conv1d_ptb= build_conv1d_model(input_shape=(x_train_ptb.shape[1], x_train_ptb.shape[2]))\n# model_conv1d_ptb.summary()\ndef summary_model(x_train_ptb):\n    checkpoint_cb = ModelCheckpoint(\"conv1d_ptb.h5\", save_best_only=True)\n\n    earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n\n    model_conv1d_ptb= build_conv1d_model(input_shape=(x_train_ptb.shape[1], x_train_ptb.shape[2]))\n    return model_conv1d_ptb.summary()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-31T15:06:28.935079Z","iopub.execute_input":"2021-07-31T15:06:28.935747Z","iopub.status.idle":"2021-07-31T15:06:28.94129Z","shell.execute_reply.started":"2021-07-31T15:06:28.935699Z","shell.execute_reply":"2021-07-31T15:06:28.940673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def history_model(model_conv1d_ptb, checkpoint_cb, earlystop_cb,\n                  x_train_ptb, y_train_ptb, x_valid_ptb, y_valid_ptb):\n    history_conv1d_ptb = model_conv1d_ptb.fit(x_train_ptb, y_train_ptb,\n                                                  epochs=1, batch_size=32, \n                                                  steps_per_epoch = 100 ,\n                                                  validation_data=(x_valid_ptb, y_valid_ptb), \n                                              validation_steps = 50,\n                                                  callbacks=[checkpoint_cb, earlystop_cb]\n                                             )\n    return history_conv1d_ptb","metadata":{"execution":{"iopub.status.busy":"2021-08-01T14:27:11.361181Z","iopub.execute_input":"2021-08-01T14:27:11.361632Z","iopub.status.idle":"2021-08-01T14:27:11.374136Z","shell.execute_reply.started":"2021-08-01T14:27:11.36153Z","shell.execute_reply":"2021-08-01T14:27:11.372722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in list(range(2)):\n#     history = model.fit(training_set_1,epochs=1)\n#     history = model.fit(training_set_2,epochs=1)\nhistory_conv1d_ptb = model_conv1d_ptb.fit(x_train_ptb, y_train_ptb,\n                                              epochs=10, batch_size=32, \n                                              steps_per_epoch = 1000 ,\n    #                                           class_weight=class_weight, \n                                              validation_data=(x_valid_ptb, y_valid_ptb), \n                                          validation_steps = 500,\n                                              callbacks=[checkpoint_cb, earlystop_cb])\n#     history_conv1d_ptb = model_conv1d_ptb.fit(x_train_ptb[100000:200000], y_train_ptb[100000:200000],\n#                                               epochs=1, batch_size=32, \n# #                                           class_weight=class_weight, \n#                                           validation_data=(x_valid_ptb[100000:200000], y_valid_ptb[100000:200000]),  \n#                                           callbacks=[checkpoint_cb, earlystop_cb])","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:34:05.984135Z","iopub.execute_input":"2021-07-30T14:34:05.984504Z","iopub.status.idle":"2021-07-30T14:48:39.950796Z","shell.execute_reply.started":"2021-07-30T14:34:05.984472Z","shell.execute_reply":"2021-07-30T14:48:39.949619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history_conv1d_ptb = model_conv1d_ptb.fit(x_train_ptb, y_train_ptb, epochs=40, batch_size=32, \n# #                                           class_weight=class_weight, \n#                                           validation_data=(x_valid_ptb, y_valid_ptb),  \n#                                           callbacks=[checkpoint_cb, earlystop_cb])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:06:43.573107Z","iopub.execute_input":"2021-07-15T21:06:43.573598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_test","metadata":{"execution":{"iopub.status.busy":"2021-07-31T17:21:14.671921Z","iopub.execute_input":"2021-07-31T17:21:14.672253Z","iopub.status.idle":"2021-07-31T17:21:14.704454Z","shell.execute_reply.started":"2021-07-31T17:21:14.672225Z","shell.execute_reply":"2021-07-31T17:21:14.703597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(x_train_ptb, y_train_ptb, x_valid_ptb, y_valid_ptb,\n                   x_test_ptb, y_test_ptb):\n    model_conv1d_ptb = history_model(x_train_ptb, y_train_ptb, x_valid_ptb, y_valid_ptb)\n    model_conv1d_ptb.evaluate(x_test_ptb,y_test_ptb)","metadata":{"execution":{"iopub.status.busy":"2021-07-31T11:00:29.788614Z","iopub.execute_input":"2021-07-31T11:00:29.788988Z","iopub.status.idle":"2021-07-31T11:00:29.793761Z","shell.execute_reply.started":"2021-07-31T11:00:29.788957Z","shell.execute_reply":"2021-07-31T11:00:29.79312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_conv1d_ptb.load_weights(\"conv1d_ptb.h5\")\nmodel_conv1d_ptb.evaluate(x_test_ptb,y_test_ptb)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:54:43.948984Z","iopub.execute_input":"2021-07-30T14:54:43.949402Z","iopub.status.idle":"2021-07-30T14:59:48.443569Z","shell.execute_reply.started":"2021-07-30T14:54:43.949363Z","shell.execute_reply":"2021-07-30T14:59:48.442317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the predictions based on the highest probability class\nconv1d_pred_proba_ptb = model_conv1d_ptb.predict (x_test_ptb)\nconv1d_pred_ptb = np.argmax(conv1d_pred_proba_ptb, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T14:59:48.445277Z","iopub.execute_input":"2021-07-30T14:59:48.445701Z","iopub.status.idle":"2021-07-30T15:04:06.189597Z","shell.execute_reply.started":"2021-07-30T14:59:48.445671Z","shell.execute_reply":"2021-07-30T15:04:06.18838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(out_test_ptb, conv1d_pred_ptb > 0.5, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:04:06.191546Z","iopub.execute_input":"2021-07-30T15:04:06.191884Z","iopub.status.idle":"2021-07-30T15:04:06.230114Z","shell.execute_reply.started":"2021-07-30T15:04:06.191851Z","shell.execute_reply":"2021-07-30T15:04:06.228267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(roc_auc_score(conv1d_pred_proba_ptb, out_test_ptb))\nprint(balanced_accuracy_score(conv1d_pred_proba_ptb, out_test_ptb))\nprint(f1_score(conv1d_pred_proba_ptb, out_test_ptb))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:07:35.573738Z","iopub.execute_input":"2021-07-30T15:07:35.574137Z","iopub.status.idle":"2021-07-30T15:07:36.471543Z","shell.execute_reply.started":"2021-07-30T15:07:35.574104Z","shell.execute_reply":"2021-07-30T15:07:36.469594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the training and validatoin results\nplt.figure(figsize=(25,12))\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['loss'],\n           color='r', label='Train loss')\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['val_loss'],\n           color='b', label='Val loss' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['f1_score'],\n           color='g', label='Train F1')\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['val_f1_score'],\n           color='c', label='Val F1' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:04:06.233627Z","iopub.status.idle":"2021-07-30T15:04:06.234117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Conv1D Residual model for PTB\n\nCreating a model based on a series of Conv1D layers with 2 residual blocks that are connected to another series of full connected dense layers","metadata":{}},{"cell_type":"code","source":"def build_conv1d_res_model (input_shape=(x_train_ptb.shape[1],1)):\n    model = keras.models.Sequential()\n    \n    input_ = tf.keras.layers.Input (shape=(input_shape))\n    \n    conv1_1 = Conv1D(64,7, padding='same', input_shape=input_shape) (input_)\n    conv1_1 = BatchNormalization() (conv1_1)\n    conv1_1 = tf.keras.layers.ReLU() (conv1_1)\n\n    conv1_2 = Conv1D(64,7, padding='same') (conv1_1)\n    conv1_2 = BatchNormalization() (conv1_2)\n    conv1_2 = tf.keras.layers.ReLU() (conv1_2)\n   \n    conv1_3 = Conv1D(64,7, padding='same') (conv1_2)\n    conv1_3 = BatchNormalization() (conv1_3)\n    conv1_3 = tf.keras.layers.ReLU() (conv1_3)\n\n    concat_1 = Concatenate()([conv1_1 , conv1_3 ])\n    max_1 = MaxPool1D(5, padding=\"same\") (concat_1)\n    \n    conv1_4 = Conv1D(128,7, padding='same') (max_1)\n    conv1_4 = BatchNormalization() (conv1_4)\n    conv1_4 = tf.keras.layers.ReLU() (conv1_4)\n\n    conv1_5 = Conv1D(128,7, padding='same', input_shape=input_shape) (conv1_4)\n    conv1_5 = BatchNormalization() (conv1_5)\n    conv1_5 = tf.keras.layers.ReLU() (conv1_5)\n    \n    conv1_6 = Conv1D(128,7, padding='same', input_shape=input_shape) (conv1_5)\n    conv1_6 = BatchNormalization() (conv1_6)\n    conv1_6 = tf.keras.layers.ReLU() (conv1_6)\n\n    concat_2 = Concatenate()([conv1_4, conv1_6])\n    max_2 = MaxPool1D(5, padding=\"same\") (concat_2)\n\n    flat = Flatten() (max_2)\n    dense_1 = Dense(512, activation='relu') (flat)\n    drop_1 = Dropout(0.5) (dense_1)\n    dense_2 = Dense(256, activation='relu') (drop_1)\n    drop_2 = Dropout(0.5) (dense_2)\n    dense_3 = Dense(128, activation='relu') (drop_2)\n    dense_4 = Dense(64, activation='relu') (dense_3)\n    dense_5 = Dense(32, activation='relu') (dense_4)\n    dense_6 = Dense(2, activation=\"softmax\") (dense_5)\n    \n    model = Model (inputs=input_ , outputs=dense_6)\n    \n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(2,\"micro\")])\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:07:42.086762Z","iopub.execute_input":"2021-07-30T15:07:42.087359Z","iopub.status.idle":"2021-07-30T15:07:42.105248Z","shell.execute_reply.started":"2021-07-30T15:07:42.087305Z","shell.execute_reply":"2021-07-30T15:07:42.104068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_cb = ModelCheckpoint(\"conv1d_res_ptb.h5\", save_best_only=True)\n\nearlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n\ninp_shape = (x_train_ptb.shape[1], x_train_ptb.shape[2])\nmodel_conv1d_res_ptb= build_conv1d_res_model(input_shape=(x_train_ptb.shape[1], x_train_ptb.shape[2]))\n#model_conv1d_res_ptb.build(inp_shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:07:47.007834Z","iopub.execute_input":"2021-07-30T15:07:47.008224Z","iopub.status.idle":"2021-07-30T15:07:47.29094Z","shell.execute_reply.started":"2021-07-30T15:07:47.008192Z","shell.execute_reply":"2021-07-30T15:07:47.289653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_conv1d_res_ptb = model_conv1d_res_ptb.fit(x_train_ptb, y_train_ptb,\n                                              epochs=10, batch_size=32, \n                                              steps_per_epoch = 1000 ,\n    #                                           class_weight=class_weight, \n                                              validation_data=(x_valid_ptb, y_valid_ptb), \n                                          validation_steps = 500,\n                                              callbacks=[checkpoint_cb, earlystop_cb])","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:07:52.518092Z","iopub.execute_input":"2021-07-30T15:07:52.518596Z","iopub.status.idle":"2021-07-30T15:10:01.90509Z","shell.execute_reply.started":"2021-07-30T15:07:52.51856Z","shell.execute_reply":"2021-07-30T15:10:01.903629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history_conv1d_res_ptb = model_conv1d_res_ptb.fit(x_train_ptb, y_train_ptb, epochs=40, batch_size=32, \n#                                           class_weight=class_weight, validation_data=(x_valid_ptb, y_valid_ptb),  \n#                                           callbacks=[checkpoint_cb, earlystop_cb])","metadata":{"execution":{"iopub.status.busy":"2021-07-15T15:25:52.511466Z","iopub.execute_input":"2021-07-15T15:25:52.51181Z","iopub.status.idle":"2021-07-15T15:26:37.586889Z","shell.execute_reply.started":"2021-07-15T15:25:52.511775Z","shell.execute_reply":"2021-07-15T15:26:37.586054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_conv1d_res_ptb.load_weights(\"conv1d_res_ptb.h5\")\nmodel_conv1d_res_ptb.evaluate(x_test_ptb,y_test_ptb)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:13:35.52394Z","iopub.execute_input":"2021-07-30T15:13:35.524373Z","iopub.status.idle":"2021-07-30T15:18:35.071683Z","shell.execute_reply.started":"2021-07-30T15:13:35.524328Z","shell.execute_reply":"2021-07-30T15:18:35.070085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the predictions based on the highest probability class\nconv1d_res_pred_proba_ptb = model_conv1d_res_ptb.predict (x_test_ptb)\nconv1d_res_pred_ptb = np.argmax(conv1d_res_pred_proba_ptb, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:18:35.07348Z","iopub.execute_input":"2021-07-30T15:18:35.07383Z","iopub.status.idle":"2021-07-30T15:23:00.200097Z","shell.execute_reply.started":"2021-07-30T15:18:35.073798Z","shell.execute_reply":"2021-07-30T15:23:00.199167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(out_test_ptb, conv1d_res_pred_ptb > 0.5, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:04:06.245834Z","iopub.status.idle":"2021-07-30T15:04:06.246336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(roc_auc_score(conv1d_res_pred_ptb, out_test_ptb))\nprint(balanced_accuracy_score(conv1d_res_pred_ptb, out_test_ptb))\nprint(f1_score(conv1d_res_pred_ptb, out_test_ptb))","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:34:32.90582Z","iopub.execute_input":"2021-07-30T15:34:32.90623Z","iopub.status.idle":"2021-07-30T15:34:33.152088Z","shell.execute_reply.started":"2021-07-30T15:34:32.906196Z","shell.execute_reply":"2021-07-30T15:34:33.150545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the training and validatoin results\nplt.figure(figsize=(25,12))\nplt.plot(history_conv1d_res_ptb.epoch, history_conv1d_res_ptb.history['loss'],\n           color='r', label='Train loss')\nplt.plot(history_conv1d_res_ptb.epoch, history_conv1d_res_ptb.history['val_loss'],\n           color='b', label='Val loss' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(history_conv1d_res_ptb.epoch, history_conv1d_res_ptb.history['f1_score'],\n           color='g', label='Train F1')\nplt.plot(history_conv1d_res_ptb.epoch, history_conv1d_res_ptb.history['val_f1_score'],\n           color='c', label='Val F1' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-30T15:04:06.248712Z","iopub.status.idle":"2021-07-30T15:04:06.249187Z"},"trusted":true},"execution_count":null,"outputs":[]}]}