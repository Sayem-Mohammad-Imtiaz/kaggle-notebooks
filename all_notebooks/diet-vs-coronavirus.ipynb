{"cells":[{"metadata":{"id":"H88zwMUupFzj"},"cell_type":"markdown","source":"# Diet Vs Coronavirus - ML Approach","execution_count":null},{"metadata":{"id":"Y-tZfJ1ppigQ"},"cell_type":"markdown","source":"## Objective\n\nThe objective of this model is to predict the percentage of deaths (per country) related to the coronavirus pandemic, taking into account statistical data about the food habbits of the population (food types: animal, eggs, fish, beer, etc.). From the predictions of the model, we can conclude which types of food have a bigger impact in the final outcome.","execution_count":null},{"metadata":{"id":"j6__Qem0r6k5"},"cell_type":"markdown","source":"## The dataset\n\nIn order to train and evaluate the model, we'll use [Covid-19 healthy dataset](https://www.kaggle.com/mariaren/covid19-healthy-diet-dataset). Unfortunately, the number of labeled examples is pretty low, so we'll create a classic ML model. ","execution_count":null},{"metadata":{"id":"dBv5MSl6tHKZ"},"cell_type":"markdown","source":"## Import relevant modules\n\nThe following hidden code cell imports the necessary packages that we'll use in order to explore, process the data and to create, run and evaluate the model.","execution_count":null},{"metadata":{"id":"ljG0I0Z7trlx","cellView":"both","trusted":true},"cell_type":"code","source":"#@title Import relevant modules\n\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras import layers\n\n# The following lines adjust the granularity of reporting. \npd.options.display.max_rows = 30\npd.options.display.float_format = \"{:.8f}\".format","execution_count":null,"outputs":[]},{"metadata":{"id":"Bbdo_IAHt8RB"},"cell_type":"markdown","source":"## Data\n\nThe following sections will be dedicated to the processes of data acquisition, exploration and processing.","execution_count":null},{"metadata":{"id":"laqsXXQ0uTPo"},"cell_type":"markdown","source":"### Data acquisition\n\nSince our dataset is not that large, we'll use pandas to load the data in memory from a .csv file.","execution_count":null},{"metadata":{"id":"5NHuzNubuqo3","outputId":"a178084a-3000-4462-8054-604c17a8e838","trusted":true},"cell_type":"code","source":"column_names = [\n  'country',\n  'alchoholic_beverages',\n  'animal_products',\n  'animal_fats',\n  'aquatic_products',\n  'cereals_excluding_beer',\n  'eggs',\n  'fish_and_seafood',\n  'fruits',\n  'meat',\n  'miscellaneous',\n  'milk_excluding_butter',\n  'offals',\n  'oilcrops',\n  'pulses',\n  'spices',\n  'starchy_roots',\n  'stimulants',\n  'sugar_crops',\n  'sugar_and_sweeteners',\n  'treenuts',\n  'vegetal_products',\n  'vegetal_oils',\n  'vegetables',\n  'obesity',\n  'undernourished',\n  'confirmed',\n  'deaths',\n  'recovered',\n  'active',\n  'population',\n  'unit',\n]\n\ndiet_data = pd.read_csv(\n  filepath_or_buffer='https://raw.githubusercontent.com/GrozescuRares/diet_vs_corona/master/diet_vs_corona.csv',\n  skiprows=1,\n  names=column_names,\n)\ndiet_data = diet_data.reindex(np.random.permutation(diet_data.index))\n\ndiet_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"JNv1KHd6wzyI"},"cell_type":"markdown","source":"### Data exploration\n\nIn this section we'll explore the dataset, since a large part of most machine learning projects is getting to know your data.","execution_count":null},{"metadata":{"id":"mneFkeeSxJuZ","cellView":"form","outputId":"e4548c80-5874-49b4-f15b-f67272baf1c5","trusted":true},"cell_type":"code","source":"#@title Get statistics on the dataset.\n\ndiet_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"XJI61Spcya3U"},"cell_type":"markdown","source":"After analyzing the statistics we identified some anomalies:\n\n\n*   There are missing values for the columns: Obesity, Confirmed, Deaths, Recovered and Active.\n*   For several columns the value of max seems very high compared to the other quantiles, which suggest that for those column we have some outlier values. For example, for the Fruits - Excluding Wine column we have a maximum value of 9.7. Given the quantile values and the mean, std values, we would expect the max value to be aproximately 3.0. This issue also occours for the the Oilcrops, Pulses, Spices, Starchy Roots and Population columns.\n\nAll this considered, we need to carefully choose our features and decide how to handle the examples which have missing values for some columns.\n\n","execution_count":null},{"metadata":{"id":"hHN3RZIYG2ja"},"cell_type":"markdown","source":"#### Find feature(s) whose raw values correlate with the label\n\nWe want to find out which features has more predictive power in the case of our problem. In order to get that information we'll use the [**correlation matrix**](https://medium.com/towards-artificial-intelligence/training-a-machine-learning-model-on-a-dataset-with-highly-correlated-features-debddf5b2e34).","execution_count":null},{"metadata":{"id":"EARRCbZxGNJ4","cellView":"form","outputId":"bc888370-679b-4780-b655-fc4f3ed41f76","trusted":true},"cell_type":"code","source":"#@title Get correlation matrix\n\ndiet_data.corr()","execution_count":null,"outputs":[]},{"metadata":{"id":"ciWgOVH_K7aR"},"cell_type":"markdown","source":"After analyzing the correlation matrix we can conclude that 'Animal Products', 'Cereals - Excluding Beer', 'Vegetal Products' and 'Obesity' correlate more with 'Deaths'. So, we'll use those values as features (numeric features).","execution_count":null},{"metadata":{"id":"8jn6PUZ6MS2m","trusted":true},"cell_type":"code","source":"# Define features and labels.\nfeature_names = ['animal_products', 'cereals_excluding_beer', 'obesity', 'vegetal_products']\nlabel_name = 'deaths'","execution_count":null,"outputs":[]},{"metadata":{"id":"VcKXT624NvhR","outputId":"5c085c3a-7d44-4ea1-d6f8-dc3e5a121e22","trusted":true},"cell_type":"code","source":"# Inspect features data\ndiet_data[feature_names].head()","execution_count":null,"outputs":[]},{"metadata":{"id":"jn0UrkwQOHa5"},"cell_type":"markdown","source":"#### Visualize features data distribution\n\nVisualizing the distribution of the data we'll help us in decideing if we need to normalize the data. We'll plot the histogram of each feature using pandas.  ","execution_count":null},{"metadata":{"id":"KHqTTu1jOr99","outputId":"4bf42825-2a01-42aa-ebcf-288e8bf978ad","trusted":true},"cell_type":"code","source":"for feature_name in feature_names:\n  diet_data.hist(column=feature_name)","execution_count":null,"outputs":[]},{"metadata":{"id":"_KNePE-ST7or"},"cell_type":"markdown","source":"By visualizing the histograms we can conclude the following:\n\n\n*   *Animal Products*, *Obesity* and *Vegetal Products* have a roughly normal distribution. We'll probably just scale their values using z-score formula.\n*   *Cereals - Excluding Beer* on the other hand, present a right skewed distribution. Maybe a log scalling we'll help us getting a normal distribution for those two features.\n\n","execution_count":null},{"metadata":{"id":"7F1StODsVw2v"},"cell_type":"markdown","source":"### Data processing\n\nIn this section we'll look at how we can normalize our data in order to obtain a normal distribution and we'll decide how should we handle the records with missing values.","execution_count":null},{"metadata":{"id":"aRTpSYoSdjaK"},"cell_type":"markdown","source":"#### Dropping records\n\nSince we have missing values for the label and due to the context of the problem, we'll drop that records.","execution_count":null},{"metadata":{"id":"MKoxA-h-Z7-R","outputId":"aec2879c-f4c7-4574-87f6-b8dc320bea34","trusted":true},"cell_type":"code","source":"# Get a data frame which only contains the features and the label\ntraining_columns = feature_names + [label_name]\ntraining_df = diet_data[training_columns]\ntraining_df = training_df.astype(np.float32)\n\n# Drop records with nan values\ntraining_df = training_df.dropna()\n\nprint('Dropped records with missing values.')","execution_count":null,"outputs":[]},{"metadata":{"id":"tiGhKqA1WUOv"},"cell_type":"markdown","source":"#### Data normalization\n\nIn the last section we plotted the histogram for all the features and we saw that the values of *Animal fats* and *Cereals - Excluding Beer* are not uniformly distributed. In this section we'll explore z-score and log scalling.\n**Note**: We'll apply the scalling on a copy of diet_data, just for visualizing the difference. The actual scalling will be done within the model creation.","execution_count":null},{"metadata":{"id":"_kdxCMJVXjx1","outputId":"28192bbb-e6ce-428f-fd6d-d1abf37e0072","trusted":true},"cell_type":"code","source":"def zscore(mean, std, val):\n  epsilon = 0.000001\n  \n  return (val - mean) / (epsilon + std)\n\nz_score_scaled_feature_names = ['animal_products', 'obesity', 'vegetal_products']\nlog_scaled_feature_names = ['cereals_excluding_beer']\n\ntraining_df_copy = training_df.copy()\nz_score_scaled_features = training_df_copy[z_score_scaled_feature_names].copy()\n\n# Apply z-score on 'Animal Products', 'Obesity' and 'Vegetal Products'\nfor feature_name in z_score_scaled_feature_names:\n  mean = z_score_scaled_features[feature_name].mean()\n  std = z_score_scaled_features[feature_name].std()\n  z_score_scaled_features[feature_name] = zscore(mean, std, z_score_scaled_features[feature_name])\n  z_score_scaled_features.hist(column=feature_name)\n\nlog_scaled_features = training_df_copy[log_scaled_feature_names].copy()\nfor feature_name in log_scaled_feature_names:\n  # Apply log scaling for 'Cereals - Excluding Beer'\n  log_scaled_features[feature_name] = np.log(log_scaled_features[feature_name])\n  log_scaled_features.hist(column=feature_name)","execution_count":null,"outputs":[]},{"metadata":{"id":"auGeDsV_ZX-p"},"cell_type":"markdown","source":"It seems that after applying z-score and log scaling we got a much more normal distribution for all of our features. So, we are definitely going to stick with this approach on model creation. ","execution_count":null},{"metadata":{"id":"ngEidChdkpkl"},"cell_type":"markdown","source":"#### Data noise and label normalization\n\nThe last thing that we need to do before creating the model is removing the noise of the label values and bring the label to a similar range as the features. For reducing the complexity of the computations, we'll keep just the first 4 digits after the floating point.\n\nFor avoiding logging 0 values which cause -inf results, we add +1 at logging","execution_count":null},{"metadata":{"id":"B2gpCfObf-N4","trusted":true},"cell_type":"code","source":"training_df[label_name] = training_df[label_name].astype(np.float32) * 100.0\ntraining_df[label_name] = training_df[label_name].round(4)\ntraining_df[label_name] = training_df[label_name].map(lambda val: np.log(val + 1))\n\ntraining_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"CxM2XQ3d6ljY","outputId":"7cf68455-bda8-4a8d-b70f-f7ccb9a64caa","trusted":true},"cell_type":"code","source":"training_df.hist(column=label_name)","execution_count":null,"outputs":[]},{"metadata":{"id":"atxxkV8kmmfA"},"cell_type":"markdown","source":"## Model\n\nThe sections bellow are dedicted to the processes of creating, training and evaluating the model.","execution_count":null},{"metadata":{"id":"ErwvQzdJpSVV"},"cell_type":"markdown","source":"### Splitting the dataset\n\nWe split the dataset into training and testing data, separating the features from the label.","execution_count":null},{"metadata":{"id":"weRwmQtCpVnU","outputId":"3f4cbb15-e0da-4320-d055-1ef2410da9a8","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(training_df[feature_names], training_df[label_name], test_size=0.10)\n\nprint('We have {} training records and {} records for evaluating the model.'.format(len(X_train), len(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"id":"eHTAYaZPwqMV"},"cell_type":"markdown","source":"### Creating the input layer\n\nIn this section we'll create the input layer that will be used by our model. When defining the columns will take into consideration the normalization methods that we discussed in the *Data normalization* phase.","execution_count":null},{"metadata":{"id":"5D1Jp6sOxNNC","outputId":"5359c2c5-bef4-457e-e9d2-72cc7be6b09f","trusted":true},"cell_type":"code","source":"# Create the features normalized using z-score.\nz_score_scaled_features = [\n  tf.feature_column.numeric_column(\n      feature_name,\n      normalizer_fn=lambda val: zscore(X_train.mean()[feature_name], X_train.std()[feature_name], val),\n  )\n  for feature_name in z_score_scaled_feature_names\n]\n\n# Create the features normalized using log scaling\nlog_scaled_features = [\n  tf.feature_column.numeric_column(\n      feature_name,\n      normalizer_fn=lambda val: tf.math.log(val),\n  )\n  for feature_name in log_scaled_feature_names\n]\n\n# Create the input layer\ninput_layer = layers.DenseFeatures(z_score_scaled_features + log_scaled_features)\n\nprint('Created input layer.')","execution_count":null,"outputs":[]},{"metadata":{"id":"vPOC2kn90wwk"},"cell_type":"markdown","source":"### Define functions that create and train a model; define plot function\n\nWe'll define a function for creating and compiling a simple linear regression model.","execution_count":null},{"metadata":{"id":"3ushe_Gx0-Dq","outputId":"8735f19a-1a8d-446d-bf23-250a13abf5cb","trusted":true},"cell_type":"code","source":"def create_model(my_learning_rate, input_layer):\n  \"\"\"Create and compile a simple linear regression model.\"\"\"\n\n  model = tf.keras.models.Sequential()\n\n  # Add the layer containing the feature columns to the model.\n  model.add(input_layer)\n\n  # Add one linear layer to the model to yield a simple linear regressor.\n  model.add(tf.keras.layers.Dense(units=1, input_shape=(1, )))\n\n  # Construct the layers into a model that TensorFlow can execute.\n  model.compile(\n    optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),\n    loss='mean_squared_error',\n    metrics=[tf.keras.metrics.RootMeanSquaredError()],\n  )\n\n  return model\n\nprint('Defined create_model function.')","execution_count":null,"outputs":[]},{"metadata":{"id":"2g4yCO_T17MA"},"cell_type":"markdown","source":"The function bellow represents the training process of the model on a given dataset.","execution_count":null},{"metadata":{"id":"TY0C29ut2JYR","outputId":"fe5e0b54-6d1c-4784-ff67-e732779ae6f1","trusted":true},"cell_type":"code","source":"def train_model(model, x, y, epochs, batch_size):\n  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n\n  features = {name:np.array(value) for name, value in x.items()}\n  label = y.to_numpy()\n\n  history = model.fit(\n    x=features,\n    y=label,\n    batch_size=batch_size,\n    epochs=epochs,\n    shuffle=True,\n  )\n\n  # The list of epochs is stored separately from the rest of history.\n  epochs = history.epoch\n  \n  # Isolate the mean absolute error for each epoch.\n  hist = pd.DataFrame(history.history)\n  rmse = hist['root_mean_squared_error']\n\n  return epochs, rmse\n\nprint('Defined train_model function.')   ","execution_count":null,"outputs":[]},{"metadata":{"id":"ZZdVWZJuLBvx"},"cell_type":"markdown","source":"We'll define the function that we are going to use in order to plot the results of the training process.","execution_count":null},{"metadata":{"id":"WhUDKg1jLMX4","outputId":"87dc6cd4-f685-4be1-a844-cacfd5022945","trusted":true},"cell_type":"code","source":"def plot_the_loss_curve(epochs, rmse):\n  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Root Mean Squared Error')\n\n  plt.plot(epochs, rmse, label=\"Loss\")\n  plt.legend()\n  plt.ylim([rmse.min()*0.94, rmse.max()* 1.05])\n  plt.show()\n\nprint('Defined plot function.')","execution_count":null,"outputs":[]},{"metadata":{"id":"NJuSagJu2qjt"},"cell_type":"markdown","source":"### Train the model\n\nIn this section we'll create the model and train it on the labeled examples.","execution_count":null},{"metadata":{"id":"zUb6hdiL214t","outputId":"3aafd9f9-6f46-4a3e-d4e5-5ee8d41e090d","trusted":true},"cell_type":"code","source":"# The following variables are the hyperparameters.\nlearning_rate = 0.003\nepochs = 64\nbatch_size = 12\n\n# Create and compile the model.\nmodel = create_model(learning_rate, input_layer)\n\n# Train the model on the training set.\nepochs, rmse = train_model(model, X_train, Y_train, epochs, batch_size)\n\nplot_the_loss_curve(epochs, rmse)","execution_count":null,"outputs":[]},{"metadata":{"id":"GHv6oa6gLr54"},"cell_type":"markdown","source":"### Evaluate the model","execution_count":null},{"metadata":{"id":"rKPEmNDXLv-c","outputId":"8c8809ad-3b66-4766-ebbd-ce562e5385e2","trusted":true},"cell_type":"code","source":"print(\"\\n: Evaluate the new model against the test set:\")\n\ntest_features = {name:np.array(value) for name, value in X_test.items()}\n\nresults = model.evaluate(x=test_features, y=Y_test.to_numpy(), batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"id":"cOGC3J-1VUOt","outputId":"5f895172-f18b-4854-c1e7-e19ed0250e39","trusted":false},"cell_type":"code","source":"new_data = {\n  'animal_products': [17.7],\n  'cereals_excluding_beer': [7.9],\n  'obesity': [10.5],\n  'vegetal_products': [26.2],\n}\n\nnew_data = {name:np.array(value) for name, value in new_data.items()}\n\nresults = model.predict(new_data)\n\nprint('The predicted deaths percentage is {}.'.format(results[0][0]))","execution_count":null,"outputs":[]},{"metadata":{"id":"Wi-cTWzW0JCW"},"cell_type":"markdown","source":"After evaluating the model, despite the fact that the predictions are not that accurate due to the very low amount of examples used for training, we can still observe that increasing the percentage of features that have a negative corellation (if their values increase, the outcome decreases) while decreasing the percentage of features that have a positive corellation, we get a lower value for the percentage of deaths. For example a distribution of:\n\n\n```\n{\n  'animal_products': 18.7,\n  'cereals_excluding_beer': 7.9,\n  'obesity': 20.5,\n  'vegetal_products': 15.2,\n}\n```\nwill always result in a greater death percentage outcome than:\n\n\n```\n{\n  'animal_products': 14.7,\n  'cereals_excluding_beer': 7.9,\n  'obesity': 20.5,\n  'vegetal_products': 19.2,\n}\n```\n\nSo, we can conlude that by changing just a little bit the proportions of fat income types, we can make a difference by the end of the day.\n\n\n","execution_count":null},{"metadata":{"id":"7IjhOz3n8-nK"},"cell_type":"markdown","source":"# Diet Vs Coronavirus - Data Approach","execution_count":null},{"metadata":{"id":"buTYVXQ59Kw7"},"cell_type":"markdown","source":"## Objective\n\nThe objective of this data analyses is to confirm that a population with a healthy diet and lifestyle has a low rate of deaths related to the coronavirus pandemic.","execution_count":null},{"metadata":{"id":"pxZMKLae-Qz1"},"cell_type":"markdown","source":"## The dataset\n\nIn order to prove our theory, we'll use [Covid-19 healthy dataset](https://www.kaggle.com/mariaren/covid19-healthy-diet-dataset). In the ML Approach, we observed that *Animal Products*, *Cereals - Excluding Beer*, *Vegetal Products* and *Obesity* are the most correlated to the deaths percentage, so we'll be using those values.","execution_count":null},{"metadata":{"id":"0K1Rb9oh_Vjv"},"cell_type":"markdown","source":"## Import relevant modules\n\nThe following hidden code cell imports the necessary packages that we'll use in order to explore, process and visualize the data","execution_count":null},{"metadata":{"cellView":"both","id":"kReG-DBX_Vjy","trusted":false},"cell_type":"code","source":"#@title Import relevant modules\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# The following lines adjust the granularity of reporting. \npd.options.display.max_rows = 30\npd.options.display.float_format = \"{:.8f}\".format","execution_count":null,"outputs":[]},{"metadata":{"id":"C81-rA16_ngP"},"cell_type":"markdown","source":"## Load the data\n\nWe'll load the data in memory using pandas, selecting only the five columns that we are intereseted in.","execution_count":null},{"metadata":{"id":"yMoJn7f8_k_y","outputId":"efc8d0c4-7b60-4cc9-928e-fbf41cc59d5e","trusted":false},"cell_type":"code","source":"column_names = [\n  'country',\n  'alchoholic_beverages',\n  'animal_products',\n  'animal_fats',\n  'aquatic_products',\n  'cereals_excluding_beer',\n  'eggs',\n  'fish_and_seafood',\n  'fruits',\n  'meat',\n  'miscellaneous',\n  'milk_excluding_butter',\n  'offals',\n  'oilcrops',\n  'pulses',\n  'spices',\n  'starchy_roots',\n  'stimulants',\n  'sugar_crops',\n  'sugar_and_sweeteners',\n  'treenuts',\n  'vegetal_products',\n  'vegetal_oils',\n  'vegetables',\n  'obesity',\n  'undernourished',\n  'confirmed',\n  'deaths',\n  'recovered',\n  'active',\n  'population',\n  'unit',\n]\nused_column_names = [\n  'animal_products',\n  'cereals_excluding_beer',\n  'vegetal_products',\n  'obesity',\n  'deaths',\n]\n\ndiet_data_simple = pd.read_csv(\n  filepath_or_buffer='https://raw.githubusercontent.com/GrozescuRares/diet_vs_corona/master/diet_vs_corona.csv',\n  skiprows=1,\n  names=column_names,\n  usecols=used_column_names,\n)\n\ndiet_data_simple = diet_data_simple.dropna()\ndiet_data_simple.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"QoKIhYIlBzyB"},"cell_type":"markdown","source":"## Analyze the data\n\nThis section is dedicated to the process of analyzing the data and confirming our theory. We'll start by taking a look on the statistics related to the dataset.","execution_count":null},{"metadata":{"cellView":"form","id":"1FUXxdfEBh_p","outputId":"4f02eaa6-eaa8-4d12-8c34-1a70bfc778e3","trusted":false},"cell_type":"code","source":"#@title Get statistics on the dataset.\n\ndiet_data_simple.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"NQC0VvkcCO2g"},"cell_type":"markdown","source":"After observing and analyzing those statistics, we consider that a good approach would be to sort the records by the deaths percentage and then selecting ten rows; five of them representing records with highest percentage of deaths, and another five with the lowest. Last but not least, we'll do an average of the values for each group of records and than we'll compare them using piecharts.","execution_count":null},{"metadata":{"id":"O3Rw5bM-EFDi","cellView":"form","outputId":"1320db97-adec-4aae-ef52-885c9f6007a5","trusted":false},"cell_type":"code","source":"#@title Sort data by deaths\ndiet_data_sorted = diet_data_simple.sort_values(by=['deaths'])\n\ndiet_data_sorted","execution_count":null,"outputs":[]},{"metadata":{"id":"y1KPAWgWE9Kb","cellView":"form","outputId":"e826dca9-f062-46ad-9010-35b87f6140d4","trusted":false},"cell_type":"code","source":"#@title Separate data in groups\ndiet_data_sorted = diet_data_sorted[diet_data_sorted.deaths != 0.0]\n\nhighest_deaths_rate_data = diet_data_sorted.tail(10)\nlowest_deaths_rate_data = diet_data_sorted.head(10)\n\nprint('Data was separated in two groups by deaths rate.')","execution_count":null,"outputs":[]},{"metadata":{"id":"vT1aOPT2FyJo","cellView":"form","outputId":"07ab61e1-26f7-4105-b27e-23a21c143d81","trusted":false},"cell_type":"code","source":"#@title Check records with the highest death rate\n\nhighest_deaths_rate_data","execution_count":null,"outputs":[]},{"metadata":{"id":"kOSCKOSHF8-q","cellView":"form","outputId":"36eceed2-5a29-400d-884f-e72d44469435","trusted":false},"cell_type":"code","source":"#@title Check records with the lowest death rate\n\nlowest_deaths_rate_data","execution_count":null,"outputs":[]},{"metadata":{"id":"4tZAQsQuGj00","cellView":"both","outputId":"abc26940-2989-4c62-c91b-8438574705e6","trusted":false},"cell_type":"code","source":"#@title Compute average for both groups\n\nhighest_deaths_rate_mean = {column_name:highest_deaths_rate_data[column_name].mean() for column_name in used_column_names[:-1]}\nprint('Average values for records with highest death rate: \\n{}'.format(highest_deaths_rate_mean))\n\nlowest_deaths_rate_mean = {column_name:lowest_deaths_rate_data[column_name].mean() for column_name in used_column_names[:-1]}\nprint('Average values for records with lowest death rate: \\n{}'.format(lowest_deaths_rate_mean))","execution_count":null,"outputs":[]},{"metadata":{"id":"IUBtX3X4H-ne","outputId":"2239610f-1a04-47c7-fe42-3d91f6b68971","trusted":false},"cell_type":"code","source":"#@title Visualize charts\n\nlabels = used_column_names[:-1]\n\nx = np.array([0, 2, 4, 6])  # the label locations\nwidth = 0.7  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(20, 12))\nrects1 = ax.bar(x - width/2, highest_deaths_rate_mean.values(), width, label='High deaths rate group')\nrects2 = ax.bar(x + width/2, lowest_deaths_rate_mean.values(), label='Low deaths rate group')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Percentage')\nax.set_title('Percentage of fat income')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend(loc='upper right')\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 1),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\nautolabel(rects1)\nautolabel(rects2)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"_3X-JZl8fsk5"},"cell_type":"markdown","source":"As you can observe from the bar chart, it is clear that a population which has a healthy diet consisting of vegetal products and cereals has a lower death rate in comparison with a population which has a higher obesity rate and consumes more animal products.\nIn conclusion, based on this data we can confirm that a population with a healthy diet and lifestyle has a low rate of deaths related to the coronavirus pandemic. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}