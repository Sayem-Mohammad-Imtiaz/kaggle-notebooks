{"cells":[{"metadata":{},"cell_type":"markdown","source":"Daniele Berto 1163243\n\nIl seguente articolo compara 6 reti convoluzionali diverse per il MNIST dataset: \n[Siddique, Fathma & Sakib, Shadman & Siddique, Md. Abu. (2019). Recognition of Handwritten Digit using Convolutional Neural Network in Python with Tensorflow and Comparison of Performance for Various Hidden Layers. 10.20944/preprints201903.0039.v2.](https://arxiv.org/pdf/1909.08490.pdf)\n\nHo provato ad utilizzare gli schemi proposti (che in passato avevo provato ad utilizzare con il MNIST dataset come fa l'articolo) con il sign-language MNIST dataset.\n\nNonostante i segni proposti siano 25, i modelli danno risultati incoraggianti.\n\nMi domando se esista un modello rigoroso per descrivere le reti neurali: è difficile riprodurre esattamente quello che l'articolo intende basandosi solo su delle immagini o sul testo scritto. Occorrerebbe uno strumento più preciso"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.optimizers import SGD\nfrom keras.layers import Dropout\nfrom sklearn.model_selection import train_test_split\nfrom numpy import argmax\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n#Carico i dataset\ndef load_dataset():\n    \n    mnist_test = pd.read_csv(\"../input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv\")\n    mnist_train = pd.read_csv(\"../input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv\")\n    \n    target_test = mnist_test.iloc[:,0].values.reshape(-1,1)\n    features_test = mnist_test.iloc[:, 1:]\n    \n    target_train = mnist_train.iloc[:,0].values.reshape(-1,1)\n    features_train = mnist_train.iloc[:, 1:]    \n    \n    img_rows, img_cols = 28, 28\n    input_shape = (img_rows, img_cols, 1)\n    y_train = to_categorical(target_train.copy())\n    y_test = to_categorical(target_test.copy())    \n    \n    X_train = features_train.values.reshape(features_train.shape[0], img_rows, img_cols, 1)\n    X_test = features_test.values.reshape(features_test.shape[0], img_rows, img_cols, 1)\n    return X_train, y_train, X_test, y_test\n\n\n#Eseguo lo scaling dei pixel\ndef prep_pixels(train, test):\n    \n    #converto da integer a float\n    normalized_train = train.astype('float32')\n    normalized_test = test.astype('float32')\n    \n    #normalizzo nel range 0-1\n    normalized_train = normalized_train / 255.0\n    normalized_test = normalized_test / 255.0\n    \n    #ritorno i valori normalizzati \n    return normalized_train, normalized_test\n\n\n#definisco una rete convoluzionale seguendo il caso 1 dell'articolo citato\ndef define_model_1():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dropout(0.50))\n    model.add(Dense(25, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n#definisco una rete convoluzionale seguendo il caso 2 dell'articolo citato\ndef define_model_2():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.25))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dropout(0.50))\n    model.add(Dense(25, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n#definisco una rete convoluzionale seguendo il caso 3 dell'articolo citato\ndef define_model_3():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(25, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n#definisco una rete convoluzionale seguendo il caso 4 dell'articolo citato\ndef define_model_4():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(25, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n#definisco una rete convoluzionale seguendo il caso 5 dell'articolo citato\ndef define_model_5():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dropout(0.50))\n    model.add(Dense(25, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n#definisco una rete convoluzionale seguendo il caso 6 dell'articolo citato\ndef define_model_6():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dropout(0.50))\n    model.add(Dense(25, activation='softmax'))\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\n#Plotto i risultati\ndef plot_histories_results(history, case_name):\n\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model case ' + case_name + ' accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n    \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model case ' + case_name + ' loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Carico il dataset\ntrainX, trainY, testX, testY = load_dataset()\n\n#Preparo i pixel\ntrainX, testX = prep_pixels(trainX, testX)\n\n#Caso 1\nmodel_1 = define_model_1()\nhistory_1 = model_1.fit(trainX, trainY, epochs=15, batch_size=100, validation_data=(testX, testY), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Case 1 maximum training accuracy: ' + str(history_1.history['accuracy'][argmax(history_1.history['accuracy'])]))\nprint('Case 1 maximum validation accuracy: ' + str(history_1.history['val_accuracy'][argmax(history_1.history['val_accuracy'])]))\nplot_histories_results(history_1, '1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Caso 2\nmodel_2 = define_model_2()\nhistory_2 = model_2.fit(trainX, trainY, epochs=15, batch_size=100, validation_data=(testX, testY), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Case 2 maximum training accuracy: ' + str(history_2.history['accuracy'][argmax(history_2.history['accuracy'])]))\nprint('Case 2 maximum validation accuracy: ' + str(history_2.history['val_accuracy'][argmax(history_2.history['val_accuracy'])]))\nplot_histories_results(history_2, '2')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Caso 3\nmodel_3 = define_model_3()\nhistory_3 = model_3.fit(trainX, trainY, epochs=15, batch_size=100, validation_data=(testX, testY), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Case 3 maximum training accuracy: ' + str(history_3.history['accuracy'][argmax(history_3.history['accuracy'])]))\nprint('Case 3 maximum validation accuracy: ' + str(history_3.history['val_accuracy'][argmax(history_3.history['val_accuracy'])]))\nplot_histories_results(history_3, '3')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Caso 4\nmodel_4 = define_model_4()\nhistory_4 = model_4.fit(trainX, trainY, epochs=15, batch_size=100, validation_data=(testX, testY), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Case 4 maximum training accuracy: ' + str(history_4.history['accuracy'][argmax(history_4.history['accuracy'])]))\nprint('Case 4 maximum validation accuracy: ' + str(history_4.history['val_accuracy'][argmax(history_4.history['val_accuracy'])]))\nplot_histories_results(history_4, '4')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Caso 5\nmodel_5 = define_model_5()\nhistory_5 = model_5.fit(trainX, trainY, epochs=15, batch_size=100, validation_data=(testX, testY), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Case 5 maximum training accuracy: ' + str(history_5.history['accuracy'][argmax(history_5.history['accuracy'])]))\nprint('Case 5 maximum validation accuracy: ' + str(history_5.history['val_accuracy'][argmax(history_5.history['val_accuracy'])]))\nplot_histories_results(history_5, '5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Caso 6\nmodel_6 = define_model_6()\nhistory_6 = model_6.fit(trainX, trainY, epochs=15, batch_size=100, validation_data=(testX, testY), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Case 6 maximum training accuracy: ' + str(history_6.history['accuracy'][argmax(history_6.history['accuracy'])]))\nprint('Case 6 maximum validation accuracy: ' + str(history_6.history['val_accuracy'][argmax(history_6.history['val_accuracy'])]))\nplot_histories_results(history_6, '6')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}