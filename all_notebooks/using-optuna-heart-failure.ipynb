{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install thop","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport collections\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\n\nimport thop\n\nimport optuna\nfrom optuna.integration import KerasPruningCallback\nfrom optuna.visualization import plot_contour, plot_edf, plot_intermediate_values,  plot_optimization_history, plot_parallel_coordinate, plot_param_importances, plot_slice \n\nimport torch \nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport torchvision\n\nimport tensorflow as tf\nimport keras \nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.models import Sequential\n\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of the dataset\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking duplicated data\ndata.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking label or unlabel dataset\nsns.countplot(x=data.DEATH_EVENT, data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Increasing the number of features\ndef feature_creation(df):\n    df['age2'] = df['age']//10\n    df['creatinine_phosphokinase2'] = df['creatinine_phosphokinase']//20 #10\n    df['creatinine_phosphokinase3'] = df['creatinine_phosphokinase2']//10\n    df['ejection_fraction2'] = df['ejection_fraction']//10\n    df['platelets2'] = df['platelets']//100\n    df['platelets3'] = df['platelets2']//100\n    df['platelets4'] = df['platelets3']//10\n    df['serum_sodium2'] = df['serum_sodium']//20\n    \n    for i in ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', 'ejection_fraction', 'high_blood_pressure', 'platelets', 'serum_creatinine', 'serum_sodium','sex', 'smoking','time']:\n        for j in ['age2', 'creatinine_phosphokinase2', 'creatinine_phosphokinase3', 'ejection_fraction2', 'platelets2', 'platelets3', 'platelets4','serum_sodium2']:\n            df[i + \"_\" + j] = df[i].astype('str') + \"_\" + df[j].astype('str')\n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = feature_creation(data)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical columns\ncat_columns = []\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncolumns = data.columns.values.tolist()\n\nfor col in columns:\n    if data[col].dtype in numerics: continue\n    cat_columns.append(col)\n    \nprint(cat_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Encoding of categorical features\nfor col in cat_columns:\n    if col in data.columns:\n        le = LabelEncoder()\n        le.fit(list(data[col].astype(str).values))\n        data[col] = le.transform(list(data[col].astype(str).values))\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"After feature number increased shape if :{data.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Shaffle dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature and Label data"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = data.drop('DEATH_EVENT', axis=1)\nlabel = data.DEATH_EVENT","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train and Valid"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_valid, y_train, y_valid = train_test_split(features, label, test_size=.2, stratify=label)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Standardize and Normalize"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Normalize\nscaler = MinMaxScaler()\nnormalized_xtrain = scaler.fit_transform(x_train)\nnormalized_xtest = scaler.fit_transform(x_valid)\n\n\n# Data Standardize\nstd = StandardScaler()\nstandardized_xtrain = scaler.fit_transform(normalized_xtrain)\nstandardized_xtest = scaler.fit_transform(normalized_xtest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Automatic Hyperparameter Tuning(Optuna)"},{"metadata":{},"cell_type":"markdown","source":"### xgboost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    #data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    #train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n    dtrain = xgb.DMatrix(standardized_xtrain, label=y_train)\n    dvalid = xgb.DMatrix(standardized_xtest, label=y_valid)\n\n    param = {\n        \"verbosity\": 0,\n        \"objective\": \"binary:logistic\",\n        \"eval_metric\": \"auc\",\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n    }\n\n    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n    if param[\"booster\"] == \"dart\":\n        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n\n    # Add a callback for pruning.\n    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-auc\")\n    bst = xgb.train(param, dtrain, evals=[(dvalid, \"validation\")], callbacks=[pruning_callback])\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_valid, pred_labels)\n    return accuracy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"maximize\"\n    )\nstudy.optimize(objective, n_trials=300)\n#print(study.best_trial)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_trial = len(study.trials)\nprint('number of trial :\\n', number_trial, '\\n')\n\nbest_trial = study.best_trial\nprint('best trials :\\n', best_trial, '\\n')\n\nbest_params = study.best_params\nprint('best parameters :\\n', best_params, '\\n')\n\nbest_value = study.best_value\nprint('best values :\\n', best_value, '\\n')\n\n#trial = study.trials\n#print('trials :\\n', trial, '\\n')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CatBoost classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    #data, target = load_breast_cancer(return_X_y=True)\n    #train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.3)\n\n    param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n        \"used_ram_limit\": \"3gb\",\n    }\n\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    gbm = cb.CatBoostClassifier(**param)\n\n    gbm.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], verbose=0, early_stopping_rounds=100)\n\n    preds = gbm.predict(x_valid)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_valid, pred_labels)\n    return accuracy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=300, timeout=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### lightgbm classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    #data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    #train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n    dtrain = lgb.Dataset(x_train, label=y_train)\n    dvalid = lgb.Dataset(x_valid, label=y_valid)\n\n    param = {\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    # Add a callback for pruning.\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n    gbm = lgb.train(\n        param, dtrain, valid_sets=[dvalid], verbose_eval=False, callbacks=[pruning_callback]\n    )\n\n    preds = gbm.predict(x_valid)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(y_valid, pred_labels)\n    return accuracy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n    )\nstudy.optimize(objective, n_trials=300)\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the optimization history. See :func:`~optuna.visualization.plot_optimization_history` for the details.\nplot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the learning curves of the trials. See :func:`~optuna.visualization.plot_intermediate_values` for the details.\nplot_intermediate_values(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize high-dimensional parameter relationships. See :func:`~optuna.visualization.plot_parallel_coordinate` for the details.\nplot_parallel_coordinate(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select parameters to visualize.\nplot_parallel_coordinate(study, params=[\"bagging_freq\", \"bagging_fraction\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select parameters to visualize.\nplot_contour(study, params=[\"bagging_freq\", \"bagging_fraction\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize individual hyperparameters as slice plot. See :func:`~optuna.visualization.plot_slice` for the details.\nplot_slice(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Select parameters to visualize.\nplot_slice(study, params=[\"bagging_freq\", \"bagging_fraction\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize parameter importances. See :func:`~optuna.visualization.plot_param_importances` for the details.\nplot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize empirical distribution function. See :func:`~optuna.visualization.plot_edf` for the details.\nplot_edf(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Optuna with Keras"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(trial):\n    # We define our MLP.\n    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n    model = Sequential()\n    for i in range(n_layers):\n        num_hidden = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128, log=True)\n        model.add(Dense(num_hidden, activation=\"relu\"))\n        dropout = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n        model.add(Dropout(rate=dropout))\n    model.add(Dense(2, activation=\"softmax\"))\n\n#     model.add(Dense(116, activation=tf.nn.relu))\n#     model.add(Dense(58, activation=tf.nn.relu))\n#     model.add(Dense(2, activation=tf.nn.softmax))\n\n    \n    # We compile our model with a sampled learning rate.\n    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n    model.compile(\n        loss=\"categorical_crossentropy\",\n        optimizer=keras.optimizers.RMSprop(lr=lr),\n        metrics=[\"accuracy\"],\n    )\n    return model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model2(trial):\n\n    # Hyperparameters to be tuned by Optuna.\n    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n    momentum = trial.suggest_float(\"momentum\", 0.0, 1.0)\n    units = trial.suggest_categorical(\"units\", [32, 64, 128, 256, 512])\n\n    # Compose neural network with one hidden layer.\n    model = tf.keras.Sequential()\n    #model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(units=116, activation=tf.nn.relu))\n    model.add(tf.keras.layers.Dense(units=58, activation=tf.nn.relu))\n    model.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))\n\n    # Compile model.\n    model.compile(\n        optimizer=tf.keras.optimizers.SGD(lr=lr, momentum=momentum, nesterov=True),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    # Clear clutter from previous session graphs.\n    keras.backend.clear_session()\n    \n    # Dataset Loading\n    x_train_ = x_train.values.astype('float32') / 255\n    x_valid_ = x_valid.values.astype('float32') / 255\n    \n    y_train_ = keras.utils.to_categorical(y_train.values, 2)\n    y_valid_ = keras.utils.to_categorical(y_valid.values, 2)\n    \n    # Generate our trial model.\n    model = create_model(trial)\n    \n    # Fit the model on the training data.\n    # The KerasPruningCallback checks for pruning condition every epoch.\n    model.fit(\n        x_train_,\n        y_train_,\n        batch_size=32,\n        callbacks=[\n            tf.keras.callbacks.EarlyStopping(patience=3),\n            KerasPruningCallback(trial, \"val_accuracy\"),\n        ],\n        epochs=20,\n        validation_data=(x_valid_, y_valid_),\n        verbose=1,\n    )\n    \n    # Evaluate the model accuracy on the validation set.\n    score = model.evaluate(x_valid_, y_valid_, verbose=0)\n    return score[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    warnings.warn(\n        \"Recent Keras release (2.4.0) simply redirects all APIs \"\n        \"in the standalone keras package to point to tf.keras. \"\n        \"There is now only one Keras: tf.keras. \"\n        \"There may be some breaking changes for some workflows by upgrading to keras 2.4.0. \"\n        \"Test before upgrading. \"\n        \"REF:https://github.com/keras-team/keras/releases/tag/2.4.0\"\n    )\n    study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n    study.optimize(objective, n_trials=100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\ncomplete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n\nprint(\"Study statistics: \")\nprint(\"  Number of finished trials: \", len(study.trials))\nprint(\"  Number of pruned trials: \", len(pruned_trials))\nprint(\"  Number of complete trials: \", len(complete_trials))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_optimization_history(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the learning curves of the trials. See :func:`~optuna.visualization.plot_intermediate_values` for the details.\nplot_intermediate_values(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize high-dimensional parameter relationships. See :func:`~optuna.visualization.plot_parallel_coordinate` for the details.\nplot_parallel_coordinate(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize individual hyperparameters as slice plot. See :func:`~optuna.visualization.plot_slice` for the details.\nplot_slice(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize parameter importances. See :func:`~optuna.visualization.plot_param_importances` for the details.\nplot_param_importances(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize empirical distribution function. See :func:`~optuna.visualization.plot_edf` for the details.\nplot_edf(study)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}