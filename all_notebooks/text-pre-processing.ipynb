{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Preprocessing","metadata":{}},{"cell_type":"code","source":"pip install autocorrect","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:16:50.000646Z","iopub.execute_input":"2021-07-26T07:16:50.001468Z","iopub.status.idle":"2021-07-26T07:17:01.572736Z","shell.execute_reply.started":"2021-07-26T07:16:50.001309Z","shell.execute_reply":"2021-07-26T07:17:01.571852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"# Importing Libraries\nimport unidecode\nimport pandas as pd\nimport re\nimport time\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom autocorrect import Speller\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nimport string\nimport timeit\nstoplist = stopwords.words('english') \nstoplist = set(stoplist)\nspell = Speller(lang='en')\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:01.574074Z","iopub.execute_input":"2021-07-26T07:17:01.574355Z","iopub.status.idle":"2021-07-26T07:17:03.689404Z","shell.execute_reply.started":"2021-07-26T07:17:01.574326Z","shell.execute_reply":"2021-07-26T07:17:03.688412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read Dataset\nDf = pd.read_csv(r'../input/fakereal-news/New Task.csv', encoding = 'latin-1')\nprint('Number of Data points : ', Df.shape[0])\nprint('Number of features :', Df.shape[1])\nprint('features :', Df.columns.values)\n# Show Dataset\nDf.drop(Df.columns[Df.columns.str.contains('Unnamed: 0',case = False)],axis = 1, inplace = True)\nDf.head()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:03.691868Z","iopub.execute_input":"2021-07-26T07:17:03.692323Z","iopub.status.idle":"2021-07-26T07:17:03.834096Z","shell.execute_reply.started":"2021-07-26T07:17:03.692277Z","shell.execute_reply":"2021-07-26T07:17:03.83284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This command tells information about the non-null values of attributes of Dataset.\nDf.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:03.835935Z","iopub.execute_input":"2021-07-26T07:17:03.83652Z","iopub.status.idle":"2021-07-26T07:17:03.862541Z","shell.execute_reply.started":"2021-07-26T07:17:03.836472Z","shell.execute_reply":"2021-07-26T07:17:03.861471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Df['News_Headline'][0]","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:03.864051Z","iopub.execute_input":"2021-07-26T07:17:03.864342Z","iopub.status.idle":"2021-07-26T07:17:03.870776Z","shell.execute_reply.started":"2021-07-26T07:17:03.864314Z","shell.execute_reply":"2021-07-26T07:17:03.869781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shows statistics for every numerical column in our dataset.\nDf.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:03.872167Z","iopub.execute_input":"2021-07-26T07:17:03.872556Z","iopub.status.idle":"2021-07-26T07:17:04.072651Z","shell.execute_reply.started":"2021-07-26T07:17:03.872526Z","shell.execute_reply":"2021-07-26T07:17:04.071637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check type of Dataframe attribute that has to processed","metadata":{}},{"cell_type":"code","source":"#Type of attribute \"Title\"\ntype(Df['News_Headline'])\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.074071Z","iopub.execute_input":"2021-07-26T07:17:04.07437Z","iopub.status.idle":"2021-07-26T07:17:04.080392Z","shell.execute_reply.started":"2021-07-26T07:17:04.074342Z","shell.execute_reply":"2021-07-26T07:17:04.079641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove newlines & tabs ","metadata":{}},{"cell_type":"code","source":"def remove_newlines_tabs(text):\n    \"\"\"\n    This function will remove all the occurrences of newlines, tabs, and combinations like: \\\\n, \\\\.\n    \n    arguments:\n        input_text: \"text\" of type \"String\". \n                    \n    return:\n        value: \"text\" after removal of newlines, tabs, \\\\n, \\\\ characters.\n        \n    Example:\n    Input : This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\n    Output : This is her first day at this place. Please, Be nice to her. \n    \n    \"\"\"\n    \n    # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n    Formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n    return Formatted_text\n# len of data :- 1618647 lac words","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-26T07:17:04.082578Z","iopub.execute_input":"2021-07-26T07:17:04.083055Z","iopub.status.idle":"2021-07-26T07:17:04.092765Z","shell.execute_reply.started":"2021-07-26T07:17:04.083022Z","shell.execute_reply":"2021-07-26T07:17:04.091988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Strip Html Tags","metadata":{}},{"cell_type":"code","source":"def strip_html_tags(text):\n    \"\"\" \n    This function will remove all the occurrences of html tags from the text.\n    \n    arguments:\n        input_text: \"text\" of type \"String\". \n                    \n    return:\n        value: \"text\" after removal of html tags.\n        \n    Example:\n    Input : This is a nice place to live. <IMG>\n    Output : This is a nice place to live.  \n    \"\"\"\n    # Initiating BeautifulSoup object soup.\n    soup = BeautifulSoup(text, \"html.parser\")\n    # Get all the text other than html tags.\n    stripped_text = soup.get_text(separator=\" \")\n    return stripped_text\n# len of string:- 1616053 lac words","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.094451Z","iopub.execute_input":"2021-07-26T07:17:04.094989Z","iopub.status.idle":"2021-07-26T07:17:04.105215Z","shell.execute_reply.started":"2021-07-26T07:17:04.094954Z","shell.execute_reply":"2021-07-26T07:17:04.104129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove Links ","metadata":{}},{"cell_type":"code","source":"def remove_links(text):\n    \"\"\"\n    This function will remove all the occurrences of links.\n    \n    arguments:\n        input_text: \"text\" of type \"String\". \n                    \n    return:\n        value: \"text\" after removal of all types of links.\n        \n    Example:\n    Input : To know more about cats and food & website: catster.com  visit: https://catster.com//how-to-feed-cats\n    Output : To know more about cats and food & website: visit:     \n    \n    \"\"\"\n    \n    # Removing all the occurrences of links that starts with https\n    remove_https = re.sub(r'http\\S+', '', text)\n    # Remove all the occurrences of text that ends with .com\n    remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n    return remove_com\n# len of words:- 1616053","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-07-26T07:17:04.106425Z","iopub.execute_input":"2021-07-26T07:17:04.106931Z","iopub.status.idle":"2021-07-26T07:17:04.123604Z","shell.execute_reply.started":"2021-07-26T07:17:04.106856Z","shell.execute_reply":"2021-07-26T07:17:04.122744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Remove WhiteSpaces","metadata":{}},{"cell_type":"code","source":"def remove_whitespace(text):\n    \"\"\" This function will remove \n        extra whitespaces from the text\n    arguments:\n        input_text: \"text\" of type \"String\". \n                    \n    return:\n        value: \"text\" after extra whitespaces removed .\n        \n    Example:\n    Input : How   are   you   doing   ?\n    Output : How are you doing ?     \n        \n    \"\"\"\n    pattern = re.compile(r'\\s+') \n    Without_whitespace = re.sub(pattern, ' ', text)\n    # There are some instances where there is no space after '?' & ')', \n    # So I am replacing these with one space so that It will not consider two words as one token.\n    text = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n    return text    \n# len of words:- 1596248 lac words","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.124838Z","iopub.execute_input":"2021-07-26T07:17:04.125359Z","iopub.status.idle":"2021-07-26T07:17:04.138312Z","shell.execute_reply.started":"2021-07-26T07:17:04.125307Z","shell.execute_reply":"2021-07-26T07:17:04.137444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step1: Remove Accented Characters\n","metadata":{}},{"cell_type":"code","source":"# Code for accented characters removal\ndef accented_characters_removal(text):\n    # this is a docstring\n    \"\"\"\n    The function will remove accented characters from the \n    text contained within the Dataset.\n       \n    arguments:\n        input_text: \"text\" of type \"String\". \n                    \n    return:\n        value: \"text\" with removed accented characters.\n        \n    Example:\n    Input : Málaga, àéêöhello\n    Output : Malaga, aeeohello    \n        \n    \"\"\"\n    # Remove accented characters from text using unidecode.\n    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n    text = unidecode.unidecode(text)\n    return text\n# Len of data:- 1593952 lac of words","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.139504Z","iopub.execute_input":"2021-07-26T07:17:04.139985Z","iopub.status.idle":"2021-07-26T07:17:04.157083Z","shell.execute_reply.started":"2021-07-26T07:17:04.139937Z","shell.execute_reply":"2021-07-26T07:17:04.155937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step2: Case Conversion","metadata":{}},{"cell_type":"code","source":"# Code for text lowercasing\ndef lower_casing_text(text):\n    \n    \"\"\"\n    The function will convert text into lower case.\n    \n    arguments:\n         input_text: \"text\" of type \"String\".\n         \n    return:\n         value: text in lowercase\n         \n    Example:\n    Input : The World is Full of Surprises!\n    Output : the world is full of surprises!\n    \n    \"\"\"\n    # Convert text to lower case\n    # lower() - It converts all upperase letter of given string to lowercase.\n    text = text.lower()\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.158596Z","iopub.execute_input":"2021-07-26T07:17:04.158978Z","iopub.status.idle":"2021-07-26T07:17:04.17108Z","shell.execute_reply.started":"2021-07-26T07:17:04.158933Z","shell.execute_reply":"2021-07-26T07:17:04.170068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step3: Reduce repeated characters and punctuations¶","metadata":{}},{"cell_type":"code","source":"# Code for removing repeated characters and punctuations\n\ndef reducing_incorrect_character_repeatation(text):\n    \"\"\"\n    This Function will reduce repeatition to two characters \n    for alphabets and to one character for punctuations.\n    \n    arguments:\n         input_text: \"text\" of type \"String\".\n         \n    return:\n        value: Finally formatted text with alphabets repeating to \n        two characters & punctuations limited to one repeatition \n        \n    Example:\n    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n    Output : Reallyy, Greeaatt !?.;:)\n    \n    \"\"\"\n    # Pattern matching for all case alphabets\n    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n    \n    # Limiting all the  repeatation to two characters.\n    Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text) \n    \n    # Pattern matching for all the punctuations that can occur\n    Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n    \n    # Limiting punctuations in previously formatted string to only one.\n    Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n    \n    # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n    return Final_Formatted\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.172362Z","iopub.execute_input":"2021-07-26T07:17:04.172662Z","iopub.status.idle":"2021-07-26T07:17:04.187017Z","shell.execute_reply.started":"2021-07-26T07:17:04.172626Z","shell.execute_reply":"2021-07-26T07:17:04.185905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explanation for using some symbols in above regex expression\n**\\1** —> is equivalent to re.search(...). group(1). It Refers to first capturing group. \\1 matches the exact same text that was matched by the first capturing group.\n\n**{1,}** --> It means we are matching for repeatation that occurs more than one times. \n\n**DOTALL** -> It matches newline character as well unlike dot operator which matches everything in the given text except newline character. \n\n**sub()** --> This function is used to replace occurrences of a particular sub-string with another sub-string. This function takes as input the following: The sub-string to replace. The sub-string to replace with.\n\n**r'\\1\\1'** --> It limits all the repeatation to two characters.\n\n**r'\\1'** --> Limits all the repeatation to only one character.\n\n**{2,}** --> It means to match for repeatation that occurs more than two times","metadata":{}},{"cell_type":"markdown","source":"## Step4: Expand contraction words","metadata":{}},{"cell_type":"code","source":"CONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n}\n# The code for expanding contraction words\ndef expand_contractions(text, contraction_mapping =  CONTRACTION_MAP):\n    \"\"\"expand shortened words to the actual form.\n       e.g. don't to do not\n    \n       arguments:\n            input_text: \"text\" of type \"String\".\n         \n       return:\n            value: Text with expanded form of shorthened words.\n        \n       Example: \n       Input : ain't, aren't, can't, cause, can't've\n       Output :  is not, are not, cannot, because, cannot have \n    \n     \"\"\"\n    # Tokenizing text into tokens.\n    list_Of_tokens = text.split(' ')\n\n    # Checking for whether the given token matches with the Key & replacing word with key's value.\n    \n    # Check whether Word is in lidt_Of_tokens or not.\n    for Word in list_Of_tokens: \n        # Check whether found word is in dictionary \"Contraction Map\" or not as a key. \n         if Word in CONTRACTION_MAP: \n                # If Word is present in both dictionary & list_Of_tokens, replace that word with the key value.\n                list_Of_tokens = [item.replace(Word, CONTRACTION_MAP[Word]) for item in list_Of_tokens]\n                \n    # Converting list of tokens to String.\n    String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens) \n    return String_Of_tokens     \n   ","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.188734Z","iopub.execute_input":"2021-07-26T07:17:04.189123Z","iopub.status.idle":"2021-07-26T07:17:04.209268Z","shell.execute_reply.started":"2021-07-26T07:17:04.189089Z","shell.execute_reply":"2021-07-26T07:17:04.20827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step5: Remove special characters","metadata":{}},{"cell_type":"code","source":"# The code for removing special characters\ndef removing_special_characters(text):\n    \"\"\"Removing all the special characters except the one that is passed within \n       the regex to match, as they have imp meaning in the text provided.\n   \n    \n    arguments:\n         input_text: \"text\" of type \"String\".\n         \n    return:\n        value: Text with removed special characters that don't require.\n        \n    Example: \n    Input : Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \n    Output :  Hello, Kajal. This is $100.05 : the payment that you will recieve! Is this okay?\n    \n   \"\"\"\n    # The formatted text after removing not necessary punctuations.\n    Formatted_Text = re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', text) \n    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n    return Formatted_Text\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.210983Z","iopub.execute_input":"2021-07-26T07:17:04.211431Z","iopub.status.idle":"2021-07-26T07:17:04.232557Z","shell.execute_reply.started":"2021-07-26T07:17:04.211385Z","shell.execute_reply":"2021-07-26T07:17:04.231378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Punctuations that I am considering Important as per my Dataset.\n**,.?!** --> These are some frequent punctuations that occurs a lot and needed to preserved as to understand the context of text.\n\n__:__ --> This one is also frequent as per the  Dataset. It is important to keep bcz it is giving sense whenever there is a occurrence of time like: **9:05 p.m.**\n\n**%** --> This one is also frequently used in many articles and telling more precisely about the data, facts & figures.\n\n**$** --> This one is used in many articles where prices are considered. So, omitting this symbol will not give much sense about those prices that left as just some numbers only.","metadata":{}},{"cell_type":"markdown","source":"## Step6: Remove stopwords","metadata":{}},{"cell_type":"code","source":"# The code for removing stopwords\nstoplist = stopwords.words('english') \nstoplist = set(stoplist)\ndef removing_stopwords(text):\n    \"\"\"This function will remove stopwords which doesn't add much meaning to a sentence \n       & they can be remove safely without comprimising meaning of the sentence.\n    \n    arguments:\n         input_text: \"text\" of type \"String\".\n         \n    return:\n        value: Text after omitted all stopwords.\n        \n    Example: \n    Input : This is Kajal from delhi who came here to study.\n    Output : [\"'This\", 'Kajal', 'delhi', 'came', 'study', '.', \"'\"] \n    \n   \"\"\"\n    # repr() function actually gives the precise information about the string\n    text = repr(text)\n    # Text without stopwords\n    No_StopWords = [word for word in word_tokenize(text) if word.lower() not in stoplist ]\n    # Convert list of tokens_without_stopwords to String type.\n    words_string = ' '.join(No_StopWords)    \n    return words_string\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.234349Z","iopub.execute_input":"2021-07-26T07:17:04.234808Z","iopub.status.idle":"2021-07-26T07:17:04.250685Z","shell.execute_reply.started":"2021-07-26T07:17:04.234728Z","shell.execute_reply":"2021-07-26T07:17:04.249834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Checking spellings for all the stopwords ","metadata":{}},{"cell_type":"markdown","source":"## Step8: Correct mis-spelled words in text","metadata":{}},{"cell_type":"code","source":"# The code for spelling corrections\ndef spelling_correction(text):\n    ''' \n    This function will correct spellings.\n    \n    arguments:\n         input_text: \"text\" of type \"String\".\n         \n    return:\n        value: Text after corrected spellings.\n        \n    Example: \n    Input : This is Oberois from Dlhi who came heree to studdy.\n    Output : This is Oberoi from Delhi who came here to study.\n      \n    \n    '''\n    # Check for spellings in English language\n    spell = Speller(lang='en')\n    Corrected_text = spell(text)\n    return Corrected_text\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.252067Z","iopub.execute_input":"2021-07-26T07:17:04.252603Z","iopub.status.idle":"2021-07-26T07:17:04.26326Z","shell.execute_reply.started":"2021-07-26T07:17:04.252555Z","shell.execute_reply":"2021-07-26T07:17:04.262241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step7: Lemmatization","metadata":{}},{"cell_type":"code","source":"# The code for lemmatization\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\ndef lemmatization(text):\n    \"\"\"This function converts word to their root words \n       without explicitely cut down as done in stemming.\n    \n    arguments:\n         input_text: \"text\" of type \"String\".\n         \n    return:\n        value: Text having root words only, no tense form, no plural forms\n        \n    Example: \n    Input : text reduced \n    Output :  text reduce\n    \n   \"\"\"\n    # Converting words to their root forms\n    lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]\n    return lemma\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.264589Z","iopub.execute_input":"2021-07-26T07:17:04.265219Z","iopub.status.idle":"2021-07-26T07:17:04.280448Z","shell.execute_reply.started":"2021-07-26T07:17:04.265185Z","shell.execute_reply":"2021-07-26T07:17:04.279293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step9: Putting all in single function","metadata":{}},{"cell_type":"code","source":"# Writing main function to merge all the preprocessing steps.\ndef text_preprocessing(text, accented_chars=True, contractions=True, lemma = True,\n                        extra_whitespace=True, newlines_tabs=True, repeatition=True, \n                       lowercase=True, punctuations=True, mis_spell=True,\n                       remove_html=True, links=True,  special_chars=True,\n                       stop_words=False):\n    \"\"\"\n    This function will preprocess input text and return\n    the clean text.\n    \"\"\"\n        \n    if newlines_tabs == True: #remove newlines & tabs.\n        Data = remove_newlines_tabs(text)\n\n    if remove_html == True: #remove html tags\n        Data = strip_html_tags(Data)\n\n    if links == True: #remove links\n        Data = remove_links(Data)\n\n    if extra_whitespace == True: #remove extra whitespaces\n        Data = remove_whitespace(Data)\n\n    if accented_chars == True: #remove accented characters\n        Data = accented_characters_removal(Data)\n\n    if lowercase == True: #convert all characters to lowercase\n        Data = lower_casing_text(Data)\n\n    if repeatition == True: #Reduce repeatitions   \n        Data = reducing_incorrect_character_repeatation(Data)\n\n    if contractions == True: #expand contractions\n        Data = expand_contractions(Data)\n\n    if punctuations == True: #remove punctuations\n        Data = removing_special_characters(Data)\n\n    stoplist = stopwords.words('english') \n    stoplist = set(stoplist)\n    \n    if stop_words == True: #Remove stopwords\n        Data = removing_stopwords(Data)\n\n    spell = Speller(lang='en')\n    \n    if mis_spell == True: #Check for mis-spelled words & correct them.\n        Data = spelling_correction(Data)\n\n    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n    lemmatizer = nltk.stem.WordNetLemmatizer()\n     \n    if lemma == True: #Converts words to lemma form.\n        Data = lemmatization(Data)\n\n           \n    return Data","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.281658Z","iopub.execute_input":"2021-07-26T07:17:04.282113Z","iopub.status.idle":"2021-07-26T07:17:04.292598Z","shell.execute_reply.started":"2021-07-26T07:17:04.282079Z","shell.execute_reply":"2021-07-26T07:17:04.291528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pre-processing for Content\nList_Content = Df['News_Headline'].to_list()\nFinal_Article = []\nComplete_Content = []\nfor article in List_Content:\n    Processed_Content = text_preprocessing(article) #Cleaned text of Content attribute after pre-processing\n    Final_Article.append(Processed_Content)\nComplete_Content.extend(Final_Article)\nDf['Processed_Title'] = Complete_Content\n","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:17:04.294122Z","iopub.execute_input":"2021-07-26T07:17:04.294418Z","iopub.status.idle":"2021-07-26T07:52:23.912253Z","shell.execute_reply.started":"2021-07-26T07:17:04.29439Z","shell.execute_reply":"2021-07-26T07:52:23.911201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Df['Processed_Title']","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:52:23.91339Z","iopub.execute_input":"2021-07-26T07:52:23.913682Z","iopub.status.idle":"2021-07-26T07:52:23.927023Z","shell.execute_reply.started":"2021-07-26T07:52:23.913655Z","shell.execute_reply":"2021-07-26T07:52:23.925606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:52:23.930989Z","iopub.execute_input":"2021-07-26T07:52:23.931416Z","iopub.status.idle":"2021-07-26T07:52:23.953846Z","shell.execute_reply.started":"2021-07-26T07:52:23.931372Z","shell.execute_reply":"2021-07-26T07:52:23.952791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Cleaned_Data = Df.to_csv('Cleaned_Data_with_Stopwords.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T07:52:23.955069Z","iopub.execute_input":"2021-07-26T07:52:23.955354Z","iopub.status.idle":"2021-07-26T07:52:24.207246Z","shell.execute_reply.started":"2021-07-26T07:52:23.955327Z","shell.execute_reply":"2021-07-26T07:52:24.206504Z"},"trusted":true},"execution_count":null,"outputs":[]}]}