{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n\n!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-23T04:28:07.192235Z","iopub.execute_input":"2021-05-23T04:28:07.192668Z","iopub.status.idle":"2021-05-23T04:28:13.344094Z","shell.execute_reply.started":"2021-05-23T04:28:07.192621Z","shell.execute_reply":"2021-05-23T04:28:13.343063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader,Dataset\nimport torch.nn as nn\n\nimport pandas as pd\nfrom transformers import  BertTokenizer,BertModel,RobertaTokenizer\nfrom sklearn.metrics import accuracy_score, roc_auc_score,f1_score\nfrom  sklearn.model_selection import train_test_split\nimport os\nimport random\nimport glob\nimport torch\nimport re\n# choose the attention mode 'n2', 'tvm' or 'sliding_chunks'\n# 'n2': for regular n2 attantion\n# 'tvm': a custom CUDA kernel implementation of our sliding window attention\n# 'sliding_chunks': a PyTorch implementation of our sliding window attention","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:28:13.347961Z","iopub.execute_input":"2021-05-23T04:28:13.348235Z","iopub.status.idle":"2021-05-23T04:28:13.355757Z","shell.execute_reply.started":"2021-05-23T04:28:13.348206Z","shell.execute_reply":"2021-05-23T04:28:13.355096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def qingli(s):\n    #pattern  = r\"(https?://|[@#])\\S*\"\n    #a = re.sub(pattern, '', s)\n    #string1 = s.apply(lambda x:re.sub('[A-z]','*',str(x)))#去除字母\n    string1 = s.apply(lambda x: re.sub('[0-9]', '*',str(x)))#去除数字\n    m=re.compile('\\s+')#定义空格\n    string2 = string1.apply(lambda x: re.sub(m, '*',x))#去除空格\n    punctuation = \"\"\"，！？｡＂#＄％＆＇（）＊＋－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘'‛“”„‟…‧﹏\"\"\"\n    re_punctuation = \"[{}]+\".format(punctuation)#去除标点符号\n    string3 = string2.apply(lambda x: re.sub(re_punctuation, '*', x))\n    a = string3.apply(lambda x: re.sub('\\*','',x))\n    return a","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:28:13.358818Z","iopub.execute_input":"2021-05-23T04:28:13.359077Z","iopub.status.idle":"2021-05-23T04:28:13.367864Z","shell.execute_reply.started":"2021-05-23T04:28:13.359053Z","shell.execute_reply":"2021-05-23T04:28:13.367006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, eps=0.4, reduction='mean',gamma=2,alpha=0.7,size_average=True):\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        self.eps = eps\n        self.reduction = reduction\n        self.gamma = gamma\n        self.alpha = alpha\n        # if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])  # long\n        # if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n        self.size_average = size_average\n    def forward(self, output, target):\n        c = output.size()[-1] #K\n        log_preds = F.log_softmax(output, dim=-1)\n        if self.reduction=='sum':\n            loss = -log_preds.sum()\n        else:\n            loss = -log_preds.sum(dim=-1)\n            if self.reduction=='mean':\n                loss = loss.mean()\n        # return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target, reduction=self.reduction)\n        return loss*self.eps/c + (1-self.eps) * self.Focalloss(output, target)\n\n    def Focalloss(self, input, target):\n            # input:size is M*2. M　is the batch　number\n            # target:size is M.\n            pt = torch.softmax(input, dim=1)\n            p = pt[:, 1]\n            loss = -self.alpha * (1 - p) ** self.gamma * (target * torch.log(p)) - \\\n                   (1 - self.alpha) * p ** self.gamma * ((1 - target) * torch.log(1 - p))\n            return loss.mean()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:28:13.370932Z","iopub.execute_input":"2021-05-23T04:28:13.371196Z","iopub.status.idle":"2021-05-23T04:28:13.382553Z","shell.execute_reply.started":"2021-05-23T04:28:13.371168Z","shell.execute_reply":"2021-05-23T04:28:13.381749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\n# from tensorboardX import SummaryWriter\n# writer = SummaryWriter(logdir='/cps/gadam/n_cifa/')\n# iter_idx = 0\n\n# from ipdb import set_trace\nimport torch.optim\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay)\n\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n        beta2_t = None\n        ratio = None\n        N_sma_max = None\n        N_sma = None\n\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                if beta2_t is None:\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    beta1_t = 1 - beta1 ** state['step']\n                    if N_sma >= 5:\n                        ratio = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / beta1_t\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:                    \n                    step_size = group['lr'] * ratio\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    step_size = group['lr'] / beta1_t\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:28:13.385235Z","iopub.execute_input":"2021-05-23T04:28:13.38571Z","iopub.status.idle":"2021-05-23T04:28:13.404169Z","shell.execute_reply.started":"2021-05-23T04:28:13.385576Z","shell.execute_reply":"2021-05-23T04:28:13.403348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\nfrom torch.optim import Optimizer\nimport torch\n \nclass Lookahead(Optimizer):\n    def __init__(self, optimizer, k=5, alpha=0.5):\n        self.optimizer = optimizer\n \n        self.k = k\n        self.alpha = alpha\n        self.param_groups = self.optimizer.param_groups\n        self.state = defaultdict(dict)\n        self.fast_state = self.optimizer.state\n        for group in self.param_groups:\n            group[\"counter\"] = 0\n \n    def update(self, group):\n        for fast in group[\"params\"]:\n            param_state = self.state[fast]\n            if \"slow_param\" not in param_state:\n                param_state[\"slow_param\"] = torch.zeros_like(fast.data)\n                param_state[\"slow_param\"].copy_(fast.data)\n            slow = param_state[\"slow_param\"]\n            slow += (fast.data - slow) * self.alpha\n            fast.data.copy_(slow)\n \n    def update_lookahead(self):\n        for group in self.param_groups:\n            self.update(group)\n \n    def step(self, closure=None):\n        loss = self.optimizer.step(closure)\n        for group in self.param_groups:\n            if group[\"counter\"] == 0:\n                self.update(group)\n            group[\"counter\"] += 1\n            if group[\"counter\"] >= self.k:\n                group[\"counter\"] = 0\n        return loss\n \n    def state_dict(self):\n        fast_state_dict = self.optimizer.state_dict()\n        slow_state = {\n            (id(k) if isinstance(k, torch.Tensor) else k): v\n            for k, v in self.state.items()\n        }\n        fast_state = fast_state_dict[\"state\"]\n        param_groups = fast_state_dict[\"param_groups\"]\n        return {\n            \"fast_state\": fast_state,\n            \"slow_state\": slow_state,\n            \"param_groups\": param_groups,\n        }\n \n    def load_state_dict(self, state_dict):\n        slow_state_dict = {\n            \"state\": state_dict[\"slow_state\"],\n            \"param_groups\": state_dict[\"param_groups\"],\n        }\n        fast_state_dict = {\n            \"state\": state_dict[\"fast_state\"],\n            \"param_groups\": state_dict[\"param_groups\"],\n        }\n        super(Lookahead, self).load_state_dict(slow_state_dict)\n        self.optimizer.load_state_dict(fast_state_dict)\n        self.fast_state = self.optimizer.state\n \n    def add_param_group(self, param_group):\n        param_group[\"counter\"] = 0\n        self.optimizer.add_param_group(param_group)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:28:13.40616Z","iopub.execute_input":"2021-05-23T04:28:13.406476Z","iopub.status.idle":"2021-05-23T04:28:13.422982Z","shell.execute_reply.started":"2021-05-23T04:28:13.406451Z","shell.execute_reply":"2021-05-23T04:28:13.42234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass FocalLoss(nn.Module):\n\n    def __init__(self, gamma=2, alpha=0.7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def forward(self, input, target):\n        # input:size is M*2. M　is the batch　number\n        # target:size is M.\n        pt = torch.softmax(input, dim=1)\n        p = pt[:, 1]\n        loss = -self.alpha * (1 - p) ** self.gamma * (target * torch.log(p)) - \\\n               (1 - self.alpha) * p ** self.gamma * ((1 - target) * torch.log(1 - p))\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:28:13.425997Z","iopub.execute_input":"2021-05-23T04:28:13.426262Z","iopub.status.idle":"2021-05-23T04:28:13.437399Z","shell.execute_reply.started":"2021-05-23T04:28:13.426211Z","shell.execute_reply":"2021-05-23T04:28:13.436712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FGM():\n    def __init__(self, model):\n        self.model = model\n        self.backup = {}\n \n    def attack(self, epsilon=0.3, emb_name='embedding.'):\n        # emb_name这个参数要换成你模型中embedding的参数名\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name:\n                self.backup[name] = param.data.clone()\n                norm = torch.norm(param.grad)\n                if norm != 0:\n                    r_at = epsilon * param.grad / norm\n                    param.data.add_(r_at)\n \n    def restore(self, emb_name='embedding.'):\n        # emb_name这个参数要换成你模型中embedding的参数名\n        for name, param in self.model.named_parameters():\n            if param.requires_grad and emb_name in name: \n                assert name in self.backup\n                param.data = self.backup[name]\n        self.backup = {}","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:28:13.438611Z","iopub.execute_input":"2021-05-23T04:28:13.438943Z","iopub.status.idle":"2021-05-23T04:28:13.448594Z","shell.execute_reply.started":"2021-05-23T04:28:13.438909Z","shell.execute_reply":"2021-05-23T04:28:13.447911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install jieba\nimport jieba\nfrom transformers import BasicTokenizer, BertTokenizer\n\n\nclass CustomBasicTokenizer(BasicTokenizer):\n    def __init__(self,\n                 vocab,\n                 do_lower_case=True,\n                 never_split=None,\n                 tokenize_chinese_chars=True,\n                 strip_accents=None):\n        super().__init__(do_lower_case=do_lower_case,\n                         never_split=never_split,\n                         tokenize_chinese_chars=tokenize_chinese_chars,\n                         strip_accents=strip_accents)\n\n        self.vocab = vocab\n\n    def _tokenize_chinese_chars(self, text):\n        output = []\n        '''\n        1、输入一个句子s，用pre_tokenize先分一次词，得到[w1,w2,…,wl]；\n        2、遍历各个wi，如果wi在词表中则保留，否则将wi用BERT自带的tokenize函数再分一次；\n        3、将每个wi的tokenize结果有序拼接起来，作为最后的tokenize结果。\n        '''\n        for wholeword in jieba.cut(text, HMM=False):\n            if wholeword in self.vocab:\n                output.append(\" \")\n                output.append(wholeword)\n                output.append(\" \")\n            else:\n                for char in wholeword:\n                    cp = ord(char)\n                    if self._is_chinese_char(cp):\n                        output.append(\" \")\n                        output.append(char)\n                        output.append(\" \")\n                    else:\n                        output.append(char)\n        return \"\".join(output)\n\n\nclass WoBertTokenizer(BertTokenizer):\n    def __init__(self,\n                 vocab_file,\n                 do_lower_case=True,\n                 do_basic_tokenize=True,\n                 never_split=None,\n                 unk_token=\"[UNK]\",\n                 sep_token=\"[SEP]\",\n                 pad_token=\"[PAD]\",\n                 cls_token=\"[CLS]\",\n                 mask_token=\"[MASK]\",\n                 tokenize_chinese_chars=True,\n                 strip_accents=None,\n                 **kwargs):\n        super().__init__(vocab_file,\n                         do_lower_case=do_lower_case,\n                         do_basic_tokenize=do_basic_tokenize,\n                         never_split=never_split,\n                         unk_token=unk_token,\n                         sep_token=sep_token,\n                         pad_token=pad_token,\n                         cls_token=cls_token,\n                         mask_token=mask_token,\n                         tokenize_chinese_chars=tokenize_chinese_chars,\n                         strip_accents=strip_accents,\n                         **kwargs)\n        if self.do_basic_tokenize:\n            self.basic_tokenizer = CustomBasicTokenizer(\n                vocab=self.vocab,\n                do_lower_case=do_lower_case,\n                never_split=never_split,\n                tokenize_chinese_chars=tokenize_chinese_chars,\n                strip_accents=strip_accents,\n            )","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:28:13.450777Z","iopub.execute_input":"2021-05-23T04:28:13.451017Z","iopub.status.idle":"2021-05-23T04:28:18.983156Z","shell.execute_reply.started":"2021-05-23T04:28:13.450994Z","shell.execute_reply":"2021-05-23T04:28:18.98207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# ————————————————确定种子——————————————————\ndef seed_torch(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.enabled = False\nseed_torch(42)\n\n# ——————————————————1. 加载模型——————————————————————\npretrain_model_path =\"hfl/chinese-bert-wwm\"#hfl/chinese-bert-wwm\n# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n\n\nclass Model(nn.Module):\n    def __init__(self, pretrain_model_path, hidden_size:int=768):\n        super(Model, self).__init__()\n        self.pretrain_model_path = pretrain_model_path\n        self.bert = BertModel.from_pretrained(pretrain_model_path)\n        self.avgpool = nn.AvgPool1d(512)\n        #冻结了除pooler层的所有的参数更新--->可选参数为encoder,pooler,embedding\n        for param in self.bert.parameters():\n            param.requires_grad = True\n#             else:\n#                 param.requires_grad = False\n#                 # param.requires_grad_(False)\n        self.embed_size  = hidden_size\n        self.linear1 = nn.Linear(2 * hidden_size, hidden_size)\n        self.embed = nn.Embedding(3,768)\n        self.cls = nn.Linear(self.embed_size, 2)\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(768, 512),\n            nn.ReLU(),\n            nn.Linear(512, 2),\n        )\n    def forward(self, input_ids, token_type_ids, attention_mask,cond):\n        weight=self.embed(cond).squeeze(1)\n        output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask,output_hidden_states =True,output_attentions=True)\n        sequence_out, cls_out = output['last_hidden_state'],output['pooler_output']\n#         sequence_out = sequence_out.permute(0,2,1).contiguous()\n#         avgpool_out =self.avgpool(sequence_out).squeeze(2)\n#         # maxpool_out = self.maxpool(sequence_out).squeeze(2)\n#         outputs = torch.cat((avgpool_out, cls_out), 1)\n#         outputs =self.linear1(outputs)\n#         cls_out=cls_out+weight\n        logits = self.linear_relu_stack(sequence_out[:,0])\n        return  logits\n\n\n\n# 2. 建立Dataset\nclass SohuDataset(Dataset):\n    def __init__(self,corpus,pretrain_model_path,max_length,is_test:bool):\n        self.corpus = corpus\n        self.tokenizer = BertTokenizer.from_pretrained(pretrain_model_path)\n        self.max_length = max_length\n        self.is_test = is_test\n    def __len__(self):\n        return self.corpus.shape[0]\n\n    def __getitem__(self, idx):\n        content = self.corpus.iloc[idx].values\n        if self.is_test:\n            output = self.tokenizer.encode_plus(content[0], content[1], truncation=True, max_length=self.max_length,\n                                                padding='max_length')\n            return {'input_ids': np.array(output['input_ids']),\n                    'token_type_ids': np.array(output['token_type_ids']),\n                    'attention_mask': np.array(output['attention_mask']),\n                    'cond':np.array([content[3]])\n                    }\n        else:\n            output = self.tokenizer.encode_plus(content[0],content[1],truncation=True,max_length=self.max_length,padding='max_length')\n            return{'input_ids':np.array(output['input_ids']),\n                  'token_type_ids':np.array(output['token_type_ids']),\n                    'attention_mask':np.array(output['attention_mask']),\n                   'label':np.array(content[2]),\n                    'cond':np.array([content[3]])\n                   }\n\n\n\n\n# def load_data(data):\n#     df = pd.DataFrame()\n#     for i in glob.glob('../input/sohudataset/*_{}'.format(data)):#sorted(glob.glob('*_{}'.format(data)),key=os.path.getsize)---->按照文件大小排序\n#         print(i)\n#         df = pd.concat([df,pd.read_csv(i)],axis=0,ignore_index=True)\n#     return df\ndata = pd.read_csv('../input/sohudata-b/train_b.csv')\ndata['text_a']=qingli(data['text_a'])\ndata['text_b']=qingli(data['text_b'])\ntrain,valid = train_test_split(data,test_size=0.1,random_state=42,shuffle=True)\ntest = pd.read_csv('../input/sohudata-b/test_b.csv')\ntest['text_a']=qingli(test['text_a'])\ntest['text_b']=qingli(test['text_b'])\nseq_len=512\ntrain_data = SohuDataset(train,pretrain_model_path,seq_len,is_test=False)\nvalid_data = SohuDataset(valid,pretrain_model_path,seq_len,is_test=False)\ntest_data = SohuDataset(test,pretrain_model_path,seq_len,is_test=True)\nBATCH_SIZE = 8\ntrain_loadar =DataLoader(train_data,batch_size=BATCH_SIZE,shuffle=True)\nvalid_loader =DataLoader(dataset=valid_data,batch_size=BATCH_SIZE)\ntest_loader =DataLoader(dataset=test_data,batch_size=BATCH_SIZE)\nthreshold =0.15\n# 3. 训练板块\n#——————————————————3.1 评估函数(验证集)————————————————————————————\ndef evaluate(valid_loader):\n    model.eval()\n    pbar = tqdm(valid_loader)\n    labels_list = []\n    pred = []\n    for input in pbar:\n        input_ids = input['input_ids'].to(device).long()\n        token_type_ids = input['token_type_ids'].to(device).long()\n        attention_mask = input['attention_mask'].to(device).long()\n        label = input['label'].to(device).long()\n        cond = input['cond'].to(device).long()\n        logits = model(input_ids, token_type_ids, attention_mask,cond)\n        logits = nn.Softmax(dim=-1)(logits)\n        labels_list.append(label.cpu().detach().numpy())\n        pred.append(logits.cpu().detach().numpy().argmax(1))\n    pred =np.concatenate(pred)\n    labels_list = np.concatenate(labels_list)\n    return f1_score(labels_list,pred),roc_auc_score(labels_list, pred)\n# 3.2 ————————————预测函数(测试集)——————————————————————————\ndef predict(test_loader):\n    model = torch.load('./best_acc.pkl')\n    model.eval()\n    pbar = tqdm(test_loader)\n    pred = []\n    for input in pbar:\n        # 这里np.array不用转为tensor之间加上cuda就行\n        input_ids = input['input_ids'].to(device).long()\n        token_type_ids = input['token_type_ids'].to(device).long()\n        attention_mask = input['attention_mask'].to(device).long()\n        cond = input['cond'].to(device).long()\n        logits = model(input_ids, token_type_ids, attention_mask,cond)\n        logits = nn.Softmax(dim=-1)(logits)\n        pred.append(logits.cpu().detach().numpy().argmax(1))\n    pred =np.concatenate(pred)\n    test['label']=pred\n    test.drop(['text_a','text_b','cond'],axis=1,inplace=True)\n    test.to_csv('sub_b.csv',index=False)\n\n\n# ——————————————————3.3训练部分————————————————————————————————————\nEPOCH =1\nLR = 2e-5\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = Model(pretrain_model_path).to(device)\n# model = torch.load('../input/sohubert/best_acc.pkl')\noptim = RAdam(model.parameters(),lr=LR,weight_decay=0.1,betas=(0.9, 0.999))\noptim = Lookahead(optim, k=5, alpha=0.5)\ncriterion = nn.CrossEntropyLoss()\nbest_score = 0.5\nf1_best=0.2\nfgm = FGM(model)\nfor epoch in range(EPOCH):\n    pbar = tqdm(train_loadar)\n    losses = []\n    labels_list = []\n    pred = []\n    for input in pbar:\n        # 这里np.array不用转为tensor之间加上cuda就行\n        input_ids =  input['input_ids'].to(device).long()\n        token_type_ids =  input['token_type_ids'].to(device).long()\n        attention_mask =  input['attention_mask'].to(device).long()\n        label =  input['label'].to(device).long()#[32]\n        cond = input['cond'].to(device).long()\n        fgm.attack() # 在embedding上添加对抗扰动\n        logits = model(input_ids, token_type_ids, attention_mask,cond)\n        # loss = nn.CrossEntropyLoss()(nn.LogSoftmax(dim=-1)(logits), label.view(-1))\n        loss = LabelSmoothingCrossEntropy()(logits.view(-1,2), label.view(-1))\n        #三件套\n        loss.backward()\n        fgm.restore()\n        optim.step()\n        optim.zero_grad()\n        losses.append(loss.cpu().detach().numpy())\n        labels_list.append(label.cpu().detach().numpy())\n        pred.append(logits.cpu().detach().numpy().argmax(1))#[:,1]\n        auc_score = roc_auc_score(np.concatenate(labels_list),np.concatenate(pred))#np.int64(np.concatenate(pred)>threshold)\n        f1 = f1_score(np.concatenate(labels_list),np.concatenate(pred))\n        pbar.set_description(f'Epoch:{epoch+1}   Loss:{np.mean(losses):.4f}   Accuracy:{auc_score:.4f}  F1_Score:{f1:.4f}')\n    print('*='*50)\n    f1,acc_score = evaluate(valid_loader)\n    if best_score<acc_score and f1_best<f1:\n        best_score = acc_score\n        f1_best=f1                    \n        print('最好验证集准确率为:{} 这次的准确率为{}  F1为{} 最好F1为{}'.format(best_score,acc_score,f1,f1_best))\n        torch.save(model, 'best_acc.pkl')\n    else:\n        print('最好验证集准确率为:{} 这次的准确率为{}  F1为{} 最好F1为{}'.format(best_score,acc_score,f1,f1_best))\n    print('*=' * 50)\n\npredict(test_loader)\n# 4. 保存模型","metadata":{"execution":{"iopub.status.busy":"2021-05-23T04:43:16.650926Z","iopub.execute_input":"2021-05-23T04:43:16.651262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}