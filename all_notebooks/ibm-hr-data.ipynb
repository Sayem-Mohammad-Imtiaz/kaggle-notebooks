{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Processing\n"},{"metadata":{},"cell_type":"markdown","source":"Removing EmployeeNumber column as it's ID/ we can also set it as index"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop('EmployeeNumber',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Removing columns which have only one value**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in data.columns:\n    if data[i].nunique()==1:\n        data.drop(i,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's print details of columns which have Cataorical Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in data.columns:\n    if data[i].dtype=='O':\n        print(i)\n        print(data[i].unique())\n        print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can covert categorical data into a suitable data format( integer ) for fitting the ML model. For this, we can use the pandas ***get_dummies* **Function. I'll prefer to use ***LabelEncoder*** because it does not increase the dimensionality of data but get_dummies do increase the dimensionality of data which may lead to a complex ML model.\n* Here I have not used LabelEncoder instead i have written a code that is doing the same as LabelEncoder."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in data.columns:\n    if data[col].dtype=='O':\n        un = data[col].unique()\n        var=0\n        for i in un:\n            data[col].replace(i,var,inplace=True)\n            var+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # Identifying as well as Separating Quantitative and Qualitative data\n*  cat contains the name of column which have name of ***Qualitative(categorical)*** data"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat=['Attrition','BusinessTravel','Department','Education','EducationField','EnvironmentSatisfaction',\n     'Gender','JobInvolvement','JobLevel','JobRole','JobSatisfaction','MaritalStatus','OverTime','WorkLifeBalance',\n     'StockOptionLevel','RelationshipSatisfaction','PerformanceRating'] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_data= data[cat]\ncat_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heatmap for Qualitative Data\n1. * For checking of highly correlated columns.\n1. * we'll drop one of two highly correlated columns(90%) which will help us to reduce the dimensionality of data "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_dims = (20,15)\nfig, ax = plt.subplots(figsize=fig_dims)\n\n#mask=np.triu(np.ones_like(cat_data.corr(),dtype=bool))\n\ncmap=sns.diverging_palette(h_neg=15,h_pos=240,as_cmap=True)\nsns.heatmap(cat_data.corr(),center=0,cmap=cmap,linewidths=1,annot=True,fmt='.2f',ax=ax);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ploting countplot for Qualitative Data\nTo visualise distribution  "},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in cat_data.columns:\n    sns.countplot(cat_data[a])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing Quantitative data"},{"metadata":{"trusted":true},"cell_type":"code","source":"quan=data.columns.to_list()\nfor i in cat:\n    quan.remove(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quan_data=data[quan]\nquan_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Heatmap for Quantitative Data\n* For checking of highly correlated columns.\n* we'll drop one of two highly correlated columns(90%) which will help us to reduce the dimensionality of data "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig_dims = (20,15)\nfig, ax = plt.subplots(figsize=fig_dims)\n\n#mask=np.triu(np.ones_like(quan_data.corr(),dtype=bool))\n\ncmap=sns.diverging_palette(h_neg=15,h_pos=240,as_cmap=True)\nsns.heatmap(quan_data.corr(),center=0,cmap=cmap,linewidths=1,annot=True,fmt='.2f',ax=ax);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pairplot for Quantitative Data\n* For understanding correlation of quantitative data visually "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(quan_data);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking data Distrubution of Quantitative Data by ploting distplot and ECDF\n* we will check for normal distrubution using this plot \n* If continues data is not in the normal form we'll try to make it normal by necessary transformation\n* Only if we are getting low Accuracy/Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in quan_data.columns:\n    sns.distplot(quan_data[a])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ecdf(data):\n    n = len(data)\n    x = np.sort(data)\n    y = np.arange(1, n+1) / n\n    return x, y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in quan_data.columns:\n    x1,y1=ecdf(quan_data[a])\n    x2,y2=ecdf(np.random.normal(np.mean(quan_data[a]),np.std(quan_data[a]),size=10000))\n    plt.plot(x1,y1,marker='.',linestyle=None)\n   \n    plt.xlabel(a)\n    plt.plot(x2,y2)\n    plt.legend(['Real', 'Theory'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observation from ECDF and Distplot\n* Most of columns are discrete in nature and very few columns are continues\n* As of know we'll not be making our continues data Normal\n* If the features or columns are selected during the Feature selection with that our accuracy/score is less then we'll go for making our data normal"},{"metadata":{"trusted":true},"cell_type":"code","source":"MonthlyIncome=quan_data['MonthlyIncome']\nquan_data.drop('MonthlyIncome',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# StandardScaler\n* With StandardScaler we'll be bringing all feature on the same scale\n* Scalling data always result in better Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler().fit(quan_data)\n\nquan_data_col_name=quan_data.columns\nquan_data_col_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quan_pre_data=pd.DataFrame(ss.transform(quan_data),columns=quan_data_col_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can observe that after scaling our distribution remain the same because scaling never change the distribution**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for a in quan_pre_data.columns:\n    sns.distplot(quan_pre_data[a])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quan_pre_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MonthlyIncome.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feture Selection for reducing complexcity of our ML model\n\n* As we know Lasso, RandomForestRegressor, GradientBoostingRegressor work on by the principal of features selection or features importance. \n* We can use this information for selecting important columns for the prediction.\n\n* We have used LassoCV which Lassos regressor with the implementation ofgrid search for Hypertuning well know as L2 regularization"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bring Quantitative and Qualitative data together for Preparing the ML model**\n* and diving data into training and testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"y=MonthlyIncome\nx=pd.concat([quan_pre_data,cat_data],axis=1)\n\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=.2,random_state=12)\nX_train.iloc[:,:13]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Fitting data to L2 regularization *"},{"metadata":{"trusted":true},"cell_type":"code","source":"lcv=LassoCV().fit(X_train,y_train)\nlcv.alpha_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lcv.score(X_train,y_train))\nprint(lcv.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Masking mask of columns that have coefficients more than Zero. ie, Important features"},{"metadata":{"trusted":true},"cell_type":"code","source":"lcv_mask=lcv.coef_!=0\nlcv_mask","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trainng RandomForestRegressor, GradientBoostingRegressor for feature Selection \nFor creating a mask of important feature by both algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr=RandomForestRegressor().fit(X_train,y_train)\ngbr=GradientBoostingRegressor().fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rfr.score(X_train,y_train))\nprint(rfr.score(X_test,y_test))\nprint()\nprint(gbr.score(X_train,y_train))\nprint(gbr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting for Important feature by RandomForestRegressor "},{"metadata":{"trusted":true},"cell_type":"code","source":"importances_rf=pd.Series(rfr.feature_importances_,index=X_train.columns)\nimportances_rf_sort=importances_rf.sort_values()\nimportances_rf_sort.plot(kind='barh',figsize=(10,10));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting for Important feature by GradientBoostingRegressor "},{"metadata":{"trusted":true},"cell_type":"code","source":"importances_gbr=pd.Series(gbr.feature_importances_,index=X_train.columns)\nimportances_gbr_sort=importances_gbr.sort_values()\nimportances_gbr_sort.plot(kind='barh',figsize=(10,10),color='red');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting for Important feature by Lasso "},{"metadata":{"trusted":true},"cell_type":"code","source":"importances_lcv=pd.Series(lcv.coef_,index=X_train.columns)\nimportances_lcv_sort=importances_lcv.sort_values()\nimportances_lcv_sort.plot(kind='barh',figsize=(10,10),color='green');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Selecting top 15 feature from RandomFroestRegressor and GradientBoostingRegressor with RFE\nwe are using RFE (Recursive feature elimination) for more control on feture elimination"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_rfr=RFE(estimator=RandomForestRegressor(), n_features_to_select=15, step=3, verbose=1).fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rfe_rfr.score(X_train,y_train))\nprint(rfe_rfr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sqrt(mean_squared_error(y_test,rfe_rfr.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr_mask=rfe_rfr.support_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfe_gbr=RFE(estimator=GradientBoostingRegressor(), n_features_to_select=15, step=3, verbose=1)\nrfe_gbr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rfe_gbr.score(X_train,y_train))\nprint(rfe_gbr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr_mask=rfe_gbr.support_\nvotes=np.sum([lcv_mask,gbr_mask,rfr_mask],axis=0)\nvotes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Voting for Feature selection by all three algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"mask = votes>=2\nmask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mask_data=X_train.loc[:,mask]\nmask_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying PCA for feature extraction\nLet's apply Principal component analysis and let's try to reduce more complexcity of our data\nPCA help our model to not get overfit\nNote:- Before applying PCA make sure you data is Scaled otherwise our model will underfit"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca=PCA().fit(mask_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca=PCA().fit(mask_data)\n\nprint(pca.explained_variance_ratio_)\nprint()\nprint(print(pca.explained_variance_ratio_.cumsum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***As we can observe we need all columns to cover the full variability of our data***\nif we okay to remove few columns to make our model less complex, we can remove them but it may lead to a bit less accuracy (this is generally used to a data set which has too many columns)\nBut in our data we have very less no. of columns and we want all feature for full variblity in this case. so, we'll be not Using PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(pca.components_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting for elbow\nPlotting for elbow point which determine no. of column of PCA which can be used for maximum variance in data\nThis is genrally used for Data set which have too many columns like more then 100."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(pca.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nhere elbow point comes on 3 that means for maximum variability by least no.of feature is 3+1=4. plus 1 because of plot stats from 0 which means 0 is also considered as column similar to indexing"},{"metadata":{"trusted":true},"cell_type":"code","source":"del data,quan_data,cat_data,quan_pre_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fitting Final ML model and Hyper parameter tuning Using **RandomizedSearchCV**\nHere we'll be fitting the ML model which contains only important data. ie, mask_data(data from feature Selection)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_dist={'loss':['ls', 'lad', 'huber', 'quantile'],\n           'n_estimators':randint(100,200),\n           'max_depth':randint(1,5)\n           }\ncv=GradientBoostingRegressor()\n\nfinal_modelCV=RandomizedSearchCV(cv,param_dist,cv=10,verbose=True,n_jobs=-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_modelCV.fit(mask_data,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_modelCV.score(mask_data,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_modelCV.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_modelCV.score(mask_data,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_modelCV.score(X_test.loc[:,mask],y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sqrt(mean_squared_error(y_train,final_modelCV.predict(X_train.loc[:,mask])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}