{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Step 1: Imports and necessary variables assignment","metadata":{}},{"cell_type":"markdown","source":"* v3 : adding 0.45 rescaling to images                      => 91.8\n* v4 : added rescaling by 0.8745 ---> dataset max value     => 90.9\n* v5 : using v3 rescaling with noisy-student weights","metadata":{}},{"cell_type":"code","source":"!pip install -q efficientnet","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:19.167557Z","iopub.execute_input":"2021-07-27T04:36:19.168037Z","iopub.status.idle":"2021-07-27T04:36:29.695961Z","shell.execute_reply.started":"2021-07-27T04:36:19.167936Z","shell.execute_reply":"2021-07-27T04:36:29.694585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom matplotlib import pyplot as plt\nimport math, os, re, warnings, random\nfrom sklearn.utils import class_weight\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom tensorflow.keras import optimizers, applications, Sequential, losses, metrics\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,LearningRateScheduler","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:29.699491Z","iopub.execute_input":"2021-07-27T04:36:29.699939Z","iopub.status.idle":"2021-07-27T04:36:36.553403Z","shell.execute_reply.started":"2021-07-27T04:36:29.699875Z","shell.execute_reply":"2021-07-27T04:36:36.552239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = \"1\"\n    \n    \nseed = 0\nseed_everything(seed)\nwarnings.filterwarnings('ignore')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:36.555999Z","iopub.execute_input":"2021-07-27T04:36:36.5565Z","iopub.status.idle":"2021-07-27T04:36:36.564778Z","shell.execute_reply.started":"2021-07-27T04:36:36.556457Z","shell.execute_reply":"2021-07-27T04:36:36.563248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU@{}'.format(tpu.master()))\nexcept ValueError:\n    tpu = None\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nelse:\n    strategy = tf.distribute.get_strategy()\n    \nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync \nprint('# REPLICAS: {}'.format(REPLICAS))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:36.567488Z","iopub.execute_input":"2021-07-27T04:36:36.568018Z","iopub.status.idle":"2021-07-27T04:36:36.587001Z","shell.execute_reply.started":"2021-07-27T04:36:36.567972Z","shell.execute_reply":"2021-07-27T04:36:36.585849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32 * REPLICAS\nLEARNING_RATE = 3e-5 * REPLICAS\nEPOCHS = 30\nHEIGHT = 300\nWIDTH = 300\nCHANNELS = 3\nES_PATIENCE = 10\nAUG_BATCH=BATCH_SIZE\nIMAGE_SIZE=[HEIGHT,WIDTH]","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:36.588983Z","iopub.execute_input":"2021-07-27T04:36:36.589433Z","iopub.status.idle":"2021-07-27T04:36:36.595689Z","shell.execute_reply.started":"2021-07-27T04:36:36.589387Z","shell.execute_reply":"2021-07-27T04:36:36.59434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Merge all fold's data into one to later use Stratified-KFold","metadata":{}},{"cell_type":"code","source":"# GCS_PATH = KaggleDatasets().get_gcs_path('leukemia-classification')\nGCS_PATH = '../input/leukemia-classification/'","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:36.598131Z","iopub.execute_input":"2021-07-27T04:36:36.598714Z","iopub.status.idle":"2021-07-27T04:36:36.607423Z","shell.execute_reply.started":"2021-07-27T04:36:36.598662Z","shell.execute_reply":"2021-07-27T04:36:36.606134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_dataset_0_all = glob.glob(GCS_PATH + '/C-NMC_Leukemia/training_data/fold_0/all/*.bmp')\ntrain_dataset_0_hem = glob.glob(GCS_PATH + '/C-NMC_Leukemia/training_data/fold_0/hem/*.bmp')\ntrain_dataset_1_all = glob.glob(GCS_PATH + '/C-NMC_Leukemia/training_data/fold_1/all/*.bmp')\ntrain_dataset_1_hem = glob.glob(GCS_PATH + '/C-NMC_Leukemia/training_data/fold_1/hem/*.bmp')\ntrain_dataset_2_all = glob.glob(GCS_PATH + '/C-NMC_Leukemia/training_data/fold_2/all/*.bmp')\ntrain_dataset_2_hem = glob.glob(GCS_PATH + '/C-NMC_Leukemia/training_data/fold_2/hem/*.bmp')\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:36.609322Z","iopub.execute_input":"2021-07-27T04:36:36.609905Z","iopub.status.idle":"2021-07-27T04:36:39.12464Z","shell.execute_reply.started":"2021-07-27T04:36:36.60986Z","shell.execute_reply":"2021-07-27T04:36:39.123515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset_0_all)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:39.130017Z","iopub.execute_input":"2021-07-27T04:36:39.130339Z","iopub.status.idle":"2021-07-27T04:36:39.13973Z","shell.execute_reply.started":"2021-07-27T04:36:39.130308Z","shell.execute_reply":"2021-07-27T04:36:39.138484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Include Validation data as well :-\nvalid_data=pd.read_csv(GCS_PATH + '/C-NMC_Leukemia/validation_data/C-NMC_test_prelim_phase_data_labels.csv')\n\nav = valid_data[valid_data['labels'] == 1]\nhv = valid_data[valid_data['labels'] == 0]\n\nVAL_PATH = GCS_PATH + '/C-NMC_Leukemia/validation_data/C-NMC_test_prelim_phase_data/'\nAVL = [VAL_PATH +  i for i in list(av.new_names)]\nHVL = [VAL_PATH +  i for i in list(hv.new_names)]\n","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:39.141928Z","iopub.execute_input":"2021-07-27T04:36:39.142708Z","iopub.status.idle":"2021-07-27T04:36:39.202947Z","shell.execute_reply.started":"2021-07-27T04:36:39.142659Z","shell.execute_reply":"2021-07-27T04:36:39.201733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merging happens here:-\nA=[]\nH=[]\nA.extend(train_dataset_0_all)\nA.extend(train_dataset_1_all)\nA.extend(train_dataset_2_all)\nA.extend(AVL)\nH.extend(train_dataset_0_hem)\nH.extend(train_dataset_1_hem)\nH.extend(train_dataset_2_hem)\nH.extend(HVL)\nprint(len(A))\nprint(len(H))\n\n# Create labels :-\nLabel_A = [1]*len(A)\nLabel_H = [0]*len(H)\n\n# Converting to pandas dataframe for easier access:-\nA.extend(H)\nLabel_A.extend(Label_H)\ndf = pd.DataFrame({'path':A, 'label':Label_A})\ndf = df.sample(frac=1).reset_index(drop=True)\n\nFILENAMES = df['path']\nLABELS = df['label']\n\nprint('Final Merged Data:-')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:39.204805Z","iopub.execute_input":"2021-07-27T04:36:39.205606Z","iopub.status.idle":"2021-07-27T04:36:39.248375Z","shell.execute_reply.started":"2021-07-27T04:36:39.205536Z","shell.execute_reply":"2021-07-27T04:36:39.24711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cw = class_weight.compute_class_weight('balanced',\n                                        np.unique(LABELS),\n                                        LABELS)\ncw = {0:cw[0], 1:cw[1]}","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:39.250277Z","iopub.execute_input":"2021-07-27T04:36:39.251091Z","iopub.status.idle":"2021-07-27T04:36:39.262565Z","shell.execute_reply.started":"2021-07-27T04:36:39.25104Z","shell.execute_reply":"2021-07-27T04:36:39.261278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cw","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:39.264132Z","iopub.execute_input":"2021-07-27T04:36:39.266274Z","iopub.status.idle":"2021-07-27T04:36:39.276017Z","shell.execute_reply.started":"2021-07-27T04:36:39.266231Z","shell.execute_reply":"2021-07-27T04:36:39.274492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Create data pipeline using tf.data.Dataset","metadata":{}},{"cell_type":"code","source":"# Define Augmentation function:-\ndef data_augment(image, label):\n    \n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel = tf.random.uniform([], 0, 1.0, dtype=tf.float32)    \n    p_shear = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_flip_left_right(image)\n    \n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    if p_pixel >= .2:\n        if p_pixel >= .8:\n            image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n        elif p_pixel >= .6:\n            image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n        elif p_pixel >= .4:\n            image = tf.image.random_brightness(image, max_delta=.1)\n        else:\n            image = tf.image.adjust_gamma(image, gamma=.6)\n            \n    if p_crop > .7:\n        if p_crop > .9:\n            image = tf.image.central_crop(image, central_fraction=.6)\n        elif p_crop > .8:\n            image = tf.image.central_crop(image, central_fraction=.7)\n        else:\n            image = tf.image.central_crop(image, central_fraction=.8)\n    elif p_crop > .4:\n        crop_size = tf.random.uniform([], int(HEIGHT*.6), HEIGHT, dtype=tf.int32)\n        image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n        \n    image = tf.image.resize(image, size=[HEIGHT, WIDTH])\n    image = tf.reshape(image, [HEIGHT, WIDTH, 3])\n    \n    return image, label","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:36:39.278381Z","iopub.execute_input":"2021-07-27T04:36:39.278956Z","iopub.status.idle":"2021-07-27T04:36:39.29489Z","shell.execute_reply.started":"2021-07-27T04:36:39.278881Z","shell.execute_reply":"2021-07-27T04:36:39.293167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse_data(filename,label):\n    image = tf.io.read_file(filename)\n    image = tf.image.decode_bmp(image)\n    image = tf.image.convert_image_dtype(image, tf.float32) /  0.45 \n    image = tf.image.resize(image, IMAGE_SIZE)\n    return image, tf.one_hot(label,2)\n\ndef load_dataset(filenames, labels ,ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n\n    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(parse_data, num_parallel_calls=AUTO)\n    return dataset\n\ndef get_dataset(FILENAMES,LABELS, ordered=False, repeated=False, augment=False):\n    dataset = load_dataset(FILENAMES, LABELS, ordered=ordered)\n    if augment:\n        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:37:07.370784Z","iopub.execute_input":"2021-07-27T04:37:07.371263Z","iopub.status.idle":"2021-07-27T04:37:07.382005Z","shell.execute_reply.started":"2021-07-27T04:37:07.371232Z","shell.execute_reply":"2021-07-27T04:37:07.380537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nnp.set_printoptions(threshold=15, linewidth=80)\nCLASSES = [0,1]\n\ndef batch_to_numpy_images_and_labels(data):\n    images, labels = data\n    numpy_images = images.numpy()\n    numpy_labels = labels.numpy()\n    labels = [str(i) for i in  numpy_labels]\n\n    return numpy_images, labels\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_flower(image, title, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n    return (subplot[0], subplot[1], subplot[2]+1)\n    \ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)//rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot=(rows,cols,1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n        title = label\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        # image = cv2.imdecode(image,cv2.IMREA)\n        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    \n    \n# Model evaluation\ndef plot_metrics(history):\n    metric_list = [m for m in list(history.keys()) if m is not 'lr']\n    size = len(metric_list)//2\n    fig, axes = plt.subplots(size, 1, sharex='col', figsize=(20, size * 4))\n    if size > 1:\n        axes = axes.flatten()\n    else:\n        axes = [axes]\n    \n    for index in range(len(metric_list)//2):\n        metric_name = metric_list[index]\n        val_metric_name = metric_list[index+size]\n        axes[index].plot(history[metric_name], label='Train %s' % metric_name)\n        axes[index].plot(history[val_metric_name], label='Validation %s' % metric_name)\n        axes[index].legend(loc='best', fontsize=16)\n        axes[index].set_title(metric_name)\n        if 'loss' in metric_name:\n            axes[index].axvline(np.argmin(history[metric_name]), linestyle='dashed')\n            axes[index].axvline(np.argmin(history[val_metric_name]), linestyle='dashed', color='orange')\n        else:\n            axes[index].axvline(np.argmax(history[metric_name]), linestyle='dashed')\n            axes[index].axvline(np.argmax(history[val_metric_name]), linestyle='dashed', color='orange')\n\n    plt.xlabel('Epochs', fontsize=16)\n    sns.despine()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:37:09.792058Z","iopub.execute_input":"2021-07-27T04:37:09.792499Z","iopub.status.idle":"2021-07-27T04:37:09.818967Z","shell.execute_reply.started":"2021-07-27T04:37:09.792458Z","shell.execute_reply":"2021-07-27T04:37:09.816287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = get_dataset(FILENAMES[:60],LABELS[:60], ordered=True,augment=True)\ntrain_iter = iter(train_dataset.unbatch().batch(20))\n\ndisplay_batch_of_images(next(train_iter))\ndisplay_batch_of_images(next(train_iter))","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:37:10.359871Z","iopub.execute_input":"2021-07-27T04:37:10.360264Z","iopub.status.idle":"2021-07-27T04:37:18.475088Z","shell.execute_reply.started":"2021-07-27T04:37:10.360231Z","shell.execute_reply":"2021-07-27T04:37:18.473648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Start training preprations","metadata":{}},{"cell_type":"code","source":"# Cosine Annealing:-\nLR_START = 1e-8\nLR_MIN = 1e-8\nLR_MAX = LEARNING_RATE\nLR_RAMPUP_EPOCHS = 3\nLR_SUSTAIN_EPOCHS = 0\nN_CYCLES = .5\n\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        progress = (epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) / (EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)\n        lr = LR_MAX * (0.5 * (1.0 + tf.math.cos(math.pi * N_CYCLES * 2.0 * progress)))\n        if LR_MIN is not None:\n            lr = tf.math.maximum(LR_MIN, lr)\n            \n    return lr\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\n\nsns.set(style='whitegrid')\nfig, ax = plt.subplots(figsize=(20, 6))\nplt.plot(rng, y)\n\n# print(f'{EPOCHS} total epochs and {NUM_TRAINING_IMAGES//BATCH_SIZE} steps per epoch')\nprint(f'Learning rate schedule: {y[0]:.3g} to {max(y):.3g} to {y[-1]:.3g}')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:37:28.816073Z","iopub.execute_input":"2021-07-27T04:37:28.816476Z","iopub.status.idle":"2021-07-27T04:37:29.441975Z","shell.execute_reply.started":"2021-07-27T04:37:28.816444Z","shell.execute_reply":"2021-07-27T04:37:29.440836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow_addons as tfa","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:37:32.954377Z","iopub.execute_input":"2021-07-27T04:37:32.954781Z","iopub.status.idle":"2021-07-27T04:37:33.102073Z","shell.execute_reply.started":"2021-07-27T04:37:32.954749Z","shell.execute_reply":"2021-07-27T04:37:33.100978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Architecture :-\ndef model_fn(input_shape, N_CLASSES):\n    input_image = L.Input(shape=input_shape, name='input_image')\n    base_model = efn.EfficientNetB3(input_tensor=input_image, \n                                    include_top=False, \n                                    weights='noisy-student', \n                                    pooling='avg')\n\n    for layer in base_model.layers:\n        if 'bn' in layer.name:\n            layer.trainable = False\n        else:\n            layer.trainable = True\n            \n    model = tf.keras.Sequential([\n        base_model,\n        L.Dropout(.25),\n        L.Dense(N_CLASSES, activation='sigmoid', name='output')\n    ])\n\n    optimizer = optimizers.Adam(lr=LEARNING_RATE)\n    model.compile(optimizer=optimizer, \n                  loss='binary_crossentropy', \n                  metrics=['accuracy',tfa.metrics.F1Score(num_classes=2, average='weighted')])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:38:15.038703Z","iopub.execute_input":"2021-07-27T04:38:15.03905Z","iopub.status.idle":"2021-07-27T04:38:15.049446Z","shell.execute_reply.started":"2021-07-27T04:38:15.03902Z","shell.execute_reply":"2021-07-27T04:38:15.048033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross Validated training loop:-\nskf = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\noof_pred = []; oof_labels = []; history_list = []\ncv1 = cv2 = 0\n\n\nfor fold,(idxT, idxV) in enumerate(skf.split(FILENAMES,LABELS)):\n    print(f'\\nFOLD: {fold+1}')\n    print(f'TRAIN: {len(idxT)} VALID: {len(idxV)}')\n    STEPS_PER_EPOCH = len(idxT) // BATCH_SIZE\n    \n    \n    K.clear_session()\n    with strategy.scope():\n        model = model_fn((None, None, CHANNELS), 2)\n        \n    model_path = f'model_{fold}.h5'\n    es = EarlyStopping(monitor='val_f1_score', mode='max', \n                    patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n\n    ## TRAIN\n    history = model.fit(x=get_dataset(FILENAMES[idxT],LABELS[idxT], ordered=False, repeated=True, augment=True), \n                        validation_data=get_dataset(FILENAMES[idxV],LABELS[idxV] , ordered=True, repeated=False, augment=False), \n                        steps_per_epoch=STEPS_PER_EPOCH, \n                        callbacks=[es, LearningRateScheduler(lrfn ,verbose=0)], \n                        epochs=EPOCHS,  \n                        verbose=1,\n                        class_weight = cw).history\n      \n    model.save_weights(model_path)\n    history_list.append(history)\n    # Save last model weights\n\n    \n    ## RESULTS\n    print(f\"#### FOLD {fold+1} OOF Accuracy = {np.max(history['val_accuracy']):.3f}\")\n    cv1 += np.max(history['val_accuracy'])\n    cv2 += np.max(history['val_f1_score'])\n    del model\n    \nprint(f'### Avg. Accuracy = {cv1/3.0} \\n ### Avg. Weighted F1 = {cv2/3.0}')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T04:38:17.882688Z","iopub.execute_input":"2021-07-27T04:38:17.883033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot metrics:-\nfor fold, history in enumerate(history_list):\n    print(f'\\nFOLD: {fold+1}')\n    plot_metrics(history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}