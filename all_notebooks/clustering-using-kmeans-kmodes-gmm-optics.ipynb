{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <a id=1>Clustering for Click-Through-Rate using KMeans, Gaussian Mixture and Text Processing</a>\n\nThe goal of this kernel is to cluster the given sample of Ad-Topics with KMeans and GMM with and without text processing and verify the accuracy of models with various metrics and/or visual methods.<br>\nThe data consists of 10 features, *viz.*, 'Daily Time Spent on Site', 'Age', 'Area Income', 'Daily Internet Usage', 'Ad Topic Line', 'City', 'Male', 'Country', Timestamp' and 'Clicked on Ad'. The meta-data of the features is as below:-<br>\n - **Daily Time Spent on a Site**: \tTotal time spent by the user on a site in minutes.\n - **Age**: User's age\n - **Area Income**: Average income of geographical area of user.\n - **Daily Internet Usage**: \tAvgerage minutes in a day user is on the internet.\n - **Ad Topic Line**: Banner topic line of the advertisement.\n - **City**: \tCity of the user.\n - **Male**: Gender of user (male=1,female=0)\n - **Country**: Country of the user.\n - **Timestamp**: \tTime at which user clicked on an Ad or closed window otherwise\n - **Clicked on Ad**: ***TARGET COLUMN***  This is a binary feature: 0 refers to the case where a user didn't click the advertisement, while 1 refers to the case when the advertisement is clicked.\n This kernel is devided into following sections:-\n \n* [Introduction](#1)\n* [Importing Required Packages](#2)\n* [Import dataset and get knowabouts](#3)\n* [Data Visualization](#4)\n* [Pre-processing](#5)\n* [Clustering](#6)\n  *  [Using KMeans](#6.1)\n  *  [Using KModes](#6.2)\n  *  [Using Gaussian Mixture Model](#6.3)\n* [Evaluation of clusters](#7)\n* [Re-cluster after text-processing](#8)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <a id= 2> Importing Required Packages</a>\nHere the imported packages are divided as per their usage. \n * Numpy, Pandas and Seaborn: For data handling and visualisation\n * re: RegEx package for text manipulation\n * sklearn packages: For pre-processing, Clustering and validation\n * ntlk.stem: For stemming the text in ad topics to avoid duplicated features in vectorizer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.mixture import GaussianMixture as GMM\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.metrics import adjusted_rand_score,calinski_harabasz_score\nfrom sklearn.metrics import davies_bouldin_score,completeness_score,homogeneity_score,v_measure_score\nfrom nltk.stem import WordNetLemmatizer\n\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom IPython.core.display import display, HTML\npd.options.display.max_rows = 500\nInteractiveShell.ast_node_interactivity = \"all\"\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id = \"3\">Import dataset and get knowabouts</a>\nThis section is all about getting data and understanding its structure. Here we try to find skew of data, null values and their occurance with respect to other features and statistical standing of data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/advertising/advertising.csv',parse_dates = ['Timestamp'])\ndisplay(HTML('<h2 id = \"inf\">Basic Information about data like data-type and count</h2>'))\ndf.info()\ndisplay(HTML('<h2>Statistical summary of data</h2>'))\n\ndf.describe()\ndisplay(HTML('<h2>Random Sample size of 5 observations</h2>'))\n\ndf.sample(5).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As in [basic information of data](#inf) we can see all features have 1000 values-it means there are no null values. But still we are to some superficial probe how many countries and cities are there or what time span is covered in dataset.","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"display(HTML('<h2 id=\"uni\">Unique Values in dataset</h2>'))\ndf.nunique()\ndisplay(HTML('<b> Besides these it has <i>'+str(df['Timestamp'].dt.year.nunique())+'</i> year values and <i>'\n             +str(df['Timestamp'].dt.month.nunique())+'</i> month values in Timestamp feature</b>'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For ease of operation let us remove spaces in feature names.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f_dct = {n : re.sub('[^A-Za-z0-9]+','_',n) for n in df.columns.values}\ndf.rename(columns = f_dct,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=4>Data Visualization</a>\nNow let us go a little deeper in data. To understand the data better we try to plot some graphs to identify their inter-relation and distribution. First, we try to identify the presence of outliers by box and whiskers plot on daily time spent, Age and internet usage. Other columns are not included being categorical. The area income column is also left as its scale will hugely affect the graph","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"p=df[['Daily_Time_Spent_on_Site', 'Age', 'Daily_Internet_Usage']].boxplot(figsize = (10,8),grid=True,fontsize=10)\nplt.suptitle('Box plot for features',fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While checking for [unique values](#uni) we found out that 237 countries and 966 cities are out there in dataset. Let's check how these are affecting click-through.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df.Country,df.Clicked_on_Ad).sort_values(1,ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(df.City,'count').sort_values('count',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we check relationship amongst features with pairplot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\np = sns.pairplot(df, hue ='Clicked_on_Ad',\n    vars=['Daily_Time_Spent_on_Site', 'Age', 'Area_Income', 'Daily_Internet_Usage']\n                 ,diag_kind='kde',   palette='bright')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=5>Pre-processing</a>\nUnder pre-processing, we try to get the data ready for modelling. During this data is cleansed, redundant/useless features are removed, new features are created as per requirement and such more jobs to do.\n<p> Here we do not have any NaN thus no need for imputation. Further, we have 2 text features which will not be useful and are required to be dropped. Also, Timestamp feature is itself meaningless but its part like dayofweek,dayofmonth,month, year etc are useful and hence are required to be generated. So, let's start with feature creation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['hour']=df['Timestamp'].dt.hour\ndf['day'] = df['Timestamp'].dt.day\ndf['month'] = df['Timestamp'].dt.month\ndf['weekday'] = df['Timestamp'].dt.weekday","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(HTML(\"<h3>Dropping unusable features</h3> We are dropping here ['Timestamp','City','Country']\"))\ndf.drop(columns=['Timestamp','City','Country'],inplace=True)\ndisplay(HTML('<b> Now shape of dataset is '+str(df.shape)+'</b>'))\ndisplay(HTML('<h3> New sample of data</h3>'))\ndf.sample(n=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling\nNow to avoid effect of different scales in different feature we will now scale the data(leaving categorical and text columns). Since we don't have outliers here, we can use StandardScaler. The standard scaler will first calculate mean $\\mu$ and standard deviation $\\sigma$ of each feature and then replace each value with it's z-score which is defined as<p><font size = 12>\n    $z = \\frac{x-\\mu}{\\sigma}$</font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[ ['Daily_Time_Spent_on_Site', 'Age', 'Area_Income', 'Daily_Internet_Usage', 'Male','weekday','day','hour','month']]\nY = df[['Clicked_on_Ad']].to_numpy().ravel()\nX_scaled = StandardScaler().fit_transform(X.copy())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our dataset is ready for clustering. we can pass on the scaled ndarray **X_scaled** for further process.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# <a id =6>Clustering</a>\nHere we will first try to use given five features ['Daily_Time_Spent_on_Site', 'Age', 'Area_Income', 'Daily_Internet_Usage', 'Male'] for clustering and review it with predetermined result **Y**.\n<p>There are various clustering models but we are taking a few as KMeans,Gaussian Mixture Model and KModes.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"6.1\">K-Means Clustering</a>\nIn K-Means, we try to divide the data in pre-determined number of clusters and check the \nbehavious of clusters later-on. It by default will use euclidian distance and tries to minimise the same by moving **means** of clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=2) #K-Means model\ncluster_km = km.fit_predict(X_scaled) #fitting means it tries to understand the data and predict will give cluster lables","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"6.2\">K-Modes Clustering</a>\nIt is similar to K-Means but here defining metric is mode instead of mean. Here, we try to divide the data in pre-determined number of clusters and check the \nbehavious of clusters later-on. It by default will use euclidian distance and tries to minimise the same by moving **modes** of clusters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from kmodes.kmodes import KModes\nkm1 = KModes(n_clusters=2,init='Cao')\ncluster_km1 = km1.fit_predict(X_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"6.3\">Gaussian Mixture Model</a>\n\nGaussian Mixture Models (GMMs) are based on Gaussian Distributions and are flexible building blocks for other machine learning algorithms. They are great approximations for general probability distributions but also because they remain somewhat interpretable even when the dataset gets very complex. Mixture Models do not require to know about data and the subpopulation to which it belongs but learn about the same later on by finding the distribution(s) for its each feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gmm = GMM(n_components=2, covariance_type='full', max_iter=100, n_init=10)\ncluster_gmm = gmm.fit_predict(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <a id=\"6.4\">OPTICS Model</a>\nOPTICS (Ordering Points To Identify the Clustering Structure) finds core sample of high density and expands clusters from them. It keeps cluster hierarchy for a variable neighborhood radius.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import OPTICS\noptics = OPTICS(min_samples=2)\ncluster_optics = optics.fit_predict(X_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=\"7\">Evaluation of clustering</a>\nThe evaluation of clusters can be in two ways:-\n - Using Indices/metrics\n    - **Internal Metrics** - Unsupervised clustering; where ground truth is unavailable.\n    - **External Metrics** - Supervised clustering; where ground truth is available.\n - Using visual methods\n\nIn this kernel I have used **Davies Bouldin Score** and **Calinski Harabasz Score** for internal metrics and **ARI, Completeness score, homogeneity score and V-measure** for external metric.\n\nFor visual evaluation, I have used TSNE to check the performance of clustering.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [km,km1,gmm,optics]\nclst = [cluster_km,cluster_km1,cluster_gmm,cluster_optics]\nt ='<table border=1 color = \"#000000\"><tr><th>model</th><th>ARI</th><th>calinski_harabasz_score</th><th>davies_bouldin_score</th>'\nt+='<th>completeness_score</th><th> homogeneity_score </th><th>v_measure_score</th>'\nfor i in range(4):\n    t = t+('<tr><td>'+str(models[i])+'</td>'+'<td>'+str(adjusted_rand_score(Y,clst[i]))+'</td>')\n    t = t+('<td>'+str(calinski_harabasz_score(X_scaled,clst[i]))+'</td>')\n    t = t+('<td>'+str(davies_bouldin_score(X_scaled,clst[i]))+'</td>')\n    t = t+('<td>'+str(completeness_score(Y,clst[i]))+'</td>')\n    t = t+('<td>'+str(homogeneity_score(Y,clst[i]))+'</td>')\n    t = t+('<td>'+str(v_measure_score(Y,clst[i]))+'</td></tr>')\nt+='</table>'    \ndisplay(HTML(t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfor i in range(4):\n    display(HTML('<h4>'+str(models[i])+'</h4>'))\n    print(classification_report(Y,clst[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne = TSNE(n_components = 2)\ntsne_out = tsne.fit_transform(X_scaled)\nfig, axs = plt.subplots(2,2, figsize=(15, 15))\nplt.suptitle('TSNE Visualisation for different cluster models',fontsize=15)\nfor i in range(4):\n    p = axs[i//2][i%2].scatter(tsne_out[:, 0], tsne_out[:, 1],marker=10,s=10,linewidths=5,c=clst[i])\n    axs[i//2][i%2].set_title(models[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id=8>Re-cluster after Text Processing</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<font size =5>$tf_i,_j = \\frac{f_j(i)}{max f_i}$\n<br>\n$w_i,_j  =  tf_i,_j \\times log_2(\\frac{N}{df_i})$\n    </font>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#from nltk.stem import WordNetLemmatizer\n#import nltk\n#nltk.download('wordnet')\ntopics = []\nstemmer = WordNetLemmatizer()\nfor i in range(X.shape[0]):\n    topic = re.sub(r'\\W',' ',df.Ad_Topic_Line[i])\n    topic = re.sub(r'\\s+[a-zA-Z]\\s+', ' ',topic)\n    \n    # remove all single characters\n    topic = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', topic)\n    \n    # Remove single characters from the start\n    topic = re.sub(r'\\^[a-zA-Z]\\s+', ' ', topic) \n    \n    # Substituting multiple spaces with single space\n    topic = re.sub(r'\\s+', ' ', topic, flags=re.I)\n    \n    # Removing prefixed 'b'\n    topic = re.sub(r'^b\\s+', '', topic)\n    \n    # Converting to Lowercase\n    topic = topic.lower()\n    \n    # Lemmatization\n    topic = topic.split()\n\n    topic = [stemmer.lemmatize(word) for word in topic]\n    topic = ' '.join(topic)\n    \n    topics.append(topic)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidfconverter = TfidfVectorizer( max_features=500,min_df=3 ,max_df=0.8,stop_words='english') #\nX = tfidfconverter.fit_transform(topics)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"X.toarray()\nlen(tfidfconverter.get_feature_names())\ntfidfconverter.get_feature_names()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.DataFrame(X.toarray(),columns=tfidfconverter.get_feature_names())\ndf1 = pd.concat([df,df1],axis='columns')\ndf1.drop(columns=['Ad_Topic_Line','Clicked_on_Ad'],inplace=True)\nX1 = StandardScaler().fit_transform(df1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clst_txt = [m.fit_predict(X1) for m in models]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t ='<table border=1 color = \"#000000\"><tr><th>model</th><th>ARI</th><th>calinski_harabasz_score</th><th>davies_bouldin_score</th>'\nt+='<th>completeness_score</th><th> homogeneity_score </th><th>v_measure_score</th>'\nfor i in range(4):\n    t = t+('<tr><td>'+str(models[i])+'</td>'+'<td>'+str(adjusted_rand_score(Y,clst[i]))+'</td>')\n    t = t+('<td>'+str(calinski_harabasz_score(X_scaled,clst[i]))+'</td>')\n    t = t+('<td>'+str(davies_bouldin_score(X_scaled,clst[i]))+'</td>')\n    t = t+('<td>'+str(completeness_score(Y,clst[i]))+'</td>')\n    t = t+('<td>'+str(homogeneity_score(Y,clst[i]))+'</td>')\n    t = t+('<td>'+str(v_measure_score(Y,clst[i]))+'</td></tr>')\nt+='</table>'    \ndisplay(HTML(t))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(4):\n    print(models[i])\n    print(classification_report(Y,clst_txt[i]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tsne_out = tsne.fit_transform(X1)\nfig, axs = plt.subplots(2,2, figsize=(15, 15))\nplt.suptitle('TSNE Visualisation for different cluster models',fontsize=15)\nfor i in range(4):\n    p = axs[i//2][i%2].scatter(tsne_out[:, 0], tsne_out[:, 1],marker=10,s=10,linewidths=5,c=clst_txt[i])\n    axs[i//2][i%2].set_title(models[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}