{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"bbb53629-d2a7-ffc6-8e0c-9f3cf2f9c497"},"source":" 1. In this problem we have to use 30 different columns and we have to predict the Stage of Breast Cancer M (Malignant)  and B (Bengin)\n 2. This analysis has been done using Basic Machine Learning Algorithm with detailed explanation\n 3. This is good for beginners like as me Lets start.\n \n4.Attribute Information:\n\n1) ID number\n\n2) Diagnosis (M = malignant, B = benign)\n\n-3-32.Ten real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\n\nb) texture (standard deviation of gray-scale values)\n\nc) perimeter\n\nd) area\n\ne) smoothness (local variation in radius lengths)\n\nf) compactness (perimeter^2 / area - 1.0)\n\ng). concavity (severity of concave portions of the contour)\n\nh). concave points (number of concave portions of the contour)\n\ni). symmetry\n\nj). fractal dimension (\"coastline approximation\" - 1)\n\n5  here 3- 32 are divided into three parts first is Mean (3-13),  Stranded Error(13-23) and  Worst(23-32) and each contain 10 parameter (radius, texture,area, perimeter, smoothness,compactness,concavity,concave points,symmetry and fractal dimension) \n\n 6. Here Mean means the means of the all cells,  standard Error of all cell and worst means the worst  cell "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3200e6e-62d4-1d74-496b-c5197a574dcb"},"outputs":[],"source":"# here we will import the libraries used for machine learning\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph. I like it most for plot\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.cross_validation import KFold # use for cross validation\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn import svm # for Support Vector Machine\nfrom sklearn import metrics # for the check the error and accuracy of the model\n# Any results you write to the current directory are saved as output.\n# dont worry about the error if its not working then insteda of model_selection we can use cross_validation"},{"cell_type":"markdown","metadata":{"_cell_guid":"fa78428b-ecbb-56a4-2904-f90d725281ec"},"source":"**Import data **"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7036d299-0057-db0a-d892-fe212c0d612c"},"outputs":[],"source":"data = pd.read_csv(\"../input/data.csv\",header=0)# here header 0 means the 0 th row is our coloumn \n                                                # header in data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3521dc85-be7e-a735-0cc1-b3437c16ab22"},"outputs":[],"source":"# have a look at the data\nprint(data.head(2))# as u can see our data have imported and having 33 columns\n# head is used for to see top 5 by default i used 2 so it will print 2 rows\n#rows If we will use\n#print(data.tail(2))# it will print last 2 rows in data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1109ea03-5b73-25fa-72ca-6695de4b0c5b"},"outputs":[],"source":"# now lets look at the type of data we have. We can use \ndata.info()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9041739c-1d90-970b-55ec-224ed821898a"},"source":"*As I said I m beginner so here I am explaining every thing in detail \n So lets decribe what these data type means\n e.g 5 radius_mean 569 non-null float64 here it means the radius_mean have 569 float type value\nnow u can see Unnamed:32 have 0 non null object it means the all values are null in this column so we cannot use this column for our analysis*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9ec57be7-c20b-5895-2f20-0f32651b3f79"},"outputs":[],"source":"# now we can drop this column Unnamed: 32\ndata.drop(\"Unnamed: 32\",axis=1,inplace=True) # in this process this will change in iour data itself \n# if you want to save your old data then you can use below code\n# data1=data.drop(\"Unnamed:32\",axis=1)\n# here axis 1 means we are droping the column\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a93a7754-8b38-6ca4-cc7d-e31cfd58afa6"},"outputs":[],"source":"# here you can check the column has been droped\ndata.columns # this gives the column name which are persent in our data no Unnamed: 32 is now there"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0da592b-04ff-9c69-b5b0-46e87ebc391d"},"outputs":[],"source":"# like this we also donot want the Id columfor our analysis\ndata.drop(\"id\",axis=1,inplace=True)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68f07b98-c05a-f3e6-7dbe-8e39ab7d13f6"},"outputs":[],"source":"# As I said above the data can be divided into three parts.lets divied the features according to their category\nfeatures_mean= list(data.columns[1:11])\nfeatures_se= list(data.columns[11:20])\nfeatures_worst=list(data.columns[21:31])\nprint(features_mean)\nprint(\"-----------------------------------\")\nprint(features_se)\nprint(\"------------------------------------\")\nprint(features_worst)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dbdd0bf6-da03-c390-0e7a-f6b181f8e127"},"outputs":[],"source":"# lets now start with features_mean \n# now as u know our diagnosis column is a object type so we can map it to integer value\ndata['diagnosis']=data['diagnosis'].map({'M':1,'B':0})"},{"cell_type":"markdown","metadata":{"_cell_guid":"19034b94-8ddf-ef3f-675d-c87b6a558c1b"},"source":"## Explore the Data now\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09c22a48-dbc2-f1c3-517a-491564ab161e"},"outputs":[],"source":"data.describe() # this will describe the all statistical function of our data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"598dce8d-1245-14e5-0e1c-ab609e51fec6"},"outputs":[],"source":"# lets get the frequency of cancer stages\nsns.countplot(data['diagnosis'],label=\"Count\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"38041fef-a314-d3b5-8594-3234b492d8d1"},"outputs":[],"source":"# from this graph u can see that there is a more number of bengin stage of cancer which can be cure\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"5b9a0c99-86ca-9b5e-687d-8da91bcf9ff5"},"source":"## Data Analysis a little feature selection"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"592f4ece-d0b1-cccf-972e-3b7feace219b"},"outputs":[],"source":"# now lets draw a correlation graph so that we can remove multi colinearity it means the columns are\n# dependenig on each other so we should avoid it because what is the use of using same column twice\n# lets check the correlation between features\n# now we will do this analysis only for features_mean then we will do for others and will see who is doing best\ncorr = data[features_worst].corr() # .corr is used for find corelation\nplt.figure(figsize=(14,14))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           xticklabels= features_mean, yticklabels= features_mean,\n           cmap= 'coolwarm') # for more on heatmap you can visit Link(http://seaborn.pydata.org/generated/seaborn.heatmap.html)"},{"cell_type":"markdown","metadata":{"_cell_guid":"b79fcdb1-8104-291f-64a1-2b5faa145f0d"},"source":"*observation*\n* the radius, parameter and area  are highly correlated as expected from their relation*\n* so from these we will use anyone of them *\n*compactness_mean, concavity_mean and concavepoint_mean are highly correlated so we will use compactness_mean from here *\n* so selected Parameter for use is perimeter_mean, texture_mean, compactness_mean, symmetry_mean*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f9ee0ccd-4364-6186-e844-87ed319c1519"},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fcd92fe8-fb89-d377-f135-c053e562e820"},"outputs":[],"source":"prediction_var = ['texture_mean','perimeter_mean','smoothness_mean','compactness_mean','symmetry_mean']\n# now these are the variables which will use for prediction"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da2eaf5c-6f86-b320-a591-fb7b7e0fb12c"},"outputs":[],"source":"#now split our data into train and test\ntrain, test = train_test_split(data, test_size = 0.3)# in this our main data is splitted into train and tset\n# we can cjheck there shape\nprint(train.shape)\nprint(test.shape)\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27cb4563-3191-8ab9-8ce2-224c2db223d0"},"outputs":[],"source":"train_X = train[prediction_var]# taking the training data input only for prediction model\nprint(train_X.shape) # to confirm the shape\ntrain_y=train.diagnosis# this is our output variable\nprint(train_y.shape)\n# same we have to do for test\ntest_X= test[prediction_var] # this is data for test and these are the input values\ntest_y =test.diagnosis   #this is the test ouput value which we will predict usin our model and test input values\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4936a3a9-e7fd-81a1-1aec-a267224569d5"},"outputs":[],"source":"model=RandomForestClassifier(n_estimators=100)# a simple random forest model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c20ee887-14c4-fd6c-98fe-d54c06d6d4e4"},"outputs":[],"source":"model.fit(train_X,train_y)# now fit our model for traiing data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"abb767bd-ed3e-6af0-9aa5-55ccbf50f9c7"},"outputs":[],"source":"prediction=model.predict(test_X)# predict for the test data\n# prediction will contain the predicted value by our model dor test input "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f37e4a4e-214c-9f56-391b-19a9a6b141ec"},"outputs":[],"source":"metrics.accuracy_score(prediction,test_y) # to check the accuracy\n# here we will use accuracy measurement between our predicted value and our test output values"},{"cell_type":"markdown","metadata":{"_cell_guid":"b8ae477d-f309-03b1-e224-f993ee9fed5b"},"source":"* Here the Accuracy for our model is 91 % which seems good*"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1340630-a7e2-2f63-5b53-0fb407f5e177"},"outputs":[],"source":"# lets now try with SVM"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74acfdf3-a6c1-e262-eaf5-742c62bf1382"},"outputs":[],"source":"model = svm.SVC()\nmodel.fit(train_X,train_y)\nprediction=model.predict(test_X)\nmetrics.accuracy_score(prediction,test_y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"3a2a515c-5b1f-4086-48cb-96dbaa67815c"},"source":"**SVM is giving only 0.85 which we can improve by using different techniques** \n**i will improve it till then beginners can understand how to model a data and they can have a overview of ML**"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}