{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importing Libraries & Functions**","metadata":{}},{"cell_type":"code","source":"import nltk\nimport pandas as pd\nimport re\nimport string \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport warnings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Loading the datasets**","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\") # To ignore warnings\n\n#reading train and test csv files\ntrain=pd.read_csv(r'/kaggle/input/nlp-getting-started/train.csv')\ntest=pd.read_csv(r'/kaggle/input/nlp-getting-started/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Visualization**","metadata":{}},{"cell_type":"code","source":"sns.set_theme(style=\"darkgrid\")\nplot=sns.countplot(x='target', data=train)\nplot.set_title(\"Disaster tweets - 1, Non-disaster tweets - 0\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing words from disaster tweets\n\n#Removing stop words and some non relevant words\neliminated_words_d=nltk.corpus.stopwords.words('english') + ['http', 'co','https','new','like','via', 'U','amp']\nplt.figure(figsize = (15,8))\nwc = WordCloud(max_words = 600 ,\n               width = 500 , \n               height = 300, \n               stopwords = eliminated_words_d).generate(\" \".join(train[train.target == 1].text))\n\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualizing words from non-disaster tweets\nelminated_words_nd=nltk.corpus.stopwords.words('english') + ['http', 'co','https']\nplt.figure(figsize = (15,8))\nwc = WordCloud(max_words = 600 ,\n               width = 400 , \n               height = 250, \n               stopwords = elminated_words_nd).generate(\" \".join(train[train.target == 0].text))\n\nplt.imshow(wc , interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Exploratory Data Analysis**","metadata":{}},{"cell_type":"markdown","source":"> **Missing Value Check**","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ignoring keyword and location features as they contain missing values**","metadata":{}},{"cell_type":"markdown","source":"# **Pre-processing**","metadata":{}},{"cell_type":"markdown","source":"# * **Cleaning the text**","metadata":{}},{"cell_type":"code","source":"#Initializing Lemmatizer here.\nwn=nltk.WordNetLemmatizer()\n\nstopwords=nltk.corpus.stopwords.words('english')\ndef clean_text(text):\n    # Changing the word to lowercase and removing the punctuation from the text\n    text=\"\".join(word.lower() for word in text if word not in string.punctuation)\n     \n    text = re.sub('http','',text)\n    #Ignoring special characters and tokenzing\n    text=re.findall('\\w+', text)\n    \n    #Removing the stopwords and lemmatizing the word i.e.changing the word into its meaningful base form\n    text=[wn.lemmatize(word) for word in text if word not in stopwords]\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# * **Setting the values**","metadata":{}},{"cell_type":"code","source":"# Ignoring keyword and location and taking only id and text in X_train and X_test\n#y_train contains the target field\nX_train=train[['id','text']]\ny_train=train['target']\nX_test=test[['id','text']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y_train.shape)\nprint(X_train.shape)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# * **Intializing Vectorizer**","metadata":{}},{"cell_type":"code","source":"#Here, TfidfVectorizer is used, it coverts text to numeric data on the basis of how important\n#thatword is in that text\n\nvectorizer=TfidfVectorizer(analyzer=clean_text) #clean text function is used as an analyzer.\ntfidf_vect_fit=vectorizer.fit(X_train['text']) #fitting the vectorizer on text of training data\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_train = tfidf_vect_fit.transform(X_train['text']) #transforming the train vectorizer\ntfidf_test = tfidf_vect_fit.transform(X_test['text'])  #transforming the text vectorizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Concatenating id and vectorized output of text for both train and test data\nX_train_vect = pd.concat([X_train['id'].reset_index(drop=True), \n           pd.DataFrame(tfidf_train.toarray())], axis=1)\nX_test_vect = pd.concat([X_test['id'].reset_index(drop=True), \n           pd.DataFrame(tfidf_test.toarray())], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Modelling**","metadata":{}},{"cell_type":"code","source":"#Initializing Random forest classifier\nrf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n\n#Fitting the model with training data\nrf_model = rf.fit(X_train_vect, y_train)\n\n#Predicting the target variable for test data\ny_pred = rf_model.predict(X_test_vect)\ny_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using Naive bayes model\ngnb = GaussianNB()\ngnb.fit(X_train_vect, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_nb=gnb.predict(X_test_vect)\ny_pred_nb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using Gradient boosting model\ngb = GradientBoostingClassifier(n_estimators=150, max_depth=11)\n\n#fitting the training set\ngb_model = gb.fit(X_train_vect, y_train)\n\n#Predicting the text to be related to disaster or not\ny_pred_gb = gb_model.predict(X_test_vect)\ny_pred_nb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Save the data - Id and predicted target in a csv file for submission.\nThe model Random Forest Classifier gave higher score as compared to Naive Bayes and Gradient Boosting Classifier**","metadata":{}},{"cell_type":"markdown","source":"************************************END******************************************************","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}