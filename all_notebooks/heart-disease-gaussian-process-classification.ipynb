{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os,warnings;warnings.filterwarnings(\"ignore\")\nimport numpy as np;import pandas as pd;import matplotlib.pyplot as plt\nimport seaborn as sns;sns.set(style='whitegrid')\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction\n***\n\n## 1.1. Problem End Goal\n***\n\nThe goal of the problem is to predict the target variable, called `condition`. It has only two unique associated values, so let's treat this a `classification problem`\n\n## 1.2. What is covered?\n***\n- A `Classification` problem, using the `UCI Dataset`. \n- A simple `Gaussian Process Regressor` turned `classifier` is used for our model; `sklearn` compatible class is that incorporates a simple ensemble weighting for the `posterior` prediction.\n- Influence of uneven weight allocation for `categorical` features. The dataset already contains a predefined `categorical` conversion into `numerical` subset. A pooly allocated weighting for categorical features can negatively affect the model performance.\n\n## 1.3. Problem Feature Description\n***\n\n**Feature List:**\n`age`\n`sex`\n`cp`\n`trestbps`\n`chol`\n`fbs`\n`restecg`\n`thalach`\n`exang`\n`oldpeak`\n`slope`\n`ca`\n`thal`\n`condition` <br>\n**Feature Types:** All features are `numerical features`, however some of them are converted `categorical features`, we'll know which is which when looking into EDA.\n\n<div class=\"alert alert-block alert-info\">\n<b>1. age </b> | Number of years a person has lived <br>\n<b>2. sex |</b> Gender of patient (Male:1/Female:0)  <br>\n<b>3. cp | </b> Chest Pain type (4 values) <br>\n<b>4. trestbps |</b> Resting Blood Pressure <br>\n<b>5. chol |</b> serum cholestoral in mg/dl <br>\n<b>6. fbs |</b> Fasting Blood Sugar > 120 mg/dl <br>\n<b>7. restecg |</b> Resting Electrocardiographic (ECG) results (values 0,1,2) <br>\n<b>8. thalach |</b> Maximum Heart Rate Achieved <br>\n<b>9. exang |</b> Exercise Induced Angina <br>\n<b>10. oldpeak |</b> oldpeak = ST depression induced by exercise relative to rest <br>\n<b>11. slope |</b> the slope of the peak exercise ST segment <br>\n<b>12. ca |</b> number of major vessels (0-3) colored by flourosopy <br>\n<b>13. thal |</b> Thalium stress test results: 3 = normal; 6 = fixed defect; 7 = reversable defect \n<br><br>\n<b>Target Variable</b>, from [original dataset](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)<br>\ncondition: diagnosis of heart disease (angiographic disease status)<br>\n<b>Value 0:</b> < 50% diameter narrowing (negative for disease) <br>\n<b>Value 1:</b> > 50% diameter narrowing (positive for disease)\n</div><br>\n\n## 1.4. Indepth Feature Description Space\n***\n\nSome interesting excerts are placed in this section\n\n**ECG Related Features** : `restecg`,`oldpeak`,`slope`\n<div class=\"alert alert-block alert-info\">\n    \n**restecg**\n> Resting electrocardiography (ECG) is a non-invasive test that can detect abnormalities including arrhythmias, evidence of coronary heart disease, left ventricular hypertrophy and bundle branch blocks. [Reference](https://www.ncbi.nlm.nih.gov/books/NBK367910/)\n\n`0: normal`,`1: having ST-T wave abnormality`,`2:showing probable or definite left ventricular hypertrophy by Estes' criteria`</div><br>"},{"metadata":{},"cell_type":"markdown","source":"**Blood Related Features** : `trestbps`,`thalach`,`fbs`,`chol`\n<div class=\"alert alert-block alert-info\">\n    \n**trestbps: Resting Blood Pressure**\n> Stress on the blood vessels makes people with hypertension more prone to heart disease, peripheral vascular disease, heart attack, stroke, kidney disease and aneurysms. Correspondingly, chronic conditions such as diabetes, kidney disease, sleep apnea and high cholesterol increase the risk for developing high blood pressure. [Reference](https://www.rush.edu/health-wellness/discover-health/6-high-blood-pressure-facts)\n\n**thalach: Maximum Heart Rate**\n> It has been shown that an increase in heart rate by 10 beats per minute was associated with an increase in the risk of cardiac death by at least 20%, and this increase in the risk is similar to the one observed with an increase in systolic blood pressure by 10 mm Hg. [Reference](https://pubmed.ncbi.nlm.nih.gov/19615487/#:~:text=It%20has%20been%20shown%20that,pressure%20by%2010%20mm%20Hg.)\n\n**fbs: Fasting Blood Sugar**\n> A test that measures blood sugar levels. Elevated levels are associated with diabetes and insulin resistance, in which the body cannot properly handle sugar (e.g. obesity). [Reference](https://my.clevelandclinic.org/health/diagnostics/16790-blood-sugar-tests)\n\n**chol: serum cholestoral**\n> The conventional view is that having high LDL cholesterol levels increases your risk of dying of cardiovascular diseases, such as heart disease. [Reference](https://www.nhs.uk/news/heart-and-lungs/study-says-theres-no-link-between-cholesterol-and-heart-disease/)\n</div><br>"},{"metadata":{},"cell_type":"markdown","source":"\n**Pain & Defect Related Features** : `cp`,`exang`,`thal`,`ca`\n<div class=\"alert alert-block alert-info\">\n\n**exang:  Exercise Induced Angina**\n> Angina is chest pain caused by reduced blood flow to the heart muscles. It's not usually life threatening, but it's a warning sign that you could be at risk of a heart attack or stroke. [Reference](https://www.nhs.uk/conditions/angina/)\n\n**ca: number of major vessels (0-3) colored by flourosopy**\n> However, number of major vessels colored by fluoroscopy had a medium effect on the CAD diagnosis that could be due to the small sample size of the study population. It might be related to the fact that the sensitivity of fluoroscopy could be as low as 35% in some cases,[86] and the system learned it from the training set.\n[Reference](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4468223/)\n<br>"},{"metadata":{},"cell_type":"markdown","source":"## 1.5. Model Selection\n***\n\n- For this problem, we will need a model which has the ability to classify. It's possibly to use a `regression` model, and add a few lines of code which changes the `regressor` prediction to the nearest class. The `regressor` format is used to utilise `ensemble` model weighting, which often is said to improve predictions. \n- Let's use the `Gaussian Process` Regressor, introduced in a [previous notebook](https://www.kaggle.com/andreikozlov/uci-airfoil-noise-prediction) & add a simple `classifier` that finds nearest unique class associated to the `posterior mean` prediction.\n- The model doesn't incorporate `probability` prediction, so `ROC` & `PR` curves, which are generally useful, but aren't looked at. \n- Instead the model is used with `GridSearchCV` & manual model parameter (`hyperparameters`) selection to prevent severe overfitting.\n\n# 2. Exploratory Data Analysis (EDA)\n***\n\nThe dataset contains quite a lot of features to choose from for building our model(s) but we first need to investigate the dataset. The feature dataset can be rougtly divided into `general patient`,`ECG`,`blood`,`pain` related features.\n\n## 2.1. Initial Dataset Impression\n***"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/heart-disease-cleveland-uci/heart_cleveland_upload.csv')\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(df.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `info()` suggests we have a full dataset w/ no missing values\n- `head()` suggests the `categorical` features described above are already converted for us\n\n## 2.2. Correlation Matrix\n***\nWe have a lot of features, let's investigate the `linear correlation` of these features, they are all formally numerical, all `categorical` features are already converted for us."},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Plot a Shifted Correlation Matrix '''\n# Diagonal correlation is always unity & less relevant, shifted variant shows only relevant cases\ndef corrMat(df,id=False):\n    \n    corr_mat = df.corr().round(2)\n    f, ax = plt.subplots(figsize=(12,7))\n    mask = np.triu(np.ones_like(corr_mat, dtype=np.bool))\n    mask = mask[1:,:-1]\n    corr = corr_mat.iloc[1:,:-1].copy()\n    sns.heatmap(corr,mask=mask,vmin=-0.3,vmax=0.3,center=0, \n                cmap='plasma',square=False,lw=2,annot=True,cbar=False)\n#     bottom, top = ax.get_ylim() \n#     ax.set_ylim(bottom + 0.5, top - 0.5) \n    ax.set_title('Shifted Linear Correlation Matrix')\n    \ncorrMat(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`condition` variable has a relatively broad range of correlated features:  <br>\n***\n- Amongst the higher positively correlated features;`thal`,`ca`,`oldpeak`,`exang`,`cp`; (0.52,0.46,0.42,0.41) <br>\n- Only one feature is negatively correlated to the target variable; `thalach` (-0.42)\n- The only feature that has little to no linear correlation to target variable:`fbs` (0.0)"},{"metadata":{"trusted":true},"cell_type":"code","source":"''' CountPlot Histograms '''\n\nplt4 = ['tab:blue','tab:orange']\ndef plot1count(x,xlabel,palt):\n    \n    plt.figure(figsize=(20,2))\n    sns.countplot(x=x,hue='condition', data=df, palette=palt)\n    plt.legend([\"<50% diameter narrowing\", \">50% diameter narrowing\"],loc='upper right')\n    plt.xlabel(xlabel)\n    plt.ylabel('Frequency')\n    plt.show()\n    \ndef plot1count_ordered(x,xlabel,order,palt):\n    \n    plt.figure(figsize=(20,2))\n    sns.countplot(x=x,hue='condition',data=df,order=order,palette=palt)\n    plt.legend([\"<50% diameter narrowing\", \">50% diameter narrowing\"],loc='upper right')\n    plt.xlabel(xlabel)\n    plt.ylabel('Frequency')\n    plt.show()\n\ndef plot2count(x1,x2,xlabel1,xlabel2,colour,rat,ind1=None,ind2=None):\n    \n    # colour, ratio, index_sort\n\n    fig,ax = plt.subplots(1,2,figsize=(20,3),gridspec_kw={'width_ratios':rat})\n    # Number of major vessels (0-3) colored by flourosopy\n    sns.countplot(x=x1,hue='condition',data=df,order=ind1,palette=colour,ax=ax[0])\n    ax[0].legend([\"<50% diameter narrowing\", \">50% diameter narrowing\"],loc='upper right')\n    ax[0].set_xlabel(xlabel1)\n    ax[0].set_ylabel('Frequency')\n\n    # Defect Information (0 = normal; 1 = fixed defect; 2 = reversable defect )\n    sns.countplot(x=x2,hue='condition', data=df,order=ind2,palette=colour,ax=ax[1])\n    ax[1].legend([\"<50% diameter narrowing\", \">50% diameter narrowing\"],loc='best')\n    ax[1].set_xlabel(xlabel2)\n    ax[1].set_ylabel('Frequency')\n    plt.show()\n    \n''' Plot n Countplots side by side '''\ndef nplot2count(lst_name,lst_label,colour,n_plots):\n    \n    ii=-1;fig,ax = plt.subplots(1,n_plots,figsize=(20,3))\n    for i in range(0,n_plots):\n        ii+=1;id1=lst_name[ii];id2=lst_label[ii]\n        sns.countplot(x=id1,hue='condition',data=df,palette=colour,ax=ax[ii])\n        ax[ii].legend([\"<50% diameter narrowing\", \">50% diameter narrowing\"],loc='upper right')\n        ax[ii].set_xlabel(id2)\n        ax[ii].set_ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3. Feature Bivariate Histograms\n***\n\nBivariate histograms are split into several categories of similar similarity `General patient`,`ECG`,`Blood`,`Pain` related features, hue for both `condition` is compared.\n\n#### 2.3.1. `General Features` & `Pain Related Features`\n***"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot2count('age','sex','Age of Patient','Gender of Patient',plt4,[2,1])\nlst1 = ['cp','exang','thal','ca']\nlst2 = ['Chest Pain Type','Excersised Induced Angina','Thalium Stress Result','Fluorosopy Vessels']\nnplot2count(lst1,lst2,plt4,4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Age & Sex; Linear Correlation (0.23,0.28)** <br>\nAge group between [29,54]; increase in frequency for a `<50%` cases `target` value. <br>\nAge group between [55,63]; distinctly larger proportion of `>50%`<br>\nAside from a couple of patients in the age group 71 to 76, `>50%` is much more populated for the higher age group, therefore identifying them are the higher risk patients<br>\n`Sex` Distribution suggests; `male` are more likely to have an association with `>50%` <br> \n`women` patient also had much more even distribution of `>50%` & `<50%` cases, compared to `male`.\n\n**Chest Pain Type; Linear Correlation (-0.41)**<br>\n`>50%` patients actually are associated with no chest pain symptoms as is seen in the data.\n\n**Excercise Induced Angina; Linear Correlation (0.42)** <br>\nHigher values of `exang` are associated with higher values of `condition` (`>50%`) <br>\n\n**Major Coloured Vessels; Linear Correlation (0.46)** <br>\nHigher values of coloured `major vessels` are associated with target variabe `>50%` <br>\n\n#### 2.3.2. `ECG Related Features`\n***"},{"metadata":{"trusted":true},"cell_type":"code","source":"lst_ecg = ['oldpeak','restecg','slope','condition']\nplot1count('oldpeak','oldpeak: ST Depression Relative to Rest',plt4)\nplot2count('restecg','slope','restecg: Resting electrocardiography (ECG)','slope: []',plt4,[1,1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**oldpeak**<br>\nFeature `oldpeak` shows a distinct association of `>50%` to higher`oldpeak` values above 1.0 <br>\nPatients w/ `<50%` tend to have an old peak of 0.0, with some slight increase possible, however very rarely above 2.0\n\n**Resting Electrocardiography (ECG)** <br>\nMost `<50%` patients have normal `ECG`, however a large portion also show probable/definite left ventricular hypertropy <br>\nHowever a large portion of `>50%` patients also show normal `ECG`, but more are are asociated with a more probable/definite left ventricular hypertropy result.\n\n**Slope** <br>\nUpsloping tends to be associated with `<50%` patients <br>\nFlat slopes are more associated with `>50%` patients <br>\nVery little variation variation exists for downsloping\n\n#### 2.3.3. `Blood Related Features`\n***"},{"metadata":{"trusted":true},"cell_type":"code","source":"lst_blood = ['trestbps','thalach','fbs','chol','condition']\nplot1count('trestbps','trestbps: Resting Blood Pressure (mmHg)',plt4)\nplot1count_ordered('thalach','thalach: Maximum Heart Rate',df['thalach'].value_counts().iloc[:30].index,plt4)\nplot2count('fbs','chol','Fasting Blood Sugar','Serum Cholestoral',plt4,[2,10],None,df['chol'].value_counts().iloc[:40].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Resting Blood Pressure; Linear Correlation (0.15)**<br>\nDistinctive peaks are visible in data, 110,120,130,140,150,160 mmHg; likely associated with common values. <br>\nLower pressure in the range 94-108 mmHg tends to be associated with a target `<50%` patients <br>\nA rather spread out relation for `>50%` is noted, but tends to slightly favour higher values of `trestbps`, esp. 160+ mmHg\n\n**Maximum Heart Rate, thalach; Linear Correlation (-0.42)**<br>\n`>50%` cases are thus more associated with lower maximum heart rate values<br>\nThe histogram data shows the higher frequency cases; we note a large number of peaks for `>50%` which are quite low (125,132,144,150), `<50%` cases are more associated with higher values, 150+, indicative of the `correlation` value.\n\n**Fasting Blood Sugar, fbs; Linear Correlation (0)**<br>\nMost patients have a `fbs` below 120, but there seems no relation to `condition` for a value higher than 120.\n\n**Serum Cholestoral, chol, Linear Correlation (0.08)**<br>\nA wide range of `chol` values are measured ranging from 126 to 564.<br>\nThe top most common values are almost all higher than 200, which is associate with an elevated value."},{"metadata":{},"cell_type":"markdown","source":"**Numerical/Categorical Division**\n***\n\nHaving revewed the features via `bivariate` histograms, we can define a `numerical/categorical` feature split:\n\n<div class=\"alert alert-block alert-info\">\n    \n- Numerical Features**: age,oldpeak,trestbps,thalach,chol <br>\n\n- Categorical Features**: sex,restecg,slope,fbs,cp,exang,thal,ca\n</div>\n<br>\n    \n- As we have a few `categorical` features that have been converted into `numerical` for us, we should question whether each of the categories is weighted correctly, as they are simply ordered from 0. \n- Higher values may indicate a larger weight, which may not be correct when features are used in the model. So let's opt for a `OneHotEncoding` approach to split these `categorical features` into separate features."},{"metadata":{"trusted":true},"cell_type":"code","source":"lst_ohe_feat = ['sex','restecg','slope','fbs','cp','exang','thal','ca']\nlst_ohe_out = []\nfor i in lst_ohe_feat:\n    tdf = pd.get_dummies(df[i],i)\n    lst_ohe_out.append(tdf)\n    \nlst_ohe_out.append(df['condition'])\ndf_ohe = pd.concat(lst_ohe_out,axis=1) # One Hot Encoding Features df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4. Bivariate PairGrids\n***\nPairgrids often reveal something interesting in pairwise feature relations, plotting a separate `hue` for both `condition` subsets."},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Draw a Bivariate Seaborn Pairgrid /w KDE density w/ '''\ndef snsPairGrid(df):\n\n    ''' Plots a Seaborn Pairgrid w/ KDE & scatter plot of df features'''\n    g = sns.PairGrid(df,diag_sharey=False,hue='condition')\n    g.fig.set_size_inches(13,13)\n    g.map_upper(sns.kdeplot,n_levels=5)\n    g.map_diag(sns.kdeplot, lw=2)\n    g.map_lower(sns.scatterplot,s=30,edgecolor=\"k\",linewidth=1,alpha=0.6)\n    g.add_legend()\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We actually have only 5 continuous numerical features, the rest are categorical numbers\nnumvars_targ = ['age','trestbps','chol','thalach','oldpeak','condition']\nsnsPairGrid(df[numvars_targ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`Condition` : `0:<50%`, `1:>50%`\n***\n- A lot of overlap in variables in the features, most notably `chol` & `trestbps` look quite similar in distribution\n- `oldpeak`,`thalach`,`age` are features with most variation in target variable `condition`.\n\n`oldpeak` is an interesting feature where a large variation in `condition` exists:\n- a lot of `>50%` cases have an elevated oldpeak\n- age group isn't really a factor in elevated `oldpeak` values\n- `chol` levels don't seem too out of the ordinary for these elevated cases\n- `thalach` levels tend to be lower, two visibles centres emerge on the KDE relation, indicating a linear relation; ("},{"metadata":{},"cell_type":"markdown","source":"# 3. Model Building\n***\n- We have a quite a lot of features we can look into when building a model that predicts `condition`\n- EDA revealed our features can be split in a few subgroups, let's investigate whether it's possible to build models based on `ECG`,`Blood`,`Patient Features` alone, as well as a mixture of them. The original dataset `categorical` featuers are used.\n\n- EDA also revealed our `numerical` features do contain converted `categorical` features, implying they have been allocated a specific value starting from zero. These values can be misinterpreted by the model, thus `OHE` will be employed. If `OHE` shows an improvement, it's likely the `standard` feature allocation for categorical values is negatively affecting the model performance.\n\n- Previously outlined, `GP` is significantly prone to overfitting, it's very easy to get a very high scores on training data, but have very poor test data accuracy. So a decent model should have similar accuracy on all tested data.\n- All models are built on the assumption; `sigma_n=0.01`\n\n## 3.1. Gaussian Process (GP) Classifier, GPC()\n\nThe <b>GPC()</b> class contains a very simplistic classification addition to the <b>Regressor</b> model based on the closest distance to each class. <br>\nThe GP model has the ability to accurately adapt to data it is provided in a high dimensional space. How the model behaves is dependent on its <b>hyperparameter</b> selection: <br><br>\nEach <b>hyperparameter</b> has it's own role in how it changes the model, with its core being the <b>covariance matrix</b>; defining all of the instance relation weights in a neat matrix format. <b>Multiple</b> covariance matrices have to be constructed to make a prediction, including matrix inversions, yet GP is the cheaper variant of all models associated with it. <br><br>\n<b>GP</b> has several <b>hyperparameters</b> which must be set to train the model, three are set `theta`,`sigma`,`sigma_n`.\n***\n- Two `hyperaparameters` are associated with the `covariance function` (`kernel` if you prefer); `theta` & `sigma`, this function is used to define all weights in the `covariance matrix`. These functions are used in both `variance` & `covariance` parts of the `covariance matrix`. These functions can be set in whatever combination suits your problem.\n- The last, `sigma_n` is a hyperparameter associated with the diagonal term in the `covariance matrix`, influencing the `variance` component only. Implying how relevant the training nodes are; (noise/noiseless) assumption.\n    \n**Model Instantiation Options, activates `__init__` content:**\n***\n\n**Hyperparameters** <br>\n- `self.theta` is the `covariance function` associated `hyperparameter`, similar for the other two. \n- `__init__` sets the parameters to a default value (`theta=10`,`sigma=10`,`sigma_n=0.01`) if not set; GPR(). \n- You can set them manually GPC(theta=1,sigma=1,sigma_n=0.01,opt=False), but `opt=False` must be present to prevent `hyperparameters` to be overwritten. <br>\n\n**Other options**\n- `self.opt` is the previously mentioned activator for `objective function` optimisation.\n- `GPC.kernel` is a common class variable of GPR, defining the type of `covariance function` used.\n- `self.mu_in` is an import for previously calculated posterior mean predictions \n- `self.se_alp` is the ensemble coefficient (current prediction multiplier)\n- `self.se_bet` is the ensemble coefficient (imported prediction multiplier)\n\n**Training the GPC() model**, `.fit(X,y)`\n***\n- Setting `hyperparameters` & calculating the training <b>covariance matrix</b>` self.Kmat`\n- Specific to the `classifier`, all unique target values are defined\n- Hyperparameters can be both set in the manner outlined above, or tuned based on a specific `objective function`. \n- Various `objective functions` exist, the full `likelihood` & simplified variant is included, \n- Scipy's `optimize.minimize` class is used together with the `L-BFGS-B` approach to find the minimum.\n\n**Making a prediction using the GPC() model**, `.predict(X)`\n***\n- The `Covariance Matrix` for Training & Test Feature Matrices needs to be calculated.\n- Commonly referred to as the `posterior mean` is the main model prediction output.\n- Specific to the `classifier`, the nearest class to each prediction is found."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator,ClassifierMixin\nfrom numpy.linalg import cholesky, det, lstsq, inv, eigvalsh, pinv\nfrom scipy.optimize import minimize\npi = 4.0*np.arctan(1.0)\n\n# Usage similar to any sklearn model\nclass GPC(BaseEstimator,ClassifierMixin):\n\n    ''' Class Instantiation Related Variables '''\n    # With just the one class specific GPC.kernel\n    def __init__(self,kernel='rbf',theta=10.0,sigma=10.0,sigma_n=1.0,opt=True,mu_in=None,se_alp=0.5,se_bet=0.5):\n        self.theta = theta            # Hyperparameter associated with covariance function\n        self.sigma = sigma            #                       ''\n        self.sigma_n = sigma_n        # Hyperparameter associated with cov.mat's diagonal component\n        self.opt = opt                # Update hyperparameters with objective function optimisation\n        GPC.kernel = kernel           # Selection of Covariance Function, class specific instantiation\n        self.mu_in = mu_in            # option to import model prediction\n        self.se_alp = se_alp          # ensemble coefficient (current prediction multiplier)\n        self.se_bet = se_bet          # ensemble coefficient (imported prediction multiplier)\n\n    ''' local covariance functions '''\n    # Covariance Functions represent a form of weight adjustor in the matrix W/K\n    # for each of the combinations present in the feature matrix\n    @staticmethod\n    def covfn(X0,X1,theta=1.0,sigma=1.0):\n\n        ''' Radial Basis Covariance Function '''\n        if(GPC.kernel is 'rbf'):\n            r = np.sum(X0**2,1).reshape(-1,1) + np.sum(X1**2,1) - 2 * np.dot(X0,X1.T)\n            return sigma**2 * np.exp(-0.5/theta**2*r)\n\n        ''' Matern Covariance Class of Funtions '''\n        if(GPC.kernel is 'matern'):\n            lid=2\n            r = np.sum(X0**2,1)[:,None] + np.sum(X1**2,1) - 2 * np.dot(X0,X1.T)\n            if(lid==1):\n                return sigma**2 * np.exp(-r/theta)\n            elif(lid==2):\n                ratio = r/theta\n                v1 = (1.0+np.sqrt(3)*ratio)\n                v2 = np.exp(-np.sqrt(3)*ratio)\n                return sigma**2*v1*v2\n            elif(lid==3):\n                ratio = r/theta\n                v1 = (1.0+np.sqrt(5)*ratio+(5.0/3.0)*ratio**2)\n                v2 = np.exp(-np.sqrt(5)*ratio)\n                return sigma**2*v1*v2\n        else:\n            print('Covariance Function not defined')\n    \n    ''' Train the GP Classifier Model'''\n    def fit(self,X,y):\n        \n        # Two Parts Associated with base GP Model:\n        # - Hyperaparemeter; theta, sigma, sigma_n selection\n        # - Definition of Training Covariance Matrix\n        # Both are recalled in Posterior Prediction, predict()\n        \n        ''' Working w/ numpy matrices'''\n        if(type(X) is np.ndarray):\n            self.X = X;self.y = y\n        else:\n            self.X = X.values; self.y = y.values\n        self.ntot,ndim = self.X.shape\n  \n        ''' Define Class Labels '''\n        self.class_labels = np.unique(self.y)\n\n        ''' Optimisation Objective Function '''\n        # Optimisation of hyperparameters via the objective funciton\n        def llhobj(X,y,noise):\n            \n            # Simplified Objective Function\n            def llh_dir(hypers):\n                K = self.covfn(X,X,theta=hypers[0],sigma=hypers[1]) + noise**2 * np.eye(self.ntot)\n                return 0.5 * np.log(det(K)) + \\\n                    0.5 * y.T.dot(inv(K).dot(y)).ravel()[0] + 0.5 * len(X) * np.log(2*pi)\n\n            # Full Likelihood Equation\n            def nll_full(hypers):\n                K = self.covfn(X,X,theta=hypers[0],sigma=hypers[1]) + noise**2 * np.eye(self.ntot)\n                L = cholesky(K)\n                return np.sum(np.log(np.diagonal(L))) + \\\n                    0.5 * y.T.dot(lstsq(L.T, lstsq(L,y)[0])[0]) + \\\n                    0.5 * len(X) * np.log(2*pi)\n\n            return nll_full # return one of the two, simplified variant doesn't always work well\n\n        ''' Update hyperparameters based on set objective function '''\n        if(self.opt==True):\n            # define the objective funciton\n            objfn = llhobj(self.X,self.y,self.sigma_n)\n            # search for the optimal hyperparameters based on given relation\n            res = minimize(objfn,[1,1],bounds=((1e-5,None),(1e-5, None)),method='L-BFGS-B')\n            self.theta,self.sigma = res.x # update the hyperparameters to \n\n        ''' Get Training Covariance Matrix, K^-1 '''\n        Kmat = self.covfn(self.X,self.X,self.theta,self.sigma) \\\n                 + self.sigma_n**2 * np.eye(self.ntot) # Covariance Matrix (Train/Train)\n        self.IKmat = pinv(Kmat) # Pseudo Matrix Inversion (More Stable)\n        return self  # return class & use w/ predict()\n\n    ''' Posterior Prediction;  '''\n    # Make a prediction based on what the model has learned (hyperparameter selection & training weights)\n    def predict(self,Xm):\n        \n        # Covariance Matrices x2 required; (Train/Test&Train/Test)\n        mtot = Xm.shape[0]  # Number of Test Matrix Instances\n        K_s = self.covfn(self.X,Xm,self.theta,self.sigma)  # Covariance Matrix (Train/Test)               \n        self.mu_s = K_s.T.dot(self.IKmat).dot(self.y)      # Posterior Mean Prediction of current model\n        \n        # Ensemble Modified Posterior Prediction\n        if(self.mu_in!=None): \n            \n            lntot = self.mu_in[0].shape[0];lmtot = self.mu_in[1].shape[0]\n            if(self.mu_s.shape[0]==lntot): j=0\n            else: j=1\n            loc_mu_s = self.se_alp * self.mu_s + self.se_bet * self.mu_in[j]\n            lc = [self.class_labels[np.abs(self.class_labels - x).argmin()] for x in loc_mu_s]\n            return np.array(lc)\n        \n        # Standard Posterior Prediction \n        else:\n            # Find the nearest class label to predicted model value, list\n            lc = [self.class_labels[np.abs(self.class_labels - x).argmin()] for x in self.mu_s]\n            return np.array(lc)\n        \n''' Sample Usage for GPC() '''\nfrom sklearn.datasets import load_iris\nfrom sklearn.dummy import DummyClassifier as DC\niris = load_iris();X = iris.data;y = iris.target \nmodel = DC(strategy=\"most_frequent\");model.fit(X,y);print(f'DC(): {model.score(X,y).round(2)}')\nmodel = GPC();model.fit(X,y);print(f'GPC(): {model.score(X,y)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Draw a single Heatmap using Seaborn '''\ndef heatmap1(values,xlabel,ylabel,xticklabels,yticklabels,\n             cmap='plasma',vmin=None,vmax=None,fmt=\"%0.2f\",title=None):\n\n    fig, ax = plt.subplots(figsize=(5,5))\n    sns.heatmap(values, ax=ax,cmap=cmap,cbar=True)\n    \n    img = ax.pcolor(values, cmap=cmap, vmin=vmin, vmax=vmax)\n    img.update_scalarmappable()\n    ax.set_xlabel(xlabel);ax.set_ylabel(ylabel)\n    ax.set_xticks(np.arange(len(xticklabels)) + 0.5)\n    ax.set_yticks(np.arange(len(yticklabels)) + 0.5)\n    ax.set_xticklabels(xticklabels);ax.set_yticklabels(yticklabels)\n    ax.set_title(title)\n    ax.set_aspect(1)\n    \n    for p, color, value in zip(img.get_paths(), img.get_facecolors(),img.get_array()):\n        x, y = p.vertices[:-2, :].mean(0)\n        if np.mean(color[:3]) > 0.5:\n            c = 'k'\n        else:\n            c = 'w'\n        ax.text(x, y, fmt % value, color=c, ha=\"center\", va=\"center\")\n        \n''' Plot Several Seaborn Heatmaps Side by Side '''\ndef heatmapn(n,values,labels,ticklabels,titles,\n              cmap='plasma',vmin=None,vmax=None,fmt=\"%0.2f\"):\n\n    ii=-1\n    fig,ax = plt.subplots(1,n,figsize=(15,5))\n    for i in range(0,n):\n        ii+=1\n        tval = values[ii];ttitle = titles[ii]\n    \n        sns.heatmap(tval,ax=ax[ii],cmap=cmap,cbar=True) \n        img = ax[ii].pcolor(tval, cmap=cmap, vmin=vmin, vmax=vmax)\n        img.update_scalarmappable()\n        ax[ii].set_xlabel(labels[0]);ax[ii].set_ylabel(labels[1])\n        ax[ii].set_xticks(np.arange(len(ticklabels[0])) + 0.5)\n        ax[ii].set_yticks(np.arange(len(ticklabels[1])) + 0.5)\n        ax[ii].set_xticklabels(ticklabels[0]);ax[ii].set_yticklabels(ticklabels[1])\n        ax[ii].set_title(ttitle)\n        ax[ii].set_aspect(1)\n    \n        # color of each matrix content\n        for p, color, value in zip(img.get_paths(), img.get_facecolors(),img.get_array()):\n            x, y = p.vertices[:-2, :].mean(0)\n            if np.mean(color[:3]) > 0.5:\n                c = 'k'\n            else:\n                c = 'w'\n            ax[ii].text(x, y, fmt % value, color=c, ha=\"center\", va=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3.2. ECG Based Model\n***\n\n- EDA investigation into ECG features revealed they have a good mix of `correlation` values to the target variable `condition`; `restecg(0.17)`,`oldpeak(0.42)`,`slope(0.33)`.\n- Let's investigate if it's viable to implement `condition` prediction based on subset feature data\n- Then, since we have a few features, we can incorporate `PolynomialFeatures()` of a high order without drastically increasing the computational cost, `StandardScaler()` as well is added in the `Poly()` Pipeline.\n\n### 3.2.1. GPC Model w/ GridSearchCV (manual theta,sigma) Search\n***\nThe assumption is that the our data has a relatively low noise level & training data are with minimal errors, `sigma_n=0.01`"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV,cross_val_score\n\nlst_theta = [0.01, 0.1, 1, 10, 100, 1000, 5000]\nlst_sig = [0.01, 0.1, 1, 10, 100, 1000, 5000]\n\ndef modelEval(ldf,lst_theta,lst_sig,feature='condition'):\n\n    # Given a dataframe, split feature/target variable\n    X = ldf.copy()\n    y = ldf[feature].copy()\n    del X[feature]\n    \n    # define parameters for gridsearch (theta,sigma)\n    param_grid = {'theta': lst_theta,'sigma': lst_sig}\n    \n    # split dataset into 5 segments, fit & predict fo each segment\n    model = GPC(opt=False)  # manual hyperparameter model\n    model.fit(X,y)\n\n    gscv = GridSearchCV(model,param_grid,cv=5) # 5 fold CV\n    gscv.fit(X.values,y.values)\n    results = pd.DataFrame(gscv.cv_results_) \n    scores = np.array(results.mean_test_score).reshape(7,7)\n    \n    # plot the cross validation mean scores of the 5 fold CV\n    heatmap1(scores,xlabel='theta',xticklabels=param_grid['theta'],\n                    ylabel='sigma',yticklabels=param_grid['sigma'])\n    \nldf1 = df[lst_ecg] # subset of ecg features\nmodelEval(ldf1,lst_theta,lst_sig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2.2. GPC Model w/ GridSearchCV (theta,sigma) Search + PolynomialFeatures()\n***\nAdditional to the previous assumption, effects of `PolynomialFeatures()` of 2nd & 7th order are investigated in a `Pipeline()` w/ `StandardScaler()`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline,Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures,StandardScaler\n\nlst_theta = [0.01, 0.1, 1, 10, 100, 1000, 5000]\nlst_sig = [0.01, 0.1, 1, 10, 100, 1000, 5000]\n\n# Model Evaluation Function for Polynomial Feature Pipeline\ndef modelEval2(ldf,lst_theta,lst_sig,feature='condition'):\n\n    # Given a dataframe, split feature/target variable\n    y = ldf[feature].copy()\n    X = ldf.copy()\n    del X[feature]     # remove target variable\n    \n    tlst = []\n    for i in [2,7]:\n    \n        # create a pipeline combining a polynomial feature \n        pipe = Pipeline(steps=[('scaler',StandardScaler()),\n                               ('poly',PolynomialFeatures(i)),\n                               ('model',GPC(opt=False))])\n\n        # pipepines require slightly different notations w/ __\n        param_grid = {'model__theta': lst_theta,'model__sigma': lst_sig}\n\n        gscv2 = GridSearchCV(pipe,param_grid,cv=10)\n        gscv2.fit(X,y)\n        ypred = gscv2.predict(X)\n        results2 = pd.DataFrame(gscv2.cv_results_)\n        scores2 = np.array(results2.mean_test_score).reshape(7,7)\n        tlst.append(scores2)\n    \n    lst_lab = ['theta','sigma'];lst_tit = ['Poly(2)','Poly(7)']\n    lst_tick = [param_grid['model__theta'],param_grid['model__sigma']]\n    \n    # Plot two Heatmaps side by side for the two Polynomial\n    heatmapn(n=2,values=tlst,labels=lst_lab,ticklabels=lst_tick,titles=lst_tit)\n\nmodelEval2(ldf1,lst_theta,lst_sig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2.3. GPC Model (theta=10,sigma=100) w/ Train_test_split()\n***\nHaving conducted a cross validation, let's determine the score for a 70/30 split using `theta=10`,`sigma=100`"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\ndef modelEval3(ldf,hyp,pred_upd,feature='condition'):\n\n    # Given a dataframe, split feature/target variable\n    y = ldf[feature].copy()\n    X = ldf.copy()\n    del X[feature]     # remove target variable\n    \n    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=73)\n\n    lst_mu = []\n    if(pred_upd!=None):\n        model = GPC(theta=hyp[0],sigma=hyp[1],opt=False,mu_in=pred_upd)\n    else:\n        model = GPC(theta=hyp[0],sigma=hyp[1],opt=False)\n\n    model.fit(X_train,y_train)\n    model.predict(X_train.values);lst_mu.append(model.mu_s)\n    model.predict(X_test.values);lst_mu.append(model.mu_s)\n    print(f'Training Score: {model.score(X_train.values,y_train.values)}')\n    print(f'Test Score: {model.score(X_test.values,y_test.values)}')\n    \n    if(pred_upd==None):\n        return lst_mu\n            \nlst_ldf1 = modelEval3(ldf1,hyp=[10,100],pred_upd=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summarising: ECG Based Models**\n<div class=\"alert alert-block alert-info\">\n    \n- ECG features whilst being important, don't seem like the only features that are needed to create a reasonable model for the prediction of target variable `condition`. The addition of more features will likely improve the model\n    \n- The `ECG` feature model reached only a peak mean cross validation score of 0.72 using 10-fold CV\n    \n- The addition of polynomial features didn't improve the model\n    \n- Standard `train_test_split` resulted in a very similar training and test scores (0.74,0.72)\n<br>"},{"metadata":{},"cell_type":"markdown","source":"## 3.3. Blood Related Feature Model & Simple Ensemble\n***\n- Features `trestbps`,`thalach`,`fbs`,`chol` are available relating to blood samples, let's use the previously used function `modelEval` to build a basic `CrossValidationCV` model using them.\n- Let's also try a simple `ensemble` approach, using the exported `posterior mean` from the `ECG` model & import it into the new model. \n- Like ECG, the correlation of various associated features is quite widespread, not too low or highly correlated to target variable, `condition`, so perhaps we might get a better model."},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Cross Validation '''\nlst_theta = [10,100, 500, 1000, 1500, 2000, 2500]\nlst_sig = [0.01,0.1,1.0,10,50,100, 500]\n\nldf2 = df[lst_blood]\nmodelEval(ldf2,lst_theta,lst_sig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Train/Test Split w/  '''\nlst_ldf2 = modelEval3(ldf2,hyp=[1000,50],pred_upd=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Ensemble Modification Train/Test Split'''\n# lst_ldf1 : ECG based model prediction\nmodelEval3(ldf2,hyp=[1000,50],pred_upd=lst_ldf1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summarising: Blood Related Feature Model & Simple Ensemble**\n\n<div class=\"alert alert-block alert-info\">\n\n- The standalone Blood Related Feature Model did slightly worse than the `ECG` model created earler\n    \n- Optimal values of `theta` lie slightly higher than the previous model, whilst `sigma` was roughtly idential to the previous model.\n    \n- An ensemble approach, using the `ECG` model output was used in the input of the `Blood Related` Model, which resulted in the highest model score up to now (0.76)"},{"metadata":{},"cell_type":"markdown","source":"## 3.4. Patient Related Features + ECG Model\n***\n`Age` & `Sex` are interesting features which may add value to the model, but not enough by itself. The two features are added to the ECG feature model features to create a new model."},{"metadata":{"trusted":true},"cell_type":"code","source":"lst = ['age','sex'] \nldf3 = df[lst+lst_ecg]\nmodelEval(ldf3,lst_theta,lst_sig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `Age` & `Sex` didn't actually seem to add anything to improve the `ECG` model\n\n## 3.5. Feature Type Based Models\n****\n\n- Now having built a few models based on the original feature matrix encoding for `categorical` feaures (0 onwads), we can see that there tends to be a limit beyond which it's a little difficult to exceed. Even `scaling` has little to no impact as implemented in a `PolynomialFeature()` pipeline.\n\n- We have two sets of feature types; `numerical` and `categorical`. `Categorical` features were applied a OHE modification, creating new feautres for every unique feature case. The two models are evaluated individually & ensemble approach is attempted."},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Numerical Features Model '''\nlst_theta = [100, 500, 1000, 1500, 2000, 2500,3000]\nlst_sig = [500,1000,1500,2000,2500,3000,3500]\n\ndf_num = df[numvars_targ].copy()\nmodelEval(df_num,lst_theta,lst_sig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst_ldf3 = modelEval3(df_num,hyp=[1500,1500],pred_upd=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Categorical Feature Model '''\nlst_theta = [10,100, 500, 1000, 1500, 2000, 2500]\nlst_sig = [0.01,0.1,1.0,10,50,100, 500]\nmodelEval(df_ohe,lst_theta,lst_sig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Ensemble Modification of Categorical Feature Model ''' \nmodelEval3(df_ohe,hyp=[100,50],pred_upd=lst_ldf3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Summarising: Feature Type Based Models**\n\n<div class=\"alert alert-block alert-info\">\n\n- The numerical feature only, consisting of a mixture of various feature types performed very similar to other tested models, peaking at a cross validation mean score of 0.74\n    \n- The OHE categorical feature set was more promising, consistenly scoring over 0.83, which is the best individual subgroup model so far\n    \n- Both Training/Test sets perform relatively similar, even with the ensemble posterior prediction adjustment, which is encouraging\n    \n- Cross Validation outlined various `hyperparameters` that give a mean cv score of 0.83, all of which give slightly different outcomes on train/test split, ranging from roughtly 0.8 to 0.89 for the test set, which seems to benefit the most from ensembling\n    \n</div><br>"},{"metadata":{},"cell_type":"markdown","source":"# 4. Thank You\n***\nThank you for reading, if you found any part of the notebook useful, please consider giving it an upvote."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}