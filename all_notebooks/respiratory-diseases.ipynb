{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# Any results you write to the current directory are saved as output.\n# 176_1b4_Al_mc_AKGC417L.txt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"os.listdir('../input/respiratory-sound-database/respiratory_sound_database/Respiratory_Sound_Database')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install pydub\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \\\n'../input/respiratory-sound-database/respiratory_sound_database/Respiratory_Sound_Database/filename_differences.txt'\n\ndf_diff = pd.read_csv(path, sep=\" \", header=None, names=['file_names'])\n\ndf_diff.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install pysoundfile","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import wave\nimport math\nimport scipy.io.wavfile as wf\n#wave file reader\n\n#Will resample all files to the target sample rate and produce a 32bit float array\ndef read_wav_file(str_filename, target_rate):\n    wav = wave.open(str_filename, mode = 'r')\n    (sample_rate, data) = extract2FloatArr(wav,str_filename)\n    \n    if (sample_rate != target_rate):\n        ( _ , data) = resample(sample_rate, data, target_rate)\n        \n    wav.close()\n    return (target_rate, data.astype(np.float32))\n\ndef resample(current_rate, data, target_rate):\n    x_original = np.linspace(0,100,len(data))\n    x_resampled = np.linspace(0,100, int(len(data) * (target_rate / current_rate)))\n    resampled = np.interp(x_resampled, x_original, data)\n    return (target_rate, resampled.astype(np.float32))\n\n# -> (sample_rate, data)\ndef extract2FloatArr(lp_wave, str_filename):\n    (bps, channels) = bitrate_channels(lp_wave)\n    \n    if bps in [1,2,4]:\n        (rate, data) = wf.read(str_filename)\n        divisor_dict = {1:255, 2:32768}\n        if bps in [1,2]:\n            divisor = divisor_dict[bps]\n            data = np.divide(data, float(divisor)) #clamp to [0.0,1.0]        \n        return (rate, data)\n    \n    elif bps == 3: \n        #24bpp wave\n        return read24bitwave(lp_wave)\n    \n    else:\n        raise Exception('Unrecognized wave format: {} bytes per sample'.format(bps))\n        \n#Note: This function truncates the 24 bit samples to 16 bits of precision\n#Reads a wave object returned by the wave.read() method\n#Returns the sample rate, as well as the audio in the form of a 32 bit float numpy array\n#(sample_rate:float, audio_data: float[])\ndef read24bitwave(lp_wave):\n    nFrames = lp_wave.getnframes()\n    buf = lp_wave.readframes(nFrames)\n    reshaped = np.frombuffer(buf, np.int8).reshape(nFrames,-1)\n    short_output = np.empty((nFrames, 2), dtype = np.int8)\n    short_output[:,:] = reshaped[:, -2:]\n    short_output = short_output.view(np.int16)\n    return (lp_wave.getframerate(), np.divide(short_output, 32768).reshape(-1))  #return numpy array to save memory via array slicing\n\ndef bitrate_channels(lp_wave):\n    bps = (lp_wave.getsampwidth() / lp_wave.getnchannels()) #bytes per sample\n    return (bps, lp_wave.getnchannels())\n\ndef slice_data(start, end, raw_data,  sample_rate):\n    max_ind = len(raw_data) \n    start_ind = min(int(start * sample_rate), max_ind)\n    end_ind = min(int(end * sample_rate), max_ind)\n    return raw_data[start_ind: end_ind]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nroot = '../input/respiratory-sound-database/respiratory_sound_database/Respiratory_Sound_Database/audio_and_txt_files/'\nfilenames = [s.split('.')[0] for s in os.listdir(path = root) if '.txt' in s]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Extract_Annotation_Data(file_name, root):\n    tokens = file_name.split('_')\n    recording_info = pd.DataFrame(data = [tokens], columns = ['Patient number', 'Recording index', 'Chest location','Acquisition mode','Recording equipment'])\n    recording_annotations = pd.read_csv(os.path.join(root, file_name + '.txt'), names = ['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n    return (recording_info, recording_annotations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"i_list = []\nrec_annotations = []\nrec_annotations_dict = {}\nfor s in filenames:\n    (i,a) = Extract_Annotation_Data(s, root)\n    i_list.append(i)\n    rec_annotations.append(a)\n    rec_annotations_dict[s] = a\nrecording_info = pd.concat(i_list, axis = 0)\nrecording_info.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_label_list = []\ncrack_list = []\nwheeze_list = []\nboth_sym_list = []\nfilename_list = []\nfor f in filenames:\n    d = rec_annotations_dict[f]\n    no_labels = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 0)].index)\n    n_crackles = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 0)].index)\n    n_wheezes = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 1)].index)\n    both_sym = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 1)].index)\n    no_label_list.append(no_labels)\n    crack_list.append(n_crackles)\n    wheeze_list.append(n_wheezes)\n    both_sym_list.append(both_sym)\n    filename_list.append(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duration_list = []\nfor i in range(len(rec_annotations)):\n    current = rec_annotations[i]\n    duration = current['End'] - current['Start']\n    duration_list.extend(duration)\n\nduration_list = np.array(duration_list)\nplt.hist(duration_list, bins = 50)\nprint('longest cycle:{}'.format(max(duration_list)))\nprint('shortest cycle:{}'.format(min(duration_list)))\nthreshold = 5\nprint('Fraction of samples less than {} seconds:{}'.format(threshold,\n                                                           np.sum(duration_list < threshold)/len(duration_list)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.signal\n\n#vtlp_params = (alpha, f_high) \ndef sample2MelSpectrum(cycle_info, sample_rate, n_filters, vtlp_params):\n    n_rows = 175 # 7500 cutoff\n    n_window = 512 #~25 ms window\n    (f, t, Sxx) = scipy.signal.spectrogram(cycle_info[0],fs = sample_rate, nfft= n_window, nperseg=n_window)\n    Sxx = Sxx[:n_rows,:].astype(np.float32) #sift out coefficients above 7500hz, Sxx has 196 columns\n    mel_log = FFT2MelSpectrogram(f[:n_rows], Sxx, sample_rate, n_filters, vtlp_params)[1]\n    mel_min = np.min(mel_log)\n    mel_max = np.max(mel_log)\n    diff = mel_max - mel_min\n    norm_mel_log = (mel_log - mel_min) / diff if (diff > 0) else np.zeros(shape = (n_filters,Sxx.shape[1]))\n    if (diff == 0):\n        print('Error: sample data is completely empty')\n    labels = [cycle_info[1], cycle_info[2]] #crackles, wheezes flags\n    return (np.reshape(norm_mel_log, (n_filters,Sxx.shape[1],1)).astype(np.float32), # 196x64x1 matrix\n            label2onehot(labels)) \n        \ndef Freq2Mel(freq):\n    return 1125 * np.log(1 + freq / 700)\n\ndef Mel2Freq(mel):\n    exponents = mel / 1125\n    return 700 * (np.exp(exponents) - 1)\n\n#Tased on Jaitly & Hinton(2013)\n#Takes an array of the original mel spaced frequencies and returns a warped version of them\ndef VTLP_shift(mel_freq, alpha, f_high, sample_rate):\n    nyquist_f = sample_rate / 2\n    warp_factor = min(alpha, 1)\n    threshold_freq = f_high * warp_factor / alpha\n    lower = mel_freq * alpha\n    higher = nyquist_f - (nyquist_f - mel_freq) * ((nyquist_f - f_high * warp_factor) / (nyquist_f - f_high * (warp_factor / alpha)))\n    \n    warped_mel = np.where(mel_freq <= threshold_freq, lower, higher)\n    return warped_mel.astype(np.float32)\n\n#mel_space_freq: the mel frequencies (HZ) of the filter banks, in addition to the two maximum and minimum frequency values\n#fft_bin_frequencies: the bin freqencies of the FFT output\n#Generates a 2d numpy array, with each row containing each filter bank\ndef GenerateMelFilterBanks(mel_space_freq, fft_bin_frequencies):\n    n_filters = len(mel_space_freq) - 2\n    coeff = []\n    #Triangular filter windows\n    #ripped from http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\n    for mel_index in range(n_filters):\n        m = int(mel_index + 1)\n        filter_bank = []\n        for f in fft_bin_frequencies:\n            if(f < mel_space_freq[m-1]):\n                hm = 0\n            elif(f < mel_space_freq[m]):\n                hm = (f - mel_space_freq[m-1]) / (mel_space_freq[m] - mel_space_freq[m-1])\n            elif(f < mel_space_freq[m + 1]):\n                hm = (mel_space_freq[m+1] - f) / (mel_space_freq[m + 1] - mel_space_freq[m])\n            else:\n                hm = 0\n            filter_bank.append(hm)\n        coeff.append(filter_bank)\n    return np.array(coeff, dtype = np.float32)\n        \n#Transform spectrogram into mel spectrogram -> (frequencies, spectrum)\n#vtlp_params = (alpha, f_high), vtlp will not be applied if set to None\ndef FFT2MelSpectrogram(f, Sxx, sample_rate, n_filterbanks, vtlp_params = None):\n    (max_mel, min_mel)  = (Freq2Mel(max(f)), Freq2Mel(min(f)))\n    mel_bins = np.linspace(min_mel, max_mel, num = (n_filterbanks + 2))\n    #Convert mel_bins to corresponding frequencies in hz\n    mel_freq = Mel2Freq(mel_bins)\n    \n    if(vtlp_params is None):\n        filter_banks = GenerateMelFilterBanks(mel_freq, f)\n    else:\n        #Apply VTLP\n        (alpha, f_high) = vtlp_params\n        warped_mel = VTLP_shift(mel_freq, alpha, f_high, sample_rate)\n        filter_banks = GenerateMelFilterBanks(warped_mel, f)\n        \n    mel_spectrum = np.matmul(filter_banks, Sxx)\n    return (mel_freq[1:-1], np.log10(mel_spectrum  + float(10e-12)))\n    \n#labels proved too difficult to train (model keep convergining to statistical mean)\n#Flattened to onehot labels since the number of combinations is very low\ndef label2onehot(c_w_flags):\n    c = c_w_flags[0]\n    w = c_w_flags[1]\n    if((c == False) & (w == False)):\n        return [1,0,0,0]\n    elif((c == True) & (w == False)):\n        return [0,1,0,0]\n    elif((c == False) & (w == True)):\n        return [0,0,1,0]\n    else:\n        return [0,0,0,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Used to split each individual sound file into separate sound clips containing one respiratory cycle each\n#output: [filename, (sample_data:np.array, start:float, end:float, crackles:bool(float), wheezes:bool(float)) (...) ]\ndef get_sound_samples(recording_annotations, file_name, root, sample_rate):\n    sample_data = [file_name]\n    (rate, data) = read_wav_file(os.path.join(root, file_name + '.wav'), sample_rate)\n    \n    for i in range(len(recording_annotations.index)):\n        row = recording_annotations.loc[i]\n        start = row['Start']\n        end = row['End']\n        crackles = row['Crackles']\n        wheezes = row['Wheezes']\n        audio_chunk = slice_data(start, end, data, rate)\n        sample_data.append((audio_chunk, start,end,crackles,wheezes))\n    return sample_data\n\n#Fits each respiratory cycle into a fixed length audio clip, splits may be performed and zero padding is added if necessary\n#original:(arr,c,w) -> output:[(arr,c,w),(arr,c,w)]\ndef split_and_pad(original, desiredLength, sampleRate):\n    output_buffer_length = int(desiredLength * sampleRate)\n    soundclip = original[0]\n    n_samples = len(soundclip)\n    total_length = n_samples / sampleRate #length of cycle in seconds\n    n_slices = int(math.ceil(total_length / desiredLength)) #get the minimum number of slices needed\n    samples_per_slice = n_samples // n_slices\n    src_start = 0 #Staring index of the samples to copy from the original buffer\n    output = [] #Holds the resultant slices\n    for i in range(n_slices):\n        src_end = min(src_start + samples_per_slice, n_samples)\n        length = src_end - src_start\n        copy = generate_padded_samples(soundclip[src_start:src_end], output_buffer_length)\n        output.append((copy, original[1], original[2]))\n        src_start += length\n    return output\n\ndef generate_padded_samples(source, output_length):\n    copy = np.zeros(output_length, dtype = np.float32)\n    src_length = len(source)\n    frac = src_length / output_length\n    if(frac < 0.5):\n        #tile forward sounds to fill empty space\n        cursor = 0\n        while(cursor + src_length) < output_length:\n            copy[cursor:(cursor + src_length)] = source[:]\n            cursor += src_length\n    else:\n        copy[:src_length] = source[:]\n    #\n    return copy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creates a copy of each time slice, but stretches or contracts it by a random amount\ndef gen_time_stretch(original, sample_rate, max_percent_change):\n    stretch_amount = 1 + np.random.uniform(-1,1) * (max_percent_change / 100)\n    (_, stretched) = resample(sample_rate, original, int(sample_rate * stretch_amount)) \n    return stretched\n\n#Same as above, but applies it to a list of samples\ndef augment_list(audio_with_labels, sample_rate, percent_change, n_repeats):\n    augmented_samples = []\n    for i in range(n_repeats):\n        addition = [(gen_time_stretch(t[0], sample_rate, percent_change), t[1], t[2] ) for t in audio_with_labels]\n        augmented_samples.extend(addition)\n    return augmented_samples\n\n#Takes a list of respiratory cycles, and splits and pads each cycle into fixed length buffers (determined by desiredLength(seconds))\n#Then takes the split and padded sample and transforms it into a mel spectrogram\n#VTLP_alpha_range = [Lower, Upper] (Bounds of random selection range), \n#VTLP_high_freq_range = [Lower, Upper] (-)\n#output:[(arr:float[],c:float_bool,w:float_bool),(arr,c,w)]\ndef split_and_pad_and_apply_mel_spect(original, desiredLength, sampleRate, VTLP_alpha_range = None, VTLP_high_freq_range = None, n_repeats = 1, dimension=13):\n    output = []\n    for i in range(n_repeats):\n        for d in original:\n            lst_result = split_and_pad(d, desiredLength, sampleRate) #Time domain\n            if( (VTLP_alpha_range is None) | (VTLP_high_freq_range is None) ):\n                #Do not apply VTLP\n                VTLP_params = None\n            else:\n                #Randomly generate VLTP parameters\n                alpha = np.random.uniform(VTLP_alpha_range[0], VTLP_alpha_range[1])\n                high_freq = np.random.uniform(VTLP_high_freq_range[0], VTLP_high_freq_range[1])\n                VTLP_params = (alpha, high_freq)\n            freq_result = [sample2MelSpectrum(d, sampleRate, dimension, VTLP_params) for d in lst_result] #Freq domain\n            output.extend(freq_result)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_all_training_samples(filenames, annotation_dict, root, target_rate, desired_length):\n    cycle_list = []\n    for file in filenames:\n        data = get_sound_samples(annotation_dict[file], file, root, target_rate)\n        cycles_with_labels = [(d[0], d[3], d[4]) for d in data[1:]]\n        cycle_list.extend(cycles_with_labels)\n    \n    #Sort into respective classes\n    no_labels = [c for c in cycle_list if ((c[1] == 0) & (c[2] == 0))]\n    c_only = [c for c in cycle_list if ((c[1] == 1) & (c[2] == 0))] \n    w_only = [c for c in cycle_list if ((c[1] == 0) & (c[2] == 1))]\n    c_w = [c for c in cycle_list if ((c[1] == 1) & (c[2] == 1))]\n    \n    #Count of labels across all cycles, actual recording time also follows similar ratios\n    #none:3642\n    #crackles:1864 \n    #wheezes:886\n    #both:506\n    none_train = no_labels\n    c_train = c_only\n    w_train = w_only\n    c_w_train = c_w\n    \n    #Training section (Data augmentation procedures)\n    #Augment w_only and c_w groups to match the size of c_only\n    #no_labels will be artifically reduced in the pipeline  later\n    w_stretch = w_train + augment_list(w_train, target_rate, 10 , 1) #\n    c_w_stretch = c_w_train + augment_list(c_w_train , target_rate, 10 , 1) \n    \n    #Split up cycles into sound clips with fixed lengths so they can be fed into a CNN\n    vtlp_alpha = [0.9,1.1]\n    vtlp_upper_freq = [3200,3800]\n    \n    train_none  = (split_and_pad_and_apply_mel_spect(none_train, desired_length, target_rate) +\n                   split_and_pad_and_apply_mel_spect(none_train, desired_length, target_rate, vtlp_alpha))\n    \n    train_c = (split_and_pad_and_apply_mel_spect(c_train, desired_length, target_rate) + \n               split_and_pad_and_apply_mel_spect(c_train, desired_length, target_rate, vtlp_alpha, vtlp_upper_freq, n_repeats = 3) ) #original samples + VTLP\n    \n    train_w = (split_and_pad_and_apply_mel_spect(w_stretch, desired_length, target_rate) + \n               split_and_pad_and_apply_mel_spect(w_stretch , desired_length, target_rate, vtlp_alpha , vtlp_upper_freq, n_repeats = 4)) #(original samples + time stretch) + VTLP\n    \n    train_c_w = (split_and_pad_and_apply_mel_spect(c_w_stretch, desired_length, target_rate) + \n                 split_and_pad_and_apply_mel_spect(c_w_stretch, desired_length, target_rate, vtlp_alpha , vtlp_upper_freq, n_repeats = 7)) #(original samples + time stretch * 2) + VTLP\n    \n    train_dict = {'none':train_none,'crackles':train_c,'wheezes':train_w, 'both':train_c_w}\n    \n    \n    return train_dict\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_sample_rate = 22000 \nsample_length_seconds = 5\nsample_dict = extract_all_training_samples(filenames, rec_annotations_dict, root, target_sample_rate, sample_length_seconds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_clips = sample_dict\nsample_height = training_clips['none'][0][0].shape[0]\nsample_width = training_clips['none'][0][0].shape[1]\nind = 1\nplt.figure(figsize = (10,10))\nplt.subplot(4,1,1)\nplt.imshow(training_clips['none'][ind][0].reshape(sample_height, sample_width))\nplt.title('None')\nplt.subplot(4,1,2)\nplt.imshow(training_clips['crackles'][ind][0].reshape(sample_height, sample_width))\nplt.title('Crackles')\nplt.subplot(4,1,3)\nplt.imshow(training_clips['wheezes'][ind][0].reshape(sample_height, sample_width))\nplt.title('Wheezes')\nplt.subplot(4,1,4)\nplt.imshow(training_clips['both'][ind][0].reshape(sample_height, sample_width))\nplt.title('Both')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}