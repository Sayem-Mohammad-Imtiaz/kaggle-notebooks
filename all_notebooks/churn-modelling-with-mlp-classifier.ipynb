{"cells":[{"metadata":{"id":"01wr4dnI2xF8","colab_type":"text"},"cell_type":"markdown","source":"# Imports"},{"metadata":{"id":"N2OSso5hztOK","colab_type":"code","outputId":"3623436a-24ea-4be2-bc7a-c9447fbd32d8","colab":{"base_uri":"https://localhost:8080/","height":54},"trusted":true},"cell_type":"code","source":"\n#the usual\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n#colored printing output\nfrom termcolor import colored\n\n#I/O\nimport io\nimport os\nimport requests\n\n#pickle\nimport pickle\n\n#math\nimport math\n\n#scipy\nfrom scipy import stats\n\n#sk learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import linear_model\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import preprocessing\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import learning_curve\nfrom itertools import combinations\nfrom mlxtend.feature_selection import ColumnSelector\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n#sns style\nimport seaborn as sns\n#sns.set_style(\"whitegrid\")\nsns.despine()\nsns.set_context(\"talk\") #larger display of plots axis labels etc..\nsns.set(style='darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"id":"UDQzcCMe219n","colab_type":"text"},"cell_type":"markdown","source":"# Functions"},{"metadata":{"id":"O-a0ULXW266j","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#check for missing value, zeros etc..\n#prints with colour output depending on set limit value\ndef print_color(text, values, limit=math.inf):\n  to_compare=False\n  if isinstance(values, float):\n    to_compare=(values>limit)\n  else:\n    to_compare=(values>limit).sum()    \n  if to_compare==True:\n    print(colored(text,color=\"magenta\",attrs=['reverse', 'blink'])+colored(values,color=\"magenta\"))\n  else:\n    print(colored(text,color=\"green\")+colored(values,color=\"green\"))\n        \ndef CheckValues(X, Y, detail=False):\n  print(\"---- in X-----\")\n  print (\"shape:\", train_x.shape)\n  if detail==True:  \n    print_color(\"percentage of Nan:\\n\", values=X.isna().mean().round(4)*100,limit=5.)\n    print_color(\"percentage of zeros:\\n\", values=X.eq(0).mean().round(4)*100,limit=5.)\n    print_color(\"percentage of negative:\\n\", values=X.lt(0).mean().round(4)*100,limit=5.)\n  \n  print_color(\"percentage of Nan for entire df:\",values=round(X.isna().mean().mean()*100,2),limit=5.)\n  print_color(\"percentage of zeros for entire df:\",values=round(X.eq(0).mean().mean()*100,2),limit=5.)\n  print_color(\"percentage of negative values for entire df:\",values=round(X.lt(0).mean().mean()*100,2),limit=5.)\n  \n  print(\"---- in Y-----\")\n  print(\"shape:\", Y.shape)\n  if detail==True:\n    print_color(\"percentage of Nan:\\n\", values=Y.isna().mean().round(4)*100,limit=5.)\n    print_color(\"percentage of zeros:\\n\", values=Y.eq(0).mean().round(4)*100,limit=5.)\n    print_color(\"percentage of negative:\\n\", values=Y.lt(0).mean().round(4)*100,limit=5.)\n  \n  print_color(\"percentage of Nan for entire df:\",values=round(Y.isna().mean().mean()*100,2),limit=5.)\n  print_color(\"percentage of zeros for entire df:\",values=round(Y.eq(0).mean().mean()*100,2),limit=5.)\n  print_color(\"percentage of negative values for entire df:\",values=round(Y.lt(0).mean().mean()*100,2),limit=5.)\n  \n\n\n#remove missing values replace by mean\ndef RemoveMissVal(X,Y, verbose=False):\n  if(verbose):\n    print(\"percentage of Nan in X before removal:\\n\",X.isna().mean().round(4)*100)\n    print(\"percentage of Nan in Y before removal:\\n\",Y.isna().mean().round(4)*100)\n  X_clean=X.fillna(X.mean())\n  Y_clean=Y.fillna(Y.mean())\n  \n  if(verbose):\n    print(\"-----DONE------------\")\n    print(\"percentage of Nan in X after removal:\\n\",X_clean.isna().mean().round(4)*100)\n    print(\"percentage of Nan in Y after removal:\\n\",Y_clean.isna().mean().round(4)*100)\n  \n  return X_clean, Y_clean\n\n#remove outliers (values larger than std_dev will be removed)\ndef RemoveSigma(X,Y,std_dev,verbose=False):\n    if(verbose):\n      print(\"---- before----\")\n      print(X.shape)\n      print(Y.shape)\n    #print(np.abs(stats.zscore(X)))\n    X_cut = X[(np.abs(stats.zscore(X)) < float(std_dev)).all(axis=1)]\n    Y_cut = Y[(np.abs(stats.zscore(X)) < float(std_dev)).all(axis=1)]\n    if(verbose):\n      print(\"---- after----\")\n      print(X_cut.shape)\n      print(Y_cut.shape)\n    return X_cut, Y_cut\n  \ndef ScatterPlots(df, n_plots=3):\n  fig, axes = plt.subplots(1, n_plots)\n  i=0\n  for key, value in df.iloc[:, :-1].iteritems(): \n    print(key) \n    df.plot(kind=\"scatter\",x=key, y=\"Survived\",color=\"orange\",ax=axes[i],figsize=(20,10))\n    i+=1\n\n#check for missing value, zeros etc..\n#prints with colour output depending on set limit value\ndef print_color(text, values, limit=math.inf):\n  to_compare=False\n  if isinstance(values, float):\n    to_compare=(values>limit)\n  else:\n    to_compare=(values>limit).sum()    \n  if to_compare==True:\n    print(colored(text,color=\"magenta\",attrs=['reverse', 'blink'])+colored(values,color=\"magenta\"))\n  else:\n    print(colored(text,color=\"green\")+colored(values,color=\"green\"))\n        \ndef CheckValues(X, Y, detail=False):\n  print(\"---- in X-----\")\n  print (\"shape:\", train_x.shape)\n  if detail==True:  \n    print_color(\"percentage of Nan:\\n\", values=X.isna().mean().round(4)*100,limit=5.)\n    print_color(\"percentage of zeros:\\n\", values=X.eq(0).mean().round(4)*100,limit=5.)\n    print_color(\"percentage of negative:\\n\", values=X.lt(0).mean().round(4)*100,limit=5.)\n  \n  print_color(\"percentage of Nan for entire df:\",values=round(X.isna().mean().mean()*100,2),limit=5.)\n  print_color(\"percentage of zeros for entire df:\",values=round(X.eq(0).mean().mean()*100,2),limit=5.)\n  print_color(\"percentage of negative values for entire df:\",values=round(X.lt(0).mean().mean()*100,2),limit=5.)\n  \n  print(\"---- in Y-----\")\n  print(\"shape:\", Y.shape)\n  if detail==True:\n    print_color(\"percentage of Nan:\\n\", values=Y.isna().mean().round(4)*100,limit=5.)\n    print_color(\"percentage of zeros:\\n\", values=Y.eq(0).mean().round(4)*100,limit=5.)\n    print_color(\"percentage of negative:\\n\", values=Y.lt(0).mean().round(4)*100,limit=5.)\n  \n  print_color(\"percentage of Nan for entire df:\",values=round(Y.isna().mean().mean()*100,2),limit=5.)\n  print_color(\"percentage of zeros for entire df:\",values=round(Y.eq(0).mean().mean()*100,2),limit=5.)\n  print_color(\"percentage of negative values for entire df:\",values=round(Y.lt(0).mean().mean()*100,2),limit=5.)\n  \n\n\n#remove missing values replace by mean\ndef RemoveMissVal(X,Y, verbose=False):\n  if(verbose):\n    print(\"percentage of Nan in X before removal:\\n\",X.isna().mean().round(4)*100)\n    print(\"percentage of Nan in Y before removal:\\n\",Y.isna().mean().round(4)*100)\n  X_clean=X.fillna(X.mean())\n  Y_clean=Y.fillna(Y.mean())\n  \n  if(verbose):\n    print(\"-----DONE------------\")\n    print(\"percentage of Nan in X after removal:\\n\",X_clean.isna().mean().round(4)*100)\n    print(\"percentage of Nan in Y after removal:\\n\",Y_clean.isna().mean().round(4)*100)\n  \n  return X_clean, Y_clean\n\n#remove outliers (values larger than std_dev will be removed)\ndef RemoveSigma(X,Y,std_dev,verbose=False):\n    if(verbose):\n      print(\"---- before----\")\n      print(X.shape)\n      print(Y.shape)\n    #print(np.abs(stats.zscore(X)))\n    X_cut = X[(np.abs(stats.zscore(X)) < float(std_dev)).all(axis=1)]\n    Y_cut = Y[(np.abs(stats.zscore(X)) < float(std_dev)).all(axis=1)]\n    if(verbose):\n      print(\"---- after----\")\n      print(X_cut.shape)\n      print(Y_cut.shape)\n    return X_cut, Y_cut\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"hPTfrYra3JJe","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def rmsle(y_pred, y_test) : \n    #clip zero values\n    assert len(y_test) == len(y_pred)\n    return np.sqrt(np.mean((np.log(list(np.asarray(y_pred).clip(min=0) + 1)) - np.log(list(np.asarray(y_test).clip(min=0) + 1)))**2))\n  \n  \ndef cross_val_predict(train_X,train_y, model, k_fold=5, use_scaling=True, Verbose=False, score_rmsle=True):\n    cv = KFold(n_splits = k_fold)\n    test_y_overall = []\n    predict_y_overall = []\n    train_X=train_X.values\n    train_y=train_y.values    \n    for train_index, test_index in cv.split(train_X):\n      train_X_fi, train_y_fi = train_X[train_index], train_y[train_index]\n      test_X_fi, test_y_fi = train_X[test_index], train_y[test_index]\n      \n      #if train_X, Y are not np arrays use thise:\n      #train_X_fi, train_y_fi = train_X.iloc[train_index], train_y.iloc[train_index]\n      #test_X_fi, test_y_fi = train_X.iloc[test_index], train_y.iloc[test_index]\n\n      #scale, train the model and evaluate it\n      scaler = StandardScaler()\n      train_scaled = scaler.fit_transform(train_X_fi)\n      test_scaled  = scaler.fit_transform(test_X_fi)\n      \n      if use_scaling:\n        model.fit(train_scaled, train_y_fi)\n        prediction = model.predict(test_scaled)\n      else:\n        model.fit(train_X_fi, train_y_fi)\n        prediction = model.predict(test_X_fi)\n      \n            \n      #store the target var and the prediction for later analysis\n      test_y_overall.extend(test_y_fi)\n      predict_y_overall.extend(prediction)     \n    \n      cross_val_error_rmsle = rmsle(predict_y_overall, test_y_overall)\n      cross_val_error_r2 = r2_score(predict_y_overall, test_y_overall)    \n      \n     #calculate and pring both rmsle and r2 scores, return only one of them \n    if(Verbose==True):\n      print(\"cross_val_error_rmsle is:\",cross_val_error_rmsle)\n      print(\"cross_val_error_r2 is:\",cross_val_error_r2)\n      \n    if score_rmsle:\n      cross_val_error=cross_val_error_rmsle\n    else:\n      cross_val_error=cross_val_error_r2\n      \n    return cross_val_error\n  \n#plot validation curve\ndef PlotValidationCurve(train_scores,valid_scores,param_range,param_name,logx=False,verbose=False):\n  train_scores_mean = np.mean(train_scores, axis=1)\n  train_scores_std = np.std(train_scores, axis=1)\n  valid_scores_mean = np.mean(valid_scores, axis=1)\n  valid_scores_std = np.std(valid_scores, axis=1)\n  if verbose==True:\n    print(\"train_scores_mean:\",train_scores_mean)\n    print(\"valid_scores_mean:\",valid_scores_mean)\n  plt.figure(figsize=(10, 5), dpi=80)\n  plt.title(\"Validation curve\")\n  plt.plot(param_range, train_scores_mean, label=\"Training score\",\n             color=\"orange\", lw=2,marker=\".\")  \n  plt.fill_between(param_range, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2,\n                 color=\"orange\", lw=0)\n    \n  plt.plot(param_range, valid_scores_mean, label=\"Cross-validation score\",\n             color=\"black\", lw=2)\n  plt.fill_between(param_range, valid_scores_mean - valid_scores_std,\n                 valid_scores_mean + valid_scores_std, alpha=0.2,\n                 color=\"black\", lw=0)\n  if(logx==True):\n    plt.xscale('log')\n  plt.ylim(-.2, 1.1)\n  plt.xlabel(str(param_name))\n  plt.ylabel(\"score\")\n  plt.ylabel(\"Score\")\n  \n  plt.legend(loc=0)\n  \n  \n  \n  \n  \ndef PlotLearningCurve(train_sizes,train_scores,valid_scores,param_range,logx=False,verbose=False,ymin=0,ymax=1.):\n#plot validation curve\n  train_scores_mean = np.mean(train_scores, axis=1)\n  train_scores_std = np.std(train_scores, axis=1)\n  valid_scores_mean = np.mean(valid_scores, axis=1)\n  valid_scores_std = np.std(valid_scores, axis=1)\n  if verbose==True:\n    print(\"train_scores_mean:\",train_scores_mean)\n    print(\"valid_scores_mean:\",valid_scores_mean)\n  plt.figure(figsize=(10, 5), dpi=80)\n  plt.title(\"Learning curve\")\n  plt.grid()\n  plt.plot(train_sizes, train_scores_mean, label=\"Training score\",\n             color=\"red\", lw=2,marker=\".\")  \n  plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2,\n                 color=\"red\", lw=0)\n    \n  plt.plot(train_sizes, valid_scores_mean, label=\"Cross-validation score\",\n             color=\"navy\", lw=2)\n  plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,\n                 valid_scores_mean + valid_scores_std, alpha=0.2,\n                 color=\"navy\", lw=0)\n  if(logx==True):\n    plt.xscale('log')\n  plt.ylim(ymin, ymax)\n  plt.xlabel(\"training set size\")\n  plt.ylabel(\"score\")\n  plt.ylabel(\"Score\")\n  \n  plt.legend(loc=0)\n  \n  \n\n  \n#Calculates the best model with all combinations fo features, of up to max_size features of X.\n#TODO: need to check implementation with rmsle\ndef best_subset(estimator, X, y, max_size=8, cv=5, use_rmsle=False, verbose=False):\n  n_features = X.shape[1]\n  subsets = (combinations(range(n_features), k + 1) \n               for k in range(min(n_features, max_size)))\n  best_size_subset = []\n  for subsets_k in subsets:  # for each list of subsets of the same size      \n      best_score = -np.inf\n      best_subset = None\n      for subset in subsets_k: # for each subset\n          estimator.fit(X.iloc[:, list(subset)], y)\n           # get the subset with the best score among subsets of the same size\n          score = estimator.score(X.iloc[:, list(subset)], y)         \n          #score=rmsle(X.iloc[:, list(subset)].values, y.values) #TODO: this needs to be fixed\n          if score > best_score:\n                best_score, best_subset = score, subset      \n        # first store the best subset of each size\n      best_size_subset.append(best_subset)\n\n    # compare best subsets of each size\n  best_score = -np.inf\n  best_subset = None\n  list_scores = []\n  for subset in best_size_subset:\n      if(use_rmsle):\n        score=cross_val_predict(X.iloc[:, list(subset)].astype(float), y, estimator, Verbose=verbose, score_rmsle=True) #home made scorer with rmsle\n      else:\n        score = cross_val_score(estimator, X.iloc[:, list(subset)], y, cv=cv).mean()\n      list_scores.append(score)\n      if score > best_score:\n        best_score, best_subset = score, subset\n  return best_subset, best_score, best_size_subset, list_scores","execution_count":null,"outputs":[]},{"metadata":{"id":"BTFgsl3aGPNQ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"id":"7m-F6Q7lRFSq","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def conf_mat(train_y,train_x):\n  # Creating the confusion matrix:\n  lr_cm = confusion_matrix(train_y, mlp.predict(train_x))\n\n  #Visualization:\n  f, ax = plt.subplots(figsize=(5,5))\n  sns.heatmap(lr_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax, cmap='inferno')\n  plt.title('Logistic Regression Classification Confusion Matrix')\n  plt.xlabel('y_pred')\n  plt.ylabel('y_truth')\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"M9xjDYugDbGc","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def undersample(train_x, train_y, print_val=False):\n  exit_indices= np.array(train_y[train_y.Exited==1].index)\n  stayed_indices = np.array(train_y[train_y.Exited==0].index)\n  if(print_val):\n    print(exit_indices.shape)\n    print(stayed_indices.shape)\n  stayed_indices_undersampled = np.random.choice(stayed_indices, len(exit_indices), replace=False)\n  if(print_val):\n    print(\"shape of stayed undersampled\",stayed_indices_undersampled.shape)\n    print(\"shape of exited\",exit_indices.shape)\n  undersampled_indices = np.concatenate([stayed_indices_undersampled, exit_indices]) \n  #print(undersampled_indices.shape)\n\n  train_x_us = train_x.loc[undersampled_indices]\n  train_y_us = train_y.loc[undersampled_indices]\n  return train_x_us, train_y_us","execution_count":null,"outputs":[]},{"metadata":{"id":"aywDVYN23UCt","colab_type":"text"},"cell_type":"markdown","source":"# load dataset"},{"metadata":{"id":"TGmQfHDa3XjT","colab_type":"code","outputId":"884527f3-ca53-40db-f1c8-1de4f0455100","colab":{"base_uri":"https://localhost:8080/","height":71},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"Ao06VAqW4GnJ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_data= pd.read_csv('../input/Churn_Modelling.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"BhNALsdg4eTe","colab_type":"text"},"cell_type":"markdown","source":"#EDA "},{"metadata":{"id":"4x_LfsUI4ku2","colab_type":"code","outputId":"cf20fd8d-7083-47e1-e8de-8df43e4697eb","colab":{"base_uri":"https://localhost:8080/","height":284},"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"luTcxcP-4rXS","colab_type":"code","outputId":"c6b88997-2bdf-46c5-aa65-03bd26dafc57","colab":{"base_uri":"https://localhost:8080/","height":447},"trusted":true},"cell_type":"code","source":"train_data.hist(bins=\"auto\",figsize=(12,7),grid=False);\n#train_data.hist(figsize=(20,15),bins=100,color=\"orange\",log=True,layout=(11,1))","execution_count":null,"outputs":[]},{"metadata":{"id":"SURBUjwD5dJ3","colab_type":"code","outputId":"976dcfc5-b7b8-473f-f167-ec2c11d42048","colab":{"base_uri":"https://localhost:8080/","height":340},"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"9SU0hwkb6aZJ","colab_type":"code","outputId":"375d9102-ba15-4a01-9d0b-3671dbaf5fd1","colab":{"base_uri":"https://localhost:8080/","height":1694},"trusted":true},"cell_type":"code","source":"sns.pairplot(train_data,hue=\"Exited\",height=2)","execution_count":null,"outputs":[]},{"metadata":{"id":"hlXJNwYVV6Jb","colab_type":"code","outputId":"12f9c9f2-f247-4774-c846-92a8a628b229","colab":{"base_uri":"https://localhost:8080/","height":1133},"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (18,18)); ax = fig.gca()\n#one_hot_churn = pd.get_dummies(churn, columns = ['Gender', 'Geography', 'HasCrCard', 'IsActiveMember'])\nsns.heatmap(train_data.corr(), annot = True, vmin= -0.5, vmax = 0.5, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"id":"8JpqbCBm7EEP","colab_type":"code","outputId":"8af10c23-7e96-42f0-e90c-7cf8ef9c1037","colab":{"base_uri":"https://localhost:8080/","height":284},"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"kGPQCizm7cln","colab_type":"code","outputId":"2a90dc02-5f9a-4c4d-da56-ea4a3ffed8f1","colab":{"base_uri":"https://localhost:8080/","height":478},"trusted":true},"cell_type":"code","source":"train_data[\"Gender\"].value_counts().plot(kind='pie',figsize= (8,8));","execution_count":null,"outputs":[]},{"metadata":{"id":"DmP4vDxF75CK","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def bar_chart(feature,input_df):\n    Exited = input_df[input_df['Exited']==1][feature].value_counts()\n    Stayed = input_df[input_df['Exited']==0][feature].value_counts()\n    df = pd.DataFrame([Exited,Stayed])\n    df.index = ['Exited','Stayed']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{"id":"yMYEMeCJ8HEh","colab_type":"code","outputId":"bc1e56ee-602c-44ba-f26b-d2f63105fcb2","colab":{"base_uri":"https://localhost:8080/","height":357},"trusted":true},"cell_type":"code","source":"bar_chart(\"Gender\",train_data)\n#train_data[\"Gender\"].value_counts().plot(kind='pie',figsize= (8,8));","execution_count":null,"outputs":[]},{"metadata":{"id":"QQ8U9HO58njp","colab_type":"code","outputId":"4a7b765c-0851-4b9f-b47e-d4fba18bd461","colab":{"base_uri":"https://localhost:8080/","height":357},"trusted":true},"cell_type":"code","source":"bar_chart(\"IsActiveMember\",train_data)","execution_count":null,"outputs":[]},{"metadata":{"id":"QqUTyktT9Oc8","colab_type":"text"},"cell_type":"markdown","source":"Males tend to stay a bit more also active members, no strong bias for geography"},{"metadata":{"id":"joPpbv-99-8E","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"mlgtU67h8yjT","colab_type":"code","outputId":"2da93886-3ac4-4a43-cbd0-7e85754667d3","colab":{"base_uri":"https://localhost:8080/","height":478},"trusted":true},"cell_type":"code","source":"train_data[\"Geography\"].value_counts().plot(kind='pie',figsize= (8,8));","execution_count":null,"outputs":[]},{"metadata":{"id":"DaaZ69MT9z5Q","colab_type":"code","outputId":"83ad0814-7847-4e8b-e2a2-9551b7a26252","colab":{"base_uri":"https://localhost:8080/","height":357},"trusted":true},"cell_type":"code","source":"bar_chart(\"Geography\",train_data)","execution_count":null,"outputs":[]},{"metadata":{"id":"9TmIfNEdEwjP","colab_type":"code","outputId":"18349d10-4679-4f08-a398-de05ba044230","colab":{"base_uri":"https://localhost:8080/","height":284},"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"x0XLkPBm-Aua","colab_type":"text"},"cell_type":"markdown","source":"* Males tend to stay a bit more, so do active members, no strong bias for geography\n* quite strong correlation with age and balance\n* hot encode Gender and Geography\n* drop  Rownumber , customer Id surname,"},{"metadata":{"id":"LnJ3Ka3z7FYr","colab_type":"code","outputId":"e5c45979-024f-4d79-e689-af8d7f1f2b8d","colab":{"base_uri":"https://localhost:8080/","height":284},"trusted":true},"cell_type":"code","source":"#train_data_red=train_data.drop([\"RowNumber\",\"CreditScore\", \"CustomerId\", \"Surname\", \"Tenure\",\"Salary\"], axis=1)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"LQgFjc5fAqLk","colab_type":"code","outputId":"10464ba1-db07-4d77-f075-735daeb5d8c0","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"train_data_red=train_data.drop([\"RowNumber\",\"CreditScore\", \"CustomerId\", \"Surname\", \"Tenure\",\"EstimatedSalary\"], axis=1)\ntrain_data_red.head()\n#train_data=train_data_red","execution_count":null,"outputs":[]},{"metadata":{"id":"qN-2L1iNAS8k","colab_type":"code","outputId":"e0304383-7f1d-4c75-83d3-2066c5505ee8","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"train_data_red['Gender'] = train_data['Gender'].map({'Female': 1, 'Male': 0})\ntrain_data_red.head()\n#CheckValues(train_x_clean,train_y_clean,detail=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"SDmLi5N7Afbn","colab_type":"code","outputId":"737a013a-07d7-4f06-c38c-6e1af362e4c9","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"one_hot = pd.get_dummies(train_data_red['Geography'])\ntrain_hot_geo = train_data_red.drop('Geography',axis = 1)\n#one_hot[\"Spain\"]\n#train_hot_geo=train_data_red.join(one_hot[\"Spain\"],one_hot[\"France\"],one_hot[\"Germany\"])\ntrain_hot_geo=train_hot_geo.join(one_hot[\"Spain\"])\ntrain_hot_geo=train_hot_geo.join(one_hot[\"France\"])\ntrain_hot_geo=train_hot_geo.join(one_hot[\"Germany\"])\ntrain_hot_geo.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"KJphg2LBD4jM","colab_type":"text"},"cell_type":"markdown","source":"keep this DF"},{"metadata":{"id":"TfggFPITDfKt","colab_type":"code","outputId":"a18002d7-19b9-42df-9ef9-91bab854833a","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"train_x = train_hot_geo.drop(columns=[\"Exited\"])#drop the label\ntrain_x.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"SreLmvFlEVPJ","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"train_y=train_hot_geo[[\"Exited\"]]","execution_count":null,"outputs":[]},{"metadata":{"id":"LlRCweL7EeLv","colab_type":"code","outputId":"5817e3c5-94f5-4d84-c12d-8acc8493de4d","colab":{"base_uri":"https://localhost:8080/","height":85},"trusted":true},"cell_type":"code","source":"print(train_x.shape)\nprint(train_y.shape)\nprint(type(train_y))\nprint(type(train_x))","execution_count":null,"outputs":[]},{"metadata":{"id":"Ev8UW4U4Ei3y","colab_type":"code","outputId":"60f29d04-0932-4bd4-dc91-9a16cc3aaa43","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"df_train=pd.concat([train_x, train_y], axis=1)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"70EBQeQmFAUJ","colab_type":"code","outputId":"c8af58fb-ec45-4bdc-acf0-0850d4d0c9e0","colab":{"base_uri":"https://localhost:8080/","height":901},"trusted":true},"cell_type":"code","source":"CheckValues(train_x,train_y,detail=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"UKjbSHybIiqA","colab_type":"code","outputId":"26d8393d-469c-4a57-f667-9f49980f63dd","colab":{"base_uri":"https://localhost:8080/","height":1052},"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize = (18,18)); ax = fig.gca()\n#one_hot_churn = pd.get_dummies(churn, columns = ['Gender', 'Geography', 'HasCrCard', 'IsActiveMember'])\nsns.heatmap(df_train.corr(), annot = True, vmin= -0.5, vmax = 0.5, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"id":"kj2ayx-NmRTY","colab_type":"text"},"cell_type":"markdown","source":"# First dirty linear regression\n1. Select a small number of features which need small or no preparation\n2. Train a first MLPClassifier with Scikit learn\n3. Look at the predicted values (to see if they make sense)"},{"metadata":{"id":"4OrgT-sShGHF","colab_type":"code","outputId":"ac99c682-84c8-4087-e1a8-ae197bd1b341","colab":{"base_uri":"https://localhost:8080/","height":224},"trusted":true},"cell_type":"code","source":"\ntrain_x.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"id":"aiMwb0ywFUCv","colab_type":"code","outputId":"9a7d001d-5e8f-4605-9cc2-e0da896c5be4","colab":{"base_uri":"https://localhost:8080/","height":976},"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\n\nmlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, alpha=1e-4,\n                    solver='sgd', verbose=0, tol=1e-4, random_state=1,\n                    learning_rate_init=.1)\n\nmlp.fit(train_x, train_y)\n\nprint(\"Training set score: %f\" % mlp.score(train_x, train_y))\nprint (\"cross_val F1 score:\",cross_val_score(mlp, train_x, train_y, scoring=\"f1_macro\",cv=3)) \n\ncm = confusion_matrix(train_y, mlp.predict(train_x))\nprint(cm)\n\nconf_mat(train_y,train_x)\n#plt.hist(mlp.predict_proba(train_x)[:,1])\n#plt.hist(mlp.predict_proba(train_x)[:,0])\n\nprint(classification_report(train_y,mlp.predict(train_x)))\n","execution_count":null,"outputs":[]},{"metadata":{"id":"CKotojpZHRZR","colab_type":"text"},"cell_type":"markdown","source":"# with undersampling"},{"metadata":{"id":"Y8GKVBGYGD7H","colab_type":"code","outputId":"8696375a-1315-412e-c300-51881ed0ca86","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"train_x.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"s1dbBEKIOxMX","colab_type":"code","outputId":"ae019a4b-a000-4edc-b5e4-f72d1f014c2c","colab":{"base_uri":"https://localhost:8080/","height":85},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"zl2vDYtEF7G2","colab_type":"text"},"cell_type":"markdown","source":"try to improve the F1 score "},{"metadata":{"id":"a0ERKvS7QQF-","colab_type":"code","outputId":"b9d42b70-e20d-4f62-a716-071d82358f32","colab":{"base_uri":"https://localhost:8080/","height":809},"trusted":true},"cell_type":"code","source":"train_x_us, train_y_us= undersample(train_x,train_y,print_val=True)\n\nmlp = MLPClassifier(hidden_layer_sizes=(5,), max_iter=1000,\n                    solver='adam', verbose=0, tol=1e-4, random_state=1,\n                    learning_rate_init=.001,early_stopping=True)\n\nmlp.fit(train_x_us, train_y_us)\nprint(\"Training set score: %f\" % mlp.score(train_x_us, train_y_us))\ncv_score=cross_val_score(mlp, train_x_us, train_y_us, scoring=\"f1_macro\",cv=3).mean()\n#print (\"cross_val F1 score:\",cross_val_score(mlp, train_x_us, train_y_us, scoring=\"f1_macro\",cv=3)) \nprint (\"cross_val F1 score:\",cv_score) \ncm = confusion_matrix(train_y_us, mlp.predict(train_x_us))\n\nprint(cm)\n\nconf_mat(train_y_us,train_x_us)\n#plt.hist(mlp.predict_proba(train_x_us)[:,1])\n#plt.hist(mlp.predict_proba(train_x_us)[:,0])\nprint(\"the mean F1 score from 3-fold cross-validation is:\",cv_score)\nprint(classification_report(train_y_us,mlp.predict(train_x_us)))","execution_count":null,"outputs":[]},{"metadata":{"id":"-iF8O6WMZW6c","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#f1_macro\n\n#params = {'hidden_layer_sizes': [i for i in range(95,105)],\n#              'activation': ['relu'],\n#              'solver': ['adam',\"sgd\"],\n#              'learning_rate': ['constant'],\n#              'learning_rate_init': [0.001],\n#              'power_t': [0.5],\n#              'alpha': [0.0001],\n#              'max_iter': [1000],\n#              'early_stopping': [False,True],\n#              'warm_start': [False]}\n\n#grid = GridSearchCV(estimator=lin_reg,param_grid=params, cv=5, n_jobs=-1,scoring=\"r2\")\n\n#grid = GridSearchCV(mlp, param_grid=params, scoring=\"f1_macro\",\n#                   cv=5, pre_dispatch='2*n_jobs')\n#grid.fit(train_x, train_y)\n#print('Best parameters:', grid.best_params_)\n#print('Best performance:', grid.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"uyLCbMMRprCt","colab_type":"text"},"cell_type":"markdown","source":"#feature engineering"},{"metadata":{"id":"Mj8h2jR6ptrr","colab_type":"code","outputId":"f59cc262-c8de-4038-9729-c47ab3a7174b","colab":{"base_uri":"https://localhost:8080/","height":258},"trusted":true},"cell_type":"code","source":"a = sns.FacetGrid(df_train, hue = 'Exited', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , df_train['Age'].max()))\na.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"id":"P9jgcCiHp8bb","colab_type":"code","outputId":"de7da84f-6dd5-453f-cb05-5306e82b2fed","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"uVh-sAQwUpJe","colab_type":"code","outputId":"40db11c5-4697-448a-da3b-066ea2ad1267","colab":{"base_uri":"https://localhost:8080/","height":318},"trusted":true},"cell_type":"code","source":"df_train.hist(\"Age\")","execution_count":null,"outputs":[]},{"metadata":{"id":"2-Tl0tZiVdJ0","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"df_ones=df_train.loc[(df_train.Exited == 1)]\ndf_zeros=df_train.loc[(df_train.Exited == 0)]","execution_count":null,"outputs":[]},{"metadata":{"id":"YjMWRd__WB9V","colab_type":"code","outputId":"45fef6ed-0e90-496e-868f-222b01a3d28c","colab":{"base_uri":"https://localhost:8080/","height":585},"trusted":true},"cell_type":"code","source":"df_ones.hist(\"Age\")\ndf_zeros.hist(\"Age\")","execution_count":null,"outputs":[]},{"metadata":{"id":"W7IDyd8TWQk8","colab_type":"code","outputId":"148062af-b7c8-44b2-8fd4-48478f2febab","colab":{"base_uri":"https://localhost:8080/","height":51},"trusted":true},"cell_type":"code","source":"print(df_ones[\"Age\"].values.mean())\nprint(df_ones[\"Age\"].values.std())","execution_count":null,"outputs":[]},{"metadata":{"id":"dvL2KruiX_YT","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"def gaussian(x, mu, sig):\n    return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n\n#gaussian(50,44,9.7)","execution_count":null,"outputs":[]},{"metadata":{"id":"iSqXcuxRajyK","colab_type":"code","outputId":"b204188e-352e-4b80-a631-be7e10067df4","colab":{"base_uri":"https://localhost:8080/","height":51},"trusted":true},"cell_type":"code","source":"print(df_zeros[\"Age\"].values.mean())\nprint(df_zeros[\"Age\"].values.std())","execution_count":null,"outputs":[]},{"metadata":{"id":"HhwpGLuAXG-X","colab_type":"code","outputId":"3d0f5c68-1a03-49ff-a35e-707b355ec9a2","colab":{"base_uri":"https://localhost:8080/","height":204},"trusted":true},"cell_type":"code","source":"#df_train=df_train.drop(columns=[\"gauss_age\",\"sqr_age\",\"tran_age\"])\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"cfEvLCDQa10b","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"df_train[\"Age_stay\"]=df_train[\"Age\"].apply(lambda x: gaussian(x,44,9.7))","execution_count":null,"outputs":[]},{"metadata":{"id":"EBevcuzxbkBv","colab_type":"code","outputId":"4c94a7b3-d70b-466b-e458-20d9008d2442","colab":{"base_uri":"https://localhost:8080/","height":224},"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Kko5jbQZbpWK","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"df_train[\"Age_exit\"]=df_train[\"Age\"].apply(lambda x: gaussian(x,37,10))","execution_count":null,"outputs":[]},{"metadata":{"id":"BZ0ZXkMSb1Jl","colab_type":"code","outputId":"673096d2-324e-4916-dc7f-0101e3444bb8","colab":{"base_uri":"https://localhost:8080/","height":1599},"trusted":true},"cell_type":"code","source":"df_train.head(50)","execution_count":null,"outputs":[]},{"metadata":{"id":"OxVmSUcay-9H","colab_type":"code","outputId":"4229495b-cef1-48cf-a65c-bcdd8dacacb8","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true},"cell_type":"code","source":"#from sklearn.mixture import GaussianMixture \n#gmm = GaussianMixture(n_components = 2) \n#gmm.fit(df_train[['Age']].values)\n#labels = gmm.predict(df_train[['Age']].values) \n#print(labels)","execution_count":null,"outputs":[]},{"metadata":{"id":"qlXNWAL10JBa","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"#a = sns.FacetGrid(df_train, hue = 'Exited', aspect=3 )\n#a.map(sns.kdeplot, 'gauss_age', shade= True)\n#a.set(xlim=(df_train['sqr_age'].min(), df_train['sqr_age'].max()))\n#a.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"id":"7yn2IA570fyL","colab_type":"code","outputId":"8b49918c-0632-44f6-f764-1309f6bf51e6","colab":{"base_uri":"https://localhost:8080/","height":51},"trusted":true},"cell_type":"code","source":"train_x=df_train.drop(columns=[\"Exited\"])\ntrain_y=df_train[[\"Exited\"]]\nprint(train_x.shape)\nprint(train_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"G76kcKm62Vl6","colab_type":"code","outputId":"84fecd30-6140-4471-b795-9d5fe673d2d8","colab":{"base_uri":"https://localhost:8080/","height":224},"trusted":true},"cell_type":"code","source":"train_x.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Xci4OZ6rSiw4","colab_type":"text"},"cell_type":"markdown","source":"With feature scaling"},{"metadata":{"id":"2YFos7Q14UxG","colab_type":"code","outputId":"3b0a13dd-963c-4f12-e04c-9e6b267d9486","colab":{"base_uri":"https://localhost:8080/","height":338},"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaled_features=scaler.fit_transform(train_x)\ntrain_x_scaled = pd.DataFrame(scaled_features, index=train_x.index, columns=train_x.columns)\ntrain_x_scaled.head()\n","execution_count":null,"outputs":[]},{"metadata":{"id":"2nUeFMd_BSWd","colab_type":"code","colab":{},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"id":"39rWXH8L63Ff","colab_type":"code","outputId":"b4ea39a7-25ee-4c50-c239-3ac65b3bc284","colab":{"base_uri":"https://localhost:8080/","height":1004},"trusted":true},"cell_type":"code","source":"\n#!should normalize after the splitting!\nx_train, x_test, y_train, y_test = train_test_split(train_x_scaled, train_y, test_size=0.2, random_state=42)\n\ntrain_x_us, train_y_us= undersample(x_train,y_train,print_val=True)\n\nmlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000,\n                    solver='adam', verbose=0, tol=1e-4, random_state=1,\n                    learning_rate_init=.01,early_stopping=True)\n\nmlp.fit(train_x_us, train_y_us)\nprint(mlp.predict(train_x_us))\nprint(\"Training set score: %f\" % mlp.score(train_x_us, train_y_us))\nprint (\"cross_val F1 score:\",cross_val_score(mlp, train_x_us, train_y_us, scoring=\"f1_macro\",cv=3)) \n\ncm = confusion_matrix(train_y_us, mlp.predict(train_x_us))\n\nprint(cm)\n\nconf_mat(train_y_us,train_x_us)\n\nprint(\"---- report for train ----- \")\nprint(classification_report(train_y_us,mlp.predict(train_x_us)))\n\nprint(\"---- report for test ----- \")\nprint(classification_report(y_test,mlp.predict(x_test)))\n\n#plt.hist(mlp.predict_proba(train_x_us)[:,1])\n#plt.hist(mlp.predict_proba(train_x_us)[:,0])","execution_count":null,"outputs":[]},{"metadata":{"id":"OCqvyINgg-Ds","colab_type":"code","outputId":"59ab0884-35a4-41c0-f14e-ea2cbb936d16","colab":{"base_uri":"https://localhost:8080/","height":318},"trusted":true},"cell_type":"code","source":"#train_y_us,mlp.predict(train_x_us))\ntrain_x.hist(\"Age\")","execution_count":null,"outputs":[]},{"metadata":{"id":"g5cx6VCsi9pR","colab_type":"code","outputId":"e15f6b45-70dc-41b5-9b74-bce3955ed6d4","colab":{"base_uri":"https://localhost:8080/","height":318},"trusted":true},"cell_type":"code","source":"x_test.hist(\"Age\")","execution_count":null,"outputs":[]},{"metadata":{"id":"Mhz9K6t5myKl","colab_type":"text"},"cell_type":"markdown","source":"I I had more time:\n* fine tune the MLP (e.g grid search while scoring on the F1)\n* Correlation with balance seems quite high, maybe something to do there\n* training curve"},{"metadata":{"id":"9YVSECBSfH_b","colab_type":"code","outputId":"069cfca4-c115-4804-c40b-13a324874112","colab":{"base_uri":"https://localhost:8080/","height":38},"trusted":true},"cell_type":"code","source":"%%html\n<marquee style='width: 30%; color: blue;'><b>Whee finished!</b></marquee>","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Copy of ml1-FI2.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["kj2ayx-NmRTY","BiYdG_58rewK","eYgOOsAAm-8B"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}