{"cells":[{"metadata":{"_uuid":"3ee939f8439396b87f7de73887572c6ed4155710"},"cell_type":"markdown","source":"特别感谢@[混沌](https://www.joinquant.net/user/c9f1650814bdb9f6a628ec2919f61bba)大佬\n\n使用Python相关算法包一是减少了要编写以及调试的代码行数，二是包的运行速度要远快于之前的代码\n\n不同版本的包使用方式和注意事项不同，需要查看函数说明\n\n为了精细控制交叉验证每一折验证使用的训练集和测试集，可能不能直接使用交叉验证的函数版本：\n* 类别属性的某个属性值出现较少，就需要通过抽样来确保每份数据中包含特定属性值的样本个数是等比例的\n* 也需要访问每折数据来计算错误统计量，比如不想使用MSE二是MAE，因为它能更好地代表实际问题中的错误\n* 需要对每折数据计算错误的情况使用线性回归来解决分类问题。例如误分率或AUC"},{"metadata":{"collapsed":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"#-*- coding: utf-8 -*-\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LassoCV\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"29af2de5-e656-413a-b0b3-7c4dddef99ab","_uuid":"4d684ee515907fa8001562bede472ea066e7563a"},"cell_type":"markdown","source":"**1) 多变量回归：预测红酒口感**\n\n之所以把红酒口感看成回归问题，是因为口感是有程度的、有序的， 而分类问题本质只有区别而无优劣次序。\n\n具体是3~8分6种可能的类别，但是考虑到5比6差，又比4好，且实际的口感得分为3时，预测值为5要比预测值为4对累积错误的贡献更大，所以回归比分类更好体现\n\n> **构建并测试模型以预测红酒的口感**\n\n构建模型的第一步是通过样本外的性能来判断模型能否满足性能要求。\n\n本例展示了用算法包sklearn.linear_model.LassoCV进行套索线性回归的10折交叉验证。\n\n下面三个图为使用不同归一化版本的预测效果，对应如下：\n1. X和Y都做归一\n\n2. X做归一，Y不做归一\n\n3. X和Y都不做归一"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# 获取红酒数据集\ndf = pd.read_csv('../input/winequality/winequality-red.csv')\ndf.columns = ['非挥发性酸','挥发性酸','柠檬酸', '残留糖分', '氯化物', '游离二氧化硫','总二氧化硫', '密度', \n              'PH值', '亚硝酸盐', '酒精含量', '品质']","execution_count":2,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"bce8cc54-be99-441c-848b-2f6c71b5d909","_uuid":"48d3e3897339076381f19581d0f0310be4a3e394","trusted":true},"cell_type":"code","source":"# 标准化 切分属性和标签\nnorm_df = (df - df.mean())/df.std()\nxData = df.values[:,:-1]; yData = df.values[:,-1] \nxNormData = norm_df.values[:,:-1]; yNormData = norm_df.values[:,-1] \nm, n = xData.shape","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"876d7d82-ea5e-4e5a-9f58-1d43841ce2a1","_uuid":"97ba121d8ef0b63eb0948021469f06db30693c3c","trusted":true},"cell_type":"code","source":"#  标准化与否的选择\n# xx = xData; \nxx = xNormData\n#y = yData\ny = yNormData\n\n# 调用sklearn.linear_model中的LassoCV \nwineModel = LassoCV(cv=10).fit(xx, y)\n\n# 显示结果\nplt.figure()\nplt.plot(wineModel.alphas_, wineModel.mse_path_, ':')\nplt.plot(wineModel.alphas_, wineModel.mse_path_.mean(axis=-1),\n         label='Average MSE Across Folds', linewidth=2)\nplt.axvline(wineModel.alpha_, linestyle='--',\n            label='CV Estimate of Best alpha')\nplt.semilogx()\nplt.legend()\nax = plt.gca()\nax.invert_xaxis()\nplt.xlabel('alpha')\nplt.ylabel('Mean Square Error')\nplt.axis('tight')\nplt.show()\n\n#print out the value of alpha that minimizes the Cv-error\nprint(\"最小均方误差对应的alpha值  \",wineModel.alpha_)\nprint(\"最小均方误差：\", min(wineModel.mse_path_.mean(axis=-1)))","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"4315f638-71c8-49f5-ac91-f9732122f454","_uuid":"f0835135e73da7c704853f22755039494831329c","trusted":true},"cell_type":"code","source":"#  标准化与否的选择\n# xx = xData; \nxx = xNormData\ny = yData\n# y = yNormData\n\n# 调用sklearn.linear_model中的LassoCV \nwineModel = LassoCV(cv=10).fit(xx, y)\n\nplt.figure()\nplt.plot(wineModel.alphas_, wineModel.mse_path_, ':')\nplt.plot(wineModel.alphas_, wineModel.mse_path_.mean(axis=-1),\n         label='Average MSE Across Folds', linewidth=2)\nplt.axvline(wineModel.alpha_, linestyle='--',\n            label='CV Estimate of Best alpha')\nplt.semilogx()\nplt.legend()\nax = plt.gca()\nax.invert_xaxis()\nplt.xlabel('alpha')\nplt.ylabel('Mean Square Error')\nplt.axis('tight')\nplt.show()\n\n#print out the value of alpha that minimizes the Cv-error\nprint(\"最小均方误差对应的alpha值  \",wineModel.alpha_)\nprint(\"最小均方误差：\", min(wineModel.mse_path_.mean(axis=-1)))","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"b0339548-bc68-4abb-8905-9cb367718bbe","_uuid":"1008fd9c8abf027a7eddc0e9aab6637a9d6c30dc","trusted":true},"cell_type":"code","source":"#  标准化与否的选择\nxx = xData; \n# xx = xNormData\ny = yData\n# y = yNormData\n\n# 调用sklearn.linear_model中的LassoCV \nwineModel = LassoCV(cv=10).fit(xx, y)\n\nplt.figure()\nplt.plot(wineModel.alphas_, wineModel.mse_path_, ':')\nplt.plot(wineModel.alphas_, wineModel.mse_path_.mean(axis=-1),\n         label='Average MSE Across Folds', linewidth=2)\nplt.axvline(wineModel.alpha_, linestyle='--',\n            label='CV Estimate of Best alpha')\nplt.semilogx()\nplt.legend()\nax = plt.gca()\nax.invert_xaxis()\nplt.xlabel('alpha')\nplt.ylabel('Mean Square Error')\nplt.axis('tight')\nplt.show()\n\n#print out the value of alpha that minimizes the Cv-error\nprint(\"最小均方误差对应的alpha值  \",wineModel.alpha_)\nprint(\"最小均方误差：\", min(wineModel.mse_path_.mean(axis=-1)))","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"c6838381-50ee-4c0c-a212-d5bb64f36677","_uuid":"8758570df9237eef3de0301b68d5340ef3688d27"},"cell_type":"markdown","source":"对X、y是否做标准化的其它选择后(调整代码中xx,y的赋值后重新绘图)，我们会发现：\n\n1） X标准化，y不做标准化： 虽然均方误差显著变小，但图形很类似没有本质区别，仅仅因为对标签进行标准化会失去与原始数据的关联而已。实际上我们可以将标准化后的标签转换会原来的标签尺度，并且这种转换的实现都是现成。所以对标签的标准化与否没有本质区别。\n\n2） X、y都不做标准化： 虽然均方误差变化不大，但均方误差曲线有扇形的陡然下降区域，这是因为X特征尺度混乱导致，算法挑选了尺度大的变量进行预测，对应的系数很小。算法使用一个较差的变量进行若干次迭代，直到α变得足够小以引进一个更好的变量，这时错误才会陡然下降。\n\n**本例建议**：应该对特征属性和标签值都做标准化。"},{"metadata":{"_cell_guid":"57e36912-fa84-445d-9036-a10e731e065a","_uuid":"75f95de5d5d2ad1ca7bfab56ac6017aab233cebd"},"cell_type":"markdown","source":"> **部署前在整个数据集上进行训练**\n\n本例展示了用算法包sklearn.linear_model.lasso_path进行套索线性回归路径分析。\n\n所谓路径分析，就是一组回归分析，本例的一组是由alphas控制，每个alpha对应一个线性回归。"},{"metadata":{"_cell_guid":"cefedf0f-4ba2-466b-b778-c6e055393833","_uuid":"6dd80a910bf0f5ce829336f33bb473e6499888c3","trusted":true},"cell_type":"code","source":"#  标准化与否的选择\n# xx = xData; \nxx = xNormData\n# y = yData\ny = yNormData\n\n\nalphas, coefs, _  = linear_model.lasso_path(xx, y,  return_models=False)\n\nplt.plot(alphas,coefs.T)\n\nplt.xlabel('alpha')\nplt.ylabel('Coefficients')\nplt.axis('tight')\nplt.semilogx()\nax = plt.gca()\nax.invert_xaxis()\nplt.show()\n\nnattr, nalpha = coefs.shape\n\n# 回归系数排序\nnzList = []\nfor iAlpha in range(1,nalpha):\n    coefList = coefs[: ,iAlpha]\n    \n    # 记录回归系数刚好变成非零的属性\n    nzCoef = np.where(coefList!=0)[0]\n    for q in nzCoef:\n        if q not in nzList:\n            nzList.append(q)\n\nprint(\"系数进入模型的次序所对应的属性排序 :\",)\nfor idx in nzList:\n    print(df.columns[idx],end=' ')\nprint(\"\")\n\n# 根据前面已获得的最佳\\alpha，寻找此例中对应的索引\nalphaStar = 0.013561387700964642\nindexLTalphaStar = [index for index in range(100) if alphas[index] > alphaStar]\nindexStar = max(indexLTalphaStar)\n\n# 进而根据上面“对应的索引”，获得最佳回归系数值\ncoefStar = coefs[:,indexStar]\nprint(\"最佳回归系数：\", coefStar)\n\n# 回归系数给出了另外一组稍微不同的顺序\nabsCoef =  np.abs(coefStar)\nidxs = np.argsort(-absCoef)\n\nprint(\"最优alpha的系数尺度所对应的属性排序 :\",)\nfor idx in idxs:\n    if absCoef[idx]!=0:\n        print(df.columns[idx],end=' ')\nprint (\"\")","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"0bfa4ba8-8039-407d-8589-01a1a74ef4d9","_uuid":"de3f284a4bcf94632ac1148e7290ef5bc646f486","trusted":true},"cell_type":"code","source":"#  标准化与否的选择\nxx = xData; \n# xx = xNormData\n# y = yData\ny = yNormData\n\n\nalphas, coefs, _  = linear_model.lasso_path(xx, y,  return_models=False)\n\nplt.plot(alphas,coefs.T)\n\nplt.xlabel('alpha')\nplt.ylabel('Coefficients')\nplt.axis('tight')\nplt.semilogx()\nax = plt.gca()\nax.invert_xaxis()\nplt.show()\n\nnattr, nalpha = coefs.shape\n\n# 回归系数排序\nnzList = []\nfor iAlpha in range(1,nalpha):\n    coefList = coefs[: ,iAlpha]\n    \n    # 记录回归系数刚好变成非零的属性\n    nzCoef = np.where(coefList!=0)[0]\n    for q in nzCoef:\n        if q not in nzList:\n            nzList.append(q)\n\nprint(\"系数进入模型的次序所对应的属性排序 :\",)\nfor idx in nzList:\n    print(df.columns[idx], end=' ')\nprint(\"\")\n\n# 根据前面已获得的最佳\\alpha，寻找此例中对应的索引\nalphaStar = 0.013561387700964642\nindexLTalphaStar = [index for index in range(100) if alphas[index] > alphaStar]\nindexStar = max(indexLTalphaStar)\n\n# 进而根据上面“对应的索引”，获得最佳回归系数值\ncoefStar = coefs[:,indexStar]\nprint(\"最佳回归系数：\", coefStar)\n\n# 回归系数给出了另外一组稍微不同的顺序\nabsCoef =  np.abs(coefStar)\nidxs = np.argsort(-absCoef)\n\nprint(\"最优alpha的系数尺度所对应的属性排序 :\",)\nfor idx in idxs:\n    if absCoef[idx]!=0:\n        print(df.columns[idx],end=' ')\nprint (\"\")","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"503e10f7-5812-4ff9-a536-a49c9f44587d","_uuid":"e78e5eb44ce1be473c72ff34055655ef61244efb"},"cell_type":"markdown","source":"对于标准化属性， 上面两种属性的排序基本一致，但在不太重要的属性上有所差异。\n\n对于非标准化属性，系数进入模型的次序几乎是混乱的，但最优alpha的系数尺度次序变化不大。几个早期进入解得系数相对于后续进入解得系数更接近于0，这种现象正好证明了系数进入模型的顺序与最佳解得系数尺度的顺序存在本质不同"},{"metadata":{"_cell_guid":"8ca443dc-45a3-4379-83ea-c8f387d90364","_uuid":"6420b462d8fb03983b1f4eaf479b1c6781a6c975"},"cell_type":"markdown","source":"> **基扩展：基于原始属性扩展新属性来改进性能**\n\n本例尝试扩展了两个属性。"},{"metadata":{"_cell_guid":"cd21f8d5-6102-4302-92e3-2badb2681429","_uuid":"373d4010fd207a13e488ffd86dfe7879d81746a6","trusted":true},"cell_type":"code","source":"# 沿用前面已经获得的数据\n\n# 扩展2个新属性\nxExtData = np.concatenate((xData, np.zeros((m,2))), axis=1)\nxExtData[:, n] = xData[:,-1]**2\nxExtData[:, n+1] = xData[:,-1]*xData[:,1]\n\nxNormExtData = (xExtData - xExtData.mean(axis=0))/np.std(xExtData,axis=0)\n\nm, n = xData.shape\n\n# 更新属性名\nnames = list(df.columns[idx])\nnames[-1] = \"alco^2\"\nnames.append(\"alco*volAcid\")\n\n#  标准化与否的选择\n# xx = xExtData\nxx = xNormExtData\n# y = yData\ny = yNormData\n\n# 调用sklearn.linear_model中的LassoCV \nwineModel = LassoCV(cv=10).fit(xx, y)\n\n# 显示结果\nplt.figure()\nplt.plot(wineModel.alphas_, wineModel.mse_path_, ':')\nplt.plot(wineModel.alphas_, wineModel.mse_path_.mean(axis=-1),\n         label='Average MSE Across Folds', linewidth=2)\nplt.axvline(wineModel.alpha_, linestyle='--',\n            label='CV Estimate of Best alpha')\nplt.semilogx()\nplt.legend()\nax = plt.gca()\nax.invert_xaxis()\nplt.xlabel('alpha')\nplt.ylabel('Mean Square Error')\nplt.axis('tight')\nplt.show()\n\n#print out the value of alpha that minimizes the Cv-error\nprint(\"最小均方误差对应的alpha值  \",wineModel.alpha_)\nprint(\"最小均方误差：\", min(wineModel.mse_path_.mean(axis=-1)))","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"3a615f6f-47a6-42a1-a21c-9490cf182878","_uuid":"233e235816a20ed90a96a32d0978a58b81ff8b0b"},"cell_type":"markdown","source":"代码中尝试添加了两个扩展属性，但似乎没有多大效果。 如果要获得更好的效果，似乎要做不少尝试。"},{"metadata":{"_cell_guid":"b516247a-639b-4e37-8987-82cced84daf1","_uuid":"6c7df01c81685550c0db312a28e6a25e69be9cb9"},"cell_type":"markdown","source":"**2) 二分类：检测水雷**\n\n“水雷&岩石”只有区别而无本质优劣，所以是分类问题。本例使用scikit-learn算法包中的ElasticNet包。\n\n> **构建并测试模型**\n\n依然是先通过样本外的性能来判断模型能否满足性能要求。\n\n本例计划用ElasticNet线性回归解决分类问题，但是10折交叉验证算法sklearn.linear_model.ElasticNetCV是基于均方误差的，而我们的分类问题的性能度量需要基于误分率或AUC的,显然不合适。 为此，我们不得不手工构建10折交叉验证循环。\n\n在手工10折交叉验证循环中用算法sklearn.linear_model.enet_path进行elasticNet线性回归路径分析。\n\n本例展示了度量分类器性能的两种方法: 误分率和AUC(ROC曲线下面积),分别对应后面的第一张图和第二张图。 第三张图则绘制基于最佳auc的ROC曲线。\n\nAUC的优势在于：能获得最佳性能并且和应用场景无关。**但不能确保在一个特定的误分率上获得最优性能。**\n\n一般而言， 误分率似乎是我们的更好选择。"},{"metadata":{"_uuid":"71c458b7e3d98169ad00b2d82683fc71cb4c26b8","_cell_guid":"e9ce7cdf-4c02-4620-a9b3-2518922c2224","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import enet_path\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\ndf = pd.read_csv('../input/sonaralldata/sonar.all-data.csv', header=None, prefix='V')\n\n# 分类标签数值化\ndf['V60'] = df.iloc[:,-1].apply(lambda v: 1.0 if v=='M' else 0.0)\n\n# 切分属性和标签 然后 标准化 \nxData = df.values[:,:-1]; yData = df.values[:,-1] \n\nxData = (xData - xData.mean(axis=0))/xData.std(axis=0)\nyData = (yData - yData.mean())/yData.std()\n\nm, n = xData.shape\n\n# 手工构建10折交叉验证循环\nnxval = 10\nfor ixval in range(nxval):\n    \n    # 第ixval折验证的训练集和测试集的切分\n    idxTest = [i for i in range(m) if i%nxval == ixval%nxval]\n    idxTrain = [i for i in range(m) if i%nxval != ixval%nxval]    \n    xTest = xData[idxTest,:]; yTest = yData[idxTest]\n    xTrain = xData[idxTrain,:]; yTrain = yData[idxTrain]\n\n    # enet_path 就是 ElasticNet正规化路径\n    # 参数：l1_ratio就是套索惩罚项的占比\n    alphas, coefs, _ = enet_path(xTrain, yTrain,l1_ratio=0.8, fit_intercept=False, return_models=False)\n\n    # 将所有额预测及其标签归集到一起\n    if ixval == 0:\n        pred = np.dot(xTest, coefs)\n        yOut = yTest\n    else:\n        pred = np.concatenate((pred, np.dot(xTest, coefs)), axis = 0)\n        yOut = np.concatenate((yOut, yTest), axis=0)  \n\n# 计算误分率\nmisClassRate = 1.0*((pred>=0)^(np.repeat(yOut, pred.shape[1]).reshape(pred.shape)>=0)).sum(axis=0)/m\n\n# 寻找最小误差\nidxMin = misClassRate.argmin()\nminError = misClassRate[idxMin]\n\nplt.figure()\nplt.plot(alphas[1:], misClassRate[1:], label='Misclassification Error Across Folds', linewidth=2)\nplt.axvline(alphas[idxMin], linestyle='--', label='CV Estimate of Best alpha')\nplt.legend()\nplt.semilogx()\nax = plt.gca()\nax.invert_xaxis()\nplt.xlabel('alpha')\nplt.ylabel('Misclassification Error')\nplt.axis('tight')\nplt.show()\n\n# 计算AUC\nauc = []\nfor iPred in range(0, pred.shape[1]):\n    predList = list(pred[:, iPred])\n    aucCalc = roc_auc_score((yOut>0.0), predList)\n    auc.append(aucCalc)\n\n# 最大auc 及其对应的最小误分率\nidxMax = np.argmax(auc)\nminError = misClassRate[idxMin]\n\nplt.figure()\nplt.plot(alphas[1:], auc[1:], label='AUC Across Folds', linewidth=2)\nplt.axvline(alphas[idxMax], linestyle='--', label='CV Estimate of Best alpha')\nplt.legend()\nplt.semilogx()\nax = plt.gca()\nax.invert_xaxis()\nplt.xlabel('alpha')\nplt.ylabel('Area Under the ROC Curve')\nplt.axis('tight')\nplt.show()\n\n\n# 绘制最佳性能分类器的ROC曲线\nfpr, tpr, thresh = roc_curve((yOut>0.0), list(pred[:, idxMax]))\nctClass = [i*0.01 for i in range(101)]\nplt.plot(fpr, tpr, linewidth=2)\nplt.plot(ctClass, ctClass, linestyle=':')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()\n\nprint('最佳误分率 = ', misClassRate[idxMin])\nprint('最佳alpha(对应最佳误分率) = ', alphas[idxMin])\nprint('')\nprint('最佳AUC = ', auc[idxMax])\nprint('最佳alpha(对应最佳AUC)   =  ', alphas[idxMax])\n\nprint('')\nprint('不同阈值对应的混淆矩阵' )\n\n# 正例数（标签为水雷）\nP = len(yOut[yOut>0.0])\n\nthr_idx = 20\nprint('')\nprint('阈值 =   ', thresh[thr_idx])\nprint('真正数(TP) = ', tpr[thr_idx]*P, '假负数(FN) = ', (1-tpr[thr_idx])*P)\nprint('假正数(FP) = ', fpr[thr_idx]*(m-P),'真负数(TN) = ', (1-fpr[thr_idx])*(m-P) )\n\nthr_idx = 40\nprint('')\nprint('阈值 =   ', thresh[thr_idx])\nprint('真正数(TP) = ', tpr[thr_idx]*P, '假负数(FN) = ', (1-tpr[thr_idx])*P)\nprint('假正数(FP) = ', fpr[thr_idx]*(m-P),'真负数(TN) = ', (1-fpr[thr_idx])*(m-P) )\n\nthr_idx = 60\nprint('')\nprint('阈值 =   ', thresh[thr_idx])\nprint('真正数(TP) = ', tpr[thr_idx]*P, '假负数(FN) = ', (1-tpr[thr_idx])*P)\nprint('假正数(FP) = ', fpr[thr_idx]*(m-P),'真负数(TN) = ', (1-fpr[thr_idx])*(m-P) )","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"befd2253-0bba-4616-af3e-109bafef843a","_uuid":"ff7ae15e59b4170692e895ce6d8913b429ba4c20"},"cell_type":"markdown","source":"阈值确定需要合理的代价设置，同时要尽量确保训练集的正例和负例的比例与实际情况相一致。\n\n> **构建并部署分类器**\n\n完成 α 的选择后， 现在应该在全部数据集上重新训练模型, 然后确定 α 在新的模型下所对应的回归系数。\n\n新模型的路径分析和最佳回归系数，各自对应一组属性的排序，这正好是**特征工程**所需要的信息。\n\n衡量变量重要性的一个指标是随着α减小，进入解得变量顺序。另一个指标是根据最优解的特征系数大小得到的排序。"},{"metadata":{"scrolled":false,"_uuid":"742dec353307829ce9ada68bccd3de43a500cd88","_cell_guid":"59b26b52-e3c0-4586-bdf4-d89c3e2df518","trusted":true},"cell_type":"code","source":"# 沿用已有的数据\n\n# 在整个数据集上进行路径分析\nalphas, coefs, _ = enet_path(xData, yData,l1_ratio=0.8, fit_intercept=False, return_models=False)\n\nplt.plot(alphas,coefs.T)\nplt.xlabel('alpha')\nplt.ylabel('Coefficients')\nplt.axis('tight')\nplt.semilogx()\nax = plt.gca()\nax.invert_xaxis()\nplt.show()\n\nnattr, nalpha = coefs.shape\n\n# 回归系数排序\nnzList = []\nfor iAlpha in range(1,nalpha):\n    coefList = coefs[: ,iAlpha]\n    \n    # 记录回归系数刚好变成非零的属性\n    nzCoef = np.where(coefList!=0)[0]\n    for q in nzCoef:\n        if q not in nzList:\n            nzList.append(q)\n\nprint(\"系数进入模型的次序所对应的属性排序 :\",)\nfor idx in nzList:\n    print(df.columns[idx],end=' ')\nprint(\"\")\n\n# 根据前面已获得的最佳\\alpha，寻找此例中对应的索引\nalphaStar = 0.020334883589342503\nindexLTalphaStar = [index for index in range(100) if alphas[index] > alphaStar]\nindexStar = max(indexLTalphaStar)\nprint(indexStar)\n\n# 进而根据上面“对应的索引”，获得最佳回归系数值\ncoefStar = coefs[:,indexStar]\nprint(\"最佳回归系数：\", coefStar)\n\n# 回归系数给出了另外一组稍微不同的顺序\nabsCoef =  np.abs(coefStar)\nidxs = np.argsort(-absCoef)\n\nprint(\"最优alpha的系数尺度所对应的属性排序 :\",)\nfor idx in idxs:\n    if absCoef[idx]!=0:\n        print(df.columns[idx],end=' ')\nprint(\"\")","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"16cd8cb5-b3cb-42f6-ba00-535187d0ea45","_uuid":"9de0b41f465c9579d29c92d470e6ec54c92f2fa0"},"cell_type":"markdown","source":"**3) 逻辑回归与惩罚逻辑回归**\n\n另一种使用惩罚线性回归模型进行分类的方法是使用惩罚逻辑回归。\n\nIRLS方法也叫Newton-Raphson，我们可以用这个方法求解惩罚逻辑回归问题。"},{"metadata":{"_uuid":"c97e185d8888899790e2392b7e75f084b8f66af1","_cell_guid":"3eb3d9b8-b39f-40bc-a4af-6ac2d4df3552","trusted":true},"cell_type":"code","source":"def S(z,gamma):\n    if gamma >= np.abs(z):\n        return 0.0\n    if z > 0.0:\n        return z - gamma\n    else:\n        return z + gamma\n\ndef glmnetIRLS(xx, y, beta, lam, alp):\n    '''\n        Glmnet和IRLS两个算法结合在一起，用于解决惩罚逻辑回归问题. 这是一个单步迭代公式\n    参数：\n        xx:  属性矩阵X\n        y:   标签向量\n        beta: 回归系数向量(更新后返回)\n        lam:  \\lambda\n        alp:  \\alpha\n    '''\n\n    m,n = xx.shape\n       \n    eta = np.dot(xx, beta) # = X beta\n    eta[eta<-100] = -100\n    \n    mu = 1.0/(1.0+np.exp(-eta))\n    \n    w = mu * (1.0 - mu)\n    \n    cond1 = (np.abs(mu)<1e-5)\n    cond2 = (np.abs(1.0-mu)<1e-5)\n    mu[cond1] = 0.0\n    mu[cond2] = 1.0\n    w[cond1|cond2] = 1e-5\n    \n    ww = np.diag(w)\n    \n    z = (y-mu)/w + eta # = eta + W^{-1} (y - mu)\n    \n    for j in range(n):\n        r = z - np.dot(xx,beta) # = z - X beta\n        \n        sumWxr = (w*xx[:,j]*r).sum()  # = (X^T W r)[j]\n        sumWxx =  (w*xx[:,j]*xx[:,j]).sum() # = (X^T W X)[j,j]\n        \n        beta[j] = beta[j] + sumWxr / sumWxx\n        \n        if j > 0:\n            avgWxx = sumWxx / m\n            beta[j] = S(beta[j]*avgWxx, lam * alp) / (avgWxx + lam * (1.0 - alp))\n    \n    return beta\n\n# 分类标签数值化\n#df['V60'] = df.iloc[:,-1].apply(lambda v: 1.0 if v=='M' else 0.0)\n\n# 切分属性和标签 然后 标准化 \nxData = df.values[:,:-1]; yData = df.values[:,-1] \n\n# 只对X正规化, y只计算均值和标准差\nxData = (xData - xData.mean(axis=0))/xData.std(axis=0)\nyMean = yData.mean();  yStd = yData.std()\n\n# 选择alpha参数\nalpha = 0.8  # ElasticNet回归\n\n# 确定lambda初始值：导致所有beta值都为零的lambda值（最简模型）\nlam = (np.abs(yData.dot(xData))/xData.shape[0]).max()/alpha\n\n# 扩展X成[1,X]\nxData = np.concatenate((np.ones((xData.shape[0],1)), xData), axis=1)\nnrow, ncol = xData.shape\n\n# beta初始化\nbeta = np.zeros(ncol)\nbeta[0] = np.log(yMean/(1-yMean))\n\n# 记录历史beta\nbetaMat = []\nbetaMat.append(list(beta[1:]))\nbeta0Mat = []\nbeta0Mat.append(beta[0])\n\n# 迭代步数\nnSteps = 100\n\n# lambda缩减乘子：Fredman建议每步迭代后都要稍微减小lambda\nlamMult = 0.93 # Fredman建议: lamMult^nSteps = 0.001\n\n# 记录属性回归系数变成非零先后次序\nnzList = []\n\n# 开始进行lam迭代计算\nfor iStep in range(nSteps):\n    lam = lam * lamMult\n    \n    deltaBeta = 100.0\n    eps = 0.01\n    iterStep = 0\n    \n    # 开始Glmnet算法迭代\n    while deltaBeta > eps:\n        iterStep += 1\n        if iterStep > 100: #100:\n            break\n        \n        # 上一步beta\n        _beta = beta.copy()\n\n        # IRL算法递归公式\n        beta = glmnetIRLS(xData, yData, beta, lam, alpha)\n        \n        # 计算精度\n        deltaBeta = np.abs(beta[1:]-_beta[1:]).sum() / np.abs(beta[1:]).sum()\n    \n    # 记录beta历史\n    betaMat.append(list(beta[1:]))\n    beta0Mat.append(beta[0])\n\n    # 记录回归系数刚好变成非零的属性\n    nzBeta = np.where(beta[1:]!=0)[0]\n    for q in nzBeta:\n        if (q in nzList) == False:\n            nzList.append(q)\n\n# 打印属性重要性排序\nfor idx in nzList:\n    print(df.columns[idx],end=' ')\nprint(\"\")\n\n# lambda-误差曲线\nplt.plot(betaMat)\nplt.xlabel(\"Step Taken\")\nplt.ylabel(\"Coefficient Values\")\nplt.show()","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"d8dff830-27e7-4a56-8448-cc11e336db72","_uuid":"f83e5017447a9c5812dd099e003b2c939395f66a"},"cell_type":"markdown","source":"**4) 多类别分类**\n\n本例采用方法的核心就是：将分类标签映射到10向量（一个1多个0，1的位置就是标签集索引），然后每个标签构建一个模型。 采用10折交叉验证的方法，计算惩罚步数所对应的误分率，并绘制成图。\n\n在手工10折交叉验证循环中用算法sklearn.linear_model.enet_path进行elasticNet线性回归路径分析。"},{"metadata":{"_uuid":"4e6d23daa42942023580953f36545696ce307d9d","_cell_guid":"76bee137-d437-4609-aeea-943dfc035bd1","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/glassdata/glass.data.csv', header=None, prefix=\"V\")\ndf.columns = ['Id','RI','Na', 'Mg', 'Al', 'Si','K', 'Ca', 'Ba', 'Fe', 'Type']\ndf = df.set_index(\"Id\")\n\n# 切分属性和标签\nxData = df.values[:,:-1]; yLabel = df.values[:,-1] \n\n# 标签值向量化(一对所有)\nlabelList = list(set(yLabel))\nlabelList.sort()\nnlabels = len(labelList)\ndef mapFunc(label):\n    idx = labelList.index(label)\n    row = [0]*nlabels\n    row[idx] = 1\n    return row\nyData = np.array([mapFunc(label) for label in yLabel])\n\n# 标准化\nxData = (xData - xData.mean(axis=0))/xData.std(axis=0)\nyMean = yData.mean(axis=0); yStd = yData.std(axis=0)\nyData = (yData - yMean)/yStd\n\n# 数据规模\nm, n = xData.shape\n\n# 手工构建n折交叉验证循环\nnxval = 10\nnAlphas= 100\nmisClass = [0.0] * nAlphas\nfor ixval in range(nxval):   \n    # 第ixval折验证的训练集和测试集的切分\n    idxTest = [i for i in range(m) if i%nxval == ixval%nxval]\n    idxTrain = [i for i in range(m) if i%nxval != ixval%nxval]    \n    xTest = xData[idxTest,:]; yTest = yData[idxTest,:]\n    xTrain = xData[idxTrain,:]; yTrain = yData[idxTrain,:]\n    labelTest = yLabel[idxTest]    \n\n    # 为yTrain的每列建立模型\n    # enet_path 就是 ElasticNet正规化路径\n    # 参数：l1_ratio就是套索惩罚项的占比\n    models = [enet_path(xTrain, yTrain[:,k] ,l1_ratio=1.0, fit_intercept=False, \n                        eps=0.5e-3, n_alphas=nAlphas , return_models=False) \n              for k in range(nlabels)]\n    \n    lenTest = m - len(yTrain)\n    for iStep in range(1,nAlphas):\n        # 组合所有模型的预测\n        allPredictions = []\n        for iModel in range(nlabels):\n            # 模型预测\n            _, coefs, _ = models[iModel]\n            predTemp = np.dot(xTest, coefs[:,iStep])\n            \n            # 去标准化后比较\n            predUnNorm = predTemp*yStd[iModel]+yMean[iModel]\n            allPredictions.append(list(predUnNorm))\n        allPredictions = np.array(allPredictions)\n\n        # 找出最大的预测和误差        \n        predictions = []\n        for i in range(lenTest):\n            listOfPredictions = allPredictions[:,i]\n            idxMax = listOfPredictions.argmax()\n            if labelList[idxMax] != labelTest[i]:\n                misClass[iStep] += 1.0\n\nmisClassPlot = [misClass[i]/m for i in range(1, nAlphas)]\n\nplt.plot(misClassPlot)\n\nplt.xlabel(\"Penalty Parameter Steps\")\nplt.ylabel((\"Misclassification Error Rate\"))\nplt.show()","execution_count":19,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}