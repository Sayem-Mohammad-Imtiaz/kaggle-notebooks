{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"> Hey kagglers, \n\n> Everyone tried their fair share of bit to dig deep and put out the best predictions for rainfall in Australia, and so do I. In this notebook you wouldn't see much of data exploration or visualization because I have tried to keep it to the minimum (but still the kernel got so big, I couldn't help, sorry), given the situation to cover the other aspects such as kinds of imputation, type of imbalances and ways to deal them, a class to run a Grid-search and transferring the best_parameters to the algoithms automatically, all you have to do is just instantiate and pass the parameters. And at last StackingClassifier.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-05T20:44:06.240582Z","iopub.execute_input":"2021-06-05T20:44:06.241294Z","iopub.status.idle":"2021-06-05T20:44:06.259856Z","shell.execute_reply.started":"2021-06-05T20:44:06.241194Z","shell.execute_reply":"2021-06-05T20:44:06.258966Z"}}},{"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, precision_recall_curve\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, classification_report\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore', category=FutureWarning)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:47.418192Z","iopub.execute_input":"2021-06-10T20:10:47.418594Z","iopub.status.idle":"2021-06-10T20:10:49.989124Z","shell.execute_reply.started":"2021-06-10T20:10:47.418501Z","shell.execute_reply":"2021-06-10T20:10:49.988323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:52.163566Z","iopub.execute_input":"2021-06-10T20:10:52.164186Z","iopub.status.idle":"2021-06-10T20:10:52.181178Z","shell.execute_reply.started":"2021-06-10T20:10:52.164134Z","shell.execute_reply":"2021-06-10T20:10:52.17962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"        \ndf = pd.read_csv('../input/weather-dataset-rattle-package/weatherAUS.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:52.419472Z","iopub.execute_input":"2021-06-10T20:10:52.419823Z","iopub.status.idle":"2021-06-10T20:10:53.043832Z","shell.execute_reply.started":"2021-06-10T20:10:52.419793Z","shell.execute_reply":"2021-06-10T20:10:53.042835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:53.045178Z","iopub.execute_input":"2021-06-10T20:10:53.04561Z","iopub.status.idle":"2021-06-10T20:10:53.095583Z","shell.execute_reply.started":"2021-06-10T20:10:53.045578Z","shell.execute_reply":"2021-06-10T20:10:53.094794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Well, we have df.info() to get all the details such  as the count and the data types. But I thought of writing my own reusable dfInfo method which will give me all the details I need to move ahead.","metadata":{}},{"cell_type":"code","source":"def dfInfo(df):\n    feature_dict = {}\n    \n    #list of all features\n    features = df.columns.tolist()\n\n    #list of datatypes of all features\n    datatype = [df[col].dtype for col in df.columns] \n\n    #Count of each feature\n    count = [df[col].count() for col in df.columns]\n\n    #Missing percentage in each feature\n    miss_percent = [(round(((len(df) - df[col].count())/len(df) * 100), 2)) for col in df.columns]\n\n    #Marking yes for missing and No for not missing \n    missing = ['Yes' if df[col].isnull().sum() != 0 else 'No' for col in df.columns] \n    \n    #Unique count of categorical features\n    unique_count =  [len(df[col].unique()) if df[col].dtype == \"object\" else \"NA\" for col in df.columns]\n    \n    #Feature Categorical or numerical\n    cat_num = [\"Catgeorical\" if df[col].dtype == \"object\" else \"Numerical\" for col in df.columns]\n    \n    feature_dict.update({\"Features\": features, \"Datatype\":datatype, \"Count\":count, \n                        \"Missing\":missing, \"Missing_percent\":miss_percent, \"CatOrNum\":cat_num, \"Unique_Count\":unique_count})\n    \n    return pd.DataFrame(data=feature_dict)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:53.09693Z","iopub.execute_input":"2021-06-10T20:10:53.097348Z","iopub.status.idle":"2021-06-10T20:10:53.10697Z","shell.execute_reply.started":"2021-06-10T20:10:53.097306Z","shell.execute_reply":"2021-06-10T20:10:53.105913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfInfo(df)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:53.111965Z","iopub.execute_input":"2021-06-10T20:10:53.112515Z","iopub.status.idle":"2021-06-10T20:10:53.540354Z","shell.execute_reply.started":"2021-06-10T20:10:53.112478Z","shell.execute_reply":"2021-06-10T20:10:53.539203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Now our next step would be to perform imputation but before moving ahead with the several imputation techniques and find out which one works best, we have to deal with categorical features, categorical features with Nan values and outliers. Categorical features with Nan values should be imputed ensuring no data leak. And outliers are treated because the imputation process otherwise will be influenced, producing values too far from the real values and resulting in invalid estimates. Outlier treatment will be done only in the training set, because testing sets in the real world are not in our control. But also remember too much outlier treatment can sometime lead to loss of information, because a data point far away from the mean doesnt necessarily always mean wrong data captured. ","metadata":{}},{"cell_type":"code","source":"#Splitting \"Date\" column into Day, Month and Year\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Day'] = df['Date'].dt.day\n\n#Will use a copy of df, excluding date column\ndata = df.drop('Date', axis=1)\n\ndata = data.dropna(axis=0, how='any', subset=[\"RainTomorrow\"])\n\nprint(data.shape)\n\ndata.head(3)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:53.899231Z","iopub.execute_input":"2021-06-10T20:10:53.899598Z","iopub.status.idle":"2021-06-10T20:10:54.105198Z","shell.execute_reply.started":"2021-06-10T20:10:53.899554Z","shell.execute_reply":"2021-06-10T20:10:54.104165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data[\"RainToday\"].replace({'No' : 0, 'Yes' : 1}, inplace=True)\ndata[\"RainTomorrow\"].replace({'No' : 0, 'Yes' : 1}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:54.106592Z","iopub.execute_input":"2021-06-10T20:10:54.106874Z","iopub.status.idle":"2021-06-10T20:10:54.275027Z","shell.execute_reply.started":"2021-06-10T20:10:54.106845Z","shell.execute_reply":"2021-06-10T20:10:54.274259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Outlier treatment**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=[20,10])\ndata.boxplot(column=['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustSpeed', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm'])\nplt.xticks(rotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:54.276372Z","iopub.execute_input":"2021-06-10T20:10:54.276774Z","iopub.status.idle":"2021-06-10T20:10:57.731998Z","shell.execute_reply.started":"2021-06-10T20:10:54.276743Z","shell.execute_reply":"2021-06-10T20:10:57.731193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = data.drop(\"RainTomorrow\", axis=1)\ny = data[\"RainTomorrow\"]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:57.733294Z","iopub.execute_input":"2021-06-10T20:10:57.733685Z","iopub.status.idle":"2021-06-10T20:10:57.818513Z","shell.execute_reply.started":"2021-06-10T20:10:57.733655Z","shell.execute_reply":"2021-06-10T20:10:57.817701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def detect_outliers(col):\n    IQR = data[col].quantile(0.75) - data[col].quantile(0.25)\n    lower_bound = data[col].quantile(0.25) - (IQR * 3)\n    upper_bound = data[col].quantile(0.75) + (IQR * 3)\n    return col + \"  outlier is < {lowerbound} and > {upperbound}\".format(lowerbound=round(lower_bound,2), upperbound=round(upper_bound,2))\n\nprint(detect_outliers(\"Rainfall\"))\nprint(detect_outliers(\"Evaporation\"))\nprint(detect_outliers(\"WindGustSpeed\"))\nprint(detect_outliers(\"WindSpeed9am\"))\nprint(detect_outliers(\"WindSpeed3pm\"))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:57.819597Z","iopub.execute_input":"2021-06-10T20:10:57.820003Z","iopub.status.idle":"2021-06-10T20:10:57.884046Z","shell.execute_reply.started":"2021-06-10T20:10:57.819971Z","shell.execute_reply":"2021-06-10T20:10:57.883327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[\"Rainfall\"] = np.where(X_train[\"Rainfall\"]>3.2, 3.2, X_train[\"Rainfall\"])\nX_train[\"Evaporation\"] = np.where(X_train[\"Evaporation\"]>21.8, 21.8, X_train[\"Evaporation\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:57.885653Z","iopub.execute_input":"2021-06-10T20:10:57.886107Z","iopub.status.idle":"2021-06-10T20:10:57.892685Z","shell.execute_reply.started":"2021-06-10T20:10:57.886056Z","shell.execute_reply":"2021-06-10T20:10:57.891962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:57.89399Z","iopub.execute_input":"2021-06-10T20:10:57.894403Z","iopub.status.idle":"2021-06-10T20:10:58.111381Z","shell.execute_reply.started":"2021-06-10T20:10:57.894374Z","shell.execute_reply":"2021-06-10T20:10:58.110447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Miissing Value Treatment**","metadata":{}},{"cell_type":"markdown","source":"> It is important to note that some algorithms like XGBoost and LightGBM treat missing values without any preprocessing.\n\n> Before we start imputing the missing values, it is important to understand the reasons behind the values missing in the dataframe. There are 3 main reasons behind it: \n\n> 1- MCAR (Missing completely at random) : The missing values in any particular feature is not linked with the missing values in other feature(s). These are just random misses.\n\n> 2- MAR (Missing at random) : There is a relationship between the way the values are missing and the particular feature in which the values are missing, but not with the missing values. For example, men are more likely to tell their age or weight than women, and because of which we might find more missing values in women against that feature.\n\n> 3- MNAR (Missing not at random) : There is a particular relationship between the missing values and the value itself. For example, people with low income cannot afford higher education compared to high income people. \n\n> Remember that in any given dataset, there may be missing values in many features, but it is not necessary that if one feature is MCAR, all other features will have MCAR. They can be missing at random (MAR) or MNAR.\n\n> With all that said, lets explore the missing data.","metadata":{}},{"cell_type":"code","source":"#One way to check the missing values in dataframe is using  missingno\nimport missingno as msno\n\nmsno.matrix(X_train) #gives you a data-dense display and help pick patterns","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:58.112583Z","iopub.execute_input":"2021-06-10T20:10:58.112872Z","iopub.status.idle":"2021-06-10T20:10:59.367946Z","shell.execute_reply.started":"2021-06-10T20:10:58.112843Z","shell.execute_reply":"2021-06-10T20:10:59.367041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> From the matrix above we can immediately notice a few things:\n\n> 1- Missing values at WindGustDir and WindGustSpeed are directly related, hence MNAR.\n\n> 2- Missing values at Pressure9am and Pressure3pm are directly influenced by each other, hence MNAR.\n\n> 3- Rainfall and RainToday are MNAR as well.\n\n> 4- We can also say Evaporation, Sunshine, Cloud9am and Cloud3pm could be a case of MNAR.\n\n> 5 - MinTemp and Temp9am are also influenced by each other, hence MNAR.\n\n> You would have got the gist by now. ","metadata":{}},{"cell_type":"code","source":"#Lets see if we are on point with the claims of MNAR we made above\nmsno.heatmap(X_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:10:59.369536Z","iopub.execute_input":"2021-06-10T20:10:59.370161Z","iopub.status.idle":"2021-06-10T20:11:00.869364Z","shell.execute_reply.started":"2021-06-10T20:10:59.370111Z","shell.execute_reply":"2021-06-10T20:11:00.86801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> And we are absolutely on point. You can see the direct relations between the missing data. You can do some extensive imputations feature wise, but I will just continue with Iterative Imputation. \n\n> I will not delete any column as it is not a good idea because it will lead to losing some important information. \n\n> There are various imputation techniques such as:\n1- imputing with a constant value\n2- imputing with mean, median and mode\n3- imputing using KNN based methods or MICE\nNote: Timeseries imputations are different such as ffill, bfill and LinearInterpolation.\n\n> I will show you both the methods of how to impute with KNN and MICE(Iterative).\n\n> But before doing that lets encode the categorical features.","metadata":{}},{"cell_type":"code","source":"#Lets find out the categorical and numerical features\nfrom sklearn.compose import make_column_selector\n\nselect_numeric_features = make_column_selector(dtype_exclude=\"object\")\nselect_categorical_features = make_column_selector(dtype_include=\"object\")\n\nnumeric = select_numeric_features(data)\ncategorical = select_categorical_features(data)\n\nprint(\"Numerical features :\", numeric)\nprint(\"Categorical features :\", categorical)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:11:00.870781Z","iopub.execute_input":"2021-06-10T20:11:00.871072Z","iopub.status.idle":"2021-06-10T20:11:00.89136Z","shell.execute_reply.started":"2021-06-10T20:11:00.871036Z","shell.execute_reply":"2021-06-10T20:11:00.890316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_num = X_train[[\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\", \"RainToday\", \"Year\", \"Month\", \"Day\"]]\nX_test_num = X_test[[\"MinTemp\", \"MaxTemp\", \"Rainfall\", \"Evaporation\", \"Sunshine\", \"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \"Pressure3pm\", \"Cloud9am\", \"Cloud3pm\", \"Temp9am\", \"Temp3pm\", \"RainToday\", \"Year\", \"Month\", \"Day\"]]\n\nX_train_num = X_train_num.reset_index(drop=True)\nX_test_num = X_test_num.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:11:00.895458Z","iopub.execute_input":"2021-06-10T20:11:00.895816Z","iopub.status.idle":"2021-06-10T20:11:00.936092Z","shell.execute_reply.started":"2021-06-10T20:11:00.895775Z","shell.execute_reply":"2021-06-10T20:11:00.934838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> We can encode in multiple ways such as:\n1- Label encoders - It will map an integer to each class in the feature but will impose a false sense of ordinal relationship between the classes (49 > 38)\n2- One hot encoding - It will expand each column feature into multiple dummy columns based on cardinality, leading the model to struggle with sparse and large data with too many dummy features.\n3- Target encoding (using category_encoder) - It uses mean encoding or median encoding involving target class. It can increase the quality of model, but high chances of overfitting and leads to data leakage, as it encodes based on the target rendering the feature biased. \n\n> I will go with hot encoding technique.\nNote: One_hot_encoding from scikit learn looks same as pandas pd.dummies but OHE from sckikit learn has many advantges. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n#drop = \"first\" helps us escape the dummy variable trap\nohe = OneHotEncoder(handle_unknown='error', categories=\"auto\", sparse=False, drop=\"first\")","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:11:00.93857Z","iopub.execute_input":"2021-06-10T20:11:00.938882Z","iopub.status.idle":"2021-06-10T20:11:00.94481Z","shell.execute_reply.started":"2021-06-10T20:11:00.938851Z","shell.execute_reply":"2021-06-10T20:11:00.943404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dframe in [X_train, X_test]:\n    dframe['WindGustDir'].fillna(X_train['WindGustDir'].mode()[0], inplace=True)\n    dframe['WindDir9am'].fillna(X_train['WindDir9am'].mode()[0], inplace=True)\n    dframe['WindDir3pm'].fillna(X_train['WindDir3pm'].mode()[0], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:11:00.946654Z","iopub.execute_input":"2021-06-10T20:11:00.947057Z","iopub.status.idle":"2021-06-10T20:11:01.092868Z","shell.execute_reply.started":"2021-06-10T20:11:00.947021Z","shell.execute_reply":"2021-06-10T20:11:01.091885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_location = ohe.fit_transform(X_train[[\"Location\"]])\nX_train_WindGustDir = ohe.fit_transform(X_train[[\"WindGustDir\"]])\nX_train_WinDir9am = ohe.fit_transform(X_train[[\"WindDir9am\"]])\nX_train_WindDir3pm = ohe.fit_transform(X_train[[\"WindDir3pm\"]])\n\nX_train = pd.concat([X_train_num, pd.DataFrame(X_train_location), pd.DataFrame(X_train_WindGustDir), pd.DataFrame(X_train_WinDir9am), pd.DataFrame(X_train_WindDir3pm)], axis=1)\n\nX_train.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:11:01.094479Z","iopub.execute_input":"2021-06-10T20:11:01.094876Z","iopub.status.idle":"2021-06-10T20:11:01.580498Z","shell.execute_reply.started":"2021-06-10T20:11:01.094832Z","shell.execute_reply":"2021-06-10T20:11:01.579241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX_test_location = ohe.fit_transform(X_test[[\"Location\"]])\nX_test_WindGustDir = ohe.fit_transform(X_test[[\"WindGustDir\"]])\nX_test_WinDir9am = ohe.fit_transform(X_test[[\"WindDir9am\"]])\nX_test_WindDir3pm = ohe.fit_transform(X_test[[\"WindDir3pm\"]])\n\nX_test = pd.concat([X_test_num, pd.DataFrame(X_test_location), pd.DataFrame(X_test_WindGustDir), pd.DataFrame(X_test_WinDir9am), pd.DataFrame(X_test_WindDir3pm)], axis=1)\n\nX_test.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:11:01.583176Z","iopub.execute_input":"2021-06-10T20:11:01.583479Z","iopub.status.idle":"2021-06-10T20:11:01.733123Z","shell.execute_reply.started":"2021-06-10T20:11:01.583452Z","shell.execute_reply":"2021-06-10T20:11:01.732389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:11:01.734164Z","iopub.execute_input":"2021-06-10T20:11:01.734559Z","iopub.status.idle":"2021-06-10T20:11:01.739343Z","shell.execute_reply.started":"2021-06-10T20:11:01.734527Z","shell.execute_reply":"2021-06-10T20:11:01.738689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = y_train.astype(int)\ny_test = y_test.astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:11:01.740368Z","iopub.execute_input":"2021-06-10T20:11:01.740768Z","iopub.status.idle":"2021-06-10T20:11:01.760359Z","shell.execute_reply.started":"2021-06-10T20:11:01.740739Z","shell.execute_reply":"2021-06-10T20:11:01.758977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Imputation : KNN and Iterative**","metadata":{}},{"cell_type":"markdown","source":"***KNN Imputation***","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\n\nX_train_Knn = X_train.copy(deep=True)\n\nknn_imputer = KNNImputer(n_neighbors=3, weights=\"uniform\")\nX_train_Knn = knn_imputer.fit_transform(X_train_Knn)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T20:11:01.762133Z","iopub.execute_input":"2021-06-10T20:11:01.762679Z","iopub.status.idle":"2021-06-10T21:25:36.770129Z","shell.execute_reply.started":"2021-06-10T20:11:01.762644Z","shell.execute_reply":"2021-06-10T21:25:36.767725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_Knn = X_test.copy(deep=True)\nX_test_Knn = knn_imputer.transform(X_test_Knn)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:25:36.776581Z","iopub.execute_input":"2021-06-10T21:25:36.776959Z","iopub.status.idle":"2021-06-10T21:44:16.976604Z","shell.execute_reply.started":"2021-06-10T21:25:36.776923Z","shell.execute_reply":"2021-06-10T21:44:16.974267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Iterative Imputing***","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nX_train_Imp = X_train.copy(deep=True)\n\nIter_Imp = IterativeImputer(max_iter=25, verbose=2, imputation_order=\"ascending\") \nX_train_Imp = Iter_Imp.fit_transform(X_train_Imp)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T21:44:16.979282Z","iopub.execute_input":"2021-06-10T21:44:16.979597Z","iopub.status.idle":"2021-06-10T22:56:25.50179Z","shell.execute_reply.started":"2021-06-10T21:44:16.979568Z","shell.execute_reply":"2021-06-10T22:56:25.500268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test_Imp = X_test.copy(deep=True)\nX_test_Imp = Iter_Imp.transform(X_test_Imp)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:56:25.503834Z","iopub.execute_input":"2021-06-10T22:56:25.504216Z","iopub.status.idle":"2021-06-10T22:56:30.511247Z","shell.execute_reply.started":"2021-06-10T22:56:25.504181Z","shell.execute_reply":"2021-06-10T22:56:30.510133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_Imp = X_train_Imp.astype(int)\nX_test_Imp = X_test_Imp.astype(int)\nX_train_Knn = X_train_Knn.astype(int)\nX_test_Knn = X_test_Knn.astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:56:30.513103Z","iopub.execute_input":"2021-06-10T22:56:30.513875Z","iopub.status.idle":"2021-06-10T22:56:30.747413Z","shell.execute_reply.started":"2021-06-10T22:56:30.513823Z","shell.execute_reply":"2021-06-10T22:56:30.746513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n#KNN Imputation train and test scaling\nX_train_Knn = scaler.fit_transform(X_train_Knn)\nX_test_Knn = scaler.transform(X_test_Knn)\n\nX_train_Imp = scaler.fit_transform(X_train_Imp)\nX_test_Imp = scaler.transform(X_test_Imp)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:56:30.758656Z","iopub.execute_input":"2021-06-10T22:56:30.759021Z","iopub.status.idle":"2021-06-10T22:56:31.423172Z","shell.execute_reply.started":"2021-06-10T22:56:30.758993Z","shell.execute_reply":"2021-06-10T22:56:31.422349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Let's check how Knn and Iterative Imputed Data performs","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression()\n\nlr.fit(X_train_Knn, y_train)\nprediction_KNN = lr.predict(X_test_Knn)\n\nlr.fit(X_train_Imp, y_train)\nprediction_IMP = lr.predict(X_test_Imp)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:56:31.424288Z","iopub.execute_input":"2021-06-10T22:56:31.424733Z","iopub.status.idle":"2021-06-10T22:56:36.851464Z","shell.execute_reply.started":"2021-06-10T22:56:31.424701Z","shell.execute_reply":"2021-06-10T22:56:36.850078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluation(y_actual, predicted):\n    cnf_matrix = confusion_matrix(y_actual, predicted)\n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\", fmt='g')\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    labels = ['No', 'Yes']\n    print(classification_report(y_actual, predicted, target_names=labels))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:56:36.853351Z","iopub.execute_input":"2021-06-10T22:56:36.853991Z","iopub.status.idle":"2021-06-10T22:56:36.86262Z","shell.execute_reply.started":"2021-06-10T22:56:36.853939Z","shell.execute_reply":"2021-06-10T22:56:36.861474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#KNN evaluation\n\nevaluation(y_test, prediction_IMP)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:56:36.864487Z","iopub.execute_input":"2021-06-10T22:56:36.865276Z","iopub.status.idle":"2021-06-10T22:56:37.259017Z","shell.execute_reply.started":"2021-06-10T22:56:36.865221Z","shell.execute_reply":"2021-06-10T22:56:37.257965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Precision recall curve for KNN\ny_pred_prob = lr.predict_proba(X_test_Knn)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:56:37.260455Z","iopub.execute_input":"2021-06-10T22:56:37.260771Z","iopub.status.idle":"2021-06-10T22:56:37.487747Z","shell.execute_reply.started":"2021-06-10T22:56:37.260739Z","shell.execute_reply":"2021-06-10T22:56:37.486994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"AUC score is: \", roc_auc_score(y_test, prediction_KNN))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:56:37.48903Z","iopub.execute_input":"2021-06-10T22:56:37.489525Z","iopub.status.idle":"2021-06-10T22:56:37.503764Z","shell.execute_reply.started":"2021-06-10T22:56:37.489488Z","shell.execute_reply":"2021-06-10T22:56:37.502027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Precision recall curve for IMP\ny_pred_prob = lr.predict_proba(X_test_Imp)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\nplt.plot(precision, recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision Recall Curve')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:56:37.506055Z","iopub.execute_input":"2021-06-10T22:56:37.506849Z","iopub.status.idle":"2021-06-10T22:56:37.770764Z","shell.execute_reply.started":"2021-06-10T22:56:37.506802Z","shell.execute_reply":"2021-06-10T22:56:37.769574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"AUC score is: \", roc_auc_score(y_test, prediction_IMP))","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:56:37.772451Z","iopub.execute_input":"2021-06-10T22:56:37.772784Z","iopub.status.idle":"2021-06-10T22:56:37.786797Z","shell.execute_reply.started":"2021-06-10T22:56:37.772752Z","shell.execute_reply":"2021-06-10T22:56:37.785822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> As AUC is a bit better with Iterative Imputation, I will use Iterative Imputed data ahead.","metadata":{}},{"cell_type":"markdown","source":"**Imbalance techniques**","metadata":{}},{"cell_type":"markdown","source":"> Why deal with Imbalance class problem? As we can see below, there is a huge difference between YES and NO in our binary target class. This will make our model be biased towards predicting more No's than Yes. And hence we will see a very good accuracy. Maybe, a whopping 98%. But did the model learn? Not at all. The classifiers strive to get a good performance in the training data and concentrate on learning the pattern of the \"majority\" class more than the \"minority\" class. If a student knows that 98% of the questions in exam will be from Science and geography and 2% from History, they will happily ignore History. While dealing with imbalance problems, we should remember that Accuracy is not the metric we should be focusing On. Metrics such as Precision score, recall, F1 score, ROC_AUC should be our main focus.\n\n> Real life applications where we have to deal with class imbalance are fake news detection, fraud detection, intrusion detection etc.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(6, 8))\nax = sns.countplot(x=\"RainTomorrow\", data=data, palette=\"Set1\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:56:37.788997Z","iopub.execute_input":"2021-06-10T22:56:37.789695Z","iopub.status.idle":"2021-06-10T22:56:37.932043Z","shell.execute_reply.started":"2021-06-10T22:56:37.789648Z","shell.execute_reply":"2021-06-10T22:56:37.930686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***<i> Borderline Smote***","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\nimport imblearn\nfrom imblearn.over_sampling import BorderlineSMOTE\nover = BorderlineSMOTE(random_state=142)\n\nXtrain_BLS, ytrain_BLS = over.fit_resample(X_train_Imp, y_train)\n\ncounter = Counter(ytrain_BLS)\n\nfor label, _ in counter.items():\n    X_train_scatter = np.array(Xtrain_BLS)\n    y_train_scatter = np.array(ytrain_BLS)\n    row_ix = np.where(y_train_scatter == label)[0]\n    plt.scatter(X_train_scatter[row_ix, 0], X_train_scatter[row_ix, 1], label=str(label))\nplt.title(f\"{counter}\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:56:37.936102Z","iopub.execute_input":"2021-06-10T22:56:37.936423Z","iopub.status.idle":"2021-06-10T22:57:58.876682Z","shell.execute_reply.started":"2021-06-10T22:56:37.936394Z","shell.execute_reply":"2021-06-10T22:57:58.875686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***<i> SVM Smote***","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SVMSMOTE\nover = SVMSMOTE(random_state=142)\nXtrain_SVM, ytrain_SVM = over.fit_resample(X_train_Imp, y_train)\n\ncounter = Counter(ytrain_SVM)\n\nfor label, _ in counter.items():\n    X_train_scatter = np.array(Xtrain_SVM)\n    y_train_scatter = np.array(ytrain_SVM)\n    row_ix = np.where(y_train_scatter == label)[0]\n    plt.scatter(X_train_scatter[row_ix, 0], X_train_scatter[row_ix, 1], label=str(label))\nplt.title(f\"{counter}\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T22:57:58.878512Z","iopub.execute_input":"2021-06-10T22:57:58.878912Z","iopub.status.idle":"2021-06-10T23:25:51.590601Z","shell.execute_reply.started":"2021-06-10T22:57:58.878872Z","shell.execute_reply":"2021-06-10T23:25:51.589324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***<i> ADASYN***","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import ADASYN\nover = ADASYN(random_state=142)\n\nXtrain_ADA, ytrain_ADA = over.fit_resample(X_train_Imp, y_train)\n\ncounter = Counter(ytrain_ADA)\n\nfor label, _ in counter.items():\n    X_train_scatter = np.array(Xtrain_ADA)\n    y_train_scatter = np.array(ytrain_ADA)\n    row_ix = np.where(y_train_scatter == label)[0]\n    plt.scatter(X_train_scatter[row_ix, 0], X_train_scatter[row_ix, 1], label=str(label))\nplt.title(f\"{counter}\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:25:51.592881Z","iopub.execute_input":"2021-06-10T23:25:51.593357Z","iopub.status.idle":"2021-06-10T23:27:17.82599Z","shell.execute_reply.started":"2021-06-10T23:25:51.593309Z","shell.execute_reply":"2021-06-10T23:27:17.824728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***<i> SMOTETomek***","metadata":{}},{"cell_type":"code","source":"from imblearn.combine import SMOTETomek\n\nsmotek = SMOTETomek(random_state=142)\nXtrain_SMT, ytrain_SMT = smotek.fit_resample(X_train_Imp, y_train)\n\ncounter = Counter(ytrain_SMT)\n\nfor label, _ in counter.items():\n    X_train_scatter = np.array(Xtrain_SMT)\n    y_train_scatter = np.array(ytrain_SMT)\n    row_ix = np.where(y_train_scatter == label)[0]\n    plt.scatter(X_train_scatter[row_ix, 0], X_train_scatter[row_ix, 1], label=str(label))\nplt.title(f\"{counter}\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:27:17.82813Z","iopub.execute_input":"2021-06-10T23:27:17.828572Z","iopub.status.idle":"2021-06-10T23:36:24.277828Z","shell.execute_reply.started":"2021-06-10T23:27:17.828527Z","shell.execute_reply":"2021-06-10T23:36:24.276675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model**\n\n> The class Model below takes inputs and runs a GridsearchCV and pulls the best parameters. Once the best parameters are generated after instatiating the class, the next steps fit the model with training sets you pass and gives you the desired output. You can perform hyperparameter tuning as per your requirement, however I havent done any hyperparameter tuning due to computing constraints. But you can play with the parameters, all you have to do is the pass the list of parameters. I will also just test the model with Borderline Smote data, but it should not stop you from testing with the others.","metadata":{}},{"cell_type":"code","source":"model = list()\nresample = list()\naccuracy = list()\nprecision = list()\nrecall = list()\nF1score = list()\nAUCROC = list()\n\nclass Model:\n    def __init__(self, X_train, y_train, X_test, y_test, model_type=None, enhanced_model_type=None, params=None, algo=None, sampling=None):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        self.params = params\n        self.algo = algo\n        self.sampling = sampling\n\n        if model_type == 'rf':\n            self.user_defined_model = RandomForestClassifier()\n        elif model_type == 'lr':\n            self.user_defined_model = LogisticRegression()\n        elif model_type == 'ada':\n            self.user_defined_model = AdaBoostClassifier()\n        elif model_type == 'sgd':\n            self.user_defined_model = SGDClassifier()\n    \n        self.cv = StratifiedKFold(n_splits=5, random_state=142, shuffle=True)\n        self.GS = GridSearchCV(self.user_defined_model, param_grid=params, cv=self.cv, scoring='roc_auc', n_jobs=-1, refit=True)\n        self.MS = self.GS.fit(X_train, y_train)\n        self.best_param = self.MS.best_params_\n        print(self.best_param)\n\n        if enhanced_model_type == 'rf':\n            self.enhanced_model = RandomForestClassifier(**self.best_param)\n        elif enhanced_model_type == 'lr':\n            self.enhanced_model = LogisticRegression(**self.best_param)\n        elif enhanced_model_type == 'ada':\n            self.enhanced_model = AdaBoostClassifier(**self.best_param)\n        elif enhanced_model_type == 'sgd':\n            self.enhanced_model = SGDClassifier(**self.best_param)\n            \n    def fit(self, X_train, y_train):\n        self.model = self.enhanced_model.fit(X_train, y_train)\n        return self.model\n\n    def predict(self, X_test):\n        y_pred = self.model.predict(X_test)\n        return y_pred\n    \n    def predict_prob(self, X_test):\n        y_prob = self.model.predict_proba(X_test)\n        return y_prob\n\n    def append_metrics(self, X_test, y_test):\n        y_pred = self.model.predict(X_test)\n        y_prob = self.model.predict_proba(X_test)\n        model.append(self.algo)\n        accuracy.append(accuracy_score(y_test, y_pred, normalize=False)) #returns the no of correctly classified samples\n        precision.append(precision_score(y_test, y_pred))\n        recall.append(recall_score(y_test, y_pred))\n        F1score.append(f1_score(y_test, y_pred))\n        AUCROC.append(roc_auc_score(y_test, y_prob[:,1]))\n        resample.append(self.sampling)\n    \n    def print_metric(self, X_train, y_train, X_test, y_test):\n        y_pred = self.model.predict(X_test)\n        y_prob = self.model.predict_proba(X_test)\n        print(\"=\"*60)\n        print(\"Confusion Matrix\")\n        print(\"-\"*30)\n        print(confusion_matrix(y_test, y_pred), \"\\n\")\n        print(\"=\"*60)\n        print(\"Classification Report\")\n        print(\"-\"*30)\n        print(classification_report(y_test, y_pred), \"\\n\")\n        print(\"=\"*60)\n        print(\"ROC-AUC score\")\n        print(\"-\"*30)\n        print(roc_auc_score(y_test, y_prob[:,1]))\n        print(\"*\"*60)\n  \n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:36:24.279521Z","iopub.execute_input":"2021-06-10T23:36:24.279799Z","iopub.status.idle":"2021-06-10T23:36:24.30124Z","shell.execute_reply.started":"2021-06-10T23:36:24.279772Z","shell.execute_reply":"2021-06-10T23:36:24.300183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Try out hyperparameter tuning as per your convenience on the below models. I just ran the models with single-value paramters as it would take a lot of time otherwise.","metadata":{}},{"cell_type":"markdown","source":"***Logistic regression with BorderlineSMOTE***","metadata":{}},{"cell_type":"code","source":"params = {'C':[10],'class_weight':['balanced'], 'solver':['lbfgs'], 'max_iter': [1000], 'n_jobs': [-1]}\nlogreg_Borderline = Model(Xtrain_BLS, ytrain_BLS, X_test_Imp, y_test, model_type='lr', enhanced_model_type='lr', params=params, algo='Logistic', sampling='BLS')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:36:24.302784Z","iopub.execute_input":"2021-06-10T23:36:24.303262Z","iopub.status.idle":"2021-06-10T23:37:08.203608Z","shell.execute_reply.started":"2021-06-10T23:36:24.303218Z","shell.execute_reply":"2021-06-10T23:37:08.20197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg_Borderline.fit(Xtrain_BLS, ytrain_BLS)\nlogreg_Borderline.predict(X_test_Imp)\nlogreg_Borderline.predict_prob(X_test_Imp)\nlogreg_Borderline.append_metrics(X_test_Imp, y_test)\nlogreg_Borderline.print_metric(Xtrain_BLS, ytrain_BLS, X_test_Imp, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:37:08.206004Z","iopub.execute_input":"2021-06-10T23:37:08.206539Z","iopub.status.idle":"2021-06-10T23:37:36.393203Z","shell.execute_reply.started":"2021-06-10T23:37:08.20648Z","shell.execute_reply":"2021-06-10T23:37:36.391994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***RandomForest with BorderlineSMOTE***","metadata":{}},{"cell_type":"code","source":"n_estimators = [1100]\nmax_features = ['sqrt']\nmax_depth = [200]\nmax_depth.append(None)\nmin_samples_split = [2]\nmin_samples_leaf = [2]\nbootstrap = [True]\n  \nparams = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrandomforest_BLS = Model(Xtrain_BLS, ytrain_BLS, X_test_Imp, y_test, model_type='rf', enhanced_model_type='rf', params=params, algo='RandomForest', sampling='BLS')","metadata":{"execution":{"iopub.status.busy":"2021-06-10T23:38:12.103698Z","iopub.execute_input":"2021-06-10T23:38:12.104376Z","iopub.status.idle":"2021-06-11T00:18:46.281753Z","shell.execute_reply.started":"2021-06-10T23:38:12.104316Z","shell.execute_reply":"2021-06-11T00:18:46.279714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"randomforest_BLS.fit(Xtrain_BLS, ytrain_BLS)\nrandomforest_BLS.predict(X_test_Imp)\nrandomforest_BLS.predict_prob(X_test_Imp)\nrandomforest_BLS.append_metrics(X_test_Imp, y_test)\nrandomforest_BLS.print_metric(Xtrain_BLS, ytrain_BLS, X_test_Imp, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:18:46.286401Z","iopub.execute_input":"2021-06-11T00:18:46.286783Z","iopub.status.idle":"2021-06-11T00:29:14.67986Z","shell.execute_reply.started":"2021-06-11T00:18:46.286744Z","shell.execute_reply":"2021-06-11T00:29:14.678247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Adaboost with BorderlineSMOTE***","metadata":{}},{"cell_type":"code","source":"n_estimators = [1500]\nlearning_rate = [0.01]\n\nparams = {'n_estimators': n_estimators,\n          'learning_rate': learning_rate}\n\nadaboost_BLS = Model(Xtrain_BLS, ytrain_BLS, X_test_Imp, y_test, model_type='ada', enhanced_model_type='ada', params=params, algo='Adaboost', sampling='BLS')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T00:29:14.682534Z","iopub.execute_input":"2021-06-11T00:29:14.682974Z","iopub.status.idle":"2021-06-11T01:31:38.194218Z","shell.execute_reply.started":"2021-06-11T00:29:14.68293Z","shell.execute_reply":"2021-06-11T01:31:38.192699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adaboost_BLS.fit(Xtrain_BLS, ytrain_BLS)\nadaboost_BLS.predict(X_test_Imp)\nadaboost_BLS.append_metrics(X_test_Imp, y_test)\nadaboost_BLS.print_metric(Xtrain_BLS, ytrain_BLS, X_test_Imp, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T01:31:38.198302Z","iopub.execute_input":"2021-06-11T01:31:38.198703Z","iopub.status.idle":"2021-06-11T01:54:14.220679Z","shell.execute_reply.started":"2021-06-11T01:31:38.198659Z","shell.execute_reply":"2021-06-11T01:54:14.219171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***SGDClassifier with BorderlineSMOTE***","metadata":{}},{"cell_type":"code","source":"params = {\n    'alpha': [1e-3],#0.001\n    'max_iter' : [1500],\n    'class_weight': ['balanced'],\n    'loss': ['log'],\n    'eta0': [0.05],\n    'penalty': ['elasticnet'],\n    'n_jobs': [-1]\n    }\nSGD_BLS = Model(Xtrain_BLS, ytrain_BLS, X_test_Imp, y_test, model_type='sgd', enhanced_model_type='sgd', params=params, algo='SGDClassifier', sampling='BLS')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T01:54:14.222441Z","iopub.execute_input":"2021-06-11T01:54:14.222773Z","iopub.status.idle":"2021-06-11T01:54:24.256706Z","shell.execute_reply.started":"2021-06-11T01:54:14.222736Z","shell.execute_reply":"2021-06-11T01:54:24.255039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SGD_BLS.fit(Xtrain_BLS, ytrain_BLS)\nSGD_BLS.predict(X_test_Imp)\nSGD_BLS.append_metrics(X_test_Imp, y_test)\nSGD_BLS.print_metric(Xtrain_BLS, ytrain_BLS, X_test_Imp, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T01:54:24.259198Z","iopub.execute_input":"2021-06-11T01:54:24.259648Z","iopub.status.idle":"2021-06-11T01:54:27.135883Z","shell.execute_reply.started":"2021-06-11T01:54:24.259595Z","shell.execute_reply":"2021-06-11T01:54:27.134796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_eval_df = pd.DataFrame({'model':model,\n                            'resample':resample,\n                            'accuracy':accuracy,\n                            'precision':precision,\n                            'recall':recall,\n                            'f1-score':F1score,\n                            'AUC-ROC':AUCROC})","metadata":{"execution":{"iopub.status.busy":"2021-06-11T01:54:27.137667Z","iopub.execute_input":"2021-06-11T01:54:27.138Z","iopub.status.idle":"2021-06-11T01:54:27.147464Z","shell.execute_reply.started":"2021-06-11T01:54:27.137964Z","shell.execute_reply":"2021-06-11T01:54:27.146283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_eval_df","metadata":{"execution":{"iopub.status.busy":"2021-06-11T01:54:27.150416Z","iopub.execute_input":"2021-06-11T01:54:27.150891Z","iopub.status.idle":"2021-06-11T01:54:27.180684Z","shell.execute_reply.started":"2021-06-11T01:54:27.150856Z","shell.execute_reply":"2021-06-11T01:54:27.179582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> If we just consider Recall, logistic regression did a great job.\n> Accuracy shows the no of samples correctly classified","metadata":{}},{"cell_type":"markdown","source":"**Voting Classifiers**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nlogreg = LogisticRegression(C=10, class_weight='balanced', solver='lbfgs', max_iter=1000, n_jobs=-1)\nxgb_classifier = XGBClassifier(gamma=0.0468, learning_rate=0.05, max_depth=3, n_estimators=1500, nthread = -1, random_state = 142)\nadaboost = AdaBoostClassifier(n_estimators = 1000, learning_rate = 0.001)\nsgdclassifier = SGDClassifier(alpha = 1e-3, max_iter = 1000, class_weight = 'balanced', loss = 'log', eta0=0.05, penalty='elasticnet', n_jobs=-1)\n\nmodels = [('log_reg', logreg), ('xgb', xgb_classifier), ('adaboost', adaboost), ('sgd', sgdclassifier)]\n\nvoting_hard = VotingClassifier(estimators=models, voting='hard', n_jobs=-1)\n\nvoting_hard.fit(Xtrain_BLS,ytrain_BLS)\n\nvoting_soft = VotingClassifier(estimators=models, voting='soft', n_jobs=-1)\n\nvoting_soft.fit(Xtrain_BLS,ytrain_BLS)\n\npred_hard=voting_hard.predict(X_test_Imp)\npred_soft=voting_soft.predict(X_test_Imp)\n\nprint(\"Hard Voting Scores\")\n\nprint(\"=\"*30)\n\nprint(\"Precision score\", precision_score(y_test, pred_hard))\n\nprint(\"Recall score\", recall_score(y_test, pred_hard))\n\nprint(\"F1 score\", f1_score(y_test, pred_hard))\n\nprint(confusion_matrix(y_test, pred_hard))\n  \nprint(classification_report(y_test, pred_hard))\n\nprint(\"=\"*30)\n\nprint(\"Soft Voting Scores\")\n\nprint(\"Precision score\", precision_score(y_test, pred_soft))\n\nprint(\"Recall score\", recall_score(y_test, pred_soft))\n\nprint(\"F1 score\", f1_score(y_test, pred_soft))\n\nprint(confusion_matrix(y_test, pred_soft))\n  \nprint(classification_report(y_test, pred_soft))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T01:54:27.182536Z","iopub.execute_input":"2021-06-11T01:54:27.182845Z","iopub.status.idle":"2021-06-11T02:32:46.564847Z","shell.execute_reply.started":"2021-06-11T01:54:27.182814Z","shell.execute_reply":"2021-06-11T02:32:46.563151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> As Voting classifier does not output class probabilities, we do not get fetch the AUCROC score directly. But we can see voting classifier did a very good job than others, in terms of recall.","metadata":{}},{"cell_type":"markdown","source":"**Stacking Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n\nlogreg = LogisticRegression(C=10, class_weight='balanced', solver='lbfgs', max_iter=1000, n_jobs=-1)\nxgb_classifier = XGBClassifier(gamma=0.0468, learning_rate=0.05, max_depth=3, n_estimators=1500, nthread = -1, random_state = 142)\nadaboost = AdaBoostClassifier(n_estimators = 1000, learning_rate = 0.001)\nsgdclassifier = SGDClassifier(alpha = 1e-3, max_iter = 1000, class_weight = 'balanced', loss = 'log', eta0=0.05, penalty='elasticnet', n_jobs=-1)\n\nestimators = [('log_reg', logreg), ('xgb', xgb_classifier), ('adaboost', adaboost), ('sgd', sgdclassifier)]\n\nfinal_estimator = RandomForestClassifier(n_estimators=1500, max_features='sqrt', max_depth=200, min_samples_split=2, min_samples_leaf=2, bootstrap=True)\n\nstack = StackingClassifier(estimators=estimators, final_estimator=final_estimator, cv=5, n_jobs=-1, passthrough=True, verbose=2)\n\nstack.fit(Xtrain_BLS, ytrain_BLS)\n\npred = stack.predict(X_test_Imp)\n\npredprob = stack.predict_proba(X_test_Imp)\n\nprint(\"Precision score\", precision_score(y_test, pred))\n\nprint(\"Recall score\", recall_score(y_test, pred))\n\nprint(\"F1 score\", f1_score(y_test, pred))\n\nprint(\"AUC_ROC score\", roc_auc_score(y_test, predprob[:,1]))\n\nprint(confusion_matrix(y_test, pred))\n  \nprint(classification_report(y_test, pred))","metadata":{"execution":{"iopub.status.busy":"2021-06-11T02:32:46.569023Z","iopub.execute_input":"2021-06-11T02:32:46.56944Z","iopub.status.idle":"2021-06-11T04:02:09.934173Z","shell.execute_reply.started":"2021-06-11T02:32:46.569402Z","shell.execute_reply":"2021-06-11T04:02:09.932822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The overall score in all the models could have been improved if we would have followed extensive feature selection process, feature engineering, feature extraction, hyperparameter tuning, multicolinearity check etc. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}