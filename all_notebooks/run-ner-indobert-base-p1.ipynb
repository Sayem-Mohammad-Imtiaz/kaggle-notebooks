{"cells":[{"metadata":{},"cell_type":"markdown","source":"# # Run run_ner_indobert.ipynb\nCredits:\nCleaned train and test data from:\nhttps://www.kaggle.com/zeyalt/scl-2021-data-science-part-1-data-cleaning/comments?select=cleaned_test.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nfrom functools import partial\ntqdm = partial(tqdm, position=0, leave=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n# Download transformers  and install required packages.\n!git clone https://github.com/huggingface/transformers\n%cd transformers\n!pip install .\n!pip install -r ./examples/_tests_requirements.txt \n%cd ..","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{},"cell_type":"markdown","source":"Add data from:\nhttps://www.kaggle.com/numerator/scl2021data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/scl2021data/cleaned_train.csv').fillna('')\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/scl2021data/cleaned_test.csv').fillna('')\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train dev split - 90:10\nimport numpy as np\n#train, dev, test =  np.split(train_df.sample(frac=1, random_state=42), \n#                       [int(.6*len(train_df)), int(.8*len(train_df))])\ntrain, dev =  np.split(train_df.sample(frac=1, random_state=42), \n                       [int(.9*len(train_df))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dev.to_csv('dev.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Process raw data by assigning entity POI or ST or O to each word\ndef preprocess_train_data(raw_df, output_name):\n    with open(output_name, 'a') as text_file:\n        for index, row in tqdm(raw_df.iterrows()):\n            address = row['raw_address'].replace(\",\", \"\").split()\n            if row['POI'] == '':\n                poi = ''\n            else:\n                poi = row['POI'].split()\n            if row['street'] == '':\n                st = ''\n            else:\n                street = row['street'].split()\n            for address_word in address:\n                if any(address_word in p for p in poi):\n                    text_file.write(address_word + ' POI \\n')  \n                elif any(address_word in s for s in street):\n                    text_file.write(address_word + ' ST \\n') \n                else:\n                    text_file.write(address_word + ' O \\n')  \n\npreprocess_train_data(train, 'train_temp.txt')\npreprocess_train_data(dev, 'dev_temp.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_temp_txt = pd.read_table('train_temp.txt')\nprint(train_temp_txt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Process raw data by assigning entity POI or ST or O to each word\ndef preprocess_test_data(raw_df, output_name):\n    start_counter = 1\n    end_counter = 0\n    raw_df['start'] = start_counter\n    raw_df['end'] = end_counter\n    with open(output_name, 'a') as text_file:\n        for index, row in tqdm(raw_df.iterrows()):\n            raw_df.loc[index, 'start'] = start_counter\n            address = row['raw_address'].replace(\",\", \"\").split()\n            for address_word in address:\n                text_file.write(address_word + '\\n')\n                start_counter += 1\n                end_counter += 1\n            raw_df.loc[index, 'end'] = end_counter\n    return raw_df\n\ntest_processed_df = preprocess_test_data(test_df, 'test_temp.txt')\ntest_processed_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess data"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Set parameters\nMAX_LENGTH = 128 #@param {type: \"integer\"}\nMODEL = \"indobenchmark/indobert-lite-base-p1\" #@param [\"chriskhanhtran/spanberta\", \"bert-base-multilingual-cased\", \"indobenchmark/indobert-lite-base-p1\", \"indobenchmark/indobert-large-p1\"]\nPATH = \"/kaggle/input/scl2021-src/\"\n!python3 $PATH/preprocess.py train_temp.txt $MODEL $MAX_LENGTH > train.txt\n!python3 $PATH/preprocess.py dev_temp.txt $MODEL $MAX_LENGTH > dev.txt\n!python3 $PATH/preprocess.py test_temp.txt $MODEL $MAX_LENGTH > test.txt\n# Generate labels.txt\n!cat train.txt dev.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt\nlabels_txt = pd.read_table('labels.txt')\nprint(labels_txt)\ntrain_txt = pd.read_table('train.txt')\nprint(train_txt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Fine-tuning Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n# install required packages\n!pip install seqeval\n!pip install datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# Set training parameters\nMAX_LENGTH = 128 #@param {type: \"integer\"}\nMODEL = \"indobenchmark/indobert-large-p1\" #@param [\"chriskhanhtran/spanberta\", \"bert-base-multilingual-cased\", \"indobenchmark/indobert-lite-base-p1\", \"indobenchmark/indobert-large-p1\"]\nOUTPUT_DIR = \"/kaggle/working/indobert-ner\" #@param [\"spanberta-ner\", \"bert-base-ml-ner\", \"indobert-ner\", \"drive/MyDrive/Shopee\"]\nBATCH_SIZE = 16 #@param {type: \"integer\"}\nNUM_EPOCHS = 3 #@param {type: \"integer\"}\nSAVE_STEPS = 2000 #@param {type: \"integer\"}\nLOGGING_STEPS = 1000 #@param {type: \"integer\"}\nSEED = 42 #@param {type: \"integer\"}\n\n!python3 $PATH/run_ner.py \\\n  --data_dir ./ \\\n  --model_type bert \\\n  --labels ./labels.txt \\\n  --model_name_or_path $MODEL \\\n  --output_dir $OUTPUT_DIR \\\n  --max_seq_length  $MAX_LENGTH \\\n  --num_train_epochs $NUM_EPOCHS \\\n  --per_gpu_train_batch_size $BATCH_SIZE \\\n  --save_steps $SAVE_STEPS \\\n  --logging_steps $LOGGING_STEPS \\\n  --seed $SEED \\\n  --do_train \\\n  --do_eval \\\n  --do_predict \\\n  --overwrite_output_dir\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Process output to submission format"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions = pd.read_table('indobert-ner/test_predictions.txt')\nprint(test_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"test_predictions = pd.read_table('indobert-ner/test_predictions.txt')\nprint(test_predictions)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"test_predictions = pd.read_table('indobert-ner/test_predictions.txt')\nprint(test_predictions)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def extract_word(file, row_start, row_end):\n    '''\n    format model prediction output in submission format\n    '''\n    poi = ''\n    st = ''\n    f=open(file)\n    lines = f.readlines()\n    i = row_start\n    while i >= row_start and i <= row_end:\n        if len(lines[i]) > 1 :\n            word = lines[i].split()[0]  \n            tag = lines[i].split()[1]\n        else:\n            tag = 'O'\n        if tag == 'POI':\n            poi = poi + ' ' + word\n        elif tag == 'ST':\n            st = st + ' ' + word\n        i += 1\n    return poi.strip() + '/' + st.strip()"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"test_processed_df['POI/street']=''\nprintcounter = 0\nfor i in tqdm(range(0, len(test_processed_df))):\n    row_start = test_processed_df.loc[i, 'start']\n    row_end = test_processed_df.loc[i, 'end']\n    test_processed_df.loc[i, 'POI/street'] = extract_word('indobert-ner/test_predictions.txt', row_start, row_end)\n    # add checkpoints\n    if (printcounter == 1000):\n        test_processed_df[['id','POI/street']].to_csv('submit.csv', index=False)\n        printcounter = 0\n        printcounter += 1\ntest_processed_df.head()"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}