{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Employee Attrition Problem:**\n\nThe key to success in any organization is attracting and retaining top talent who stays and works together. One of the key tasks is to prevent an employee from leaving the company. We will use machine learning to predict the employees which are going to leave the company and try to prevent it from happening.\n\nDescription of few variables:\n\nData Description:\n\n1. **status** – Current employment status (Employed / Left)\n1\n2. **department** – Department employees belong(ed) to\n3. **salary** – Salary level relative to rest of their department\n4. **tenure** – Number of years at the company\n5. **recently_promoted** – Was the employee promoted in the last 3 years?\n6. **n_projects** – Number of projects employee is staffed on\n7. **avg_monthly_hrs** – Average number of hours worked per month\n8. **satisfaction** – Score for employee’s satisfaction with the company (higher is better)\n9. **last_evaluation** – Score for most recent evaluation of employee (higher is better)\n10. **filed_complaint** – Has the employee filed a formal complaint in the last 3 years?\n","metadata":{}},{"cell_type":"markdown","source":"# Importing Python Libraries","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport warnings\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importing Dataset and Performing Descriptive Statistics**","metadata":{}},{"cell_type":"code","source":"#Importing Train dataset into Colab\ndata=pd.read_csv('../input/predicting-employee-status/employee_data (1).csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#First few rows of the dataframe\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all types of colums\ndata.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Can Statistic data on each column to understand the data better\ndata.describe(include='all')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Dublicate Rows**\n\nBefore we will start working on the data, let's make sure there is no duplicate data in our dataset.","metadata":{}},{"cell_type":"code","source":"# View all dublicate row\ndata.duplicated().sum()\ndata[data.duplicated()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nWe can see that most of the dublicated rows caused due has Null values, so that are not really dublicated. So we wont remove them.","metadata":{}},{"cell_type":"markdown","source":"# **Handle Missing Values**\n\nLet's check if we have missings cells in our dataset.","metadata":{}},{"cell_type":"code","source":"# Finding number of null values in individual column\ndata.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Overall Observations Missing Value Analysis:**\n\nBelow are colums with the  missing values in our dataset:\n\n1. departmen\n2. filed_complaint\n3. last_evaluation\n4. recently_promoted\n5. satisfaction\n6. tenure\n\nLet's plot the missing values.","metadata":{}},{"cell_type":"code","source":"nullTable=round((data.isnull().sum()/data.shape[0])*100,2)\nnullValueCols=pd.DataFrame(nullTable,columns=['Missing Value %'])\nnullValueCols.reset_index(inplace=True)\nnullValueCols.rename(columns={'index': 'Column Name'},inplace=True)\nnullValueCols[nullValueCols['Missing Value %']!=0]\nprint(nullTable)\n\nsns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='plasma')\nplt.title(\"Heat map plotting the missing values in the columns\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fill Missing Values in depertment:**\n\nLet's check this column values in order to decide what values we can fill the the missing spots\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'department', data = data)\nplt.title(\"Countplot for department Column\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The department column is a categorial column and has diffrent types of departments. Some of the workers in the company does not have any department value, maybe they are not in specific department so we will create for them a new department type called 'other'.","metadata":{}},{"cell_type":"code","source":"# filling all null values with new department type 'other'\ndata['department'] = data['department'].fillna('other')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fill Missing Values in filed_complaint and recently_promoted:**\n\nLet's check this column values in order to decide what values we can fill the the missing spots","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'filed_complaint', data = data)\nplt.title(\"Countplot for filed_complaint Column\")\nplt.show()\n\nplt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'recently_promoted', data = data)\nplt.title(\"Countplot for recently_promoted Column\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"filed_complaint is saying if the employee has filed a formal complaint in the last 3 years, as we can see in the plot 1 is when the employee field a complaint. All missing values are when the employee didnt file it. As for this , the missing values will be 0. Also recently_promoted is the same, only the one who recently promoted are mask with 1 , all the missing ones should be 0.","metadata":{}},{"cell_type":"code","source":"# filling all null values of filed_complaint and recently_promoted with 0\ndata['filed_complaint'] = data['filed_complaint'].fillna(0)\ndata['recently_promoted'] = data['recently_promoted'].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fill Missing Values in last_evaluation & satisfaction & tenure:**\n\nLet's check this column values in order to decide what values we can fill the the missing spots","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'last_evaluation', data = data.sample(n=1000))\nplt.title(\"Countplot for last_evaluation Column\")\nplt.show()\n\nplt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'satisfaction', data = data.sample(n=1000))\nplt.title(\"Countplot for satisfaction Column\")\nplt.show()\n\nplt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'tenure', data = data.sample(n=1000))\nplt.title(\"Countplot for tenure Column\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nAs we can in the plot all those feilds are numerical and distrubuted well. We will fill when with their mean.","metadata":{}},{"cell_type":"code","source":"data['last_evaluation'] = data['last_evaluation'].fillna(data['last_evaluation'].mean())\ndata['satisfaction'] = data['satisfaction'].fillna(data['satisfaction'].mean())\ndata['tenure'] = data['tenure'].fillna(data['tenure'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Verify That There is no Missing Values:**","metadata":{}},{"cell_type":"code","source":"sns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='plasma')\nplt.title(\"Heat map plotting the missing values in the columns\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Outliers**\n\nLet's plot our numerical colums to see if we have some outliers","metadata":{}},{"cell_type":"code","source":"NewNumeric=data[['last_evaluation','n_projects','satisfaction','tenure']]\nNewNumericMelt=NewNumeric.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp=sns.boxplot(x='variable',y='value',data=NewNumericMelt)\nbp.set_xticklabels(bp.get_xticklabels(),rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nWe have some outliers in tenure column. Most the people stays in the company for 3-4 years. But some people are more than 6 years or even 10(maybe the founders). Let's check how much from the dataset this outliers represents:","metadata":{}},{"cell_type":"code","source":"100 * (data[\"tenure\"] > 6).sum() / data.shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see the outliers are 3.76% of out data. Let's see how many people who stays more that 6 years left the company.","metadata":{}},{"cell_type":"code","source":"above6years = data[data['tenure'] > 6]\n(above6years['status'] == 'Left').sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no people at the level that left the comapny. Since our goal is to find those people we will remove this outlier data becouse it wont have us quite.","metadata":{}},{"cell_type":"code","source":"# saving all the people who are working only less then 6 years\ndata = data[data['tenure'] < 6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NewNumeric=data[['last_evaluation','n_projects','satisfaction','tenure']]\nNewNumericMelt=NewNumeric.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp=sns.boxplot(x='variable',y='value',data=NewNumericMelt)\nbp.set_xticklabels(bp.get_xticklabels(),rotation=90)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's make sure we dont have any outliers now","metadata":{}},{"cell_type":"markdown","source":"# **Imbalance Analysis**\n\nBefore we will start working on the data, let's make sure the data is balanced and we have enought cases of people who Left the company. If not , we will use SMOTE to create more data.\n\n\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nAttrplot=sns.countplot(x = 'status', data = data)\nplt.title(\"Countplot for status Column\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nGood for us ,we dont have imbalance data. third of the rows are decribing emplyees who left the comapny. We are good to go.","metadata":{}},{"cell_type":"markdown","source":"# **Create Feature and Targets Matrixes**","metadata":{}},{"cell_type":"code","source":"# Convert 'Left' for 1 and 'Employed' for 0\nY = np.where(data['status'].values == 'Left', 1, 0)\n\n# dropping the target column and create the matrix of features\nX = data.drop(['status'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Categorial Analysis**","metadata":{}},{"cell_type":"code","source":"# view the number of unique values each column\nfeatures=data.columns\nfor i in features:\n    uniqueValues=data[i].nunique()\n    print(i,uniqueValues)\n    \ndata.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observation:**\n\nOur categorial colums are:\n1. department\n2. filed_complaint(Aready 0 and 1)\n3. recently_promoted(Aready 0 and 1)\n4. salary\n\nAll those feilds are already objects and all others are numbers(float). We can run get_dummies.","metadata":{}},{"cell_type":"code","source":"# make all categorial columns to separeted columns\nX = pd.get_dummies(X, drop_first=True)\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Numeric Analysis**","metadata":{}},{"cell_type":"markdown","source":"**Observation:**\n\nOur numeric colums are:\n1. avg_monthly_hrs\n2. last_evaluation\n3. n_projects\n4. satisfaction\n5. tenure\n\nWe will Scale those columns with the StandardScaler(x-std/mean)","metadata":{}},{"cell_type":"code","source":"num_cols=['avg_monthly_hrs','last_evaluation','n_projects','satisfaction','tenure']\nscaler = StandardScaler()\nX[num_cols] = scaler.fit_transform(X[num_cols])\nX.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Train Test Split**\n\nWe will split our data for train and test so we can verify our performance. Since only 30% of our data refers to people who left the company we want our train and test data to be balanced with this samples.","metadata":{}},{"cell_type":"code","source":"# train test split\n\n\ny_left = Y[Y == 1]\nx_left = X[Y == 1]\ny_stay = Y[Y == 0]\nx_stay = X[Y == 0]\n\nx_train_left, x_test_left, y_train_left, y_test_left = train_test_split(x_left,y_left , test_size = .25, random_state=45)\nx_train_stay, x_test_stay, y_train_stay, y_test_stay = train_test_split(x_stay,y_stay , test_size = .25, random_state=45)\nx_train = np.concatenate((x_train_left, x_train_stay), axis=0)\ny_train = np.concatenate((y_train_left, y_train_stay), axis=0)\nx_test = np.concatenate((x_test_left, x_test_stay), axis=0)\ny_test = np.concatenate((y_test_left, y_test_stay), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Building**","metadata":{}},{"cell_type":"markdown","source":"For every model I am using, I am running first RandomGridSearch to find the general rage of the optimize parameters from wide range parameters. Then I will run GridSearchCV to find the best parameters from small range of parameters by running each one. ","metadata":{}},{"cell_type":"markdown","source":"**Goal:**\nOur main goal is to predict the highest percentege of the people who are going to leave the company. It will be better to find more of them and mistake in some. Meaning the recall is more important for us then precision. ","metadata":{}},{"cell_type":"markdown","source":"# **DecisionTree**","metadata":{}},{"cell_type":"markdown","source":"first let's run Random Grid Search to find the general rage of the optimize parameters","metadata":{}},{"cell_type":"code","source":"#creating a random search for some hyper parameters given in param_grid_1\ndt=DecisionTreeClassifier()\nparam_grid={\n    'criterion':['gini','entropy'],\n    'max_depth':np.arange(4,20,1),\n    'min_samples_split':np.arange(0.001,0.1,0.01),\n    'max_features':['log2','sqrt','auto'],\n    'min_weight_fraction_leaf':np.arange(0.001,0.25,0.05)\n}\nr_search=RandomizedSearchCV(dt,param_distributions=param_grid,n_iter=10,verbose=1)\nr_search.fit(x_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting best performing hyper parameters from random search \nr_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's run GridSearchCV for specific parameters to get the best values","metadata":{}},{"cell_type":"code","source":"param_grid = {'min_weight_fraction_leaf': np.arange(0.001,0.01,0.001),\n 'min_samples_split': np.arange(0.07,0.12,0.01),\n 'max_features': ['auto'],\n 'max_depth': np.arange(10,20,1),\n 'criterion': ['entropy']}\ndt=DecisionTreeClassifier()\ngrid_search=GridSearchCV(estimator=dt,param_grid = param_grid,cv=5,verbose=1,n_jobs=-1)\ngrid_search.fit(x_train,y_train)\ngrid_search.best_params_\ny_predictions = grid_search.best_estimator_.predict(x_test)\nprint(classification_report(y_test,y_predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt=DecisionTreeClassifier(criterion= 'entropy',max_depth= 17,max_features= 'auto',min_samples_split= 0.07,min_weight_fraction_leaf= 0.006)\ndt.fit(x_train,y_train)\ny_predictions = dt.predict(x_test)\nprint(classification_report(y_test,y_predictions))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Random Forest**","metadata":{}},{"cell_type":"markdown","source":"first let's run Random Grid Search to find the general rage of the optimize parameters","metadata":{}},{"cell_type":"code","source":"dt=RandomForestClassifier()\nparam_grid={\n    'criterion':['gini','entropy'],\n    'max_depth':np.arange(4,20,1),\n    'min_samples_split':np.arange(0.001,0.1,0.01),\n    'max_features':['log2','sqrt','auto'],\n    'min_weight_fraction_leaf':np.arange(0.001,0.25,0.05),\n    'n_estimators': np.arange(50,500,50)\n}\nr_search=RandomizedSearchCV(dt,param_distributions=param_grid,n_iter=50,verbose=1)\nr_search.fit(x_train,y_train)\nr_search.best_params_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's run GridSearchCV for specific parameters to get the best values","metadata":{}},{"cell_type":"code","source":"param_grid = {'min_weight_fraction_leaf': np.arange(0.001,0.005,0.001),\n 'min_samples_split': np.arange(0.01,0.06,0.01),\n 'max_features': ['auto'],\n 'max_depth': np.arange(15,20,1),\n 'criterion': ['entropy','gini'],\n 'n_estimators': [100]}\ndt=RandomForestClassifier()\ngrid_search=GridSearchCV(estimator=dt,param_grid = param_grid,cv=5,verbose=1,n_jobs=-1, scoring='recall')\ngrid_search.fit(x_train,y_train)\ngrid_search.best_params_#getting best parameters of grid search\nm_best = grid_search.best_estimator_\nrf_predictions_val_y=m_best.predict(x_test)\nprint(classification_report(y_test,rf_predictions_val_y))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(max_depth=18, random_state=45)\nclf.fit(x_train, y_train)\nrf_predictions_val_y=clf.predict(x_test)\nprint(classification_report(y_test,rf_predictions_val_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Gradient Boosting**","metadata":{}},{"cell_type":"code","source":"param_grid = {'learning_rate': np.arange(0.1,1,0.05),\n 'max_depth': np.arange(1,15,2),\n 'max_features': ['auto', 'sqrt', 'log2'],\n 'max_depth': np.arange(15,20,1),\n 'n_estimators': np.arange(80,150,20)}\ngradient_boosting=GradientBoostingClassifier()\ngrid_search=GridSearchCV(estimator=gradient_boosting,param_grid = param_grid,cv=5,verbose=1,n_jobs=-1, scoring='recall')\ngrid_search.fit(x_train,y_train)\ngrid_search.best_params_#getting best parameters of grid search\nm_best = grid_search.best_estimator_\nrf_predictions_val_y=m_best.predict(x_test)\nprint(classification_report(y_test,rf_predictions_val_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gradient_boosting=GradientBoostingClassifier(learning_rate= 0.15,max_depth= 8,max_features= 'log2',n_estimators= 100)\ngradient_boosting.fit(x_train, y_train)\nrf_predictions_val_y=gradient_boosting.predict(x_test)\nprint(classification_report(y_test,rf_predictions_val_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **AdaBoost**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nclf = AdaBoostClassifier(n_estimators=10, random_state=0, learning_rate= 1)\nclf.fit(x_train, y_train)\nrf_predictions_val_y=gradient_boosting.predict(x_test)\nprint(classification_report(y_test,rf_predictions_val_y))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}