{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"57e07a7a-c8db-2375-7144-29fdd9fc2222"},"source":"## Goal of the project\n\nThe goal was testing the python package pgmpy. The objective was building a model to predict how likely a given project was of being fully funded (reaching at least 100% of it's funding goal), given some information which were available at the moment the project was created. \n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"748e9897-cffb-4db0-5031-1bb272bad79f"},"outputs":[],"source":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nimport glob\nimport re\n\ndir = '../input/'\n\n\ndata = pd.DataFrame()\nfor f in glob.glob((dir+'*.csv')): # all files in the directory that matchs the criteria.\n    data = pd.concat([data,pd.read_csv(f)])\n    \nuseless_columns = [\"id\",\"url\",\"category_url\",\"igg_image_url\",\"compressed_image_url\",\"card_type\",\n                   \"category_slug\",\"source_url\",\"friend_team_members\",\"friend_contributors\", \"partner_name\", \"in_forever_funding\"]\ndata = data.drop(useless_columns, axis = 1)\n\nleak_column = ['nearest_five_percent']\ndata = data.drop(leak_column , axis = 1)\ndata = data[data.amt_time_left == 'No time left']\ndata = data.drop('amt_time_left' , axis = 1)    \n\ndef Remove_Non_Numeric(column):\n    return re.sub(r\"\\D\", \"\", str(column))\n\ndata.balance = data.balance.apply(Remove_Non_Numeric)\ndata.collected_percentage = data.collected_percentage.apply(Remove_Non_Numeric)\n\ndata = data[data.collected_percentage.values != '']\ndata = data[data.collected_percentage.values != ' ']\ndata.collected_percentage = data.collected_percentage.apply(float)\n\ndef Clean_Funding(column):\n    if  \"true\" in column.lower():\n        return 1\n    elif \"false\" in column.lower() :\n        return -1\n    else:\n        return 0\n    \nimport re\nfrom nltk.corpus import stopwords\n\ndef clean_text(text):    \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",text) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops]   \n    return( \" \".join( meaningful_words ))   \n\nnew_titles = data.title.apply(lambda title: clean_text(str(title)))\ndata.title = new_titles\n\ndata = data.drop_duplicates()\n\ndata['suc'] = 0\ndata.loc[(data.collected_percentage >= 100), 'suc'] = 1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"494d0857-6184-c607-2098-487a7fd07784"},"outputs":[],"source":"data.shape"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6de1a4c5-f946-6434-5746-dee973691a2d"},"outputs":[],"source":"data.head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2ca18c08-a2ee-50b2-02c7-398ce69ebf1c"},"outputs":[],"source":"print('Success rate')\nprint(data.suc.sum()/data.suc.count()*100)\nprint('')\nprint('Failure rate')\nprint(100 - data.suc.sum()/data.suc.count()*100)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"18cdaa10-bcd1-fbd6-3c3a-e11437ee28ff"},"outputs":[],"source":"print(data.currency_code.unique())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"849f72a1-f54d-1f68-3cca-db96bea6ebbb"},"outputs":[],"source":"print(data.category_name.unique())\nprint(len(data.category_name.unique()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fc104f27-d6e1-ddf6-d5df-194aa6e88a76"},"outputs":[],"source":"ax_cur = sns.countplot(y=\"currency_code\", hue=\"suc\", data=data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c72603ed-0291-40fe-68f7-c3896b2abf04"},"outputs":[],"source":"a4_dims = (11.7, 8.27)\nfig, ax_cat = plt.subplots(figsize=a4_dims)\n\nsns.countplot(ax = ax_cat, y=\"category_name\", hue=\"suc\", data=data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"071a98a4-f313-7c2f-59fd-c91a8659eea1"},"outputs":[],"source":"data['category_name'].value_counts().head(10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"300d6fbf-d956-b177-008f-19f72b9eb1e5"},"outputs":[],"source":"top_cat = data['category_name'].value_counts().head(10).index\n\na4_dims = (11.7, 8.27)\nfig, ax_cat = plt.subplots(figsize=a4_dims)\n\nsns.countplot(ax = ax_cat, y=\"category_name\", hue=\"suc\", data=data[data.category_name.isin(top_cat)])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1a8cc81-b39a-44d3-c6e7-def625fa3245"},"outputs":[],"source":"trainRatio = 0.75\nnp.random.seed(1230)\ntrainIdx = np.random.rand(len(data)) < trainRatio"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"53679441-33ee-f56f-0da7-cccf23eb899d"},"outputs":[],"source":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer = \"word\",   \n                             tokenizer = None,    \n                             preprocessor = None, \n                             stop_words = None,   \n                             max_features = 5000) "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a35ece1e-8b37-8c75-5a07-386bbc42d902"},"outputs":[],"source":"titles = vectorizer.fit_transform(data[trainIdx].title)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8a08284b-a3c3-9be9-e636-800bd92cea52"},"outputs":[],"source":"words = vectorizer.get_feature_names()\ncounts = np.sum(titles, axis=0)\n\nWord_Count = pd.DataFrame(counts.transpose(), columns = {'word_counts'} )\nWord_Count['Word'] = words\nWord_Count = Word_Count.sort_values(by = \"word_counts\", ascending = False)\n#Word_Count = Word_Count.set_index('Word')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d9f4490b-e92f-359e-9ff4-bf643e3b19a1"},"outputs":[],"source":"success_projectIdx = np.logical_and(trainIdx,(data.collected_percentage >= 100))\nfailed_projectIdx = np.logical_and(trainIdx, (data.collected_percentage < 100))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf303fdf-7d56-845b-f7ce-84b09f7c2295"},"outputs":[],"source":"success_titles = vectorizer.fit_transform(data[success_projectIdx].title)\nsuccess_words = vectorizer.get_feature_names()\ncounts = np.sum(success_titles, axis=0)\n\nsuccess_Word_Count = pd.DataFrame(counts.transpose(), columns = {'suc_counts'} )\nsuccess_Word_Count['Word'] = success_words\nsuccess_Word_Count = success_Word_Count.sort_values(by = \"suc_counts\", ascending = False)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf048424-8e83-0cdf-0053-b3fca9d51db6"},"outputs":[],"source":"# I could just use word_count - success_words for this\nfailed_titles = vectorizer.fit_transform(data[failed_projectIdx].title)\nfailed_words = vectorizer.get_feature_names()\ncounts = np.sum(failed_titles, axis=0)\n\nfailed_Word_Count = pd.DataFrame(counts.transpose(), columns = {'failed_counts'} )\nfailed_Word_Count['Word'] = failed_words\nfailed_Word_Count = failed_Word_Count.sort_values(by = \"failed_counts\", ascending = False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3b8ad800-038d-1486-a38f-adbb3fd7b2c3"},"outputs":[],"source":"Word_Count = Word_Count.merge(failed_Word_Count, how='left', left_on = \"Word\", right_on = \"Word\")\nWord_Count = Word_Count.merge(success_Word_Count, how='left', left_on = \"Word\", right_on = \"Word\")\n\nWord_Count = Word_Count.sort_values(by=\"word_counts\", ascending=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"812e3b9f-d89a-b7fe-2367-e681e46cff35"},"outputs":[],"source":"The first thing I've tried was building a model based on the most popular words, but this ended up not being informative enough to build a good model. For curiosity sake I left the list of words that appeared on more then 1% of the project titles, as well as its success rate "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fd48364-c568-014d-570b-8103e1c8c69a"},"outputs":[],"source":"Word_Count['suc_rate'] = (Word_Count[\"suc_counts\"] -  Word_Count[\"failed_counts\"])/ Word_Count['word_counts']\n\nWord_Count['suc_rate'] = (Word_Count['suc_rate'] - min(Word_Count['suc_rate']))/(max(Word_Count['suc_rate']) - min(Word_Count['suc_rate']))\nWord_Count['suc_rate'] = (Word_Count['suc_rate'] - 0.5)*2\nWord_Count = Word_Count.sort_values(by=\"suc_rate\", ascending=False)\nrelevant_word_count = Word_Count[Word_Count.word_counts > trainIdx.sum()/100]\nprint(relevant_word_count)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2f15a8d-21d0-4a2b-0345-5a2d740b3ff6"},"outputs":[],"source":"# Testing how pgmpy performs"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"87301677-f8bb-32ac-e5b9-838253082e4e"},"outputs":[],"source":"Quick remainder of what the data looks like"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"55a4f99d-f1d6-b0d4-c9a6-c7d9daf67ae6"},"outputs":[],"source":"data.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f4e711d9-9bb3-58c6-a53d-951d66b4eeec"},"source":"As a way to test pgmpy this was okay. pgmpy seems to be able to handle up to 10 features with an acceptable perfomance, although most of those were binary, but the train set was pretty large (about 150k rows). When using more features, it would quickly run out of memory. This might be improved by changing the enviroment, but nevertherless is a bad sign. Finally, it's not able to handle continous variables for training, which is quite a drawback. As far as the model goes, it was able to come with a decent model for which models were more likely to fail given only the project title, the currency, and category, withou any major problems or demading a lot of time"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"959da817-2cc9-a4f3-8718-03d2f7c61772"},"outputs":[],"source":"# filtering words that appear on more then 100 titles\nnon_specific_word_count = Word_Count[Word_Count.word_counts > 100]\nnon_specific_word_count = non_specific_word_count.sort_values(by=\"suc_rate\", ascending=False)\nprint(non_specific_word_count.head())\nprint(non_specific_word_count.tail())\n\n# quidditch is actually a game from the harry potter series"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e793b61-2709-3b1d-b7f5-1a191688545d"},"outputs":[],"source":"We will now reduce this by \"success clusters\" i.e.: groups in different success bands. Since we have around 1000 words we will break this in groups of about 100 words"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"12b96057-4027-5aef-c493-8a25db78b69f"},"outputs":[],"source":"dictionarySize = len(non_specific_word_count)\nclusterSize = dictionarySize // 6\n\nwordCluster = [ non_specific_word_count[i:i + clusterSize] for i in range(0, dictionarySize, clusterSize )]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8c04e2f6-9d99-6826-38de-b440a31dcbc2"},"outputs":[],"source":"from functools import partial\n\ndef matchCluster(cluster, title):\n    titleSet = set(title.split(' '))\n    clusterSet = set(cluster.Word.values)\n    common = list(titleSet & clusterSet)\n    return 1 if len(common) > 0 else 0\n\nfor i in range(len(wordCluster)):\n    cluster = wordCluster[i]\n    \n    matchClusterDF = partial(matchCluster, cluster)\n    data['has_cluster_' + str(i)]  = data.title.apply(matchClusterDF)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"342acb9e-1e75-6e3b-ce65-c66eb55c89d1"},"outputs":[],"source":"wordCluster[0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5104ab7-5a47-4590-7551-3096e5d89683"},"outputs":[],"source":"wordCluster[-1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08f259b5-1aa0-1010-ecdf-3a6f5af9caee"},"outputs":[],"source":"from sklearn import preprocessing\n\nle_cat = preprocessing.LabelEncoder()\nle_cat.fit(data.category_name.unique())\ndata.category_name = le_cat.transform(data.category_name)\n\nle_cur = preprocessing.LabelEncoder()\nle_cur.fit(data.currency_code.unique())\ndata.currency_code = le_cur.transform(data.currency_code)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6d6a3a1c-8d1b-8273-8431-38bedb174aab"},"outputs":[],"source":"import pgmpy\n\nfrom pgmpy.estimators import ParameterEstimator, BayesianEstimator\nfrom pgmpy.models import BayesianModel\n\nmodelRelations = []\nmodelRelations.append(('currency_code', 'suc'))\nmodelRelations.append(('category_name', 'suc'))\nfor i in range(len(wordCluster)):\n    modelRelations.append(('has_cluster_' + str(i), 'suc'))\n\nmodel1 = BayesianModel(modelRelations)  # everything implies in suc"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec8cc0bf-30e8-ef26-22a5-a5f18f74bbbe"},"outputs":[],"source":"model1.fit(data.iloc[trainIdx], estimator=BayesianEstimator, prior_type=\"BDeu\", )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1f05bc34-3154-2663-b381-ed6db413de8f"},"outputs":[],"source":"# creates evidence\ndef createsEvidence(df_point, Word_Count):\n    evidence = {}\n    evidence['currency_code'] = df_point['currency_code']\n    evidence['category_name'] = df_point['category_name']\n    \n    for i in range(len(wordCluster)):\n        key = 'has_cluster_' + str(i)\n        evidence[key] = df_point[key]\n    \n    return evidence"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2215d2d8-4e1a-372b-3d31-36c763dcc9e7"},"outputs":[],"source":"from numpy import random\nfrom sklearn import preprocessing\n\nsampleSize = 200\n\ntestIdx = np.where(~trainIdx)\ntestSize = len(testIdx[0])\ntestSample = random.choice(testSize, sampleSize)\ntestIdx = testIdx[0][testSample]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b270fabb-8d3a-f927-1124-770c50b10582"},"outputs":[],"source":"from pgmpy.inference import VariableElimination\n\npredictions = {}\nfor pred_idx in testIdx:\n    infer = VariableElimination(model1)\n    evidence = createsEvidence(data.iloc[pred_idx], Word_Count)\n    result = infer.query(['suc'], evidence = evidence)\n    predictions[pred_idx] = (result['suc'].values[1], data.iloc[pred_idx].suc)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bb61d932-3366-2c98-3676-6e31451b106e"},"outputs":[],"source":"totalTrue = 0\ntruePositive = 0\ntotalPositive = 0\n\nfor pred_idx in testIdx:\n    if(predictions[pred_idx][1]):\n        totalTrue += 1\n        if predictions[pred_idx][0] > 0.5:\n            truePositive += 1\n    if predictions[pred_idx][0] > 0.5:\n        totalPositive += 1\n        \n   \nprecision = truePositive/totalTrue\nrecall = truePositive/totalPositive\nprint('')   \nprint(\"precision: %.2f\"%precision)    \nprint(\"recall: %.2f\"%recall)    \n     "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f8838f2-fc7d-ee9e-e68b-ce4d9d948f8f"},"outputs":[],"source":"totalFalse = 0\ntrueNegative = 0\ntotalNegative = 0\n\nfor pred_idx in testIdx:\n    if(not predictions[pred_idx][1]):\n        totalFalse += 1\n        if predictions[pred_idx][0] < 0.5:\n            trueNegative += 1\n    if predictions[pred_idx][0] < 0.5:\n        totalNegative += 1\n\nprecision = trueNegative/totalFalse\nrecall = trueNegative/totalNegative\nprint('')   \nprint(\"precision: %.2f\"%precision)    \nprint(\"recall: %.2f\"%recall)  \n        "}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}