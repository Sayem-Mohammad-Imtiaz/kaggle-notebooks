{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"機器學習的方式可以大致分成：監督式學習、非監督式學習、強化學習\n\n前面介紹了決策樹模型、循環神經網路等監督式學習的模型，都是透過已知價格去修正預測價格，讓模型更精準。\n\n而強化學習的不同之處為：\n\n- 計算式裡面沒有監督訊號，也沒有 Label (已知價格)。只有 Reward (反饋)\n- 反饋有延時，不是能立即反映\n- 訓練時的輸入與時序相關\n- Agent(決策者) 執行的動作會影響之後的資料\n\n我們現在要做的是：\n\n- 針對一個具體問題 (跳的愈遠愈好) 得到一個最佳的策略 (跳躍的時機點)\n- 使得在該策略下獲得的回報 (分數) 最大\n- 這裡的策略其實就是一系列的 Action (序列資料)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## 使用強化學習模擬交易\n\n[參考算法](https://github.com/SaAPro/agent-trading-deep-evolution-strategy)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"spy_prices = pd.read_csv('../input/quantitative-trading/SPY_2018.csv')\nspy_prices.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DeepEvolutionStrategy:\n\n    inputs = None\n\n    def __init__(\n        self, weights, reward_function, population_size, sigma, learning_rate\n    ):\n        self.weights = weights\n        self.reward_function = reward_function\n        self.population_size = population_size\n        self.sigma = sigma\n        self.learning_rate = learning_rate\n\n    def _get_weight_from_population(self, weights, population):\n        weights_population = []\n        for index, i in enumerate(population):\n            jittered = self.sigma * i\n            weights_population.append(weights[index] + jittered)\n        return weights_population\n\n    def getWeights(self):\n        return self.weights\n\n    def train(self, epoch = 100, print_every = 1):\n        lasttime = time.time()\n        for i in range(epoch):\n            population = []\n            rewards = np.zeros(self.population_size)\n            for k in range(self.population_size):\n                x = []\n                for w in self.weights:\n                    x.append(np.random.randn(*w.shape))\n                population.append(x)\n            for k in range(self.population_size):\n                weights_population = self._get_weight_from_population(\n                    self.weights, population[k]\n                )\n                rewards[k] = self.reward_function(weights_population)\n            rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)\n            for index, w in enumerate(self.weights):\n                A = np.array([p[index] for p in population])\n                self.weights[index] = (\n                    w\n                    + self.learning_rate\n                    / (self.population_size * self.sigma)\n                    * np.dot(A.T, rewards).T\n                )\n            if (i + 1) % print_every == 0:\n                print(\n                    f'訓練週期 {i + 1}. 最終獎勵：{self.reward_function(self.weights)}'\n                )\n        print('=====================================')\n        print(f'訓練時間：{time.time() - lasttime} 秒')\n\n\nclass Model:\n    def __init__(self, input_size, layer_size, output_size):\n        self.weights = [\n            np.random.randn(input_size, layer_size),\n            np.random.randn(layer_size, output_size),\n            np.random.randn(1, layer_size),\n        ]\n\n    def predict(self, inputs):\n        feed = np.dot(inputs, self.weights[0]) + self.weights[-1]\n        decision = np.dot(feed, self.weights[1])\n        return decision\n\n    def getWeights(self):\n        return self.weights\n\n    def set_weights(self, weights):\n        self.weights = weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Agent:\n\n    POPULATION_SIZE = 15\n    SIGMA = 0.1\n    LEARNING_RATE = 0.03\n\n    def __init__(self, model, window_size, trend, skip, initial_money):\n        self.model = model\n        self.window_size = window_size\n        self.half_window = window_size // 2\n        self.trend = trend\n        self.skip = skip\n        self.initial_money = initial_money\n        self.es = DeepEvolutionStrategy(\n            self.model.getWeights(),\n            self.getReward,\n            self.POPULATION_SIZE,\n            self.SIGMA,\n            self.LEARNING_RATE,\n        )\n\n    def act(self, sequence):\n        decision = self.model.predict(np.array(sequence))\n        return np.argmax(decision[0])\n    \n    def getState(self, t):\n        window_size = self.window_size + 1\n        d = t - window_size + 1\n        block = self.trend[d : t + 1] if d >= 0 else -d * [self.trend[0]] + self.trend[0 : t + 1]\n        res = []\n        for i in range(window_size - 1):\n            res.append(block[i + 1] - block[i])\n        return np.array([res])\n\n    def getReward(self, weights):\n        initial_money = self.initial_money\n        starting_money = initial_money\n        self.model.weights = weights\n        state = self.getState(0)\n        inventory = []\n        quantity = 0\n        for t in range(0, len(self.trend) - 1, self.skip):\n            action = self.act(state)\n            next_state = self.getState(t + 1)\n            \n            if action == 1 and starting_money >= self.trend[t]:\n                inventory.append(self.trend[t])\n                starting_money -= close[t]\n                \n            elif action == 2 and len(inventory):\n                bought_price = inventory.pop(0)\n                starting_money += self.trend[t]\n\n            state = next_state\n        return ((starting_money - initial_money) / initial_money) * 100\n\n    def fit(self, iterations, checkpoint):\n        self.es.train(iterations, print_every = checkpoint)\n\n    def buy(self):\n        initial_money = self.initial_money\n        state = self.getState(0)\n        starting_money = initial_money\n        states_sell = []\n        states_buy = []\n        inventory = []\n        for t in range(0, len(self.trend) - 1, self.skip):\n            action = self.act(state)\n            next_state = self.getState(t + 1)\n            \n            if action == 1 and initial_money >= self.trend[t]:\n                inventory.append(self.trend[t])\n                initial_money -= self.trend[t]\n                states_buy.append(t)\n                print('第 %s 日 > 購買 1 股 價格 %s，總資產 %s' % (\n                    str(t).rjust(3),\n                    str(\"%.5f\" % self.trend[t]).rjust(10),\n                    str(\"%.3f\" % initial_money).rjust(10)\n                ))\n            \n            elif action == 2 and len(inventory):\n                bought_price = inventory.pop(0)\n                initial_money += self.trend[t]\n                states_sell.append(t)\n                try:\n                    invest = ((close[t] - bought_price) / bought_price) * 100\n                except:\n                    invest = 0\n                print('第 %s 日 > 賣出 1 股 價格 %s，總資產 %s > 獲利 %s %%' % (\n                    str(t).rjust(3),\n                    str(\"%.5f\" % close[t]).rjust(10),\n                    str(\"%.3f\" % initial_money).rjust(10),\n                    str(\"%.2f\" % invest).rjust(5)\n                ))\n            state = next_state\n\n        invest = ((initial_money - starting_money) / starting_money) * 100\n        total_gains = initial_money - starting_money\n        return states_buy, states_sell, total_gains, invest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"close = spy_prices.Close.values.tolist()\nwindow_size = 30\nskip = 1\ninitial_money = 10000\n\nmodel = Model(input_size = window_size, layer_size = 500, output_size = 3)\nagent = Agent(model = model, \n              window_size = window_size,\n              trend = close,\n              skip = skip,\n              initial_money = initial_money)\nagent.fit(iterations = 500, checkpoint = 100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"states_buy, states_sell, total_gains, invest = agent.buy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 觀察 Agent 績效"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15, 5))\nplt.plot(close, color='r', lw=2.)\nplt.plot(close, '^', markersize=10, color='m', label = '買入信號', markevery = states_buy)\nplt.plot(close, 'v', markersize=10, color='k', label = '賣出信號', markevery = states_sell)\nplt.title(f'總獲利 {total_gains}，投資報酬率 {invest} %')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 用上最基本的運算架構，還沒用上GPU並行運算，下個部份將調用 TF-Agents 完成"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}