{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Some basic info about this dataset\n- Has 4 Junction","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\nfrom datetime import datetime, timedelta, date\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/traffic-prediction-dataset/traffic.csv', parse_dates=True, index_col='DateTime')\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Some describe in this dataset**","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Extract Year, Month, Day, Hour** from **index**\n- I split Year, Month, Day, Hour from data for plotting purpose","metadata":{}},{"cell_type":"code","source":"# extract year from date\ndf['Year'] = pd.Series(df.index).apply(lambda x: x.year).to_list()\n\n# extract month from date\ndf['Month'] = pd.Series(df.index).apply(lambda x: x.month).to_list()\n\n# extract day from date\ndf['Day'] = pd.Series(df.index).apply(lambda x: x.day).to_list()\n\n# extract hour from date\ndf['Hour'] = pd.Series(df.index).apply(lambda x: x.hour).to_list()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Drop the ID column**\nI think ID does not effect to this dataset, so just drop it!","metadata":{}},{"cell_type":"code","source":"df.drop('ID', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I make a `make_hist` function for making `histogram` with `kde` plot, for plotting 4 junction","metadata":{}},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"def make_hist(junction=1):\n    data = df[df['Junction'] == junction]\n    f, ax = plt.subplots(figsize=(17, 5))\n    ax = sns.histplot(data['Vehicles'], kde=True, stat='probability')\n    ax.set_title(f'Plot show the distribution of data in junction {junction}')\n    ax.grid(True, ls='-.', alpha=0.75)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_hist(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_hist(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_hist(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_hist(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In 4 plot, its can show us that `Vehicles` in each junction is normal distribution with skew","metadata":{}},{"cell_type":"code","source":"df.tail(1).Year[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_time_series_plot(junction=1):\n    f, ax = plt.subplots(figsize=(17, 5))\n    data=df[df.Junction == junction]\n    ax = sns.lineplot(data=data, y='Vehicles', x='DateTime', ax=ax)\n    start = data.head(1)\n    end = data.tail(1)\n    ax.set_title(f'Plot show amounts of Vehicles in junction {junction} from {start.Month[0]}-{start.Year[0]} to {end.Month[0]}-{end.Year[0]}', fontsize=15)\n    ax.grid(True, ls='-.', alpha=0.75)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_time_series_plot(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_time_series_plot(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_time_series_plot(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_time_series_plot(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(3, 4, figsize=(20, 10))\n\nfor i, year in enumerate(range(2015, 2018)):\n  for j, junction in enumerate(range(1, 5)):\n    sns.lineplot(data=df[(df.Junction == junction) & (df.Year == year)], x='Month', y='Vehicles', ax=ax[i, j])\n    ax[i, j].grid(True, alpha=0.75, ls='-.')\n\nplt.xlabel('Year')\nplt.ylabel('Junction')\nf.suptitle('Line plot showing the pattern amounts of Vehicles by Year and by Junction', fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normal data histogram**","metadata":{}},{"cell_type":"code","source":"f, axis = plt.subplots(3, 4, figsize=(20, 10))\n\nfor i, year in enumerate(range(2015, 2018)):\n  for j, junction in enumerate(range(1, 5)):\n    sns.histplot(df[(df.Junction == junction) & (df.Year == year)]['Vehicles'], kde=True, ax=axis[i, j], stat='probability')\n    axis[i, j].grid(True, alpha=0.75, ls='-.')\n\nplt.xlabel('Year')\nplt.ylabel('Junction')\nf.suptitle('Histogram showing the distribution of Vehicles by Year and by Junction', fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Z Score data distribution and Histogram with Z Score Vehicles form Data**\n- Mean = 0\n- Standard Deviation = 1","metadata":{}},{"cell_type":"code","source":"standardization = lambda x: StandardScaler().fit_transform(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"z_df = df.copy()\nz_df['Vehicles'] = standardization(z_df.Vehicles.values.reshape(-1, 1))\nz_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axis = plt.subplots(3, 4, figsize=(20, 10))\n\nfor i, year in enumerate(range(2015, 2018)):\n  for j, junction in enumerate(range(1, 5)):\n    sns.histplot(z_df[(z_df.Junction == junction) & (z_df.Year == year)]['Vehicles'], kde=True, ax=axis[i, j], stat='probability')\n    axis[i, j].grid(True, alpha=0.75, ls='-.')\n    \nplt.xlabel('Year')\nplt.ylabel('Junction')\nf.suptitle('Histogram showing the distribution of Vehicles by Year and by Junction when data transfrom to Z Score', fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Boxplot for Vehicles**\n- In boxplot, we can see a lot of outliers (those which is the dot)","metadata":{}},{"cell_type":"code","source":"f, axis = plt.subplots(3, 4, figsize=(20, 10))\n\nfor i, year in zip(range(3), range(2015, 2018)):\n  for j, junction in zip(range(4), range(1, 5)):\n    sns.boxplot(x=df[(df.Junction == junction) & (df.Year == year)]['Vehicles'], ax=axis[i, j])\n    axis[i, j].grid(True, alpha=0.75, ls='-.')\n\nplt.xlabel('Year')\nplt.ylabel('Junction')\nf.suptitle('Boxplot showing the range of amounts Vehicles by Year and by Junction', fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Heatmap about data attributes**\n- The value closer to 1 or -1 is best correlation to each other.\n- As close as to -1, that pair of attribute is more **negative** correlation.\n- As close as to 1, that pair of attribute is more **positive** correlation.\n- As close as to 0, that pair of attribute is **not** correlating to each other.","metadata":{}},{"cell_type":"code","source":"corr = df.corr()\nf, ax = plt.subplots(figsize=(16, 7))\nsns.heatmap(corr, annot=True, fmt='.2f', vmin=-1, vmax=1, square=True, linewidths=1)\nf.suptitle('Heatmap showing the correlation of data attributes', fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_list_data(dataf, drop=[]):\n  # drop cột DateTime ở các data\n  for i in drop:\n    try:\n      dataf.drop(drop, axis=1, inplace=True)\n    except:\n      print(f\"{i} doesn't has in data\")\n  # create a list of dataframe has the data in that junction and remove the junction identify\n  dataf = [dataf[dataf.Junction == i].drop('Junction', axis=1) for i in range(5)]\n  return dataf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = get_list_data(df)\nfor i in data:\n    print(i.head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=4, figsize=(20, 15))\nfor i in range(4):\n    ax[i].plot(data[i + 1].resample('D').sum().Vehicles, label=f'Vehicles of {i + 1} Junction', lw=2)\n    ax[i].grid(True, alpha=0.75, lw=1, ls='-.')\n    ax[i].set_title(f'Junction {i + 1}')\nf.suptitle('Plots show amounts of Vehicles by Junction, each Junction by day (24h)', fontsize=20);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=4, figsize=(20, 15))\nfor i in range(4):\n    ax[i].plot(data[i + 1].resample('M').sum().Vehicles, label=f'Vehicles of {i + 1} Junction', lw=2)\n    ax[i].grid(True, alpha=0.75, lw=1, ls='-.')\n    ax[i].set_ylabel('Số lượng', fontsize=15)\n    ax[i].set_title(f'Junction {i + 1}')\nf.suptitle('Plots show amounts of Vehicles by Junction, each Junction by Month', fontsize=20);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=4, figsize=(22, 20))\nfor i in range(4):\n    ax[i].plot(data[i + 1].resample('12H').sum().Vehicles, label=f'Vehicles of {i + 1} Junction', lw=1)\n    ax[i].grid(True, alpha=0.75, lw=1, ls='-.')\n    ax[i].set_title(f'Junction {i + 1}')\nf.suptitle('Plots show amounts of Vehicles by Junction, each Junction by haft day(12h)', fontsize=20);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=4, figsize=(22, 15))\nfor i in range(4):\n    ax[i].plot(data[i + 1].resample('6H').sum().Vehicles, label=f'Vehicles of {i + 1} Junction', lw=2)\n    ax[i].grid(True, alpha=0.75, lw=1, ls='-.')\n    ax[i].set_title(f'Junction {i + 1}')\nf.suptitle('Plots show amounts of Vehicles by Junction, each Junction by 1/4 day (6h)', fontsize=20);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(17, 5))\nfoo = data[1][:400]\nfoo.Vehicles.plot(lw=3)\nfoo.Vehicles.rolling('D').mean().plot(lw=3)\nfoo.Vehicles.rolling('D').std().plot(lw=3)\nplt.legend(['Junction 1', 'Rolling Mean A Day', 'Rolling Std A Day'])\nplt.grid(True, alpha=0.75, ls='-.')\nplt.title('Plot show amounts of Vehicles first 400 hours', fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(17, 5))\nfoo = data[2][:400]\nfoo.Vehicles.plot(lw=3)\nfoo.Vehicles.rolling('D').mean().plot(lw=3)\nfoo.Vehicles.rolling('D').std().plot(lw=3)\nplt.legend(['Junction 2', 'Rolling Mean A Day', 'Rolling Std A Day'])\nplt.grid(True, alpha=0.75, ls='-.')\nplt.title('Plot show amounts of Vehicles first 400 hours', fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(17, 5))\nfoo = data[3][:400]\nfoo.Vehicles.plot(lw=3)\nfoo.Vehicles.rolling('D').mean().plot(lw=3)\nfoo.Vehicles.rolling('D').std().plot(lw=3)\nplt.legend(['Junction 3', 'Rolling Mean A Day', 'Rolling Std A Day'])\nplt.grid(True, alpha=0.75, ls='-.')\nplt.title('Plot show amounts of Vehicles first 400 hours', fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(17, 5))\nfoo = data[4][:400]\nfoo.Vehicles.plot(lw=3)\nfoo.Vehicles.rolling('D').mean().plot(lw=3)\nfoo.Vehicles.rolling('D').std().plot(lw=3)\nplt.legend(['Junction 4', 'Rolling Mean A Day', 'Rolling Std A Day'])\nplt.grid(True, alpha=0.75, ls='-.')\nplt.title('Plot show amounts of Vehicles first 400 hours', fontsize=20)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Auto correlation plot**\n- The data lie outside the blue has 95% effect to data","metadata":{}},{"cell_type":"code","source":"def make_autocorrelation(junction=1):\n    f, ax = plt.subplots(figsize=(17, 6), nrows=2)\n    plot_acf(data[junction].Vehicles, title=f\"Autocorrelation of amounts of Vehicles in Junction {junction}\", ax=ax[0])\n    plot_pacf(data[junction].Vehicles, title=f\"Partial Autocorrelation of amounts of Vehicles Junction {junction}\", ax=ax[1])\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_autocorrelation(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_autocorrelation(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_autocorrelation(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_autocorrelation(4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Month has **negative** correlation with Year\n- The correlation of Vehicles and Year is equal to Vehicles and Hour\n\n**=> Can drop Year or Hour because it's the same**","metadata":{}},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"**Create a make metrics function to return R² Score and RMSE from a list of models**","metadata":{}},{"cell_type":"code","source":"def make_metrics(models):\n    return pd.DataFrame({\n        'name': [model.name for model in models[1:]],\n        'r2': [model.r2 for model in models[1:]],\n        'rmse': [model.rmse for model in models[1:]]\n    })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create a function to create a new dataset**","metadata":{}},{"cell_type":"code","source":"z_data = get_list_data(z_df)\nfor i in z_data:\n    print(i.head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create a class for a frame for machine learning model**","metadata":{}},{"cell_type":"code","source":"class Model:\n  def __init__(self, name, data, predict_features, test_size, ml_model):\n    self.name = name\n    self.data = data\n    self.predict_features = predict_features\n    self.is_trained = False\n    self.test_size = test_size\n    self.ml_model = ml_model\n    self.do_things()\n\n  def cal_rmse(self):\n    self.rmse = mean_squared_error(self.ytest, self.ypredict, squared=False)\n    return self.rmse\n\n  def prequisite(self, test_size):\n    self.features = [i for i in self.data.columns if i != self.predict_features]\n    self.X = self.data[self.features].values\n    self.y = self.data[self.predict_features].values\n    self.Xtrain, self.Xtest, self.ytrain, self.ytest = train_test_split(self.X, self.y, test_size=test_size)\n    return None\n\n  def fit(self):\n    self.is_trained = True\n    self.ml_model.fit(self.Xtrain, self.ytrain)\n    self.ypredict = self.ml_model.predict(self.Xtest)\n    return self.ml_model\n\n  def cal_r2_score(self):\n    self.r2 = r2_score(self.ytest, self.ypredict)\n    return self.r2\n\n  def do_things(self) -> None:\n    self.prequisite(self.test_size)\n    self.fit()\n    self.cal_rmse()\n    self.cal_r2_score()\n    return None\n\n  def feature_importances(self, ax) -> None:\n    feature_importances = self.ml_model.feature_importances_\n    index = lag_models[1].features\n    data = pd.DataFrame(pd.Series(feature_importances, index=index).nlargest(10)).reset_index()\n    data.columns = ['Features', 'Value']\n    g = sns.barplot(data=data, x='Features', y='Value', ax=ax)\n    for p in g.patches:\n        ax.annotate(\n            format(p.get_height(), '.2f'),\n            (p.get_x() + p.get_width() / 2, p.get_height() + 0.02),\n            ha='center', va='center', weight='bold', fontsize=9\n        )\n    ax.set_title(f'Plot of {self.name}', fontsize=12)\n    ax.grid(True, ls='-.', alpha=0.7)\n    ax.set_ylim(0, 1)\n\n  def __repr__(self) -> str:\n    if not self.is_trained:\n      return f'<{self.name}> (is not trained yet)>'\n    return f'<({self.name}: [R² Score: {self.r2}], [RMSE: {self.rmse}])>'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training models for 4 junction with normal data**","metadata":{}},{"cell_type":"code","source":"models = [None]\nfor i in range(1, 5):\n    models += [\n        Model(\n            ml_model=RandomForestRegressor(),\n            name=f'Dataset of junction {i}',\n            data=data[i],\n            predict_features='Vehicles',\n            test_size=1/4\n        )\n    ]\n    \nmake_metrics(models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training models for 4 junction with Z Score Normalization**","metadata":{}},{"cell_type":"code","source":"z_models = [None]\nfor i in range(1, 5):\n    z_models += [\n        Model(\n            ml_model=RandomForestRegressor(),\n            name=f'Dataset of junction {i}',\n            data=z_data[i],\n            predict_features='Vehicles',\n            test_size=1/4\n        )\n    ]\n\nmake_metrics(z_models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create lag data**","metadata":{}},{"cell_type":"code","source":"lag_df = df.copy()\nfor i in range(1, 3):\n    lag_df[f'Vehicles_lag_{i}'] = df.Vehicles.shift(i)\n\n# drop all rows with nan, because lag data cause nan\nlag_df.dropna(inplace=True)\nlag_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lag_data = get_list_data(lag_df, drop=['Year'])\nfor i in lag_data:\n    print(i.head(1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lag data is appropriate for time series data, use for create the auto correlation**","metadata":{}},{"cell_type":"code","source":"lag_models = [None]\nfor i in range(1, 5):\n    lag_models += [\n        Model(\n            ml_model=RandomForestRegressor(),\n            name=f'Dataset of junction {i} with lag data',\n            data=lag_data[i],\n            predict_features='Vehicles',\n            test_size=1/3\n        )\n    ]\n\nmake_metrics(lag_models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature importances of a model**\n\n*The correlate value as close as 1 is best*","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 8))\nk = 1\nfor i in range(2):\n    for j in range(2):\n        lag_models[k].feature_importances(ax[i, j])\n        k += 1\nf.suptitle('Plots show how features in each dataset correlating to each model', fontsize=15, fontweight='bold')        \nf.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predict for next 4 months**\n\nI predict for each Junction separately. I use previous prediction amounts of Vehicles for predicting next amounts of Vehicles.","metadata":{}},{"cell_type":"code","source":"for junction in range(1, 5):\n    cur_time = lag_data[junction].tail(1).index[0] # get the current time, the last time of that dataset\n    end_time = pd.Timestamp(2017, 11, 1, 0, 0, 0) # the end time after 4 months that we want to predict\n    new_data = lag_data[junction].copy() # create a copy of dataset with that junction\n    features = lag_models[junction].features # get features of each models in that junction\n    while cur_time != end_time:\n        last = new_data.tail(1).copy() # get the last row of dataset, just make a copy!\n        new_data = pd.concat([new_data, last]) # concatenate the copy dataset with it's last row\n        for i in range(1, 3): # create lag data\n            new_data[f'Vehicles_lag_{i}'] = new_data.Vehicles.shift(i) # shift by periods i\n        new_data.iloc[len(new_data) - 1, [1, 2, 3]] = [cur_time.month, cur_time.day, cur_time.hour] # assign value for those columns\n        last = new_data[features].tail(1).values # create a new last data that drop all nan\n        new_data.iloc[len(new_data) - 1, 0] = round(lag_models[1].ml_model.predict(last)[0]) # predicting for vehicles\n        cur_time += timedelta(hours=1) # add to a cur_time 1 hour\n    new_data.index = pd.date_range(\n        start=lag_data[junction].head(1).index.values[0],\n        end=pd.Timestamp(2017, 11, 1, 0, 0, 0),\n        freq='H'\n    ) # reassign index with the new time range with start is the start of data\n      # and end time is the end time that initialize in start of the loop\n    new_data.to_csv(f'vehicles_for_next_4_months_in_junction_{junction}.csv') # to csv that file\n    print(f'|==Predicted for Junction {junction}==|')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nThe highest RMSE is about 5.6, so when using last amounts of Vehicles to predict next hour amounts of Vehicles seem like that RMSE is raising. You guys can puts a review of your own into the comments section below. Be honest, i'm very newbie, so if has something may not right in my kernel, you can tell me about that, i'm very appreciate about that. Thanks for reading my kernel!","metadata":{}}]}