{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df = pd.read_csv('../input/covid19-sentiments/COVID-19_Sentiments.csv')\ncovid_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if covid_df.duplicated().any():\n    print('Duplicates Found')\nelse:    \n    print('No Duplicates Found')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df = covid_df.drop_duplicates().reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df.Sentiments.fillna(covid_df['Sentiments'].mean(),inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df.Sentiments.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def response(x):\n    if x > 0:\n        return 1\n    elif x < 0:\n        return -1\n    else :\n        return 0\ncovid_df['Sentiments'] = covid_df['Sentiments'].apply(response)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df.Sentiments.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df.drop(['Text_Id', 'Date', 'Location'], axis =1, inplace = True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df = covid_df[covid_df.Sentiments != 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"covid_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Cleaning","metadata":{}},{"cell_type":"code","source":"import string\nstring.punctuation\n\ndef remove_punctuation(text):\n    no_punct = [words for words in text if words not in string.punctuation]\n    words_wo_punct = ''.join(no_punct)\n    return words_wo_punct\ncovid_df[\"Text_wo_punct\"] = covid_df['Text'].apply(lambda x: remove_punctuation(x))\ncovid_df.head()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\nimport re\ntokenizer = RegexpTokenizer(r'\\w+') \ncovid_df['Text_wo_punct'] = covid_df['Text_wo_punct'].apply(lambda x: tokenizer.tokenize(x.lower()))\ncovid_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop = list(stopwords.words('english'))\nremove = ['i','a','d','o','y']\nstop = [ele for ele in stop if ele not in remove ]\nprint(len(stop))\n\ndef remove_stopwords(text):\n    words = [words for words in text if words not in stop]\n    return words\ncovid_df['Text_wo_punct_wo_sw'] = covid_df['Text_wo_punct'].apply(lambda x: remove_stopwords(x))\ncovid_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\ndef lemmatise(text):\n    lem_text = ' '.join([lemm.lemmatize(words) for words in text])\n    return lem_text\ncovid_df['Final_Text'] = covid_df['Text_wo_punct_wo_sw'].apply(lambda x: lemmatise(x))\ncovid_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training Model for prediction **","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(covid_df.Final_Text, covid_df.Sentiments, test_size = 0.3, random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good = x_train[y_train[y_train == 1].index]\n#neutral = x_train[y_train[y_train == 0].index]\nbad = x_train[y_train[y_train == -1].index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\n\n\nplt.figure(figsize = (20,20))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800).generate(\" \".join(good))\n\nplt.imshow(wc,interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.figure(figsize = (20,20))\n#wc = WordCloud(min_font_size = 3, max_words = 3000, width =1600, height = 800).generate(\" \".join(neutral))\n#plt.imshow(wc,interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20,20))\nwc = WordCloud(min_font_size = 3, max_words = 3000, width = 1600, height = 800).generate(\" \".join(bad))\nplt.imshow(wc, interpolation = 'bilinear')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nobj = TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\nobj_xtrain = obj.fit_transform(x_train)\nobj_xtest = obj.transform(x_test)\nprint(obj_xtrain.shape)\nprint(obj_xtest.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=0)\nlr_tfidf=lr.fit(obj_xtrain,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lr_tfidf_predict = lr.predict(obj_xtest)\nprint(lr.score(obj_xtest,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lr_tfidf_score=accuracy_score(y_test,lr_tfidf_predict)\n#print(lr_tfidf_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(obj_xtrain, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mnb_tfidf_predict = classifier.predict(obj_xtest)\nprint(classifier.score(obj_xtest, y_test))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mnb_tfidf_score = accuracy_score(y_test,mnb_tfidf_predict)\n#print(mnb_tfidf_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC(C=0.5, random_state=42)\nlinear_svc.fit(obj_xtrain, y_train)\n\nprint(linear_svc.score(obj_xtest,y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}