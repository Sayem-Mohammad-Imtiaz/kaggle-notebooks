{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Covid-19 Maps - USA\n\nThe intent of this is to implement learnings from the Geospatial Analysis course here on Kaggle. What better way than to apply my learnings to visualise the Covid-19 cases that have been recorded in the US. There are of course gaps in my general coding ability and this analysis (e.g. different styles of maps could be used, could try manipulating the dataset in other ways).\n\n10 different maps will be produced using three different styles\n\n**Styles**\n1. Heat map\n1. Heat map with time\n1. Choropleth map\n\n**Heat Maps**\n1. Recorded Cases\n      * Through the use of a Geocoder and,\n      * Through the use of an existing US county shapefile  \n2. Recorded Deaths \n3. Recorded Cases with Time\n\n**Choropleth Maps**\n1. Recorded Cases\n    * Without capping the continuous colour range\n    * Through capping the continuous colour range\n    * Using a discrete colour range\n5. Recorded Deaths\n6. Death Rate\n7. Cases to Population Size","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Step 1 - Import Libraries**","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport geopandas as gpd\nfrom shapely.geometry import LineString\nfrom geopandas.tools import geocode\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, FastMarkerCluster\nfrom folium import plugins\nimport math\nimport webbrowser\nfrom IPython.display import HTML\nimport matplotlib.pyplot as plt\nfrom pandasql import sqldf\nimport plotly.express as px\n\n#turn off settingwithcopywarning off\npd.options.mode.chained_assignment = None\n\ngeo_code = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 2 - Inspect Covid-19 Dataset**\n> courtesy of [NYT's github CSV](https://www.kaggle.com/fireballbyedimyrnmom/us-counties-covid-19-dataset)\n\nDataset gets updated frequently. For performance reasons I will restrict dataset to look at only 2 months worth of data.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Have a look at initial dataset\nUS_covid_data_base_file = pd.read_csv(\"../input/us-counties-covid-19-dataset/us-counties.csv\")\n#Restrict dates to be before 30th April for performance reasons\nUS_covid_data_date_restricted = US_covid_data_base_file.loc[US_covid_data_base_file['date']<'2020-04-30']\nprint(\"Shape of initial dataset: \" + str(US_covid_data_date_restricted.shape))\ndf1 = US_covid_data_date_restricted\ndf1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The [source](https://www.kaggle.com/fireballbyedimyrnmom/us-counties-covid-19-dataset) of this data explains that this dataset tracks the cumulative cases however if someone didn't know this it would be useful to first figure out if the dataset tracks incremental cases or cumulative cases. \n\nUsing King, Washington as an example you can plot the 'cases' column over time to determine if the dataset is tracking incremental or cumulative cases. If it is an increasing function it would probably be safe to assume this dataset is recording the cumulative case count over time and not new cases per day.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Use one location as a test example\nUS_covid_data_cumulative_or_new_test = df1.loc[(df1['county']=='King')& (df1['state']=='Washington')]\nplt.figure()\n\n#set x and y variables\nx = US_covid_data_cumulative_or_new_test['date']\ny = US_covid_data_cumulative_or_new_test['cases']\n\n#setting x ticks to be the start and end date so that the x-axis isn't messy\nx_ticks = [x.min(),x.max()]\n\n#plot variables to inspect if 'cases' column is cumulative cases or new cases per day\nplt.plot(x, y)\nplt.ylabel('Cases')\nplt.xlabel('Date')\nplt.xticks(x_ticks,rotation=70)\nplt.title('King, Washington Cases')\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above graph confirms the data is displaying cumulative cases. Since this is the case we will only be concerned about the max date of the dataset when creating static maps.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As always we have to clean up the dataset before we can use it in the way we want to. \n\nInspecting the 'county' column will be our starting point. As seen below there are two things to note:\n1. Values in the 'county' column recorded as 'unknown'. As it is impossible to pinpoint an exact location when mapping, these cases will be removed from the dataset later on. An alternative method to deal with this situation could be to inspect the 'state' column of these unknowns and then add the 'unknown' counties to another county from the same state.\n1. Some counties appear more than once e.g. Washington which appears ~ 30 times. Cases like these are dealt with later through creating a new column that concatenates the county column with the state column to produce a unique identifier. This is important when Geocoding based on a string of text.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Conditional on the rows where we have the max date, inspect the count of all the distinct county names\ninspect_county_values = df1.loc[df1['date'] == df1['date'].max()]\ninspect_county_values['county'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Moving on now, we'll inspect the 'fips' column for null values and check what counties are associated with these. It should be expected that most null fips should be associated with the county = 'Unknown' which is fine because we will drop these rows later on using the dropna() function.\n\nAs we will see below, Kansas City and New York City are the only counties that return null fips values. When mapping through the use of the Geocoder we will want to fill in these null fips values so that when we use the dropna() function we only lose the rowns where county = 'unknown'\n\nRemembering these two counties will also be important as we try to manually account for them when using an external US counties shapefile to map the Covid-19 cases. \n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Inspect the initial dataset for counties that dont have fips\n#This is important for later when mapping with a separate shapefile that contains a list of GEOID's (fips)\n#When inspecting eliminate counties = 'unknown' since you can't reconcile that to a county location\nUS_covid_nulls = df1.loc[(df1['county'] != 'Unknown')& (df1['date'] == df1['date'].max())]\n\nUS_covid_nulls = US_covid_nulls[US_covid_nulls.isnull().any(axis=1)].county.value_counts()\n\nprint(\"Counties which can be manually accounted for when mapping using shapefile: \\n\" + str(US_covid_nulls))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point we have:\n1. Figured out that the dataset represents cumulative cases/deaths - the implication of this is if we want to map the most up to date information then we only need to take the rows where the max date is present\n1. Found some counties are recorded as 'unknown' - all unknown counties also have no value in the 'fips' column. Since we know we can't accurately map these cases we will use utilise the fact that their associated 'fips' value is null and use the dropna() function to get rid of these rows\n1. Found some counties occur more than once - they will have different state names so we will create a new row concatenating the county and state in order to create a unique identifier that will be used in the Geocoder\n1. Found that Kansas City and New York City are counties in the dataset that do not have values in the 'fips' column\n    * In the case of creating a map with the Geocoder we do not want to lose these rows when using the dropna() function so we will fill in the 'fips' value of both with an arbitrary number in order to keep them\n    * In the case of building maps with an external US counties shapefile it will either\n        1. locate their fips in the external shapefile or,\n        1. use google to get their latitude and longitude if the county can't be found in the shapefile         \n\nIn the code below we account for points 1 and 3","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Since cumulative cases, use just the max date so that you have the total cases to date\nUS_covid_data = df1.loc[df1.date == df1.date.max()]\nprint(\"Rows as of max date: \\n\\n\" + str(US_covid_data['date'].value_counts()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Because it's possible the name of a county may exist in more than one state, concat the county name with the state name so that it is unique\nUS_covid_data['concat'] = df1['county']+str(', ')+df1['state']\n#Inspect to see if there are any duplicates - there shouldnt be any \nUS_covid_data['concat'].value_counts(ascending=False)\n#Ascending = False so if first row equals 1 then every value in concat column is unique","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 3 - Building a Geocoder**\nWe will account for points 2 and 4 shortly but before we do that we will attempt to Geocode every row of data based on the concatenation of the county and state name. We could account for points 2 and 4 first but it's possible the Geocoder wont geocode every row thereby meaning will still be left with rows with empty values (latitude and longitude columns which get added).\n\nThis means we'd have to use dropna() again since we cant create a map where information to be mapped are null.\n\nSo we will geocode first then account for NY and Kansas City then drop rows with null values.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Geocode the concat column\nif geo_code:\n    def my_geocoder(row):\n            try:\n                point = geocode(row, provider='nominatim').geometry.iloc[0]\n                return pd.Series({'Latitude': point.y, 'Longitude': point.x, 'geometry': point})\n            except:\n                return None\n\n    US_covid_data[['Latitude', 'Longitude', 'geometry']] = US_covid_data.apply(lambda x: my_geocoder(x['concat']), axis=1)\n    \n    US_covid_data.to_csv('US_covid_data_maxdate_geocoded.csv', index=False)\nelse:\n    US_covid_data = pd.read_csv('US_covid_data_maxdate_geocoded.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fill in null fips value with arbitray number \nblank_fips_counties = ['New York City', 'Kansas City']\n\nfor i in blank_fips_counties:\n    US_covid_data.loc[US_covid_data['county'] == i,'fips'] = 1\n\n\n#Drop any rows where county = 'Unknown' through use of dropna() since fips value for every 'unknown' county is null. Note that any other locations that couldnt be geocoded will also be dropped \nUS_covid = US_covid_data.dropna()\nprint(\"Percentage of rows that could be geocoded:\\n\"+str((US_covid.shape[0]/US_covid_data.shape[0])*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Geocoder has succesfully geocoded most of the locations. However we won't know if these coded locations are correct until we create the map. It's possible some locations which we know should be in the US have been coded to locations outside of the US.\n\nFor the final step before mapping we must understand how the mapping works.If we were to map the data as is then it would map it as if each row referred to 1 case. In order to map based on the 'cases' column we will duplicate the rows based on the values in this column. For example if King, Washington has 235 cases recorded to date we will be duplicating that row to result in 235 rows.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#In order to map, each row needs to be replicated based on the value in the 'cases' column\n#For example if row x has a cases count of 635 then row x needs to appear 635 times\nUS_covid = US_covid.loc[US_covid.index.repeat(US_covid['cases'])]\nprint(\"Shape of dataset to be mapped: \" +str(US_covid.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Heat Map using Geocoder - Cases**\nThe dataset is now ready to be mapped\n\nAs seen below, building the Geocoder wasn't as useful as hoped. It's evident that not all locations were geocoded to the US when you zoom out. Because of this we will move on to mapping the Covid-19 cases with an external shapefile of US counties.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Create base map\nmap1 = folium.Map(location=[40, -95], zoom_start=4)\n\n#Marker Cluster\nmap1.add_child(FastMarkerCluster(US_covid[['Latitude', 'Longitude']].values.tolist()))\n#Heat Map\nHeatMap(data=US_covid[['Latitude', 'Longitude']], radius=16.5, blur =16.5).add_to(map1)\n\nmap1.save('plot_data.html')   \nHTML('<iframe src=plot_data.html width=800 height=600></iframe>')\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Step 4 - Geocoding with an external file**\nAs seen with the heat map above, some data has been mapped to locations outside of the US. A new dataset will now be introduced to account for this. This dataset contains point and shape coordinates for all US counties and was sourced from [the home of the U.S. Government’s open data](https://catalog.data.gov/dataset/tiger-line-shapefile-2017-nation-u-s-current-county-and-equivalent-national-shapefile)\n\nAs seen in the code below only the .shp file will be used however it order for it to be read the files that accompany this shp file from the link above must be found in the file directory. \n\nNote that from the shapefile (GeoDataFrame) we will only take the 1) county name, 2) geoid (so we can join with fips of Covid-19 dataset) and the 3) latitude and 4) longitude columns","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Read the file\nus_counties_shapefile_base = gpd.read_file(\"../input/us-counties-geocoded/tl_2017_us_county.shp\")\ndf2 = us_counties_shapefile_base\ndf2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"us_counties_dataframe = pd.DataFrame(df2[['NAME','GEOID', 'INTPTLAT', 'INTPTLON']])\nus_counties_dataframe.to_csv('geocodes.csv', index = False)\nus_counties_dataframe['GEOID'] = us_counties_dataframe['GEOID'].astype('float64')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen earlier Kansas City and New York City were present in the Covid-19 dataset however they did not have a fips value associated with them. We will therefore check the US counties GeoDataFrame to find their fips. In the shapefile, the equivalent column is the GEOID column\n\nOnly New York City will be found in this GeoDataFrame. We will use google later on to get the latitude and longitude coordinates of Kansas City","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#check df2 Kansas City, New York City and Joplin to see if you can manually account for these two when merging df1 and df2\nfor i in [\"Kansas\", \"New York\"]:\n    check = us_counties_dataframe[us_counties_dataframe['NAME'].str.contains(i)]\n    print(\"Check for \"+str(i)+\"\\n\" +str(check)+\"\\n\")\n\n#Can manually account for New York City as there is only one row that returns from df2\n#Kansas City, Missouri - will have to get the coordinates from google","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this stage the original Covid-19 dataset needs to be prepared so that it can be joined to the GeoDataFrame. As with earlier, use the max() function on the 'date' column in order to take only what is necessary","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Original Dataframe of Covid cases in USA\nUS_covid_data = df1\nUS_covid_data = US_covid_data.loc[US_covid_data.date == US_covid_data.date.max()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From our search in the GeoDataFrame above we found that the GEOID for New York City is 36061. Insert this value into 'fips' column of the Covid-19 dataframe.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\"NY City before update: \\n\" +str(US_covid_data.loc[US_covid_data['county'] == 'New York City'])+\"\\n\\n\\n\")\nUS_covid_data.at[US_covid_data.loc[US_covid_data['county'] == 'New York City'].index[0],'fips'] = 36061.0\nprint(\"NY City after update: \\n\" +str(US_covid_data.loc[US_covid_data['county'] == 'New York City'])+\"\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point we can join the county GeoDataFrame with the Covid-19 dataframe. The type of join we use is a left join with the Covid-19 dataframe being the left table. Have used a left join so we don't lose the Kansas City and Joplin rows which we will shortly account for. Any other rows that can't be joined to the GeoDataFrame will be dropped later as the counties associated with these rows are 'unknown'","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Merge df1 and df2 together \nconcat_result = US_covid_data.merge(us_counties_dataframe[['GEOID','INTPTLAT', 'INTPTLON']],left_on = 'fips', right_on = 'GEOID', how = 'left')\nprint(\"Rows in left df: \"+str(US_covid_data.shape[0]))\nprint(\"Rows in joint df: \"+str(concat_result.shape[0]))\nconcat_result\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As stated earlier, Kansas City can be accounted for by inserting the latitude and longitude values. We couldn't account for it earlier in the same way New York City was because there was no GEOID from the GeoDataFrame for Kansas City. Using Google, the latitude and longitude for Kansas City is 39.0997 and -94.5786\n\nShould also note that we will fill in the 'fips' and 'GEOID' column with arbitrary numbers so that this row isn't dropped when we use the dropna() function later on.\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Manually update Kansas City, Missouri with 39.0997°, -94.5786°\nprint(\"Kansas City before update: \\n\" +str(concat_result.loc[concat_result['county'] == 'Kansas City'])+\"\\n\\n\\n\")\nconcat_result.at[concat_result.loc[concat_result['county'] == 'Kansas City'].index[0],'INTPTLAT'] = '+39.0997000'\nconcat_result.at[concat_result.loc[concat_result['county'] == 'Kansas City'].index[0],'INTPTLON'] = '-94.5786000'\n#Also add in arbitrary numbers to fips and GEOID column so that this doesn't get dropped when you use dropna() later\nconcat_result.at[concat_result.loc[concat_result['county'] == 'Kansas City'].index[0],'fips'] = 1\nconcat_result.at[concat_result.loc[concat_result['county'] == 'Kansas City'].index[0],'GEOID'] = 1\nprint(\"Kansas City after update: \\n\" +str(concat_result.loc[concat_result['county'] == 'Kansas City'])+\"\\n\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the below set of code we will clean up the joint dataframe so that we can successfully map the covid cases. This includes removing the '+' from the latitude column, removing rows where there are null values (this is where county = 'unknown') and duplicating each row based on the value in the 'cases' column","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Remove the '+' from the latitude column so that it can be mapped \nconcat_result['INTPTLAT'] = concat_result['INTPTLAT'].astype('str')\nconcat_result['INTPTLAT']\nconcat_result['INTPTLAT'] = concat_result['INTPTLAT'].str[1:]\nconcat_result['INTPTLAT']\n\n\n#Remove rows with NaN's - this will be where county = Unknown\nconcat_result.dropna(how = 'any', inplace = True)\n\n#Mapping the Covid Cases - required to duplicate rows based on value in 'cases' column\nconcat_result_cases = concat_result.loc[concat_result.index.repeat(concat_result['cases'])]\nprint(\"Shape of dataset to be mapped: \" +str(concat_result_cases.shape))\n\n#Convert latitude and longitude columns so that it's compatible with mapping\nconcat_result_cases['INTPTLAT'] = concat_result_cases['INTPTLAT'].astype('float64')\nconcat_result_cases['INTPTLON'] = concat_result_cases['INTPTLON'].astype('float64')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Heat Map - Cases**\n","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Create base map\nmap2 = folium.Map(location=[40, -95], zoom_start=4)\n\n#Marker Cluster\nmap2.add_child(FastMarkerCluster(concat_result_cases[['INTPTLAT', 'INTPTLON']].values.tolist()))\n#Heat Map\nHeatMap(data=concat_result_cases[['INTPTLAT', 'INTPTLON']], radius=16.5, blur = 16.5).add_to(map2)\n\nmap2.save('plot_data2.html')   \nHTML('<iframe src=plot_data2.html width=800 height=600></iframe>')\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using the joint dataframe we can quickly create a heat map reflecting the recorded deaths. We can start just after the point where rows containing nulls were dropped in order to map the cases. This time however, the rows are duplicated based on the 'deaths' column as opposed to the 'cases' column.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Mapping the Covid Deaths\nconcat_result_deaths = concat_result.loc[concat_result.index.repeat(concat_result['deaths'])]\nconcat_result_deaths = concat_result_deaths.loc[concat_result_deaths['deaths']!=0]\nprint(\"Shape of dataset to be mapped: \" +str(concat_result_deaths.shape))\n\nconcat_result_deaths['INTPTLAT'] = concat_result_deaths['INTPTLAT'].astype('float64')\nconcat_result_deaths['INTPTLON'] = concat_result_deaths['INTPTLON'].astype('float64')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Heat Map - Deaths**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create base map\nmap3 = folium.Map(location=[40, -95], zoom_start=4)\n\n#Marker Cluster\nmap3.add_child(FastMarkerCluster(concat_result_deaths[['INTPTLAT', 'INTPTLON']].values.tolist()))\n#Heat Map\nHeatMap(data=concat_result_deaths[['INTPTLAT', 'INTPTLON']], radius=16.5, blur = 16.5).add_to(map3)\n\nmap3.save('plot_data3.html')   \nHTML('<iframe src=plot_data3.html width=800 height=600></iframe>')\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With part 1 now complete we will move on to [part 2](https://www.kaggle.com/blakkmagic/covid-maps-usa-part-2) of this analysis which will introduce a heat map with a time element.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}