{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/quality-prediction-in-a-mining-process/MiningProcess_Flotation_Plant_Database.csv', decimal=',')\ndf['date'] = pd.to_datetime(df['date'])\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape: ', df.shape)\nprint('Columns: ')\nprint(df.columns)\nprint('Datatypes:')\nprint(df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show the hours with less than 180 records (missing data within the hour)."},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = df.groupby('date').count()\ncounts[counts['% Iron Feed'] < 180]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Luckily only the first hour is missing 6 data points and one other hour is missing one. When creating an accurate time series index, we will arbitrarily take out the first couple 20 second intervals to match the amount of records found for those hours. \n\nWe will now create the 20 second frequency Datetime Index."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get a series of unique hourly timestamps\nhours = pd.Series(df['date'].unique())\nhours.index = hours\nlen(hours)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a date time index from the first to the last hour included in the date column\ndate_range = pd.date_range(start=df.iloc[0,0], end='2017-09-09 23:59:40', freq='20S')\n# remove first couple observations consistent with the counts exploration above\ndate_range = date_range[6:]\ndate_range[-5:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create lists from both the hours series and the new datetime index\nhours_list = hours.index.format()\nprint(hours_list[:5])\nseconds_list = date_range.format()\nprint(seconds_list[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# match the new datetime index to the hours series and only append the timestamps if the datea and hour match the hours list\nnew_index = []\nfor idx in seconds_list:\n    if (idx[:13] + ':00:00') in hours_list:\n        new_index.append(idx)\n\n#remove the one missing interval within the hour which we found earlier using the counts\nnew_index.remove('2017-04-10 00:00:00')\nnew_index[-20:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(new_index))\nprint(len(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['index'] = new_index\ndf['index'] = pd.to_datetime(df['index'])\ndf.index = df['index']\ndf = df.loc[:, df.columns[:-1]]\ndf.rename(columns={'date': 'datetime hours'}, inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking which variables have hourly vs 20-sec frequency\n\nWe can determine the frequency of the variables by grouping the dataframe by hours and counting the number of unique values. For hourly variables it should be 1, for the higher frequency variables it should be close to 180."},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_avg = []\nfor col in df.columns:\n    unique_avg.append(df.groupby('datetime hours').apply(lambda x: len(x[col].unique())).mean())\nplt.plot(np.arange(len(unique_avg)), unique_avg)\nplt.title('Average Count of Unique Values per Hour for every Variable')\nplt.ylabel('Count')\nplt.xticks(list(range(len(unique_avg))), list(df.columns), rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Only the Iron and Silica Feed and Concentrate variables seem hourly, the rest seems to contain higher frequency measurements. Yet, the unique averages are much higher than 1 for Silica Concentrate especially, which could indicate some inconsistencies."},{"metadata":{},"cell_type":"markdown","source":"### Interpolation Cleaning"},{"metadata":{},"cell_type":"markdown","source":"We further checked individual variables to see if there are any outliers etc. We noticed that there seemed to be some interpolated values which can be detrimental to any modelling attempts."},{"metadata":{"trusted":true},"cell_type":"code","source":"# some values for Silica Concentration seem interpolated so we're removing the values for all those hours\n#some imports\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\n#get list of % Silica Concentrate values, see the first 5 that have more than one hourly value\nsilica_unique = df.groupby('datetime hours').apply(lambda x: len(x['% Silica Concentrate'].unique()))\nprint(silica_unique[silica_unique > 1][:5])\n\n# plot before the interpolations are taken out of the dataframe\nplt.plot(df['% Silica Concentrate'][df['datetime hours'] == silica_unique[silica_unique > 1].index[0]])\n\n# take out interpolated hours for % Silica Concentrate\ninterpolated_hours = silica_unique[silica_unique >1].index.format()\nclean_df=df[~df['datetime hours'].isin(interpolated_hours)]\n\n#finish the graph\nplt.title('Interpolated hour of % Silica Concentrate')\nplt.xlabel('Datetime Index [Day Hour:Minute]')\nplt.ylabel('Silica Concentrate [%]')\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we will graph some of the input variables and check for more interpolations. Both Iron Feed and Silica Feed seem to have a large amount of interpolated values. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(clean_df.index, clean_df['% Iron Feed'])\nplt.plot(clean_df.index, clean_df['% Silica Feed'])\nplt.title('Iron and Silica Percentage of Input Feed')\nplt.legend(loc='best')\nplt.ylabel('Iron/Silica Feed [%]')\nplt.xlabel('Date [Year-Month]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that there are some missing values for the Iron and Silica Feed as well, so let us investigate the most frequently occuring values."},{"metadata":{"trusted":true},"cell_type":"code","source":"#if_unique = clean_df.groupby('datetime hours').apply(lambda x: len(x['% Iron Feed'].unique()))\n#sf_unique = clean_df.groupby('datetime hours').apply(lambda x: len(x['% Silica Feed'].unique()))\n#print(if_unique[if_unique > 1][:5])\nprint('Count of unique hours in cleaned df: ', len(clean_df.groupby('datetime hours').mean()))\nprint('Count of unique % Iron Feed values: ',len(clean_df['% Iron Feed'].unique()))\nprint('Count of unique % Silica Feed values: ',len(clean_df['% Silica Feed'].unique()))\nprint('Reference: Count of unique % Silica Concentrate values: ',len(clean_df['% Silica Concentrate'].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to get unique values of a df column and their counts \ndef get_unique_counts(column):\n    df = pd.DataFrame()\n    \n    uv_list, count_list = list(column.unique()), []\n    \n    for uv in uv_list:\n        count_list.append(len(column[column == uv]))\n        \n    df['unique_values'] = uv_list\n    df['count'] = count_list\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Less than "},{"metadata":{"trusted":true},"cell_type":"code","source":"if_unique = get_unique_counts(clean_df['% Iron Feed']).sort_values('count',ascending=False)\nsf_unique = get_unique_counts(clean_df['% Silica Feed']).sort_values('count',ascending=False)\nprint(if_unique.head(10))\nprint(sf_unique.head(10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The four highest frequencies show the same count for both variables, let's look at the graphs to confirm that these were interpolated."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(6):\n    clean_df['% Silica Feed'][clean_df['% Silica Feed'] == sf_unique.iloc[i,0]].plot()\n    clean_df['% Iron Feed'][clean_df['% Iron Feed'] == if_unique.iloc[i,0]].plot() \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_df.groupby([clean_df.index.date, clean_df.index.hour]).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will remove the intervals that feature seemingly unclean data, i.e. the four highest frequency observations. "},{"metadata":{"trusted":true},"cell_type":"code","source":"dirty_idx = []\nfor i in range(4):\n    dirty_idx.extend(clean_df['% Silica Feed'][clean_df['% Silica Feed'] == sf_unique.iloc[i,0]].index.format())\ndirty_idx\nprint(len(dirty_idx), len(clean_df))\nclean_df=clean_df[~clean_df.index.isin(dirty_idx)]\nprint(clean_df.shape)\nclean_df['% Silica Feed'].plot()\nclean_df['% Iron Feed'].plot() \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"pair_cols = list(df.columns[1:8])\npair_cols.extend(df.columns[-2:])\nprint(pair_cols)\n#smol_df = clean_df.loc[:,pair_cols]\nsns.pairplot(clean_df.loc[:,pair_cols])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No apparent meaningful patterns besides between Iron and Silica Concentrate and Iron and Silica Feed (which are to be expected).\n\nWe also decided to check which minute of the hour showed the highest correlation with % Silica Concentrate for each variable. Our hypothesis was that they should peak around when the measurements where usually taken. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# minute correlations \ncorr_df = pd.DataFrame(index=clean_df.columns[1:])\nfor minute in range(60):\n    min_df = clean_df[clean_df.index.minute == minute]\n    corr_df[str(minute)] = min_df.groupby([min_df.index.date, min_df.index.hour, min_df.index.minute]).mean().corr().iloc[:,-1]\ncorr_df = corr_df.transpose()\n\ncorr_df.iloc[:,:-2].plot(legend=False)\nplt.title(\"Correlations of Variables vs Silica Concentrate Grouped by Minute\")\nplt.ylabel(\"Correlation\")\nplt.xlabel(\"Minute of the Hour\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(actual, preds):\n    return np.sqrt(np.sum((np.array(actual)-np.array(preds))**2) / len(actual))\ndef mape(actual, preds):\n    return np.sum(np.abs((np.array(actual)-np.array(preds))/(np.array(actual)))) / len(actual)\ndef mae(actual, preds):\n    return np.sum(np.abs((np.array(actual)-np.array(preds)))) / len(actual)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XDBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=0\nend=100\n\nX = clean_df.iloc[start*24*180:end*24*180,1:-2]\ny = clean_df.iloc[start*24*180:end*24*180,-1]\nX_test = clean_df.iloc[(end)*24*180:,1:-2]\ny_test = clean_df.iloc[(end)*24*180:,-1]\n\nxgbr= xgb.XGBRegressor(max_depth=8, n_estimators=50, min_sample_split = 500, subsample=0.5, silent=True, colsample_bytree=0.8, gamma=100)\n\nxgbr.fit(X,y)\nprint(xgbr.feature_importances_)\nprint(xgbr.score(X,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds=xgbr.predict(X_test)\npred_df = pd.DataFrame(preds, columns=['predictions'])\nprint('Train RMSE: ' + str(rmse(y, xgbr.predict(X))))\nprint('RMSE: ' + str(rmse(y_test, preds)))\nprint('MAE: ' + str(mae(y_test, preds)))\nprint('MAPE: ' + str(mape(y_test, preds)))\nplt.plot(y_test, label = 'Actual')\npred_df.index = y_test.index\nplt.plot(pred_df, label = 'Prediction')\nplt.legend()\nplt.ylim(0,6)\nplt.title('XGBoost Regressor Model Forecast')\nplt.xticks(rotation='vertical')\nplt.ylabel('Silica Concentrate [%]')\nplt.xlabel('Time [Days]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nstart=0\nend=100\nX = clean_df.iloc[start*24*180:end*24*180,1:-2]\ny = clean_df.iloc[start*24*180:end*24*180,-1]\nX_test = clean_df.iloc[(end)*24*180:,1:-2]\ny_test = clean_df.iloc[(end)*24*180:,-1]\n\nrr= Ridge(alpha= 1, fit_intercept=False, normalize=True)\n\nrr.fit(X,y)\nprint(rr.get_params)\nprint(rr.score(X,y))\nprint(rr.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds=rr.predict(X_test)\npred_df = pd.DataFrame(preds, columns=['predictions'])\ny_pred= df.iloc[140*24*180:147*24*180,-1]\nprint('Train RMSE: ', rmse(y, rr.predict(X)))\nprint('RMSE: ', rmse(y_test, preds))\nprint('MAPE: ', mape(y_test, preds))\nprint('MAE: ', mae(y_test, preds))\nplt.plot(y_test, label = 'Actual')\npred_df.index = y_test.index\nplt.plot(pred_df, label = 'Predictions')\nplt.title('Ridge Regression Model Forecast')\nplt.xticks(rotation='vertical')\nplt.ylabel('Silica Concentrate [%]')\nplt.xlabel('Time [Days]')\nplt.ylim(0,6)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(clean_df['% Silica Concentrate'])\nplt.title('Distribution Plot for % Silica Concentrate')\nplt.ylabel('Relative Frequency')\nplt.xlabel('Silica Concentrate [%]')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create hour column\ncdf = clean_df.copy(deep=True)\ncdf['hour'] = cdf.index.hour\n\n# get labels \ncdf['label'] = 0\ncdf['label'][cdf['% Silica Concentrate'] > 3] = 1\nprint(cdf['label'][cdf['label'] == 1].count())\nprint(cdf['label'][cdf['label'] == 0].count())\nprint(cdf['label'][cdf['label'] == 0].count() / cdf['label'][cdf['label'] == 1].count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random \n\nrandom.seed(69)\n#start=0\n#end=138\n\nmdf = cdf.drop(columns = ['datetime hours', '% Iron Concentrate', '% Silica Concentrate'])\n\n#create and sample train set for equal class distribution\n#train = mdf.iloc[start*24*180:end*24*180]\ntrain = mdf.iloc[:-14*24*180]\nzero_idx = train[train['label'] == 0].index\nsample_idx = random.sample(list(zero_idx), train[train['label'] == 1].shape[0])\nsample_idx.extend(list(train[train['label'] == 1].index))\nsample_idx = pd.DatetimeIndex(sample_idx).sort_values()\ntrain = train.reindex(sample_idx)\n\nX = train.iloc[:,:-1]\ny = train.iloc[:,-1]\n\n#X_eval = mdf.iloc[(end)*24*180:(end+7)*24*180,:-1]\n#y_eval = mdf.iloc[(end)*24*180:(end+7)*24*180,-1]\n#X_test = mdf.iloc[(end+7)*24*180:(end+14)*24*180,:-1]\n#y_test = mdf.iloc[(end+7)*24*180:(end+14)*24*180,-1]\n#X_eval = mdf.iloc[-14*24*180:-7*24*180,:-1]\n#y_eval = mdf.iloc[-14*24*180:-7*24*180,-1]\nX_test = mdf.iloc[-14*24*180:,:-1]\ny_test = mdf.iloc[-14*24*180:,-1]\n\nprint(y[y == 0].count() / y[y==1].count())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initial XGBoost attempt"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbc= xgb.XGBClassifier(max_depth=4, n_estimators=5, subsample=0.5, eval_metric='logloss', colsample_bytree=0.8, \n                        min_child_weight=100, gamma=50)\n\nxgbc.fit(X,y)\nprint(xgbc.feature_importances_)\nprint(xgbc.score(X,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\npreds=xgbc.predict(X_test)\nresults = confusion_matrix(y_test, preds) \nprint('Test Set Results')\nprint('Confusion Matrix :')\nprint(results) \nprint('Accuracy Score :',accuracy_score(y_test, preds) )\nprint('Report : ')\nprint(classification_report(y_test, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_preds=xgbc.predict(X_eval)\neval_results = confusion_matrix(y_eval, eval_preds) \nprint('Evaluation Set Results')\nprint('Confusion Matrix :')\nprint(eval_results) \nprint('Accuracy Score :',accuracy_score(y_eval, eval_preds) )\nprint('Report : ')\nprint(classification_report(y_eval, eval_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_preds=xgbc.predict(X)\ntrain_results = confusion_matrix(y, train_preds) \nprint('Training Set Results')\nprint('Confusion Matrix :')\nprint(train_results) \nprint('Accuracy Score :',accuracy_score(y, train_preds) )\nprint('Report : ')\nprint(classification_report(y, train_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils.multiclass import unique_labels\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        print()\n        #print('Confusion matrix, without normalization')\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(np.array(y_test), np.array(preds), classes=np.array(['pure (below cutoff)', 'impure (above cutoff)']), normalize=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Grid Search"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random \n\nrandom.seed(69)\n#start=0\nend=-14\n\nmdf = cdf.drop(columns = ['datetime hours', '% Iron Concentrate', '% Silica Concentrate'])\n\n#create and sample train set for equal class distribution\n#train = mdf.iloc[start*24*180:end*24*180]\ntrain = mdf.iloc[:-14*24*180]\nzero_idx = train[train['label'] == 0].index\nsample_idx = random.sample(list(zero_idx), train[train['label'] == 1].shape[0])\nsample_idx.extend(list(train[train['label'] == 1].index))\nsample_idx = pd.DatetimeIndex(sample_idx).sort_values()\ntrain = train.reindex(sample_idx)\n\nX = train.iloc[:,:-1]\ny = train.iloc[:,-1]\n\nX_eval = mdf.iloc[(end)*24*180:(end+7)*24*180,:-1]\ny_eval = mdf.iloc[(end)*24*180:(end+7)*24*180,-1]\nX_test = mdf.iloc[(end+7)*24*180:,:-1]\ny_test = mdf.iloc[(end+7)*24*180:,-1]\n#X_test = mdf.iloc[-14*24*180:,:-1]\n#y_test = mdf.iloc[-14*24*180:,-1]\n\nprint(y[y == 0].count() / y[y==1].count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#max_depth_list = [5,7,10,15]\n#n_trees_list  = [50, 75, 100, 150, 200]\n\n#feature_1 = []\n#feature_2 = []\n#train_acc = []\n#test_acc = []\n#test_precision = []\n#test_recall = []\n#test_trueones = []\n\n#for max_depth in max_depth_list:\n#    for n_tree in n_trees_list:\n#        xgbc= xgb.XGBClassifier(max_depth=max_depth, n_estimators=n_tree, subsample=0.5, eval_metric='logloss', colsample_bytree=0.8, \n#                        min_child_weight=100, gamma=50)\n#        \n#        xgbc.fit(X,y)\n#        pred = xgbc.predict(X_test)\n#        \n#        feature_1.append(max_depth)\n#        feature_2.append(n_tree)\n#        train_acc.append(xgbc.score(X,y))\n#        test_acc.append(xgbc.score(X_test, y_test))\n#        cm = confusion_matrix(y_test, pred) \n#        test_trueones.append(cm[1,1])\n#        test_precision.append((cm[1,1]) / (cm[0,1] + cm[1,1]))\n#        test_recall.append((cm[1,1]) / (cm[1,0] + cm[1,1]))\n#        print(max_depth,n_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#result_df = pd.DataFrame()\n#result_df['feature_1'] = feature_1\n#result_df['feature_2'] = feature_2\n#result_df['train_acc'] = train_acc\n#result_df['test_acc'] = test_acc\n#result_df['test_precision'] = test_precision\n#result_df['test_recall'] = test_recall\n#result_df['test_trueones'] = test_trueones\n#result_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#max_depth_list2 = [2,3,4,5,6,8]\n#n_trees_list2  = [3,5,6,7,8,9,10]#\n\n#feature_12 = []\n#feature_22 = []\n#train_acc2 = []\n#test_acc2 = []\n#test_precision2 = []\n#test_recall2 = []\n#test_trueones2 = []\n\n#for max_depth in max_depth_list2:\n#    for n_tree in n_trees_list2:\n#        xgbc= xgb.XGBClassifier(max_depth=max_depth, n_estimators=n_tree, subsample=0.5, eval_metric='logloss', colsample_bytree=0.8, \n#                        min_child_weight=100, gamma=50)\n#       \n#        xgbc.fit(X,y)\n#        pred = xgbc.predict(X_eval)\n#        \n#        feature_12.append(max_depth)\n#        feature_22.append(n_tree)\n#        train_acc2.append(xgbc.score(X,y))\n #       test_acc2.append(xgbc.score(X_eval, y_eval))\n#        cm = confusion_matrix(y_eval, pred) \n#        test_trueones2.append(cm[1,1])\n#        test_precision2.append((cm[1,1]) / (cm[0,1] + cm[1,1]))\n#        test_recall2.append((cm[1,1]) / (cm[1,0] + cm[1,1]))\n#        #print(max_depth,n_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#result_df2 = pd.DataFrame()\n#result_df2['max_depth'] = feature_12\n#result_df2['n_trees'] = feature_22\n#result_df2['train_acc'] = train_acc2\n#result_df2['test_acc'] = test_acc2\n#result_df2['test_precision'] = test_precision2\n#result_df2['test_recall'] = test_recall2\n#result_df2['test_trueones'] = test_trueones2\n#result_df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nmax_depth_list = [2,3,4,5]\nn_trees_list  = [2,3,4,5,6,7,8]\nmin_child_weight_list = [10,50,100,200,300]\ngamma_list = [1,10,30,50,100]\n\nfeature_1 = []\nfeature_2 = []\nfeature_3 = []\nfeature_4 = []\ntrain_acc = []\ntest_acc = []\ntest_precision = []\ntest_recall = []\ntest_trueones = []\n\nfor max_depth in max_depth_list:\n    for n_tree in n_trees_list:\n        for min_child_weight in min_child_weight_list:\n            for gamma in gamma_list:\n                xgbc= xgb.XGBClassifier(max_depth=max_depth, n_estimators=n_tree, subsample=0.5, eval_metric='logloss', #colsample_bytree=0.8, \n                        min_child_weight=min_child_weight, gamma=gamma)\n       \n                xgbc.fit(X,y)\n                pred = xgbc.predict(X_eval)\n\n                feature_1.append(max_depth)\n                feature_2.append(n_tree)\n                feature_3.append(min_child_weight)\n                feature_4.append(gamma)\n                train_acc.append(xgbc.score(X,y))\n                test_acc.append(xgbc.score(X_eval, y_eval))\n                cm = confusion_matrix(y_eval, pred) \n                test_trueones.append(cm[1,1])\n                test_precision.append((cm[1,1]) / (cm[0,1] + cm[1,1]))\n                test_recall.append((cm[1,1]) / (cm[1,0] + cm[1,1]))\n                #print(max_depth,n_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df3 = pd.DataFrame()\nresult_df3['max_depth'] = feature_1\nresult_df3['n_trees'] = feature_2\nresult_df3['min_child_weight'] = feature_3\nresult_df3['gamma'] = feature_4\nresult_df3['train_acc'] = train_acc\nresult_df3['test_acc'] = test_acc\nresult_df3['test_precision'] = test_precision\nresult_df3['test_recall'] = test_recall\nresult_df3['test_trueones'] = test_trueones\nresult_df3.to_csv('/kaggle/working/xgb_grid_search_results.csv')\nresult_df3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df3.sort_values(['test_trueones'], ascending=False).head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df3.groupby([result_df3['max_depth'], result_df3['n_trees']])['test_trueones'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df3[(result_df3['max_depth'] == 4) & (result_df3['n_trees'] == 8)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbc= xgb.XGBClassifier(max_depth=5, n_estimators=2, subsample=0.5, eval_metric='logloss', min_child_weight = 300)\n\nxgbc.fit(X,y)\nprint(xgbc.feature_importances_)\nprint(xgbc.score(X,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\npreds=xgbc.predict(X_eval)\nresults = confusion_matrix(y_eval, preds) \nprint('Test Set Results')\nprint('Confusion Matrix :')\nprint(results) \nprint('Accuracy Score :',accuracy_score(y_eval, preds) )\nprint('Report : ')\nprint(classification_report(y_eval, preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(np.array(y_eval), np.array(preds), classes=np.array(['pure (below cutoff)', 'impure (above cutoff)']), normalize=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, thresholds = roc_curve(np.array(y_eval), xgbc.predict_proba(X_eval)[:,1])\nroc_auc = roc_auc_score(np.array(y_eval), xgbc.predict_proba(X_eval)[:,1])\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0,1], [0,1], linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbc= xgb.XGBClassifier(max_depth=3, n_estimators=7, subsample=0.5, eval_metric='logloss', min_child_weight=300)\n\nxgbc.fit(X,y)\nprint(xgbc.feature_importances_)\nprint(xgbc.score(X,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\npreds=xgbc.predict(X_eval)\nresults = confusion_matrix(y_eval, preds) \nprint('Test Set Results')\nprint('Confusion Matrix :')\nprint(results) \nprint('Accuracy Score :',accuracy_score(y_eval, preds) )\nprint('Report : ')\nprint(classification_report(y_eval, preds))\nplot_confusion_matrix(np.array(y_eval), np.array(preds), classes=np.array(['pure (below cutoff)', 'impure (above cutoff)']), normalize=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, thresholds = roc_curve(np.array(y_eval), xgbc.predict_proba(X_eval)[:,1])\nroc_auc = roc_auc_score(np.array(y_eval), xgbc.predict_proba(X_eval)[:,1])\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0,1], [0,1], linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Reg"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(fit_intercept=False, C=0.1)\n\nlr.fit(X,y)\nprint(lr.decision_function(X))\nprint(lr.score(X,y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\npreds=lr.predict(X_eval)\nresults = confusion_matrix(y_eval, preds) \nprint('Test Set Results')\nprint('Confusion Matrix :')\nprint(results) \nprint('Accuracy Score :',accuracy_score(y_eval, preds) )\nprint('Report : ')\nprint(classification_report(y_eval, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Grid"},{"metadata":{"trusted":true},"cell_type":"code","source":"intercept_list = [True, False]\nC_list  = [10,1,0.1,0.01,0.001,0.0001]\n\nfeature_1 = []\nfeature_2 = []\ntrain_acc = []\ntest_acc = []\ntest_precision = []\ntest_recall = []\ntest_trueones = []\n\nfor C in C_list:\n    for intercept in intercept_list:\n        lr = LogisticRegression(fit_intercept=intercept, C=C, solver='liblinear')\n\n       \n        lr.fit(X,y)\n        pred = lr.predict(X_eval)\n        \n        feature_1.append(C)\n        feature_2.append(intercept)\n        train_acc.append(lr.score(X,y))\n        test_acc.append(lr.score(X_eval, y_eval))\n        cm = confusion_matrix(y_eval, pred) \n        test_trueones.append(cm[1,1])\n        test_precision.append((cm[1,1]) / (cm[0,1] + cm[1,1]))\n        test_recall.append((cm[1,1]) / (cm[1,0] + cm[1,1]))\n        #print(C,intercept)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df = pd.DataFrame()\nresult_df['C'] = feature_1\nresult_df['intercept'] = feature_2\n#result_df['min_child_weight'] = feature_3\n#result_df['gamma'] = feature_4\nresult_df['train_acc'] = train_acc\nresult_df['test_acc'] = test_acc\nresult_df['test_precision'] = test_precision\nresult_df['test_recall'] = test_recall\nresult_df['test_trueones'] = test_trueones\nresult_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"C_list  = [10, 7.5, 5, 4, 3, 2,1,0.75, 0.5, 0.3, 0.1,0.05, 0.01]\n\nfeature_1 = []\nfeature_2 = []\ntrain_acc = []\ntest_acc = []\ntest_precision = []\ntest_recall = []\ntest_trueones = []\n\nfor C in C_list:\n    lr = LogisticRegression(fit_intercept=True, C=C, solver='liblinear')\n\n\n    lr.fit(X,y)\n    pred = lr.predict(X_eval)\n\n    feature_1.append(C)\n    #feature_2.append(intercept)\n    train_acc.append(lr.score(X,y))\n    test_acc.append(lr.score(X_eval, y_eval))\n    cm = confusion_matrix(y_eval, pred) \n    test_trueones.append(cm[1,1])\n    test_precision.append((cm[1,1]) / (cm[0,1] + cm[1,1]))\n    test_recall.append((cm[1,1]) / (cm[1,0] + cm[1,1]))\n    #print(C,intercept)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df2 = pd.DataFrame()\nresult_df2['C'] = feature_1\n#result_df2['intercept'] = feature_2\nresult_df2['train_acc'] = train_acc\nresult_df2['test_acc'] = test_acc\nresult_df2['test_precision'] = test_precision\nresult_df2['test_recall'] = test_recall\nresult_df2['test_trueones'] = test_trueones\nresult_df2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(fit_intercept=True, C=7.50, solver='liblinear')\n\nlr.fit(X,y)\n\nlr_preds= lr.predict(X_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nresults = confusion_matrix(y_eval, lr_preds) \nprint('Logistic Regression Results')\nprint('Confusion Matrix :')\nprint(results) \nprint('Accuracy Score :',accuracy_score(y_eval, lr_preds) )\nprint('Report: ')\nprint(classification_report(y_eval, lr_preds))\nplot_confusion_matrix(np.array(y_eval), np.array(lr_preds), classes=np.array(['pure (below cutoff)', 'impure (above cutoff)']), normalize=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Putting it all together"},{"metadata":{"trusted":true},"cell_type":"code","source":"import random \n\nrandom.seed(69)\n\nmdf = df.drop(columns = ['datetime', '% Iron Concentrate', '% Silica Concentrate'])\n\ntrain = mdf.iloc[:-14*24*180,:]\nzero_idx = train[train['label'] == 0].index\nsample_idx = random.sample(list(zero_idx), train[train['label'] == 1].shape[0])\nsample_idx.extend(list(train[train['label'] == 1].index))\nsample_idx = pd.DatetimeIndex(sample_idx).sort_values()\ntrain = train.reindex(sample_idx)\n\nX = train.iloc[:,:-1]\ny = train.iloc[:,-1]\n\nX_eval = mdf.iloc[-14*24*180:-7*24*180,:-1]\ny_eval = mdf.iloc[-14*24*180:-7*24*180,-1]\nX_test = mdf.iloc[-7*24*180:,:-1]\ny_test = mdf.iloc[-7*24*180:,-1]\n\nprint(y[y == 0].count() / y[y==1].count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(fit_intercept=True, C=3.0, solver='liblinear')\nxgbc= xgb.XGBClassifier(max_depth=2, n_estimators=2, eval_metric='logloss', subsample=0.5)\nxgbc2 = xgb.XGBClassifier(max_depth=4, n_estimators=8, eval_metric='logloss', subsample=0.5, min_child_weight=300)\n\nlr.fit(X,y)\nxgbc.fit(X,y)\nxgbc2.fit(X,y)\n\nlr_preds= lr.predict(X_eval)\nxgb_pred = xgbc.predict(X_eval)\nxgb2_pred = xgbc2.predict(X_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nresults = confusion_matrix(y_eval, xgb_pred) \nprint('XGBoost Results')\nprint('Confusion Matrix :')\nprint(results) \nprint('Accuracy Score :',accuracy_score(y_eval, xgb_pred) )\nprint('Report: ')\nprint(classification_report(y_eval, xgb_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nresults = confusion_matrix(y_eval, xgb2_pred) \nprint('XGBoost Results')\nprint('Confusion Matrix :')\nprint(results) \nprint('Accuracy Score :',accuracy_score(y_eval, xgb2_pred) )\nprint('Report: ')\nprint(classification_report(y_eval, xgb2_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nresults = confusion_matrix(y_eval, lr_preds) \nprint('Logistic Regression Results')\nprint('Confusion Matrix :')\nprint(results) \nprint('Accuracy Score :',accuracy_score(y_eval, lr_preds) )\nprint('Report: ')\nprint(classification_report(y_eval, lr_preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### building an hourly classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_df = pd.DataFrame()\neval_df['LogReg'] = lr_preds\neval_df['XGBoost'] = xgb_pred\neval_df['XGBoost2'] = xgb2_pred\neval_df.index = y_eval.index\neval_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hour_counts = eval_df.groupby([eval_df.index.date, eval_df.index.hour]).sum() / 180\nhour_counts['Actual'] = y_eval.groupby([y_eval.index.date, y_eval.index.hour]).mean()\nhour_counts.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"Actual\", y=\"LogReg\", data=hour_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, thresholds = roc_curve(np.array(hour_counts[\"Actual\"]), np.array(hour_counts[\"LogReg\"]))\nroc_auc = roc_auc_score(np.array(hour_counts[\"Actual\"]), np.array(hour_counts[\"LogReg\"]))\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0,1], [0,1], linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"Actual\", y=\"XGBoost\", data=hour_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, thresholds = roc_curve(np.array(hour_counts[\"Actual\"]), np.array(hour_counts[\"XGBoost\"]))\nroc_auc = roc_auc_score(np.array(hour_counts[\"Actual\"]), np.array(hour_counts[\"XGBoost\"]))\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0,1], [0,1], linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x=\"Actual\", y=\"XGBoost2\", data=hour_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, thresholds = roc_curve(np.array(hour_counts[\"Actual\"]), np.array(hour_counts[\"XGBoost2\"]))\nroc_auc = roc_auc_score(np.array(hour_counts[\"Actual\"]), np.array(hour_counts[\"XGBoost2\"]))\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0,1], [0,1], linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_eval_preds = (hour_counts['LogReg'] + hour_counts['XGBoost'] + hour_counts['XGBoost2']) / 3\nsns.boxplot(x=hour_counts[\"Actual\"], y=avg_eval_preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nfpr, tpr, thresholds = roc_curve(np.array(hour_counts[\"Actual\"]), np.array(avg_eval_preds))\nroc_auc = roc_auc_score(np.array(hour_counts[\"Actual\"]), np.array(avg_eval_preds))\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0,1], [0,1], linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cutoff_list = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\nacc = []\ntruezeros = []\nfalsezeros = []\ntrueones = []\nfalseones = []\nimp_precision = []\nimp_recall = []\n\nfor cutoff in cutoff_list:\n    ddf = pd.DataFrame(index=avg_eval_preds.index)\n    ddf['class'] = 0\n    ddf['class'][avg_eval_preds > cutoff] = 1\n    \n    cm = confusion_matrix(hour_counts[\"Actual\"], ddf['class']) \n    acc.append((cm[1,1]+cm[0,0]) / len(ddf))\n    truezeros.append(cm[0,0])\n    falsezeros.append(cm[1,0])\n    trueones.append(cm[1,1])\n    falseones.append(cm[0,1])\n    imp_precision.append((cm[1,1]) / (cm[0,1] + cm[1,1]))\n    imp_recall.append((cm[1,1]) / (cm[1,0] + cm[1,1]))\n    #print(C,intercept)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cutoffs = pd.DataFrame()\ncutoffs['Cutoff'] = cutoff_list\ncutoffs['Accuracy'] = acc\ncutoffs['True Zeros'] = truezeros\ncutoffs['False Zeros'] = falsezeros\ncutoffs['True Ones'] = trueones\ncutoffs['False Ones'] = falseones\ncutoffs['Impure Precision'] = imp_precision\ncutoffs['Impure Recall'] = imp_recall\ncutoffs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#for i in range(len(cutoff_list)):\n#    ddf = pd.DataFrame(index=avg_eval_preds.index)\n#    ddf['class'] = 0\n#    ddf['class'][avg_eval_preds > cutoff_list[i]] = 1\n#    \n#    print(cutoff_list[i])\n#    plot_confusion_matrix(np.array(hour_counts[\"Actual\"]), np.array(ddf['class']), classes=np.array(['pure (below cutoff)', 'impure (above cutoff)']), normalize=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ddf = pd.DataFrame(index=hour_counts['LogReg'].index)\nddf['class'] = 0\nddf['class'][hour_counts['LogReg'] > 0.45] = 1\n\nplot_confusion_matrix(np.array(hour_counts[\"Actual\"]), np.array(ddf['class']), classes=np.array(['pure (below cutoff)', 'impure (above cutoff)']), normalize=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Apply method to Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import random #\n\n#random.seed(69)\n\n#mdf = df.drop(columns = ['datetime', '% Iron Concentrate', '% Silica Concentrate'])\n\n#train = mdf.iloc[:-14*24*180]\n#zero_idx = train[train['label'] == 0].index\n#sample_idx = random.sample(list(zero_idx), train[train['label'] == 1].shape[0])\n#sample_idx.extend(list(train[train['label'] == 1].index))\n#sample_idx = pd.DatetimeIndex(sample_idx).sort_values()\n#train = train.reindex(sample_idx)\n\n#X = train.iloc[:,:-1]\n#y = train.iloc[:,-1]\n\n#X_eval = mdf.iloc[-42*24*180:-14*24*180,:-1]\n#y_eval = mdf.iloc[-42*24*180:-14*24*180,-1]\n#X_test = mdf.iloc[-14*24*180:,:-1]\n#y_test = mdf.iloc[-14*24*180:,-1]\n\n#print(y[y == 0].count() / y[y==1].count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lr = LogisticRegression(fit_intercept=True, C=3.0, solver='liblinear')\n#xgbc= xgb.XGBClassifier(max_depth=2, n_estimators=2, eval_metric='logloss', subsample=0.5)\n#xgbc2 = xgb.XGBClassifier(max_depth=4, n_estimators=8, eval_metric='logloss', subsample=0.5, min_child_weight=300)\n\n#lr.fit(X,y)\n#xgbc.fit(X,y)\n#xgbc2.fit(X,y)\n\n#lr_preds= lr.predict(X_eval)\n#xgb_pred = xgbc.predict(X_eval)\n#xgb2_pred = xgbc2.predict(X_eval)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#eval_df = pd.DataFrame()\n#eval_df['LogReg'] = lr_preds\n#eval_df['XGBoost'] = xgb_pred\n#eval_df['XGBoost2'] = xgb2_pred\n#eval_df.index = X_test.index\n#hour_counts = eval_df.groupby([eval_df.index.date, eval_df.index.hour]).sum() / 180\n#hour_counts['Actual'] = y_test.groupby([y_test.index.date, y_test.index.hour]).mean()\n#hour_counts['Average'] = (hour_counts['LogReg'] + hour_counts['XGBoost'] + hour_counts['XGBoost2']) / 3\n#hour_counts['Prediction'] = 0\n#hour_counts['Prediction'][hour_counts['Average'] > 0.45] = 1\n#plot_confusion_matrix(np.array(hour_counts[\"Actual\"]), np.array(hour_counts[\"Prediction\"]), classes=np.array(['pure (below cutoff)', 'impure (above cutoff)']), normalize=False)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n#results = confusion_matrix(hour_counts[\"Actual\"], hour_counts[\"Prediction\"]) \n#print('Logistic Regression Results')\n#print('Confusion Matrix :')\n#print(results) \n#print('Accuracy Score :',accuracy_score(hour_counts[\"Actual\"], hour_counts[\"Prediction\"]) )\n#print('Report: ')\n#print(classification_report(hour_counts[\"Actual\"], hour_counts[\"Prediction\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}