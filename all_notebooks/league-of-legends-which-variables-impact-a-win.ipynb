{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Analysing which variables contribute the most for the Blue Team to Win."},{"metadata":{},"cell_type":"markdown","source":"## Importing Libraries & importing the csv data file"},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing and defining Data Frame\ndf = pd.read_csv(\"../input/league-of-legends-diamond-ranked-games-10-min/high_diamond_ranked_10min.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking the Data Frame info and first look of the data/descriptive statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"# cheking info of the df\ndef data_inv(df):\n    print('dataframe: ',df.shape[0])\n    print('dataset variables: ',df.shape[1])\n    print('-'*10)\n    print('dateset columns: \\n')\n    print(df.columns)\n    print('-'*10)\n    print('data-type of each column: \\n')\n    print(df.dtypes)\n    print('-'*10)\n    print('missing rows in each column: \\n')\n    c=df.isnull().sum()\n    print(c[c>0])\ndata_inv(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the df:\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a copy/checkpoint before deleting unnecessary columns\ndf_1 = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cleaning the Data Frame"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping columns\ndf_1 = df_1.drop(['blueGoldDiff', 'blueExperienceDiff','redGoldDiff',\n       'redExperienceDiff','gameId'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check number of diferent values\ndf_1.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the probability of blue team winning is inversely correlated with red team, so the task is to analyse the Blue Team.\ndf_blue = df_1.drop(['redWardsPlaced','redWardsDestroyed',\n       'redFirstBlood', 'redKills', 'redDeaths', 'redAssists',\n       'redEliteMonsters', 'redDragons', 'redHeralds', 'redTowersDestroyed',\n       'redTotalGold', 'redAvgLevel', 'redTotalExperience',\n       'redTotalMinionsKilled', 'redTotalJungleMinionsKilled', 'redCSPerMin', 'redGoldPerMin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting some graphs to better visualize the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = df_blue['blueWins']\ny = df_blue['blueTotalGold']\nplt.bar(x, y)\nplt.xticks(range(0,2))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total Gold vs Total minions killed, as we can see more minions killed doesn't equate more gold \nx1 = df_blue['blueTotalMinionsKilled']\ny1 = df_blue['blueTotalGold']\nplt.scatter(x1, y1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation between the variables,\n# To avoid Multicollinearity, the independent variables must not be over 0,7 correlation or the regression output will \n        # be erroneous, for example: Blue Kills is highly correlated with Blue Assists, and one must be omitted from the model\ncorr = df_blue.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,10))\nsns.heatmap(corr, annot =  True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Standardizing the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating an input table with only the independent variables, ommiting the correlating ones,\n# creating the target variable = Blue Wins\ndf_blue.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unscaled_inputs = df_blue.filter(['blueWardsPlaced', 'blueWardsDestroyed', 'blueFirstBlood',\n       'blueKills', 'blueDeaths','blueEliteMonsters','blueHeralds', 'blueTowersDestroyed','blueAvgLevel','blueTotalMinionsKilled', 'blueTotalJungleMinionsKilled'], axis=1)\ntarget = df_blue.filter(['blueWins'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Scale just the non categorical variables, in this case 'Blue first Blood' is categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the libraries needed to create the Custom Scaler\n# note that all of them are a part of the sklearn package\n# moreover, one of them is actually the StandardScaler module, \n# so you can imagine that the Custom Scaler is build on it\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\n\n# create the Custom Scaler class\n\nclass CustomScaler(BaseEstimator,TransformerMixin): \n    \n    # init or what information we need to declare a CustomScaler object\n    # and what is calculated/declared as we do\n    \n    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n        \n        # scaler is nothing but a Standard Scaler object\n        self.scaler = StandardScaler(copy,with_mean,with_std)\n        # with some columns 'twist'\n        self.columns = columns\n        self.mean_ = None\n        self.var_ = None\n        \n    \n    # the fit method, which, again based on StandardScale\n    \n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.var_ = np.var(X[self.columns])\n        return self\n    \n    # the transform method which does the actual scaling\n\n    def transform(self, X, y=None, copy=None):\n        \n        # record the initial order of the columns\n        init_col_order = X.columns\n        \n        # scale all features that you chose when creating the instance of the class\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n        \n        # declare a variable containing all information that was not scaled\n        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n        \n        # return a data frame which contains all scaled features and all 'not scaled' features\n        # use the original order (that you recorded in the beginning)\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical columns to omit\ncolumns_to_omit = ['blueFirstBlood']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the columns to scale, based on the columns to omit\n# use list comprehension to iterate over the list\ncolumns_to_scale = [x for x in unscaled_inputs.columns.values if x not in columns_to_omit]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blue_scaler = CustomScaler(columns_to_scale)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blue_scaler.fit(unscaled_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_inputs = blue_scaler.transform(unscaled_inputs)\nscaled_inputs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test, Train and Split the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_split(scaled_inputs, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(scaled_inputs, target, train_size=0.8, random_state=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LogisticRegression()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regression score\nreg.score(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Intercept\nintercept = reg.intercept_\nintercept","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a Summary Table to visualize the Variable and respective Coefficients and Odds Ratio\nvariables = unscaled_inputs.columns.values\nvariables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table = pd.DataFrame(columns=['Variables'], data = variables)\nsummary_table['Coef'] = np.transpose(reg.coef_)\n# add the intercept at index 0\nsummary_table.index = summary_table.index + 1\nsummary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n# calculate the Odds Ratio and add to the table\nsummary_table['Odds Ratio'] = np.exp(summary_table.Coef)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_table.sort_values(by=['Odds Ratio'], ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculating P-values"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nx = sm.add_constant(x_train)\nlogit_model=sm.Logit(y_train,x)\nresult=logit_model.fit()\nprint(result.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusions"},{"metadata":{},"cell_type":"markdown","source":"1- The Variables \"Blue Wards Placed\", \"Blue Wards Destroyed\"have no statistical significance, so the next step is to remove them from the model\n\n2- The Variable that has the biggest impact in the odds of winning is the number of Kills for the Blue Team, for every Kill the odds increase by 132%.\n    \n3- The Second variable that impacts the outcome is, as expected, the number of Deaths, this time it impacts negatively:for every Death, the odds of winning decreases by 50% for the Blue team\n    \n4- Surprizingly, killing Heralds decreases the odds of winning by about 12%\n\n5- Killing Elite monsters and Total minions killed are the next biggest impact in winning with 30% and 20% increase, respectively"},{"metadata":{},"cell_type":"markdown","source":"## Testing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing the data is important to evalute the accuracy on a dataset that the model has never seen, to see if it's Overfitting\n  # a test score 10% below the training reveals an overfitting\nreg.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted_prob = reg.predict_proba(x_test)\npredicted_prob[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_blue['predicted'] = reg.predict_proba(scaled_inputs)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_blue","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}