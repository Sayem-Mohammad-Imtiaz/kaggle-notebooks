{"cells":[{"metadata":{},"cell_type":"markdown","source":"> Hi! I'm Mauricio Ruanova. My friends call me Mau. I am a Data Science and Machine Learning Engineer. \n\nThis notebook contains an heuristic prediction for the weather in Seattle. \n\nUsing a dataset that contains the complete records of daily rainfall patterns from January 1st, 1948 to December 12, 2017. \n\nMaybe if it rained Yesterday and it is raining Today then it is likely to raing Tomorrow.\n\nBut how much can we predict using the numbers provided?"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.options.mode.chained_assignment = None  # default='warn'\nfrom sklearn.model_selection import train_test_split \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/did-it-rain-in-seattle-19482017/seattleWeather_1948-2017.csv')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numrows = df.shape[0] - 2 # 25551-2 = 25549\nnumrows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create an empty dataframe\nheuristic_df = pd.DataFrame({'yesterday':[0.0]*numrows,\n                             'today':[0.0]*numrows,\n                             'tomorrow':[0.0]*numrows,\n                             'guess':[False]*numrows, #logical guess\n                             'rain_tomorrow':[False]*numrows, #historical observation\n                             'correct':[False]*numrows, #TRUE if your guess matches the historical observation\n                             'true_positive':[False]*numrows, #TRUE If you said it would rain and it did\n                             'false_positive':[False]*numrows,#TRUE If you sait id would rain and it didn't\n                             'true_negative':[False]*numrows, #TRUE if you said it wouldn't rain and it didn't\n                             'false_negative':[False]*numrows}) #TRUE if you said it wouldn't raing and it did\nheuristic_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"heuristic_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build a loop to add your heuristic model guesses as a column to this dataframe\nhere is a loop that populates the dataframe created earlier with the total perciptation from yesterday and today.\n\nThen the guess is set to true if rained both yesterday and today "},{"metadata":{"trusted":true},"cell_type":"code","source":"for z in range(numrows):\n    # start at time 2 in the data frame\n    i = z + 2\n    # pull values from the dataframe\n    yesterday = df.iloc[(i-2),1]\n    today = df.iloc[(i-1),1]\n    tomorrow = df.iloc[i,1]\n    rain_tomorrow = df.iloc[(i),1]\n    heuristic_df.iat[z,0] = yesterday\n    heuristic_df.iat[z,1] = today\n    heuristic_df.iat[z,2] = tomorrow\n    heuristic_df.iat[z,3] = False # set guess default to False\n    heuristic_df.iat[z,4] = rain_tomorrow\n    # example hueristic : if today > 0.0 and yesterday > 0.0:\n    if yesterday >= 0.9 or today >= 0.05: # 0.707073 # my own heuristic based on personal experience\n        heuristic_df.iat[z,3] = True\n    if heuristic_df.iat[z,3] == heuristic_df.iat[z,4]:\n        heuristic_df.iat[z,5] = True\n        if heuristic_df.iat[z,3] == True:\n            heuristic_df.iat[z,6] = True #true positive\n        else:\n            heuristic_df.iat[z,8] = True #true negative\n    else:\n        heuristic_df.iat[z,5] = False\n        if heuristic_df.iat[z,3] == True:\n            heuristic_df.iat[z,7] = True #false positive\n        else:\n            heuristic_df.iat[z,9] = True #false negative\nheuristic_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1 = heuristic_df[['yesterday']]\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = heuristic_df[['today']]\ndata2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3 = heuristic_df[['tomorrow']]\ndata3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = heuristic_df.dropna()\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pd.Series(np.where(X['tomorrow'].dropna() > 0, 1, 0)) # integer 0 or 1?\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prevent overfitting with split train test\n\nBreak the dataset into two parts, training and testing. \n\nUse the first 80% of the dataset for training and the last 20% for testing. \n\nEvaluate both sets of data using your function. \n\nWhat difference do you see in the calculated values (Precision and Recall)?\n\n- Separate a dataset into training and testing subsets\n- Calculate Precision and Recall for training and test sets\n- Calculate SSE for both training and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) #train = 80% / test = 20%\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head() # the first 80% of the dataset for training","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomForestClassifier\n\nFit the model.\n\nPrediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\n# Fitting train data into model\nclf.fit(X_train, y_train)\n# Prediction\ny_pred = clf.predict(X_test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importances\nAs expected, the plot suggests that 3 features are informative, while the remaining are not."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.feature_importances_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = clf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf.estimators_],\n             axis=0)\nindices = np.argsort(importances)\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.title(\"Feature Importances\")\nplt.barh(range(X.shape[1]), importances[indices], color=\"r\", xerr=std[indices], align=\"center\")\nplt.yticks(range(X.shape[1]), indices)\nplt.ylim([-1, X.shape[1]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix\n\nThe four concepts of true/false negative/positive are measures of your guesses:\n\nThese values then serve as a core measure of performance.\n\nThis information can also be organized using a confusion matrix:\n\n| Confusion Matrix | Predicted Positives | Predicted Negatives |\n| ---------------- | ------------------- | ------------------- |\n| Positives        | True Positives      | False Positives     |\n| Negatives        | False Negatives     | True Negatives      |\n\n### Precision\nThe percent of the time you predict positive that you are correct.\n### Recall\nThe percentage of positive guesses you got correct that you should have gotten correct."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification report\nBuild a text report showing the main classification metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(metrics.classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Accuracy classification score\nIn multilabel classification, this function computes subset accuracy: \n\nthe set of labels predicted for a sample must exactly match the corresponding set of labels in y_true."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('accuracy score: ', accuracy_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### True Positives"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_positives = X_test['true_positive'].value_counts()\ntrue_positives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# type(true_positives) # pandas.core.series.Series","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# true_positives.array # <PandasArray> [3914, 1195] Length: 2, dtype: int64","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### True Negatives"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_negatives = X_test['true_negative'].value_counts()\ntrue_negatives","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The accuracy of your predicitions\nWe used this simple approach in the first part to see what percent of the time we were correct \n\ncalculated as (true positive + true negative) / number of guesses"},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = (true_positives.array[1] + true_negatives.array[1]) / X_train.shape[0] # number of guesses\nprint('accuracy: ', accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The precision of your predicitions\nPrecision is the percent of your postive prediction which are correct\n\nmore specifically it is calculated (num true positive)/(num true positive + num false positive)"},{"metadata":{"trusted":true},"cell_type":"code","source":"precision = true_positives.array[1] / (true_positives.array[1]+true_positives.array[0])\nprint('precision: ', precision)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The recall of your predicitions\nRecall the percent of the time you are correct when you predict positive\n\nmore specifically it is calculated (num true positive)/(num true positive + num false negative)"},{"metadata":{"trusted":true},"cell_type":"code","source":"recall = true_negatives.array[1] / (true_positives.array[1]+true_positives.array[0])\nprint('recall: ', recall)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sum of Squared Error (SSE) Cost of your prediction\nAdding up the difference in your prediction and the actual value after you have squared each individual difference.\n\nhttps://www.wikihow.com/Calculate-the-Sum-of-Squares-for-Error-(SSE)"},{"metadata":{"trusted":true},"cell_type":"code","source":"### The sum of squared error (SSE) of your predictions\nmean = X_test.mean().array[2] # tomorrow\nprint('mean: ', mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['deviation'] = X_test['tomorrow'] - mean\nX_test['deviation'] = X_test['tomorrow'] - mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test['squared'] = X_test['deviation']**2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}