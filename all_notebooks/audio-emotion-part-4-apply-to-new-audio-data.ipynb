{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center>Audio Emotion Recognition</center>\n## <center>Part 4 - Apply to new audio data</center>\n#### <center> 31st August 2019 </center> \n#### <center> 19th August 2019, Eu Jin Lok </center> \n#### <center> 24th March 2021, Yuan-Fu Liao </center> "},{"metadata":{},"cell_type":"markdown","source":"## Introduction \nIn [part 3](https://www.kaggle.com/yfliao/audio-emotion-part-3-baseline-model) we built a simple baseline model and we serialise it. We also have an early indication of the accuracy to be expected across the 3 models namely \n\n- Gender at __81%__ absolute accuracy<br/>\n- Emotion at __50%__ absolute accuracy<br/>\n- Gender and Emotion at __43%__ absolute accuracy <br/>\n\nSo whilst it's all well and good in our little sandbox experiment here, question remains how well will it generalise across new unseen data? The accuracy measurement that we recorded above is based on the same dataset. Whilst we ensured the accuracy measure is based on a slice of the data that the model hasn't seen before, it is still made from the data source that will include the same speaker voice, the same audio quality, audio background etc. The model we trained could very well have picked up speakers unique characteristics rather than the audio features (data leakage). So how will this model perform once we apply it to a completely new dataset with different audio quality, speaker and background noises? It is here which we will run a quick test where I will record a new audio file and run it through the model  \n\n\n1. [Audio Recording](#audio)\n2. [Data preparation and prediction](#data)\n3. [Final thoughts](#final)\n\nUpvote this notebook if you like, and be sure to check out the other parts which are now available:\n* [Part 1 | Explore data](https://www.kaggle.com/yfliao/audio-emotion-part-1-explore-data)\n* [Part 2 | Feature Extract](https://www.kaggle.com/yfliao/audio-emotion-part-2-feature-extract)\n* [Part 3 | Baseline model](https://www.kaggle.com/yfliao/audio-emotion-part-3-baseline-model)\n* [Part 4 | Apply to new audio data](https://www.kaggle.com/yfliao/audio-emotion-part-4-apply-to-new-audio-data)\n* [Part 5 | Data augmentation](https://www.kaggle.com/yfliao/audio-emotion-part-5-data-augmentation)\n* [Part 6 | 2D CNN Implementation](https://www.kaggle.com/yfliao/audio-emotion-part-6-2d-cnn-66-accuracy)\n    \nMost importantly, I want to thank the 4 authors for their excellent dataset, without it, writing this notebook could not have been possible. The original source of the dataset links are below:\n\n- [TESS](https://tspace.library.utoronto.ca/handle/1807/24487)\n- [CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D)\n- [SAVEE](http://kahlan.eps.surrey.ac.uk/savee/Database.html)\n- [RAVDESS](https://zenodo.org/record/1188976#.XYP8CSgzaUk)\n- [RAVDESS_Kaggle](https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing required libraries \nfrom keras.models import Sequential, Model, model_from_json\nimport matplotlib.pyplot as plt\nimport keras \nimport pickle\nimport wave  # !pip install wave\nimport os\nimport pandas as pd\nimport numpy as np\nimport sys\nimport warnings\nimport librosa\nimport librosa.display\nimport IPython.display as ipd  # To play sound in the notebook\n\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"audio\"></a>\n## 1. Audio recording \nI've used python itself to run the audio recorder and recorded a track using my wife's voice. However to record the audio, you need a microphone and is not possible to do this on Kaggle itself. So I've had to prerecord the audio and host it on this notebook. But I've provided the code below to run the recording, it opens up an audio channel for 4 seconds of recording before saving it as a WAV file. Remember your computer needs to be connected to a microphone audio source for it to work.\n\n```python\nCHUNK = 1024 \nFORMAT = pyaudio.paInt16 \nCHANNELS = 2 \nRATE = 44100 \nRECORD_SECONDS = 4\nWAVE_OUTPUT_FILENAME = \"test audio\\\\testing.wav\"\n\np = pyaudio.PyAudio()\n\nstream = p.open(format=FORMAT,\n                channels=CHANNELS,\n                rate=RATE,\n                input=True,\n                frames_per_buffer=CHUNK) #buffer\n\nprint(\"* recording\")\n\nframes = []\n\nfor i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n    data = stream.read(CHUNK)\n    frames.append(data) # 2 bytes(16 bits) per channel\n\nprint(\"* done recording\")\n\nstream.stop_stream()\nstream.close()\np.terminate()\n\nwf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\nwf.setnchannels(CHANNELS)\nwf.setsampwidth(p.get_sample_size(FORMAT))\nwf.setframerate(RATE)\nwf.writeframes(b''.join(frames))\nwf.close()\n```"},{"metadata":{},"cell_type":"markdown","source":"--------------------------------\nSo the this audio file below is a pre-recorded audio file which we will use to test the model. I've recorded this using the pyaudio code above, and uploaded it here on this notebook. You can download it to reproduce the results. Lets play the file and see how the wave plot looks like..."},{"metadata":{"trusted":true},"cell_type":"code","source":"#data, sampling_rate = librosa.load('/kaggle/input/happy-audio/Liza-happy-v3.wav')\n#ipd.Audio('/kaggle/input/happy-audio/Liza-happy-v3.wav')\ndata, sampling_rate = librosa.load('/kaggle/input/speech-emotion-recognition-with-cnn/results/Liza-happy-v3.wav')\nipd.Audio('/kaggle/input/speech-emotion-recognition-with-cnn/results/Liza-happy-v3.wav')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"data\"></a>\n## 2. Data preparation and prediction\nSo what we're aiming for is __female_happy__ prediction as the correct answer. Now that we have the audio file ready, we'll need to convert it to the correct data format for our model, and we'll need the serialised models for the weights and the model architecture that we performed in [part 3](https://www.kaggle.com/ejlok1/audio-emotion-part-3-baseline-model). I'm not going to re-run the entire code just to get the files, so what I've done is downloaded the 'model_json.json' and 'Emotion_Model.h5' files and loaded it into this notebook. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading json and model architecture \n#json_file = open('/kaggle/input/saved-model/model_json.json', 'r')\njson_file = open('/kaggle/input/speech-emotion-recognition-with-cnn/results/model_json.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\n#loaded_model.load_weights(\"/kaggle/input/saved-model/Emotion_Model.h5\")\nloaded_model.load_weights(\"/kaggle/input/speech-emotion-recognition-with-cnn/results/saved_models/Emotion_Model.h5\")\nprint(\"Loaded model from disk\")\n\n# the optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And next step transform the audio data..."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets transform the dataset so we can apply the predictions\n\n#X, sample_rate = librosa.load('/kaggle/input/happy-audio/Liza-happy-v3.wav'\n#                              ,res_type='kaiser_fast'\n#                              ,duration=2.5\n#                              ,sr=44100\n#                              ,offset=0.5\n#                             )\n\nX, sample_rate = librosa.load('/kaggle/input/speech-emotion-recognition-with-cnn/results/Liza-happy-v3.wav'\n                              ,res_type='kaiser_fast'\n                              ,duration=2.5\n                              ,sr=44100\n                              ,offset=0.5\n                             )\n\nsample_rate = np.array(sample_rate)\nmfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\nnewdf = pd.DataFrame(data=mfccs).T\nnewdf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And finally run the predictions over it using our saved baseline model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply predictions\nnewdf= np.expand_dims(newdf, axis=2)\nnewpred = loaded_model.predict(newdf, \n                         batch_size=16, \n                         verbose=1)\n\nnewpred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"--------------------\nThe prediction is in the form of numbers, we'll need to append the labels to it before we can make sense as to what it has predicted... The labels code can be found again, in part 3, but instead of re-running it, i've just uploaded the file into this notebook. Its simple a mapping file between the labels and the ID pairs."},{"metadata":{"trusted":true},"cell_type":"code","source":"#filename = '/kaggle/input/labels/labels'\nfilename = '/kaggle/input/speech-emotion-recognition-with-cnn/results/labels'\ninfile = open(filename,'rb')\nlb = pickle.load(infile)\ninfile.close()\n\n# Get the final predicted label\nfinal = newpred.argmax(axis=1)\nfinal = final.astype(int).flatten()\nfinal = (lb.inverse_transform((final)))\nprint(final) #emo(final) #gender(final) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"final\"></a>\n## 3. Final thoughts\nSo our model predicted __female surprise__, which on hindsight, going back listening to the audio again, I would actually agree with the prediction! Even thou I gave specific instructions to my wife to record a happy audio, I suppose the emotion exhibited is very similar to suprise. This is a very pleasant surprise indeed.  \n\nIn our next section we will be looking to enhance the model!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}