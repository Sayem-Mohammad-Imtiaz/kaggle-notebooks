{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndata1=pd.read_csv('../input/train_LZdllcl.csv')\ndata3=pd.read_csv('../input/train_LZdllcl.csv')\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1[['department', 'region', 'education', 'gender',\n       'recruitment_channel']].head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col=['department','region','education','gender','recruiment_channel']\ncols=['department', 'region', 'education', 'gender',\n       'recruitment_channel', 'no_of_trainings', 'age', 'previous_year_rating',\n       'length_of_service', 'KPIs_met >80%', 'awards_won?',\n       'avg_training_score']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict_department=dict((v,k) for k,v in (dict(enumerate(list(data3.department.unique())))).items())\ndict_region=dict((v,k) for k,v in (dict(enumerate(list(data3.region.unique())))).items())\ndict_recr=dict((v,k) for k,v in (dict(enumerate(list(data3.recruitment_channel.unique())))).items())\ndict_edu=dict((v,k) for k,v in (dict(enumerate(list(data3.education.unique())))).items())\ndict_gen=dict((v,k) for k,v in (dict(enumerate(list(data3.gender.unique())))).items())\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data3['department']=data3['department'].map(dict_department)\ndata3['region']=data3['region'].map(dict_region)\ndata3['recruitment_channel']=data3['recruitment_channel'].map(dict_recr)\ndata3['education']=data3['education'].map(dict_edu)\ndata3['gender']=data3['gender'].map(dict_gen)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,matthews_corrcoef\nx_train,x_test,y_train,y_test=train_test_split(data3[cols], data3['is_promoted'], test_size=0.20, random_state=123)\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nclf=XGBClassifier().fit(x_train[cols],y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (classification_report(y_test,clf.predict(x_test[cols])))\nprint(matthews_corrcoef(y_test,clf.predict(x_test[cols])))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data1.dtypes\n#There are continuous variable and categorical variable. \n#We will first start with numerical variable.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are broadly two variable categories-\n - Categorical Varaibles\n - Numerical/Continuous Variables"},{"metadata":{},"cell_type":"markdown","source":"There are two major classes of categorical data- Nominal and Ordinal\n\n**Nominal**- in this there is no concept of order. e.g. racial types- Asian, American, Europians, etc. There is no order here.\n**Ordinal**- we have some sense order amongst the values. e.g. Shoe sizes S, M, L, XL, XXL"},{"metadata":{},"cell_type":"markdown","source":"**#Handling Ordinal Variables**- Few popular methods\n1. Creation of Dummies\n2. Mean Encoding\n\n**Dummy creation**- In this we create dummies for the all the unique value that the variable can take. Let's say variable can take m unique values, then we create m-1 dummies.\n"},{"metadata":{},"cell_type":"markdown","source":"Let's take column **department**. There are in total 9 unique values. we will create in total 8 dummies. But the problem with this approach if number of unique values that the variable can take is high (high cardinality). It can increases the number columns drastically."},{"metadata":{"trusted":true},"cell_type":"code","source":"print (len(data1.department.unique()))\ndum_department=pd.get_dummies(data1['department'], prefix='department', drop_first=False)\ndum_department.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Mean Encoding**- proportion of positive labels present for a particular value of a categorical variable. Problem with this methodology is overfitting. mean encoding is one of the key transformation applied to categorical variables when we are using Gradient Boosting. Since, in Gradient Boosting tree height is monitored and always restricted to low tree height. As exposing tree to higher tree height leads to overfitting."},{"metadata":{"trusted":true},"cell_type":"code","source":"data1[['department','is_promoted']].head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dpt=pd.DataFrame(data1.department.value_counts())\ndpt=dpt.reset_index()\ndpt.columns=['department','count']\ntarget_label=data1[['department','is_promoted']].groupby(['department']).sum()\ntarget_label=target_label.reset_index()\nfinal_encoded=pd.merge(dpt,target_label,on='department',how='left')\nfinal_encoded['mean_encoded']=final_encoded['is_promoted']/final_encoded['count']\nfinal_encoded=final_encoded[['department','mean_encoded']]\nfinal_encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1=pd.merge(data1,final_encoded,on='department',how='left')\ndata1[['department','mean_encoded']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As most of the machine learning algorithms recognise only numbers. Therefore, all non-numeric ordinal variables needs to be transformed into numeric. This can be achieved using a dictionary using Python's map function."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Before Starting with any modelling, one must first check problem context and try to get as much information possible. For the given dataset we are provided with average training score. But, before we make any comments on this feature's importance we should take a step back and think. How a person would be promoted in a multistate and multi department company-\n* Promotion would happen department wise\n* Promotion would be region wise\n* No. of promotion would be dependent on a particular department. Some departments would be inherently promoting more number of people than other\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data1[data1['is_promoted']==1]['avg_training_score'], color='r')\nsns.distplot(data1[data1['is_promoted']==0]['avg_training_score'], color='g')\n#Observation- Idea behind doing dist plot, plotting separately for categories to identify regions where there are no overlaps. If we can find pockets of\n#non-overlap, then the variable can clearly differenitate or classify the target and would add value to the model.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.preprocessing import CategoricalEncoder\ndata1=pd.merge(data1,data1[['region','department','avg_training_score']].groupby(['region','department']).mean(),how='left',on=['region','department'])\ndata1=data1.rename(columns={'avg_training_score_x':'avg_training_score','avg_training_score_y':'mean_reg_dpt'})\ndata1['new_avg_trng_score']=data1['avg_training_score']/data1['mean_reg_dpt']\ndata1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(16,7))\nsns.distplot(data1[data1.is_promoted==0]['new_avg_trng_score'],color='g',label='Not Promoted')\nsns.distplot(data1[data1.is_promoted==1]['new_avg_trng_score'],color='r',label='Promoted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#as we can see now the engineered feature would be able to classify more accurately, \n#as the regions of non-overlap are more clearly defined"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (data1.age.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data1[data1['is_promoted']==1]['age'], color='r')\nsns.distplot(data1[data1['is_promoted']==0]['age'], color='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Binning Strategies and handling numeric variables**\n\nBinning is of two types- **Fixed** and **Adaptive**\n\n**Fixed Bining** as the name suggest is fixed- boundries are predefined, which may lead to imperfect bins with with less irregular density in few bins\n\n**Adaptive Binning**- Quantile based binning is a good strategy to use for adaptive binning. Quantiles are specific values or cut-points which help in partitioning the continuous valued distribution of a specific numeric field into  discrete contiguous bins or intervals. Thus, q-Quantiles help in partitioning a numeric attribute into q equal partitions. Popular examples of quantiles include the 2-Quantile known as the median which divides the data distribution into two equal bins, 4-Quantiles known as the quartiles which divide the data into 4 equal bins and 10-Quantiles also known as the deciles which create 10 equal width bins. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data1['age_bin']=pd.qcut(data1['age'], q=[0,.10,.20,.30,.40,.50,.60,.70,.80,.90,1], labels=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data1[data1['is_promoted']==1]['age_bin'], color='r')\nsns.distplot(data1[data1['is_promoted']==0]['age_bin'], color='g')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#few things to take care while handling numerical variables. \n* Clipping values after certain threshold- Outlier clipping- It's a very simple exercise. One would look at quantile plot. And decides values after which  variables is not adding more value. e.g. no. of likes seen in light of product getting picked by a customer. Likes can be bucketed into certain quantiles based on the data.\n* Binning"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(data1[data1['is_promoted']==1]['length_of_service'], color='r')\nsns.distplot(data1[data1['is_promoted']==0]['length_of_service'], color='g')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the plot it can be said that age bins is not adding value. But our modelling is not bivariate modelling.So, it may happen that age might become important once it is seen with other variables.\nFrom Domain standspoint- Age should be one of the factor which should be important while promoting people. Individually it doesn't make sense to promote people based on age, but with other varaible, it may be useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.columns\ncols=['gender','no_of_trainings', 'age',\n       'length_of_service', 'KPIs_met >80%', 'awards_won?',\n       'avg_training_score']\ncols_a=['gender','no_of_trainings', 'age',\n       'length_of_service', 'KPIs_met >80%', 'awards_won?', 'mean_reg_dpt','new_avg_trng_score']\ncategory_cols = ['gender','recruitment_channel', 'region', 'department']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.gender.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2=data1\ndata2['gender']=data2['gender'].map({'m':1,'f':0})\n#data2['recruitment_channel']=data2['recruitment_channel'].map({'sourcing':1,'other':0,'referred':2})\ndum_recr=pd.get_dummies(data2['recruitment_channel'], prefix_sep='recr', drop_first=True)\n#data2['department']=data2['department'].map({'Sales & Marketing':0, 'Operations':1, 'Technology':2, 'Analytics':3,\n#       'R&D':4, 'Procurement':5, 'Finance':6, 'HR':7, 'Legal':8})\ndum_dpt=pd.get_dummies(data2['department'], prefix_sep='dpt', drop_first=True)\ndata2=pd.concat([dum_dpt, dum_recr,data2], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols=cols+list(dum_recr.columns)+list(dum_dpt.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(data2[cols], data2['is_promoted'], test_size=0.20, random_state=123)\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#automated feature selection using Standard Scikit Package. One of the most popular such algorithm is Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(data1[cols_a], data1['is_promoted'], test_size=0.20, random_state=123)\nclf=XGBClassifier().fit(x_train[cols_a],y_train)\nprint (classification_report(y_test,clf.predict(x_test[cols_a])))\nprint(matthews_corrcoef(y_test,clf.predict(x_test[cols_a])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The bar plot below on feature name and feature importance, produced many of the machine learning packages like Random Forest, LASSO Regression, CATBOOST, etc. helps in reducing the feature scope."},{"metadata":{"trusted":true},"cell_type":"code","source":"category_cols = ['gender']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier().fit(x_train[cols_a],y_train)\nprint (classification_report(y_test,clf.predict(x_test[cols_a])))\nprint(matthews_corrcoef(y_test,clf.predict(x_test[cols_a])))\nclf.feature_importances_\nsns.barplot(y=cols_a , x=clf.feature_importances_)\n# Gender, awards_won, recruitment channel and no_of_traings recieved are few features marked as least important. \n#Let's analyse them, before we cross them off from our list\n#gender- it points to fact that the dataset we have, belongs to a region where getting promoted is gender insensitive. More of just work culture.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}