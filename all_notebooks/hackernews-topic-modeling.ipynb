{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n### Part I\n    \n    Topic modelling and LDA in a nutshell \n\n### Part II:\n\n    Load input data.\n\n    Pre-process that data.\n\n    Transform documents into bag-of-words vectors.\n\n    Train LDA model.\n    \n    Visualize LDA model using pyLDAvis \n","metadata":{}},{"cell_type":"markdown","source":"# What is topic modelling?\n\n___\nA type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.\n> [Topic Modeling - Intro & Implementation](https://www.kaggle.com/akashram/topic-modeling-intro-implementation)\n___\n\n- Topic modeling is a form of unsupervised learning that identifies hidden relationships in data.\n\n- Being unsupervised, topic modeling doesn’t need labeled data. It can be applied directly to a set of text documents to extract information.\n\n- Topic modeling works in an exploratory manner, looking for the themes (or topics) that lie within a set of text data.\n\n- There is no prior knowledge about the themes required in order for topic modeling to work.\n\n- It discovers topics using a probabilistic framework to infer the themes within the data based on the words observed in the documents.\n\n- Topic modeling is a versatile way of making sense of an unstructured collection of text documents.\n\n- It can be used to automate the process of sifting through large volumes of text data and help to organize and understand it.\n\n- Once key topics are discovered, text documents can be grouped for further analysis, to identify trends, for instance, or as a form of classification.\n\nSee: https://highdemandskills.com/topic-modeling-intuitive/\n\n\n### LDA: Latent Dirichlet Allocation\n\nSource: http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n\n\n\nSuppose you have the following set of sentences:\n\n    I like to eat broccoli and bananas.\n    I ate a banana and spinach smoothie for breakfast.\n    Chinchillas and kittens are cute.\n    My sister adopted a kitten yesterday.\n    Look at this cute hamster munching on a piece of broccoli.\n\nWhat is latent Dirichlet allocation? It’s a way of automatically discovering topics that these sentences contain. For example, given these sentences and asked for 2 topics, LDA might produce something like\n\n    Sentences 1 and 2: 100% Topic A\n    Sentences 3 and 4: 100% Topic B\n    Sentence 5: 60% Topic A, 40% Topic B\n    Topic A: 30% broccoli, 15% bananas, 10% breakfast, 10% munching, … (at which point, you could interpret topic A to be about food)\n    Topic B: 20% chinchillas, 20% kittens, 20% cute, 15% hamster, … (at which point, you could interpret topic B to be about cute animals)\n\n### LDA model\n\n- LDA topic modeling discovers topics that are hidden (latent) in a set of text documents.\n\n- It does this by inferring possible topics based on the words in the documents. It uses a generative probabilistic model and Dirichlet distributions to achieve this.\n\n- The inference in LDA is based on a Bayesian framework. This allows the model to infer topics based on observed data (words) through the use of conditional probabilities.\n\n- A generative probabilistic model works by observing data, then generating data that’s similar to it in order to understand the observed data. This is a powerful way to analyze data and goes beyond mere description—by learning how to generate observed data, a generative model learns the essential features that characterize the data.\n","metadata":{}},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/hackernews-umbrella-topics/hn.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.keyw.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby('keyw').count()['title'].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_priv=df[(df['keyw']=='privacy') & ~df['text'].isna()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs=df_priv['text'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(docs))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(docs[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-process and vectorize the documents","metadata":{}},{"cell_type":"code","source":"# Tokenize the documents.\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\n# Split the documents into tokens.\ntokenizer = RegexpTokenizer(r'\\w+')\nfor idx in range(len(docs)):\n    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n\n# Remove numbers, but not words that contain numbers.\ndocs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n\n# Remove words that are only one character.\ndocs = [[token for token in doc if len(token) > 1] for doc in docs]\n\ndocs = [[token for token in doc if token not in stop_words] for doc in docs]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs[0][:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lemmatize the documents.\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndocs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs[0][:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute bigrams.\nfrom gensim.models import Phrases\n\n# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\nbigram = Phrases(docs, min_count=20)\nfor idx in range(len(docs)):\n    for token in bigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"docs[1][-30:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove rare and common tokens.\nfrom gensim.corpora import Dictionary\n\n# Create a dictionary representation of the documents.\ndictionary = Dictionary(docs)\n\n# Filter out words that occur less than 20 documents, or more than 50% of the documents.\ndictionary.filter_extremes(no_below=20, no_above=0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dictionary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bag-of-words\n\n`\"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\",\"likes\",\"movies\",\"too\"`\n\n`BoW1 = {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1};`","metadata":{}},{"cell_type":"code","source":"# Bag-of-words representation of the documents.\ncorpus = [dictionary.doc2bow(doc) for doc in docs]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus[0][:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\n\n- How many topics? \n    - we can experiment or check the coherence score\n- `chunksize` controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory.  Chunksize can however influence the quality of the model\n- `passes` controls how often we train the model on the entire corpus. Another word for passes might be `epochs`. `iterations` is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of “passes” and “iterations” high enough.","metadata":{}},{"cell_type":"code","source":"# Train LDA model.\nfrom gensim.models import LdaModel\n\n# Set training parameters.\nnum_topics = 10\nchunksize = 2000\npasses = 20\niterations = 400\neval_every = None  # Don't evaluate model perplexity, takes too much time.\n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\nmodel = LdaModel(\n    corpus=corpus,\n    id2word=id2word,\n    chunksize=chunksize,\n    alpha='auto',\n    eta='auto',\n    iterations=iterations,\n    num_topics=num_topics,\n    passes=passes,\n    eval_every=eval_every\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_topics = model.top_topics(corpus) #, num_words=20)\n\n# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\navg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\nprint('Average topic coherence: %.4f.' % avg_topic_coherence)\n\nfrom pprint import pprint\npprint(top_topics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyLDAvis.gensim_models\npyLDAvis.enable_notebook()\npyLDAvis.gensim_models.prepare(model, corpus, dictionary)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper parameters\n\n- there is no universally \"best\" choice\n- `alpha` is a parameter that controls the prior distribution over topic weights in each document, while `eta` is a parameter for the prior distribution over word weights in each topic. In gensim, both default to a symmetric, 1 / num_topics prior.\n- `alpha` and `eta` can be thought of as smoothing parameters when we compute how much each document \"likes\" a topic (in the case of alpha) or how much each topic \"likes\" a word (in the case of eta)\n\n# Coherence \nhttps://rare-technologies.com/what-is-topic-coherence/\nhttps://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0","metadata":{}},{"cell_type":"markdown","source":"### Source:\nhttps://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}