{"cells":[{"metadata":{},"cell_type":"markdown","source":"# DSI Case"},{"metadata":{},"cell_type":"markdown","source":"During the COVID-19 pandemic in 2020, the total sales increase in e-commerce increased by 37% in **Indonesia**. Due to increasingly fierce competition between competitors, you and your Product Manager are in discussion regarding how to stay afloat and compete in the e-commerce industry. After that, you decide to make an innovation or offer so that users will still choose you as their online shopping media.\n\nFor that, you are assigned to perform transaction-related analysis of user data. However, the problem is that the company is doing efficiency in terms of managing promotional funds in 2021. As a data analyst, what insights and recommendations can you give to the company?"},{"metadata":{},"cell_type":"markdown","source":"**Objectives:** \n* user acqusition & user retention through new program or offer\n    1. how new user use and get to know ecom\n    2. how to keep new user and old user stay\n\n**Approach:**\n* Data deep dive to know our customer more and then we go from there"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\npd.set_option('display.max_rows', 10000)\npd.set_option('display.max_columns', 100)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/ecommerce-data/data.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(path,header= 0,encoding=\"ISO-8859-1\", dtype = {'CustomerID': str,'InvoiceID': str})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df.copy()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Cleaning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.InvoiceDate = df_clean.InvoiceDate.astype(str)\ndf_clean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For this case purpose, change the year and month so the data we have ranging from January 2020 to December 2020"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean['InvoiceDate'] = df_clean['InvoiceDate'].apply(lambda x: x.replace('2011','2020'))\ndf_clean = df_clean[~df_clean.InvoiceDate.str.contains('2010')]\ndf_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove country as we assume all is in Indonesia"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df_clean.drop(columns = 'Country')\ndf_clean = df_clean.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"check the % of missing value to get a glimpse"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_percentage = df_clean.isnull().sum() / df_clean.shape[0] * 100\nmissing_percentage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"check the number of null value in description"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean[df_clean['Description'].isnull()].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from the table, most of the description also have 0 unit price, we absolutely want to remove this kind of data. let's check if all null description have 0 unit price."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean[df_clean.Description.isnull()].UnitPrice.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"can be concluded that all null description have 0 unit, so we have to remove them all"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df_clean[df_clean['Description'].notnull()]\ndf_clean.Description.isna().value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean[df_clean['UnitPrice']==0.0].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"remove all the rest from UnitPrice that has 0 value, because it's not normal"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df_clean[df_clean['UnitPrice']!=0.0]\n(df_clean['UnitPrice']==0.0).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"remove negative value"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean = df_clean[(df_clean['UnitPrice']>0) & (df_clean['Quantity'] > 0)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_clean.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"next, remove duplicate value"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('number of duplicates: {}'.format(df_clean.duplicated().sum()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = df_clean.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('number of duplicates: {}'.format(data.duplicated().sum()))\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just based on curiosity, let's check the data that have a large unitprice"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.UnitPrice > 200].head() #to check it fully, remove the head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from the large unitprice data, we can see that most of them were DOT, M, and there is this \"AMAZONFEE\" that have super large unitprice"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[(data.StockCode == 'DOT') | (data.StockCode == 'M') | (data.StockCode == 'AMAZONFEE')].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"before we remove them, let's make a box plot to make sure are they an extreme outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(y=data.UnitPrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, the box which is majority of the data can't be seen, so removing the extreme outliers can be our option. In real life case, I think the best choice is to verify to the data collection, are they really customer purchase or not. But since we can't do that now, let's assume that these isn't customer purchase (since the stockcode itself is suspicious). Let's remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[(data.StockCode != 'DOT') & (data.StockCode != 'M') & (data.StockCode != 'AMAZONFEE')].copy()\nsns.boxplot(y=data.UnitPrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"this boxplot indicates that there's still something suspicious going on, so we will check again"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.UnitPrice>200].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ok the item with stockcode POST and B are suspicious to. For the same reason as before, we will remove them too."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[(data.StockCode != 'B') & (data.StockCode != 'POST')].copy()\nsns.boxplot(y=data.UnitPrice)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.UnitPrice>200].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ok this looks fine"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ok there's stil something quite off, the max quantity, let's check it."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.Quantity>500].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ok after checking, it looks fine."},{"metadata":{},"cell_type":"markdown","source":"next thing i want to do, is looking from some potential odd description by using the descrption length"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['des_len'] = data.Description.apply(lambda x: len(x))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.des_len.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.des_len < 10].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"everything looks normal"},{"metadata":{},"cell_type":"markdown","source":"next, let's see from the invoice number"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['noinvo_len'] = data.InvoiceNo.apply(lambda x: len(x))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.noinvo_len.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"oke everything looks fine"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(columns = ['des_len', 'noinvo_len'])\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Before moving forward, i want to replace null value in customer id to guest, just in case."},{"metadata":{"trusted":true},"cell_type":"code","source":"value = {'CustomerID':'Guest'}\ndata = data.fillna(value = value)\ndata[data.CustomerID == 'Guest'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Mining**"},{"metadata":{},"cell_type":"markdown","source":"Enriching Data:\n\nAdding 'TotalPrice' column"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['TotalPrice'] = data.Quantity * data.UnitPrice\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Customer Segmentation"},{"metadata":{},"cell_type":"markdown","source":"I want to know who is our customer really is, based on their purchase behavior. Let's group them so how much our customer for each time purchase."},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = data.groupby(['InvoiceNo','InvoiceDate','CustomerID']).sum()\ndata2 = data2.drop(columns = 'UnitPrice')\ndata2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As stated in the study case, we have a limited budget. So before rolling out promo, let's narrow our scope to focus more only to our majority of customer."},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the data, for now we know that our customer are mainly a reseller."},{"metadata":{},"cell_type":"markdown","source":"As stated in the study case, we have a limited budget. Thus, before rolling out promo, let's narrow our scope to focus more only to our majority of customer. So let's see the outliers and remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import skew","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skew(data2.TotalPrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data is highly skewed, let's remove the outlier with this formula: mean-stddev <= data <= mean+stddev"},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = data2.query('TotalPrice >= 0 and TotalPrice <= 518.593623 + 1799.695926')\n#we use 0 because the mean-stddev is minus, so instead we just use zero","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data2.TotalPrice)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.displot(data2.TotalPrice)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skew(data2.Quantity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's also remove the outlier in quantity"},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = data2.query('Quantity >= 0  and Quantity <= 220.835074 + 248.776217')\ndata2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(skew(data2.TotalPrice))\nprint(skew(data2.Quantity))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ok since it looks pretty much all right, now we have a smaller scope and we will focus on this kind of customer "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(data2.TotalPrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok now let's filter our main table, only with the data that we already sort before."},{"metadata":{"trusted":true},"cell_type":"code","source":"data2 = data2.reset_index()\ninvoice = data2['InvoiceNo'].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data.InvoiceNo.isin(invoice)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Timeseries Trend"},{"metadata":{},"cell_type":"markdown","source":"plot the total user each month"},{"metadata":{"trusted":true},"cell_type":"code","source":"#first we make a new column named month\ndata['Month'] = data.InvoiceDate.dt.to_period('M')\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#f, ax = plt.subplots(figsize=(20, 6))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"since we want to see the customer/user, let's drop 'Guest' User"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data.CustomerID != 'Guest']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_month = data.groupby('Month').CustomerID.nunique().reset_index()\nuser_month.columns = ['month','total_user']\nuser_month.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 6))\n\nsns.lineplot(data = user_month)\nplt.xlabel('Month')\nplt.ylabel('Unique User')\nplt.title('Unique User by Month')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from the graph, we know that overall, we have a good unique user each month. Keep in mind that in december, we only collect data upto December 9th."},{"metadata":{},"cell_type":"markdown","source":"# Insights so far\n\n* We know that majority of our customer were a reseller\n* We have a good amount of unique user each month, with the trend of upward through the end of the year\n\n# Action\n\nFrom the information that we have, in order to reach our goals, we want to roll out promotion. But before that happen, we have to know to whom will we target the promotion. To answer it, first we do clustering to know our customer even more."},{"metadata":{},"cell_type":"markdown","source":"# Customer Classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cust = data[['CustomerID','InvoiceDate','Quantity','UnitPrice','TotalPrice','StockCode']]\ndata_cust.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For clustering, let's group them based on their purchase behavior"},{"metadata":{"trusted":true},"cell_type":"code","source":"#total unique item bought per cust\ntotal_bought = data_cust.groupby('CustomerID').StockCode.nunique().reset_index()\ntotal_bought.columns = ['cust_id','total_product']\ntotal_bought.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#total transaction value\ntotal_trx = data_cust.groupby('CustomerID').TotalPrice.sum().reset_index()\ntotal_trx.columns = ['cust_id','total_trx']\ntotal_trx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.InvoiceDate.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Day since last transactions happen\ndata['LastTrx'] = (pd.to_datetime('2020-12-09 12:50:00') - data.InvoiceDate).dt.days\ndata.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cus_recent_trx = data.groupby('CustomerID').LastTrx.min().reset_index()\ncus_recent_trx.columns = ['cust_id','recent_trx']\ncus_recent_trx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#buying frequency in a year\ncus_frequency = data_cust.groupby('CustomerID').InvoiceDate.nunique().reset_index()\ncus_frequency.columns = ['cust_id','freq']\ncus_frequency.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge the 4 table\ncust = pd.DataFrame()\ncust['cust_id'] = cus_recent_trx.cust_id\ncust = cust.merge(total_bought, on='cust_id')\ncust = cust.merge(total_trx, on='cust_id')\ncust = cust.merge(cus_recent_trx, on='cust_id')\ncust = cust.merge(cus_frequency, on='cust_id')\ncust.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# K-means Clustering"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate sum of squared distances\nssd = []\nK = range(1,10)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(cust)\n    ssd.append(km.inertia_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot sum of squared distances / elbow method\nplt.figure(figsize=(10,6))\nplt.plot(K, ssd, 'bx-')\nplt.xlabel('k')\nplt.ylabel('ssd')\nplt.title('Elbow Method For Optimal k')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To determine the optimal number of clusters, we have to select the value of k at the “elbow” ie the point after which the distortion/inertia start decreasing in a linear fashion.\n\nIn this case, we select K = 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=4)\nmodel = kmeans.fit(cust)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.labels_\ncust['Cluster'] = pred\ncust.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,6))\n\nsns.scatterplot(data=cust, x=\"total_trx\", y=\"recent_trx\", hue=\"Cluster\")\nplt.title('Cluster by Total Transaction and Recencys')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To sum up our cluster, let's make another table to see what majority of the cluster looks like."},{"metadata":{"trusted":true},"cell_type":"code","source":"customers = cust.groupby('Cluster').mean().reset_index()\ncustomers.sort_values('total_trx')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contribution = cust.groupby('Cluster').total_trx.sum().reset_index()\ncontribution['Contribution (%)'] = (contribution.total_trx/contribution.total_trx.sum())*100\ncontribution","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From \"Customers\" & \"Contribution\" Table, now we can classify our customer based on which cluster they belong. Let's determine what kind of customers is in each cluster.\n\nCluster 0: Low unique product, low spending, not recent trx, low freq, high contribution --> **Seasonal Customer**\n\nCluster 1: Low unique product, low spending, not recent trx, low freq, high contribution --> **Seasonal Customer**\n\nCluster 2: medium unique product, medium spending, recent trx, medium freq, high contribution --> **Loyal Customer**\n\nCluster 3: high unique product, very high spending, recent trx, high freq, low contribution --> **Dropshipper**\n"},{"metadata":{},"cell_type":"markdown","source":"Let's see what our customer distribution looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.displot(data=cust,x='Cluster')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph we know that most our customer is from cluster 0 and 1"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nFrom our the insight that we got, we know that:\n1. Most of our customer are reseller\n2. From further classification, we know that our best customers are seasonal customer, and our loyal customer are medium spender reseller\n3. Based on this fact, we want to focus our budget to strengthen our business by targeting those kind of customers.\n\n\n# Proposed Action\n\n**Proposed Idea 1:**\n\n**Idea:** Make a VIP based membership\n\n**Goals:** To reward the customer in cluster 2 with more benefit (discount, free shipping, etc.) so we can keep them, and also become selling point to the customer that outside that cluster\n\n**Proposed Idea 2:**\n \n**Idea:** Rollout seasonal promotion like seasonal discount, bundle offers, etc.\n\n**Goals:** To attract more new customer and to keep the loyal one to keep using our service.\n\n**Proposed Idea 3:**\n\n**Idea:** Make a seasonal personalization like seasonal/holiday item category, push notification on trending items, etc.\n\n**Goals:** To help our customer navigate through our website, so the chance of converting is much higher."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}