{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Hands-on Assignment Python","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Objective:\nThe shared data file summarizes a set of features about articles published by Mashable in two years. The aim is to predict the popularity of the news based on the number of shares and subsequently categorize them as Popular or Unpopular news articles. \n\nUrl to dataset:\nhttps://archive.ics.uci.edu/ml/datasets/Online+News+Popularity\n\nCitation:\nK. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal.","metadata":{}},{"cell_type":"markdown","source":"****NOTE: This dataset is one-hot encoded. Please merge into one column for the purpose of Exploratory Data Analysis. ****","metadata":{}},{"cell_type":"markdown","source":"### 1. Analyse the characteristics of Dataset and understand the features(refer to the metadata file)","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport os\npath = \"../input/online-news-popularity/OnlineNewsPopularity.csv\"\ndf = pd.read_csv(path)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:30.063418Z","iopub.execute_input":"2021-06-02T22:20:30.063915Z","iopub.status.idle":"2021-06-02T22:20:30.855547Z","shell.execute_reply.started":"2021-06-02T22:20:30.063817Z","shell.execute_reply":"2021-06-02T22:20:30.85444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:30.856977Z","iopub.execute_input":"2021-06-02T22:20:30.857322Z","iopub.status.idle":"2021-06-02T22:20:30.866456Z","shell.execute_reply.started":"2021-06-02T22:20:30.857289Z","shell.execute_reply":"2021-06-02T22:20:30.86547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:30.868246Z","iopub.execute_input":"2021-06-02T22:20:30.868548Z","iopub.status.idle":"2021-06-02T22:20:30.919119Z","shell.execute_reply.started":"2021-06-02T22:20:30.868519Z","shell.execute_reply":"2021-06-02T22:20:30.91822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:30.920568Z","iopub.execute_input":"2021-06-02T22:20:30.920851Z","iopub.status.idle":"2021-06-02T22:20:30.953322Z","shell.execute_reply.started":"2021-06-02T22:20:30.920824Z","shell.execute_reply":"2021-06-02T22:20:30.952643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[ ' data_channel_is_lifestyle',\n       ' data_channel_is_entertainment', ' data_channel_is_bus',\n       ' data_channel_is_socmed', ' data_channel_is_tech',\n       ' data_channel_is_world',]].head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:30.954287Z","iopub.execute_input":"2021-06-02T22:20:30.954677Z","iopub.status.idle":"2021-06-02T22:20:30.97299Z","shell.execute_reply.started":"2021-06-02T22:20:30.954648Z","shell.execute_reply":"2021-06-02T22:20:30.971973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[       ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',\n       ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',\n       ' weekday_is_sunday', ' is_weekend']].head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:30.974214Z","iopub.execute_input":"2021-06-02T22:20:30.974493Z","iopub.status.idle":"2021-06-02T22:20:31.001762Z","shell.execute_reply.started":"2021-06-02T22:20:30.974466Z","shell.execute_reply":"2021-06-02T22:20:31.000608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = \"weekday_is_friday\"\nairs_on = [       ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',\n       ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',\n       ' weekday_is_sunday', ' is_weekend']\nfor day in airs_on:\n    i = day.index(\"is_\")\n    print(day[i+3:])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:31.003322Z","iopub.execute_input":"2021-06-02T22:20:31.003746Z","iopub.status.idle":"2021-06-02T22:20:31.015664Z","shell.execute_reply.started":"2021-06-02T22:20:31.003703Z","shell.execute_reply":"2021-06-02T22:20:31.014667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chanel_type = [ ' data_channel_is_lifestyle',\n       ' data_channel_is_entertainment', ' data_channel_is_bus',\n       ' data_channel_is_socmed', ' data_channel_is_tech',\n       ' data_channel_is_world',]\nfor chanel in chanel_type:\n    i = chanel.index(\"is_\")\n    print(chanel[i+3:])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:31.018233Z","iopub.execute_input":"2021-06-02T22:20:31.018617Z","iopub.status.idle":"2021-06-02T22:20:31.02805Z","shell.execute_reply.started":"2021-06-02T22:20:31.018576Z","shell.execute_reply":"2021-06-02T22:20:31.027251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Perform Data Cleaning:\n* Removing Duplicates\n* Missing Data Analysis(If any)\n* Imputing Missing Data / Dropping rows with missing data(upto your judgement)\n* Please go ahead and do more analysis / cleaning if you deem necessary.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"The below piece of code shows us the columns that have na values and we can see that none do so no need to clean that type of data","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:31.029684Z","iopub.execute_input":"2021-06-02T22:20:31.030136Z","iopub.status.idle":"2021-06-02T22:20:31.287684Z","shell.execute_reply.started":"2021-06-02T22:20:31.030103Z","shell.execute_reply":"2021-06-02T22:20:31.286714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:31.28901Z","iopub.execute_input":"2021-06-02T22:20:31.2893Z","iopub.status.idle":"2021-06-02T22:20:31.306394Z","shell.execute_reply.started":"2021-06-02T22:20:31.289268Z","shell.execute_reply":"2021-06-02T22:20:31.305226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.columns[df.isna().any()]]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:31.307631Z","iopub.execute_input":"2021-06-02T22:20:31.307937Z","iopub.status.idle":"2021-06-02T22:20:31.327351Z","shell.execute_reply.started":"2021-06-02T22:20:31.307905Z","shell.execute_reply":"2021-06-02T22:20:31.326331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below piece of code shows us the number of duplicated rows and here again we see there are no duplicates","metadata":{}},{"cell_type":"code","source":"df.duplicated().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:31.328574Z","iopub.execute_input":"2021-06-02T22:20:31.328896Z","iopub.status.idle":"2021-06-02T22:20:31.44168Z","shell.execute_reply.started":"2021-06-02T22:20:31.328864Z","shell.execute_reply":"2021-06-02T22:20:31.440796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:31.442961Z","iopub.execute_input":"2021-06-02T22:20:31.443256Z","iopub.status.idle":"2021-06-02T22:20:31.551192Z","shell.execute_reply.started":"2021-06-02T22:20:31.443228Z","shell.execute_reply":"2021-06-02T22:20:31.550119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The below piece of codes shows us that the data has no null values in it","metadata":{}},{"cell_type":"code","source":"df.isnull().sum().sum()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:31.552466Z","iopub.execute_input":"2021-06-02T22:20:31.552803Z","iopub.status.idle":"2021-06-02T22:20:31.570665Z","shell.execute_reply.started":"2021-06-02T22:20:31.552739Z","shell.execute_reply":"2021-06-02T22:20:31.569499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reverse_one_hot_chanel(row):\n    hot_columns = [' data_channel_is_lifestyle',\n       ' data_channel_is_entertainment', ' data_channel_is_bus',\n       ' data_channel_is_socmed', ' data_channel_is_tech',\n       ' data_channel_is_world',]\n    for column in df.columns:\n        if(column in hot_columns):\n            if round(row[column]) == round(1.0):\n                i = column.index(\"is_\")\n                return column[i+3:]\ndef reverse_one_hot_weekday(row):\n    hot_columns = [       ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',\n       ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',\n       ' weekday_is_sunday', ' is_weekend']\n    for column in df.columns:\n        if(column in hot_columns):\n            if round(row[column]) == round(1.0):\n                i = column.index(\"is_\")\n                return column[i+3:]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:31.572422Z","iopub.execute_input":"2021-06-02T22:20:31.572831Z","iopub.status.idle":"2021-06-02T22:20:31.581203Z","shell.execute_reply.started":"2021-06-02T22:20:31.572787Z","shell.execute_reply":"2021-06-02T22:20:31.580202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"chanel\"]=df.apply(reverse_one_hot_chanel,axis=1)\ndf[\"weekday\"] = df.apply(reverse_one_hot_weekday, axis =1)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:31.5827Z","iopub.execute_input":"2021-06-02T22:20:31.583276Z","iopub.status.idle":"2021-06-02T22:20:35.038059Z","shell.execute_reply.started":"2021-06-02T22:20:31.583229Z","shell.execute_reply":"2021-06-02T22:20:35.037053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[[\"chanel\",\"weekday\"]].head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:35.039338Z","iopub.execute_input":"2021-06-02T22:20:35.03966Z","iopub.status.idle":"2021-06-02T22:20:35.058071Z","shell.execute_reply.started":"2021-06-02T22:20:35.039629Z","shell.execute_reply":"2021-06-02T22:20:35.057126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Perform Exploratory Data Analysis: \n* Outlier Analysis / Removal\n* Correlation Plot\n* Check for Class Imbalance\n* Univariate Analysis / Bi-variate Analysis with Target Variable(Convert Shares to binary variables: If shares > 1400 , news is considered popular)\n* Please go ahead and do more analysis if you deem necessary.\n* Please share a concise note of your findings from the EDA. (One you would share with your client.)\n* Which models would you opt for classification and why?\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:35.059325Z","iopub.execute_input":"2021-06-02T22:20:35.05961Z","iopub.status.idle":"2021-06-02T22:20:35.064013Z","shell.execute_reply.started":"2021-06-02T22:20:35.059584Z","shell.execute_reply":"2021-06-02T22:20:35.062631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = df[\" shares\"]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:35.065844Z","iopub.execute_input":"2021-06-02T22:20:35.066314Z","iopub.status.idle":"2021-06-02T22:20:35.078199Z","shell.execute_reply.started":"2021-06-02T22:20:35.066266Z","shell.execute_reply":"2021-06-02T22:20:35.077095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This snippet of code shows us how many news websites are popular while also showing us which are unpopular  without the removal of outliers","metadata":{}},{"cell_type":"code","source":"print(len(target[target>1400]))\nprint(len(target)-len(target[target>1400]))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:35.079899Z","iopub.execute_input":"2021-06-02T22:20:35.080486Z","iopub.status.idle":"2021-06-02T22:20:35.110777Z","shell.execute_reply.started":"2021-06-02T22:20:35.080439Z","shell.execute_reply":"2021-06-02T22:20:35.109519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the below piece of code we can see the figure shwos practivcally nothing but many outliers in order for use to clear these outliers we will ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 7))\nplt.boxplot(target)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:35.112078Z","iopub.execute_input":"2021-06-02T22:20:35.112363Z","iopub.status.idle":"2021-06-02T22:20:35.265651Z","shell.execute_reply.started":"2021-06-02T22:20:35.112335Z","shell.execute_reply":"2021-06-02T22:20:35.264507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# finding the 1st quartile\nq1 = np.quantile(target, 0.25)\n \n# finding the 3rd quartile\nq3 = np.quantile(target, 0.75)\nmed = np.median(target)\n \n# finding the iqr region\niqr = q3-q1\n \n# finding upper and lower whiskers\nupper_bound = q3+(1.5*iqr)\nlower_bound = q1-(1.5*iqr)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:35.266913Z","iopub.execute_input":"2021-06-02T22:20:35.267211Z","iopub.status.idle":"2021-06-02T22:20:35.276238Z","shell.execute_reply.started":"2021-06-02T22:20:35.267183Z","shell.execute_reply":"2021-06-02T22:20:35.275203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graph is now much easier to read with the removal of serval major outliers but we can still see a vast number of outliers in the data. we can run this same process again to further narrow down the data","metadata":{}},{"cell_type":"code","source":"# boxplot of data within the whisker\nnew_target = target[(target >= lower_bound) & (target <= upper_bound)]\n\nplt.figure(figsize=(12, 7))\nplt.boxplot(new_target)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:35.280509Z","iopub.execute_input":"2021-06-02T22:20:35.280843Z","iopub.status.idle":"2021-06-02T22:20:35.405518Z","shell.execute_reply.started":"2021-06-02T22:20:35.280813Z","shell.execute_reply":"2021-06-02T22:20:35.404559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q1 = np.quantile(new_target, 0.25)\n \n# finding the 3rd quartile\nq3 = np.quantile(new_target, 0.75)\nmed = np.median(new_target)\n \n# finding the iqr region\niqr = q3-q1\n \n# finding upper and lower whiskers\nupper_bound = q3+(1.5*iqr)\nlower_bound = q1-(1.5*iqr)\nfinal_target = new_target[(new_target >= lower_bound) & (new_target <= upper_bound)]\nplt.figure(figsize=(12, 7))\nplt.boxplot(final_target)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:35.407498Z","iopub.execute_input":"2021-06-02T22:20:35.407821Z","iopub.status.idle":"2021-06-02T22:20:35.538632Z","shell.execute_reply.started":"2021-06-02T22:20:35.407791Z","shell.execute_reply":"2021-06-02T22:20:35.53794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the below code snippet shows us the distribution that the final target follows we can see that the majjority of the data seems to fall near 1000","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\nsns.histplot(final_target)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:35.539897Z","iopub.execute_input":"2021-06-02T22:20:35.54048Z","iopub.status.idle":"2021-06-02T22:20:36.655658Z","shell.execute_reply.started":"2021-06-02T22:20:35.540434Z","shell.execute_reply":"2021-06-02T22:20:36.654492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target= target[final_target]\ndf=df.iloc[target.index]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:36.657015Z","iopub.execute_input":"2021-06-02T22:20:36.65734Z","iopub.status.idle":"2021-06-02T22:20:36.673188Z","shell.execute_reply.started":"2021-06-02T22:20:36.65731Z","shell.execute_reply":"2021-06-02T22:20:36.672213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14, 7))\nsns.pairplot(df, y_vars=\" shares\", x_vars=df.columns.values)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:20:36.674729Z","iopub.execute_input":"2021-06-02T22:20:36.675188Z","iopub.status.idle":"2021-06-02T22:21:25.280721Z","shell.execute_reply.started":"2021-06-02T22:20:36.675142Z","shell.execute_reply":"2021-06-02T22:21:25.279386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from the above plots we can see the relationship between the target (shares) and all other features","metadata":{}},{"cell_type":"code","source":"#get correlations of each features in dataset\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(100,100))\n#plot heat map\nsns.set(font_scale = 2)\n\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:25.282612Z","iopub.execute_input":"2021-06-02T22:21:25.283152Z","iopub.status.idle":"2021-06-02T22:21:46.331959Z","shell.execute_reply.started":"2021-06-02T22:21:25.283098Z","shell.execute_reply":"2021-06-02T22:21:46.330796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data has been well cleaned no missing values or duplicates so that was a big plus. \nWe can see alot of positive/ negative correlations from this plot and the correlation heat map. This will make the feature selection rather simple. In order to determine the popularity of the webiste we will use three different classifers the ones in mind currently are Logistic Regression, Preceptron, and Naive Bayes","metadata":{}},{"cell_type":"markdown","source":"### 4. Feature Engineering / Selection:\nPerform Feature Engineering based on conclusions from EDA and use relevant feature selection techniques for analysis.","metadata":{}},{"cell_type":"code","source":"#one is popular zero is not\nans = []\nfor i in df[\" shares\"]:\n    if i >1400:\n        ans.append(1)\n    else:\n        ans.append(0)\ndf[\"target\"] = ans","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:46.333382Z","iopub.execute_input":"2021-06-02T22:21:46.333754Z","iopub.status.idle":"2021-06-02T22:21:46.365494Z","shell.execute_reply.started":"2021-06-02T22:21:46.333677Z","shell.execute_reply":"2021-06-02T22:21:46.364696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\ndf[\"url\"] = le.fit_transform(df[\"url\"])\ndf[\"chanel\"] = le.fit_transform(df[\"chanel\"])\ndf[\"weekday\"] = le.fit_transform(df[\"weekday\"])\n\nX = df[[c for c in df.columns if c != \" shares\"  and c!= \"target\"]]\nX = X.apply(lambda x: x**2)\ny = df[\"target\"]\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeature_scores = pd.concat([dfcolumns,dfscores],axis=1)\nfeature_scores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(\"Top 10 best features\")\nprint(feature_scores.nlargest(10,'Score'))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:46.366615Z","iopub.execute_input":"2021-06-02T22:21:46.367019Z","iopub.status.idle":"2021-06-02T22:21:46.894407Z","shell.execute_reply.started":"2021-06-02T22:21:46.366971Z","shell.execute_reply":"2021-06-02T22:21:46.893221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='bar')\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:46.896155Z","iopub.execute_input":"2021-06-02T22:21:46.896616Z","iopub.status.idle":"2021-06-02T22:21:49.86586Z","shell.execute_reply.started":"2021-06-02T22:21:46.89657Z","shell.execute_reply":"2021-06-02T22:21:49.864896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"true_best_features = set(feature_scores[\"Feature\"]).intersection(set(feat_importances.nlargest(10).index))\nprint(true_best_features)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:49.867199Z","iopub.execute_input":"2021-06-02T22:21:49.867503Z","iopub.status.idle":"2021-06-02T22:21:49.874507Z","shell.execute_reply.started":"2021-06-02T22:21:49.867474Z","shell.execute_reply":"2021-06-02T22:21:49.87329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above snippets of code used two seperate meeasure to determine the top 10 featueres the first used sklearns select best module and the second used the ExtraTreeClassifer to determine them. Then we took the two resulting sets from both methods and took the intersection to ensure we got the best possible features.","metadata":{}},{"cell_type":"markdown","source":"### 5. Model Training:\n* Split your dataset into Train and Test.(80/20 Split)\n* Create a classification model to predict whether the news is popular or unpopular.(If shares > 1400 , news is considered popular)\n* Try atleast 3 classification models and check for overfitting.","metadata":{"execution":{"iopub.status.busy":"2021-05-27T20:41:25.642798Z","iopub.execute_input":"2021-05-27T20:41:25.643259Z","iopub.status.idle":"2021-05-27T20:41:25.654575Z","shell.execute_reply.started":"2021-05-27T20:41:25.643153Z","shell.execute_reply":"2021-05-27T20:41:25.653519Z"}}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import train_test_split\nxTrain,xTest,yTrain,yTest =train_test_split(np.array(X[true_best_features]), np.array(y), test_size = 0.2, random_state = 2)\ndisplay(xTrain.shape,xTest.shape,yTrain.shape,yTest.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:49.875913Z","iopub.execute_input":"2021-06-02T22:21:49.876301Z","iopub.status.idle":"2021-06-02T22:21:49.904003Z","shell.execute_reply.started":"2021-06-02T22:21:49.876259Z","shell.execute_reply":"2021-06-02T22:21:49.902928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(random_state=0, max_iter =100000)\nrf = RandomForestClassifier(max_features=5,n_estimators = 10)\nnb = BernoulliNB()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:49.905339Z","iopub.execute_input":"2021-06-02T22:21:49.905638Z","iopub.status.idle":"2021-06-02T22:21:49.911184Z","shell.execute_reply.started":"2021-06-02T22:21:49.905609Z","shell.execute_reply":"2021-06-02T22:21:49.910002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nstart = time.time()\nlr.fit(xTrain,yTrain)\nfinish = time.time()- start\nprint(f\"Logisitic Regression took {finish} seconds to complete training\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:49.912643Z","iopub.execute_input":"2021-06-02T22:21:49.912933Z","iopub.status.idle":"2021-06-02T22:21:50.086857Z","shell.execute_reply.started":"2021-06-02T22:21:49.912904Z","shell.execute_reply":"2021-06-02T22:21:50.085687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nrf.fit(xTrain,yTrain)\nfinish = time.time()- start\nprint(f\"Random Forest took {finish} seconds to complete training\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:50.088858Z","iopub.execute_input":"2021-06-02T22:21:50.089679Z","iopub.status.idle":"2021-06-02T22:21:50.309049Z","shell.execute_reply.started":"2021-06-02T22:21:50.089627Z","shell.execute_reply":"2021-06-02T22:21:50.3082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nnb.fit(xTrain,yTrain)\nfinish = time.time()- start\nprint(f\"Naive Baynes took {finish} seconds to complete training\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:50.310366Z","iopub.execute_input":"2021-06-02T22:21:50.310901Z","iopub.status.idle":"2021-06-02T22:21:50.327762Z","shell.execute_reply.started":"2021-06-02T22:21:50.310867Z","shell.execute_reply":"2021-06-02T22:21:50.326604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_train_score= lr.score(xTrain,yTrain)\nnb_train_score = nb.score(xTrain,yTrain)\nrf_train_score = rf.score(xTrain,yTrain)\n\nprint(f\"Logistic Regression Training Score {lr_train_score}\")\nprint(f\"Random Forest Training Score {rf_train_score}\")\nprint(f\"Naive Baynes Training Score {nb_train_score}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:50.329822Z","iopub.execute_input":"2021-06-02T22:21:50.330672Z","iopub.status.idle":"2021-06-02T22:21:50.402089Z","shell.execute_reply.started":"2021-06-02T22:21:50.330617Z","shell.execute_reply":"2021-06-02T22:21:50.400808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_test_score= lr.score(xTest,yTest)\nnb_test_score = nb.score(xTest,yTest)\nrf_test_score = rf.score(xTest,yTest)\n\nprint(f\"Logistic Regression Test Score {lr_test_score}\")\nprint(f\"Random Forest Test Score {rf_test_score}\")\nprint(f\"Naive Baynes Test Score {nb_test_score}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:50.404168Z","iopub.execute_input":"2021-06-02T22:21:50.405041Z","iopub.status.idle":"2021-06-02T22:21:50.438356Z","shell.execute_reply.started":"2021-06-02T22:21:50.404967Z","shell.execute_reply":"2021-06-02T22:21:50.437154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(lr, xTrain, yTrain, cv=10)\nprint(f\"Cross Validation Scores for Logistic Regression\\n{scores}\\n\")\nscores = cross_val_score(nb, xTrain, yTrain, cv=10)\nprint(f\"Cross Validation Scores for Naive Baynes \\n{scores}\\n\")\nscores = cross_val_score(rf, xTrain, yTrain, cv=10)\nprint(f\"Cross Validation Scores for Random Forest\\n{scores}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:50.440441Z","iopub.execute_input":"2021-06-02T22:21:50.441339Z","iopub.status.idle":"2021-06-02T22:21:53.849591Z","shell.execute_reply.started":"2021-06-02T22:21:50.441281Z","shell.execute_reply":"2021-06-02T22:21:53.848815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the above code snippets that the ranking of the classifiers goes as follows: Random Forest, Naive Baynes, Logistic Regression. \nHowever this is for only one of the random partions of the data. In the below code we will train with 50 different random partions of the data to ensure the trends continue.\nWe will fit the models to the training data then compare the current models test score to the best models and at the end we will select the version of the model that performs the best on the test data","metadata":{}},{"cell_type":"code","source":"lr_best = (None,0)\nrf_best =  (None,0)\nnb_best = (None,0)\nfor i in range(50):\n    xTrain,xTest,yTrain,yTest =train_test_split(np.array(X[true_best_features]), np.array(y), test_size = 0.2, random_state = 2)\n    lr = LogisticRegression(random_state=0, max_iter =100000)\n    rf = RandomForestClassifier(max_features=5,n_estimators = 10)\n    nb = BernoulliNB()\n    lr.fit(xTrain,yTrain)\n    rf.fit(xTrain,yTrain)\n    nb.fit(xTrain,yTrain)\n    lr_test_score= lr.score(xTest,yTest)\n    nb_test_score = nb.score(xTest,yTest)\n    rf_test_score = rf.score(xTest,yTest)\n    if(lr_test_score>lr_best[1]):\n        lr_best = (lr,lr_test_score)\n        \n    if(nb_test_score>nb_best[1]):\n        nb_best = (nb,nb_test_score)\n    \n    if(rf_test_score>rf_best[1]):\n        rf_best = (rf,rf_test_score)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:21:53.851157Z","iopub.execute_input":"2021-06-02T22:21:53.851543Z","iopub.status.idle":"2021-06-02T22:22:13.975476Z","shell.execute_reply.started":"2021-06-02T22:21:53.851509Z","shell.execute_reply":"2021-06-02T22:22:13.974148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr, lr_top_score = lr_best\nrf, rf_top_score = rf_best\nnb, nb_top_score = nb_best\nprint(f\"Logistic Regression best possible score {lr_top_score}\")\nprint(f\"Random Forest best possible score {rf_top_score}\")\nprint(f\"Naive Baynes best possible score {nb_top_score}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:22:13.977522Z","iopub.execute_input":"2021-06-02T22:22:13.97841Z","iopub.status.idle":"2021-06-02T22:22:13.987674Z","shell.execute_reply.started":"2021-06-02T22:22:13.978358Z","shell.execute_reply":"2021-06-02T22:22:13.986413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"perfrom cross valdiation again","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(lr, xTrain, yTrain, cv=10)\nprint(f\"Cross Validation Scores for Logistic Regression\\n{scores}\\n\")\nscores = cross_val_score(nb, xTrain, yTrain, cv=10)\nprint(f\"Cross Validation Scores for Naive Baynes \\n{scores}\\n\")\nscores = cross_val_score(rf, xTrain, yTrain, cv=10)\nprint(f\"Cross Validation Scores for Random Forest\\n{scores}\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:22:13.989422Z","iopub.execute_input":"2021-06-02T22:22:13.989959Z","iopub.status.idle":"2021-06-02T22:22:17.324478Z","shell.execute_reply.started":"2021-06-02T22:22:13.989899Z","shell.execute_reply":"2021-06-02T22:22:17.323194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Model Evaluation\n* Plot ROC curve & report the AUC.\n* Recommend which model is best with your reasoning.\n","metadata":{}},{"cell_type":"markdown","source":"Each of the classifiers individual probabilites ","metadata":{}},{"cell_type":"code","source":"r_probs = [0 for _ in range(len(yTest))]\nlr_probs = lr.predict_proba(xTest)\nrf_probs = rf.predict_proba(xTest)\nnb_probs = nb.predict_proba(xTest)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:22:17.326109Z","iopub.execute_input":"2021-06-02T22:22:17.326538Z","iopub.status.idle":"2021-06-02T22:22:17.35758Z","shell.execute_reply.started":"2021-06-02T22:22:17.326493Z","shell.execute_reply":"2021-06-02T22:22:17.356191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_probs =lr_probs[:,1]\nrf_probs =rf_probs[:,1]\nnb_probs =nb_probs[:,1]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:22:17.368153Z","iopub.execute_input":"2021-06-02T22:22:17.368676Z","iopub.status.idle":"2021-06-02T22:22:17.382522Z","shell.execute_reply.started":"2021-06-02T22:22:17.368629Z","shell.execute_reply":"2021-06-02T22:22:17.380441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finds the AUC scores for each Classifier","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\nr_auc = roc_auc_score(yTest, r_probs)\nrf_auc = roc_auc_score(yTest, rf_probs)\nnb_auc = roc_auc_score(yTest, nb_probs)\nlr_auc = roc_auc_score(yTest, lr_probs)\n\nprint(f\"Random (chance) Prediction: AUROC = {r_auc}\")\nprint(f\"Random Forest Prediction: AUROC = {rf_auc}\")\nprint(f\"Naive Baynes (Bernoulli) Prediction: AUROC =  {nb_auc}\")\nprint(f\"Logistic Regression Prediction: AUROC =  {lr_auc}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:22:17.387554Z","iopub.execute_input":"2021-06-02T22:22:17.388232Z","iopub.status.idle":"2021-06-02T22:22:17.436074Z","shell.execute_reply.started":"2021-06-02T22:22:17.388176Z","shell.execute_reply":"2021-06-02T22:22:17.434884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculate and plot the ROC Curves","metadata":{}},{"cell_type":"code","source":"r_fpr, r_tpr, _ = roc_curve(yTest, r_probs)\nrf_fpr, rf_tpr, _ = roc_curve(yTest, rf_probs)\nnb_fpr, nb_tpr, _ = roc_curve(yTest, nb_probs)\nlr_fpr, lr_tpr, _ = roc_curve(yTest, lr_probs)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:22:17.446061Z","iopub.execute_input":"2021-06-02T22:22:17.446837Z","iopub.status.idle":"2021-06-02T22:22:17.459302Z","shell.execute_reply.started":"2021-06-02T22:22:17.446779Z","shell.execute_reply":"2021-06-02T22:22:17.458305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.figure(figsize=(20,10))\n\nplt.plot(r_fpr, r_tpr, linestyle='--', label=f\"Random Chance\")\nplt.plot(rf_fpr, rf_tpr, marker='.', label=f\"Random Forest\")\nplt.plot(nb_fpr, nb_tpr, marker='.', label=f\"Naive Baynes (Bernoulli)\")\nplt.plot(lr_fpr, lr_tpr, marker='.', label=f\"Logistic Regression\")\nplt.legend(loc =\"lower right\",prop={'size': 20})\n# Title\nplt.title('ROC Plot')\n# Axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\n\n# Show legend\n# Show plot\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T22:22:17.460724Z","iopub.execute_input":"2021-06-02T22:22:17.461319Z","iopub.status.idle":"2021-06-02T22:22:17.749644Z","shell.execute_reply.started":"2021-06-02T22:22:17.461285Z","shell.execute_reply":"2021-06-02T22:22:17.74869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the analysis we've conducted we have concluded that the best features to use are {' kw_avg_avg', ' num_keywords', ' kw_avg_max', ' rate_positive_words', ' timedelta', ' abs_title_subjectivity', ' is_weekend', ' shares', ' min_negative_polarity', ' max_positive_polarity'}. After the features were selected we continued to test the models against various means to ensure no overfitting of training data was occuring and the model that proved to be the best by means of sklearn metrics score, cross validation score, AUC score, and by the ROC score is Random Forest Classifier.\n","metadata":{}}]}