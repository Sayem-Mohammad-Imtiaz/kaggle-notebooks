{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"Dataset description\nCement (cement) -- quantitative -- kg in a m3 mixture -- Input Variable\nBlast Furnace Slag (slag) -- quantitative -- kg in a m3 mixture -- Input Variable\nFly Ash (ash) -- quantitative -- kg in a m3 mixture -- Input Variable\nWater (water) -- quantitative -- kg in a m3 mixture -- Input Variable\nSuperplasticizer (superplastic) -- quantitative -- kg in a m3 mixture -- Input Variable\nCoarse Aggregate (coarseagg) -- quantitative -- kg in a m3 mixture -- Input Variable\nFine Aggregate (fineagg) -- quantitative -- kg in a m3 mixture -- Input Variable\nAge(age) -- quantitative -- Day (1~365) -- Input Variable\nConcrete compressive strength(strength) -- quantitative -- MPa -- Output Variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Steps:\nImport the libs\nFirst look at the data\nExploratory data analysis\n3.1. IQR and ouliers analysis (box plot)\n3.2. Distribution of independent variables\n3.3. Pair plots\nHeat map analysis\nK-Means clustering - deal with multiple gaussians\nModel building - Iteration 1 --> Use data as is\nModel building - Iteration 2 --> Include the results of outlier treatment\nModel building - Iteration 3 --> Include the results of K-means clustering\nModel building - Iteration 4 --> Normalize/Standardize the data before model building\nHyper parameter training on best model from all iterations\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('../input/compressive-strength-of-concrete'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Loading Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/compressive-strength-of-concrete/npvproject-concrete.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Simplifying Column names, since they appear to be too lengthy."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"req_col_names = [\"Cement\", \"BlastFurnaceSlag\", \"FlyAsh\", \"Water\", \"Superplasticizer\",\n                 \"CoarseAggregate\", \"FineAggregare\", \"Age\", \"CC_Strength\"]\ncurr_col_names = list(data.columns)\n\nmapper = {}\nfor i, name in enumerate(curr_col_names):\n    mapper[name] = req_col_names[i]\n\ndata = data.rename(columns=mapper)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Checking for 'null' values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Observations:\nAll of the data in the dataset is numerical\nNo null/NAN data\nAge data appears to have outliers because max value is very large as \ncompared to 3rd IQR value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are no null values in the data.\n\n##### Checking the pairwise relations of Features."},{"metadata":{"trusted":true},"cell_type":"code","source":"Exploratory data analysis\nBox plots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12, 6))\nax = sns.boxplot(data=data)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Observations\nAge column appears to be having maximum number of outliers\nSlag, Water, superplastic, fineagg features have some outliers\nAll features except age and strength have same units(kg in m3 mixture) but have different scales. Thus we might need to scale\nthe data so as to avoid bias in algorithms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"> ## Distribution of independent variables\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\n\ncols = [i for i in data.columns if i != 'strength']\n\nfig = plt.figure(figsize=(15, 20))\n\nfor i,j in itertools.zip_longest(cols, range(len(cols))):\n    plt.subplot(4,2,j+1)\n    ax = sns.distplot(data[i],color='green',rug=True)\n    plt.axvline(data[i].mean(),linestyle=\"dashed\",label=\"mean\", color='black')\n    plt.legend()\n    plt.title(i)\n    plt.xlabel(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Observations\nDiagonal analysis + dist plots analysis\nDistribution of cement appears nearly normal\nSlag and ash has 2 gaussians and is skewed\nWater and Superplastic have near normal distributions\nAge data has long tail which confirms the presence of outliers\nStrength is normally distributed\nOff-diagonal analysis with strength\nCement has strong correlation with strength\nSlag is a very weak predictor because the distribution is like a cloud\nash, coarseagg and fineagg are also weak predictors\nWater appears to have a negative correlation with strength\nSuperplastic appears to have positive correlation with strength\nage also has strong correlation with strength\nOff-diagonal analysis between other features\nCement and slag have strong correlation\nWater and super plastic have strong negative correlation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be no high correlation between independant variables (features). This can be further confirmed by plotting the Pearson Correlation coefficients between the features."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\n\nsns.heatmap(corr, annot=True, cmap='Blues')\nb, t = plt.ylim()\nplt.ylim(b+0.5, t-0.5)\nplt.title(\"Feature Correlation Heatmap\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"3D graph between 3 most important features and target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(14,11))\n\nax  = fig.gca(projection = \"3d\")\n#plt.subplot(111,projection = \"3d\") \n\nplot =  ax.scatter(data[\"Cement\"],\n           data[\"CC_Strength\"],\n           data[\"Superplasticizer\"],\n           linewidth=1,edgecolor =\"k\",\n           c=data[\"Age\"],s=100,cmap=\"cool\")\n\nax.set_xlabel(\"Cement\")\nax.set_ylabel(\"CC_Strength\")\nax.set_zlabel(\"Superplasticizer\")\n\nlab = fig.colorbar(plot,shrink=.5,aspect=5)\nlab.set_label(\"AGE\",fontsize = 15)\n\nplt.title(\"3D plot for cement,compressive strength and super plasticizer\",color=\"navy\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EDA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.distplot(data.CC_Strength)\nax.set_title(\"Compressive Strength Distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,7))\nsns.scatterplot(y=\"CC_Strength\", x=\"Cement\", hue=\"Water\", size=\"Age\", data=data, ax=ax, sizes=(20, 200))\nax.set_title(\"CC Strength vs (Cement, Age, Water)\")\nax.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusions from Strength vs (Cement, Age, Water)\n* Compressive strength increases with amount of cement\n* Compressive strength increases with age\n* Cement with low age requires more cement for higher strength\n* The older the cement is the more water it requires\n* Concrete strength increases when less water is used in preparing it"},{"metadata":{},"cell_type":"markdown","source":"\n### Data Preprocessing\nSeparating Input Features and Target Variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.iloc[:,:-1]         # Features - All columns but last\ny = data.iloc[:,-1]          # Target - Last Column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Splitting data into Training and Test splits"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scaling\nStandardizing the data i.e. to rescale the features to have a mean of zero and standard deviation of 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Building\nTraining Machine Learning Algorithms on the training data and making predictions on Test data.\n\n#### 1. Linear Regression\n* The Go-to method for Regression problems.\n* The Algorithm assigns coefficients to each input feature to form a linear relation between input features and target variable, so as to minimize an objective function.\n* The objective function used in this case is Mean Squared Error.\n* There are three versions of Linear Regression\n    - Linear Regression - No regularisation\n    - Lasso Regression - L1 regularisation (Tries to push coefficients to zero)\n    - Ridge Regression - L2 regularisation (Tries to keep coefficients as low as possible)\ncomparing these three algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing models\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\n\n# Linear Regression\nlr = LinearRegression()\n# Lasso Regression\nlasso = Lasso()\n# Ridge Regression\nridge = Ridge()\n\n# Fitting models on Training data \nlr.fit(X_train, y_train)\nlasso.fit(X_train, y_train)\nridge.fit(X_train, y_train)\n\n# Making predictions on Test data\ny_pred_lr = lr.predict(X_test)\ny_pred_lasso = lasso.predict(X_test)\ny_pred_ridge = ridge.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluation\nComparing the Root Mean Squared Error (RMSE), Mean Squared Error (MSE), Mean Absolute Error(MAE) and R2 Score."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nprint(\"Model\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"LinearRegression \\t {:.2f} \\t\\t {:.2f} \\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_lr)),mean_squared_error(y_test, y_pred_lr),\n            mean_absolute_error(y_test, y_pred_lr), r2_score(y_test, y_pred_lr)))\nprint(\"\"\"LassoRegression \\t {:.2f} \\t\\t {:.2f} \\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_lasso)),mean_squared_error(y_test, y_pred_lasso),\n            mean_absolute_error(y_test, y_pred_lasso), r2_score(y_test, y_pred_lasso)))\nprint(\"\"\"RidgeRegression \\t {:.2f} \\t\\t {:.2f} \\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_ridge)),mean_squared_error(y_test, y_pred_ridge),\n            mean_absolute_error(y_test, y_pred_ridge), r2_score(y_test, y_pred_ridge)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The performance seem to be similar with all the three methods.\n\n##### Plotting the coefficients"},{"metadata":{"trusted":true},"cell_type":"code","source":"coeff_lr = lr.coef_\ncoeff_lasso = lasso.coef_\ncoeff_ridge = ridge.coef_\n\nlabels = req_col_names[:-1]\n\nx = np.arange(len(labels)) \nwidth = 0.3\n\nfig, ax = plt.subplots(figsize=(10,6))\nrects1 = ax.bar(x - 2*(width/2), coeff_lr, width, label='LR')\nrects2 = ax.bar(x, coeff_lasso, width, label='Lasso')\nrects3 = ax.bar(x + 2*(width/2), coeff_ridge, width, label='Ridge')\n\nax.set_ylabel('Coefficient')\nax.set_xlabel('Features')\nax.set_title('Feature Coefficients')\nax.set_xticks(x)\nax.set_xticklabels(labels, rotation=45)\nax.legend()\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{:.2f}'.format(height), xy=(rect.get_x() + rect.get_width() / 2, height),\n                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\nautolabel(rects1)\nautolabel(rects2)\nautolabel(rects3)\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lasso Regression, reduces the complexity of the model by keeping the coefficients as low as possible. Also, Coefficients with Linear and Ridge are almost same.\n\n##### Plotting predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12,4))\n\nax1.scatter(y_pred_lr, y_test, s=20)\nax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nax1.set_ylabel(\"True\")\nax1.set_xlabel(\"Predicted\")\nax1.set_title(\"Linear Regression\")\n\nax2.scatter(y_pred_lasso, y_test, s=20)\nax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nax2.set_ylabel(\"True\")\nax2.set_xlabel(\"Predicted\")\nax2.set_title(\"Lasso Regression\")\n\nax3.scatter(y_pred_ridge, y_test, s=20)\nax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nax3.set_ylabel(\"True\")\nax3.set_xlabel(\"Predicted\")\nax3.set_title(\"Ridge Regression\")\n\nfig.suptitle(\"True vs Predicted\")\nfig.tight_layout(rect=[0, 0.03, 1, 0.95])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the graphs between predicted and true values of the target variable, we can conclude that Linear and Ridge Regression perform well as the predictions are closer to the actual values. While Lasso Regression reduces the complexity at the cost of loosing performance in this case. (The closer the points are to the black line, the less the error is.)\n\n\n#### 2. Decision Trees\n\nAnother algorithm that would give better performance in this case would be Decision Trees, since we have a lot of zeros in some of the input features as seen from their distributions in the pair plot above. This would help the decision trees build trees based on some conditions on features which can further improve performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndtr = DecisionTreeRegressor()\n\ndtr.fit(X_train, y_train)\n\ny_pred_dtr = dtr.predict(X_test)\n\nprint(\"Model\\t\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Decision Tree Regressor \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_dtr)),mean_squared_error(y_test, y_pred_dtr),\n            mean_absolute_error(y_test, y_pred_dtr), r2_score(y_test, y_pred_dtr)))\n\nplt.scatter(y_test, y_pred_dtr)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Decision Tree Regressor\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Root Mean Squared Error (RMSE) has come down from 10.29 to 7.31, so the Decision Tree Regressor has improved the performance by a significant amount. This can be observed in the plot as well as more points are on the line.\n\n#### 3. Random Forest Regressor\n\nSince Using a Decision Tree Regressor has improved our performance, we can further improve the performance by ensembling more trees. Random Forest Regressor trains randomly initialized trees with random subsets of data sampled from the training data, this will make our model more robust"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=100)\n\nrfr.fit(X_train, y_train)\n\ny_pred_rfr = rfr.predict(X_test)\n\nprint(\"Model\\t\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Random Forest Regressor \\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_rfr)),mean_squared_error(y_test, y_pred_rfr),\n            mean_absolute_error(y_test, y_pred_rfr), r2_score(y_test, y_pred_rfr)))\n\nplt.scatter(y_test, y_pred_rfr)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Decision Tree Regressor\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The RMSE with Random Forest Regressor is now 5.11, we have reduced the error by ensembling multiple trees.\n\n#### 4. Multi Layer Perceptron\n\nA Multi Layer Perceptron or a Neural Network is capable of learning complex non linear functions mapping input features to target variable, which a linear model like a linear regression cannot do. A Decision tree is still a non linear representation of the data but it can fail to model minute relations between the features and targets. So, the Neural Networks will almost everytime give a better performance compared to the algorithms we have used in this notebook before.\n\nWe will use the sklearn MLPRegressor in this case, since the dataset is small there might not be a significant improvment in performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neural_network import MLPRegressor\n\nmlp = MLPRegressor(hidden_layer_sizes=(100,50), max_iter=1000)\n\nmlp.fit(X_train, y_train)\n\ny_pred_mlp = rfr.predict(X_test)\n\nprint(\"Model\\t\\t\\t\\t RMSE \\t\\t MSE \\t\\t MAE \\t\\t R2\")\nprint(\"\"\"Multi Layer Perceptron \\t\\t {:.2f} \\t\\t {:.2f} \\t\\t{:.2f} \\t\\t{:.2f}\"\"\".format(\n            np.sqrt(mean_squared_error(y_test, y_pred_mlp)),mean_squared_error(y_test, y_pred_mlp),\n            mean_absolute_error(y_test, y_pred_mlp), r2_score(y_test, y_pred_mlp)))\n\nplt.scatter(y_test, y_pred_mlp)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Decision Tree Regressor\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [lr, lasso, ridge, dtr, rfr, mlp]\nnames = [\"Linear Regression\", \"Lasso Regression\", \"Ridge Regression\", \n         \"Decision Tree Regressor\", \"Random Forest Regressor\", \"Multi Layer Perceptron\"]\nrmses = []\n\nfor model in models:\n    rmses.append(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n\nx = np.arange(len(names)) \nwidth = 0.3\n\nfig, ax = plt.subplots(figsize=(10,7))\nrects = ax.bar(x, rmses, width)\nax.set_ylabel('RMSE')\nax.set_xlabel('Models')\nax.set_title('RMSE with Different Algorithms')\nax.set_xticks(x)\nax.set_xticklabels(names, rotation=45)\nautolabel(rects)\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion\nRandom Forest Regressor is the best choice for this problem."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}