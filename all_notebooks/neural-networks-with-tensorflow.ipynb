{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport math\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"../input/fashion/fashionmnist/fashion-mnist_train.csv\")\ntest=pd.read_csv(\"../input/fashion/fashionmnist/fashion-mnist_test.csv\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()             #datayı incele","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()                     #datayı incele","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()                     #datayı incele","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[train[\"label\"]==1].append(train[train[\"label\"]==2]).append(train[train[\"label\"]==3]).append(train[train[\"label\"]==4]).append(train[train[\"label\"]==5])\ntest = test[test[\"label\"]==1].append(test[test[\"label\"]==2]).append(test[test[\"label\"]==3]).append(test[test[\"label\"]==4]).append(test[test[\"label\"]==5])                                                                                                                                        \nx_train = train.drop([\"label\"], axis=1).values.T\ny_train = train[[\"label\"]].values.reshape(-1,1).T\nx_test =  test.drop([\"label\"], axis=1).values.T    # LABELLARI AYIR ARRAYE DÖNÜŞTÜR \ny_test =  test[[\"label\"]].values.reshape(-1,1).T\ndef convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)].T\n    return Y\ny_train = convert_to_one_hot(y_train,10)\ny_test = convert_to_one_hot(y_test,10)\nprint(\"Shape of x_train\",x_train.shape)\nprint(\"Shape of y_train\",y_train.shape)\nprint(\"Shape of x_test\",x_test.shape)\nprint(\"Shape of y_test\",y_test.shape)              #Shape of DATA\nx_train = x_train/255\nx_test = x_test/255                     # NORMALIZE DATA\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_placeholders(n_x, n_y):\n    X = tf.placeholder(tf.float32, [n_x, None], name=\"X\")\n    Y = tf.placeholder(tf.float32, [n_y, None], name=\"Y\")\n    \n    return X, Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def initialize_params(layer_dims):\n    \n    parameters={}\n    for l in range(1,len(layer_dims)):\n        parameters[\"w\" + str(l)] = tf.get_variable(\"w\"+str(l), [layer_dims[l],layer_dims[l-1]], initializer = tf.contrib.layers.xavier_initializer())   \n        parameters[\"b\" + str(l)] = tf.get_variable(\"b\"+str(l), [layer_dims[l],1], initializer = tf.zeros_initializer())   \n    return parameters    \n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward_prop(x, parameters):\n    z_a_deposu = {}\n    z_a_deposu[\"A0\"] = x\n    for l in range(1,int(len(parameters)/2)+1):\n        z_a_deposu[\"z\" + str(l)] =  tf.add(tf.matmul(parameters[\"w\" + str(l)],z_a_deposu[\"A\" + str(l-1)] ), parameters[\"b\" + str(l)])        \n        if l == int(len(parameters)/2):\n            break\n        z_a_deposu[\"A\" + str(l)] = tf.nn.relu(z_a_deposu[\"z\" + str(l)])\n    zL =z_a_deposu[\"z\" + str(l)]\n    return zL","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def compute_cost(zL, y,parameters):\n    \n    logits = tf.transpose(zL)\n    labels = tf.transpose(y)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)) \n    \n    #L2 REGULARIZATION\n    num_parameters=len(parameters)\n    L =int(num_parameters/2)\n    reg = 0\n    for l in range(L):\n        reg += tf.nn.l2_loss(parameters[\"w\"+ str(l+1)])\n    LAMBDA = 0.01\n    cost = tf.reduce_mean(cost + LAMBDA*reg)\n    return cost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_mini_batches(X,Y, minibatch_size):\n    m = Y.shape[1]            # number of examples\n    \n    # Lets shuffle X and Y\n    permutation = list(np.random.permutation(m))            # shuffled index of examples\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation]\n    \n    minibatches = []                                        # we will append all minibatch_Xs and minibatch_Ys to this minibatch list \n    number_of_minibatches = int(m/minibatch_size)           # number of mini batches \n    \n    for k in range(number_of_minibatches):\n        minibatch_X = shuffled_X[:,k*minibatch_size: (k+1)*minibatch_size ]\n        minibatch_Y = shuffled_Y[:,k*minibatch_size: (k+1)*minibatch_size ]\n        minibatch_pair = (minibatch_X , minibatch_Y)                        #tuple of minibatch_X and miinibatch_Y\n        minibatches.append(minibatch_pair)\n    if m%minibatch_size != 0 :\n        last_minibatch_X = shuffled_X[:,(k+1)*minibatch_size: m ]\n        last_minibatch_Y = shuffled_Y[:,(k+1)*minibatch_size: m ]\n        last_minibatch_pair = (last_minibatch_X , last_minibatch_Y)\n        minibatches.append(last_minibatch_pair)\n    return minibatches","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(X_train, Y_train, X_test, Y_test, learning_rate = 1,\n          num_epochs = 1500, minibatch_size = 32):\n    tf.reset_default_graph()\n    n_x, m = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n    n_y = Y_train.shape[0]                            # n_y : output size\n    costs = []                                        # To keep track of the cost\n    \n    X, Y = create_placeholders(n_x, n_y)\n\n    parameters = initialize_params(layer_dims)\n    zL = forward_prop(X, parameters)\n\n    cost = compute_cost(zL, Y,parameters)\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n    init = tf.global_variables_initializer()\n    minibatches = random_mini_batches(X_train,Y_train, minibatch_size)\n    index = []\n    with tf.Session() as sess:\n        \n        sess.run(init)\n        \n        for epoch in range(num_epochs):\n            epoch_cost = 0\n            num_minibatches = int(m / minibatch_size)\n            for minibatch in minibatches:\n                minibatch_X , minibatch_Y = minibatch\n                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n                epoch_cost += minibatch_cost/num_minibatches\n                       \n            if epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))  # Print the cost every epoch\n            index.append(epoch)\n            costs.append(epoch_cost)\n                \n        # plot the cost\n        plt.plot(index , costs)\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # lets save the parameters in a variable\n        parameters = sess.run(parameters)\n        print(\"Parameters have been trained!\")\n\n        # Calculate the correct predictions\n        correct_prediction = tf.equal(tf.argmax(zL), tf.argmax(Y))\n\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n        print(\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n        print(\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n        \n        \n        return parameters","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layer_dims= [x_train.shape[0],25,12,10]  \n_ = model(x_train, y_train, x_test, y_test, learning_rate = 0.0001, num_epochs = 1500, minibatch_size = 32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}