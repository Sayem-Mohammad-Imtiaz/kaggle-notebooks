{"cells":[{"metadata":{"_uuid":"cf0a64944074fb1bd2153e7d9a4aa33f4eefe452"},"cell_type":"markdown","source":" # Titanic: Machine Learning from Disaster\n**Start here! Predict survival on the Titanic and get familiar with ML basics**\n Overview\n The data has been split into two groups:\n\n training set (train.csv)\n test set (test.csv)\n The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n\n The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\n We also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like."},{"metadata":{"_uuid":"3ae6a27d57cca4cebdd84237c303734a2f7c4b32"},"cell_type":"markdown","source":"# Step 1: Let's start by importing the data files and take a look at it"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nimport os\nwarnings.filterwarnings('ignore')\n# Add the complete dataset to the repository. The data is added to ../input/ directory\n!ls ../input/\n\n#Read the first 5 headers of the dataset \ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"846c27a8563eba7600ac52c1601799a82fdfd9dd"},"cell_type":"markdown","source":"looking at the data files:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"387fbee6307bff896195d5599589f59f2560a381","scrolled":false},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a50d272b734cc32a09cfbaa1c1b9c324970ba8af"},"cell_type":"markdown","source":"Looking at the dataset, we need to think about the features which could be useful to predict the survival. For this one can start by thinking about how the features are correlated with the survival. This can be done during data cleaning."},{"metadata":{"_uuid":"e786ea12dd2c546afb13c8426076ed8ab15c7f1c"},"cell_type":"markdown","source":"# 2: Data cleaning"},{"metadata":{"_uuid":"163f6005235b2100abb92436c11b3eee1ee305e0"},"cell_type":"markdown","source":"Evaluate if the data needs cleaning. First, checking if there are any missing values in the data"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"0eba058456ee8b2c7c51a37169e729a12bbfce3e"},"cell_type":"code","source":"NanExist = False\nif train.count().min() == train.shape[0] and test.count().min() == test.shape[0] :\n    print('There is no missing data!') \nelse:\n    NanExist = True\n    print('we have NAN!!!')\nif NanExist == True:\n    NumOfNan = pd.concat([train.isnull().sum(), test.isnull().sum()], axis=1, keys=['Train Data', 'Test Data']) \n    print(NumOfNan[NumOfNan.sum(axis=1) > 0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d49fc735ba94950151df7bc68ec252ba0a141b26"},"cell_type":"markdown","source":"Now we want to create array of the train and test data with the features only which we want to work with. Here **Age** is also one of the important parameter for prediction. It is very important to fix the missing values in **Age**. As mentioned above, it is not advicable to just replace the **Age** with 0 because large number (177) of **Age** values are missing and it will effect the Survival prediction. Here, we are going to use the **Name** where the title are giving some clue about the **Age**. For this first we need to extract title from the **Name** column. "},{"metadata":{"trusted":true,"_uuid":"a369a75d60dd3bf209a534fade8b67bd0f1513d1"},"cell_type":"code","source":"title_train = (train['Name'].str.split(',').str[1]).str.split('.').str[0]\ntitle_test = (test['Name'].str.split(',').str[1]).str.split('.').str[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"489104fcab76092e9eacde1993a1f4fa9aff81c2"},"cell_type":"markdown","source":"Based on the title the easiest way is to replace the missing  value of **Age** for  title with **Miss** and **Master** with 0 and rest of the title with 18"},{"metadata":{"trusted":true,"_uuid":"9b59c92acf8987cf330716c8b915b8665a7fb185"},"cell_type":"code","source":"for i in range(0,len(title_train)): #both have same dimension\n    if np.isnan(train['Age'][i]) == True:\n        if 'Miss' in title_train[i] or 'Master' in title_train[i]:\n            train['Age'][i] = 0\n        else:train['Age'][i] = 18\nfor i in range(0,len(title_test)): \n    if np.isnan(test['Age'][i]) == True:\n        if 'Miss' in title_test[i] or 'Master' in title_test[i]:\n            test['Age'][i] = 0\n        else:test['Age'][i] = 18\nsum(train[\"Age\"].isna()) # checking train\ntrain_orig = train.copy() # save the original data \nsum(test[\"Age\"].isna())  #checking test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2afafa14b3b60369a896c4b1ce92042223f627bb"},"cell_type":"markdown","source":"Here in this array  \"**Sex**\" and \"**Embarked**\" are categorical features and have strings instead of numeric values. We need to encode these strings into numeric data, so the algorithm can perform its calculations."},{"metadata":{"trusted":true,"_uuid":"4eb8a6626a6cd7c8eb5589535252d9fab6661da0"},"cell_type":"code","source":"train['Sex'] = train['Sex'].replace('male', 1)\ntrain['Sex'] = train['Sex'].replace('female', 2)\n\ntest['Sex'] = test['Sex'].replace('male', 1)\ntest['Sex'] = test['Sex'].replace('female', 2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56389a9e32364a7b7cef3810041cfc1d0db9fca7"},"cell_type":"markdown","source":"Similarly for **Embarked**, there are 2 missing values. Here, there are 3 categories in **Embarked**, so best is to fill in wiht the frequent port. "},{"metadata":{"trusted":true,"_uuid":"4bc40c33af48fce65aed9b36a7360ed24070ffc2"},"cell_type":"code","source":"fp = train['Embarked'].dropna().mode()[0]\ntrain['Embarked'] = train['Embarked'].fillna(fp)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5ad4a339e3d89784232d3894e0ff7cd6207c682"},"cell_type":"markdown","source":"Mapping the values for Embarked"},{"metadata":{"trusted":true,"_uuid":"ca2dc3f7fe1471f3ace8d03bd42e8fbb43e3d2b9"},"cell_type":"code","source":"train['Embarked'] = train['Embarked'].map({'S': 0, 'C':1,'Q':2}).astype(int)\ntest['Embarked'] = test['Embarked'].map({'S': 0, 'C':1,'Q':2}).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3e4c05c03051d728bcd795f0656057937a41822"},"cell_type":"markdown","source":"Now the data is resonably cleaned. Here, we are not using **Cabin** data so we can leave this column as it is.\nNext is to convert the DataFrames into array"},{"metadata":{"trusted":true,"_uuid":"737cafb4d38fb5dda15d101aabcad7172a78c5a6"},"cell_type":"code","source":"#Converting Pandas DataFrame to numpy arrays so that they can be used in sklearn\ntrain_feature = train[['Sex','Age','Pclass','SibSp','Parch','Embarked']].values\ntrain_class = train['Survived'].values\nfeature_names = ['Sex','Age','Pclass','SibSp','Parch','Embarked']\ntest_feature = test[['Sex','Age','Pclass','SibSp','Parch','Embarked']].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3294768ba2ee5273c1415ce0452f9924a93aa2c5"},"cell_type":"markdown","source":"# 3: Apply Classifier"},{"metadata":{"_uuid":"565e6855d4eefb702f04be4123d723c73e68d92a"},"cell_type":"markdown","source":"# **Logistic regression**"},{"metadata":{"trusted":true,"_uuid":"1864fe3fc761075063054472660764820b51e509"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf.fit(train_feature,train_class) # This is applying the fitting\n\ntest_predict = clf.predict(test_feature) # this is the predicted RESULT\ncv_score = clf.score(\n    train_feature,train_class)\ncv_score\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d36252356e4af91a48e111deb03e3c6f26b203c"},"cell_type":"markdown","source":"Try with polynomial fitting"},{"metadata":{"trusted":true,"_uuid":"48b7740d9c3fab39f28731e9a4bd82ded0cb9396"},"cell_type":"code","source":"from sklearn import preprocessing\npoly = preprocessing.PolynomialFeatures(degree=2)\npoly_train_feature = poly.fit_transform(train_feature)\npoly_test_feature = poly.fit_transform(test_feature)\nclassfier = LogisticRegression()\nclassifier_ = classfier.fit(poly_train_feature, train_class)\npoly_test_predict = classifier_.predict(poly_test_feature)\nprint(classifier_.score(poly_train_feature, train_class))\n#print(classifier_.score(poly_test_feature,poly_test_predict))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"806af5e25165372776d8f5b33d7a66c243e9881a"},"cell_type":"code","source":"LogReg_TestResult= pd.DataFrame({'PassengerId':test['PassengerId'], 'Survived':poly_test_predict})\nLogReg_TestResult.head()\nLogReg_TestResult.to_csv('PLogReg_TestResult.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17652471728878975a907db75e72a2a092f5d892"},"cell_type":"markdown","source":"# 2: Random Forest"},{"metadata":{"trusted":true,"_uuid":"534e1819886159b685e9503ff24e079c4cd12201"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n#This has to be improved\nrclf = RandomForestClassifier(criterion='gini',n_estimators=1000,\n                             min_samples_split=10,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)\nseed= 42\nrclf =RandomForestClassifier(n_estimators=1000, criterion='entropy', max_depth=5, min_samples_split=2,\n                           min_samples_leaf=1, max_features='auto',    bootstrap=False, oob_score=False, \n                           n_jobs=1, random_state=seed,verbose=0)\nrclf.fit(train_feature,train_class)\ntest_predict = rclf.predict(test_feature)\nprint(rclf.score(train_feature, train_class))\n#cv_score = rclf.score(test_feature,test_predict)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a98d33921cc1768516bb9b77a15728feeab6215d"},"cell_type":"code","source":"RandForst_TestResult= pd.DataFrame({'PassengerId':test['PassengerId'], 'Survived':test_predict})\nRandForst_TestResult.head()\nRandForst_TestResult.to_csv('RandForst_Test.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}