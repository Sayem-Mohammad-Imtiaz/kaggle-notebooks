{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# numpy\nimport numpy\nprint('numpy: %s' % numpy.__version__)\n# scipy\nimport scipy\nprint('scipy: %s' % scipy.__version__)\n# matplotlib\nimport matplotlib\nprint('matplotlib: %s' % matplotlib.__version__)\n# pandas\nimport pandas\nprint('pandas: %s' % pandas.__version__)\n# scikit-learn\nimport sklearn\nprint('sklearn: %s' % sklearn.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas import read_csv\ntrain_file = '../input/higgs-boson-data/higgs_train_10k.csv'\ntest_file = '../input/higgs-boson-data/higgs_test_5k.csv'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [\n    'response',\n    'x1',\n    'x2',\n    'x3',\n    'x4',\n    'x5',\n    'x6',\n    'x7',\n    'x8',\n    'x9',\n    'x10',\n    'x11',\n    'x12',\n    'x13',\n    'x14',\n    'x15',\n    'x16',\n    'x17',\n    'x18',\n    'x19',\n    'x20',\n    'x21',\n    'x22',\n    'x23',\n    'x24',\n    'x25',\n    'x26',\n    'x27',\n    'x28']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = read_csv(train_file, names=names)\ntest_data = read_csv(test_file, names=names)\nprint(train_data.shape)\nprint(test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# give the peek into the dataset\npeek = train_data.head(20)\nprint(peek)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# datatype of each feataure\ntypes = train_data.dtypes\nprint(types)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#base statistics for data\nfrom pandas import set_option\nset_option('display.width', 100)\nset_option('precision', 5)\ndescription = train_data.describe()\nprint(description)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class distribution for train and test\ntrain_data_class = train_data.groupby('response').size()\nprint(train_data_class)\ntest_data_class = test_data.groupby('response').size()\nprint(test_data_class)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pearsons correlation to understand feature independence\ncorrelations = train_data.corr(method='pearson')\nprint(correlations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization of correlations\nimport matplotlib.pyplot as pyplot\nfig = pyplot.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = numpy.arange(0,29,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names)\nax.set_yticklabels(names)\npyplot.rcParams['figure.figsize'] = (27,27)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualization using pyplot and histograms of training and testing data\nfrom matplotlib import pyplot\npyplot.rcParams['figure.figsize'] = (13,13)\ntrain_data.hist()\npyplot.show()\ntest_data.hist()\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# boxplot visualization of train and test data\npyplot.rcParams['figure.figsize'] = (12,12)\ntrain_data.plot(kind='box', subplots=True, layout=(6,6), sharex=False, sharey=False)\npyplot.show()\ntest_data.plot(kind='box', subplots=True, layout=(6,6), sharex=False, sharey=False)\npyplot.rcParams['figure.figsize'] = (12,12)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train data\ntrain_array = train_data.values\n# separate array into input and output variables\nX_train = train_array[:,1:28]\ny_train = train_array[:,0]\n# test data\ntest_array = test_data.values\n# separate array into input and output variables\nX_test = test_array[:,1:28]\ny_test = test_array[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import time\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\n\nfrom sklearn import manifold\nmethods = ['standard', 'ltsa', 'hessian', 'modified']\nlabels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']\n\nn_neighbors = 10\nn_components = 2\ncolor=y_train\n\nfor i, method in enumerate(methods):\n    t0 = time()\n    Ytransformed = manifold.Isomap(n_neighbors, n_components).fit_transform(X_train)\n    t1 = time()\n    print(\"Isomap: %.2g sec\" % (t1 - t0))\n    ax = fig.add_subplot(257)\n    plt.scatter(Ytransformed[:, 0], Ytransformed[:, 1],c=color, cmap=plt.cm.Spectral)\n    plt.title(labels[i])\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    plt.axis('tight')\n    plt.show()\n\n\nt0 = time()\nmds = manifold.MDS(n_components, max_iter=100, n_init=1)\nYtransformed = mds.fit_transform(X_train)\nt1 = time()\nprint(\"MDS: %.2g sec\" % (t1 - t0))\nax = fig.add_subplot(258)\nplt.scatter(Ytransformed[:, 0], Ytransformed[:, 1], c=color,cmap=plt.cm.Spectral)\nplt.title(\"MDS (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\nplt.show()\n\n\nt0 = time()\nse = manifold.SpectralEmbedding(n_components=n_components,\n                                n_neighbors=n_neighbors)\nYtransformed = se.fit_transform(X_train)\nt1 = time()\nprint(\"SpectralEmbedding: %.2g sec\" % (t1 - t0))\nax = fig.add_subplot(259)\nplt.scatter(Ytransformed[:, 0], Ytransformed[:, 1], c=color,cmap=plt.cm.Spectral)\nplt.title(\"SpectralEmbedding (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\nplt.show()\n\nt0 = time()\ntsne = manifold.TSNE(n_components=n_components, init='pca', random_state=0)\nYtransformed = tsne.fit_transform(X_train)\nt1 = time()\nprint(\"t-SNE: %.2g sec\" % (t1 - t0))\nax = fig.add_subplot(2, 5, 10)\nplt.scatter(Ytransformed[:, 0], Ytransformed[:, 1], c=color,cmap=plt.cm.Spectral)\nplt.title(\"t-SNE (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature decomposition  with PCA\nfrom sklearn.decomposition import PCA\n# feature extraction\npca = PCA(n_components=2)\nfit = pca.fit(X_train)\nprojected = pca.fit_transform(X_train)\n\npyplot.scatter(projected[:, 0], projected[:, 1],\n               c=y_train, edgecolor='none', alpha=0.5)\npyplot.xlabel('PCA component 1')\npyplot.ylabel('PCA component 2')\npyplot.rcParams['figure.figsize'] = (8, 8)\npyplot.colorbar()\npyplot.show()\npca = PCA(n_components=25)\nfit = pca.fit(X_train)\npyplot.plot(numpy.cumsum(fit.explained_variance_ratio_))\npyplot.xlabel('number of components')\npyplot.ylabel('cumulative explained variance')\npyplot.show()\n# summarize components\nprint(\"Explained Variance: %s\" % fit.explained_variance_ratio_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn.feature_selection import chi2\n\nmin_max_scaler = preprocessing.MinMaxScaler()\nscaler = min_max_scaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nchi2_score = chi2(X_train_scaled, y_train)[0]\nfeatures = [\n    'x1',\n    'x2',\n    'x3',\n    'x4',\n    'x5',\n    'x6',\n    'x7',\n    'x8',\n    'x9',\n    'x10',\n    'x11',\n    'x12',\n    'x13',\n    'x14',\n    'x15',\n    'x16',\n    'x17',\n    'x18',\n    'x19',\n    'x20',\n    'x21',\n    'x22',\n    'x23',\n    'x24',\n    'x25',\n    'x26',\n    'x27',\n    'x28']\nfscores = zip(features, chi2_score)\nwchi2 = sorted(fscores, key=lambda x: x[1], reverse=True)\nscores_labels = numpy.asarray(wchi2)\nprint(scores_labels)\nlabel = [row[0] for row in scores_labels]\nprint(label)\nscore = [row[1] for row in scores_labels]\nprint(score)\ny_pos = numpy.arange(len(score))\nyrange = range(len(score))\nprint(yrange)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# perform grid search to find the best parameter for Logistic Regression,\n# Perceptron, Naive Bayes, LDA algorithm\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\n\n# using roc AUC as scoring\nscoring = 'accuracy'\n\n# Naive Bayes\nnaiveBayes = GaussianNB()\nnbscore = cross_val_score(naiveBayes, X_train, y_train, cv=3, scoring=scoring)\nprint('Naive Bayes CV score =', np.mean(nbscore))\n\n\n# penalty\npenalties = numpy.array(['l1', 'l2'])\n# C for logistic regression\nc_values = numpy.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001])\n# max iteration\niters = numpy.array([100, 150])\nLR_param_grid = {'penalty': penalties, 'C': c_values, 'max_iter': iters}\n\n# logistic regression as algorithm\ngridLogisticRegression = LogisticRegression()\n# Using GridSearchCV on Training Data for LR\ngrid = GridSearchCV(\n    estimator=gridLogisticRegression,\n    param_grid=LR_param_grid,\n    scoring=scoring)\ngrid.fit(X_train, y_train)\nprint('LR CVScore ', grid.best_score_)\nprint('LR Penalty', grid.best_estimator_.penalty)\nprint('LR C', grid.best_estimator_.C)\nprint('LR Max Iterations', grid.best_estimator_.max_iter)\n\n\n# Perceptron\n# Using GridSearchCV on Training Data for perceptron\n# alphas\nalphas = numpy.array([0.001, 0.0001, 0.00001, 0.000001])\n# iterations\npereptorn_param_grid = {'alpha': alphas, 'max_iter': iters}\ngrid = GridSearchCV(\n    estimator=Perceptron(),\n    param_grid=pereptorn_param_grid,\n    scoring=scoring)\ngrid.fit(X_train, y_train)\nprint('Perceptron CVScore ', grid.best_score_)\nprint('Perceptron alpha', grid.best_estimator_.alpha)\nprint('Perceptron Max Iterations', grid.best_estimator_.max_iter)\n\n# LDA\ntols = numpy.array([0.001, 0.00001, 0.001])\nlda_param_grid = {'tol': tols}\ngrid = GridSearchCV(\n    estimator=LinearDiscriminantAnalysis(),\n    param_grid=lda_param_grid,\n    scoring=scoring)\ngrid.fit(X_train, y_train)\nprint('LDA CVScore ', grid.best_score_)\nprint('LDA tol', grid.best_estimator_.tol)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nimport numpy\n# gamma parameter in SVM\ngammas = numpy.array([1, 0.1, 0.01, 0.001])\n# C for logistic regression\nc_values = numpy.array([100, 1, 0.1, 0.01])\nsvm_param_grid = {'gamma': gammas, 'C': c_values}\nsvm = SVC(kernel='rbf')\nscoring = 'accuracy'\ngrid = GridSearchCV(estimator=svm, param_grid=svm_param_grid, scoring=scoring)\ngrid.fit(X_train, y_train)\nprint(grid.best_score_)\nprint(grid.best_estimator_.gamma)\nprint(grid.best_estimator_.C)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Modified the Code for changes\n# Original Authors: Robert McGibbon, Joel Nothman, Guillaume Lemaitre\n\n\nfrom __future__ import print_function, division\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn import preprocessing\n\n# transform the features using MinMaxScaler as many are negatives\nmin_max_scaler = preprocessing.MinMaxScaler()\nscaler = min_max_scaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\n\nprint(__doc__)\n\npipe = Pipeline([\n    ('reduce_dim', PCA()),\n    ('classify', LogisticRegression())\n])\n\nN_FEATURES_OPTIONS = [10, 15, 20]\nC_OPTIONS = [0.001, 0.1, 1, 10, 100, 1000]\nmax_iter_OPTIONS = [100, 150]\nparam_grid = [\n    {\n        'reduce_dim': [PCA(iterated_power=10)],\n        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n        'classify__C': C_OPTIONS,\n        'classify__max_iter':max_iter_OPTIONS\n    },\n    {\n        'reduce_dim': [SelectKBest(chi2)],\n        'reduce_dim__k': N_FEATURES_OPTIONS,\n        'classify__C': C_OPTIONS,\n        'classify__max_iter':max_iter_OPTIONS\n    },\n]\nreducer_labels = ['PCA', 'KBest(chi2)']\n\ngrid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)\ngrid.fit(X_train_scaled, y_train)\n\nmean_scores = np.array(grid.cv_results_['mean_test_score'])\n# scores are in the order of param_grid iteration, which is alphabetical\nmean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n# select score for best C\nmean_scores = mean_scores.max(axis=0)\nbar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *\n               (len(reducer_labels) + 1) + .5)\n\nplt.figure()\nCOLORS = ['tomato', 'darkolivegreen', 'lightsteelblue']\nfor i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n    plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])\n\nplt.title(\"Comparing feature reduction techniques\")\nplt.xlabel('Reduced number of features')\nplt.xticks(bar_offsets + len(reducer_labels) / 2, N_FEATURES_OPTIONS)\nplt.ylabel('Classification accuracy')\nplt.ylim((0, 1))\nplt.legend(loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# learning curves\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n\n\ndef plot_learning_curve(estimator, name, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title('Learning Curves for ' + name)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"No. Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"b\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"b\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\nestimator = LogisticRegression(C=0.1, penalty='l1', max_iter=100)\nplot_learning_curve(estimator, 'Tuned Logistic Regression', X_train, y_train)\nplt.rcParams['figure.figsize'] = (7, 7)\nplt.show()\nestimator = SVC(C=100, gamma=0.01, kernel='rbf')\nplot_learning_curve(estimator, 'Tuned SVM', X_train, y_train)\nplt.rcParams['figure.figsize'] = (7, 7)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}