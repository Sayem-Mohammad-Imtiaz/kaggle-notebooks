{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Standardization & Normalization\n\nTransformation is a highly essential step required for any data science project. There is a significant boost in terms of performance of a model from transformation. \n\nFor eg, Linear Regression, internal concept of Gradient Decent is used to determine the Global Minima. Where the objective is to identify the coefficient slopes/parameters which helps smoother derivition. \n\nKNN, works of Euclidian distance which tries to find the nearest points to either classify or solve regression problems. K-Means/Heirarchical Clustering also uses Eucledian distance. \nFor instance, if a dataset consists of 2 features as below:\n\n|#|Feature 1|Feature 2|\n|---|----|----|\n|0. |24|56|\n|1. |21|100|\n|2. |45|67|\n\nWhere (x1,y1) = (24,56)\nand (x2,y2) = (21,100)\n\nEvery points have direction and vector.\n\nScaling will be performed in order to make the processing much faster. \nAn eg of such scaling includes MinMax/Standard Scaler. \n\nOn the other hand, for Tree/Ensemble based techniques, transformation is not necessarily require.\n\nFor Deep Learning Techniques such as ANN, CNN, or RNN scaling, standardization or normalization is also a must. \n\n---\n### Types of Transformation\n---\n\n1. Normalisation and Standardization\n2. Scaling to Minimum and Maximum Values\n3. Scaling to Median and Quantiles\n4. Gaussian Transformation \n    * a. Logarithmic Transformation\n    * b. Reciprocal Transformation\n    * c. Square Root Transformation\n    * d. Exponential Transformation\n    * e. Box Cox Transformation"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/titanicdataset-traincsv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf = pd.read_csv('../input/titanicdataset-traincsv/train.csv', usecols=['Pclass', 'Age', 'Fare', 'Survived'])\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1. Standardization\n\nWhere all features and variables are brought to a similar scale. By centering the variables at zero.\n\nz = (x - (x_mean))/std\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#NOT A GOOD PRACTISE\n\ndf['Age'].fillna(df.Age.median(),inplace=True)\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Performing standardization\n#Using StandardScaler from Scikit-Learn\n\nfrom sklearn.preprocessing import StandardScaler\nscalar = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"|fit|fit_transform|\n|----|----|\n|Used for training|When data is required to modified where the features are 'transformed'|\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_scaled = scalar.fit_transform(df)\ndf_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(df_scaled)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The transformation is taking placed based on each distinct field"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['Pclass'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_scaled[:,1],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['Age'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_scaled[:,2],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['Fare'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_scaled[:,3],bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. MinMax Scaling\nAim of this scaling is to transform/scale the values between 0 to 1 / 0 and 1\nWorks well with DL techniques such as CNN\n\nX_scaled = ((X - X.min) / (X.max-X.min))"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmin_max = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_min_max = pd.DataFrame(min_max.fit_transform(df), columns=df.columns)\ndf_min_max","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['Pclass'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_min_max['Pclass'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['Age'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_min_max['Age'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['Fare'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_min_max['Fare'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Robust Scaler\nUsed to scale features according to median and quantiles (IQR)\n\nBest in presence of outliers.\n\nIQR = Q3 - Q1\nX_scaled = (X-X.median)/IQR"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_robust_scaler = pd.DataFrame(scaler.fit_transform(df),columns = df.columns)\ndf_robust_scaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['Pclass'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_robust_scaler['Pclass'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['Age'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_robust_scaler['Age'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df['Fare'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_robust_scaler['Fare'],bins=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. Gaussian Transformation \n\nThe main purpose of using this type of transformation is that ML algorithms such as Linear and Logistic assumes that the features are normally distributed. From normally distributed data, they provide better performance. Hence, the following techniques are applied in order to make the features normally/Gaussian distributed.\n\n* a. Logarithmic Transformation\n* b. Reciprocal Transformation\n* c. Square Root Transformation\n* d. Exponential Transformation\n* e. Box Cox Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Pclass', inplace = True, axis = 1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"QQ Plots are used in order to check whether a distribution is normal or Gaussian"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy.stats as stat\nimport pylab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_data(df,feature):\n    plt.figure(figsize = (10,6))\n    #(1st row, 2nd column, 1st index)\n    plt.subplot(1,2,1)\n    df[feature].hist()\n    #(1st row, 2nd column, 2nd index)\n    plt.subplot(1,2,2)\n    stat.probplot(df[feature],dist='norm',plot=pylab)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"--- \n\nAge\n---\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(df, 'Age')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the plot points fall within the red lines, then the feature is normally distributed"},{"metadata":{},"cell_type":"markdown","source":"#### a. Logarithmic Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Age_log'] = np.log(df['Age'])\nplot_data(df,'Age_log')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### b. Reciprocal Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Age_reciprocal'] = 1/df['Age']\nplot_data(df,'Age_reciprocal')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### c. Square Root Transformation\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Age_squared'] = df.Age**(1/2)\nplot_data(df,'Age_squared')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### d. Exponential Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Age_exp'] = df.Age**(1/1.2)\nplot_data(df, 'Age_exp')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### e. Box Cox Transformation\n\nThe Box-Cox transformation is defined as:\n\nT(Y)=(Y exp(λ)−1)/λ\n\nwhere Y is the response variable and λ is the transformation parameter. λ varies from -5 to 5. In the transformation, all values of λ are considered and the optimal value for a given variable is selected.\n\n\nλ = parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['AgeBcox'],parameters = stat.boxcox(df['Age'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(parameters)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(df, 'AgeBcox')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"--- \n\nFare\n---\n---"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data(df,'Fare')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Fare_log'] = np.log1p(df['Fare'])\nplot_data(df,'Fare_log')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Fare_exp'] = df.Fare**(1/1.2)\nplot_data(df, 'Fare_exp')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Fare_squared'] = df.Fare**(1/2)\nplot_data(df,'Fare_squared')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['FareBcox'],parameters = stat.boxcox(df['Fare']+1)\nplot_data(df,'FareBcox')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}