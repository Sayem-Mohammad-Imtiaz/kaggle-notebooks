{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Day 9**\n\n**Logistic Regression**\n\nIt is one of the popular ML Algorithm used in the case of predicting various categorical datasets","metadata":{}},{"cell_type":"markdown","source":"**Problem Statement**\n\nClassification of Iris Flower's using Logistic Regression","metadata":{}},{"cell_type":"code","source":"#lets import the libraries\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom math import ceil\n\n#plots\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n#algorithms\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom pandas.plotting import parallel_coordinates\n\n#Advanced Optimization\nfrom scipy import optimize as op","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets load the dataset\niris = pd.read_csv('../input/data-science-machine-learning-and-ai-using-python/Iris.csv')\niris.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets visualize the species\n#plot the species with respect to sepal length\nsepalPlt = sb.FacetGrid(iris, hue='Species',size=6).map(plt.scatter, \"SepalLengthCm\", \"SepalWidthCm\")\nplt.legend(loc='upper left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot the species with respect to petal length\npetalPlt = sb.FacetGrid(iris, hue='Species',size=6).map(plt.scatter, \"PetalLengthCm\", \"PetalWidthCm\")\nplt.legend(loc='upper left')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let splot parallel coordinates of the petal and sepal\nparallel_coordinates(iris.drop(\"Id\", axis=1), \"Species\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets setup the data for training our model\nspecies = ['Iris-setosa','Iris-versicolor','Iris-virginica']\n\n#number of examples\nm = iris.shape[0]\n\n#features\nn = 4\n\n#number of classes\nk = 3\n\nX = np.ones((m, n+1))\nY = np.array((m,1))\n\nX[:,1] = iris['PetalLengthCm']\nX[:,2] = iris['PetalWidthCm']\nX[:,3] = iris['SepalLengthCm']\nX[:,4] = iris['SepalWidthCm']\n\n#lets provide labels\nY = iris['Species']\n\n#mean normalization\nfor j in range(n):\n  X[:,j] = (X[:,j] - X[:,j].mean())\n\n#lets split dataset\nX_train,X_test, Y_train,Y_test = train_test_split(X,Y, test_size=0.2, random_state=11)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Logistic Regression\ndef sigmoid(z):\n  return 1.0/(1 + np.exp(-z))\n\n#Regularised cost functions\ndef regCostFunction(theta, X, Y, _lambda = 0.1):\n  m = len(Y)\n  h = sigmoid(X.dot(theta))\n  reg = (_lambda/(2*m) * np.sum(theta **2))\n\n  return ((1/m) * (-Y.T.dot(np.log(h)) -(1-Y).T.dot(np.log(1-h))) + reg)\n\ndef regGradient(theta, X, Y, _lambda = 0.1):\n  m,n = X.shape\n  theta = theta.reshape((n,1))\n  Y = Y.reshape((m,1))\n  h = sigmoid(X.dot(theta))\n  reg = _lambda * theta / m\n\n  return ((1/m) * X.T.dot(h-Y)) + reg\n\n#Optimal Theta\ndef logisticRegression(X,Y,theta):\n  res = op.minimize(fun = regCostFunction, x0 = theta, args = (X,Y), method='TNC', jac=regGradient)\n\n  return res.x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets train our model\nall_theta = np.zeros((k, n+1))\n\n#one vs all\ni = 0\nfor flower in species:\n  #set the labels 0 and 1\n  tmp_y = np.array(Y_train == flower, dtype= int)\n  optTheta = logisticRegression(X_train, tmp_y, np.zeros((n + 1, 1)))\n  all_theta[i] = optTheta\n  i += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets make predictions\nP = sigmoid(X_test.dot(all_theta.T))  #probability for each flower\np = [species[np.argmax(P[i, :])] for i in range(X_test.shape[0])]\n\n\n#lets print the accuracy\nprint(\"Test Accuracy : \", accuracy_score(Y_test, p) * 100, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Decision Tree**\n\nA tree shaped algorithm to find the coarse of action. Each node in the tree represnt a action.","metadata":{}},{"cell_type":"markdown","source":"**Problem Statement**\n\nUse Ml to Predict the selling price of houses baesd on some economic factors by using Decision Tree Model","metadata":{}},{"cell_type":"code","source":"#lets import the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets initialise the dataset\nboston = pd.read_csv('../input/data-science-machine-learning-and-ai-using-python/Boston.csv')\n\n#lets view the dataset\nboston.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets visualize the dataset using scatter plot\nx = boston['rm']\ny = boston['medv']\n\n#plot the scatter plot\nplt.scatter(x,y, color='g')\nplt.xlabel('Avg rooms per dwelling')\nplt.ylabel('Median values of the home')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now lets define the feature variable and target variable of our dataset\nX = pd.DataFrame(x)  #feature variable\nY = pd.DataFrame(y)  #target variable","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets divide the data into training set and test set\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train,Y_test = train_test_split(X,Y,test_size=0.20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Building the model with Decision Tree Regressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nregressor = DecisionTreeRegressor(criterion='mse',random_state=100, max_depth=4, min_samples_leaf = 1)\n\n#train the model\nregressor.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let visulaize the tree using graphviz\nfrom sklearn.tree import export_graphviz\n\nexport_graphviz(regressor, out_file = 'regression_tree.dot')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets predict the values\ny_pred = regressor.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets print the values\nprint(y_pred[4:9])\nprint(Y_test[4:9])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets find out the rmse value\nfrom sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_pred, Y_test)\nrmse = np.sqrt(mse)\nprint(rmse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}