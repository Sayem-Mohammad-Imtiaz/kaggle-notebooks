{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"pcpath = \"../input/california-house-price/housing_train.csv\"\n\nimport numpy as np\nimport pandas as pd\n# loading from url cause errors\ndata = pd.read_csv(pcpath, sep=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pcpath = \"../input/california-house-price/housing_train.csv\"\ntest_data = pd.read_csv(test_pcpath, sep=',')\ntest_data.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_anspath = \"../input/california-house-price/housing_test_sample_solution.csv\"\ntest_ans = pd.read_csv(test_anspath)\ntest_ans.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"ocean_proximity\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('~%.2f%% of data is corrupted' % ((data.notna().sum(axis=1) < 10).sum(axis=0) / data.shape[0] * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in data.keys()[1:]:\n    print(i, (data[i].notna() == False).sum(axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = data[data.notna().sum(axis=1) == 10]\ntrain_data[\"ocean_proximity\"] = pd.factorize(train_data[\"ocean_proximity\"])[0]\ntrain_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = np.asarray(train_data.corr())\ncorr_matrix = a - (np.indices((10, 10))[0] == np.indices((10, 10))[1]).astype(np.float32)\nprint(np.round(np.max(corr_matrix, axis=0), decimals=2))\nprint(np.round(np.min(corr_matrix, axis=0), decimals=2))\ntrain_data.corr().round(decimals=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nall_other_keys = list(filter(lambda x: x not in  ['median_house_value'], train_data.keys()))\na = []\nfor i in all_other_keys:\n    lnreg = LinearRegression(fit_intercept = True, normalize=True, n_jobs=-1)\n    train_len = int(train_data.shape[0] * 9 / 10)\n    lnreg.fit(np.asarray(train_data[i][:train_len]).reshape(-1, 1), np.asarray(train_data['median_house_value'][:train_len]))\n    a += [np.sqrt(np.mean(np.square(np.asarray(train_data['median_house_value'][train_len:]) -lnreg.predict(np.asarray(train_data[i][train_len:]).reshape(-1, 1)))))]\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20, 7))\nplt.plot(a, 'o')\nplt.xticks(np.arange(len(all_other_keys)), all_other_keys)\nplt.show()\nprint(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ols(X, Y):\n    return np.linalg.inv(X.T @ X) @ X.T @ Y\ndef errorl2(X, Y, w):\n    X, Y, w = np.array(X), np.array(Y), np.array(w)\n    return np.mean(np.square(X @ w - Y))\ndef errorl1(X, Y, w):\n    X, Y, w = np.array(X), np.array(Y), np.array(w)\n    return np.mean(np.abs(X @ w - Y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"other_keys = train_data.keys()[train_data.keys() != 'total_bedrooms']\nN = train_data.shape[0]\n_X = train_data[other_keys]\nY = train_data['total_bedrooms']\nX = _X\n\nprint('all keys %.2f %.2f' % (errorl2(X, Y, ols(X, Y)), errorl1(X, Y, ols(X, Y))))\n__X = np.array(_X)\nfor i in range(9):\n    X = __X[:,i:i+1]\n    print(other_keys[i], '%.2f %.2f' % (errorl2(X, Y, ols(X, Y)), errorl1(X, Y, ols(X, Y))))\nX = _X[['total_rooms', 'population', 'households']]\nX = np.array(X)\nprint('total_rooms, population, households %.2f %.2f' % (errorl2(X, Y, ols(X, Y)), errorl1(X, Y, ols(X, Y))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_d = test_data[test_data.keys()[1:]]\ntest_d[\"ocean_proximity\"] = pd.factorize(test_d[\"ocean_proximity\"])[0]\nother_keys = test_d.keys()[test_d.keys() != 'total_bedrooms']\nbad_rows = test_d.notna().sum(axis=1) < 9\ngood_rows = bad_rows == False\nw = ols(test_d.loc[good_rows][other_keys], test_d.loc[good_rows]['total_bedrooms'])\ntest_d.loc[bad_rows, 'total_bedrooms'] = np.array(test_d.loc[bad_rows][other_keys]) @ w\nx_test = np.array(test_d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error, r2_score\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"other_keys = train_data.keys()[train_data.keys() != \"median_house_value\"]\nx_train, y_train = np.array(train_data[other_keys]), np.array(train_data['median_house_value'])\nx_test = np.array(test_d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport xgboost as xgb\n\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import StandardScaler\nfrom geopy.distance import geodesic\n\nimport gc\n\nhousing = pd.read_csv(r\"../input/california-house-price/housing_train.csv\")\ncity_lat_long = pd.read_csv(r\"../input/california-housing-feature-engineering/cal_cities_lat_long.csv\")\ncity_pop_data = pd.read_csv(r\"../input/california-housing-feature-engineering/cal_populations_city.csv\")\ncounty_pop_data = pd.read_csv(r\"../input/california-housing-feature-engineering/cal_populations_county.csv\")\n\ncity_coords = {}\n\nfor dat in city_lat_long.iterrows():\n    row = dat[1]\n    if row['Name'] not in city_pop_data['City'].values:   \n        continue           \n    else: \n        city_coords[row['Name']] = (float(row['Latitude']), float(row['Longitude']))\n        \nnewport_ri = (41.49008, -71.312796)\ncleveland_oh = (41.499498, -81.695391)\nx = geodesic(newport_ri, cleveland_oh)\nx #distance stored in km, see units on printing\nprint(x)\ntype(x.kilometers) #is it a float?\n\ndef closest_point(location, location_dict):\n    \"\"\" take a tuple of latitude and longitude and \n        compare to a dictonary of locations where\n        key = location name and value = (lat, long)\n        returns tuple of (closest_location , distance) \"\"\"\n    closest_location = None\n    for city in location_dict.keys():\n        distance = geodesic(location, location_dict[city]).kilometers\n        if closest_location is None:\n            closest_location = (city, distance)\n        elif distance < closest_location[1]:\n            closest_location = (city, distance)\n    return closest_location\n  \ntest = (39.524325, -122.293592) #likely 'Willows'\nclosest_point(test, city_coords)\n\ncity_pop_dict = {}\nfor dat in city_pop_data.iterrows():\n\trow = dat[1]\n\tcity_pop_dict[row['City']] =  row['pop_april_1990']\n\n\nbig_cities = {}\n\nfor key, value in city_coords.items():\n    if city_pop_dict[key] > 500000:\n        big_cities[key] = value\n    \nfrom pandas import Series\n\ndef progress_coroutine(print_on = 10000):\n    print(\"Starting progress monitor\")\n\n    iterations = 0\n    while True:\n        yield\n        iterations += 1\n        if (iterations % print_on == 0):\n            print(\"{} iterations done\".format(iterations))\n\ndef percentage_coroutine(to_process, print_on_percent = 0.001):\n    print(\"Starting progress percentage monitor\")\n\n    processed = 0\n    count = 0\n    print_count = to_process*print_on_percent\n    while True:\n        yield\n        processed += 1\n        count += 1\n        if (count >= print_count):\n            count = 0\n            pct = (float(processed)/float(to_process) * 100)\n\n            print(\"{}% finished\".format(pct))\n\ndef trace_progress(func, progress = None):\n    def callf(*args, **kwargs):\n        if (progress is not None):\n            progress.send(None)\n\n        return func(*args, **kwargs)\n\n    return callf\n\n\ndef f(x):\n    return closest_point((x['latitude'],x['longitude']),city_coords)\nco2 = percentage_coroutine(len(housing))\nco2.__next__()\n\nhousing['close_city'] = housing.apply(trace_progress(f, progress = co2), axis = 1)\nhousing['close_city_name'] = [x[0] for x in housing['close_city'].values]\nhousing['close_city_dist'] = [x[1] for x in housing['close_city'].values]\nhousing['close_city_pop'] = [city_pop_dict[x] for x in housing['close_city_name'].values]\n\nhousing = housing.drop('close_city', axis=1)\nhousing.head()\n\n\n#add the data relating to the points to the closest big city\ndef g(x):\n    return closest_point((x['latitude'],x['longitude']),big_cities)\nco2 = percentage_coroutine(len(housing))\nco2.__next__()\nhousing['big_city'] = housing.apply(trace_progress(g, progress = co2), axis = 1)\nhousing['big_city_name'] = [x[0] for x in housing['big_city'].values]\nhousing['big_city_dist'] = [x[1] for x in housing['big_city'].values]\n\nhousing = housing.drop('big_city', axis=1)\n\nhousing.to_csv(r\"./new_geo_data_train.csv\")\n_new_geo_data_train = housing.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing = pd.read_csv(r\"../input/california-house-price/housing_test.csv\")\ncity_lat_long = pd.read_csv(r\"../input/california-housing-feature-engineering/cal_cities_lat_long.csv\")\ncity_pop_data = pd.read_csv(r\"../input/california-housing-feature-engineering/cal_populations_city.csv\")\ncounty_pop_data = pd.read_csv(r\"../input/california-housing-feature-engineering/cal_populations_county.csv\")\n\ncity_coords = {}\n\nfor dat in city_lat_long.iterrows():\n    row = dat[1]\n    if row['Name'] not in city_pop_data['City'].values:   \n        continue           \n    else: \n        city_coords[row['Name']] = (float(row['Latitude']), float(row['Longitude']))\n        \nnewport_ri = (41.49008, -71.312796)\ncleveland_oh = (41.499498, -81.695391)\nx = geodesic(newport_ri, cleveland_oh)\nx #distance stored in km, see units on printing\nprint(x)\ntype(x.kilometers) #is it a float?\n\ndef closest_point(location, location_dict):\n    \"\"\" take a tuple of latitude and longitude and \n        compare to a dictonary of locations where\n        key = location name and value = (lat, long)\n        returns tuple of (closest_location , distance) \"\"\"\n    closest_location = None\n    for city in location_dict.keys():\n        distance = geodesic(location, location_dict[city]).kilometers\n        if closest_location is None:\n            closest_location = (city, distance)\n        elif distance < closest_location[1]:\n            closest_location = (city, distance)\n    return closest_location\n  \ntest = (39.524325, -122.293592) #likely 'Willows'\nclosest_point(test, city_coords)\n\ncity_pop_dict = {}\nfor dat in city_pop_data.iterrows():\n\trow = dat[1]\n\tcity_pop_dict[row['City']] =  row['pop_april_1990']\n\n\nbig_cities = {}\n\nfor key, value in city_coords.items():\n    if city_pop_dict[key] > 500000:\n        big_cities[key] = value\n    \nfrom pandas import Series\n\ndef progress_coroutine(print_on = 10000):\n    print(\"Starting progress monitor\")\n\n    iterations = 0\n    while True:\n        yield\n        iterations += 1\n        if (iterations % print_on == 0):\n            print(\"{} iterations done\".format(iterations))\n\ndef percentage_coroutine(to_process, print_on_percent = 0.001):\n    print(\"Starting progress percentage monitor\")\n\n    processed = 0\n    count = 0\n    print_count = to_process*print_on_percent\n    while True:\n        yield\n        processed += 1\n        count += 1\n        if (count >= print_count):\n            count = 0\n            pct = (float(processed)/float(to_process) * 100)\n\n            print(\"{}% finished\".format(pct))\n\ndef trace_progress(func, progress = None):\n    def callf(*args, **kwargs):\n        if (progress is not None):\n            progress.send(None)\n\n        return func(*args, **kwargs)\n\n    return callf\n\n\ndef f(x):\n  return closest_point((x['latitude'],x['longitude']),city_coords)\nco2 = percentage_coroutine(len(housing))\nco2.__next__()\n\nhousing['close_city'] = housing.apply(trace_progress(f, progress = co2), axis = 1)\nhousing['close_city_name'] = [x[0] for x in housing['close_city'].values]\nhousing['close_city_dist'] = [x[1] for x in housing['close_city'].values]\nhousing['close_city_pop'] = [city_pop_dict[x] for x in housing['close_city_name'].values]\n\nhousing = housing.drop('close_city', axis=1)\nhousing.head()\n\n\n#add the data relating to the points to the closest big city\ndef g(x):\n  return closest_point((x['latitude'],x['longitude']),big_cities)\nco2 = percentage_coroutine(len(housing))\nco2.__next__()\nhousing['big_city'] = housing.apply(trace_progress(g, progress = co2), axis = 1)\nhousing['big_city_name'] = [x[0] for x in housing['big_city'].values]\nhousing['big_city_dist'] = [x[1] for x in housing['big_city'].values]\n\nhousing = housing.drop('big_city', axis=1)\n\nhousing.to_csv(r\"./new_geo_data_test.csv\")\n_new_geo_data_test = housing.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_new_geo_data_test = housing.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('./b.txt', 'w') as f:\n    f.write('1')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\npcpath = r\"./new_geo_data.csv\"\n# ndata = pd.read_csv(pcpath, sep=',')\nndata = _new_geo_data_train\nbig_city_name = ndata['big_city_name'].unique()\nocean_proximity = ndata[\"ocean_proximity\"].unique()\nprint(ocean_proximity)\nprint(big_city_name)\nother_keys = list(filter(lambda x: x not in  ['Unnamed: 0', 'close_city_name', 'big_city_name', 'income_cat'], ndata.keys()))\nndata_train = ndata[other_keys]\nndata_train.loc[:, \"ocean_proximity\"] = pd.factorize(ndata_train[\"ocean_proximity\"])[0]\nndata_train = ndata_train[ndata_train.keys()[ndata_train.keys() != 'ocean_proximity']]\nfor i in big_city_name[:-1]:\n  ndata_train[i] = ndata['big_city_name'] == i\nfor i in ocean_proximity[:-1]:\n  ndata_train[i] = ndata['ocean_proximity'] == i\n\nother_keys = list(filter(lambda x: x not in  ['total_bedrooms', 'median_house_value'], ndata_train.keys()))\nbad_rows = ndata_train.notna().sum(axis=1) < ndata_train.shape[1]\ngood_rows = bad_rows == False\nln_room = LinearRegression(fit_intercept = True, normalize=True, n_jobs=-1)\nln_room.fit(ndata_train.loc[good_rows, other_keys], ndata_train.loc[good_rows, 'total_bedrooms'])\nndata_train.loc[bad_rows, 'total_bedrooms'] = ln_room.predict(ndata_train.loc[bad_rows, other_keys])\n\nother_keys = ndata_train.keys()[ndata_train.keys() != 'median_house_value']\nndata_train = ndata_train.sort_index()\nprint(ndata_train[other_keys].head(1))\nx_train = np.array(ndata_train[other_keys])\ny_train = np.array(ndata_train['median_house_value']).reshape(-1, 1)\nln = LinearRegression(fit_intercept = True, normalize=True, n_jobs=-1)\nx_test_len = int(x_train.shape[0] * 1 / 10)\nkf = KFold(n_splits=10)\nkf.get_n_splits(x_train)\nnp.random.seed(42)\nidxs = np.arange(x_train.shape[0])\nnp.random.shuffle(idxs)\nx_train, y_train = x_train[idxs], y_train[idxs]\ni = 0\nfor train_index, test_index in kf.split(x_train):\n    if i == 6:\n        break\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_tr, X_te = x_train[train_index], x_train[test_index]\n    y_tr, y_te = y_train[train_index], y_train[test_index]\n    ln.fit(X_tr, y_tr)\n    print(\"RMSE ==> \",np.sqrt(mean_squared_error(y_train,ln.predict(x_train))))\n    print(\"RMSE ==> \",np.sqrt(mean_squared_error(y_tr,ln.predict(X_tr))))\n    print(\"RMSE ==> \",np.sqrt(mean_squared_error(y_te,ln.predict(X_te))))\n    i+=1\n#print(\"RMSE ==> \",np.sqrt(mean_squared_error(y_train[:x_train_len],ln.predict(x_train[:x_train_len]))))\n#print(\"RMSE ==> \",np.sqrt(mean_squared_error(y_train[x_train_len:],ln.predict(x_train[x_train_len:]))))\n\npcpath = r\"./new_geo_data_test.csv\"\nndata = _new_geo_data_test\n# ndata = pd.read_csv(pcpath, sep=',')\nother_keys = list(filter(lambda x: x not in  ['Id', 'Unnamed: 0', 'close_city_name', 'big_city_name'], ndata.keys()))\nndata_train = ndata[other_keys]\nndata_train.loc[:, \"ocean_proximity\"] = pd.factorize(ndata[\"ocean_proximity\"])[0]\nndata_train = ndata_train[ndata_train.keys()[ndata_train.keys() != 'ocean_proximity']]\nprint(ndata['big_city_name'].unique())\nprint(ndata[\"ocean_proximity\"].unique())\nocean_dict = {'INLAND':'INLAND', 'LessThan1h':'<1H OCEAN', '<1H OCEAN':'<1H OCEAN',\n              'NEAR BAY':'NEAR BAY', 'NEAR OCEAN':'NEAR OCEAN', 'ISLAND':'ISLAND'}\nfor i in big_city_name[:-1]:\n    ndata_train[i] = ndata['big_city_name'] == i\nfor i in ocean_proximity[:-1]:\n    print(ocean_dict[i])\n    ndata_train[i] = ndata['ocean_proximity'] == ocean_dict[i]\n  \nother_keys = ndata_train.keys()[ndata_train.keys() != 'total_bedrooms']\nbad_rows = ndata_train.notna().sum(axis=1) < ndata_train.shape[1]\ngood_rows = bad_rows == False\nprint(ndata_train.head(1))\nndata_train.loc[bad_rows, 'total_bedrooms'] = ln_room.predict(ndata_train.loc[bad_rows, other_keys])\n\nndata_train = ndata_train.sort_index()\nx_test = np.array(ndata_train)\npcpath = r\"../input/california-house-price/housing_test_sample_solution.csv\"\nmodel_sol = pd.read_csv(pcpath, sep=',')\ny_test_ans = ln.predict(x_test)\nmodel_sol['median_house_value'] = y_test_ans.flatten()\nmodel_sol.to_csv(r\"/kaggle/working/model_solution.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Финальный результат вычислялся на такой модели lightgbm.LGBMRegressor() снизу: "},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm\nlgb = lightgbm.LGBMRegressor()\n\nfrom sklearn.model_selection import KFold\n\npcpath = r\"C:\\Users\\danil\\Desktop\\6семестр_309\\матпрак\\data\\new_geo_data.csv\"\nndata = pd.read_csv(pcpath, sep=',')\nbig_city_name = ndata['big_city_name'].unique()\nocean_proximity = ndata[\"ocean_proximity\"].unique()\nprint(ocean_proximity)\nprint(big_city_name)\nother_keys = list(filter(lambda x: x not in  ['Unnamed: 0', 'close_city_name', 'big_city_name', 'income_cat'], ndata.keys()))\nndata_train = ndata[other_keys]\nndata_train.loc[:, \"ocean_proximity\"] = pd.factorize(ndata_train[\"ocean_proximity\"])[0]\nndata_train = ndata_train[ndata_train.keys()[ndata_train.keys() != 'ocean_proximity']]\nfor i in big_city_name[:-1]:\n  ndata_train[i] = ndata['big_city_name'] == i\nfor i in ocean_proximity[:-1]:\n  ndata_train[i] = ndata['ocean_proximity'] == i\n\nother_keys = list(filter(lambda x: x not in  ['total_bedrooms', 'median_house_value'], ndata_train.keys()))\nbad_rows = ndata_train.notna().sum(axis=1) < ndata_train.shape[1]\ngood_rows = bad_rows == False\nln_room = LinearRegression(fit_intercept = True, normalize=True, n_jobs=-1)\nln_room.fit(ndata_train.loc[good_rows, other_keys], ndata_train.loc[good_rows, 'total_bedrooms'])\nndata_train.loc[bad_rows, 'total_bedrooms'] = ln_room.predict(ndata_train.loc[bad_rows, other_keys])\n\nother_keys = ndata_train.keys()[ndata_train.keys() != 'median_house_value']\nndata_train = ndata_train.sort_index()\nprint(ndata_train[other_keys].head(1))\nx_train = np.array(ndata_train[other_keys])\ny_train = np.array(ndata_train['median_house_value']).reshape(-1, 1)\nln = LinearRegression(fit_intercept = True, normalize=True, n_jobs=-1)\nx_test_len = int(x_train.shape[0] * 1 / 10)\nkf = KFold(n_splits=10)\nkf.get_n_splits(x_train)\nnp.random.seed(42)\nidxs = np.arange(x_train.shape[0])\nnp.random.shuffle(idxs)\nx_train, y_train = x_train[idxs], y_train[idxs]\ni = 0\nfor train_index, test_index in kf.split(x_train):\n    if i == 6:\n        break\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_tr, X_te = x_train[train_index], x_train[test_index]\n    y_tr, y_te = y_train[train_index], y_train[test_index]\n    lgb.fit(X_tr, y_tr)\n    print(\"RMSE ==> \",np.sqrt(mean_squared_error(y_train,lgb.predict(x_train))))\n    print(\"RMSE ==> \",np.sqrt(mean_squared_error(y_tr,lgb.predict(X_tr))))\n    print(\"RMSE ==> \",np.sqrt(mean_squared_error(y_te,lgb.predict(X_te))))\n    i+=1\n#print(\"RMSE ==> \",np.sqrt(mean_squared_error(y_train[:x_train_len],ln.predict(x_train[:x_train_len]))))\n#print(\"RMSE ==> \",np.sqrt(mean_squared_error(y_train[x_train_len:],ln.predict(x_train[x_train_len:]))))\n\npcpath = r\"C:\\Users\\danil\\Desktop\\6семестр_309\\матпрак\\data\\new_geo_data_test.csv\"\nndata = pd.read_csv(pcpath, sep=',')\nother_keys = list(filter(lambda x: x not in  ['Id', 'Unnamed: 0', 'close_city_name', 'big_city_name'], ndata.keys()))\nndata_train = ndata[other_keys]\nndata_train.loc[:, \"ocean_proximity\"] = pd.factorize(ndata[\"ocean_proximity\"])[0]\nndata_train = ndata_train[ndata_train.keys()[ndata_train.keys() != 'ocean_proximity']]\nprint(ndata['big_city_name'].unique())\nprint(ndata[\"ocean_proximity\"].unique())\nocean_dict = {'INLAND':'INLAND', 'LessThan1h':'<1H OCEAN', \n              'NEAR BAY':'NEAR BAY', 'NEAR OCEAN':'NEAR OCEAN', 'ISLAND':'ISLAND'}\nfor i in big_city_name[:-1]:\n  ndata_train[i] = ndata['big_city_name'] == i\nfor i in ocean_proximity[:-1]:\n  print(ocean_dict[i])\n  ndata_train[i] = ndata['ocean_proximity'] == ocean_dict[i]\n  \nother_keys = ndata_train.keys()[ndata_train.keys() != 'total_bedrooms']\nbad_rows = ndata_train.notna().sum(axis=1) < ndata_train.shape[1]\ngood_rows = bad_rows == False\nprint(ndata_train.head(1))\nndata_train.loc[bad_rows, 'total_bedrooms'] = ln_room.predict(ndata_train.loc[bad_rows, other_keys])\n\nndata_train = ndata_train.sort_index()\nx_test = np.array(ndata_train)\npcpath = r\"C:\\Users\\danil\\Desktop\\6семестр_309\\матпрак\\data\\housing_test_sample_solution.csv\"\nmodel_sol = pd.read_csv(pcpath, sep=',')\ny_test_ans = lgb.predict(x_test)\nmodel_sol['median_house_value'] = y_test_ans.flatten()\nmodel_sol.to_csv(r\"C:\\Users\\danil\\Desktop\\6семестр_309\\матпрак\\data\\model_solution.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}