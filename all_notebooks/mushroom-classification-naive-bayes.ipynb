{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Importing important libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR=\"../input/mushroom-classification/mushrooms.csv\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(DIR)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print first 20 rows.\ndf.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Wrangling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the missing values in our dataset.\nmissing_value=df.isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Printing the columns with missing values. [False- Number of non-missing values; True- Number of missing values]\nfor columns in missing_value.columns.values.tolist():\n    print (columns)\n    final=missing_value[columns].value_counts()\n    print (final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dataset shape is 8124 rows and 23 columns.\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying label encoder to transform labels into numeric form so as to convert it into the machine-readable form\ndf1=df.apply(LabelEncoder().fit_transform)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe function will calculate the mean, standard deviation, min value, max value, values under 25 percentile, 50 percentile and 75 percentile respectively.\ndf1.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation function will calcuate the correlation of each feature with each other feature.\ndf1.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting heat map to visualize the correlation of each feature.\nsns.heatmap(df1.corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since correlation of feature 'veil-type' is very low with other features, we can drop this feature as it will not help in the classification.\ndf2=df1.drop(['veil-type'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of our dataset after dropping the 'veil-type' feature.\ndf2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')\nsns.regplot(df2['cap-shape'],df2['class'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the pearson coefficient and p-value. In this, pearson coefficient is 0.0529 which means the feature 'cap-shape' is not much positively linearly dependent on target variable which is 'class'.\n#p-value indicates the probability for strong coorelation. In this case, p-value is <0.001 which is the prediction of high correlation. \nscipy.stats.pearsonr(df2['cap-shape'],df2['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(df2['cap-surface'],df2['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here pearson coefficient is 0.178 which is positive but not close to 1 hence it is not much linealy dependent on 'class'.\n# p-value is <0.001 which is the prediction for high correlation. \nscipy.stats.pearsonr(df2['cap-surface'],df2['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(df2['gill-spacing'],df2['class'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pearson Coefficient is -0.348 which is negative and little bit closer to 1. Hence, it will be negatively linear dependent on 'class'.\n#p-value is <0.0001 which is the indication for high correlation.\nscipy.stats.pearsonr(df2['gill-spacing'],df2['class'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing the dataset into X (features) and y (target) variables.\nX=df2.drop(df1[['class']],axis=1)\ny=df2[['class']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using train_test_split function to divide data into training and test dataset.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('size of X train data and y train data is {} and {} respectively.'.format(X_train.shape, y_train.shape))\nprint ('size of X test data and y test data is {} and {} respectively'.format(X_test.shape, y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting the train and test dataset into numpy array.\nX_Train=np.array(X_train)\nX_Test=np.array(X_test)\ny_Train=np.array(y_train).reshape(-1,)\ny_Test=np.array(y_test).reshape(-1,)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CUSTOM NAIVE BAYES IMPLEMENTATION"},{"metadata":{},"cell_type":"markdown","source":"# PRIOR PROBABILITY\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def prior_probability(y_t,labels):\n    for i in range(y_t.shape[0]):\n        numerator= (y_t== labels).sum()\n    denominator= y_t.shape[0]\n    return numerator/denominator\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prior_probability(y_Train,0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Posterior Probability "},{"metadata":{"trusted":true},"cell_type":"code","source":"def prior_probability(y_Train,label):\n    for i in range(y_Train.shape[0]):\n        numerator= (y_Train== label).sum()\n    denominator= y_Train.shape[0]\n    return numerator/denominator\n\n\n\n#Function to calculate the conditional probability.\ndef cond_prob(X_Train,y_Train,feature_col,feature_val,label):\n    x_fil=X_Train[y_Train==label]\n    num=np.sum(x_fil[:,feature_col]==feature_val)\n    den=np.sum(y_Train==label)\n    return num/float(den)\n\ndef prediction(X_Train,y_Train,X_Test):\n    L= np.unique(y_Train)\n    n=X_Train.shape[1]\n    pp=[]\n    for label in L:\n        Likelihood=1\n        for k in range(n):\n            cond= cond_prob(X_Train,y_Train,k,X_Test[k],label)\n            Likelihood*=cond\n        prior_prob=prior_probability(y_Train,label)\n        posterior_p= Likelihood*prior_prob\n        pp.append(posterior_p)\n    return np.argmax(pp)\n\n#Function to calculate the accuracy of our prediction.\ndef score(X_Train,y_Train,X_Test,y_Test):\n    pred=[]\n    for i in range(X_Test.shape[0]):\n        pred_list=prediction(X_Train,y_Train,X_Test[i])\n        pred.append(pred_list)\n    pred=np.array(pred)\n    accuracy=(np.sum(pred==y_Test)/y_Test.shape[0])\n    return accuracy\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (prediction(X_Train,y_Train,X_Test[4]))\nprint (y_Test[4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy is 99 percent.\nprint (score(X_Train,y_Train,X_Test,y_Test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}