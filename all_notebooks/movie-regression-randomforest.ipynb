{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# READ THE DATA"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/tmdb-movie-metadata/tmdb_5000_movies.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['original_language'] = df['original_language'].apply(lambda x : 1 if x=='en' else 0 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['original_language'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename(columns={\"original_language\": \"English\"},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Figuring out how to accept string data as lists in python "},{"metadata":{"trusted":true},"cell_type":"code","source":"a = df['production_countries'][0]\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import ast\na = ast.literal_eval(a)\na","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['release_date']=df['release_date'].fillna('1992-09-04')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['release_date'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['release_date']= pd.to_datetime(df['release_date']) \ndf['release_date']=df['release_date'].apply(lambda x: int(x.year))\ndf['release_date'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DROP NO ESSENTIAL FEATURES"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['homepage', 'id','keywords','original_title','overview','status','tagline','title','English'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['production_companies']=df['production_companies'].apply(lambda x: ast.literal_eval(x))\n\ndf['production_companies']=df['production_companies'].apply(lambda x: len(x))\ndf['production_companies'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['genres']=df['genres'].apply(lambda x: ast.literal_eval(x))\n\ndf['genres']=df['genres'].apply(lambda x: len(x))\ndf['genres'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"# Some Data needs to be length of a list instead of whole list "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['production_countries']=df['production_countries'].apply(lambda x: ast.literal_eval(x))\n\ndf['production_countries']=df['production_countries'].apply(lambda x: len(x))\ndf['production_countries'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['spoken_languages']=df['spoken_languages'].apply(lambda x: ast.literal_eval(x))\ndf['spoken_languages']=df['spoken_languages'].apply(lambda x: len(x))\ndf['spoken_languages'].head()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RENAMING SOME COLUMNS"},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.rename(columns={\"spoken_languages\": \"Number of spoken_languages\"},inplace=True)\n#df.rename(columns={\"production_countries\": \"Number of countries produced in\"},inplace=True)\n#df.rename(columns={\"production_companies\": \"Number of producers\"},inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Filling nan values and changing datatypes "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['runtime']=df['runtime'].fillna(df['runtime'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['popularity']=df['popularity'].apply(lambda x: int(x))\ndf['runtime']=df['runtime'].apply(lambda x: int(x))\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# REPLACING 0s "},{"metadata":{"trusted":true},"cell_type":"code","source":"df['production_companies']=df['production_companies'].replace(0,1)\ndf['production_countries']=df['production_countries'].replace(0,1)\n\nquant = 0.0156\ndf['revenue']=df['revenue'].replace(0,df['revenue'].quantile(quant))\ndf['budget']=df['budget'].replace(0,df['budget'].quantile(quant))\ndf['popularity']=df['popularity'].replace(0,df['popularity'].quantile(quant))\ndf['runtime']=df['runtime'].replace(0,df['runtime'].quantile(quant))\n\ndf['spoken_languages']=df['spoken_languages'].replace(0,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df.drop(['runtime'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating features and target label "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop(['revenue'],axis=1)\ny = df['revenue']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaling the data "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MaxAbsScaler\nscaler = MaxAbsScaler()\nX=scaler.fit_transform(X)\n#y=scaler.fit_transform(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Trying PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# import matplotlib.pyplot as plt\n# from sklearn.decomposition import PCA\n\n# pca = PCA()\n# principalComponents = pca.fit_transform(X)\n\n# plt.figure()\n# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n# plt.xlabel('Number of Components')\n# plt.ylabel('Variance (%)') #for each component\n# plt.title('Explained Variance')\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pca = PCA(n_components=5)\n#X = pca.fit_transform(X)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Importing the regression Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor as forest\nclf = forest(max_depth=40,max_features=0.4,n_estimators=45,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitiing the data for validation after trainin g"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training and evaluating the model "},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Saving pickles"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\n\nfilename = 'RandomForest_model.pickle'\npickle.dump(clf, open(filename, 'wb'))\n\nfilename_scaler = 'scaler_model.pickle'\npickle.dump(scaler, open(filename_scaler, 'wb'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# HYPER PARAMETER TUNING"},{"metadata":{"trusted":true},"cell_type":"code","source":"##Hyper parameter tuning \n\n# n_estimators = [int(x) for x in np.linspace(start = 40, stop = 120, num = 10)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 75, num = 10)]\n# max_depth.append(None)\n# # Minimum number of samples required to split a node\n# min_samples_split = [2, 5, 10]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 4]\n\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                }\n\n# from sklearn.model_selection import GridSearchCV\n# grid_search = GridSearchCV(estimator=clf,param_grid=random_grid,cv=2,n_jobs =-1,verbose = 3)\n# grid_search.fit(X_train, y_train)\n# grid_search.best_params_\n\n\n\n# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n# [Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   49.3s\n# [Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  6.5min\n# [Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 15.5min\n# [Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 26.2min\n# [Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 33.7min\n# /opt/conda/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n#   \"timeout or by a memory leak.\", UserWarning\n\n# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n# [Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   49.3s\n# [Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  6.5min\n# [Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 15.5min\n# [Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 26.2min\n# [Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 33.7min\n# /opt/conda/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n#   \"timeout or by a memory leak.\", UserWarning\n# [Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed: 53.7min\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}