{"cells":[{"metadata":{},"cell_type":"markdown","source":"The main goal is to show various techniques for checking normal distribution:\n\n1. Normal distribution parameters check\n2. Hypothesis testing\n3. Graphic representation of density functions.\n\nExcept for working with above methods, I will perform needed dataset modification in order to typos desposal and converting numerical variables into categorical.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Import required libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport scipy.stats\n\nfrom sklearn.preprocessing import OrdinalEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Upload the dataset","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/goodreadsbooks/books.csv', error_bad_lines=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skipped_lines_percent = round(4/(df.shape[0]+4)*100,4)\nskipped_lines_percent","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Skipped lines account for about 0.0359 % of length of whole dataset, hence such skipping is acceptable.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3. Take a look at basic information about the dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset consists of **11123 rows and 12 columns**. The **columns names** are below:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Dataframe modification \n### 4.1 Fix column names ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I will rename whitespace in \"num_pages\" column name.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = df.columns.str.replace(' ', '')\ndf.columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Remove needless columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Some columns like:\n* bookID\n* isbn\n* isbn13\n* title\n* authors\n\nare not interesting from the viewpoint of distribution analysis, so they will be removed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of columns before removing: {df.shape[1]}')\ndf = df.drop(['bookID', 'isbn', 'isbn13', 'title', 'authors'], axis=1)\nprint(f'Number of columns after removing: {df.shape[1]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns.to_list()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Convert some categorical data into numerical","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.nunique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence column \"language_code\" has only 27 unique values, it can be easy mapped to numerical values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.language_code.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Language codes with prefix 'en-' like: en-US, en-CA, en-GB will be replaced by 'eng'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.language_code = df.language_code.replace(to_replace ='en-..', value = 'eng', regex = True)\nnp.sort(df.language_code.unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that mentioned values have been replaced by 'eng'. Last thing is to convert this column to a column with numerical values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"before = df.language_code.unique()\n\nenc = OrdinalEncoder()\ndf.language_code = enc.fit_transform(df.language_code.values.reshape(-1, 1)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(data={'before': before,\n                   'after': df.language_code.unique()}).sort_values(by='before')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Map of \"language code\" values will enable the further distribution analysis.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.4 Feature engineering\n\n### 4.4.1 Publication date\n\"publication_date\" may be valuable for distribution analysis, especially when years will be extracted.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['year'] = df.publication_date.str.rsplit(\"/\", n=3, expand=True)[2].astype(int)\n# n=3 because value is splitted into 3 parts: day, month and year\n# [2] because we are interested only in 'year'\n\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To avoid data leakage \"publication_date\" will be removed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['publication_date'], axis=1)\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Normal distribution analysis\nI will perform the analysis going through 3 approaches:\n* using basic stats to see normal distribution parameters.\n* hypothesis testing of normal distribution\n* graphic representation of density functions","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Last glance at basic statistics to check if datapoints looks good.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Normal distribution parameters check\n\nParameters that are indicative for normal distribution are:\n* mean \n* median \n* kurtosis\n* skewness.\n\nThe mean and median [should have the same value](https://en.wikipedia.org/wiki/Normal_distribution), and kurtosis and skewness [be equal to 0](https://en.wikipedia.org/wiki/Normal_distribution).\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.agg(['mean', 'median', 'kurtosis', 'skew']).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean and median have similar values for: \n* average_rating (left-skewed distribution)\n* num_pages (right-skewed distribution)\n* language_code (right-skewed distribution)\n* year (with a skew that is the closest to 0, left-skewed distribution)\n\nThe \"year\" and \"average_rating\" are our front-runners in the race for normal distribution ;)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Hypothesis testing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\np_value_list = []\nalpha = 0.05\n\nfor i in df._get_numeric_data().columns:\n    p_value = scipy.stats.normaltest(df[i])[1] # to get only p_value without a statistic\n    p_value_list.append(p_value)\n    if p_value < alpha:\n        results.append('rejected')\n    else:\n        results.append('not rejected')\n        \npd.DataFrame(data={'variable': df._get_numeric_data().columns,\n                    'p_value': p_value_list,\n                    'null hypothesis': results})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to hypothesis testing, none of variables comes from a normal distribution. It's hard to find a feature that is the closest to be normal because all p value are 0.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Graphic representation of density functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(3,2, figsize=(15, 10))\nsns.distplot(df.average_rating, color='skyblue', ax=axes[0, 0])\nsns.distplot(df.num_pages, color='olive', ax=axes[0, 1])\nsns.distplot(df.ratings_count, color='gold', ax=axes[1, 0])\nsns.distplot(df.text_reviews_count, color='teal', ax=axes[1, 1])\nsns.distplot(df.year, color='skyblue', ax=axes[2, 0])\nsns.countplot(x = 'language_code', data = df, ax=axes[2,1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, \"average_rating\" is the most normal variable.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 6. Conlusion\n\nThe dataset consisted of 12 columns. Five of them (bookID, isbn, isbn13, title and authors) have been removed because checking them for statistics wasn't reasonable. \nThe column \"publication_date\" has been replaced by column \"year\".  \nEventually, the dataset had 7 columns: categorical (\"language_code\" and \"publisher\") and numerical (the rest).\n\nUse of 3 different methods showed different results. Despite the fact that the distribution of \"average_rating\" variable looks like a normal distribution (5.3), then none of the numerical variables hasn't a normal distribution (5.1 and 5.2). ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}