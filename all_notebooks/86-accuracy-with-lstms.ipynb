{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/first-gop-debate-twitter-sentiment/Sentiment.csv')\ndata = data[['text','sentiment']]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data.sentiment != \"Neutral\"]\ndata['sentiment']= pd.get_dummies(data['sentiment'], drop_first = True)\ndata = data.reset_index(drop=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = []\n\nfor i in range(0, data.shape[0]):\n    tweet = re.sub('[^a-zA-Z]', ' ', data['text'][i])\n    tweet = tweet.lower()\n    tweet = word_tokenize(tweet)\n    # Reduce words to their root form\n    tweet = [WordNetLemmatizer().lemmatize(w) for w in tweet if not w in set(stopwords.words('english'))]\n    # Lemmatize verbs by specifying pos\n    tweet = [WordNetLemmatizer().lemmatize(w, pos='v') for w in tweet if not w in set(stopwords.words('english'))]\n    tweet = ' '.join(tweet)\n    corpus.append(tweet)\n\nprint(corpus[0:3])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##applying TF-IDF to check the most relevant words after preprocessing , so we can remove some of these word to improve the model accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfIdfVectorizer=TfidfVectorizer(use_idf=True)\ntfIdf = tfIdfVectorizer.fit_transform(corpus)\ndf = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\ndf = df.sort_values('TF-IDF', ascending=False)\npd.set_option('display.max_rows', None)\ndf.head(20)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0, len(corpus)):\n    corpus[i] = re.sub('co','', corpus[i])\n    corpus[i] = re.sub('rt','', corpus[i])\n    corpus[i] = re.sub('http','', corpus[i])\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words=None)\ntokenizer.fit_on_texts(corpus)\nencoded_docs = tokenizer.texts_to_sequences(corpus)\npadded_sequence = pad_sequences(encoded_docs,maxlen=200)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenizer.word_index['trump'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(corpus[0])\nprint(encoded_docs[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(padded_sequence[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Build the model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import Embedding\nfrom keras.initializers import Constant\n\nvocab_size = len(tokenizer.word_index) + 1\nembedding_vector_length = 200\n\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_vector_length,\n                    input_length=25) )\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(3, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting the targets to numpy array to feed it into the model\ntarget = np.asarray(data['sentiment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL = model.fit(padded_sequence,target,validation_split=0.2, epochs=5, batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_word ='love trump'\ntw = tokenizer.texts_to_sequences([test_word])\ntw = pad_sequences(tw,maxlen=200)\nprediction = int(model.predict(tw).round().item())\nprint(prediction)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"finally this is the best accuracy i could get after hyperparametes tuning + TF-IDF as guidance for removing some words"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}