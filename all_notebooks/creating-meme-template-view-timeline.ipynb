{"cells":[{"metadata":{},"cell_type":"markdown","source":"### *This notebook demonstrates one way to build a timeseries from the dataset, but as with all analysis assumptions have to be made*","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pickle, os\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom datetime import datetime\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n\ndef get_day(s):\n    return str(datetime.fromtimestamp(s))[:-9]\n\ndef round_wk(i):\n    return int(i / (60*60*24*7)) * 60*60*24*7","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initial data processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"r_obs = pd.read_csv('/kaggle/input/most-viewed-memes-templates-of-2018/reddit_observations.csv',index_col='reddit_obs_num')\ni_obs = pd.read_csv('/kaggle/input/most-viewed-memes-templates-of-2018/imgur_observations.csv',index_col='imgur_obs_num')\nposts = pd.read_csv('/kaggle/input/most-viewed-memes-templates-of-2018/reddit_posts.csv',index_col='meme_id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Add Reddit upvotes to Imgur data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"link_subreddit = posts[['reddit_post_id','subreddit']].drop_duplicates().set_index('reddit_post_id')['subreddit']\n\ni_obs = i_obs.join(r_obs['upvotes'], on='reddit_obs_num')\ni_obs = i_obs.join(link_subreddit, on='reddit_post_id').sort_values('timestamp')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get first and last observation for each post","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"first_obs = i_obs.drop_duplicates(subset='reddit_post_id', \n                                  keep='first').set_index('reddit_post_id')[['upvotes','imgur_viewcount']].sort_index()\nlast_obs = i_obs.drop_duplicates(subset='reddit_post_id', \n                                 keep='last').set_index('reddit_post_id')[['upvotes','imgur_viewcount']].sort_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Get difference between first last and ratio of views to upvotes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"delta = last_obs - first_obs\ndelta = delta[(delta['upvotes']>0)&(delta['imgur_viewcount']>0)]\ndelta = delta.join(link_subreddit)\ndelta['ratio'] = delta['imgur_viewcount']/delta['upvotes']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Assumption 1: fill missing views using\n\n- We only have views for a small subset of the dataset so we must estimate views for the majority using the upvotes they recieve\n- The ratio of people who upvote to view varies between Subreddits, so we estimate this ratio \n- We get an initial estimate by regressing the deltas (target = how many views the image increased by, predictor = how much upvotes the post gained)\n- However for many Subreddits we have a small number of samples, so the initial estimate cannot be considered reliable\n- Therefore we adjust the initial estimate using the average ratio across all the large Subreddits \n- The adjusted ratio is a weighted average of the Subreddit's initial estimate and the average ratio, such that; Subreddits with <10 samples use the average, those with >100 use their initial estimate, those with 10-100 have weighting linearly tapered depending","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subreddit_list = posts['subreddit'].unique()\nsubreddit_ratios = pd.DataFrame(columns=['ratio','n'])\n\nlr = LinearRegression(fit_intercept=True, normalize=False) \n\nfor s in subreddit_list: \n    y = delta.loc[delta['subreddit']==s, 'imgur_viewcount']\n    if len(y)>0:\n        X = delta.loc[delta['subreddit']==s, 'upvotes'].values.reshape(-1,1)\n        lr.fit(X, y)\n        subreddit_ratios.loc[s,'ratio'] = lr.coef_[0]\n    subreddit_ratios.loc[s,'n'] = len(y)\n\nsubreddit_ratios['average'] = subreddit_ratios.loc[subreddit_ratios['n']>100,'ratio'].mean()\n\ndef corrected_coef(c,n,av,lo_thr=10,hi_thr=100):\n    if n < lo_thr: return av\n    if n > hi_thr: return c\n    w1 = hi_thr - n\n    w2 = n - lo_thr\n    return np.average([av,c], weights=[w1,w2])\n        \nsubreddit_ratios['adjusted_ratio'] = [corrected_coef(c,n,av) for c,n,av in subreddit_ratios[['ratio','n','average']].values]\nratios = subreddit_ratios['adjusted_ratio'].to_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = subreddit_ratios['adjusted_ratio'].sort_values().plot(kind='bar', figsize=(12,5))\nt2 = ax.set_title('Estimated views per upvote, for each Subreddit')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Apply assumption 1 to get estimated views for all reddit observations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"link_post = posts[['reddit_post_id','subreddit','meme_template']].drop_duplicates().set_index('reddit_post_id')\nr_obs = r_obs.join(link_post, on='reddit_post_id').sort_values('timestamp')\nr_obs['basic_estimated_views'] = [u*ratios[s] for u,s in r_obs[['upvotes','subreddit']].values]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Assumption 2: propagation beyond Reddit\n- Obviously our estimates being based on Reddit data means our estimates are biased towards Reddit users, so we make this propagation assumption to mitigate this bias\n- We assume that if a meme template is viewed on many different Subreddits, then it is truly going viral and spreading wide and far across the internet (equally we assume that if a meme template is viewed on only 1 Subreddit that is is probably not spreading much beyond Reddit)\n- The maximum 'propogation' we give a meme template is a multiplier of 2.5x if the Subreddit with the most views for this meme accounts for 40% or less of the total views for that meme (and the minimum is 1x, if all 100% of views are contained within 1 Subreddit).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_obs = r_obs.sort_values('basic_estimated_views').drop_duplicates(subset=['reddit_post_id','meme_template','subreddit'], keep='last')\ntemplate_x_subreddit = max_obs.pivot_table(columns='meme_template', index='subreddit', values='basic_estimated_views', aggfunc='sum')\ntemplate_concerntration = template_x_subreddit.max()/template_x_subreddit.sum()\ntemplate_propagation = (1 / template_concerntration.apply(lambda x: max([x,0.4]))).to_dict()\nr_obs['complex_estimated_views'] = [v*template_propagation[t] for v,t in r_obs[['basic_estimated_views','meme_template']].values]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final wrangling to get a time series of daily views for each meme template","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"r_obs['day'] = r_obs['timestamp'].apply(get_day)\nday_x_post = r_obs.pivot_table(index='day',columns='reddit_post_id',aggfunc='max',values='complex_estimated_views')\nday_x_post = day_x_post.interpolate(limit_direction='forward').replace(np.nan,0)\nday_x_post_delta = day_x_post.diff()\nday_x_post_delta[day_x_post_delta < 0] = 0\nday_x_post_delta.iloc[0] = day_x_post.iloc[0]\nday_x_post_delta = day_x_post_delta.T.join(posts.set_index('reddit_post_id')['meme_template'])\nday_x_template_delta = day_x_post_delta.groupby('meme_template').sum()\ndaily_views = day_x_template_delta.astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"daily_views.loc[['harold','stefan_pref']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Display cumulative views for two example templates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cumulative_views = daily_views.T.cumsum()\nax = cumulative_views[['harold','stefan_trickery']].plot(\n            figsize=(12,5), ylim=(0), xlim=(0,364)) \nt = ax.set_ylabel('Cumulative views', fontsize=12)\n# RIP Stefan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot total views for the 250 meme templates","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total_views = cumulative_views.iloc[-1].sort_values(ascending=False)\nfig,ax = plt.subplots(figsize=(12,5))\nax.bar(x=range(len(total_views)), height=total_views)\nax.set_yticklabels([str(int(i/1000000))+'m' for i in ax.get_yticks()])\nax.set_xlim(-1,250)\nax.set_xticks([0,49,99,149,199,249])\nax.set_xticklabels([1,50,100,150,200,250])\nax.set_ylabel('Views throughout 2018', fontsize=14)\nax.set_xlabel('Meme templates (ranked by views)', fontsize=14)\nt = ax.text(130, 1.3e8, color='r', fontsize=12,\n            s='FUN FACT:\\nThe top meme template has more views\\nthan the templates ranked 200-250 combined')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_views.iloc[0] > total_views.iloc[-51:].sum() ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}