{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Complete Exploratory Analysis, Model Selection and Hyper-Parameter Optimization of the Flu-Shot Prediction dataset","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Table of Contents\n* [Introduction](#intro)\n* [Preliminary analysis & Data Processing](#prelim)\n* [Exploratory Data Analysis and Visualizations](#eda)\n* [Feature Selection, Extraction & Engineering](#fe)\n* [Data Splitting and Test data processing](#split)\n* [Analyzing Several Models on the Dataset](#models)\n    - [Ensemble of Models](#ensemble)\n    - [Models with Hyper Parameter Optimization (GSCV & RSCV)](#hpo)\n* [Final Notes & Submission](#final)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Introduction <a id=\"intro\"></a> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> The Objective is to perform a multilabel classification of whether a person will/will not take the:\n  >  1. H1N1 vaccine \n  >  2. Seasonal Flu vaccine","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### The data can be found here: https://www.kaggle.com/darkknight98/flu-shot-prediction","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"### Necessary imports ###\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow as tf\nimport sklearn as sk\nimport matplotlib as mpl\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score\nfrom sklearn.model_selection import train_test_split,StratifiedKFold,cross_val_score\nfrom sklearn.preprocessing import LabelEncoder,OrdinalEncoder\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.ensemble import RandomForestClassifier as RF\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom catboost import Pool,CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostRegressor\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import svm, model_selection,tree, linear_model, neighbors, naive_bayes, ensemble \nfrom sklearn import discriminant_analysis, gaussian_process","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Preliminary analysis & Data Processing <a id=\"prelim\"></a> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Reading the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.read_csv(r'/kaggle/input/flu-shot-prediction/training_set_features.csv')\nY = pd.read_csv(r'/kaggle/input/flu-shot-prediction/training_set_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of X: ',X.shape)\nprint('Shape of Y: ',Y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We see that there are 36 columns/features in the training set \n\n> There are 3 columns in the target file(ground truth) which include respondent_id, h1n1_vaccine,seasonal_vaccine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Several features including a persons 'behavior' and his/her 'opinion' about the vaccine, marital status, geographic location all are found, so the data consists a significantly large amount of info","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Joining the X & Y into a single dataframe ###\nZ = Y\nZ.drop('respondent_id',axis = 1,inplace = True)\nframes = [X,Z]\ndata = pd.concat(frames,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking out the number of NULLs/NaNs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns **'health_insurance', 'employment_industry' , 'employment_occupation'** contain too many NULLs","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# close to half the values of these columns are NULL so discarding\ndata.drop(['health_insurance','employment_industry','employment_occupation'],axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Exploratory Data Analysis and Visualizations <a id=\"eda\"></a> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Firstly Analyzing the number of people who took each vaccine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('h1n1_vaccine',data=Y,kind='count',size = 3.5)\nsns.factorplot('seasonal_vaccine',data=Y,kind='count',size = 3.5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Considerably larger number of people have chosen to take the seasonal flu vaccine compared to those who took the H1N1 vaccine","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Next Finding the categorical features present in the data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"str_cols = data.select_dtypes(include = 'object').columns\ndata[str_cols].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 10 categorical features!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Populating the NULLs/NaNs with suitable substitutes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"P.S: All Categorical features which are NULL are substituted with the mode of data, and the Numeric features with NULLs are substituted with the means","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# All the NULL values are populated with the mode\nfor col in data.columns:\n    if data[col].isnull().sum() and data[col].dtypes != 'object':\n        data[col].loc[(X[col].isnull())] = data[col].median()\nfor col in data.columns:\n    if data[col].isnull().sum() and data[col].dtypes == 'object':\n        data[col].loc[(data[col].isnull())] = data[col].mode().max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sanity Check below!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Features are encoded using Label Encodings","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"P.S: Please consider trying out Mean Encodings, One Hot Encoding or Ordinal Encodings and notice the differences","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"LE = LabelEncoder()\nfor col in str_cols:\n    data[col] = LE.fit_transform(data[col]) # Converts to int64","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sanity Check again!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data[str_cols].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plotting Correlation Maps with the set of Encoded and Null Populated Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\ng = sns.heatmap(corr,  vmax=.3, center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": .5}, annot=True, fmt='.2f', cmap='Spectral')\nsns.despine()\ng.figure.set_size_inches(30,25)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*  High Positive correlations between the 'behavioral_features' is noted, some of them may be redundant.\n \n*  High Positive Correlations between opinion of h1n1 risk, doctor recommendation of vaccines Vs whether the person really took the vaccine. Seems fairly obvious\n\n*  Overall the data features seems to be positively correlating with the act of taking the vaccination, except with some rare differences\n\n* Can mainly notice that there are many redundant features/sparsely correlated features which should be taken care of.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Now Plotting the Heatmap of all the Categorical features which were label encoded for a better idea","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"datum = data[str_cols]\ncorr = datum.corr()\ng = sns.heatmap(corr,  vmax=.3, center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": .5}, annot=True, fmt='.2f', cmap='PuOr')\nsns.despine()\ng.figure.set_size_inches(10,10)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Abundance of plots below!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* #### Pairwise Correlation plots of all the relevant and majorly significant features are given below. \n* #### Many features which are neutral and of no 'suggestive' value to the predictions should be removed later on to avoid redundancy.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def vaccination_rate_plot(col, target, data, ax=None):\n    \"\"\"Stacked bar chart of vaccination rate for `target` against \n    `col`. \n    \n    Args:\n        col (string): column name of feature variable\n        target (string): column name of target variable\n        data (pandas DataFrame): dataframe that contains columns \n            `col` and `target`\n        ax (matplotlib axes object, optional): matplotlib axes \n            object to attach plot to\n    \"\"\"\n    counts = (data[[target, col]]\n                  .groupby([target, col])\n                  .size()\n                  .unstack(target)\n             )\n    group_counts = counts.sum(axis='columns')\n    props = counts.div(group_counts, axis='index')\n\n    props.plot(kind=\"barh\", stacked=True, ax=ax)\n    ax.invert_yaxis()\n    ax.legend().remove()\n\ncols_to_plot = [\n     'household_adults','opinion_h1n1_sick_from_vacc',\n       'household_children', 'age_group', 'education', 'race',\n       'sex', 'income_poverty', 'marital_status',\n       'rent_or_own', 'employment_status', 'hhs_geo_region',\n       'census_msa'\n       \n]\n\nfig, ax = plt.subplots(\n    len(cols_to_plot), 2, figsize=(9,len(cols_to_plot)*2.5)\n)\nfor idx, col in enumerate(cols_to_plot):\n    vaccination_rate_plot(\n        col, 'h1n1_vaccine', data, ax=ax[idx, 0]\n    )\n    vaccination_rate_plot(\n        col, 'seasonal_vaccine', data, ax=ax[idx, 1]\n    )\n    \nax[0, 0].legend(\n    loc='lower center', bbox_to_anchor=(0.5, 1.05), title='h1n1_vaccine'\n)\nax[0, 1].legend(\n    loc='lower center', bbox_to_anchor=(0.5, 1.05), title='seasonal_vaccine'\n)\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Census MSA, Race or Geographic Region don't seem to be suggestive and are kind of neutral. Might not be that helpful\n* Employment status, Living on Rent and Marital Status maybe relevant for H1N1, but not as such for Seasonal\n* Since there are a majority of people who have taken Seasonal compared to H1N1 vaccine, we might as well focus on the ones relevant to seasonal more, as there is a clearer distinction.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### More Plots!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## The gender distribution of the data\nsns.factorplot('sex',data=data,kind='count',size = 3.5)\n\n## The number of people in each category of concern\nsns.factorplot('h1n1_concern',kind = 'count',data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('h1n1_knowledge',kind = 'count',data = data)\nsns.factorplot('hhs_geo_region',kind = 'count',data = data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('race',kind = 'count',data = data)\nsns.factorplot('education',kind = 'count',data = data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Even More...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[data[\"seasonal_vaccine\"] == 1].groupby(data['hhs_geo_region']).sum()\nplt.title('Geographic region')\nplt.plot(df['seasonal_vaccine'])\nplt.show()\n### people living in geographic regions 1,3,6 are more likely to get vaccinated than the rest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[data[\"h1n1_vaccine\"] == 1].groupby(data['hhs_geo_region']).sum()\nplt.title('Geographic region')\nplt.plot(df['h1n1_vaccine'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[data[\"h1n1_vaccine\"] == 1].groupby(data['education']).sum()\nplt.title('Education')\nplt.plot(df['h1n1_vaccine'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Try to Infer what features are insignificant from the above plots","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4. Feature Selection, Extraction & Engineering <a id=\"fe\"></a> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Creating a new feature 'cleanliness' which combines the behaviour's","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cleanliness'] =  data['behavioral_antiviral_meds']+ data['behavioral_avoidance']+\\\n                        data['behavioral_face_mask']+data['behavioral_wash_hands']+\\\n                       data['behavioral_large_gatherings'] + data['behavioral_outside_home']+\\\n                       data['behavioral_touch_face']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['cleanliness']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('cleanliness',kind = 'count',data = data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the total number of people who have taken the H1N1 vaccine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[data[\"h1n1_vaccine\"]==1].count()[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Plots for the new feature for each vaccine to check the correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[data[\"h1n1_vaccine\"] == 1].groupby(data['cleanliness']).sum()\nplt.title('Cleanliness')\nplt.plot(df['h1n1_vaccine'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[data[\"seasonal_vaccine\"] == 1].groupby(data['cleanliness']).sum()\nplt.title('Cleanliness')\nplt.plot(df['seasonal_vaccine'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"People with cleanliness 2 or above are more likely to get vaccinated than the rest","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Creating a new feature 'opinion' which combines the several 'opinion' columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['opinion'] = data['opinion_h1n1_vacc_effective'] + data['opinion_h1n1_risk']+\\\n                  data['opinion_h1n1_sick_from_vacc'] + data['opinion_seas_vacc_effective']+\\\n                  data['opinion_seas_risk'] + data['opinion_seas_sick_from_vacc']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('opinion',kind = 'count',data = data,size=9)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ooh! a normal distribution like plot is always good to see!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data[data[\"h1n1_vaccine\"] == 1].groupby(data['opinion']).sum()\nplt.title('Opinion')\nplt.plot(df['h1n1_vaccine'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating seperate opinions for both vaccine's","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['opinion_h1n1'] = data['opinion_h1n1_vacc_effective'] + data['opinion_h1n1_risk']-\\\n                  data['opinion_h1n1_sick_from_vacc'] \ndata['opinion_seasonal'] = data['opinion_seas_vacc_effective']+\\\n                  data['opinion_seas_risk'] - data['opinion_seas_sick_from_vacc']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Engineering some more features!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['concern>=2'] = np.where(data['h1n1_concern']>=2,1,0) \n# Since those who have concern>=2 have a pronounced effect on vaccination\n\ndata['good_opinion_vacc'] = np.where(data['opinion_seas_vacc_effective'] == 3,1,0)\n#If the opinion is 3 it means he/she has a good opinion and is more likely to take the vaccine\n\ndata['good_knowledge'] = np.where(data['h1n1_knowledge'] == 2,1,0)\n#Same kind of logic as mentioned previously","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['risk'] = np.where(data['opinion_h1n1_risk']>=4,1,0)\n\n#data['sick'] = np.where(data['opinion_h1n1_sick_from_vacc'] == 3,1,0)\n\n### Generating a column combining concern and knowledge, since they have seemingly good correlations!\n\ndata['concern_knowledge'] = data['h1n1_concern']+data['h1n1_knowledge']\n\n## square of age group to subtely improve the correlation of that feature with the vaccine probabilty\ndata['a^2'] = data['age_group']*data['age_group']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking everythig out!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing all the redundant, irrelevant and non-contributory features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Removing seemingly irrelevant features ######\n\ndata.drop(['race','child_under_6_months','opinion_h1n1_sick_from_vacc','opinion_seas_sick_from_vacc','household_adults','behavioral_antiviral_meds','behavioral_large_gatherings', 'behavioral_outside_home', 'behavioral_antiviral_meds','marital_status',\n           'behavioral_avoidance','behavioral_face_mask','income_poverty','hhs_geo_region','employment_status','education','census_msa'],axis=1,inplace = True)\n\nY_label = data[['h1n1_vaccine','seasonal_vaccine']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Whisker plots","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Below plot indicates that there are few outliers and mostly similar distributions for doctor reccomendation (highly correlated feature) and opinion (an engineered feature).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(x=\"doctor_recc_seasonal\", y=\"opinion_seasonal\", data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"cleanliness (an engineered feature) vs the 'opinion seasonal' ( highly correlated feature)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(x=\"cleanliness\", y=\"opinion_seasonal\", data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cleanliness and Opinion","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.boxplot(x=\"cleanliness\", y=\"opinion\", data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A final Correlation Map with our selected features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\ng = sns.heatmap(corr,  vmax=.3, center=0,\n            square=True, linewidths=1, cbar_kws={\"shrink\": .5}, annot=True, fmt='.2f', cmap='coolwarm')\nsns.despine()\ng.figure.set_size_inches(30,25)\n    \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We now see that the current 26 features are very well/ atleast better correlated with the ground truth labels than the previous Map!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Appending Y labels to the end of the dataframe ####\n\ndata.drop(['respondent_id','h1n1_vaccine','seasonal_vaccine'],axis=1,inplace = True)\nframes =[data,Y_label]\ndata = pd.concat(frames,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = data.columns[:-2]\nprint(features)\nlabels = ['h1n1_vaccine', 'seasonal_vaccine']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Data Splitting and Test data processing <a id=\"split\"></a> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Choosing 80%, 20% for train and validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test = train_test_split(data,test_size = 0.2,shuffle = True)\ntrain_x,train_y = train[features],train[labels]\ntest_x,test_y = test[features],test[labels]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A Utility function is defined below which performs the exact same preprocessing, encodings, feature engineering and other transformations (which was done on the train data) on the Test data given to us.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_test(Test_X):\n    Test_X.drop(['respondent_id','health_insurance','employment_industry','employment_occupation'],axis=1,inplace = True)\n\n    # All the NULL values are populated with the mode\n            \n    d = Test_X\n    str_cols = d.select_dtypes(include = 'object').columns\n\n    ### LabelEcoding all categorical types #####\n    for col in Test_X.columns:\n        if Test_X[col].isnull().sum() and Test_X[col].dtypes != 'object':\n            Test_X[col].loc[(Test_X[col].isnull())] = Test_X[col].median()\n    for col in Test_X.columns:\n        if Test_X[col].isnull().sum() and Test_X[col].dtypes == 'object':\n            Test_X[col].loc[(Test_X[col].isnull())] = Test_X[col].mode().max()\n    LE = LabelEncoder()\n    for col in str_cols:\n        Test_X[col] = LE.fit_transform(Test_X[col]) # Converts to int64\n            \n    data = d\n    ### Synthesizing two new features cleanliness level of the individual and opinion of vaccine ####\n    data['opinion'] = data['opinion_h1n1_vacc_effective'] + data['opinion_h1n1_risk']+\\\n                  data['opinion_h1n1_sick_from_vacc'] + data['opinion_seas_vacc_effective']+\\\n                  data['opinion_seas_risk'] + data['opinion_seas_sick_from_vacc']\n    data['cleanliness'] =  data['behavioral_antiviral_meds']+ data['behavioral_avoidance']+\\\n                        data['behavioral_face_mask']+data['behavioral_wash_hands']+\\\n                       data['behavioral_large_gatherings'] + data['behavioral_outside_home']+\\\n                       data['behavioral_touch_face']\n    data['opinion_h1n1'] = data['opinion_h1n1_vacc_effective'] + data['opinion_h1n1_risk']-\\\n                      data['opinion_h1n1_sick_from_vacc'] \n    data['opinion_seasonal'] = data['opinion_seas_vacc_effective']+\\\n                      data['opinion_seas_risk'] - data['opinion_seas_sick_from_vacc']\n\n    data['concern>=2'] = np.where(data['h1n1_concern']>=2,1,0)\n    data['good_opinion_vacc'] = np.where(data['opinion_seas_vacc_effective'] == 3,1,0) # 5 before\n    data['good_knowledge'] = np.where(data['h1n1_knowledge'] == 2,1,0)\n    data['risk'] = np.where(data['opinion_h1n1_risk']>=4,1,0)\n    data['concern_knowledge'] = data['h1n1_concern']+data['h1n1_knowledge']\n    data['a^2'] = data['age_group']*data['age_group']\n    ###### Dropping other features #########\n    data.drop(['race','child_under_6_months','opinion_h1n1_sick_from_vacc','opinion_seas_sick_from_vacc','household_adults','behavioral_antiviral_meds','behavioral_large_gatherings', 'behavioral_outside_home', 'behavioral_antiviral_meds','marital_status',\n           'behavioral_avoidance','behavioral_face_mask','income_poverty','hhs_geo_region','employment_status','education','census_msa'],axis=1,inplace = True)\n    Test_X = data\n    return Test_X","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading and transforming test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_X = pd.read_csv(r'/kaggle/input/flu-shot-prediction/test_set_features.csv')\nTest_X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_X = transform_test(Test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_X.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarity Check!!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_X.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Test_X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Analyzing Several Models on the Dataset <a id=\"models\"></a> ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"##### Model Generation #####","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* ### **Remember that our task is not a Multi'Class' Classification problem, but instead a Multi'Label' Classification problem.**\n* ### **Thus I have used the Binary Relevance Classifier to train a model to individually predict for each 'h1n1_vaccine' and for 'seasonal_vaccine' separately and later combine them to calculate the overall accuracy** ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 6.1 Ensembling with 20 Classifiers and Linear models <a id=\"ensemble\"></a> ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    #gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    #svm.SVC(probability=True),\n    #svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    XGBClassifier()    \n    ]\n\n\n\n#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = data['h1n1_vaccine']\n\n#index through MLA and save performance to table\nrow_index = 0\ndata1 = data.copy()\nfor alg in MLA:\n    data = data1\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    print('Executing ',MLA_name)\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, data[features], data['h1n1_vaccine'], cv  = cv_split)\n    #print(cv_results.keys())\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    #MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n    #save MLA predictions - see section 6 for usage\n    alg.fit(data[features], data['h1n1_vaccine'])\n    MLA_predict[MLA_name] = alg.predict(data[features])\n    row_index+=1\n\n    \n#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Utility Function to print the accuracy of the model ###\n\ndef find_acc(clf,X,truth,s):\n    pred = clf.predict_proba(X.values)\n    pred = pred.toarray()\n    accuracy = roc_auc_score(truth,pred)\n    print(s+\" Accuracy is : \",accuracy*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"hpo\"></a> \n## 6.2 Trying out Hyper Parameter Optimization (HPO) on some Select Models. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Tried out HPO with \n\n    1. Grid Search CV\n    2. Randomized Search CV","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Check out the model and technique of HPO used in the comments given in each cell","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Random Forest Classifier with Grid Search CV for HPO #####\n\nfrom sklearn.model_selection import GridSearchCV\ngrid_1 = {      \"n_estimators\"      : [100,200,500],\n               \"criterion\"         : [\"gini\", \"entropy\"],\n               \"max_features\"      : ['sqrt','log2',0.2,0.5,0.8],\n               \"max_depth\"         : [3,4,6,10],\n               \"min_samples_split\" : [2, 5, 20,50] }\nRF=RandomForestClassifier()\ngs = GridSearchCV(RF, grid_1, n_jobs=-1, cv=2,verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Training & Performance ###\n\nclf = BinaryRelevance(classifier=gs, require_dense=[True,True])\nclf.fit(train_x,train_y)\nfind_acc(clf,train_x,train_y,'Training')\nfind_acc(clf,test_x,test_y,'Cross Validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Random Forest Classifier with Randomized Search CV for HPO #####\n\nfrom sklearn.model_selection import RandomizedSearchCV\nn_estimators = [100,200,500]\nmax_features = ['auto', 'sqrt']\nmax_depth = [15,20,25]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrf = RF\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20,\\\n                               cv = 3, verbose=2, random_state=42, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Training & Performance ###\nclf = BinaryRelevance(classifier=rf_random, require_dense=[True,True])\nclf.fit(train_x,train_y)\nfind_acc(clf,train_x,train_y,'Training')\nfind_acc(clf,test_x,test_y,'Cross Validation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### A sample NN model for reference","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Neural Network based Classifier on Keras sample ##\n\ndef NN_classifier(input_dim,output_dim):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(25,input_dim=input_dim,activation = 'relu'),\n        tf.keras.layers.Dense(128,activation = 'relu'),\n        tf.keras.layers.Dense(64,activation = 'relu'),\n        tf.keras.layers.Dense(output_dim,activation = 'softmax')\n    ])\n    model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### To use NN Model ####\n\n# KERAS_PARAMS = dict(epochs=100, batch_size=50, verbose=True)\n# clf = BinaryRelevance(classifier=Keras(NN_classifier, False, KERAS_PARAMS), require_dense=[True,True])\n# clf.fit(train_x,train_y)\n\n#### To use RF basic #####\n\n# RandomForestClassifier(max_depth = 5,random_state = 42,verbose = True)\n\n#### To use RSCV RF ####\n\n# rf_random\n\n#### To us GSCV RF ####\n\n# gs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nparam2 = {\n            'learning_rate':     [0.01,0.1],\n            'max_depth':         [2,3,4],\n            'lambda':            [1.0,1.5],\n            'subsample':        [0.1,0.2,0.3],\n            'colsample_bytree': [0.3,0.5,0.6],\n            'min_split_loss' :  [0.01,0.2],\n            'min_split_size' : [2,4,6]\n        }\nclassifier = XGBClassifier(feature_names = features,verbose = False)\nxgb = GridSearchCV(classifier, param2, n_jobs=-1, cv=2,verbose = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Training & Performance ###\nclf = BinaryRelevance(classifier=xgb, require_dense=[True,True])\nclf.fit(train_x,train_y)\nfind_acc(clf,train_x,train_y,'Training')\nfind_acc(clf,test_x,test_y,'Cross Validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Random Forest Classifier with Randomized Search CV for HPO #####\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nn_estimators = [100,200]\nmax_features = ['auto', 'sqrt']\nmax_depth = [2,3,7,5,15,20,25]\nmax_depth.append(None)\nmin_samples_split = [0.2,0.3,0.6,1.2,1.5,2.0,3.0,4.0]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrf = XGBClassifier(feature_names = features,verbose = False)\netc = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 20,\\\n                               cv = 4, verbose=2, random_state=42, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Training & Performance ###\nclf = BinaryRelevance(classifier=etc, require_dense=[True,True])\nclf.fit(train_x,train_y)\nfind_acc(clf,train_x,train_y,'Training')\nfind_acc(clf,test_x,test_y,'Cross Validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cb_hps = {  \n            'depth':              [2,3,4,5,7],\n            'l2_leaf_reg':        [2,3,5,10],\n            'random_seed':        [5,8,10],\n            'colsample_bylevel': [0.3,0.5,0.6],\n            'n_estimators':      [100,200]\n        }\n\ncb = CatBoostClassifier()\ncbt = RandomizedSearchCV(estimator = cb, param_distributions = cb_hps, n_iter = 20,\\\n                               cv = 4, verbose=2, random_state=42, n_jobs = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Training & Performance ###\n\nclf = BinaryRelevance(classifier=cbt, require_dense=[True,True])\nclf.fit(train_x,train_y)\nfind_acc(clf,train_x,train_y,'Training')\nfind_acc(clf,test_x,test_y,'Cross Validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Training a binary relevance classifier for the two labels h1n1 vaccine and seasonal vaccine #####\n\nclassifier = CatBoostClassifier(\nn_estimators = 300,depth = 5,l2_leaf_reg =0.5,\n                                random_seed = 2,colsample_bylevel = 0.9,verbose = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Training & Performance ###\n\nclf = BinaryRelevance(classifier=classifier, require_dense=[True,True])\nclf.fit(train_x,train_y)\nfind_acc(clf,train_x,train_y,'Training')\nfind_acc(clf,test_x,test_y,'Cross Validation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Final Notes & Submission <a id=\"final\"></a> ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. We see that our training accuracies are consistently within the range of 85% - 90%\n2. Our validation accuracies are also close to the training accuracies which indicates that the model hasn't fallen prey to overfitting/bias\n3. Feel free to tweak around with any of the values of the hyper-parameters or fiddle with the models. These were the best results that i could obtain with the given models\n4. Disclaimer: On submission to the competition site the model may give varying accuracies.\n5. I have tried to demonstrate as many different model performances to minimize your time in developing a better more accurate model!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Preparing the submission file in the specified format as given in 'submission_format.csv'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = clf.predict_proba(Test_X.values)\nsubmission_df = pd.DataFrame.sparse.from_spmatrix(submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trial = submission_df\ntrial['respondent_id'] = trial.index+26707 # Converting the default indices\ntrial['h1n1_vaccine'] = trial[0].astype('float64') # Data required to be in float64 \ntrial['seasonal_vaccine'] = trial[1].astype('float64')\ntrial.set_index('respondent_id') # Make index as respondent_id\ntrial.drop([0,1],axis=1,inplace=True) # Drop old index axis\nsubmission_df = trial ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df # Check if all OK","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.to_csv('submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## That's It!\n## You've now seen so many techniques of Preprocessing, EDA, Feature Selection, Feature Engineering,Ensembling, Linear Models and HPO!!\n## I hope this notebook has helped you out towards your competitive ML journey\n\n## Kindly upvote if you found the content useful !\n\n### P.S: It takes a lot of time to develop these notebooks error free for you to understand, motivation is highly appreciated :-P ;-) ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}