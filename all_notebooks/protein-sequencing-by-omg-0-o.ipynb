{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# **CONTEXT**\n\nThis is a protein data set retrieved from Research Collaboratory for Structural Bioinformatics (RCSB) Protein Data Bank (PDB).\n\nThe PDB archive is a repository of atomic coordinates and other information describing proteins and other important biological macromolecules. Structural biologists use methods such as X-ray crystallography, NMR spectroscopy, and cryo-electron microscopy to determine the location of each atom relative to each other in the molecule. They then deposit this information, which is then annotated and publicly released into the archive by the wwPDB.\n\nThe constantly-growing PDB is a reflection of the research that is happening in laboratories across the world. This can make it both exciting and challenging to use the database in research and education. Structures are available for many of the proteins and nucleic acids involved in the central processes of life, so you can go to the PDB archive to find structures for ribosomes, oncogenes, drug targets, and even whole viruses. However, it can be a challenge to find the information that you need, since the PDB archives so many different structures. You will often find multiple structures for a given molecule, or partial structures, or structures that have been modified or inactivated from their native form."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# **WORK DONE**\nThis notebooks shows how to classify protein families soley based on their sequence of aminoacids. This work is based on the current success of deep learning models in natural language processing (NLP) and assumes the proteins sequences can be viewed as as a language. Please note, that there are notable search engines such as BLAST for this task."},{"metadata":{"trusted":true,"_uuid":"93d2647053a0e4780fd455dae2a5c68a9d658b7e"},"cell_type":"code","source":"# Importing Libraries and DATA\nimport numpy as np\nimport pandas as pd\ndf_dup = pd.read_csv('../input/pdb_data_no_dups.csv')\ndf_seq = pd.read_csv('../input/pdb_data_seq.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19fc48ead3175dfbd509c9a546237273424ec034"},"cell_type":"code","source":"# Merge the two Data set together\ndf = df_dup.merge(df_seq,how='inner',on='structureId')\n\n# Drop rows with missing labels\ndf = df[[type(c) == type('') for c in df.classification.values]]\ndf = df[[type(c) == type('') for c in df.sequence.values]]\n\n# select proteins\ndf = df[df.macromoleculeType_x == 'Protein']\ndf.reset_index()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"524e0f4dad4ff6faf6403a2c8f6e8d4481148dec"},"cell_type":"markdown","source":"# Visualize and preprocess dataset\nTo build a model with appropriate number of instances per class. Let us only focus on the ten most common classes."},{"metadata":{"trusted":true,"_uuid":"fa1960781f7c49fbbee9f8029ee457cbbdb96a8f"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, LabelBinarizer\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\ncnt = Counter(df.classification)\n# select only 10 most common classes!\ntop_classes = 10\ntmp = np.array([[c[0], c[1]] for c in cnt.most_common()[:top_classes]])\n[classes, counts] = tmp[:,0], tmp[:,1].astype(int)\n\nN = sum(counts)\nplt.bar(range(len(classes)), counts/float(N))\nplt.xticks(range(len(classes)), classes, rotation='vertical')\n#plt.xlabel('protein class')\nplt.ylabel('frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bfaf8f88e5b93dd943421f513570a0c87f023e8"},"cell_type":"markdown","source":"# Transform labels"},{"metadata":{"trusted":true,"_uuid":"1e0460c1f78de14cd50330bcab38de2637ed9ee1"},"cell_type":"code","source":"df = df[[c in classes for c in df.classification]]\nseqs = df.sequence.values\n\n# Transform labels into one-hot\nlb = LabelBinarizer()\nY = lb.fit_transform(df.classification)\n\nlengths = [len(s) for s in seqs]\nplt.hist(lengths, bins=100, normed=True)\nplt.xlabel('sequence length')\nplt.ylabel('frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36c231d4ed8fc33a7147a157d31d530377244330"},"cell_type":"markdown","source":"# Further preprocessing of sequences with keras\n1. ** Tokenizer**: translates every character of the sequence into a number\n2. **pad_sequences:** ensures that every sequence has the same length (max_length). \n3. **train_test_split:** from sklearn splits the data into training and testing samples."},{"metadata":{"trusted":true,"_uuid":"33c67e967f7400423bca55a33318684a71eba6a2"},"cell_type":"code","source":"from keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom sklearn.model_selection import train_test_split\n\nmax_length = 512\n\n#create and fit tokenizer\ntokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts(seqs)\n#represent input data as word rank number sequences\nX = tokenizer.texts_to_sequences(seqs)\nX = sequence.pad_sequences(X, maxlen=max_length)\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=.7)\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5b06bf09b7f3b5e19b6bc123de737a937624f16"},"cell_type":"markdown","source":"# Let's build up the keras model and get it on fire"},{"metadata":{"trusted":true,"_uuid":"1cb278b0d051466013ae29386a583ce328324ee0"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\n\nembedding_dim = 16\n\n# create the model\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.index_docs)+1, embedding_dim, input_length=max_length))\nmodel.add(Conv1D(filters=64, kernel_size=6, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(top_classes, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])\n\nprint(model.summary())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d430804724644389c9cc1ef0d6a5238648f5771d"},"cell_type":"code","source":"model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"040b9f6097e1f9164acf10dee19e8240fc4cca2e"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \ntrain_pred = model.predict(X_train)\ntest_pred = model.predict(X_test)\nprint(\"train-acc = \" + str(accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_pred, axis=1))))\nprint(\"test-acc = \" + str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))))\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))\nnp.set_printoptions(precision=2)\n\n# Plot normalized confusion matrix\nplt.figure(figsize=(10,10))\nplot_confusion_matrix(cnf_matrix, classes=lb.classes_, normalize=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7de2cddfe42f55ba0cf5332ccec58ae8b82171f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}