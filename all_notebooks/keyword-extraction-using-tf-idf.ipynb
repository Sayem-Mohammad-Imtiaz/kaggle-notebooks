{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n\n\n"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load the dataset\ndf = pd.read_csv('/kaggle/input/nips-papers/papers.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"{} abstracts are missing\".format(df[df['abstract']=='Abstract Missing']['abstract'].count()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pprint\nsample = 943\npprint.pprint(\"TITLE:{}\".format(df['title'][sample]))\npprint.pprint(\"ABSTRACT:{}\".format(df['abstract'][sample]))\npprint.pprint(\"FULL TEXT:{}\".format(df['paper_text'][sample][:1000]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nstop_words = set(stopwords.words('english'))\n##Creating a list of custom stopwords\nnew_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n             \"show\", \"result\", \"large\", \n             \"also\", \"one\", \"two\", \"three\", \n             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\nstop_words = list(stop_words.union(new_words))\n\ndef pre_process(text):\n    \n    # lowercase\n    text=text.lower()\n    \n    #remove tags\n    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    \n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    \n    ##Convert to list from string\n    text = text.split()\n    \n    # remove stopwords\n    text = [word for word in text if word not in stop_words]\n\n    # remove words less than three letters\n    text = [word for word in text if len(word) >= 3]\n\n    # lemmatize\n    lmtzr = WordNetLemmatizer()\n    text = [lmtzr.lemmatize(word) for word in text]\n    \n    return ' '.join(text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ndocs = df['paper_text'].apply(lambda x:pre_process(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"docs[1][0:103]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import CountVectorizer\n#docs = docs.tolist()\n#create a vocabulary of words, \ncv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n                   max_features=10000,  # the size of the vocabulary\n                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n                  )\nword_count_vector=cv.fit_transform(docs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(word_count_vector)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n\ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n\n    for idx, score in sorted_items:\n        fname = feature_names[idx]\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get feature names\nfeature_names=cv.get_feature_names()\n\ndef get_keywords(idx, docs):\n\n    #generate tf-idf for the given document\n    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n\n    #sort the tf-idf vectors by descending order of scores\n    sorted_items=sort_coo(tf_idf_vector.tocoo())\n\n    #extract only the top n; n here is 10\n    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n    \n    return keywords\n\ndef print_results(idx,keywords, df):\n    # now print the results\n    print(\"\\n=====Title=====\")\n    print(df['title'][idx])\n    #print(\"\\n=====Abstract=====\")\n    #print(df['abstract'][idx])\n    print(\"\\n===Keywords===\")\n    for k in keywords:\n        print(k,keywords[k])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx=238\nkeywords=get_keywords(idx, docs)\nprint_results(idx,keywords, df)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}