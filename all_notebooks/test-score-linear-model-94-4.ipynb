{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-16T17:55:53.983494Z","iopub.execute_input":"2021-07-16T17:55:53.983839Z","iopub.status.idle":"2021-07-16T17:55:54.006701Z","shell.execute_reply.started":"2021-07-16T17:55:53.983768Z","shell.execute_reply":"2021-07-16T17:55:54.004721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****TEST SCORE","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom statsmodels.stats.diagnostic import het_white","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:13:59.706366Z","iopub.execute_input":"2021-07-16T18:13:59.706674Z","iopub.status.idle":"2021-07-16T18:13:59.712782Z","shell.execute_reply.started":"2021-07-16T18:13:59.706647Z","shell.execute_reply":"2021-07-16T18:13:59.711801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_score  = pd.read_csv('/kaggle/input/predict-test-scores-of-students/test_scores.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T17:59:47.39066Z","iopub.execute_input":"2021-07-16T17:59:47.390989Z","iopub.status.idle":"2021-07-16T17:59:47.418004Z","shell.execute_reply.started":"2021-07-16T17:59:47.390967Z","shell.execute_reply":"2021-07-16T17:59:47.41684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_score.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:00:00.725069Z","iopub.execute_input":"2021-07-16T18:00:00.725463Z","iopub.status.idle":"2021-07-16T18:00:00.765333Z","shell.execute_reply.started":"2021-07-16T18:00:00.725433Z","shell.execute_reply":"2021-07-16T18:00:00.764466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_score.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:00:18.230851Z","iopub.execute_input":"2021-07-16T18:00:18.231113Z","iopub.status.idle":"2021-07-16T18:00:18.25422Z","shell.execute_reply.started":"2021-07-16T18:00:18.231089Z","shell.execute_reply":"2021-07-16T18:00:18.252892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_score.describe()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:04.994097Z","iopub.execute_input":"2021-07-16T18:01:04.994497Z","iopub.status.idle":"2021-07-16T18:01:05.021916Z","shell.execute_reply.started":"2021-07-16T18:01:04.994469Z","shell.execute_reply":"2021-07-16T18:01:05.021053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We drop the id since it has no role in the predicting task at all.","metadata":{}},{"cell_type":"code","source":"df_test_score = df_test_score.drop(columns=['student_id'])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:01:43.484703Z","iopub.execute_input":"2021-07-16T18:01:43.485468Z","iopub.status.idle":"2021-07-16T18:01:43.491952Z","shell.execute_reply.started":"2021-07-16T18:01:43.48538Z","shell.execute_reply":"2021-07-16T18:01:43.491043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is transformed into categorical.","metadata":{}},{"cell_type":"code","source":"for col in ['school' ,'school_setting', 'school_type', 'classroom', 'teaching_method', 'gender', 'lunch']:\n    df_test_score[col] = df_test_score[col].astype('category')","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:02:15.834148Z","iopub.execute_input":"2021-07-16T18:02:15.834623Z","iopub.status.idle":"2021-07-16T18:02:15.845203Z","shell.execute_reply.started":"2021-07-16T18:02:15.834589Z","shell.execute_reply":"2021-07-16T18:02:15.844657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We make sure there is no null values.","metadata":{}},{"cell_type":"code","source":"df_test_score.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:02:44.848712Z","iopub.execute_input":"2021-07-16T18:02:44.84912Z","iopub.status.idle":"2021-07-16T18:02:44.860338Z","shell.execute_reply.started":"2021-07-16T18:02:44.849088Z","shell.execute_reply":"2021-07-16T18:02:44.859304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****EDA","metadata":{}},{"cell_type":"markdown","source":"We start the exploration of the numerical variables.","metadata":{}},{"cell_type":"code","source":"df_test_score.hist()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:03:28.007278Z","iopub.execute_input":"2021-07-16T18:03:28.00759Z","iopub.status.idle":"2021-07-16T18:03:28.604026Z","shell.execute_reply.started":"2021-07-16T18:03:28.007567Z","shell.execute_reply":"2021-07-16T18:03:28.603084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More interesting than the single univariate plots, we can explore relationship betewwen each pair of variables. immediately it outcomes a linear relathionship between de variables pretest and posttest. By the other hand, it seems to exists a different behaviour in extreme regions of n_students where variable is too high or to low.","metadata":{}},{"cell_type":"code","source":"attributes = ['n_student', 'pretest', 'posttest']\nscatter_matrix(df_test_score[attributes])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:03:50.780369Z","iopub.execute_input":"2021-07-16T18:03:50.780709Z","iopub.status.idle":"2021-07-16T18:03:51.535874Z","shell.execute_reply.started":"2021-07-16T18:03:50.780679Z","shell.execute_reply":"2021-07-16T18:03:51.535316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How much variance is explained by pretest? The answer is below","metadata":{}},{"cell_type":"code","source":"lin_reg =  LinearRegression()\nx = np.array(df_test_score['pretest']).reshape(-1, 1)\ny = np.array(df_test_score['posttest']).reshape(-1, 1)\nlin_reg.fit(x, y)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:04:22.295065Z","iopub.execute_input":"2021-07-16T18:04:22.295394Z","iopub.status.idle":"2021-07-16T18:04:22.49968Z","shell.execute_reply.started":"2021-07-16T18:04:22.295371Z","shell.execute_reply":"2021-07-16T18:04:22.498826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r_sq = lin_reg.score(x, y)\nprint('coefficient of determination:', r_sq)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:04:32.566501Z","iopub.execute_input":"2021-07-16T18:04:32.566922Z","iopub.status.idle":"2021-07-16T18:04:32.572333Z","shell.execute_reply.started":"2021-07-16T18:04:32.566898Z","shell.execute_reply":"2021-07-16T18:04:32.571313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A 90% of the variance is already explained by one variable. So, the strategy consists of including the most relevant variables and eliminating the noising ones. We also explore the categorical-response relationship.","metadata":{}},{"cell_type":"code","source":"for col in ['school' , 'school_setting', 'school_type', 'classroom', 'teaching_method', 'gender', 'lunch']:\n  sns.catplot(x=col, y=\"posttest\", kind=\"box\", data=df_test_score)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:05:55.170158Z","iopub.execute_input":"2021-07-16T18:05:55.170603Z","iopub.status.idle":"2021-07-16T18:06:00.090891Z","shell.execute_reply.started":"2021-07-16T18:05:55.170575Z","shell.execute_reply":"2021-07-16T18:06:00.089557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Variable Selection","metadata":{}},{"cell_type":"markdown","source":"We are going to use a base tree model to select the variables, i.e. random forest. We proved two more models (XGboost and CART) with the same variable selection.","metadata":{}},{"cell_type":"markdown","source":"Let's use numerical encoding for the categorical features.","metadata":{}},{"cell_type":"code","source":"le = preprocessing.LabelEncoder()\nle.fit(df_test_score['gender'])\nle.transform(df_test_score['gender'])\ndf_train = df_test_score\nfor col in ['school' , 'school_setting', 'school_type', 'classroom', 'teaching_method', 'gender', 'lunch']:\n  le.fit(df_train[col])\n  df_train[col] = le.transform(df_train[col])","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:08:39.407267Z","iopub.execute_input":"2021-07-16T18:08:39.407579Z","iopub.status.idle":"2021-07-16T18:08:39.421614Z","shell.execute_reply.started":"2021-07-16T18:08:39.407556Z","shell.execute_reply":"2021-07-16T18:08:39.420479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.drop(columns=['posttest'])\ny = df_train['posttest']\ndt_model = RandomForestRegressor()\ndt_model.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:09:18.864736Z","iopub.execute_input":"2021-07-16T18:09:18.865022Z","iopub.status.idle":"2021-07-16T18:09:19.482557Z","shell.execute_reply.started":"2021-07-16T18:09:18.864999Z","shell.execute_reply":"2021-07-16T18:09:19.482057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_list = list(X.columns)\n# Set the style\nimportances = list(dt_model.feature_importances_)\nplt.style.use('fivethirtyeight')\nx_values = list(range(len(importances)))\nplt.bar(x_values, importances, orientation = 'vertical')\nplt.xticks(x_values, feature_list, rotation='vertical')\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:10:11.847871Z","iopub.execute_input":"2021-07-16T18:10:11.848346Z","iopub.status.idle":"2021-07-16T18:10:12.021921Z","shell.execute_reply.started":"2021-07-16T18:10:11.848311Z","shell.execute_reply":"2021-07-16T18:10:12.020707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We select only those variables that explain the 95% of the variance. As a result, only two variables are selected.","metadata":{}},{"cell_type":"code","source":"feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n\nsorted_importances = [importance[1] for importance in feature_importances]\nsorted_features = [importance[0] for importance in feature_importances]\n# Cumulative importances\ncumulative_importances = np.cumsum(sorted_importances)\n# Make a line graph\nplt.plot(x_values, cumulative_importances, 'g-')\n# Draw line at 95% of importance retained\nplt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n# Format x ticks and labels\nplt.xticks(x_values, sorted_features, rotation = 'vertical')\n# Axis labels and title\nplt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:10:55.630137Z","iopub.execute_input":"2021-07-16T18:10:55.630467Z","iopub.status.idle":"2021-07-16T18:10:55.837383Z","shell.execute_reply.started":"2021-07-16T18:10:55.630443Z","shell.execute_reply":"2021-07-16T18:10:55.836151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sel_var = df_test_score[['pretest', 'teaching_method', 'posttest']]","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:11:10.577713Z","iopub.execute_input":"2021-07-16T18:11:10.578135Z","iopub.status.idle":"2021-07-16T18:11:10.584607Z","shell.execute_reply.started":"2021-07-16T18:11:10.578103Z","shell.execute_reply":"2021-07-16T18:11:10.583342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Model Training","metadata":{}},{"cell_type":"markdown","source":"Finally, we train a simple linear regression model.","metadata":{}},{"cell_type":"code","source":"X = df_sel_var.drop(columns=['posttest'])\ny = df_sel_var['posttest']\nX_training_data, X_testing_data, y_training_data, y_testing_data = train_test_split(X, y, test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:11:58.334024Z","iopub.execute_input":"2021-07-16T18:11:58.334372Z","iopub.status.idle":"2021-07-16T18:11:58.344759Z","shell.execute_reply.started":"2021-07-16T18:11:58.334343Z","shell.execute_reply":"2021-07-16T18:11:58.343195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model =  LinearRegression()\nfinal_model.fit(X_training_data, y_training_data)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:12:08.752738Z","iopub.execute_input":"2021-07-16T18:12:08.753085Z","iopub.status.idle":"2021-07-16T18:12:08.767441Z","shell.execute_reply.started":"2021-07-16T18:12:08.753055Z","shell.execute_reply":"2021-07-16T18:12:08.766687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r_sq = final_model.score(X_training_data, y_training_data)\nprint('Coefficient of determination for training data:', r_sq)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:12:29.860519Z","iopub.execute_input":"2021-07-16T18:12:29.861052Z","iopub.status.idle":"2021-07-16T18:12:29.869428Z","shell.execute_reply.started":"2021-07-16T18:12:29.861017Z","shell.execute_reply":"2021-07-16T18:12:29.868343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1 = sm.add_constant(X_training_data)\nmodelOLS = sm.OLS(y_training_data, X1)\nmodelOLS_fit = modelOLS.fit()\nmodelOLS_fit.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:13:21.313281Z","iopub.execute_input":"2021-07-16T18:13:21.313639Z","iopub.status.idle":"2021-07-16T18:13:21.361749Z","shell.execute_reply.started":"2021-07-16T18:13:21.31361Z","shell.execute_reply":"2021-07-16T18:13:21.360136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Jarque-Bera test ensures the normality of the residuals and all the coeficients are significant and the rest of hipothesis underliying linear model, except ot Heteroskedasticity.","metadata":{}},{"cell_type":"code","source":"white_test = het_white(modelOLS_fit.resid,  modelOLS_fit.model.exog)","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:14:22.400934Z","iopub.execute_input":"2021-07-16T18:14:22.401283Z","iopub.status.idle":"2021-07-16T18:14:22.408344Z","shell.execute_reply.started":"2021-07-16T18:14:22.401254Z","shell.execute_reply":"2021-07-16T18:14:22.407453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#white_test\nlabels = ['LM Statistic', 'LM-Test p-value', 'F-Statistic', 'F-Test p-value']\nprint(dict(zip(labels, white_test)))","metadata":{"execution":{"iopub.status.busy":"2021-07-16T18:14:32.790625Z","iopub.execute_input":"2021-07-16T18:14:32.791129Z","iopub.status.idle":"2021-07-16T18:14:32.79755Z","shell.execute_reply.started":"2021-07-16T18:14:32.791093Z","shell.execute_reply":"2021-07-16T18:14:32.796799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Heteroskedasticity assumption is also checked.","metadata":{}},{"cell_type":"code","source":"y_pred = final_model.predict(X_testing_data)\nprint('Validation MAE', mean_absolute_error(y_testing_data, y_pred))\nprint('Validation RMSE', sqrt(mean_squared_error(y_testing_data, y_pred)))\nprint('Validation R^2', final_model.score(X_testing_data, y_testing_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The predictions are in generral quiet accurate.","metadata":{}}]}