{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### Hi, welcome to my project! Today we will be using Logistic Regression Classifier algorithms to predict human activities.\n#### We will use the Human Activity Recognition with Smartphones database, which was built from the recordings of study participants performing activities of daily living (ADL) while carrying a smartphone with an embedded inertial sensors. The objective is to classify activities into one of the six activities (walking, walking upstairs, walking downstairs, sitting, standing, and laying) performed.\n\nFor each record in the dataset it is provided:\n\nTriaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.\nTriaxial Angular velocity from the gyroscope.\nA 561-feature vector with time and frequency domain variables.\nIts activity label.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns, pandas as pd, numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading our csv file:","metadata":{}},{"cell_type":"code","source":"filepath = '../input/logistic-regression/Human_Activity_Recognition_Using_Smartphones_Data.csv'\ndata = pd.read_csv(filepath, sep=',')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploring our data:","metadata":{}},{"cell_type":"code","source":"data.dtypes.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data columns are all floats except for the activity label.","metadata":{}},{"cell_type":"code","source":"data.iloc[:,:-1].min().value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.iloc[:,:-1].max().value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data are all scaled from -1 (minimum) to 1.0 (maximum).","metadata":{}},{"cell_type":"markdown","source":"Examine the breakdown of activities--they are relatively balanced.","metadata":{}},{"cell_type":"code","source":"data.Activity.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.Activity.value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scikit learn classifiers won't accept a sparse matrix for the prediction column. Thus, either LabelEncoder needs to be used to convert the activity labels to integers, or if DictVectorizer is used, the resulting matrix must be converted to a non-sparse array.\nWe are going to use LabelEncoder to fit_transform the \"Activity\" column, and look at 5 random values.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndata['Activity'] = le.fit_transform(data.Activity)\ndata['Activity'].sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le.inverse_transform([0,1,2,3,4,5])      # Only to know which one corresponds to each number","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le.transform(['STANDING'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's calculate the correlations between each column:","metadata":{}},{"cell_type":"code","source":"feature_cols = data.columns[:-1]\ncorr_values = data[feature_cols].corr()\n\n# Simplify by emptying all the data below the diagonal\ntril_index = np.tril_indices_from(corr_values)\n\n# Make the unused values NaNs\nfor coord in zip(*tril_index):\n    corr_values.iloc[coord[0], coord[1]] = np.NaN\n    \n# Stack the data and convert to a data frame\ncorr_values = (corr_values\n               .stack()\n               .to_frame()\n               .reset_index()\n               .rename(columns={'level_0':'feature1',\n                                'level_1':'feature2',\n                                0:'correlation'}))\n\n# Get the absolute values for sorting\ncorr_values['abs_correlation'] = corr_values.correlation.abs()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(corr_values)   # Number of rows in the tuple (table) is: n(n-1)/2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A histogram of the absolute value correlations:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_context('talk')\nsns.set_style('white')\n\nax = corr_values.abs_correlation.hist(bins=50, figsize=(12, 8))\nax.set(xlabel='Absolute Correlation', ylabel='Frequency');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The most highly correlated values\ncorr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.9')\n### END SOLUTION","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting our dataset:\nThis can be done using any method, but for this project we will use Scikit-learn's StratifiedShuffleSplit to maintain the same ratio of predictor classes.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\n# Get the split indexes\nstrat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n                                          test_size=0.3, \n                                          random_state=42)\n\ntrain_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.Activity))\n\n# Create the dataframes\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, 'Activity']\n\nX_test  = data.loc[test_idx, feature_cols]\ny_test  = data.loc[test_idx, 'Activity']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the ratio between the classes in our y-test only to compare with the original dataset","metadata":{}},{"cell_type":"code","source":"y_test.value_counts(normalize=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building Logistic Regression models:","metadata":{}},{"cell_type":"markdown","source":"#### Firstly let's fit a logistic regression model without any regularization using all of the features. \n\n#### Then we will use cross validation to determine the hyperparameters fitting models using L1 and L2 regularization.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression \n\n#Standard logistic regression:\nlr = LogisticRegression(solver='liblinear').fit(X_train,y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV\n\n# L1 regularized logistic regression\nlr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# L2 regularized logistic regression\nlr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2', solver='liblinear').fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparing the magnitudes of the coefficients for each model. As one-vs-rest fitting was used, each set of coefficients can be plotted separately.","metadata":{}},{"cell_type":"markdown","source":"Just to have an idea how this looks like, lets see the coefficients for \"lr\" model and transpose it:","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(lr.coef_).T ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we are going to combine all the coefficients into a dataframe\ncoefficients = list()\n\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\n\nfor lab,mod in zip(coeff_labels, coeff_models):\n    coeffs = mod.coef_\n    coeff_label = pd.MultiIndex(levels=[[lab], [0,1,2,3,4,5]], \n                                 codes=[[0,0,0,0,0,0], [0,1,2,3,4,5]])\n    coefficients.append(pd.DataFrame(coeffs.T, columns=coeff_label))\n\ncoefficients = pd.concat(coefficients, axis=1)\n\ncoefficients.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we know from logistic regression when we have multiclass label the model will find a set of coefficients for each class, due to the \"one vs rest method\". Thus, each model contain 6 sets of coefficients which differ from one another. \n\nIn the following step I'm going to focus on each class, so I will plot the sets of coefficients obtained for the first class and labeling by color the 3 models, then apply the same for second class and so on and so forth.","metadata":{}},{"cell_type":"markdown","source":"### Displaying six separate plots for each of the multi-class coefficients:","metadata":{}},{"cell_type":"code","source":"fig, axList = plt.subplots(nrows=3, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(12,12)\n\nfor loc, ax in enumerate(axList):\n    data = coefficients.xs(loc, level=1, axis=1)\n    data.plot(marker='o', ls='', ms=2.0, ax=ax, legend=False)\n    \n    if ax is axList[0]:\n        ax.legend(loc=4)\n        \n    ax.set(title='Coefficient Set '+str(loc))\n\nplt.tight_layout()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predict and store the class for each model.\nStore the probability for the predicted class for each model.","metadata":{}},{"cell_type":"code","source":"# Predict the class and the probability for each\ny_pred = list()\ny_prob = list()\n\ncoeff_labels = ['lr', 'l1', 'l2']\ncoeff_models = [lr, lr_l1, lr_l2]\n\nfor lab,mod in zip(coeff_labels, coeff_models):\n    y_pred.append(pd.Series(mod.predict(X_test), name=lab))\n    y_prob.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n    \ny_pred = pd.concat(y_pred, axis=1)\ny_prob = pd.concat(y_prob, axis=1)\n\ny_pred.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_prob.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below we are displaying all rows which have different predicted classes by models \"lr != l1\", We can see this for all columns in a better way ploting confusion matrix.","metadata":{}},{"cell_type":"code","source":"y_pred[y_pred['lr']!=y_pred['l1']] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Computing error metrics:","metadata":{}},{"cell_type":"markdown","source":"We could see in detail the error metrics for the 3 models by using classification report:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint('Classification report for Logistic regression without regularization:')\nprint(classification_report(y_test,y_pred['lr']))\n\nprint('Classification report for Logistic regression with L1(Lasso) regularization:')\nprint(classification_report(y_test,y_pred['l1']))\n    \nprint('Classification report for Logistic regression with L2(Ridge) regularization:')\nprint(classification_report(y_test,y_pred['l2']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to summarize and average the values obtained we will do the following:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\n\nmetrics = list()\ncm = dict()\n\nfor lab in coeff_labels:\n\n    # Precision, recall, f-score from the multi-class support function, we will average them because we will have one value per class\n    precision, recall, fscore, _ = score(y_test, y_pred[lab], average='weighted')\n    \n    # The usual way to calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred[lab])\n    \n    # ROC-AUC scores can be calculated by binarizing the data\n    auc = roc_auc_score(label_binarize(y_test, classes=[0,1,2,3,4,5]),\n              label_binarize(y_pred[lab], classes=[0,1,2,3,4,5]), \n              average='weighted')\n    \n    # Last, the confusion matrix\n    cm[lab] = confusion_matrix(y_test, y_pred[lab])\n    \n    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n                              'fscore':fscore, 'accuracy':accuracy,\n                              'auc':auc}, \n                             name=lab))\n\nmetrics = pd.concat(metrics, axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In the table above we don't see a difference statistically significant in the error metrics for the 3 models, even just using the first one (without regularization) we can expect to perform great at predicting the activities**. In order to see a difference a bit more highlighted we could plot their corresponding confusion matrix as following:","metadata":{}},{"cell_type":"markdown","source":"# Displaying the confusion matrix for each model:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(confusion_matrix=cm['lr'], display_labels=lr.classes_)\ndisp.plot(cmap='Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(confusion_matrix=cm['l1'], display_labels=lr.classes_)\ndisp.plot(cmap='Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(confusion_matrix=cm['l2'], display_labels=lr.classes_)\ndisp.plot(cmap='Blues')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le.inverse_transform([1,2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can infer from the 3 confusion matrix that every model has a slight problem at predicting classes 1 and 2, these correspond to activities 'SITTING' and 'STANDING' respectively, we suppose the root of this is due to the similarity in measurement of angles or position.","metadata":{}}]}