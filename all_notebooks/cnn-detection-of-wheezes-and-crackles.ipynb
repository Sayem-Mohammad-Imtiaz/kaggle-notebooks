{"cells":[{"metadata":{"_uuid":"cf450d47a8c6c1d2d14775c9a0bcdf846d1ea48e"},"cell_type":"markdown","source":"# Overview\nImplementation of a convolutional neural network used to identify wheezes and crackles in an audio file which is fed Mel-Spectrograms as inputs. During processing, audio clips are copied to 5 second long buffers, and are split into multiple segments if necessary, with zero padding added to fill the rest of the buffer. During training, Mel-Spectrograms are transposed and wrapped around the time-axis to help allow the network to learn to identify features occurring at arbitrary times within the recording. Data augmentation was employed in the form of audio stretching (speeding up and down) as well as Vocal Tract Length Perturbation, especially for the scarcer '*wheeze*' and '*wheeze and crackles*' classes. A one hot data labelling scheme was chosen as earlier attempts at using a multi-label scheme using a Sigmoid output layer resulted in poor training results (which in hindsight may have been caused by an excessively high learning rate). Currently, both the '*wheeze*' and '*wheeze and crackles*' classes pose the greatest challenge in the area of classification, and frequently produce false negatives, as indicated by the poor recall scores.  Overall validation accuracy currently stands at roughly 70%."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import wave\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ba6c451b8e6ba07de187ee7c79f0a6e9319a594e"},"cell_type":"code","source":"os.listdir('../input/respiratory_sound_database')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_no_diagnosis = pd.read_csv('../input/demographic_info.txt', names = \n                 ['Patient number', 'Age', 'Sex' , 'Adult BMI (kg/m2)', 'Child Weight (kg)' , 'Child Height (cm)'],\n                 delimiter = ' ')\n\ndiagnosis = pd.read_csv('../input/respiratory_sound_database/Respiratory_Sound_Database/patient_diagnosis.csv', names = ['Patient number', 'Diagnosis'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"912a707b2fcf4ab2711b19bfa4341c5b58d9dcc2"},"cell_type":"code","source":"df =  df_no_diagnosis.join(diagnosis.set_index('Patient number'), on = 'Patient number', how = 'left')\ndf['Diagnosis'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e1d7ddaac465d3af5b8594b86ea6033e1048b08"},"cell_type":"code","source":"root = '../input/respiratory_sound_database/Respiratory_Sound_Database/audio_and_txt_files/'\nfilenames = [s.split('.')[0] for s in os.listdir(path = root) if '.txt' in s]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06d7a65e5eb860b69d08e3978fc89bd5a97056e7"},"cell_type":"code","source":"def Extract_Annotation_Data(file_name, root):\n    tokens = file_name.split('_')\n    recording_info = pd.DataFrame(data = [tokens], columns = ['Patient number', 'Recording index', 'Chest location','Acquisition mode','Recording equipment'])\n    recording_annotations = pd.read_csv(os.path.join(root, file_name + '.txt'), names = ['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n    return (recording_info, recording_annotations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91a9bc4000bb65013bcc035a0b9d9564651ebd33"},"cell_type":"code","source":"i_list = []\nrec_annotations = []\nrec_annotations_dict = {}\nfor s in filenames:\n    (i,a) = Extract_Annotation_Data(s, root)\n    i_list.append(i)\n    rec_annotations.append(a)\n    rec_annotations_dict[s] = a\nrecording_info = pd.concat(i_list, axis = 0)\nrecording_info.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"101df59261909482911400c0869f90cd300d36f3"},"cell_type":"code","source":"no_label_list = []\ncrack_list = []\nwheeze_list = []\nboth_sym_list = []\nfilename_list = []\nfor f in filenames:\n    d = rec_annotations_dict[f]\n    no_labels = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 0)].index)\n    n_crackles = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 0)].index)\n    n_wheezes = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 1)].index)\n    both_sym = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 1)].index)\n    no_label_list.append(no_labels)\n    crack_list.append(n_crackles)\n    wheeze_list.append(n_wheezes)\n    both_sym_list.append(both_sym)\n    filename_list.append(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92e261fa3987a54f8ce52ff93fbcf2289ac20faf"},"cell_type":"code","source":"file_label_df = pd.DataFrame(data = {'filename':filename_list, 'no label':no_label_list, 'crackles only':crack_list, 'wheezes only':wheeze_list, 'crackles and wheezees':both_sym_list})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c91a8fed39a97efa50dfb4b4e834008efe9e713f"},"cell_type":"markdown","source":"# Distribution of data classes"},{"metadata":{"trusted":true,"_uuid":"26e1727c204af6104d584c3088daa638e8ed6227","scrolled":true},"cell_type":"code","source":"w_labels = file_label_df[(file_label_df['crackles only'] != 0) | (file_label_df['wheezes only'] != 0) | (file_label_df['crackles and wheezees'] != 0)]\nfile_label_df.sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4288dfa4a454a3626499348b48c490ae9523b4ec"},"cell_type":"markdown","source":"# Utility functions for reading .wav files (especially pesky 24bit .wav)"},{"metadata":{"trusted":true,"_uuid":"0595b1640a188ca09640694b48a876ce6df41944"},"cell_type":"code","source":"import wave\nimport math\nimport scipy.io.wavfile as wf\n#wave file reader\n\n#Will resample all files to the target sample rate and produce a 32bit float array\ndef read_wav_file(str_filename, target_rate):\n    wav = wave.open(str_filename, mode = 'r')\n    (sample_rate, data) = extract2FloatArr(wav,str_filename)\n    \n    if (sample_rate != target_rate):\n        ( _ , data) = resample(sample_rate, data, target_rate)\n        \n    wav.close()\n    return (target_rate, data.astype(np.float32))\n\ndef resample(current_rate, data, target_rate):\n    x_original = np.linspace(0,100,len(data))\n    x_resampled = np.linspace(0,100, int(len(data) * (target_rate / current_rate)))\n    resampled = np.interp(x_resampled, x_original, data)\n    return (target_rate, resampled.astype(np.float32))\n\n# -> (sample_rate, data)\ndef extract2FloatArr(lp_wave, str_filename):\n    (bps, channels) = bitrate_channels(lp_wave)\n    \n    if bps in [1,2,4]:\n        (rate, data) = wf.read(str_filename)\n        divisor_dict = {1:255, 2:32768}\n        if bps in [1,2]:\n            divisor = divisor_dict[bps]\n            data = np.divide(data, float(divisor)) #clamp to [0.0,1.0]        \n        return (rate, data)\n    \n    elif bps == 3: \n        #24bpp wave\n        return read24bitwave(lp_wave)\n    \n    else:\n        raise Exception('Unrecognized wave format: {} bytes per sample'.format(bps))\n        \n#Note: This function truncates the 24 bit samples to 16 bits of precision\n#Reads a wave object returned by the wave.read() method\n#Returns the sample rate, as well as the audio in the form of a 32 bit float numpy array\n#(sample_rate:float, audio_data: float[])\ndef read24bitwave(lp_wave):\n    nFrames = lp_wave.getnframes()\n    buf = lp_wave.readframes(nFrames)\n    reshaped = np.frombuffer(buf, np.int8).reshape(nFrames,-1)\n    short_output = np.empty((nFrames, 2), dtype = np.int8)\n    short_output[:,:] = reshaped[:, -2:]\n    short_output = short_output.view(np.int16)\n    return (lp_wave.getframerate(), np.divide(short_output, 32768).reshape(-1))  #return numpy array to save memory via array slicing\n\ndef bitrate_channels(lp_wave):\n    bps = (lp_wave.getsampwidth() / lp_wave.getnchannels()) #bytes per sample\n    return (bps, lp_wave.getnchannels())\n\ndef slice_data(start, end, raw_data,  sample_rate):\n    max_ind = len(raw_data) \n    start_ind = min(int(start * sample_rate), max_ind)\n    end_ind = min(int(end * sample_rate), max_ind)\n    return raw_data[start_ind: end_ind]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d075d96d6f2dd83f26f17568ad355d815f8003ba"},"cell_type":"markdown","source":"# Distribution of respiratory cycle lengths"},{"metadata":{"trusted":true,"_uuid":"bc1f720697a0bff33f021b4ed1d1c9694200caff"},"cell_type":"code","source":"duration_list = []\nfor i in range(len(rec_annotations)):\n    current = rec_annotations[i]\n    duration = current['End'] - current['Start']\n    duration_list.extend(duration)\n\nduration_list = np.array(duration_list)\nplt.hist(duration_list, bins = 50)\nprint('longest cycle:{}'.format(max(duration_list)))\nprint('shortest cycle:{}'.format(min(duration_list)))\nthreshold = 5\nprint('Fraction of samples less than {} seconds:{}'.format(threshold,\n                                                           np.sum(duration_list < threshold)/len(duration_list)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a44dbd5d5906f0e75a3fcb0dc520f5615600ece"},"cell_type":"markdown","source":"# Mel spectrogram implementation (With VTLP)"},{"metadata":{"trusted":true,"_uuid":"39cb6a31ccb2c4d9388dfa1f23a5973c65a9d5b4"},"cell_type":"code","source":"import scipy.signal\n\n#vtlp_params = (alpha, f_high) \ndef sample2MelSpectrum(cycle_info, sample_rate, n_filters, vtlp_params):\n    n_rows = 175 # 7500 cutoff\n    n_window = 512 #~25 ms window\n    (f, t, Sxx) = scipy.signal.spectrogram(cycle_info[0],fs = sample_rate, nfft= n_window, nperseg=n_window)\n    Sxx = Sxx[:n_rows,:].astype(np.float32) #sift out coefficients above 7500hz, Sxx has 196 columns\n    mel_log = FFT2MelSpectrogram(f[:n_rows], Sxx, sample_rate, n_filters, vtlp_params)[1]\n    mel_min = np.min(mel_log)\n    mel_max = np.max(mel_log)\n    diff = mel_max - mel_min\n    norm_mel_log = (mel_log - mel_min) / diff if (diff > 0) else np.zeros(shape = (n_filters,Sxx.shape[1]))\n    if (diff == 0):\n        print('Error: sample data is completely empty')\n    labels = [cycle_info[1], cycle_info[2]] #crackles, wheezes flags\n    return (np.reshape(norm_mel_log, (n_filters,Sxx.shape[1],1)).astype(np.float32), # 196x64x1 matrix\n            label2onehot(labels)) \n        \ndef Freq2Mel(freq):\n    return 1125 * np.log(1 + freq / 700)\n\ndef Mel2Freq(mel):\n    exponents = mel / 1125\n    return 700 * (np.exp(exponents) - 1)\n\n#Tased on Jaitly & Hinton(2013)\n#Takes an array of the original mel spaced frequencies and returns a warped version of them\ndef VTLP_shift(mel_freq, alpha, f_high, sample_rate):\n    nyquist_f = sample_rate / 2\n    warp_factor = min(alpha, 1)\n    threshold_freq = f_high * warp_factor / alpha\n    lower = mel_freq * alpha\n    higher = nyquist_f - (nyquist_f - mel_freq) * ((nyquist_f - f_high * warp_factor) / (nyquist_f - f_high * (warp_factor / alpha)))\n    \n    warped_mel = np.where(mel_freq <= threshold_freq, lower, higher)\n    return warped_mel.astype(np.float32)\n\n#mel_space_freq: the mel frequencies (HZ) of the filter banks, in addition to the two maximum and minimum frequency values\n#fft_bin_frequencies: the bin freqencies of the FFT output\n#Generates a 2d numpy array, with each row containing each filter bank\ndef GenerateMelFilterBanks(mel_space_freq, fft_bin_frequencies):\n    n_filters = len(mel_space_freq) - 2\n    coeff = []\n    #Triangular filter windows\n    #ripped from http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/\n    for mel_index in range(n_filters):\n        m = int(mel_index + 1)\n        filter_bank = []\n        for f in fft_bin_frequencies:\n            if(f < mel_space_freq[m-1]):\n                hm = 0\n            elif(f < mel_space_freq[m]):\n                hm = (f - mel_space_freq[m-1]) / (mel_space_freq[m] - mel_space_freq[m-1])\n            elif(f < mel_space_freq[m + 1]):\n                hm = (mel_space_freq[m+1] - f) / (mel_space_freq[m + 1] - mel_space_freq[m])\n            else:\n                hm = 0\n            filter_bank.append(hm)\n        coeff.append(filter_bank)\n    return np.array(coeff, dtype = np.float32)\n        \n#Transform spectrogram into mel spectrogram -> (frequencies, spectrum)\n#vtlp_params = (alpha, f_high), vtlp will not be applied if set to None\ndef FFT2MelSpectrogram(f, Sxx, sample_rate, n_filterbanks, vtlp_params = None):\n    (max_mel, min_mel)  = (Freq2Mel(max(f)), Freq2Mel(min(f)))\n    mel_bins = np.linspace(min_mel, max_mel, num = (n_filterbanks + 2))\n    #Convert mel_bins to corresponding frequencies in hz\n    mel_freq = Mel2Freq(mel_bins)\n    \n    if(vtlp_params is None):\n        filter_banks = GenerateMelFilterBanks(mel_freq, f)\n    else:\n        #Apply VTLP\n        (alpha, f_high) = vtlp_params\n        warped_mel = VTLP_shift(mel_freq, alpha, f_high, sample_rate)\n        filter_banks = GenerateMelFilterBanks(warped_mel, f)\n        \n    mel_spectrum = np.matmul(filter_banks, Sxx)\n    return (mel_freq[1:-1], np.log10(mel_spectrum  + float(10e-12)))\n    \n#labels proved too difficult to train (model keep convergining to statistical mean)\n#Flattened to onehot labels since the number of combinations is very low\ndef label2onehot(c_w_flags):\n    c = c_w_flags[0]\n    w = c_w_flags[1]\n    if((c == False) & (w == False)):\n        return [1,0,0,0]\n    elif((c == True) & (w == False)):\n        return [0,1,0,0]\n    elif((c == False) & (w == True)):\n        return [0,0,1,0]\n    else:\n        return [0,0,0,1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4a75526cc7d116004ba0e18f1a0be153f5e91e6"},"cell_type":"markdown","source":"# Data preparation utility functions"},{"metadata":{"trusted":true,"_uuid":"0019f4252e39cebcdb53e5a5f3ca98b2d1262e42"},"cell_type":"code","source":"#Used to split each individual sound file into separate sound clips containing one respiratory cycle each\n#output: [filename, (sample_data:np.array, start:float, end:float, crackles:bool(float), wheezes:bool(float)) (...) ]\ndef get_sound_samples(recording_annotations, file_name, root, sample_rate):\n    sample_data = [file_name]\n    (rate, data) = read_wav_file(os.path.join(root, file_name + '.wav'), sample_rate)\n    \n    for i in range(len(recording_annotations.index)):\n        row = recording_annotations.loc[i]\n        start = row['Start']\n        end = row['End']\n        crackles = row['Crackles']\n        wheezes = row['Wheezes']\n        audio_chunk = slice_data(start, end, data, rate)\n        sample_data.append((audio_chunk, start,end,crackles,wheezes))\n    return sample_data\n\n#Fits each respiratory cycle into a fixed length audio clip, splits may be performed and zero padding is added if necessary\n#original:(arr,c,w) -> output:[(arr,c,w),(arr,c,w)]\ndef split_and_pad(original, desiredLength, sampleRate):\n    output_buffer_length = int(desiredLength * sampleRate)\n    soundclip = original[0]\n    n_samples = len(soundclip)\n    total_length = n_samples / sampleRate #length of cycle in seconds\n    n_slices = int(math.ceil(total_length / desiredLength)) #get the minimum number of slices needed\n    samples_per_slice = n_samples // n_slices\n    src_start = 0 #Staring index of the samples to copy from the original buffer\n    output = [] #Holds the resultant slices\n    for i in range(n_slices):\n        src_end = min(src_start + samples_per_slice, n_samples)\n        length = src_end - src_start\n        copy = generate_padded_samples(soundclip[src_start:src_end], output_buffer_length)\n        output.append((copy, original[1], original[2]))\n        src_start += length\n    return output\n\ndef generate_padded_samples(source, output_length):\n    copy = np.zeros(output_length, dtype = np.float32)\n    src_length = len(source)\n    frac = src_length / output_length\n    if(frac < 0.5):\n        #tile forward sounds to fill empty space\n        cursor = 0\n        while(cursor + src_length) < output_length:\n            copy[cursor:(cursor + src_length)] = source[:]\n            cursor += src_length\n    else:\n        copy[:src_length] = source[:]\n    #\n    return copy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24efa87087b3144117d988eb9efa40610fa1e602"},"cell_type":"markdown","source":" # Data augmentation\n Two basic forms employed : audio stretching (speeding up or down)  as well as Vocal Tract Length perturbation"},{"metadata":{"trusted":true,"_uuid":"ddf33192a05fbcff53418e9a1bbf189a0f8931ff"},"cell_type":"code","source":"#Creates a copy of each time slice, but stretches or contracts it by a random amount\ndef gen_time_stretch(original, sample_rate, max_percent_change):\n    stretch_amount = 1 + np.random.uniform(-1,1) * (max_percent_change / 100)\n    (_, stretched) = resample(sample_rate, original, int(sample_rate * stretch_amount)) \n    return stretched\n\n#Same as above, but applies it to a list of samples\ndef augment_list(audio_with_labels, sample_rate, percent_change, n_repeats):\n    augmented_samples = []\n    for i in range(n_repeats):\n        addition = [(gen_time_stretch(t[0], sample_rate, percent_change), t[1], t[2] ) for t in audio_with_labels]\n        augmented_samples.extend(addition)\n    return augmented_samples\n\n#Takes a list of respiratory cycles, and splits and pads each cycle into fixed length buffers (determined by desiredLength(seconds))\n#Then takes the split and padded sample and transforms it into a mel spectrogram\n#VTLP_alpha_range = [Lower, Upper] (Bounds of random selection range), \n#VTLP_high_freq_range = [Lower, Upper] (-)\n#output:[(arr:float[],c:float_bool,w:float_bool),(arr,c,w)]\ndef split_and_pad_and_apply_mel_spect(original, desiredLength, sampleRate, VTLP_alpha_range = None, VTLP_high_freq_range = None, n_repeats = 1):\n    output = []\n    for i in range(n_repeats):\n        for d in original:\n            lst_result = split_and_pad(d, desiredLength, sampleRate) #Time domain\n            if( (VTLP_alpha_range is None) | (VTLP_high_freq_range is None) ):\n                #Do not apply VTLP\n                VTLP_params = None\n            else:\n                #Randomly generate VLTP parameters\n                alpha = np.random.uniform(VTLP_alpha_range[0], VTLP_alpha_range[1])\n                high_freq = np.random.uniform(VTLP_high_freq_range[0], VTLP_high_freq_range[1])\n                VTLP_params = (alpha, high_freq)\n            freq_result = [sample2MelSpectrum(d, sampleRate, 50, VTLP_params) for d in lst_result] #Freq domain\n            output.extend(freq_result)\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0c71f1dfc7b6a4cd1df05c116b47295687e4b33"},"cell_type":"code","source":"str_file = filenames[11]\nlp_test = get_sound_samples(rec_annotations_dict[str_file], str_file, root, 22000)\nlp_cycles = [(d[0], d[3], d[4]) for d in lp_test[1:]]\nsoundclip = lp_cycles[1][0]\n\nn_window = 512\nsample_rate = 22000\n(f, t, Sxx) = scipy.signal.spectrogram(soundclip, fs = 22000, nfft= n_window, nperseg=n_window)\nprint(sum(f < 7000))\n\nplt.figure(figsize = (20,10))\nplt.subplot(1,2,1)\nmel_banks = FFT2MelSpectrogram(f[:175], Sxx[:175,:], sample_rate, 50)[1]\nplt.imshow(mel_banks, aspect = 1)\nplt.title('No VTLP')\n\nplt.subplot(1,2,2)\nmel_banks = FFT2MelSpectrogram(f[:175], Sxx[:175,:], sample_rate, 50, vtlp_params = (0.9,3500))[1]\nplt.imshow(mel_banks, aspect = 1)\nplt.title('With VTLP')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eee12319864b3551bfa459cf4e3ac034ffbd9463"},"cell_type":"markdown","source":"# Utility used to import all training samples"},{"metadata":{"trusted":true,"_uuid":"b373daadbf268f81e5d0bed77090166f7033f0b5"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef extract_all_training_samples(filenames, annotation_dict, root, target_rate, desired_length, train_test_ratio = 0.2):\n    cycle_list = []\n    for file in filenames:\n        data = get_sound_samples(annotation_dict[file], file, root, target_rate)\n        cycles_with_labels = [(d[0], d[3], d[4]) for d in data[1:]]\n        cycle_list.extend(cycles_with_labels)\n    \n    #Sort into respective classes\n    no_labels = [c for c in cycle_list if ((c[1] == 0) & (c[2] == 0))]\n    c_only = [c for c in cycle_list if ((c[1] == 1) & (c[2] == 0))] \n    w_only = [c for c in cycle_list if ((c[1] == 0) & (c[2] == 1))]\n    c_w = [c for c in cycle_list if ((c[1] == 1) & (c[2] == 1))]\n    \n    #Count of labels across all cycles, actual recording time also follows similar ratios\n    #none:3642\n    #crackles:1864 \n    #wheezes:886\n    #both:506\n    none_train, none_test = train_test_split(no_labels, test_size = train_test_ratio)\n    c_train, c_test  = train_test_split(c_only, test_size = train_test_ratio)\n    w_train, w_test  = train_test_split(w_only, test_size = train_test_ratio)\n    c_w_train, c_w_test  = train_test_split(c_w, test_size = train_test_ratio)\n    \n    #Training section (Data augmentation procedures)\n    #Augment w_only and c_w groups to match the size of c_only\n    #no_labels will be artifically reduced in the pipeline  later\n    w_stretch = w_train + augment_list(w_train, target_rate, 10 , 1) #\n    c_w_stretch = c_w_train + augment_list(c_w_train , target_rate, 10 , 1) \n    \n    #Split up cycles into sound clips with fixed lengths so they can be fed into a CNN\n    vtlp_alpha = [0.9,1.1]\n    vtlp_upper_freq = [3200,3800]\n    \n    train_none  = (split_and_pad_and_apply_mel_spect(none_train, desired_length, target_rate) +\n                   split_and_pad_and_apply_mel_spect(none_train, desired_length, target_rate, vtlp_alpha))\n    \n    train_c = (split_and_pad_and_apply_mel_spect(c_train, desired_length, target_rate) + \n               split_and_pad_and_apply_mel_spect(c_train, desired_length, target_rate, vtlp_alpha, vtlp_upper_freq, n_repeats = 3) ) #original samples + VTLP\n    \n    train_w = (split_and_pad_and_apply_mel_spect(w_stretch, desired_length, target_rate) + \n               split_and_pad_and_apply_mel_spect(w_stretch , desired_length, target_rate, vtlp_alpha , vtlp_upper_freq, n_repeats = 4)) #(original samples + time stretch) + VTLP\n    \n    train_c_w = (split_and_pad_and_apply_mel_spect(c_w_stretch, desired_length, target_rate) + \n                 split_and_pad_and_apply_mel_spect(c_w_stretch, desired_length, target_rate, vtlp_alpha , vtlp_upper_freq, n_repeats = 7)) #(original samples + time stretch * 2) + VTLP\n    \n    train_dict = {'none':train_none,'crackles':train_c,'wheezes':train_w, 'both':train_c_w}\n    \n    #test section \n    test_none  = split_and_pad_and_apply_mel_spect(none_test, desired_length, target_rate)\n    test_c = split_and_pad_and_apply_mel_spect(c_test, desired_length, target_rate)\n    test_w = split_and_pad_and_apply_mel_spect(w_test, desired_length, target_rate)\n    test_c_w = split_and_pad_and_apply_mel_spect(c_w_test, desired_length, target_rate)\n    \n    test_dict = {'none':test_none,'crackles':test_c,'wheezes':test_w, 'both':test_c_w}\n    \n    return [train_dict, test_dict]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c49ed33718a224a03c414841706efd662cbe1985"},"cell_type":"code","source":"target_sample_rate = 22000 \nsample_length_seconds = 5\nsample_dict = extract_all_training_samples(filenames, rec_annotations_dict, root, target_sample_rate, sample_length_seconds) #sample rate lowered to meet memory constraints\ntraining_clips = sample_dict[0]\ntest_clips = sample_dict[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60cd8372f5e529d04abba33fed4f8559db2b8a35"},"cell_type":"code","source":"def print_sample_count(src_dict):\n    print('none:{}\\ncrackles:{}\\nwheezes:{}\\nboth:{}'.format(len(src_dict['none']),\n                                                        len(src_dict['crackles']),\n                                                        len(src_dict['wheezes']),\n                                                        len(src_dict['both'])))\n\nprint('Samples Available')\nprint('[Training set]')\nprint_sample_count(training_clips)\nprint('')\nprint('[Test set]')\nprint_sample_count(test_clips)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"547638c6b0e43da791fa0c73cea6bea1629c51b5"},"cell_type":"code","source":"#Example of tiled sound samples\nsample_height = training_clips['none'][0][0].shape[0]\nsample_width = training_clips['none'][0][0].shape[1]\nind = 1\nplt.figure(figsize = (10,10))\nplt.subplot(4,1,1)\nplt.imshow(training_clips['none'][ind][0].reshape(sample_height, sample_width))\nplt.title('None')\nplt.subplot(4,1,2)\nplt.imshow(training_clips['crackles'][ind][0].reshape(sample_height, sample_width))\nplt.title('Crackles')\nplt.subplot(4,1,3)\nplt.imshow(training_clips['wheezes'][ind][0].reshape(sample_height, sample_width))\nplt.title('Wheezes')\nplt.subplot(4,1,4)\nplt.imshow(training_clips['both'][ind][0].reshape(sample_height, sample_width))\nplt.title('Both')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ada1ead97b26f8eb051594f67d7dbe04096f4a7c"},"cell_type":"markdown","source":"# Data Pipeline"},{"metadata":{"trusted":true,"_uuid":"79d3651b4b6c73d3cc7a33e6a434227529b1d171"},"cell_type":"code","source":"import scipy.signal\n\n#Interleaved sampling between classes\n#Used to ensure a balance of classes for the training set\nclass data_generator():\n    #sound_clips = [[none],[crackles],[wheezes],[both]]\n    #strides: How far the sampling index for each category is advanced for each step\n    def __init__(self, sound_clips, strides):\n        self.clips = sound_clips\n        self.strides = strides\n        self.lengths = [len(arr) for arr in sound_clips]\n    \n    def n_available_samples(self):\n        return int(min(np.divide(self.lengths, self.strides))) * 4\n    \n    def generate_keras(self, batch_size):\n        cursor = [0,0,0,0]\n        while True:\n            i = 0\n            X,y = [],[]\n            for c in range(batch_size):\n                cat_length = self.lengths[i]\n                cat_clips = self.clips[i]\n                cat_stride = self.strides[i]\n                cat_advance = np.random.randint(low= 1,high = cat_stride + 1)\n                clip = cat_clips[(cursor[i] + cat_advance) % cat_length]\n                cursor[i] = (cursor[i] + self.strides[i]) % cat_length #advance cursor\n                s = (self.rollFFT(clip))\n                X.append(s[0])\n                y.append(s[1])\n                i = (i + 1) % 4 # go to next class\n            yield (np.reshape(X, (batch_size, sample_height, sample_width, 1)),\n                   np.reshape(y,(batch_size,4)))\n\n    #Transpose and wrap each array along the time axis\n    def rollFFT(self, fft_info):\n        fft = fft_info[0]\n        n_col = fft.shape[1]\n        pivot = np.random.randint(n_col)\n        return ((np.roll(fft, pivot, axis = 1)), fft_info[1])\n\n#Used for validation set\nclass feed_all():\n    #sound_clips = [[none],[crackles],[wheezes],[both]]\n    #strides: How far the sampling index for each category is advanced for each step\n    def __init__(self, sound_clips, roll = True):\n        merged = []\n        for arr in sound_clips:\n            merged.extend(arr)\n        np.random.shuffle(merged)\n        self.clips = merged\n        self.nclips = len(merged)\n        self.roll = roll\n    \n    def n_available_samples(self):\n        return len(self.clips)\n    \n    def generate_keras(self, batch_size):\n        i = 0\n        while True:\n            X,y = [],[]\n            for b in range(batch_size):\n                clip = self.clips[i]\n                i = (i + 1) % self.nclips\n                if(self.roll):\n                    s = (self.rollFFT(clip))\n                    X.append(s[0])\n                    y.append(s[1])\n                else:\n                    X.append(clip[0])\n                    y.append(clip[1])\n                    \n            yield (np.reshape(X, (batch_size,sample_height, sample_width,1)),\n                   np.reshape(y,(batch_size, 4)))\n\n    #Transpose and wrap each array along the time axis\n    def rollFFT(self, fft_info):\n        fft = fft_info[0]\n        n_col = fft.shape[1]\n        pivot = np.random.randint(n_col)\n        return ((np.roll(fft, pivot, axis = 1)), fft_info[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e09789c053f36f4b3863f2471b308b33608ea8eb"},"cell_type":"code","source":"[none_train, c_train, w_train, c_w_train] = [training_clips['none'], training_clips['crackles'], training_clips['wheezes'], training_clips['both']]\n[none_test, c_test, w_test,c_w_test] =  [test_clips['none'], test_clips['crackles'], test_clips['wheezes'], test_clips['both']]\n\nnp.random.shuffle(none_train)\nnp.random.shuffle(c_train)\nnp.random.shuffle(w_train)\nnp.random.shuffle(c_w_train)\n\n#Data pipeline objects\ntrain_gen = data_generator([none_train, c_train, w_train, c_w_train], [1,1,1,1])\ntest_gen = feed_all([none_test, c_test, w_test,c_w_test])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de804e4de514356b3428d5bd817d9e44478daa57"},"cell_type":"markdown","source":"# CNN implementation"},{"metadata":{"trusted":true,"_uuid":"405dedb74c832336bc72c0c17682f575e99ec904"},"cell_type":"code","source":"batch_size = 128\nn_epochs = 15\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21a2aa15b0ea5798ae2b796786ee583b8d60548e"},"cell_type":"code","source":"#Keras implementation\nfrom keras import Sequential\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.layers import Conv2D, Dense, Activation, Dropout, MaxPool2D, Flatten, LeakyReLU\nimport tensorflow as tf\nK.clear_session()\n\nmodel = Sequential()\nmodel.add(Conv2D(128, [7,11], strides = [2,2], padding = 'SAME', input_shape = (sample_height, sample_width, 1)))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(MaxPool2D(padding = 'SAME'))\n\nmodel.add(Conv2D(256, [5,5], padding = 'SAME'))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(MaxPool2D(padding = 'SAME'))\n\nmodel.add(Conv2D(256, [1,1], padding = 'SAME'))\nmodel.add(Conv2D(256, [3,3], padding = 'SAME'))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(MaxPool2D(padding = 'SAME'))\n\nmodel.add(Conv2D(512, [1,1], padding = 'SAME'))\nmodel.add(Conv2D(512, [3,3], padding = 'SAME',activation = 'relu'))\nmodel.add(Conv2D(512, [1,1], padding = 'SAME'))\nmodel.add(Conv2D(512, [3,3], padding = 'SAME', activation = 'relu'))\nmodel.add(MaxPool2D(padding = 'SAME'))\nmodel.add(Flatten())\n\nmodel.add(Dense(4096, activation = 'relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(512, activation = 'relu'))\nmodel.add(Dense(4, activation = 'softmax'))\n\nopt = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.00, amsgrad=False)\n\nmodel.compile(optimizer =  opt , loss = 'categorical_crossentropy', metrics = ['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4856cd835f68b42499eea9c368e490553393328"},"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\n\nplot_model(model, show_shapes=True, show_layer_names = True)\nfrom IPython.display import Image\nImage(filename='model.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd95c81edb02b9e90c44a628a8b0c2dcd4bf7558"},"cell_type":"code","source":"stats = model.fit_generator(generator = train_gen.generate_keras(batch_size), \n                            steps_per_epoch = train_gen.n_available_samples() // batch_size,\n                            validation_data = test_gen.generate_keras(batch_size),\n                            validation_steps = test_gen.n_available_samples() // batch_size, \n                            epochs = n_epochs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db9acc2ae412bafe344d83c094ffcb0e2ca42c61"},"cell_type":"code","source":"plt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.title('Accuracy')\nplt.plot(stats.history['acc'], label = 'training acc')\nplt.plot(stats.history['val_acc'], label = 'validation acc')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(stats.history['loss'], label = 'training loss')\nplt.plot(stats.history['val_loss'], label = 'validation loss')\nplt.legend()\nplt.title('Loss')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39bd3325b3569af3f3d6a89d19750c8c573af0fb"},"cell_type":"code","source":"test_set = test_gen.generate_keras(test_gen.n_available_samples()).__next__()\npredictions = model.predict(test_set[0])\npredictions = np.argmax(predictions, axis = 1)\nlabels = np.argmax(test_set[1], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3502b18c0ff81329872907c4277764b26689d0c3"},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(labels, predictions, target_names = ['none','crackles','wheezes','both']))\nprint(confusion_matrix(labels, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63b8d408b222e3599d6b74f7c26f953c1dbcc1d8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}