{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reading Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding the data","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrmat = df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True, annot=True);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict = {}\nfor i in list(df.columns):\n    dict[i] = df[i].value_counts().shape[0]\n\npd.DataFrame(dict,index=[\"unique count\"]).transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Separating into features and targets","metadata":{}},{"cell_type":"code","source":"#features = columns for column in df.columns if column not in \"Outcome\"\ncon_cols = list(df.drop('Outcome',axis=1).columns)\ntarget = ['Outcome']\nprint(\"Feature columns: \",con_cols)\nprint(\"Target columns: \",target)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking for Missing Values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Box Plot","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(9)\nfig.set_figwidth(8)\nfig.set_figheight(45)\ni=0\nfor col in df.columns:\n    sns.boxplot(y=df[col], ax=axs[i])\n    i=i+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Histogram","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(9)\nfig.set_figwidth(8)\nfig.set_figheight(45)\ni=0\nfor col in df.columns:\n    sns.histplot(x=df[col], ax=axs[i],kde=True)\n    i=i+1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Skewness along the index axis","metadata":{}},{"cell_type":"code","source":"df.skew(axis = 0, skipna = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Removing the outliers","metadata":{}},{"cell_type":"code","source":"df.drop(df[df[\"Pregnancies\"] > 14].index,inplace=True)\ndf.drop(df[df[\"Glucose\"] < 50].index,inplace=True)\ndf.drop(df[df[\"BloodPressure\"] > 120].index,inplace=True)\ndf.drop(df[df[\"SkinThickness\"] > 80].index,inplace=True)\ndf.drop(df[df[\"Insulin\"] > 600].index,inplace=True)\ndf.drop(df[df[\"BMI\"] > 55].index,inplace=True)\ndf.drop(df[df[\"DiabetesPedigreeFunction\"] > 2].index,inplace=True)\ndf.drop(df[df[\"Age\"] > 70].index,inplace=True)\n\nprint(\"Shape of dataset: \", df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing the skewness","metadata":{}},{"cell_type":"code","source":"# for Insulin\nfig, axs = plt.subplots(2)\nsns.kdeplot(df['Insulin'],color='Purple',fill=True, ax=axs[0])\n# Removing the skewness using a log function and checking the distribution again\ndf['Insulin'] = df['Insulin'].map(lambda i : np.log(i) if i > 0 else 0)\nsns.kdeplot(df['Insulin'],color='Orange',fill=True, ax=axs[1])\ndf['Insulin'].skew(axis = 0, skipna = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for DiabetesPedigreeFunction\nfig, axs = plt.subplots(2)\nsns.kdeplot(df['DiabetesPedigreeFunction'],color='Purple',fill=True, ax=axs[0])\n# Removing the skewness using a log function and checking the distribution again\ndf['DiabetesPedigreeFunction'] = df['DiabetesPedigreeFunction'].map(lambda i : np.log(i) if i > 0 else 0)\nsns.kdeplot(df['DiabetesPedigreeFunction'],color='Orange',fill=True, ax=axs[1])\ndf['DiabetesPedigreeFunction'].skew(axis = 0, skipna = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for Age\nfig, axs = plt.subplots(2)\nsns.kdeplot(df['Age'],color='Purple',fill=True, ax=axs[0])\n# Removing the skewness using a log function and checking the distribution again\ndf['Age'] = df['Age'].map(lambda i : np.log(i) if i > 0 else 0)\nsns.kdeplot(df['Age'],color='Orange',fill=True, ax=axs[1])\ndf['Age'].skew(axis = 0, skipna = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Making features model ready","metadata":{}},{"cell_type":"code","source":"# creating a copy of dataframe\ndf1 = df\n\n# separating the features and target \nX = df1.drop(['Outcome'],axis=1)\ny = df1[['Outcome']]\nfeature_cols = list(X.columns)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting the dataset into the Training set and Test set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train[feature_cols] = sc.fit_transform(X_train[feature_cols])\nX_test[feature_cols] = sc.transform(X_test[feature_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"Packages","metadata":{}},{"cell_type":"code","source":"# Base Models\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Ensembling and Boosting\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n\n# Metrics\nfrom sklearn.metrics import accuracy_score,classification_report\n\n# Hyper-parameter tuning\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Base Modeling","metadata":{}},{"cell_type":"code","source":"models = [\n    ('SVC', SVC()),\n    ('DecisionTreeClassifier',DecisionTreeClassifier()),\n    ('KNeighborsClassifier',KNeighborsClassifier()),\n    ('LogisticRegression',LogisticRegression()),\n    ('RandomForestClassifier',RandomForestClassifier()),\n    ('GradientBoostingClassifier',GradientBoostingClassifier())\n]\n\n\nprint(\"The accuracy scores of the models are :\")\nfor model_name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(model_name, \": \", accuracy_score(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter tuning using GridSearchCV","metadata":{}},{"cell_type":"markdown","source":"Defining Function for Hyperparameter tuning using Grid Search CV","metadata":{}},{"cell_type":"code","source":"def AccuracyHyperParameterTune(classifier,param_grid,X_train, y_train, X_test, y_test):\n\n    # initialize grid search\n    grid = GridSearchCV(\n    estimator=classifier, param_grid=param_grid, scoring=\"accuracy\", verbose=1,\n    n_jobs=1,cv=5 )\n\n    # fit the model and extract best score\n    grid.fit(X_train, y_train)\n\n    print(\"Support Vector Classifier: \", grid.best_score_)\n    print(\"Best parameters set:\")\n    print(grid.best_params_)\n\n    # Getting Accuracy\n    y_pred = grid.best_estimator_.predict(X_test)\n    print(\"Classification Report\")\n    print(classification_report(y_test, y_pred))\n    return grid.best_estimator_;","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decision Tree Classifier tuning","metadata":{}},{"cell_type":"code","source":"# define the model\nclassifier = DecisionTreeClassifier()\n\n# define a grid of parameters\nparam_grid = {'criterion':['gini','entropy'],\n              'splitter':['best','random'],\n              'max_depth':[2,3,4,5,6,7,8],\n              'max_features':['auto','sqrt','log2'],\n             }\n\ngridBestEstimator = AccuracyHyperParameterTune(classifier,param_grid,X_train, y_train, X_test, y_test)\nprint(gridBestEstimator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"K Neighbors Classifier tuning","metadata":{}},{"cell_type":"code","source":"# define the model\nclassifier = KNeighborsClassifier()\n\n# define a grid of parameters\nparam_grid = {'n_neighbors':[2,3,4,5,6,7,8],\n              'weights':['uniform','distance'],\n              'algorithm':['auto','ball_tree','kd_tree','brute'],\n              'leaf_size':[26,27,28,29,30,31]\n             }\n\ngridBestEstimator = AccuracyHyperParameterTune(classifier,param_grid,X_train, y_train, X_test, y_test)\nprint(gridBestEstimator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SV Classifier tuning","metadata":{}},{"cell_type":"code","source":"# define the model\nclassifier = SVC()\n\n# define a grid of parameters\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['linear','rbf']\n             }\n\ngridBestEstimator = AccuracyHyperParameterTune(classifier,param_grid,X_train, y_train, X_test, y_test)\nprint(gridBestEstimator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LogisticRegression Classifier tuning","metadata":{}},{"cell_type":"code","source":"# define the model\nclassifier = LogisticRegression()\n\n# define a grid of parameters\nparam_grid = {'C': np.logspace(-4, 4, 50),\n              'penalty': ['l1', 'l2']\n             }\n\ngridBestEstimator = AccuracyHyperParameterTune(classifier,param_grid,X_train, y_train, X_test, y_test)\nprint(gridBestEstimator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RandomForestClassifier tuning","metadata":{}},{"cell_type":"code","source":"# define the model\nclassifier = RandomForestClassifier()\n\n# define a grid of parameters\nparam_grid = {'bootstrap': [True, False],\n              'max_depth': [10, 40, None],\n              'min_samples_leaf': [1, 2, 4],\n              'min_samples_split': [2, 5, 10],\n              'n_estimators': [200, 600, 1400]\n             }\n\ngridBestEstimator = AccuracyHyperParameterTune(classifier,param_grid,X_train, y_train, X_test, y_test)\nprint(gridBestEstimator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}