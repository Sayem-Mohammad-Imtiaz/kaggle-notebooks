{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Importing the libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualisation (2-D)\n%matplotlib inline\nimport seaborn as sns # data visualisation (3-D)\nplt.style.use('seaborn') # set style for graph\nimport warnings \nwarnings.filterwarnings('ignore') # ignore all types of warnings\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading the red wine dataset\nwine = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(wine.head(), wine.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# data doesn't contain any null value ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# its look like our data contains some outliers in most of the features, let's take a look at them","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets check the correlation of the features\nwine.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.heatmap(wine.corr(), square = True, cmap = 'Blues')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# its looks like features are not strongly correlated to each other\n# lets do some visualisation to detect the outliers in the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 6))\nsns.set_context('talk')\nsns.boxplot(wine['quality'], wine['free sulfur dioxide'], data = wine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# free sulphur dioxide contains some extreme values in the dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.boxplot(wine['quality'], wine['citric acid'], data = wine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# citric acid only contains 2 extremes in it which is avoidable for now","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.boxplot(wine['quality'], wine['total sulfur dioxide'], data = wine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# total sulphur dioxide containes some outliers and we have to remove them","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.boxplot(wine['quality'], wine['sulphates'], data = wine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sulphates also contains so many outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (10, 8))\nsns.boxplot(wine['quality'], wine['residual sugar'], data = wine)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# residual sugar contains outliers too","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Detection and Removal of Outliers in dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers = []  # list to store outliers value\n\n# method for detecting the outliers using interquantilerange technique \ndef detect_outliers(data): \n    quantile1, quantile3 = np.percentile(data, [25, 75])  # create two quantiles for 25% and 75%\n    iqr_val = quantile3 - quantile1                       # interquantilerange value\n    lower_bound_value = quantile1 - (1.5 * iqr_val)       # lower limit of the data, anything greater are not outliers\n    upper_bound_value = quantile3 + (1.5 * iqr_val)       # upper limit of the data, anything less are not outliers\n    \n    for i in data:\n        if lower_bound_value < i < upper_bound_value:     # if data[value] is greater than lbv and less than ubv than it is not considered as an outlier\n            pass\n        else:\n            outliers.append(i)\n            \n    return lower_bound_value, upper_bound_value           # return lower bound and upper bound value for the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fwith the help of boxplot visualisation we can notice that the outliers are only present through the upper bound value so we only check ubv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detection of outliers from residual sugar\ndetect_outliers(wine['residual sugar'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine = wine.drop(wine[wine['residual sugar'] > 3.65].index) # drop values which contains outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detection of outliers from sulphates\ndetect_outliers(wine['sulphates'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine = wine.drop(wine[wine['sulphates'] > 0.99].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detection of outliers from free sulphur dioxide\ndetect_outliers(wine['free sulfur dioxide'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine = wine.drop(wine[wine['free sulfur dioxide'] > 40.5].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detect outliers from total sulphur dioxide\ndetect_outliers(wine['total sulfur dioxide'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine = wine.drop(wine[wine['total sulfur dioxide'] > 112].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# outliers are removed from the dataset. now do some preprocessing on data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets change the wine quality in only two categories 'bad' and 'good' it makes it easier for classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine['quality'] = pd.cut(wine['quality'], bins = [0, 6, 8], labels = ['bad', 'good'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine['quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets do labelencoding on wine quality to make it '0' and '1'\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nwine['quality'] = le.fit_transform(wine['quality'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the dataset into featurees and label\nX = wine.drop(['quality'], axis = 1)\ny = wine['quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the dataset into training and testing set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets take a look at the labels\nsns.countplot(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  it is clearly seen here that their is an imbalance in the dataset, 1's are very few as compared to 0's","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets make the dataset balance by using over sampling \nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 42, ratio = 0.9)\nX_train, y_train = sm.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets normalize data \nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building classification model using randomForestClassiifier\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = clf.predict(X_test)  # predictions generated by our model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nprint(classification_report(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have get an accuracy of 92 % \n# lets check cross validation score for our predictions\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf, X_train, y_train, cv = 10)\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cross validation mean is 91","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators = 90, criterion = 'gini', max_features = 'auto')\nclf.fit(X_train, y_train)# lets use gridsearchcv to parameter tuning \nfrom sklearn.model_selection import GridSearchCV\nparam = {\n    'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200],\n    'criterion' : ['gini', 'entropy'],\n    'max_features' : ['auto', 'sqrt', 'log2']\n} \n\ngrid_search = GridSearchCV(estimator = clf,\n                          param_grid = param,\n                          scoring = 'f1',\n                          cv = 'warn',\n                          n_jobs = -1)\n\ngrid_search = grid_search.fit(X_train, y_train)\n\nprint(f'best score : {grid_search.best_score_}')\nprint(f'best parameters : {grid_search.best_params_}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets make changes in parameters in RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators = 60, criterion = 'gini', max_features = 'log2')\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_update = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions_update))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Great ! we got an accuracy of 94","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf, X_train, y_train, cv = 10)\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Great! we got a score of 93","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hope you like ths notebook\n# thank you","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}