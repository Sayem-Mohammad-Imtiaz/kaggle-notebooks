{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.1","nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3}}},"cells":[{"execution_count":null,"metadata":{"collapsed":true},"source":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt \n%matplotlib inline","cell_type":"code","outputs":[]},{"metadata":{},"source":"Lets import the data into a pandas dataframe for analysis.  After importing let's review the features of the data to see what features and labels to use in our model.   ","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"df = pd.read_csv('../input/mushrooms.csv')","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"df.head()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"df['class'].unique()","cell_type":"code","outputs":[]},{"metadata":{},"source":"For the mushrooms class it appears we only have two classes P and E.  Lets take a look and see if there are any NAs in the data.  From the below results set it appears that the data does not have any missing information therefore we can use the data as is and prepare our data for our model.  ","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"df.isnull().sum()","cell_type":"code","outputs":[]},{"metadata":{},"source":"Lets review the total observations and features of the data.  From the below we can see there are a total of 8124 observations and 23 variables we can see the data types for this dataset is object.  For classification we are going to convert the features to integer values to run it through the various models.    ","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"df.shape","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"df.info()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"plt.figure(figsize=(7,7))\nsns.countplot('class', data=df)\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{},"source":"As we saw from above all the obeservations contains string values therefore we cannot use the string values to be fitted into an algorithm therefore using the sklearn.preprocessing we will be using the LabelEncoder to endcode the labels to numeric values.   ","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder() \nfor col in df.columns: \n    df[col]=labelencoder.fit_transform(df[col])\n    \ndf.head()","cell_type":"code","outputs":[]},{"metadata":{},"source":"Now that we have encoded the the string values into interger we can begin the preparation of the data for algorithm to see which algorithm will fit this data. Lets do some data visualization to see how the data is distributed.  ","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"plt.figure(figsize=(15,7))\nsns.boxplot(df)\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{},"source":"We can now start separating the data for the model we know the entire data has 23 features. We will use the class feature as a label therefore we will have one label and 22 features.   ","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true},"source":"X = df.iloc[:, 1:]\ny = df.iloc[:, 0]","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"X.head()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"y.head()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"X.describe()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(), annot=True, cmap='seismic_r', linewidths=.5)\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{},"source":"Lets standardize the data by scaling the data to be -1 and 1 ","cell_type":"markdown"},{"execution_count":null,"metadata":{},"source":"from sklearn.preprocessing import StandardScaler \nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"from sklearn.decomposition import PCA\nN = df.values\npca = PCA(n_components=2)\nx = pca.fit_transform(N)\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4, random_state=5)\nX_clustered = kmeans.fit_predict(N)\n\nLABEL_COLOR_MAP = {0 : 'g',\n                   1 : 'y',\n                   2 : 'r',\n                   3 : 'b'\n                  }\n\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\nplt.figure(figsize = (15,7))\nplt.scatter(x[:,0],x[:,1], c= label_color)\nplt.show()","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"X_clustered","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{"collapsed":true},"source":"pca=PCA(n_components=20)\n\nX = pca.fit_transform(X)\n\n","cell_type":"code","outputs":[]},{"metadata":{},"source":"Lets split the data into train and test to fit it in the model we alrady have our Features in variables X and our label in variable y.","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true},"source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=5)","cell_type":"code","outputs":[]},{"metadata":{},"source":"We are now ready to perform the Machine learning on the data. Using this data we will know which algorithm perform well with this data by using scoring system within the algorithm.  It will be compile at the end to see which model performed well.  Note any accuracy scores close or at 100% will be consider as accurate.    ","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true},"source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"logreg = LogisticRegression() \nlogreg.fit(X_train, y_train)\nlog_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nacc_log","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"svc = SVC()\nsvc.fit(X_train, y_train)\nsvc_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, y_train) * 100, 2)\nacc_svc","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nknn_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"gaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ngnb_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\nacc_gaussian","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"perceptron = Perceptron()\nperceptron.fit(X_train, y_train)\nper_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, y_train) * 100, 2)\nacc_perceptron","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\nlin_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\nacc_linear_svc","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"sgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nsgd_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\nacc_sgd","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\ndec_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nran_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest","cell_type":"code","outputs":[]},{"execution_count":null,"metadata":{},"source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","cell_type":"code","outputs":[]},{"metadata":{},"source":"The following models performed well with this dataset, Support Vector Machines, KNN, Random Forrest, Decision Trees all have a accrucy score of 100%.   This show that these models have a high prediction rate.","cell_type":"markdown"},{"execution_count":null,"metadata":{"collapsed":true},"source":"","cell_type":"code","outputs":[]}],"nbformat_minor":1,"nbformat":4}