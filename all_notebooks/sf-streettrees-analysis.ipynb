{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"#Import the encessary libraries\nimport numpy as np \nimport pandas as pd \nimport os\nimport re\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#To get the dataset file name to work with\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"403ada89a0a2800beae3be917c86d26a55fadea5"},"cell_type":"code","source":"#Read the CSV file into a DataFrame\ndf_SFTrees = pd.read_csv(\"../input/san_francisco_street_trees.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a9a6d8eaddb7406dad98a733f20813830991498"},"cell_type":"code","source":"#Look at few observations\ndf_SFTrees.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c735d24eb8290fd3dc26773d221c8af12c81c9d"},"cell_type":"code","source":"#Understand the total rows, the columns, the non-null values, count of each data type and total memory\ndf_SFTrees.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66ee84c0481cfb5242d904f58260b25f725f974c"},"cell_type":"code","source":"#Preview Object data type columns. 'object' includes String, Unicode.. \ndf_SFTrees.describe(include=\"object\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"243e0bf7ffc6dbda18bff541f53a672444b655f7"},"cell_type":"code","source":"#Preview Integer data type columns. 'integer' includes int8, int16, int32, int64\ndf_SFTrees.describe(include=\"integer\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0a1eda5dcaf34597cbf8ee73c9b2f6703e8f16a"},"cell_type":"code","source":"#Preview Floating data type columns. 'floating' includes float16, float32, float64, float128\ndf_SFTrees.describe(include=\"floating\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e670ba3ee1f121c25efc88d768e6a3b1c527920"},"cell_type":"markdown","source":"* Let us consider the \"**Permit_Notes**\" column\n\n    As per Pandas' documentation:\n\n    **isna() function**:\n        #Return a boolean same-sized object indicating if the values are NA. \n        #NA values, such as None or numpy.NaN, get mapped to True values. Everything else gets mapped to False values. \n        #Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).  \n      \n    **notna() function**:\n        #Return a boolean same-sized object indicating if the values are not NA. \n        #NA values, such as None or numpy.NaN, get mapped to False values.Non-missing values get mapped to True. \n        #Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).     "},{"metadata":{"trusted":true,"_uuid":"335a3a3e43ccb3fab7d542bbda8f64b1b1942a02"},"cell_type":"code","source":"print(\"Total Rows: {0}\".format (df_SFTrees.shape[0]))\nprint(\"Count of valid Permit Notes: {0:d}, {1:.0%} of the total rows in the dataset\".format (df_SFTrees[\"permit_notes\"].notna().sum(),df_SFTrees[\"permit_notes\"].notna().sum()/df_SFTrees.shape[0]))\nprint(\"Count of missing Permit Notes: {:d}, {:.0%} of the total rows in the dataset\".format (df_SFTrees[\"permit_notes\"].isna().sum(),df_SFTrees[\"permit_notes\"].isna().sum()/df_SFTrees.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9ea7fd1b930a918e7bad7e564d2ada260ea17745"},"cell_type":"markdown","source":"**Outcome**:\n\nOnly 27% of the records have permit data. I am guessing for others, these were not captured as permits are usually obtained..."},{"metadata":{"_uuid":"0d53c0eb5a178069517399a8593d118ed33fcc6b"},"cell_type":"markdown","source":"Let us consider the \"**Plant_Date**\" column next\n  \n1.   Let us look at missing info, like we did for \"Permit Notes\". This time, we are using a different way to calculate the percentage.\n2.  Let us group and chart this data, for valid dates"},{"metadata":{"trusted":true,"_uuid":"e3ee063a623673c0d49baed670a325b106c25224"},"cell_type":"code","source":"#1. Let us look at missing info, like we did for \"Permit Notes\"\nprint(\"Total rows: {0}\".format (df_SFTrees.shape[0]))\nprint(\"Count of valid \\'Plant Date\\' rows: {0:d}, {1:.0%} of the total rows in the dataset\".format (df_SFTrees[\"plant_date\"].notna().sum(),df_SFTrees[\"plant_date\"].notna().mean()))\nprint(\"Count of missing \\'Plant Date\\' rows: {:d}, {:.0%} of the total rows in the dataset\".format (df_SFTrees[\"plant_date\"].isna().sum(),df_SFTrees[\"plant_date\"].isna().mean()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5763fe544a3908dc50e77ab975d683aca3459e88"},"cell_type":"code","source":"#2. Let us group and chart this data, for valid dates\n\n#This function extracts the 'year' from observations with valid 'date'\ndef valid_year(plant_date):\n    plantdate = str(plant_date)\n    try:\n        ts = pd.to_datetime(plantdate)\n        return int(ts.year)\n    except:\n        return int(1900)\n\n#Create a new column to hold the date\ndf_SFTrees[\"plantdate\"] = df_SFTrees[\"plant_date\"].apply(valid_year)\ndf_SFTrees.drop(\"plant_date\", inplace=True, axis=1)\n\nsns.set(style=\"white\")\nplanting_date = sns.factorplot(x=\"plantdate\", data=df_SFTrees[df_SFTrees[\"plantdate\"]>1900], kind=\"count\",palette=\"PiYG\", size=8, aspect=1.5)\nplanting_date.set_xticklabels(step=3)  #The 'step' parameter values ensures the 'X' axis is not crampped","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6fa1940d6c68affaaecc76b745c35ee63f7a5849"},"cell_type":"markdown","source":"**Note**:\n    1. We have data for 64873 observations, which constiture 34% of the total dataset. \n    2. We are missing data for 123589 observations, which constitute 66% of the total data set. These could swing our patterns..\n\n**Outcome**:\n    1. There seems to be trees older than 30+ years\n    2. We see high plantation during the late nineties and early 2000\n    3. Since late nineties, though the plantation has varied each year, their count have been better than the prior years"},{"metadata":{"trusted":true,"_uuid":"65a88b34a2107859826adee6b3fdabf092f6882e"},"cell_type":"code","source":"#Info on trees ploted before 1970.\ndf_SFTrees[(df_SFTrees[\"plantdate\"]<1970)  & (df_SFTrees[\"plantdate\"]>1900) ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4151b1565357805ec8167aa5d6f9a637a28ede37"},"cell_type":"markdown","source":"Outcome:\n    1. We see around 510 trees\n    2. Most of the observations are missing 'Tree' type. We will see if we can work with 'Species' and 'TreeID' column to fix this..\n    3. All under 'Private' care takers."},{"metadata":{"_uuid":"6b38d0ad3ebbacbd2f5be7c78da4002814d934c3"},"cell_type":"markdown","source":"Let us explore the Plant_Type column\n    1. Look at the missing values in the column\n    2. Update the 'case' of text"},{"metadata":{"trusted":true,"_uuid":"da239e4904df65a8412bba478835ce6cfb789bed"},"cell_type":"code","source":"#1. Let us look at missing info, like we did for \"Permit Notes\"\nprint(\"Total rows: {0}\".format (df_SFTrees.shape[0]))\nprint(\"Count of valid \\'Plant Type\\' rows: {0:d}, {1:.0%} of the total rows in the dataset\".format (df_SFTrees[\"plant_type\"].notna().sum(),df_SFTrees[\"plant_type\"].notna().sum()/df_SFTrees.shape[0]))\nprint(\"Count of missing \\'Plant Type\\' rows: {:d}, {:.0%} of the total rows in the dataset\".format (df_SFTrees[\"plant_type\"].isna().sum(),df_SFTrees[\"plant_type\"].isna().sum()/df_SFTrees.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09890fe2d4aa8da7e54ade081bda7f3058aceb93"},"cell_type":"code","source":"#Verify uniqueness in 'plant_type' column\ndf_SFTrees.groupby([\"plant_type\"])[\"plant_type\"].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3358cac69fa267242d869c247bf01e3bb88e4d5d"},"cell_type":"code","source":"#Cleanse the 'plant_type' column by updating the 'lower case' value, \"tree\"\ndf_SFTrees.loc[df_SFTrees[\"plant_type\"]==\"tree\",[\"plant_type\"]]=\"Tree\"\n#Verify uniqueness in 'plant_type' column now\ndf_SFTrees.groupby([\"plant_type\"])[\"plant_type\"].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18c9c3e00d80310dc2076b1a53f59577356dda97","collapsed":true},"cell_type":"code","source":"#Let us use One Hot Encoding to turn this 2-value only (\"Landscaping and Tree\") column to a interger column with values 1 and 0 only. \n#Then, drop one of the columns\ndf_PlantType = pd.get_dummies(df_SFTrees[\"plant_type\"])\ndf_SFTrees = pd.concat ([df_SFTrees, df_PlantType], axis=1)\ndf_SFTrees.drop(\"Landscaping\", axis=1, inplace=True) #Instead of this, we can use the 'drop_first' attribute of get_dummies function, to drop first column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4738b06376c176dcea4ccd0d78b24eea498ceb5c"},"cell_type":"code","source":"df_SFTrees.head(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e7440d5355fe07f9301ab15ebbc8576924b5c970"},"cell_type":"code","source":"#Drop the original column\ndf_SFTrees.drop(\"plant_type\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a101451525feacdcbdc10819e886c6e6ba03f3d"},"cell_type":"markdown","source":"Let us explore the Plot_Size column\n    1. Look at the missing values in the column\n    2. Try to make the values consistent.."},{"metadata":{"trusted":true,"_uuid":"d6201f0d196b4612cae6aa67bc84394b8a7d1c89"},"cell_type":"code","source":"#1. Let us look at missing info, like we did for \"Permit Notes\"\nprint(\"Total rows: {0}\".format (df_SFTrees.shape[0]))\nprint(\"Count of values in \\'Plot Size\\' rows: {0:d}, {1:.0%} of the total rows in the dataset\".format (df_SFTrees[\"plot_size\"].notna().sum(),df_SFTrees[\"plot_size\"].notna().sum()/df_SFTrees.shape[0]))\nprint(\"Count of missing \\'Plot Size\\'  rows: {:d}, {:.0%} of the total rows in the dataset\".format (df_SFTrees[\"plot_size\"].isna().sum(),df_SFTrees[\"plot_size\"].isna().sum()/df_SFTrees.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"450b6dba08525f9fd73265db454013413f3411a1"},"cell_type":"code","source":"#A brief look into the values in \"plot_size\" column reveals several things\"\n    #1. It has text and numeric values\n    #2. The numeric values are inconsistent. View the values given by the function below:\n    \npl_size =df_SFTrees[\"plot_size\"]\npl_count = df_SFTrees[\"plot_size\"].count()\nJ= 0\nfor i  in range(pl_count):\n    if str(pl_size[i])[ :5] != \"Width\":\n           if len(str(pl_size[i]))>6:\n                print(pl_size[i])\n                J = J + 1\n        \nprint (J)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db6cac19793640787da5786dbe48760e9471ba0d","collapsed":true},"cell_type":"code","source":"#The following function tries to bring the \"plot_size\" column into something consistent.\n#It uses Regex to look for matching text, tries to extract what is required and then tries to mutiply the values\n#EG: 3X3 or 3X3' will become 9 ('X' and apostrophe removed). I am assuming all valid values are in \"feet\" and none in \"metres\". \n#    If there are meters, note \"1 feet\" = \".3048 metre\"\n\ndef format_plot(Sizes):\n    Size = str(Sizes)\n    Size = Size.lower()\n    st=1\n    en=1\n    Size = re.sub('[a-w|y-z|\\' \\'|\\-|\\`]', '', Size)  \n    Size = re.sub('[/]','x', Size)\n    \n    if len(Size)<1:\n        Size='0'\n    if Size[-1]==\"x\":\n        Size = Size[:-1]\n    \n    try:\n        if Size.lower().find(\"x\")>0:\n            if len (Size[: Size.lower().find(\"x\")].strip())<=0:\n                st = 1\n            else:\n                st= float(Size[: Size.lower().find(\"x\")].strip())\n            if len(Size[Size.lower().find(\"x\")+1:].strip())<=0:\n                en = 1\n            else:\n                en = float(Size[Size.lower().find(\"x\")+1:].strip())\n            \n            Size = st * en\n            return (float(Size))\n    except:\n            return -1.     \n\n#Cleanse the Plot Size column\ndf_SFTrees[\"plotsize\"] = df_SFTrees[\"plot_size\"].apply(format_plot)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fb25798e06a74568a7664f58d0d146c592b3e1b"},"cell_type":"code","source":"#Let us look at what values were not processed\n#We need to restrict the values one cane enter, for plot_size. Else, we need to write a function that can do cleansing for values like below\ndf_SFTrees[df_SFTrees[\"plotsize\"] ==-1] [[\"plot_size\",\"plotsize\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75aee152d6cfe237b2a9078d664201bc38a63fcc","collapsed":true},"cell_type":"code","source":"#Since there are very few \"invalid\" plotsize columns, let us delete them\ndf_SFTrees.drop(df_SFTrees[df_SFTrees[\"plotsize\"] ==-1].index, axis=0, inplace=True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"27a7265227e944260035b9bd7ab759c1f7629bf5"},"cell_type":"markdown","source":"Let us explore the Site_Info column\n    1. Look at the missing values in the column\n    2. Look at column values"},{"metadata":{"trusted":true,"_uuid":"b84b1e756398819660abb1f14d73bb7c6ae655a7"},"cell_type":"code","source":"#1. Let us look at missing info, like we did for \"Permit Notes\"\nprint(\"Total rows: {0}\".format (df_SFTrees.shape[0]))\nprint(\"Count of values in \\'Site_Info\\' rows: {0:d}, {1:.0%} of the total rows in the dataset\".format (df_SFTrees[\"site_info\"].notna().sum(),df_SFTrees[\"site_info\"].notna().sum()/df_SFTrees.shape[0]))\nprint(\"Count of missing \\'Site_Info\\'  rows: {:d}, {:.0%} of the total rows in the dataset\".format (df_SFTrees[\"site_info\"].isna().sum(),df_SFTrees[\"site_info\"].isna().sum()/df_SFTrees.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26c7723bf808b1031a07a723586655fe66d76fd8","scrolled":false},"cell_type":"code","source":"#Let us see the possible values for this column\ndf_SFTrees.groupby(\"site_info\")[\"site_info\"].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bd3d916339dab9820c13a48b4cc8a54640f316c"},"cell_type":"code","source":"#Let us see the possible values for this column\ndf_SFTrees.groupby([\"care_assistant\",\"species\"])[[\"species\"]].count().rename(columns= {\"species\": \"count\"}).sort_values(by=\"count\",ascending=False).head(20).unstack(0).plot.barh()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cd2275e2c094f8d3ada6be81af69c7b7ed9c5c80"},"cell_type":"code","source":"#We could cluster the data and see how many clusters are there, what are the size of the clusters, outliers, etc...","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}