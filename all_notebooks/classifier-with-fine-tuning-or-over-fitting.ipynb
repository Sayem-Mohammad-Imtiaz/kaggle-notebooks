{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The goal is to predict rain in Austrilia based on the pastinformation or features given in the dataset,Always remember below check list\n#Any machine learning problem should be tackled with below steps. \n\n#1. Look at the big picture.\n#2. Get the data.\n#3. Discover and visualize the data to gain insights.\n#4. Prepare the data for Machine Learning algorithms.\n#5. Select a model and train it.\n#6. Fine-tune your model.\n#7. Present your solution.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Read the data into Pandas Data frame\ndf=pd.read_csv('/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf.info()\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numberical=[var for var in df.columns if df[var].dtype !='O']\nprint(\" There are {} categorical variables\\n\".format(len(numberical)))\nprint('The numberical variables are:',numberical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Date'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Date']=pd.to_datetime(df['Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Year']=df['Date'].dt.year\ndf['Month']=df['Date'].dt.month\ndf['Day']=df['Date'].dt.day","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('Date',inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical = [var for var in df.columns if df[var].dtype=='O']\n\nprint('There are {} categorical variables\\n'.format(len(categorical)))\n\nprint('The categorical variables are :', categorical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(figsize=(6,8))\nax=sns.countplot(y='RainTomorrow',data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[categorical].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#percentage of missing values in each of the catagorical variables\ndf[categorical].isnull().sum().sort_values(ascending=False)/len(df)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in categorical:\n    \n    print(var, ' contains ', len(df[var].unique()), ' labels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# exporeing missing values in catagorical variables\n\ndf[categorical].isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#expore Location variable\nprint('location contains',len(df['Location'].unique()),'Labels')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Location.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets to one hot encoding of location to eliminate catagorical futures and get dummy variable using pandas\n\npd.get_dummies(df.Location,drop_first=True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#now lets expore windgustdir variable\n\nprint('WindGustDir contains',len(df['WindGustDir'].unique()),'labels')\nprint(\"these are the values for WindGustDir\",df['WindGustDir'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets to one hot encoding of location to eliminate windgustdir catagorical futures and get dummy variable using pandda\n\npd.get_dummies(df.WindGustDir,drop_first=True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies(df.WindGustDir,drop_first=True,dummy_na=True).sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#there are 9330 missing values for WindGustDir\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('WinDir9am contains',len(df['WindDir9am'].unique()),'Lables')\nprint('WinDir9am has values:',df['WindDir9am'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['WindDir9am'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets to one hot encoding of location to eliminate windgustdir catagorical futures and get dummy variable using pandda\n\npd.get_dummies(df.WindDir9am,drop_first=True,dummy_na=True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.get_dummies(df.WindDir9am,drop_first=True,dummy_na=True).sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# there are 10013 missing values for WindDir9AM ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets expore WindDir3PM variable\n\nprint('WindDir3PM variable contains',len(df['WindDir3pm'].unique()),'Lables')\nprint(\"WindDir3PM Values contains\",df.WindDir3pm.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets get dummy variables for these WindDir3Pm variable via one hot encoding\n\npd.get_dummies(df.WindDir3pm,drop_first=True,dummy_na=True).sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# there are 3778 missing values for WindDir3pm variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets expore RainToday variable \nprint('RainToday Contains',len(df['RainToday'].unique()),'Lables')\ndf['RainToday'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets do one hot encoding of RainToday variable and get dummy variables \npd.get_dummies(df.RainToday,drop_first=True,dummy_na=True).sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#there are 1406 missing values for RainToday variable\n\n#NOW LETS expore numarical variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {} numerical variables\\n\".format(len(numberical)))\nprint(\"These numerical varibales are :\",numberical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[numberical].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# exporing missing values in numerical variables \n\ndf[numberical].isnull().sum().sort_values(ascending=False)/len(df)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view outliers in numerical variables\n\nprint(round(df[numberical].describe()),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets draw box plot to figure outliers \n#as above statitics shows that there may be outliers in Rainfall, Evaporation, WindSpeed9am and WindSpeed3pm columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\n\nplt.subplot(2,2,1)\nfig=df.boxplot(column='Rainfall')\nfig.set_title('')\nfig.set_ylabel('Rainfall')\n\nplt.subplot(2,2,2)\nfig=df.boxplot(column='Evaporation')\nfig.set_title('')\nfig.set_ylabel('Evaporation')\n\nplt.subplot(2,2,3)\nfig=df.boxplot(column='WindSpeed9am')\nfig.set_title('')\nfig.set_ylabel('WindSpeed9am')\n\n\nplt.subplot(2,2,4)\nfig=df.boxplot(column='WindSpeed3pm')\nfig.set_title('')\nfig.set_ylabel('WindSpeed3pm')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# above box plots confirms that there are outliers for these variables\n\n#Check the distribution of variable\nplt.figure(figsize=(15,10))\nplt.subplot(2, 2, 1)\nfig = df.Rainfall.hist(bins=10)\nfig.set_xlabel('Rainfall')\nfig.set_ylabel('RainTomorrow')\n\n\nplt.subplot(2, 2, 2)\nfig = df.Evaporation.hist(bins=10)\nfig.set_xlabel('Evaporation')\nfig.set_ylabel('RainTomorrow')\n\n\nplt.subplot(2, 2, 3)\nfig = df.WindSpeed9am.hist(bins=10)\nfig.set_xlabel('WindSpeed9am')\nfig.set_ylabel('RainTomorrow')\n\n\nplt.subplot(2, 2, 4)\nfig = df.WindSpeed3pm.hist(bins=10)\nfig.set_xlabel('WindSpeed3pm')\nfig.set_ylabel('RainTomorrow')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mat=df.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,12))\nsns.heatmap(corr_mat,annot=True,square=True,linecolor='White',fmt='.2f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_var = ['MinTemp', 'MaxTemp', 'Temp9am', 'Temp3pm', 'WindGustSpeed', 'WindSpeed3pm', 'Pressure9am', 'Pressure3pm']\n\nsns.pairplot(df[num_var],kind='scatter',diag_kind='hist')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#declar X,y\n\nX=df.drop(['RainTomorrow'],axis=1)\ny=df['RainTomorrow']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature engineering\n\nX_train.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.isnull().sum().sort_values(ascending=False)/len(X_train)*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute missing values in X_train,X_test\n\nfor df1 in [X_train,X_test]:\n    for col in numberical:\n        col_median=X_train[col].median()\n        df1[col].fillna(col_median,inplace=True)\n        \nfor df2 in [X_train,X_test]:\n    df2['WindGustDir'].fillna(X_train['WindGustDir'].mode()[0], inplace=True)\n    df2['WindDir9am'].fillna(X_train['WindDir9am'].mode()[0], inplace=True)\n    df2['WindDir3pm'].fillna(X_train['WindDir3pm'].mode()[0], inplace=True)\n    df2['RainToday'].fillna(X_train['RainToday'].mode()[0], inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\nencoder=ce.BinaryEncoder(cols=['RainToday'])\n\nX_train=encoder.fit_transform(X_train)\nX_test=encoder.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat([X_train[numberical], X_train[['RainToday_0', 'RainToday_1']],\n                     pd.get_dummies(X_train.Location), \n                     pd.get_dummies(X_train.WindGustDir),\n                     pd.get_dummies(X_train.WindDir9am),\n                     pd.get_dummies(X_train.WindDir3pm)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = pd.concat([X_test[numberical], X_test[['RainToday_0', 'RainToday_1']],\n                     pd.get_dummies(X_test.Location), \n                     pd.get_dummies(X_test.WindGustDir),\n                     pd.get_dummies(X_test.WindDir9am),\n                     pd.get_dummies(X_test.WindDir3pm)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature scaling \n\ncols=X_train.columns\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler=MinMaxScaler()\n\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=pd.DataFrame(X_train,columns=[cols])\nX_test=pd.DataFrame(X_test,columns=[cols])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Model Training\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogmodel=LogisticRegression(solver='liblinear',random_state=0)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predict results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=logmodel.predict(X_test)\npred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logmodel.predict_log_proba(X_test)[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the accurary of the score\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n\nprint(\"the accuraty of the model is \",accuracy_score(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train = logmodel.predict(X_train)\n\ny_pred_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Training set score: {:.4f}'.format(logmodel.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(logmodel.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg100=LogisticRegression(C=100,solver='liblinear',random_state=0)\nlogreg100.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train set score:{:4f}'.format(logreg100.score(X_train,y_train)))\nprint('Train set score:{:4f}'.format(logreg100.score(X_test,y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg001=LogisticRegression(C=0.01,solver='liblinear',random_state=0)\nlogreg001.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Verify that we are not overfitting the model by checking the scores againt train set and test test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train set score:{:4f}'.format(logreg001.score(X_train,y_train)))\nprint('Train set score:{:4f}'.format(logreg001.score(X_test,y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_accracy=(22067/(22067+6372))\nprint('Null accuracy score: {0:0.4f}'. format(null_accracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm=confusion_matrix(y_test,pred)\nprint('Confusion Matrix\\n\\n',cm)\nprint('\\nTrue Positive (TP)=',cm[0,0])\nprint('\\nTrue Negative (TN)=',cm[1,1])\nprint('\\nFalse Positive aka Type-1 Error (TN)=',cm[0,1])\nprint('\\nFalse Negative aka Type-2 Error (TN)=',cm[1,0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Classification report to understand the metrics and evaluate our model performance\n\nprint(classification_report(y_test,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now lets try to imporvoe our model using famous GridSearch ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.model_selection import GridSearchCV\n\n \nparameters=[{'penalty':['l1','l2']},{'C':[1,10]}]\n\ngrid_search=GridSearchCV(estimator=logmodel,param_grid=parameters,scoring='accuracy',cv=5,verbose=1)\n\ngrid_search.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Grid Search CV Best score:{:.4f}\\n\\n'.format(grid_search.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We see that the best score for Grid search is 0.975 which is very good but is this reliable ??/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print estimator that was chosen by the GridSearch\nprint('\\n\\nEstimator that was chosen by the search :','\\n\\n', (grid_search.best_estimator_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets try using popular XGBoost ensemble to see if we can get more accurate predictions for this classification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nxgbclassifier = XGBClassifier(n_estimators=100,learning_rate=0.05)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=X_train.loc[:,~X_train.columns.duplicated()]\nX_test=X_test.loc[:,~X_test.columns.duplicated()]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgbclassifier.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict=xgbclassifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets try to evaluate our model now to see if we did any good after using XGBclassifer with few hyperparametes. The goal is not to overfit our mode\nprint(classification_report(y_test,predict))\ncm1=confusion_matrix(y_test,predict)\nprint('Confusion Matrix\\n\\n',cm1)\nprint('\\nTrue Positive (TP)=',cm1[0,0])\nprint('\\nTrue Negative (TN)=',cm1[1,1])\nprint('\\nFalse Positive aka Type-1 Error (TN)=',cm1[0,1])\nprint('\\nFalse Negative aka Type-2 Error (TN)=',cm1[1,0])\ncm_matrix = pd.DataFrame(data=cm1, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#XBG classifer with hyperparameters(n_estimators=100,learning_rate=0.05) showing that type 1 error eliments 743 and type to error ???\n#Is this overfitting????? that what matters , we need to make sure we are not overfitting the model.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_train1 = xgbclassifier.predict(X_train)\n\ny_pred_train1\nprint('Train set score:{:4f}'.format(xgbclassifier.score(X_train,y_train)))\nprint('Test set score:{:4f}'.format(xgbclassifier.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as we can see train set score is showing as 100% accurate and test set score is 0.97.. can you validate what went wrong?\n#A good data scientist would know exactly if we are overfitting our model , while tacking realworld business problesm real would come in tacking overfitting situation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have seen that model is too imporved after Grid search and also using XGBClassifer with few hyperparameters n_estimators and learning rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# when using classical logistic regression we seen that our accuracy was around 85% .\n#When we try to improve our model using famous Grid search and ensemble XGBclassifer we tend to get great predictions but we need to make we are not overfitting ..\n#Can you identify if the model is overfitting ??? i will leave it up to you for open discussion?? \n#is this classic example of overfitting our model while trying to improve the same","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}