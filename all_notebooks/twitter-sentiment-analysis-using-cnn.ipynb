{"cells":[{"metadata":{"_uuid":"52bf27bebf8e95c02bb8b123a1d535075e8376c4"},"cell_type":"markdown","source":"# Twitter Sentiment Analysis Using CNN"},{"metadata":{"_uuid":"61b9efbf1cce15100bb9526da2017193b001fac5"},"cell_type":"markdown","source":"We have seen some applications of CNNs for text classification that are giving good results, so we try here to use CNNs for classifying sentiment of tweets as SAD or HAPPY."},{"metadata":{},"cell_type":"markdown","source":"## Solution"},{"metadata":{},"cell_type":"markdown","source":"Download dependencie file before starting"},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n!gunzip GoogleNews-vectors-negative300-SLIM.bin.gz","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"414a6b7e831613c3c9a88d9633fa412de72ebd8a"},"cell_type":"markdown","source":"## Load the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# This is for making some large tweets to be displayed\npd.options.display.max_colwidth = 100\n\n# I got some encoding issue, I didn't knew which one to use !\n# This post suggested an encoding that worked!\n# https://stackoverflow.com/questions/19699367/unicodedecodeerror-utf-8-codec-cant-decode-byte\ntrain_data = pd.read_csv(\"../input/train.csv\", encoding='ISO-8859-1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2df12f6473bd02427771d3ca0ae2de291ec03b83"},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c7ab7592afcdc9199eb2aa95b4d4fb819527d5b"},"cell_type":"markdown","source":"# Prepare the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import KeyedVectors\n\nembed = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300-SLIM.bin', binary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the vocabulary used by the embedding\n\n# dictionary for efficient search\nembed_vocab = {}\nfor word in embed.vocab:\n    embed_vocab[word] = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets = train_data['SentimentText']\ntweets_words = []\ntweets_vocab = {}\nfor tweet in tweets:\n    # Don't include words that can't actually be mapped to a vector using embeddings\n    words = tweet.split()\n    filtered_words = []\n    for word in words:\n        tweets_vocab[word] = True\n        if embed_vocab.get(word) is not None:\n            filtered_words.append(word)\n    tweets_words.append(filtered_words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We check how many words from the dataset's vocabulary\n# are found in the embedding vocab\n\ndef vocab_coverage(compared, base):\n    hit, miss = 0, 0\n    for word in compared:\n        if base.get(word) is not None:\n            hit += 1\n        else:\n            miss += 1\n\n    print(\"{} words were found\".format(hit))\n    print(\"{} words weren't found\".format(miss))\n    \nvocab_coverage(tweets_vocab, embed_vocab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We checked how well the vocab of our tweets datasets matches with the vocab of the embedding, looks like not a good fit, so we may get big improvement by training an embedding on this tweets."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We check the distribution of length of tweets after word filtering\nfrom collections import Counter\n\nlen_counts = Counter([len(tweet) for tweet in tweets_words])\n\nprint(len_counts)\npd.DataFrame([len(tweet) for tweet in tweets_words]).hist(bins=list(range(30)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"I think we should truncate to 20 words length and pad short tweets (filtered tweets here)"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets_words_idx = []\nfor tweet in tweets_words:\n    indexs = []\n    for word in tweet:\n        idx = embed.vocab[word].index\n        indexs.append(idx)\n    tweets_words_idx.append(indexs)\n\nprint(\"An example tweet: {}\".format(tweets_words_idx[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_labels = np.array(train_data['Sentiment'])\ndef prepare_data(all_tweets, all_labels, max_words):\n    labels = []\n    tweets = []\n    for i in range(len(all_tweets)):\n        tweet = all_tweets[i]\n        if not tweet:\n            continue\n        diff = max_words - len(tweet)\n        if diff > 0: # need to pad\n            tweet = [0 for j in range(diff)] + tweet\n        elif diff < 0:\n            tweet = tweet[: max_words]\n        tweets.append(tweet)\n        labels.append(all_labels[i])\n    return (np.array(tweets), np.array(labels))\n\ntweets, labels = prepare_data(tweets_words_idx, all_labels, 20)\nassert len(tweets) == len(labels)\n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare datsets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split train/val/test\nimport random\n\n# to reproduce\nrandom.seed(73)\n\nds_size = len(tweets)\ntrain_ds_ratio = 0.7\nidxs = list(range(ds_size))\nrandom.shuffle(idxs)\n\nend_train = int(ds_size * 0.7)\ntrain_idxs = idxs[:end_train]\nremaining_idxs = idxs[end_train:]\ntest_end = int(len(remaining_idxs) * 0.5)\ntest_idxs = remaining_idxs[:test_end]\nval_idxs = remaining_idxs[test_end:]\n\ntrain_data, train_label = tweets[train_idxs], labels[train_idxs]\ntest_data, test_label = tweets[test_idxs], labels[test_idxs]\nval_data, val_label = tweets[val_idxs], labels[val_idxs]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nbatch_size = 64\n\ndatasets = {\n    'train': TensorDataset(torch.from_numpy(train_data), torch.from_numpy(train_label)),\n    'val': TensorDataset(torch.from_numpy(val_data), torch.from_numpy(val_label)),\n    'test': TensorDataset(torch.from_numpy(test_data), torch.from_numpy(test_label)),\n}\n\ndataloaders = {\n    'train': DataLoader(datasets['train'], batch_size=batch_size),\n    'val': DataLoader(datasets['val'], batch_size=batch_size),\n    'test': DataLoader(datasets['test'], batch_size=batch_size),\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Architecture and Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First checking if GPU is available\ntrain_on_gpu=torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass SentimentCNN(nn.Module):\n    \"\"\"\n    The embedding layer + CNN model that will be used to perform sentiment analysis.\n    \"\"\"\n\n    def __init__(self, embed_model, vocab_size, output_size, embedding_dim,\n                 num_filters=100, kernel_sizes=[3, 4, 5], freeze_embeddings=True, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(SentimentCNN, self).__init__()\n\n        # set class vars\n        self.num_filters = num_filters\n        self.embedding_dim = embedding_dim\n        \n        # 1. embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        # set weights to pre-trained\n        self.embedding.weight = nn.Parameter(torch.from_numpy(embed_model.vectors)) # all vectors\n        # (optional) freeze embedding weights\n        if freeze_embeddings:\n            self.embedding.requires_grad = False\n        \n        # 2. convolutional layers\n        self.convs_1d = nn.ModuleList([\n            nn.Conv2d(1, num_filters, (k, embedding_dim), padding=(k-2, 0)) \n            for k in kernel_sizes])\n        \n        # 3. final, fully-connected layer for classification\n        self.fc = nn.Linear(len(kernel_sizes) * num_filters, output_size) \n        \n        # 4. dropout and sigmoid layers\n        self.dropout = nn.Dropout(drop_prob)\n        self.sig = nn.Sigmoid()\n        \n    \n    def conv_and_pool(self, x, conv):\n        \"\"\"\n        Convolutional + max pooling layer\n        \"\"\"\n        # squeeze last dim to get size: (batch_size, num_filters, conv_seq_length)\n        # conv_seq_length will be ~ 200\n        x = F.relu(conv(x)).squeeze(3)\n        \n        # 1D pool over conv_seq_length\n        # squeeze to get size: (batch_size, num_filters)\n        x_max = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x_max\n\n    def forward(self, x):\n        \"\"\"\n        Defines how a batch of inputs, x, passes through the model layers.\n        Returns a single, sigmoid-activated class score as output.\n        \"\"\"\n        # embedded vectors\n        embeds = self.embedding(x) # (batch_size, seq_length, embedding_dim)\n        # embeds.unsqueeze(1) creates a channel dimension that conv layers expect\n        embeds = embeds.unsqueeze(1)\n        \n        # get output of each conv-pool layer\n        conv_results = [self.conv_and_pool(embeds, conv) for conv in self.convs_1d]\n        \n        # concatenate results and add dropout\n        x = torch.cat(conv_results, 1)\n        x = self.dropout(x)\n        \n        # final logit\n        logit = self.fc(x) \n        \n        # sigmoid-activated --> a class score\n        return self.sig(logit)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the model w/ hyperparams\n\nvocab_size = len(embed_vocab)\noutput_size = 1 # binary class (1 or 0)\nembedding_dim = len(embed['for']) # 300-dim vectors\nnum_filters = 100\nkernel_sizes = [3, 4, 5]\n\nnet = SentimentCNN(embed, vocab_size, output_size, embedding_dim,\n                   num_filters, kernel_sizes)\n\nprint(net)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\n# training loop\ndef train(net, train_loader, valid_loader, epochs, print_every=100):\n\n    # move model to GPU, if available\n    if(train_on_gpu):\n        net.cuda()\n\n    counter = 0 # for printing\n    min_loss = np.Inf\n    # train for some number of epochs\n    net.train()\n    for e in range(epochs):\n\n        # batch loop\n        for inputs, labels in train_loader:\n            counter += 1\n\n            if(train_on_gpu):\n                inputs, labels = inputs.cuda(), labels.cuda()\n\n            # zero accumulated gradients\n            net.zero_grad()\n\n            # get the output from the model\n            output = net(inputs)\n\n            # calculate the loss and perform backprop\n            loss = criterion(output.squeeze(), labels.float())\n            loss.backward()\n            optimizer.step()\n\n            # loss stats\n            if counter % print_every == 0:\n                # Get validation loss\n                val_losses = []\n                net.eval()\n                for inputs, labels in valid_loader:\n\n                    if(train_on_gpu):\n                        inputs, labels = inputs.cuda(), labels.cuda()\n\n                    output = net(inputs)\n                    val_loss = criterion(output.squeeze(), labels.float())\n\n                    val_losses.append(val_loss.item())\n                \n                if np.mean(val_losses) < min_loss:\n                    min_loss = np.mean(val_losses)\n                    torch.save(net.state_dict(), \"model.pth\")\n                    print(\"New val loss... saving model\")\n                \n                net.train()\n                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(counter),\n                      \"Loss: {:.6f}...\".format(loss.item()),\n                      \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training params\n\nepochs = 5 # this is approx where I noticed the validation loss stop decreasing\nprint_every = 100\n\ntrain(net, dataloaders['train'], dataloaders['val'], epochs, print_every=print_every)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net.load_state_dict(torch.load(\"model.pth\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16c0270e6a97b641762eb9291bcf7d5ad491f450"},"cell_type":"markdown","source":"# Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get test data loss and accuracy\n\ntest_losses = [] # track loss\nnum_correct = 0\n\n\nnet.eval()\n# iterate over test data\nfor inputs, labels in dataloaders['test']:\n\n    if(train_on_gpu):\n        inputs, labels = inputs.cuda(), labels.cuda()\n    \n    # get predicted outputs\n    output = net(inputs)\n    \n    # calculate loss\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    \n    # convert output probabilities to predicted class (0 or 1)\n    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n    \n    # compare predictions to true label\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n\n# -- stats! -- ##\n# avg test loss\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n\n# accuracy over all test data\ntest_acc = num_correct / len(dataloaders['test'].dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}