{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Asteroid Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Predicting whether an asteroid is hazardous or not.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n    \n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#load data\nasteroids_data = pd.read_csv('/kaggle/input/nasa-asteroids-classification/nasa.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"asteroids_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target variable, **Hazardous**, has labels True and False. We will transform these labels to the binary, 1 and 0, where 1 is True and 0 is False.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#binarising Hazardous\nasteroids_data['Hazardous'].replace({True: 1, False: 0}, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"asteroids_data['Hazardous'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n**ATTRIBUTES**\n* **Neo Reference ID** - Near Earth Object (NEO) reference ID number for an asteroid (or a comet) which comes close to earth\n* **Name** - 'Name' of asteroid (same as NEO Reference ID)\n* **Absolute Magnitude** - A measure of the asteroid's luminosity (in H) (the brightness of an asteroid if it is 1 astronomical unit away from both the Sun and the observer, and the angle between the Sun, asteroid, and Earth is 0 degrees) \n* **Est Dia in (in KM, M, Miles, and Feet) (min)** - Minimum estimated diameter of the asteroid (Note: Since asteroids cannot be directly measured and because they have irregular shapes, their diameters are estimates. These estimates are calculated using its absolute magnitude and geometric albedo.)   \n* **Est Dia in (in KM, M, Miles, and Feet) (max)**  - Maximum estimated diameter of the asteroid      \n* **Close Approach Date** - Date at which the asteroid approaches close to Earth      \n* **Epoch Date Close Approach** - Date at which the asteroid approaches close to Earth (in epoch time)   \n* **Relative Velocity (in km per sec, km per hr, and miles per hour)** - Asteroid's velocity relative to earth\n* **Miss Dist.(in Astronomical, lunar, km, and miles)** - Distance by which the asteroid misses Earth\n* **Orbiting Body**                   \n* **Orbit ID** - An ID of JPL NEA orbit that JPL Nasa uses in its analysis               \n* **Orbit Determination Date** - Date at which the asteroid's orbit was determined\n* **Orbit Uncertainity** - A measure of the uncertainity ('measurement errors') in the calculated orbit \n* **Minimum Orbit Intersection** - The closest distance between Earth and the asteroid in their respective orbits (in astronomical units)\n* **Jupiter Tisserand Invariant** -  A value used to differentiate between asteroids and Jupiter-family comets    \n* **Epoch Osculation** - The instance of time at which the asteroid's position and velocity vectors (from which its osculating orbit is calculated) is specified           \n* **Eccentricity** - A value which specifies by how much the asteroid's orbit deviates from a perfect circle               \n* **Semi Major Axis** - The longest radius of an elliptical orbit; a easure of the asteroid's average distance from the Sun  (asteroids orbit the Sun)              \n* **Inclination** - Measures the tilt of the asteroid's orbit around the Sun                 \n* **Asc Node Longitude** - (copying from NASA) 'Angle in the ecliptic plane between the inertial-frame x-axis and the line through the ascending node'\n* **Orbital Period** - Time taken for asteroid to complete a single orbit around the Sun\n* **Perihelion Distance** - Distance of point in asteroid's orbit which is closest to the Sun\n* **Perihelion Arg** - (copying from Nasa) 'The angle (in the body's orbit plane) between the ascending node line and perihelion measured in the direction of the body's orbit'\n* **Aphelion Dist** - Distance of point in asteroid's orbit which is farthest from the Sun\n* **Perihelion Time** - Length of time of asteroid's passage through the perihelion stage\n* **Mean Anomaly** - (copying from Nasa) 'The product of an orbiting body's mean motion and time past perihelion passage'\n* **Mean Motion** - (copying from Nasa) 'The angular speed required for a body to make one orbit around an ideal ellipse with a specific semi-major axis'\n* **Equinox** - An astronomical standard to measure against (currently 'J2000.0')\n* **Hazardous** - Is the asteroid hazardous? (True or False)  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, we will collate all the variables (except the target **Hazardous**) in a list called **features**, and we will drop the features we have decided not to include in the model as we go along. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of features\nfeatures = asteroids_data.columns.tolist()\nfeatures = features[0:-1]\n#check\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"asteroids_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a small dataset with only 4687 instances. There are 40 attributes but a few of them measure the same feature of the asteroid but in different standards. These attributes are **Est Dia** (in various measures), **Close Approach Date** and **Epoch Date Close Approach**, **Relative Velocity** (in various measures), and **Miss Dist.**(in various measures). There are no missing values.\n\nThere are 5 non-numerical attribute, of which one is a boolean (**Hazardous**), and the others (**Close Approach Date**, **Orbiting Body**,**Orbit Determination Date**, **Equinox**) are of type object. Let's have a look at the object type attribute.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"asteroids_data[['Close Approach Date', 'Orbiting Body','Orbit Determination Date', 'Equinox']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Two of the object types are dates.  We do not need the date features for our model. \n\nHow many unique vales do **Orbiting Body** and **Equinox** have?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#unique values\nprint('Orbiting Body unique values: {}'.format(asteroids_data['Orbiting Body'].unique()))\nprint('Equinox unique values: {}'.format(asteroids_data['Equinox'].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each of the two features have only a single unique value so they are useless for training the model.\n\nLet's examine the numerical type values.\n\n* **NEO Reference ID, Name, and Orbit ID** are variables which are used by researchers to identify the asteroid or an orbit. We do not need these for the model.\n* Among the featuress which measure the same thing in different standards, we will select **Est Dia in KM(min), Est Dia in KM(max), Relative Velocity in km per hour**, and **Miss Dist.(kilometers)**. We will drop the other representations of these measurements.\n* Many of the reamining attributes of numeric type are orbital elements which are used to calculate the asteroid's orbit.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Before we move on, we will first drop the features we do not need for the model from out list of features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in ['Neo Reference ID', 'Name', 'Est Dia in M(min)', 'Est Dia in M(max)', 'Est Dia in Miles(min)', 'Est Dia in Miles(max)', \n          'Est Dia in Feet(min)', 'Est Dia in Feet(max)', 'Close Approach Date', 'Epoch Date Close Approach', 'Relative Velocity km per sec', \n          'Miles per hour', 'Miss Dist.(Astronomical)', 'Miss Dist.(lunar)', 'Miss Dist.(miles)', 'Orbiting Body', 'Orbit ID', 'Orbit Determination Date', 'Epoch Osculation', 'Equinox']:\n    features.remove(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are now {} features.'.format(len(features)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CREATE TRAINING, VALIDATION, AND TEST SET**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will now split the dataset into train, validation, and test sets in the proportions 60%, 20%, and 20% respectively.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\n#split data into train, cross validation, and test sets\nasteroid_set, asteroid_test = train_test_split(asteroids_data, test_size=0.2, random_state=7)\nasteroid_train, asteroid_cv = train_test_split(asteroid_set, test_size=0.25, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"NASA'a CNEOS defines[ petentially hazardous asteroids](https://cneos.jpl.nasa.gov/about/neo_groups.html) as an asteroid with \"a minimum orbit intersection distance (MOID) of 0.05 au or less and an absolute magnitude (H) of 22.0 or less\". Note: 1 au (astronomical unit) = 149597870700 m (roughly distance between Earth and Sun)\n\nWe will plot Minimum Orbit Intersection, Absolute Magnitude, and Hazardous in a scatter plot to see this relationship.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#pairplot to see relationship between 'Minimum Orbit Intersection', 'Absolute Magnitude', and 'Hazardous'\nsns.pairplot(asteroid_train[['Minimum Orbit Intersection', 'Absolute Magnitude', 'Hazardous' ]], diag_kind = 'hist', hue='Hazardous', palette = {1:'red', 0:'blue'})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.scatterplot(asteroid_train['Minimum Orbit Intersection'], asteroid_train['Absolute Magnitude'], hue = asteroid_train['Hazardous'], palette={1: 'red', 0:'blue'})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From these plots, we can see that asteroids in the datatset which are hazardous have an **Absolute Magnitude** between roughly 15 and 23 H, with a **Minimum Orbit Intersection** between roughly 0 and 0.05 au. This dataset reflects the NASA's definition of potentially hazardous asteroids. \n\nIf we just have these two features to train a model to predict whether an asteroid is hazardous, we can implement a simple binary logistic regression classifier. Let's try that.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**BIVARIATE BINARY CLASSIFIER**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"asteroid_train[['Minimum Orbit Intersection','Absolute Magnitude']].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the plots and the table above, there are no significant outliers in the context of asteroid measurements. However, since a couple of points are scattered well away from the bulk of the distribution, we will use the robust scsler to scale the data before training. The robust scaler uses the interquartile range to scale, and is thus not influenced by outliers.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First, we will scale **Absolute Magnitude** and **Minimum Orbit Intersection** using sklearn's Robust scaler. We will scale these two features in the train, validation, and test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import robust_scale\n\nfor col in ['Absolute Magnitude', 'Minimum Orbit Intersection']:\n    asteroid_train[col + \"_scaled\"] = robust_scale(asteroid_train[col])\n    asteroid_cv[col + \"_scaled\"] = robust_scale(asteroid_cv[col])\n    asteroid_test[col + \"_scaled\"] = robust_scale(asteroid_test[col])\n    features.append(col + '_scaled')\n    features.remove(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now train a logistic regression model using these two scaled features on the training set, and test on the validation set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport scikitplot as skplt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(asteroid_train[['Absolute Magnitude_scaled','Minimum Orbit Intersection_scaled']], asteroid_train['Hazardous'])\npredictions = lr.predict(asteroid_cv[['Absolute Magnitude_scaled','Minimum Orbit Intersection_scaled']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We know that the target variable skews to 0 (it has prominently more 0 labels than 1 labels), therefore accuracy alone will not be a good evaluation of the model. Below is a function that will return a confusion matrix and will calculate precision, recall, null accuracy, specificity, and the F_score.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\nfrom sklearn.metrics import accuracy_score\n\n\ndef evaluation(test_y, predictions):\n    \n    #accuracy score\n    accuracy = accuracy_score(test_y, predictions)\n    print(\"The classification accuracy is {:.2f} %.\" .format(accuracy*100))\n    \n  \n    y_test_mean = test_y.mean()\n    #null accuracy\n    null_accuracy = max(y_test_mean, 1-y_test_mean)\n    print('The null accuracy is {:.2f} %.'.format(null_accuracy*100))\n    \n    #confusion matrix\n    skplt.metrics.plot_confusion_matrix(test_y, predictions)\n    \n    conf_matrix = confusion_matrix(test_y, predictions)\n    \n    TN = conf_matrix[0,0] #true negatives\n    FP = conf_matrix[0,1] #false positives\n    FN = conf_matrix[1,0] #false negatives\n    TP = conf_matrix[1,1] #true positives\n    \n    #precision\n    precision = TP/(TP+FP)*100\n    print('The precision is {:.2f} %.'.format(precision))\n    #sensitivity/ recall\n    recall = TP/(FN+TP)*100\n    print('The sensitivity/recall is {:.2f} %.'.format(recall))\n    #specificity\n    specificity = TN/(FP+TN)*100\n    print('The specificity is {:.2f} %.'.format(specificity))\n    #F_score\n    F_score = (2*precision*recall)/(precision + recall)\n    print('The F score is {:.2f} %.'.format(F_score))\n    \n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation(asteroid_cv['Hazardous'], predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the model predicts the label correctly 92.75% of the time, this does not give a true picture of the model's performance as the distribution of the target variable is skewed. This is reflected in the null accuracy score (the score if the model predicted the dominant class at all times). The classification accuracy is only four precentage points greater than the null accuracy. \n\nFor the task of hazardous asteroid prediction, it is important that when an asteroid is hazardous, the model correctly predicts that it is. This is measured by recall and this model's recall is 89.91%. If we wanted to increase recall, we can lower the prediction threshold from 0.5 (the default in sklearn's Log Reg estimator) to a lower number. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**SVM CLASSIFIER**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Could a SVM classifier yield better results? We will use the Gaussian kernel function. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn.svm import SVC\n\nsvm = SVC(C=1.5, kernel='rbf')\nsvm.fit(asteroid_train[['Absolute Magnitude_scaled','Minimum Orbit Intersection_scaled']], asteroid_train['Hazardous'])\nsvm_pred = svm.predict(asteroid_cv[['Absolute Magnitude_scaled','Minimum Orbit Intersection_scaled']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluation(asteroid_cv['Hazardous'], svm_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The evaluation scores of the SVM classifier model on the validation data is an improvement on the evaluation scores of the Logistic Regression model. We will predict using the test set as well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#test set predictions\nsvm_pred = svm.predict(asteroid_test[['Absolute Magnitude_scaled','Minimum Orbit Intersection_scaled']])\nevaluation(asteroid_test['Hazardous'], svm_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The recall on the test set is higher than on the validation set, and the precision is lower.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**DECISION BOUNDARIES**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Both of the models above have only two features which means we can plot the decision boundary of each model. It will give a useful insight into how each classifier splits the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = asteroid_test['Absolute Magnitude_scaled']\nX2 = asteroid_test['Minimum Orbit Intersection_scaled']\n#create meshgrid for contour plot\nx1, x2 = np.meshgrid(np.arange(X1.min() - 0.25, X1.max() + 0.25, 0.01), \n                     np.arange(X2.min() - 0.25, X2.max() + 0.25, 0.01))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Boundary for logistic classifier model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"L = lr.predict(np.c_[x1.ravel(), x2.ravel()])\nL = L.reshape(x1.shape)\n#plot boundary and points\nplt.figure(figsize=(12,10))\nplt.contourf(x2, x1, L, cmap=plt.cm.twilight, alpha=0.8)\n#plt.scatter(asteroid_test['Minimum Orbit Intersection_scaled'],asteroid_test['Absolute Magnitude_scaled'], \n          # cmap=plt.cm.coolwarm, s=20, edgecolors='k')\nsns.scatterplot(asteroid_test['Minimum Orbit Intersection_scaled'],asteroid_test['Absolute Magnitude_scaled'], hue=asteroid_test['Hazardous'],  palette = {1:'red', 0:'blue'})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The decision boundary of the logistic regression classifier is a straight line that cuts through the points to minimise the mean squared error. The trained boundary captures most of the points of the test data set in the correct class but could be improved. Would polynomial features help?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Decision Boundary for svm (with gaussian kernel) classifier model**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = svm.predict(np.c_[x1.ravel(), x2.ravel()])\nZ = Z.reshape(x1.shape)\n#plot boundary and points\nplt.figure(figsize=(12,10))\nplt.contourf(x2, x1, Z, cmap=plt.cm.twilight, alpha=0.8)\n#plt.scatter(asteroid_test['Minimum Orbit Intersection_scaled'],asteroid_test['Absolute Magnitude_scaled'], \n          # cmap=plt.cm.coolwarm, s=20, edgecolors='k')\nsns.scatterplot(asteroid_test['Minimum Orbit Intersection_scaled'],asteroid_test['Absolute Magnitude_scaled'], hue=asteroid_test['Hazardous'],  palette = {1:'red', 0:'blue'})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The decision boundary of the svm classifier is a curved line that cuts through the points in a way as to maximise the distance between the positive and negative examples. Here the trained boundary curves around the test data points in such a way that it captures nearly all the points in its correct class.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Based on the our observation of the logistic regression model, it will be insightful to train and test a model with polynomial features of two or three degrees.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#train and test Logistic regression model with polynomial features\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfor i in [2,3]:\n    poly = PolynomialFeatures(i)\n    X = poly.fit_transform(asteroid_train[['Absolute Magnitude_scaled','Minimum Orbit Intersection_scaled']])\n    \n    lr_poly = LogisticRegression()\n    lr_poly.fit(X, asteroid_train['Hazardous'])\n    X_cv = poly.fit_transform(asteroid_cv[['Absolute Magnitude_scaled','Minimum Orbit Intersection_scaled']])\n    pred_poly = lr_poly.predict(X_cv)\n    \n    print('Logistic Regression with polynomial features degree {} scores: \\n'.format(i))\n    evaluation(asteroid_cv['Hazardous'], pred_poly)\n    print('\\n')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The logistic regression classifier with polynomial features has better precision, recall, and accuracy than the linear logistic regressor. There hardly any difference in the scores betwwen the model with polynomial features of degree 2 and that with polynomial features of degree 3. The svm model has better recall than any of the logistic regressor models. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}