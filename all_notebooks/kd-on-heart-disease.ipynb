{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn \nprint('The scikit-learn version is {}.'.format(sklearn.__version__))\nimport numpy \nprint('The scikit-learn version is {}.'.format(numpy.__version__))\nimport scipy \nprint('The scikit-learn version is {}.'.format(scipy.__version__))\nimport joblib \nprint('The scikit-learn version is {}.'.format(joblib.__version__))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import StandardScaler  \nfrom sklearn.neighbors import KNeighborsClassifier   \nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/heart.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# renaming column names to be more interpretable \n\nupdated_cols = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'SerumCholestoral', 'FastingBloodSugar', 'RestingECG', \n               'MaxHeartRate', 'ExeriseEnducedAngina', 'OldPeak', 'SlopeOldPeak', 'MajorVessels', 'Thal', 'Output']\ndf.columns = updated_cols\ndf.to_csv(\"..heart1.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:, :-1].values  \ny = df.iloc[:, -1].values  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Decision Tree. CART Algorithm**"},{"metadata":{"trusted":true},"cell_type":"code","source":"error = []\n\n# Calculating error for K values between 1 and 40\nfor i in range(1, 15):  \n    clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=i, min_samples_split=2,\n                                  min_samples_leaf=1, random_state=0)\n    clf.fit(X_train, y_train)\n    pred_i = clf.predict(X_test)\n    error.append(np.mean(pred_i != y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))  \nplt.plot(range(1, 15), error, color='red', linestyle='dashed', marker='o',  \n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate DT Depth')  \nplt.xlabel('Depth')  \nplt.ylabel('Mean Error')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=4, min_samples_split=2,\n                                  min_samples_leaf=1, random_state=0)\nclf = clf.fit(X_train, y_train )\n(pd.Series(clf.feature_importances_, index=df.columns[:-1])\n   .nlargest(13)\n   .plot(kind='barh'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nexport_graphviz(clf, out_file='tree.dot', feature_names = df.columns[:-1],\n                class_names = df.columns[-1],\n                rounded = True, proportion = False, precision = 2, filled = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert to png\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display in python\nplt.figure(figsize = (37, 45))\nplt.imshow(plt.imread('tree.png'))\nplt.axis('off');\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.savefig('tree.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ny_pred = clf.predict(X_test)\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. K-NN**\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()  \nscaler.fit(X_train)\n\nX_train_s = scaler.transform(X_train)  \nX_test_s = scaler.transform(X_test)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = KNeighborsClassifier(n_neighbors=4)  \nclassifier.fit(X_train_s, y_train)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test_s)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix  \nprint(confusion_matrix(y_test, y_pred))  \nprint(classification_report(y_test, y_pred))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"error = []\n\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):  \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train_s, y_train)\n    pred_i = knn.predict(X_test_s)\n    error.append(np.mean(pred_i != y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 6))  \nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',  \n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')  \nplt.xlabel('K Value')  \nplt.ylabel('Mean Error')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['Age'],color='Red',hist_kws={'alpha':1,\"linewidth\": 2}, kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True,cmap='YlGnBu',fmt='.2f',linewidths=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')\nplt.figure(figsize=(30,15))\nsns.pairplot(df, hue='Output', palette='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 4\n\n\nX = np.c_[X_train[:,0], X_train[:,7]].reshape(len(X_train),2) # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\ny = y_train\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA' ])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n\nfor weights in ['uniform', 'distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    \n    \n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"2-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.K-Means**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nimport numpy as np\nX_train_s = np.c_[X_train[:,4], X_train[:,7]].reshape(len(X_train_s),2)\nX_test_s =  np.c_[X_test_s[:,4], X_test_s[:,7]].reshape(len(X_test_s),2)\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X_train_s)\ny_pred = kmeans.predict(X_test_s)\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.cluster_centers_\nX_train_s","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(X_train_s[:,0], X_train_s[:,1], c=kmeans.labels_, cmap='rainbow')  \nplt.scatter(kmeans.cluster_centers_[:,0] ,kmeans.cluster_centers_[:,1], color='black')  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Naive bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\nprint(\"Number of mislabeled points out of a total %d points : %d\"\n     % (X_test.shape[0],(y_test != y_pred).sum()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\ny_pred = gnb.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5. SVM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nclf = svm.SVC(gamma='scale')\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy_score(y_test, y_pred)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6. AdaBoost**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.datasets import make_classification\nada=AdaBoostClassifier()\nsearch_grid={'n_estimators':[500,1000,2000],'learning_rate':[.001,0.01,.1]}\nsearch=GridSearchCV(estimator=ada,param_grid=search_grid,scoring='accuracy',n_jobs=1,cv=5)\nsearch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n          learning_rate=1.0, n_estimators=50, random_state=None)\nclf.fit(X_train, y_train)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(pd.Series(clf.feature_importances_, index=df.columns[:-1])\n   .nlargest(13)\n   .plot(kind='barh'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score=np.mean(cross_val_score(clf,X_train,y_train,scoring='accuracy',cv=5,n_jobs=1))\nscore","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}