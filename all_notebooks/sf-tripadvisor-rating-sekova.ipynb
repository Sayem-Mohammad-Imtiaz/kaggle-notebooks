{"cells":[{"metadata":{},"cell_type":"markdown","source":"# import"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\n\n# Загружаем специальный удобный инструмент для разделения датасета:\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\nRANDOM_SEED = 42","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n!pip freeze > requirements.txt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATA"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\nsample_submission = pd.read_csv(DATA_DIR+'/sample_submission.csv')\ndf_cities = pd.read_csv('../input/world-cities/worldcities.csv')\ndf_cost=pd.read_csv('/kaggle/input/2020-cost-of-living/cost of living 2020.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cities.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cost.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Подробнее по признакам:\n* City: Город \n* Cuisine Style: Кухня\n* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n* Price Range: Цены в ресторане в 3 категориях\n* Number of Reviews: Количество отзывов\n* Reviews: 2 последних отзыва и даты этих отзывов\n* URL_TA: страница ресторана на 'www.tripadvisor.com' \n* ID_TA: ID ресторана в TripAdvisor\n* Rating: Рейтинг ресторана"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Reviews[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Как видим, большинство признаков у нас требует очистки и предварительной обработки."},{"metadata":{},"cell_type":"markdown","source":"## 1. Обработка NAN \nИ так, пропуски есть в следующих столбцах:\n* Cuisine Style\n* Price Range\n* Number of Reviews\n* Reviews"},{"metadata":{},"cell_type":"markdown","source":"Какие варианты кухни самые популярные?"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Cuisine Style']=data['Cuisine Style'].fillna(\"['no_data']\") #заменяем пропуск техническим сообщением\ndef make_a_list (line):\n    line=line[1:-1]\n    line=line.replace(\"'\", \"\")\n    line=line.split(', ')\n    return (line)\ncousine_list=[]\ndata['Cuisine Style']=data['Cuisine Style'].apply(make_a_list)\ndata['Cuisine Style'].apply(lambda x: cousine_list.extend(x))\nfrom collections import Counter\nCounter(cousine_list).most_common(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Заменяем 'no_data' на два самых популярных значения"},{"metadata":{"trusted":true},"cell_type":"code","source":"def replacement (line):\n    if line == ['no_data']:\n        return ['European', 'Vegetarian Friendly']\n    else:\n        return line\ndata['Cuisine Style']=data['Cuisine Style'].apply(replacement)\ncousine_list=[]\ndata['Cuisine Style'].apply(lambda x: cousine_list.extend(x))\nCounter(cousine_list).most_common(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"markdown","source":"Самая распространённая ценовая категория:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Price Range'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Средняя ценовая категория самая распространённая. На нее и заменим пропуски."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Price Range']=data['Price Range'].fillna('$$ - $$$')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Что с количеством отзывов? Посмотрим на распределение. Найдём среднее, медиану и моду."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Number of Reviews'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Распределение не похоже на нормальное. Возможно, потом придётся очищать данные от выбросов."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['Number of Reviews'].mean(), data['Number of Reviews'].median())\ndata['Number of Reviews'].value_counts(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Пропущенных значений не так уж и много. Для начала попробуем заполнить пропуски модой - нулевым значением."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Number of Reviews'].fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Пропуски, а за одно и пустые значения я пока что заменю технической строкой."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Reviews']=data['Reviews'].fillna(\"[['no_data'], ['no_date']]\")\ndata['Reviews']=data['Reviews'].apply(lambda x: x.replace(\"[[], []]\", \"[['no_data'], ['no_date']]\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Обработка признаков\nДля начала посмотрим какие признаки у нас могут быть категориальными."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Возьмем следующий признак \"Price Range\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"def sign_to_range (x):\n    if x == \"$\":\n        return 1\n    elif x == \"$$ - $$$\":\n        return 2.5\n    else:\n        return 4\ndata['Price Range']=data['Price Range'].apply(sign_to_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cuisine Style. После замены пропусков в ячейках этого столбца лежат списки. Добавим Dummie-переменные к наиболее популярным вариантам кухонь. Наименее популярные отнесем к параметру \"others\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Сначала заново сформируем список вариантов кухни.\ncousine_list=[]\ndata['Cuisine Style'].apply(lambda x: cousine_list.extend(x))\n#Теперь посмотрим, какие кухни наиболее популярны, а какие - нет.\nCounter(cousine_list).most_common(150)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Мы, ведь, не будем спорить с тем, что 'Vegetarian Friendly' - это не то же, что 'Vegan Options', а 'Japanese' - не то же, что 'Sushi'. Не вижу смысла вводить дополнительные опции, чтобы объединять некоторые элементы списка под одним заголовком. А, вот, объединить, n (число может быть изменено при оптимизации) наименее популярных вариантов в один - 'others' - всё же стоит."},{"metadata":{"trusted":true},"cell_type":"code","source":"top=list(pd.Series(dict((Counter(cousine_list).most_common(70)))).index) #Очень странный костыль, чтобы сформировать множество самых частых кухонь.\nothers=(list(set(cousine_list)-set(top))) # формируем множество самых редких вариантов\ndef other(cuisines):\n    for cuisine in cuisines:\n        if cuisine in others:\n            return 'other'\n        else:\n            return cuisine\n\ndata['Cuisine Style']=data['Cuisine Style'].apply(other) # меняем наименее популярные варианты на \"other\"\ndummi_cuisine=pd.get_dummies(data['Cuisine Style'].apply(pd.Series).stack()).sum(level=0) # Создаём сет dummie-переменных\ndata=pd.concat([data,dummi_cuisine], axis=1) # Соединяем с сетом \"data\"\ndata['Cuisine Style']=data['Cuisine Style'].apply(lambda x: len(x)) # В исходные ячейки возвращаем новый признак\ndata.sample(2) #смотрим, что получилось","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Отзывы. Самый сложный с точки зрения обработки формат данных."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Reviews']=data['Reviews'].replace(\"[[], []]\", \"[['no_review'], ['01/01/2000']]\") #Заменяем пустые значения техническими.\nrev_dict=set()\ndef Reviews_reader(line):\n    line=line[2:-2]\n    line=line.split('], [')\n    line[0]=line[0].split(', ')\n    line[1]=line[1].split(', ')\n   \n    for rev in line[0]:\n        rev=(rev[1:-1]).lower()\n        rev=rev.replace('!','')\n        rev=rev.replace('.','')\n        rev=rev.split(' ')\n        for word in rev:\n            rev_dict.add(word)\n    return(line)\ndata['Reviews']=data['Reviews'].apply(Reviews_reader)\nrev_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_in_review={'Good':['gusto','nya','bellisimo','dequate','pleasantly','wunderfull','delucious','excellient','picturesque','👍👍','good','great','best','excellent','nice','delicious','lovely','tasty','amazing','fantastic','perfect','wonderful','pleasant','cozy','awesome','yummy','fabulous','cool','fine','brilliant','enjoyable','good!','outstanding','delicious!','charming','affordable','delightful','comfortable', '+','gorgeous','👏👏👏👏👏'],\n               'Bad':['weak','only?','grubby','awseome','wash','ameri','weakest','filthy','disasterous','becareful','miserable','foo','bad','poor','stop','worst','disappointing','terrible','overpriced','rude','disappointed','horrible','mediocre','unfriendly','worse','dirty','disappointment','fo','waste','satisfying']}\n# У меня получился вот такой словарь эпитетов, встречающихся в отзывах. Видимо, хвалить люди любят всё же больше, чем ругать. Ну, или кто-то что-то накручивает.\n\ndef Reviews_counter (line): # вводим функцию для перевода отзывов в численное значение\n    count=0\n    for word in line[0]:\n        if word in word_in_review['Good']:\n            count+=1\n        elif word in word_in_review['Bad']:\n            count-=1\n    return(count)\ndata['Reviews']=data['Reviews'].apply(Reviews_counter)\ndata.sample(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Теперь попробуем найти особые \"фишки\" городов. Соберём множество городов, на основании этого множества создадим DF, содержащий важные признаки городов."},{"metadata":{"trusted":true},"cell_type":"code","source":"cityes=set()\ndata['City'].apply(lambda x: cityes.add(x))\ncityes\nlen(cityes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"И так, у на 31 город. Добавим для городов такие признаки как средняя температура января и июля, количество осадков в год, количество туристов, посещающих город за год. Я брала из из открытых источников.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"Jan_temp={'Paris':4.9, 'Stockholm':-2.3, 'London':5.0, 'Berlin':0.7, 'Munich':-0.9, 'Oporto':10,\n       'Milan':1.1, 'Bratislava':-0.4, 'Vienna':0.3, 'Rome':8.1, 'Barcelona':8.9, 'Madrid':5.9,\n       'Dublin':5.4, 'Brussels':3.3, 'Zurich':0.4, 'Warsaw':-1.8, 'Budapest':-0.4, 'Copenhagen':1.3,\n       'Amsterdam':3.3, 'Lyon':2.6, 'Hamburg':1.3, 'Lisbon':11.4, 'Prague':-1.4, 'Oslo':-2.9,\n       'Helsinki':-5, 'Edinburgh':4, 'Geneva':1.8, 'Ljubljana':-0.5, 'Athens':10.2,\n       'Luxembourg':0.8, 'Krakow':-3.6}\nJul_temp={'Paris':19.4, 'Stockholm':17.9, 'London':18.7, 'Berlin':18.6, 'Munich':17.4, 'Oporto':19.5,\n       'Milan':1.1, 'Bratislava':-0.4, 'Vienna':0.3, 'Rome':8.1, 'Barcelona':8.9, 'Madrid':5.9,\n       'Dublin':15.3, 'Brussels':17.6, 'Zurich':18.4, 'Warsaw':18.2, 'Budapest':21.2, 'Copenhagen':17.2,\n       'Amsterdam':16.5, 'Lyon':21, 'Hamburg':17.3, 'Lisbon':22.4, 'Prague':18.7, 'Oslo':17.1,\n       'Helsinki':17, 'Edinburgh':14.8, 'Geneva':19.7, 'Ljubljana':20.4, 'Athens':27.9,\n       'Luxembourg':17.4, 'Krakow':-17.9}\ntourists={'Paris':19.0, 'Stockholm':2.7, 'London':19.5, 'Berlin':6.2, 'Munich':4.2, 'Oporto':2.8,\n       'Milan':6.6, 'Bratislava':1, 'Vienna':6.6, 'Rome':10.3, 'Barcelona':7.0, 'Madrid':5.6,\n       'Dublin':5.4, 'Brussels':4.2, 'Zurich':1.5, 'Warsaw':2.8, 'Budapest':4.0, 'Copenhagen':3.2,\n       'Amsterdam':8.8, 'Lyon':3.5, 'Hamburg':6.8, 'Lisbon':3.6, 'Prague': 9.1, 'Oslo':0.7,\n       'Helsinki':0.4, 'Edinburgh':4.4, 'Geneva':1.3, 'Ljubljana':0.4, 'Athens':0.24,\n       'Luxembourg':0.9, 'Krakow':8.1}\nrains={'Paris':6.37, 'Stockholm':5.27, 'London':6.21, 'Berlin':5.7, 'Munich':6.22, 'Oporto':11.78,\n       'Milan':10.13, 'Bratislava':6.94, 'Vienna':10.31, 'Rome':9.34, 'Barcelona':6.12, 'Madrid':4.5,\n       'Dublin':7.67, 'Brussels':7.82, 'Zurich':10.85, 'Warsaw':10.02, 'Budapest':5.64, 'Copenhagen':11.64,\n       'Amsterdam':8.05, 'Lyon':7.63, 'Hamburg':7.38, 'Lisbon':6.91, 'Prague': 4.86, 'Oslo':7.40,\n       'Helsinki':6.5, 'Edinburgh':7.06, 'Geneva':9.34, 'Ljubljana':12.90, 'Athens':3.97,\n       'Luxembourg':8.31, 'Krakow':6.78}\n\ndef january_temp_column(C):\n    for  city in Jan_temp:\n        if city==C:\n            return(Jan_temp[city])\n    \ndata['january_temp']=data['City'].apply(january_temp_column)\n\ndef july_temp_column(C):\n    for  city in Jul_temp:\n        if city==C:\n            return(Jul_temp[city])\n        \ndata['july_temp']=data['City'].apply(january_temp_column)\n\ndef tourist_flow_column(C):\n    for  city in tourists:\n        if city==C:\n            return(tourists[city])\n        \ndata['tourists_flow']=data['City'].apply(tourist_flow_column)\n\ndef rain_column(C):\n    for  city in tourists:\n        if city==C:\n            return(rains[city])\ndata['rains']=data['City'].apply(rain_column)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Посмотрим на другие источники данных о городах"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cities.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Здесь нас интересует, пожалуй, только население."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cities=df_cities.drop(['city_ascii','lat','lng','iso2','iso3','admin_name','capital','id'], axis='columns')\ndf_cities=df_cities.loc[df_cities.city.isin(cityes)]\ndf_cities","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Упс. Кажется, все европейские топонимы есть в США. Придётся очистить фрейм от этих значений."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cities=df_cities.loc[df_cities['country']!='United States']\nnew_city=set()\ndf_cities.city.apply(lambda x: new_city.add(x))\ncityes-new_city #Этих городов нет в списке. Их надо добавить.\ndf_cities.loc[2586]=['Krakow', 'Poland',779115]\ndf_cities.loc[2587]=['Oporto', 'Portugal',240000]\ndf_cities.loc[2588]=['Zurich', 'Germany',1300000]\ndf_cities.head(35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cost.sample(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Здесь интереснее: есть и индекс цен в ресторанах, и \"индекс бигмака\" и много чего еще. Но, к сожалению, для стран, а не городов. Для начала возьму такие параметры как Индекс цен в ресторанах, индекс бигмака и цену аренды."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cost=df_cost.drop(['Rank 2020','Cost of Living Index','Cost of Living Plus Rent Index','Groceries Index','Local Purchasing Power Index','Unnamed: 9'],axis='columns')\ndf_cost=df_cost.loc[df_cost.Country.isin(df_cities['country'])]\ndf_cost.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Мержим!\ndf_cities=df_cities.merge(df_cost, left_on='country',right_on= 'Country', how='inner')\ndf_cities=df_cities.drop(['country','Country'], axis='columns')\ndf_cities.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Мержим с большим фреймом\ndata=data.merge(df_cities, left_on='City',right_on= 'city', how='inner')\ndata=data.drop(['city','City','URL_TA','ID_TA'],axis='columns')\ndata.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### И один из моих любимых - [корреляция признаков](https://ru.wikipedia.org/wiki/Корреляция)\nНа этом графике уже сейчас вы сможете заметить, как признаки связаны между собой и с целевой переменной."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,10)\ndata_samp=data[['Restaurant_id','Cuisine Style','Ranking','Price Range','Number of Reviews','Reviews','Rating','sample','january_temp','july_temp','tourists_flow','rains','population','Rent Index','Restaurant Price Index','McMeal($)']]\nsns.heatmap(data_samp.drop(['sample'], axis=1).corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Вообще благодаря визуализации в этом датасете можно узнать много интересных фактов, например:\n* где больше Пицерий в Мадриде или Лондоне?\n* в каком городе кухня ресторанов более разнообразна?\n\nпридумайте свои вопрос и найдите на него ответ в данных)"},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing\nТеперь, для удобства и воспроизводимости кода, завернем всю обработку в одну большую функцию."},{"metadata":{"trusted":true},"cell_type":"code","source":"# на всякий случай, заново подгружаем данные\ndf_train = pd.read_csv(DATA_DIR+'/main_task.csv')\ndf_test = pd.read_csv(DATA_DIR+'/kaggle_task.csv')\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ndef preproc_data(df_input):\n    \n    df_output = df_input.copy()\n    \n    # ################### 1. Предобработка ############################################################## \n    # убираем не нужные для модели признаки\n    df_output.drop(['Restaurant_id','ID_TA','URL_TA'], axis = 1, inplace=True)\n    \n    \n    # ################### 2. NAN ############################################################## \n    # Далее заполняем пропуски, вы можете попробовать заполнением средним или средним по городу и тд...\n    df_output['Number of Reviews']=df_output['Number of Reviews'].fillna(0, inplace=True)\n    df_output['Cuisine Style']=df_output['Cuisine Style'].fillna(\"['European', 'Vegetarian Friendly']\")\n    df_output['Price Range']=data['Price Range'].fillna('$$ - $$$')\n    df_output['Reviews']=df_output['Reviews'].replace(\"[[], []]\", \"[['no_review'], ['01/01/2000']]\") #Потом пригодится\n    df_output['Reviews']=df_output['Reviews'].fillna(\"[['no_review'], ['01/01/2000']]\")\n    # ################### 3. Encoding ############################################################## \n    # для One-Hot Encoding в pandas есть готовая функция - get_dummies. Особенно радует параметр dummy_na\n    #Обрабатываем колонку разброса цен\n    def sign_to_range (x):\n        if x == \"$\":\n            return 1\n        elif x == \"$$ - $$$\":\n            return 2.5\n        else:\n            return 4\n    df_output['Price Range']=df_output['Price Range'].apply(sign_to_range)\n    \n    #Обрабатываем варианты кухонь\n    cousine_list=[]\n    df_output['Cuisine Style'].apply(lambda x: cousine_list.extend(x))\n    top=list(pd.Series(dict((Counter(cousine_list).most_common(70)))).index) #Очень странный костыль, чтобы сформировать множество самых частых кухонь.\n    others=(list(set(cousine_list)-set(top))) # формируем множество самых редких вариантов\n    def other(cuisines):\n        for cuisine in cuisines:\n            if cuisine in others:\n                return 'other'\n            else:\n                return cuisine\n\n    df_output['Cuisine Style']=df_output['Cuisine Style'].apply(other) # меняем наименее популярные варианты на \"other\"\n    dummi_cuisine=pd.get_dummies(df_output['Cuisine Style'].apply(pd.Series).stack()).sum(level=0) # Создаём сет dummie-переменных\n    df_output=pd.concat([df_output,dummi_cuisine], axis=1) # Соединяем с сетом \n    df_output['Cuisine Style']=df_output['Cuisine Style'].apply(lambda x: len(x))\n    \n    # Города. Feature Engineering будет здесь.\n    Jan_temp={'Paris':4.9, 'Stockholm':-2.3, 'London':5.0, 'Berlin':0.7, 'Munich':-0.9, 'Oporto':10,\n       'Milan':1.1, 'Bratislava':-0.4, 'Vienna':0.3, 'Rome':8.1, 'Barcelona':8.9, 'Madrid':5.9,\n       'Dublin':5.4, 'Brussels':3.3, 'Zurich':0.4, 'Warsaw':-1.8, 'Budapest':-0.4, 'Copenhagen':1.3,\n       'Amsterdam':3.3, 'Lyon':2.6, 'Hamburg':1.3, 'Lisbon':11.4, 'Prague':-1.4, 'Oslo':-2.9,\n       'Helsinki':-5, 'Edinburgh':4, 'Geneva':1.8, 'Ljubljana':-0.5, 'Athens':10.2,\n       'Luxembourg':0.8, 'Krakow':-3.6}\n    Jul_temp={'Paris':19.4, 'Stockholm':17.9, 'London':18.7, 'Berlin':18.6, 'Munich':17.4, 'Oporto':19.5,\n       'Milan':1.1, 'Bratislava':-0.4, 'Vienna':0.3, 'Rome':8.1, 'Barcelona':8.9, 'Madrid':5.9,\n       'Dublin':15.3, 'Brussels':17.6, 'Zurich':18.4, 'Warsaw':18.2, 'Budapest':21.2, 'Copenhagen':17.2,\n       'Amsterdam':16.5, 'Lyon':21, 'Hamburg':17.3, 'Lisbon':22.4, 'Prague':18.7, 'Oslo':17.1,\n       'Helsinki':17, 'Edinburgh':14.8, 'Geneva':19.7, 'Ljubljana':20.4, 'Athens':27.9,\n       'Luxembourg':17.4, 'Krakow':-17.9}\n    tourists={'Paris':19.0, 'Stockholm':2.7, 'London':19.5, 'Berlin':6.2, 'Munich':4.2, 'Oporto':2.8,\n       'Milan':6.6, 'Bratislava':1, 'Vienna':6.6, 'Rome':10.3, 'Barcelona':7.0, 'Madrid':5.6,\n       'Dublin':5.4, 'Brussels':4.2, 'Zurich':1.5, 'Warsaw':2.8, 'Budapest':4.0, 'Copenhagen':3.2,\n       'Amsterdam':8.8, 'Lyon':3.5, 'Hamburg':6.8, 'Lisbon':3.6, 'Prague': 9.1, 'Oslo':0.7,\n       'Helsinki':0.4, 'Edinburgh':4.4, 'Geneva':1.3, 'Ljubljana':0.4, 'Athens':0.24,\n       'Luxembourg':0.9, 'Krakow':8.1}\n    rains={'Paris':6.37, 'Stockholm':5.27, 'London':6.21, 'Berlin':5.7, 'Munich':6.22, 'Oporto':11.78,\n       'Milan':10.13, 'Bratislava':6.94, 'Vienna':10.31, 'Rome':9.34, 'Barcelona':6.12, 'Madrid':4.5,\n       'Dublin':7.67, 'Brussels':7.82, 'Zurich':10.85, 'Warsaw':10.02, 'Budapest':5.64, 'Copenhagen':11.64,\n       'Amsterdam':8.05, 'Lyon':7.63, 'Hamburg':7.38, 'Lisbon':6.91, 'Prague': 4.86, 'Oslo':7.40,\n       'Helsinki':6.5, 'Edinburgh':7.06, 'Geneva':9.34, 'Ljubljana':12.90, 'Athens':3.97,\n       'Luxembourg':8.31, 'Krakow':6.78}\n\n    def january_temp_column(C):\n        for  city in Jan_temp:\n            if city==C:\n                return(Jan_temp[city])\n    \n    df_output['january_temp']=df_output['City'].apply(january_temp_column)\n\n    def july_temp_column(C):\n        for  city in Jul_temp:\n            if city==C:\n                return(Jul_temp[city])\n        \n    df_output['july_temp']=df_output['City'].apply(january_temp_column)\n\n    def tourist_flow_column(C):\n        for  city in tourists:\n            if city==C:\n                return(tourists[city])\n        \n    df_output['tourists_flow']=df_output['City'].apply(tourist_flow_column)\n\n    def rain_column(C):\n        for  city in tourists:\n            if city==C:\n                return(rains[city])\n    df_output['rains']=df_output['City'].apply(rain_column) \n    df_output=df_output.merge(df_cities, left_on='City',right_on= 'city', how='inner')\n   \n    # Отзывы. самое сложное.\n    word_in_review=word_in_review={'Good':['gusto','nya','bellisimo','dequate','pleasantly','wunderfull','delucious','excellient','picturesque','👍👍','good','great','best','excellent','nice','delicious','lovely','tasty','amazing','fantastic','perfect','wonderful','pleasant','cozy','awesome','yummy','fabulous','cool','fine','brilliant','enjoyable','good!','outstanding','delicious!','charming','affordable','delightful','comfortable', '+','gorgeous','👏👏👏👏👏'],\n                                   'Bad':['weak','only?','grubby','awseome','wash','ameri','weakest','filthy','disasterous','becareful','miserable','foo','bad','poor','stop','worst','disappointing','terrible','overpriced','rude','disappointed','horrible','mediocre','unfriendly','worse','dirty','disappointment','fo','waste','satisfying']}\n    def Reviews_reader(line):\n        line=line[2:-2]\n        line=line.split('], [')\n        line[0]=line[0].split(', ')\n        line[1]=line[1].split(', ')\n   \n        for rev in line[0]:\n            rev=(rev[1:-1]).lower()\n            rev=rev.replace('!','')\n            rev=rev.replace('.','')\n            rev=rev.split(' ')\n            \n        return(line)\n    df_output['Reviews']=df_output['Reviews'].apply(Reviews_reader)\n\n    def Reviews_counter (line): # вводим функцию для перевода отзывов в численное значение\n        count=0\n        for word in line[0]:\n            if word in word_in_review['Good']:\n                count+=1\n            elif word in word_in_review['Bad']:\n                count-=1\n        return(count)\n    df_output['Reviews']=df_output['Reviews'].apply(Reviews_counter)\n    \n    # ################### 5. Clean #################################################### \n    # убираем признаки которые еще не успели обработать, \n    # модель на признаках с dtypes \"object\" обучаться не будет, просто выберим их и удалим\n    object_columns = [s for s in df_output.columns if df_output[s].dtypes == 'object']\n    df_output.drop(object_columns, axis = 1, inplace=True)\n    names = df_output.columns.values\n    scaler=MinMaxScaler()\n    df_output = pd.DataFrame(scaler.fit_transform(df_output))\n    df_output.columns=names\n    return  df_output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">По хорошему, можно было бы перевести эту большую функцию в класс и разбить на подфункции (согласно ООП). "},{"metadata":{},"cell_type":"markdown","source":"#### Запускаем и проверяем что получилось"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc = preproc_data(data)\ndf_preproc.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_preproc.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Rating.values            # наш таргет\nX = train_data.drop(['Rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Перед тем как отправлять наши данные на обучение, разделим данные на еще один тест и трейн, для валидации. \nЭто поможет нам проверить, как хорошо наша модель работает, до отправки submissiona на kaggle.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n# выделим 20% данных на валидацию (параметр test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# проверяем\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model \nСам ML"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Импортируем необходимые библиотеки:\nfrom sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\nfrom sklearn import metrics # инструменты для оценки точности модели","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Обучаем модель на тестовом наборе данных\nmodel.fit(X_train, y_train)\n\n# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n# Предсказанные значения записываем в переменную y_pred\ny_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission\nЕсли все устраевает - готовим Submission на кагл"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = test_data.drop(['Rating'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_submission = model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(predict_submission)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission['Rating'] = predict_submission[:10000]\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}