{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import keras\nfrom keras import optimizers\nfrom keras.preprocessing import image\nfrom keras.engine import Layer\nfrom keras.layers import Conv2D, Conv3D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate\nfrom keras.layers import Activation, Dense, Dropout, Flatten\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import TensorBoard\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\nfrom skimage.transform import resize, rescale\nfrom skimage.io import imsave, imread\nfrom time import time\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nfrom PIL import Image, ImageFile\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/celeba-dataset/list_eval_partition.csv')\ndata['partition'].value_counts().sort_index()\ndata_dir = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba'\ndata['train_file'] = data.image_id.apply(lambda x: data_dir + '/{0}'.format(x))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading images\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n\nplt.figure(figsize=(15, 4))\nfor idx, (_, entry) in enumerate(data.sample(n=5).iterrows()):\n    img = imread(entry.train_file)\n    print(img.shape)\n    plt.subplot(1, 5, idx+1)\n    plt.imshow(imread(entry.train_file))\n    plt.axis('off')\n    plt.title(entry.image_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## constants\nnum_train_samples = 95\nnum_val_samples = 10\nimg_height = 224\nimg_width = 224\nbatch_size = 4\nepochs = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Custom data generator\n\nclass colorizationGenerator():\n  \n  def __init__(self, path, files, batch_size):\n    self.path = path\n    self.files = files\n    self.batch_size = batch_size\n\n  def get_input(self, path, filename):\n    img = imread(os.path.join(path,filename))\n    return(img)\n\n  def get_output(self, path, filename):\n    img = imread(os.path.join(path,filename))\n    img = resize(img,(224,224,3))\n    labImage = rgb2lab(img)\n    abch = labImage[:,:,1:]/128.0\n    return np.array(abch)\n\n  def preprocess_input(self, img):\n    img = resize(img,(224,224,3))\n    labImage = rgb2lab(img)\n    lch = labImage[:,:,0]/100.0\n    l3ch = gray2rgb(lch)\n    return np.array(l3ch)\n\n  def image_generator(self):\n      while True:\n            # Select files (paths/indices) for the batch\n            batch_paths  = np.random.choice(a    = self.files, \n                                            size = self.batch_size)\n            batch_input  = []\n            batch_output = [] \n            \n            # Read in each input, perform preprocessing and get labels\n            for input_path in batch_paths:\n                currinput = self.get_input(self.path, input_path)\n                output = self.get_output(self.path, input_path)\n              \n                currinput = self.preprocess_input(currinput)\n                batch_input += [currinput]\n                batch_output += [output]\n                \n            # Return a tuple of (input, output) to feed the network\n            batch_x = np.array(batch_input)\n            batch_y = np.array(batch_output)\n          \n            yield(batch_x, batch_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## The model\nmodelInput = Input(shape=(img_height, img_width, 3))\nvggModel = keras.applications.vgg16.VGG16(weights='imagenet', include_top=False, input_tensor=modelInput)\n\noutput = vggModel.layers[-1].output\nvgg16TruncModel = Model(vggModel.input, output)\nfor layer in vgg16TruncModel.layers:\n  layer.trainable=False\n\ndecoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(vgg16TruncModel.output)\n# decoder_output = BatchNormalization()(decoder_output)\ndecoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(decoder_output)\n# decoder_output = BatchNormalization()(decoder_output)\ndecoder_output = UpSampling2D((2, 2))(decoder_output)\ndecoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n# decoder_output = BatchNormalization()(decoder_output)\ndecoder_output = UpSampling2D((2, 2))(decoder_output)\ndecoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n# decoder_output = BatchNormalization()(decoder_output)\ndecoder_output = UpSampling2D((2, 2))(decoder_output)\ndecoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n# decoder_output = BatchNormalization()(decoder_output)\ndecoder_output = UpSampling2D((2, 2))(decoder_output)\ndecoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\ndecoder_output = UpSampling2D((2, 2))(decoder_output)\nmodel = Model(inputs=modelInput, outputs=decoder_output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='mse' , metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataGenerator = colorizationGenerator(data_dir,\n                                      os.listdir(data_dir), batch_size)\n\nfrom keras.callbacks import ModelCheckpoint\n\ncheckpoint_callback = ModelCheckpoint('/best_model_final.h5', monitor='loss', verbose=1, save_best_only=True, mode='min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_img = imread(data_dir + '/000001.jpg')\ntest_img = resize(test_img,(224,224,3))\nlab = rgb2lab(test_img)\nprint(lab.shape)\nl = lab[:,:,0]\na = lab[:,:,1]\nb = lab[:,:,2]\nprint(np.max(l))\nprint(np.max(a))\nprint(np.max(b))\nplt.imshow(test_img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"images = next(dataGenerator.image_generator())\nprint(images[1][0].shape)\nprint(np.max(images[0][0][0]))\nprint(np.max(images[1][0][:,:,0]))\nprint(np.max(images[1][0][:,:,1]))\ncur = np.zeros((224, 224, 3))\ncur[:,:,0] = images[0][0][:,:,0]*100.0\ncur[:,:,1:] = images[1][0]*128.0\noutput = lab2rgb(cur)\nplt.imshow(output)\nprint(np.max(cur[:,:,0]))\nprint(np.max(cur[:,:,1]))\nprint(np.max(cur[:,:,2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(\n    dataGenerator.image_generator(),\n    steps_per_epoch=num_train_samples // batch_size,\n    validation_data=valGenerator.image_generator(),\n    validation_steps=1,\n    epochs=epochs,\n    callbacks=[checkpoint_callback]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nmse = history.history['loss']\n\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, mse, label='Training MSE')\nplt.plot(history.history['val_loss'], label='Validation MSE')\nplt.legend(loc='upper right')\nplt.title('Training MSE')\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}