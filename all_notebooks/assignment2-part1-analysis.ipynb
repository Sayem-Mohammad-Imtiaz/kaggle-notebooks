{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade scikit-learn==0.22\n!pip install mlrose\n!pip install cairocffi\n!pip install CairoSVG\n!pip install pygal\n!pip install sklearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import logging\nimport pygal\nimport mlrose\nimport mlrose.decay\nimport numpy as np\nimport random\nimport time\nimport itertools\nimport sklearn.metrics\nimport sklearn.preprocessing\nimport sklearn.datasets\n\nfrom collections import defaultdict\n\nimport argparse\nimport inspect\nimport logging\nimport sys\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport glob\n\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def partition(datasets, probs):\n    \"\"\"Splits data into two partitions.\n    \n    Args:\n        datasets: Any number of datasets of the same size. These will be\n            partitioned together, so if the first element of one dataset goes\n            to the right then the first element of all the other datasets will\n            do the same.\n        probs: The probability that the data is split into each of the buckets.\n        \n    Returns:\n        A number of buckets containing some portion of the datasets according\n        to probs.\n    \"\"\"\n    buckets = [[] for _ in probs]\n    for item in zip(*datasets):\n        coin = random.uniform(0, 1)\n        ptotal = 0\n        for bucket_index, pcurr in enumerate(probs):\n            ptotal += pcurr\n            if coin < ptotal:\n                break\n        else:\n            continue\n        buckets[bucket_index].append(item)\n    return [list(zip(*d)) for d in buckets]\n\n\nclass MedianBinarizer:\n    \"\"\"Bins data as: <med = 0 and >med = 1.\"\"\"\n    def __init__(self):\n        self._binarizer = None\n\n    def fit(self, x):\n        median = np.median(x)\n        self._binarizer = sklearn.preprocessing.Binarizer(median)\\\n            .fit([[i] for i in x])\n        return self\n\n    def transform(self, x):\n        if self._binarizer is None:\n            raise TypeError(\"fit has not been called\")\n        return self._binarizer.transform([[i] for i in x])\n\n\ndef make_boolean(data):\n    \"\"\"Transforms labels into two classes (>avg, <avg).\"\"\"\n    samples, labels = data\n    average = sum(labels) / len(labels)\n    labels = [1 if label > average else -1 for label in labels]\n    return samples, labels\n\n\nPLOTFUNCS = {}\n\n\ndef register_plotfunc(prefix, x_title, plotfunc):\n    PLOTFUNCS[\"%s_%s\" % (prefix, x_title)] = (plotfunc, x_title)\n\ndef get_plotfuncs():\n    return list(PLOTFUNCS.keys())\n\n\ndef get_plotfunc(plotfunc):\n    return PLOTFUNCS[plotfunc][0]\n\n\ndef get_xtitle(plotfunc):\n    return PLOTFUNCS[plotfunc][1]\n\n\ndef run_timer():\n    def backend():\n        curr = None\n        prev = time.time()\n        while True:\n            curr = time.time()\n            yield curr - prev\n            prev = curr\n    timer = backend()\n    next(timer)\n    return timer\n\nclass Plotter:\n    def __init__(self, xtitle):\n        self.lerr = []\n        self.terr = []\n        self.ftimes = []\n        self.stimes = []\n        self.xtitle = xtitle\n        \n    @property\n    def learning_plot(self):\n        plot = pygal.XY(\n            #stroke=False,\n            x_title=self.xtitle,\n            y_title=\"error\")\n        plot.add(\"training\", self.lerr)\n        plot.add(\"testing\", self.terr)\n        return plot\n\n    @property\n    def fit_timing_plot(self):\n        plot = pygal.XY(\n            #stroke=False,\n            show_legend=False,\n            x_title=self.xtitle,\n            y_title=\"fit_time\")\n        plot.add(\"\", self.ftimes)\n        return plot\n\n    @property\n    def score_timing_plot(self):\n        plot = pygal.XY(\n            #stroke=False,\n            show_legend=False,\n            x_title=self.xtitle,\n            y_title=\"score_time\")\n        plot.add(\"\", self.stimes)\n        return plot\n    \n    def plot(self, classifier, xval, ldata, tdata):\n        logging.info(\"plotting data point %s...\", xval)\n        timer = run_timer()\n        classifier.fit(*ldata)\n        self.ftimes.append((xval, next(timer)))\n\n        lx, ly = ldata\n        lscore = 1 - sklearn.metrics.accuracy_score(ly, classifier.predict(lx))\n        self.lerr.append((xval, lscore))\n        tx, ty = tdata\n        tscore = 1 - sklearn.metrics.accuracy_score(ty, classifier.predict(tx))\n        self.terr.append((xval, tscore))\n        self.stimes.append((xval, next(timer)))\n\n    def write(self, outdir, name):\n        def write_plot(plot, suffix=\"\"):\n            plot.title = (name + suffix).replace('_',' ')\n            #plot.render_to_file(outdir + \"%s%s.svg\" % (name, suffix))\n            plot.render_to_png(outdir + \"%s%s.png\" % (name, suffix))\n\n        write_plot(self.learning_plot)\n        write_plot(self.fit_timing_plot, \"_ftime\")\n        write_plot(self.score_timing_plot, \"_stime\")\n        \nprint ('common defines done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ITER_MIN = 500\nITER_MAX = 10_000\nITER_STEP = 500\n\n\ndef iterfunc(algorithm, factor=1, nodes=None):\n    def plotfunc(ldata, tdata, plotter):\n        for max_iter in range(\n                int(ITER_MIN / factor),\n                int(ITER_MAX / factor + 1),\n                int(ITER_STEP / factor)):\n            classifier = mlrose.NeuralNetwork(\n                nodes or [10],\n                algorithm=algorithm,\n                max_iters=max_iter)\n            plotter.plot(classifier, max_iter, ldata, tdata)\n    return plotfunc\n\n\nNODE_MIN = 1\nNODE_MAX = 20\nNODE_STEP = 1\n\ndef nodefunc(algorithm, factor=1):\n    def plotfunc(ldata, tdata, plotter):\n        for nodes in range(NODE_MIN, NODE_MAX + 1, NODE_STEP):\n            classifier = mlrose.NeuralNetwork(\n                [nodes],\n                algorithm=algorithm,\n                max_iters=4000 / factor)\n            plotter.plot(classifier, nodes, ldata, tdata)\n    return plotfunc\n\n\nDELTA_MIN = .02\nDELTA_MAX = .5\nDELTA_STEP = .02\n\n\ndef deltafunc(algorithm):\n    def plotfunc(ldata, tdata, plotter):\n        delta = DELTA_MIN\n        while delta < DELTA_MAX + 1e-8:\n            classifier = mlrose.NeuralNetwork(\n                [10],\n                algorithm=algorithm,\n                max_iters=4000,\n                learning_rate=delta)\n            plotter.plot(classifier, delta, ldata, tdata)\n            delta += DELTA_STEP\n    return plotfunc\n\nTEMP_MIN = .5\nTEMP_MAX = 10\nTEMP_STEP = .5\n\ndef tempfunc():\n    def plotfunc(ldata, tdata, plotter):\n        temp = TEMP_MIN\n        while temp < TEMP_MAX + 1e-8:\n            classifier = mlrose.NeuralNetwork(\n                    [10],\n                    algorithm=\"simulated_annealing\",\n                    max_iters=2000,\n                    schedule=mlrose.decay.GeomDecay(temp))\n            plotter.plot(classifier, temp, ldata, tdata)\n            temp += TEMP_STEP\n    return plotfunc\n\n\nhillclimb_iter = iterfunc(\"random_hill_climb\")\nhillclimb_node = nodefunc(\"random_hill_climb\")\nannealing_iter = iterfunc(\"simulated_annealing\")\nannealing_node = nodefunc(\"simulated_annealing\")\ngenetic_iter = iterfunc(\"genetic_alg\", 50)\ngenetic_node = nodefunc(\"genetic_alg\", 50)\n\nannealing_iter_2 = iterfunc(\"simulated_annealing\", nodes=[2])\nannealing_iter_10 = iterfunc(\"simulated_annealing\", nodes=[10])\nannealing_iter_20 = iterfunc(\"simulated_annealing\", nodes=[20])\n\nhillclimb_delta = deltafunc(\"random_hill_climb\")\nannealing_delta = deltafunc(\"random_hill_climb\")\n\nannealing_temp = tempfunc()\n    \nprint ('neural net defines done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = None\ndf_x = None\ndf_y = None\nto_encode = []\n\ndf_asteroid = pd.read_csv('../input/asteroid-dataset/dataset.csv')\n    \ninputs = df_asteroid\n\ninputs.pha.replace(('Y', 'N'), (1, 0), inplace = True)\ninputs['pha'] = inputs['pha'].fillna(0)\ninputs['pha'] = inputs.pha.astype(int)\n\ninputs.neo.replace(('Y', 'N'), (1, 0), inplace = True)\ninputs['neo'] = inputs['neo'].fillna(0)\ninputs['neo'] = inputs.neo.astype(int)\n\ninputs = inputs.drop(['id', 'spkid', 'full_name', 'name', 'prefix', 'orbit_id', 'pdes', 'equinox', 'diameter', 'albedo', \n                      'diameter_sigma'], axis='columns')\ninputs.dropna(inplace=True)\ntarget = inputs['class']\n\ndf = inputs\ninputs1 = inputs.drop(['class'], axis='columns')\ndf_x = inputs1.columns\ndf_y = 'class'\nto_encode = inputs.columns\n        \nprint(df)\nle = sklearn.preprocessing.LabelEncoder\nencoderDict = defaultdict(le)\nfor column in to_encode:\n    print('Encoding: ' + column)\n    df[column] = df[column].dropna()\n    df = df[df[column].notnull()]\n    df[column] = encoderDict[column].fit_transform(df[column])\n    print(encoderDict[column].classes_)\n\nprint(df_x)\nprint(df_y)\n\nprint(df.columns)\nprint(df.dtypes)\n\ndf = df.head(5000)\n\ndf = df.dropna()  \n\ndf = df.sample(frac=1).reset_index(drop=True)\nprint('Dataset size: ' + str(df.size))\nprint('Features: ' + str(df_x))\nprint('Target Decision: ' + df_y)\n\nx1 = df.loc[:, df_x]\ny1 = df.loc[:, df_y]\nprint('ready with x1 and y1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x1 = x1.to_numpy()\ny1 = y1.to_numpy()\nprint('dataframe to array')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(lx1, ly1), (tx1, ty1) = partition((x1, y1), (.8, .2))\nscaler = sklearn.preprocessing.StandardScaler().fit(lx1)\nlx1 = scaler.transform(lx1)\ntx1 = scaler.transform(tx1)\n\nbinarizer = MedianBinarizer().fit(ly1)\nly1 = binarizer.transform(ly1)\nty1 = binarizer.transform(ty1)\nprint('ready with ly1 and ty1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.seterr(over=\"ignore\")\nregister_plotfunc(\"neuralnet_hillclimb\", \"max_iter\", hillclimb_iter)\nregister_plotfunc(\"neuralnet_hillclimb\", \"hidden_nodes\", hillclimb_node)\nregister_plotfunc(\"neuralnet_annealing\", \"max_iter\", annealing_iter)\nregister_plotfunc(\"neuralnet_annealing\", \"hidden_nodes\", annealing_node)\nregister_plotfunc(\"neuralnet_genetic\", \"max_iter\", genetic_iter)\nregister_plotfunc(\"neuralnet_genetic\", \"hidden_nodes\", genetic_node)\nregister_plotfunc(\n    \"neuralnet_annealing_2node\", \"max_iter\", annealing_iter_2)\nregister_plotfunc(\n    \"neuralnet_annealing_10node\", \"max_iter\", annealing_iter_10)\nregister_plotfunc(\n    \"neuralnet_annealing_20node\", \"max_iter\", annealing_iter_20)\nregister_plotfunc(\"neuralnet_hillclimb\", \"delta\", hillclimb_delta)\nregister_plotfunc(\"neuralnet_annealing\", \"delta\", annealing_delta)\nregister_plotfunc(\"neuralnet_annealing\", \"init_temp\", annealing_temp)\nprint('register complete')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ldata = (lx1, ly1)\ntdata = (tx1, ty1)\nprint('ready for part1 and part2 from here')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotfuncs = ['neuralnet_nodes','hillclimb_max_iter', 'hillclimb_hidden_nodes', 'annealing_max_iter', 'annealing_hidden_nodes', \n             'genetic_max_iter', 'genetic_hidden_nodes', 'annealing_2node_max_iter', 'annealing_10node_max_iter', \n             'annealing_20node_max_iter', 'hillclimb_delta', 'annealing_delta', 'annealing_init_temp']\nprint('ready with plotfuncs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if \"neuralnet_nodes\" in plotfuncs:\n    plotfuncs.remove(\"neuralnet_nodes\")\nprint('check for neuralnodes for the next step')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"funcs = [\n    get_plotfunc(\"neuralnet_annealing_2node_max_iter\"),\n    get_plotfunc(\"neuralnet_annealing_10node_max_iter\"),\n    get_plotfunc(\"neuralnet_annealing_20node_max_iter\")]\ntitles = [\"2 nodes\", \"10 nodes\", \"20 nodes\"]\nplots = [Plotter(\"max_iter\") for _ in range(3)]\n\nfor func, plot in zip(funcs, plots):\n   func(ldata, tdata, plot)\nsplot = pygal.XY(x_title=\"max_iter\", y_title=\"score\")\ntplot = pygal.XY(x_title=\"max_iter\", y_title=\"fit_time\")\nfor plotter, title in zip(plots, titles):\n    print('plotting for ' + title)\n    splot.add(title + \" training\", plotter.lerr)\n    splot.add(title + \" testing\", plotter.terr)\n    tplot.add(title, plotter.ftimes)\nsplot.title = \"neuralnet nodes\"\ntplot.title = \"neuralnet nodes ftime\"\n#splot.render_to_file(\"%sneuralnet_nodes.svg\" % (\"\",))\nsplot.render_to_png(\"%sneuralnet_nodes.png\" % (\"\",))\n#tplot.render_to_file(\"%sneuralnet_nodes_ftime.svg\" % (\"\",))\ntplot.render_to_png(\"%sneuralnet_nodes_ftime.png\" % (\"\",))\nprint('done with splot and tplot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for plotfunc in plotfuncs:\n    plot = get_plotfunc('neuralnet_' + plotfunc)\n    xtitle = get_xtitle('neuralnet_' + plotfunc)\n    \n    print(\"plotting \" + plotfunc + \"...\")\n    plotter = Plotter(xtitle)\n    try:\n        plot(ldata, tdata, plotter)\n    except KeyboardInterrupt:\n        print(\"caught keyboard interrupt. plotting and continuing.\")\n        plotter.write(plotfunc, plotfunc)\n        continue\n    except Exception:\n        print(\"error in \" + plotfunc + \". continuing...\")\n        continue\n    plotter.write('', plotfunc)\nprint ('done with plotting')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport glob\n\nfor image_path in glob.glob(\"/kaggle/working/*.png\"):\n    img = mpimg.imread(image_path)\n    plt.ion()\n    plt.figure()\n    plt.axis('off') \n    plt.imshow(img)\n    plt.show()\n    plt.close()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}