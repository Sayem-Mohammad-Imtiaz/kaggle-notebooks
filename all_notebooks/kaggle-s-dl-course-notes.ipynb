{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Kaggle's Deep Learning Course Notes**\nIn this notebook I'm going to summarize the course topics that I found most important, so I can refer to it later, and also for anyone interested in reading about Deep Learning.\n\nI will use the popular Red Wine dataset to exemplify the concepts covered in the course. The dataset can be seen as either a classification or a regression problem where we want to determine the quality of wines based on their physicochemical properties.","metadata":{}},{"cell_type":"markdown","source":"## **Setup**","metadata":{}},{"cell_type":"code","source":"# libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom learntools.deep_learning_intro.dltools import animate_sgd\nfrom tensorflow.keras import callbacks\n\n# plotting\nplt.style.use('seaborn-whitegrid')\n\n# matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('animation', html='html5')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-07-15T21:31:52.599707Z","iopub.execute_input":"2021-07-15T21:31:52.600123Z","iopub.status.idle":"2021-07-15T21:31:58.654099Z","shell.execute_reply.started":"2021-07-15T21:31:52.600013Z","shell.execute_reply":"2021-07-15T21:31:58.653322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Dataset**","metadata":{}},{"cell_type":"code","source":"red_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')\nred_wine.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:31:58.655239Z","iopub.execute_input":"2021-07-15T21:31:58.655613Z","iopub.status.idle":"2021-07-15T21:31:58.707208Z","shell.execute_reply.started":"2021-07-15T21:31:58.655588Z","shell.execute_reply":"2021-07-15T21:31:58.706283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"red_wine.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:31:58.708865Z","iopub.execute_input":"2021-07-15T21:31:58.709129Z","iopub.status.idle":"2021-07-15T21:31:58.729981Z","shell.execute_reply.started":"2021-07-15T21:31:58.709103Z","shell.execute_reply":"2021-07-15T21:31:58.729057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data Manipulation**","metadata":{}},{"cell_type":"code","source":"X = red_wine.copy()\ny = X.pop('quality')\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.2, random_state=42)\n\n# neural networks tend to perform best when their inputs are on a common scale\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_valid = scaler.transform(X_valid)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:31:58.731491Z","iopub.execute_input":"2021-07-15T21:31:58.731857Z","iopub.status.idle":"2021-07-15T21:31:58.748932Z","shell.execute_reply.started":"2021-07-15T21:31:58.73182Z","shell.execute_reply":"2021-07-15T21:31:58.747947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model Creation**","metadata":{}},{"cell_type":"markdown","source":"The easiest way to create a model in Keras is through *keras.Sequential*, which creates a neural network as a stack of layers.","metadata":{}},{"cell_type":"code","source":"# number of features\ninput_shape = [11]\n\n# setup model\nmodel = keras.Sequential([\n    # the hidden ReLU layers (hidden because we never see their outputs)\n    layers.Dense(units=512, activation='relu', input_shape=input_shape),\n    layers.Dense(units=512, activation='relu'),\n    # the linear output layer and the number of units (neurons), in this case we have just one output, the quality of the wine\n    layers.Dense(units=1)\n])","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2021-07-15T21:31:58.75026Z","iopub.execute_input":"2021-07-15T21:31:58.750721Z","iopub.status.idle":"2021-07-15T21:31:58.84396Z","shell.execute_reply.started":"2021-07-15T21:31:58.750687Z","shell.execute_reply":"2021-07-15T21:31:58.843013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Internally, Keras represents the weights of a neural network with **tensors**. Tensors are basically TensorFlow's version of a Numpy array with a few differences that make them better suited to deep learning. One of the most important is that tensors are compatible with [GPU](https://www.kaggle.com/docs/efficient-gpu-usage) and [TPU](https://www.kaggle.com/docs/tpu)) accelerators. TPUs, in fact, are designed specifically for tensor computations.\n\nThe usual way of attaching an activation function to a `Dense` layer is to include it as part of the definition with the `activation` argument. Sometimes though you'll want to put some other layer between the `Dense` layer and its activation function. In this case, we can define the activation in its own `Activation` layer, like so:\n\n```\nlayers.Dense(units=512),\nlayers.Activation('relu')\n```\n\nThis is completely equivalent to the ordinary way: `layers.Dense(units=512, activation='relu')`.\n\nThere is a whole family of variants of the `'relu'` activation -- `'elu'`, `'selu'`, and `'swish'`, among others. Sometimes one activation will perform better than another on a given task, so you could consider experimenting with activations as you develop a model. The ReLU activation tends to do well on most problems, so it's a good one to start with. Check out the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/activations) for other activation functions.","metadata":{}},{"cell_type":"markdown","source":"A model's weights are kept in its `weights` attribute as a list of tensors.","metadata":{}},{"cell_type":"code","source":"model.weights","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"scrolled":true,"execution":{"iopub.status.busy":"2021-07-15T21:31:58.845002Z","iopub.execute_input":"2021-07-15T21:31:58.845306Z","iopub.status.idle":"2021-07-15T21:31:58.881198Z","shell.execute_reply.started":"2021-07-15T21:31:58.845277Z","shell.execute_reply":"2021-07-15T21:31:58.880486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keras represents weights as tensors, but also uses tensors to represent data. When you set the `input_shape` argument, you are telling Keras the dimensions of the array it should expect for each example in the training data. Setting `input_shape=[3]` would create a network accepting vectors of length 3, like `[0.2, 0.4, 0.6]`.","metadata":{}},{"cell_type":"markdown","source":"## **Untrained Model's Random Weights**","metadata":{}},{"cell_type":"markdown","source":"Regression problems are like \"curve-fitting\" problems: we're trying to find a curve that best fits the data. Let's take a look at the \"curve\" produced by an untrained linear model.\n \nBefore training, a model's weights are set randomly. Run the cell below a few times to see the different lines produced with a random initialization.","metadata":{}},{"cell_type":"code","source":"example_model = keras.Sequential([\n    layers.Dense(1, input_shape=[1])\n])\n\nx_var = tf.linspace(-1.0, 1.0, 100)\ny_var = example_model.predict(x_var)\n\nplt.figure(dpi=100)\nplt.plot(x_var, y_var, 'k')\nplt.xlim(-1, 1)\nplt.ylim(-1, 1)\nplt.xlabel(\"Input x\")\nplt.ylabel(\"Target y\")\nw, b = example_model.weights\nplt.title(\"Weight: {:0.2f}\\nBias: {:0.2f}\".format(w[0][0], b[0]))\nplt.show()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2021-07-15T21:31:58.882232Z","iopub.execute_input":"2021-07-15T21:31:58.882602Z","iopub.status.idle":"2021-07-15T21:31:59.374381Z","shell.execute_reply.started":"2021-07-15T21:31:58.882575Z","shell.execute_reply":"2021-07-15T21:31:59.373479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Compile Method**","metadata":{}},{"cell_type":"markdown","source":"To define the loss and optimizer we'll use the model's `compile` method.\n\nAdam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning (it is \"self tuning\", in a sense). Adam is a great general-purpose optimizer.","metadata":{}},{"cell_type":"code","source":"model.compile(\n    optimizer=\"adam\",\n    loss=\"mae\")","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:31:59.376271Z","iopub.execute_input":"2021-07-15T21:31:59.376534Z","iopub.status.idle":"2021-07-15T21:31:59.550177Z","shell.execute_reply.started":"2021-07-15T21:31:59.37651Z","shell.execute_reply":"2021-07-15T21:31:59.549072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Training the Model**","metadata":{}},{"cell_type":"markdown","source":"Once you've defined the model and compiled it with a loss and optimizer you're ready for training.","metadata":{}},{"cell_type":"code","source":"history = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=32, # works best with powers of 2\n    epochs=50,\n    verbose=0 # suppress output since we'll plot the curves\n)","metadata":{"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2021-07-15T21:31:59.552126Z","iopub.execute_input":"2021-07-15T21:31:59.552525Z","iopub.status.idle":"2021-07-15T21:32:08.041408Z","shell.execute_reply.started":"2021-07-15T21:31:59.552481Z","shell.execute_reply":"2021-07-15T21:32:08.040348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.","metadata":{}},{"cell_type":"markdown","source":"The next step is to look at the loss curves and evaluate the training. The fit method keeps a record of the loss produced during training in a History object. When we train a model we can plot the loss on the training set epoch by epoch. We can also plot the validation data. These plots are called the learning curves. To train deep learning models effectively, we need to be able to interpret them.","metadata":{}},{"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:32:08.042815Z","iopub.execute_input":"2021-07-15T21:32:08.043128Z","iopub.status.idle":"2021-07-15T21:32:08.280838Z","shell.execute_reply.started":"2021-07-15T21:32:08.043097Z","shell.execute_reply":"2021-07-15T21:32:08.280151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the learning rate and the batch size, you have some control over:\n- How long it takes to train a model\n- How noisy the learning curves are\n- How small the loss becomes","metadata":{}},{"cell_type":"markdown","source":"To get a better understanding of these two parameters, we'll look at the linear model, our simplest neural network. Having only a single weight and a bias, it's easier to see what effect a change of parameter has.","metadata":{}},{"cell_type":"code","source":"# experiment with different values for the learning rate, batch size, and number of examples\nanimate_sgd(\n    learning_rate=0.1,\n    batch_size=32,\n    num_examples=1600,\n    # can also change these\n    steps=30, # total training steps (batches seen)\n    true_w=3.0, # the slope of the data\n    true_b=2.0, # the bias of the data\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:32:08.281706Z","iopub.execute_input":"2021-07-15T21:32:08.282047Z","iopub.status.idle":"2021-07-15T21:32:15.38137Z","shell.execute_reply.started":"2021-07-15T21:32:08.28202Z","shell.execute_reply":"2021-07-15T21:32:15.380307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Smaller batch sizes give noisier weight updates and loss curves. This is because each batch is a small sample of data and smaller samples tend to give noisier estimates. Smaller batches can have an \"averaging\" effect though which can be beneficial.\n\nSmaller learning rates make the updates smaller and the training takes longer to converge. Large learning rates can speed up training, but don't \"settle in\" to a minimum as well. When the learning rate is too large, the training can fail completely.","metadata":{}},{"cell_type":"markdown","source":"## **Overfitting vs Underfitting**","metadata":{}},{"cell_type":"markdown","source":"You might think about the information in the training data as being of two kinds: signal and noise. The signal is the part that generalizes, the part that can help our model make predictions from new data. The noise is that part that is only true of the training data; the noise is all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can't actually help the model make predictions. The noise is the part might look useful but really isn't.\n\nThe training loss will go down either when the model learns signal or when it learns noise. But the validation loss will go down only when the model learns signal. (Whatever noise the model learned from the training set won't generalize to new data.) So, when a model learns signal both curves go down, but when it learns noise a gap is created in the curves. The size of the gap tells you how much noise the model has learned.\n\nA model's capacity refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.\n\nYou can increase the capacity of a network either by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.\n\nIf the validation loss begins to rise very early, while the training loss continues to decrease, is an indication that the network has begun to overfit. At this point, we would need to try something to prevent it, either by reducing the number of units or through a method like early stopping. Training with early stopping also means we're in less danger of stopping the training too early, before the network has finished learning signal. So besides preventing overfitting from training too long, early stopping can also prevent underfitting from not training long enough. Just set your training epochs to some large number (more than you'll need), and early stopping will take care of the rest.\n\nWe'll define an early stopping callback that waits some epochs (`patience`) for a change in validation loss of at least the `min_delta` and keeps the weights with the best loss (`restore_best_weights`).","metadata":{}},{"cell_type":"code","source":"early_stopping = callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nmodel = keras.Sequential([\n    layers.Dense(units=512, activation='relu', input_shape=input_shape),\n    layers.Dense(units=512, activation='relu'),\n    layers.Dense(units=1)\n])\n\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"mae\")\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=32,\n    epochs=200,\n    verbose=0,\n    callbacks=[early_stopping] # added the early_stoping\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:32:15.383153Z","iopub.execute_input":"2021-07-15T21:32:15.383555Z","iopub.status.idle":"2021-07-15T21:32:24.330028Z","shell.execute_reply.started":"2021-07-15T21:32:15.38351Z","shell.execute_reply":"2021-07-15T21:32:24.329285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Dropout Layer**","metadata":{}},{"cell_type":"markdown","source":"Some layers can do preprocessing or transformations of other sorts.\n\nOne of these is the \"dropout layer\", which can help correct overfitting. To recognize these spurious patterns a network will often rely on very a specific combinations of weight, a kind of \"conspiracy\" of weights. Being so specific, they tend to be fragile: remove one and the conspiracy falls apart.\n\nThis is the idea behind dropout. To break up these conspiracies, we randomly drop out some fraction of a layer's input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust.\n\nYou could also think about dropout as creating a kind of ensemble of networks. The predictions will no longer be made by one big network, but instead by a committee of smaller networks. Individuals in the committee tend to make different kinds of mistakes, but be right at the same time, making the committee as a whole better than any individual. (If you're familiar with random forests as an ensemble of decision trees, it's the same idea.)\n\nIn Keras, the dropout rate argument rate defines what percentage of the input units to shut off. Put the Dropout layer just before the layer you want the dropout applied to. (When adding dropout, you may need to increase the number of units in your Dense layers).","metadata":{}},{"cell_type":"markdown","source":"## **Batchnorm Layer**","metadata":{}},{"cell_type":"markdown","source":"With neural networks, it's generally a good idea to put all of your data on a common scale, perhaps with something like scikit-learn's StandardScaler or MinMaxScaler. The reason is that SGD will shift the network weights in proportion to how large an activation the data produces. Features that tend to produce activations of very different sizes can make for unstable training behavior.\n\nNow, if it's good to normalize the data before it goes into the network, maybe also normalizing inside the network would be better! In fact, we have a special kind of layer that can do this, the batch normalization layer. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.\n\nMost often, batchnorm is added as an aid to the optimization process (though it can sometimes also help prediction performance). Models with batchnorm tend to need fewer epochs to complete training. Moreover, batchnorm can also fix various problems that can cause the training to get \"stuck\". Consider adding batch normalization to your models, especially if you're having trouble during training. Batch normalization can be used at almost any point in a network.","metadata":{}},{"cell_type":"markdown","source":"## **Example**","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential([\n    layers.BatchNormalization(input_shape=[11]),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(1)\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='mae'\n)\n\nearly_stopping = callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=32,\n    epochs=200,\n    verbose=0,\n    callbacks=[early_stopping]\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:32:24.331149Z","iopub.execute_input":"2021-07-15T21:32:24.331421Z","iopub.status.idle":"2021-07-15T21:32:32.042012Z","shell.execute_reply.started":"2021-07-15T21:32:24.331395Z","shell.execute_reply":"2021-07-15T21:32:32.040975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Classification**","metadata":{}},{"cell_type":"markdown","source":"Let's treat this problem as a classification problem. If the quality of the wine is > 5, we'll classify it as good (1), and if it's <= 5, bad (0).\n\nAccuracy is one of the many metrics in use for measuring success on a classification problem. The problem with accuracy (and most other classification metrics) is that it can't be used as a loss function. SGD needs a loss function that changes smoothly, but accuracy, being a ratio of counts, changes in \"jumps\". So, we have to choose a substitute to act as the loss function. This substitute is the cross-entropy function.\n\nCross-entropy is a sort of measure for the distance from one probability distribution to another. The idea is that we want our network to predict the correct class with probability 1.0. The further away the predicted probability is from 1.0, the greater will be the cross-entropy loss.\n\nThe cross-entropy and accuracy functions both require probabilities as inputs, meaning, numbers from 0 to 1. To convert the real-valued outputs produced by a dense layer into probabilities, we attach a different kind of activation function, the sigmoid activation.","metadata":{}},{"cell_type":"code","source":"y_valid = y_valid.apply(\n    lambda x: 1 if x > 5 else 0)\ny_train = y_train.apply(\n    lambda x: 1 if x > 5 else 0)","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:32:32.043157Z","iopub.execute_input":"2021-07-15T21:32:32.043442Z","iopub.status.idle":"2021-07-15T21:32:32.049892Z","shell.execute_reply.started":"2021-07-15T21:32:32.043413Z","shell.execute_reply":"2021-07-15T21:32:32.049146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.Sequential([\n    layers.BatchNormalization(input_shape=[11]),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(512, activation='relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(0.3),\n    layers.Dense(1, activation='sigmoid') # sigmoid activation to convert the outputs into probabilities\n])\n\nmodel.compile(\n    optimizer='adam', # adam optimizer works great for classification too\n    loss='binary_crossentropy', # add the cross-entropy loss and accuracy metric to the model\n    metrics=['binary_accuracy'] # for two-class problems use the 'binary' versions\n)\n\nearly_stopping = callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=32,\n    epochs=200,\n    verbose=0,\n    callbacks=[early_stopping]\n)\n\nhistory_df = pd.DataFrame(history.history)\n\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_binary_accuracy'].max()))","metadata":{"execution":{"iopub.status.busy":"2021-07-15T21:32:32.050854Z","iopub.execute_input":"2021-07-15T21:32:32.051113Z","iopub.status.idle":"2021-07-15T21:32:38.680758Z","shell.execute_reply.started":"2021-07-15T21:32:32.051089Z","shell.execute_reply":"2021-07-15T21:32:38.679836Z"},"trusted":true},"execution_count":null,"outputs":[]}]}