{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Quellen\n\nhttps://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/\n\nhttps://stackoverflow.com/questions/56030884/how-to-define-specific-number-of-convolutional-kernels-filters-in-pytorch\n\nhttps://www.kaggle.com/hanjoonchoe/cnn-time-series-forecasting-with-pytorch\n\nhttps://medium.com/@sumanshusamarora/understanding-pytorch-conv1d-shapes-for-text-classification-c1e1857f8533\n\nhttps://datascience.stackexchange.com/questions/78030/multivariate-time-series-analysis-when-is-a-cnn-vs-lstm-appropriate\n\nhttps://discuss.pytorch.org/t/solved-concatenate-time-distributed-cnn-with-lstm/15435\n\nhttps://d2l.ai/chapter_convolutional-modern/resnet.html\n\nhttp://jmir.sourceforge.net/manuals/jSymbolic_manual/featureexplanations_files/featureexplanations.html\n\nhttps://medium.com/analytics-vidhya/understanding-and-implementation-of-residual-networks-resnets-b80f9a507b9c\n\nhttps://ai.stackexchange.com/questions/3156/how-to-select-number-of-hidden-layers-and-number-of-memory-cells-in-an-lstm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install music21 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n# from music21 import converter, corpus, instrument, midi, note, chord, pitch, meter\nfrom music21 import *\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nimport numpy as np\n\nimport os # handle files \nfrom torch.utils.data import Dataset ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n# L_out = (L_in+2*padding-dilation*(kernel_size-1)-1)*0.5 + 1\ndef whatPad(L_in,L_out,kernel_size,stride,dilation):\n    pad = ((L_out-1)*stride-L_in+dilation*(kernel_size-1)+1)*0.5\n    return pad\n\nL_in = 128\nL_out = L_in\nkernel_size = 5\nstride = 1\ndilation = 1\n\nprint(whatPad(L_in,L_out,kernel_size,stride,dilation))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Angepasste Dataset-Klasse\n# Liest ein Verzeichnis mit Komponisten-Unterverzeichnis mit Midi-Dateien ein.\n# Aus den Midi-Datein werden pro 'Part' 'num_subparts' viele Teile mit 'num_notes' zusammenhängenden Noten extrahiert.\n# Aus den Noten-Objekten werden Features ausgelesen (z.B. Midi-Zahl und Quarter Length).\n# Damit entsteht für jeden Subpart eine verschachtelte Liste, für die gilt Länge = Anzahl Noten = n.\n# Für jeden Note gibt es in dieser Liste eine eigene Liste mit den zur Note gehörigen Features.\n# --> [[Midi-Zahl_0,Quarter_Length_0],[Midi-Zahl_1,Quarter_Length_1],...,[Midi-Zahl_n-1,Quarter_Length_n-1]]\n# oder für m = Anzahl Feature (pro Note)\n# --> [[Feature_0_0,Feature_0_1,...,Feature_0_m-1],...,[Feature_n-1_0,Feature_n-1_1,...,Feature_n-1_m-1]]\n# Erstellt Torch-Tensoren aus den Listen, deren 'shape' angepasst ist um den conv1d Layern als Input gefüttert zu werden.\nclass FeatureDataset(Dataset):\n    \n    def __init__(self, path, composer_list, num_notes, num_subparts, num_features):\n        \n        # Eingaben abspeichern, falls man bei einem bereits initialisierten FeatureDataset-Objekt nochmal darauf zugreifen möchte\n        self.path = path\n        self.composer_list = composer_list\n        self.num_subparts = num_subparts\n        self.num_notes = num_notes\n        self.num_features = num_features\n        \n        # Leere Listen für das Extrahieren der Daten\n        subparts_notes = [] # wir extrahieren zu erst die subparts als Liste von Note-Objekten\n        x_train = [] # hier werden später die einzelnen subparts gesammelt\n        y_train = [] # hier werden die zu den subparts gehörenden Komponisten IDs gesammelt\n        \n        # Datei einlesen\n        composer_id = 0\n        for composer in self.composer_list: # alle Komponisten durchgehen\n            \n            print(composer)\n            print('----------*----------*----------')\n            \n            directory = os.path.join(self.path,composer) # bestimme das Verzeichnis für den Komponisten. Urpsrungspfad mit dem Komponistennamen zusammenführen -> Unterverzeichnis\n            for file in os.listdir(directory): # alle Dateien im Verzeichnis des Komponisten durchgehen\n                filename = os.fsdecode(file) # Dateiname als String speichern, zum vergleichen in der nächsten Zeile\n                if filename.endswith(\".mid\"): # wenn die Datei eine Midi-Datei ist, weiterverarbeiten, sonst ignorieren\n                    \n                    print(filename)\n                    \n                    path_to_midi = os.path.join(directory, filename) # Pfad zur Datei erstellen (s.o.)\n                    score = converter.parse(path_to_midi) # mit dem Music21 converter die Datei auslesen und in ein Score-Objekt umwandeln\n                    \n                    print(len(score.parts),' parts')\n                    print('----------*----------')\n                    \n                    # for i,part in enumerate(score.parts): # alle parts im Score Objekt durchegehen\n                    for part in score.parts: # Score Objekte bestehen eventuell aus mehreren Part Objekte. Daher alle Parts durchgehen.\n                        for _ in range(self.num_subparts): # num_subparts oft getNNotes aufrufen\n                            notes = self.getNNotes(part, self.num_notes)\n                            if len(notes)>0: # falls getNNotes keine leere Liste zurückgegeben hat:\n                                subparts_notes.append(notes) # in subparts_notes ablegen\n                                y_train.append(composer_id) # den Komponisten in y ablegen\n                                \n            composer_id += 1 # nächster Komponist\n        \n        \n        # Bis jetzt haben wir nur Noten-Objekte ausgelesen\n        \n        # Features aus den Noten-Objekten auslesen\n        for subpart in subparts_notes: # die einzelnen Subparts wieder durchlaufen\n            subpart_list = [] # leere Liste für jeden Subpart erstellen\n            for note in subpart: # die Noten im Subpart durchlaufen\n                feature_list = [] \n                feature_list.append(note.pitch.ps/127) # MIDI-Zahl, normieren auf [0,1]\n                feature_list.append(note.quarterLength) # Dauer in Viertel-Länge\n                subpart_list.append(feature_list.copy()) # Kopie(!) der einzelnen Features ablegen\n            x_train.append(subpart_list.copy())\n            \n        \n        # Erstellen von Tensoren.\n        # Das Torch-Netzwerk möchte mit float32 Werten arbeiten\n        # torch.nn.CrossEntropyLoss() braucht die Y-Werte mit dtype = long\n        \n        self.X_train = torch.tensor(x_train, dtype=torch.float32) #float32\n        self.Y_train = torch.tensor(y_train, dtype=torch.long)\n        \n        print(\"Training Shape before reshape\", self.X_train.shape, self.Y_train.shape)\n        \n        # Conv1d Layer benötigen die Input-Werte in dieser bestimmten Form [Batch_Size, Input_Channel, Sequence_Length]\n        # für LSTM oder auch Linear (bzw. Dense), kann dies wieder anders aussehen.\n        \n        self.X_train = torch.reshape(self.X_train, (self.X_train.shape[0], self.num_features, self.X_train.shape[1]))\n        \n        print(\"Training Shape after reshape\", self.X_train.shape, self.Y_train.shape)\n    \n    # vermutlich für den Dataloader nötig\n    def __len__(self):\n        return len(self.Y_train)\n    \n    # get-Methode muss entsprechend der Tensoren angepasst werden, ist hier recht simpel\n    def __getitem__(self, idx):\n        return self.X_train[idx], self.Y_train[idx]\n    \n    # n zusammenhängende Noten aus einem Part (zufälliger n-Subpart)\n    # Dazu wird ein zufälliger, aber zulässiger Startindex im Part ausgewählt, ab diesem werden dann die Note-Objekte ausgelesen\n    def getNNotes(self, part, n):  \n        notes_list = [] # leere Liste um die Note-Onjekte aufzunehmen\n        flat_part = part.flat # Auflösen der verschachtelten Struktur das Part-Objektes -> Flatten = 'platt machen'\n        noteIter= flat_part.getElementsByClass(note.Note) # Iterator über alle Note-Objekte im aufgelösten Part\n    \n        num_notes = len(noteIter) # damit hat die n-te Note den Index num_notes-1\n\n        '''\n        #Ist in der nächsten IF-Abfrage enthalten\n        if numNotes < n: # der Part muss mindestens n Noten enthalten, sonst können wir keinen n-langen Subpart bilden\n            return notesList # -> Rückgabe von leerer Liste\n        '''\n        \n        max_start_index = num_notes-1-n # maximaler Index an dem man anfangen kann um einen n-langen Subpart zu bilden\n        \n        if max_start_index < 0: # Ist der berechnete Index negativ, sind weniger als n Noten im Part enthalten\n            return notes_list # -> Rückgabe von leerer Liste\n        \n        \n        start_index = random.randint(0,max_start_index) # wir können einen zufälligen Index zwischen 0 und dem maximal möglichen Startindex wählen\n\n        for i in range(n): # n Noten ab dem Startindex in der Liste ablegen\n            notes_list.append(noteIter[start_index+i])\n\n        return notes_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modell","metadata":{}},{"cell_type":"code","source":"# Da eindimensionale Zeitreihendaten (wie oben beschrieben) vorliegen, genügen Conv1D-Layer, statt der z.B. aus der Bildverarbeitung bekannten Conv2D-Layer.\n# Die Hyperparameter sind zum Großteil durch Ausprobieren entstanden.\nclass CONV_LSTM1(nn.Module):\n    def __init__(self, num_classes, hidden_size, batch_s, num_features, num_filter1, num_filter2, num_fitler3):\n        super(CONV_LSTM1, self).__init__()\n        \n        self.debug = False # Falls True, printet der Forward Pass die Gestalten der Tensoren.\n        self.res = True # Falls Ture, wird die 'Abkürzung' für den Residualblock genutzt. Falls False, ist das Netzwerk eine normales Conv-Net.\n        \n        # ----------*----------*----------\n        # Allgemeine Parameter\n        \n        self.batch_size = batch_s # batch size\n        self.num_classes = num_classes # Anzahl der Klassen (Komponisten), relevant für den letzten Layer\n        \n        # Conv Parameter\n        self.num_features = num_features # Anzahl der Inputchannels des ersten Conv-Layers\n        self.num_filter1 = num_filter1\n        self.num_filter2 = num_filter2\n        self.num_filter3 = num_filter3\n        \n        # LSTM Paramter\n        self.hidden_size = hidden_size # hidden state\n        self.num_layers = 1 # number of layers\n        \n        # ----------*----------*----------*----------*----------*----------*----------*----------*----------\n        # Layer\n        # ---------- Res Block 1 Start ----------\n        self.conv1_id = nn.Conv1d(self.num_features, out_channels=self.num_filter1, kernel_size = 1, stride = 1)\n        self.b_norm1_id = nn.BatchNorm1d(self.num_filter1)\n        \n        self.conv1a = nn.Conv1d(self.num_features, out_channels=self.num_filter1, kernel_size=5, padding = 2)\n        self.b_norm1a = nn.BatchNorm1d(self.num_filter1)\n        \n        self.conv1b = nn.Conv1d(self.num_filter1, out_channels=self.num_filter1, kernel_size=5, padding = 2)\n        self.b_norm1b = nn.BatchNorm1d(self.num_filter1)\n        \n        self.conv1c = nn.Conv1d(self.num_filter1, out_channels=self.num_filter1, kernel_size=5, padding = 2)\n        self.b_norm1c = nn.BatchNorm1d(self.num_filter1)\n        \n        # ---------- Res Block 1 Ende ----------\n        \n        self.drop1 = nn.Dropout(p=0.1) # Standard Dropout Layer für stabileres Training\n        \n        self.pool1 = nn.MaxPool1d(2, stride = 2) # Standard Pooling, auch hier 1-D\n        \n        # ---------- Res Block 2 Start ----------\n        self.conv2_id = nn.Conv1d(self.num_filter1, out_channels=self.num_filter2, kernel_size = 1, stride = 1)\n        self.b_norm2_id = nn.BatchNorm1d(self.num_filter2)\n        \n        self.conv2a = nn.Conv1d(self.num_filter1, out_channels=self.num_filter2, kernel_size=5, padding = 2)\n        self.b_norm2a = nn.BatchNorm1d(self.num_filter2)\n        \n        self.conv2b = nn.Conv1d(self.num_filter2, out_channels=self.num_filter2, kernel_size=5, padding = 2)\n        self.b_norm2b = nn.BatchNorm1d(self.num_filter2)\n        \n        self.conv2c = nn.Conv1d(self.num_filter2, out_channels=self.num_filter2, kernel_size=5, padding = 2)\n        self.b_norm2c = nn.BatchNorm1d(self.num_filter2)\n        \n        # ---------- Res Block 2 Ende ----------\n        \n        self.drop2 = nn.Dropout(p=0.25) # Standard Dropout Layer für stabileres Training\n        \n        self.pool2 = nn.MaxPool1d(2, stride = 2) # Standard Pooling, auch hier 1-D\n        \n        # ---------- Res Block 3 Start ----------\n        self.conv3_id = nn.Conv1d(self.num_filter2, out_channels=self.num_filter3, kernel_size = 1, stride = 1)\n        self.b_norm3_id = nn.BatchNorm1d(self.num_filter3)\n        \n        self.conv3a = nn.Conv1d(self.num_filter2, out_channels=self.num_filter3, kernel_size=5, padding = 2)\n        self.b_norm3a = nn.BatchNorm1d(self.num_filter3)\n        \n        self.conv3b = nn.Conv1d(self.num_filter3, out_channels=self.num_filter3, kernel_size=5, padding = 2)\n        self.b_norm3b = nn.BatchNorm1d(self.num_filter3)\n        \n        self.conv3c = nn.Conv1d(self.num_filter3, out_channels=self.num_filter3, kernel_size=5, padding = 2)\n        self.b_norm3c = nn.BatchNorm1d(self.num_filter3)\n        \n        # ---------- Res Block 3 Ende ----------\n        \n        self.drop3 = nn.Dropout(p=0.5) # Standard Dropout Layer für stabileres Training\n        \n        self.pool3 = nn.MaxPool1d(2, stride = 2) # Standard Pooling, auch hier 1-D\n        \n        # Die convolotional Layer sollen idealerweise kurzfristige Zusammenhänge/Muster aus der Zeitreihe herausarbeiten\n        # Für langfristige Strukturen nutzt man zusätzlich einen LSTM-Layer.\n        self.lstm1 = nn.LSTM(input_size=self.num_filter3, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True) \n        \n        self.flat1 = nn.Flatten()\n        \n        self.fc1 = nn.Linear(4096,1024)\n        \n        self.fc2 = nn.Linear(1024,512)\n        \n        self.fc3 = nn.Linear(512, self.num_classes)\n        # ----------*----------*----------*----------*----------*----------*----------*----------*----------\n        \n        \n    def forward(self,x):\n        if self.debug:\n            print('----------*--------- Forward Start ----------*----------')\n            print('Input Shape')\n            print(x.shape)\n            print('----------Res Block 1 Start--------')\n                \n        # ----------*----------*----------*----------*----------*----------\n        # Res Block 1\n        # ----------*----------*----------\n        # Layer 1_id - Conv\n        if self.res:\n            if self.debug:\n                print('conv1_id')\n            x_id = self.conv1_id(x)\n            if self.debug:\n                print(x_id.shape)\n        \n        # ----------*----------*----------\n        # Layer 1a - Conv Layer\n        if self.debug:\n            print('conv1a')\n        x_res = self.conv1a(x) \n        x_res = self.b_norm1a(x_res)\n        x_res = F.relu(x_res) \n        \n        # ----------*----------*----------\n        # Layer 1b - Conv Layer\n        if self.debug:\n            print(x_res.shape)\n            print('conv1b')\n            \n        x_res = self.conv1b(x_res)\n        x_res = self.b_norm1b(x_res)\n        x_res = F.relu(x_res) \n            \n        # ----------*----------*----------\n        # Layer 1c - Conv Layer\n        if self.debug:\n            print(x_res.shape)\n            print('conv1c')\n            \n        x_res = self.conv1c(x_res)\n        x_res = self.b_norm1c(x_res)\n        x_res = F.relu(x_res)\n        \n        if self.debug:\n            print(x_res.shape)\n            print('----------Res Block 1 Ende--------')\n        # ----------*----------*----------  \n        # Aktivierung \n        if self.res:\n            if self.debug:\n                print('Relu(x+x_res)')\n            x = x_id+x_res\n        else:\n            if self.debug:\n                print('Relu(x)')\n            x = x_res\n        x = F.relu(x)\n        \n        # Dropout \n        if self.debug:\n            print(x.shape)\n            print('drop1')\n        x = self.drop1(x)\n        \n        \n        # Pool\n        if self.debug:\n            print(x.shape)\n            print('pool1')\n        x = self.pool1(x)\n        \n        \n        # ----------*----------*----------*----------*----------*----------\n        # Res Block 2\n        if self.debug:\n            print(x.shape)\n            print('----------Res Block 2 Start--------')       \n        # ----------*----------*----------\n        # Layer 2_ind - Conv Layer\n        if self.res:\n            if self.debug:\n                print('conv2_id')\n            x_id = self.conv2_id(x)\n            if self.debug:\n                print(x_id.shape)\n        # ----------*----------*----------\n        # Layer 2a - Conv Layer\n        if self.debug:\n            print('conv2a')\n            \n        x_res = self.conv2a(x) \n        x_res = self.b_norm2a(x_res)\n        x_res = F.relu(x_res)  \n        # ----------*----------*----------\n        # Layer 2b - Conv Layer\n        if self.debug:\n            print(x_res.shape)\n            print('conv2b')\n            \n        x_res = self.conv2b(x_res)\n        x_res = self.b_norm2b(x_res)\n        x_res = F.relu(x_res) \n        \n        if self.debug:\n            print(x_res.shape)\n        \n        # ----------*----------*----------\n        # Layer 2c - Conv Layer\n        if self.debug:\n            print(x_res.shape)\n            print('conv2c')\n            \n        x_res = self.conv2c(x_res)\n        x_res = self.b_norm2c(x_res)\n        x_res = F.relu(x_res)\n        \n        if self.debug:\n            print(x_res.shape)\n            print('----------Res Block 2 Ende--------')\n        # ----------*----------*----------  \n        # Aktivierung \n        if self.res:\n            if self.debug:\n                print('Relu(x+x_res)')\n            x = x_id+x_res\n        else:\n            if self.debug:\n                print('Relu(x)')\n            x = x_res\n        x = F.relu(x)\n        \n        # Dropout \n        if self.debug:\n            print(x.shape)\n            print('drop2')\n        x = self.drop2(x)    \n        \n        # Pool\n        if self.debug:\n            print(x.shape)\n            print('pool2')\n        x = self.pool2(x)\n        \n        # ----------*----------*----------*----------*----------*----------\n        # Res Block 3\n        if self.debug:\n            print(x.shape)\n            print('----------Res Block 3 Start--------')       \n        # ----------*----------*----------\n        # Layer 2_ind - Conv Layer\n        if self.res:\n            if self.debug:\n                print('conv3_id')\n            x_id = self.conv3_id(x)\n            if self.debug:\n                print(x_id.shape)\n        # ----------*----------*----------\n        # Layer 3a - Conv Layer\n        if self.debug:\n            print('conv3a')\n            \n        x_res = self.conv3a(x) \n        x_res = self.b_norm3a(x_res)\n        x_res = F.relu(x_res)  \n        # ----------*----------*----------\n        # Layer 3b - Conv Layer\n        if self.debug:\n            print(x_res.shape)\n            print('conv3b')\n            \n        x_res = self.conv3b(x_res)\n        x_res = self.b_norm3b(x_res)\n        x_res = F.relu(x_res) \n        \n        if self.debug:\n            print(x_res.shape)\n        \n        # ----------*----------*----------\n        # Layer 3c - Conv Layer\n        if self.debug:\n            print(x_res.shape)\n            print('conv3c')\n            \n        x_res = self.conv3c(x_res)\n        x_res = self.b_norm3c(x_res)\n        x_res = F.relu(x_res)\n        \n        if self.debug:\n            print(x_res.shape)\n            print('----------Res Block 3 Ende--------')\n        # ----------*----------*----------  \n        # Aktivierung \n        if self.res:\n            if self.debug:\n                print('Relu(x+x_res)')\n            x = x_id+x_res\n        else:\n            if self.debug:\n                print('Relu(x)')\n            x = x_res\n        x = F.relu(x)\n        \n        # Dropout \n        if self.debug:\n            print(x.shape)\n            print('drop3')\n        x = self.drop3(x)    \n        \n        # Pool\n        if self.debug:\n            print(x.shape)\n            print('pool3')\n        x = self.pool3(x)\n        \n        # ----------*----------*----------*----------*----------*----------\n\n        if self.debug:\n            print(x.shape)\n            print('permute')\n        \n        # ----------*----------*----------\n        # Layer 6 - LSTM Layer\n        '''\n        x = x.permute(0,2,1) # wegen Batch_First = True im LSTM Layer \n        \n        if self.debug:\n            print(x.shape)\n            print('lstm')\n        \n        h_0_1 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state - 'cell state'\n        c_0_1 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state - 'output'\n        x, (hn, cn) = self.lstm1(x, (h_0_1, c_0_1)) # dem LSTM muss Input, Hidden und Internal State übergeben werden\n        '''\n        # ----------*----------*----------\n        # Layer 7 - Flat Layer\n        if self.debug:\n            print(x.shape)\n            print('flat')\n        \n        x = self.flat1(x)\n        \n        # ----------*----------*----------\n        # Layer 8 - Dense Layer\n        if self.debug:\n            print(x.shape)\n            print('fc1')\n  \n        x = self.fc1(x)\n        x = F.relu(x) \n        \n        # ----------*----------*----------\n        # Layer 9 - Dense Layer\n        if self.debug:\n            print(x.shape)\n            print('fc2')\n            \n        x = self.fc2(x)\n        x = F.relu(x)\n        \n        # ----------*----------*----------\n        # Layer 10 - Dense Layer Output\n        if self.debug:\n            print(x.shape)\n            print('fc3/output')\n        \n        x = self.fc3(x)\n        \n        if self.debug:\n            print(x.shape)\n            print('----------*--------- Forward Ende ----------*----------')\n        \n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparamter für den Datensatz","metadata":{}},{"cell_type":"code","source":"# Datensatz einlesen und in ein Dataset übertragen\npath = '../input/musicnet-dataset/musicnet_midis/musicnet_midis/'\npath_train = '../input/musicnet-midis-testsplit/musicnet_midis/Training'\npath_test = '../input/musicnet-midis-testsplit/musicnet_midis/Test'\n\ncomposerList = ['Brahms','Dvorak','Cambini','Faure','Haydn','Ravel']\n#composerList = ['Brahms','Ravel']\n\nn_Notes = 128\nn_Subparts = 25\nn_features = 2\n\n# Batchsize\nbatch_s = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('../input/musicnet-midis-testsplit/musicnet_midis/Training')\nos.listdir(path_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Erzeugen des Feature-Set-Objekts und des Trainings- Und Validierungs-Loaders","metadata":{}},{"cell_type":"code","source":"feature_set = FeatureDataset(path_train, composerList, n_Notes, n_Subparts, n_features)\ntest_set = FeatureDataset(path_test, composerList, n_Notes, n_Subparts, n_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split\ntrain_size = int(0.8 * len(feature_set)) # split 0.2\nval_size = len(feature_set) - train_size\n\ntrain_dataset, val_dataset = torch.utils.data.random_split(feature_set, [train_size, val_size])\n\n# train_loader erstellen\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_s, shuffle = True)\nvalidation_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_s)\n\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_s)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparamter für das Modell","metadata":{}},{"cell_type":"code","source":"num_classes = len(composerList)\ninput_size = n_Notes \n#hidden_size = int(input_size/5)\nhidden_size = 64\n#num_features = 4\nlearning_rate = 0.0001 \n\nnum_filter1 = 64\nnum_filter2 = 128\nnum_filter3 = 256","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv_lstm1 = CONV_LSTM1(num_classes, hidden_size, batch_s, n_features,num_filter1,num_filter2,num_filter3)\n\n# Loss und Optimizer festlegen, beide standard bei Multiclass-Klassifizierung\n# torch.nn.CrossEntropyLoss ist nicht = categorical cross entropy aus Tensorflow\ncriterion = torch.nn.CrossEntropyLoss() \noptimizer = torch.optim.Adam(conv_lstm1.parameters(), lr=learning_rate) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Das untrainierte Modell","metadata":{}},{"cell_type":"code","source":"# Zur Bewertung der Performance \ndef hit(target, pred): # nimmt Zielwerte und Vorhersagen an und vergleicht diese\n    # das LSTM gibt keine als Wahrscheinlichkeitsverteilung interpretierbaren Werte aus, deshalb muss Softmax angewandt werden.\n    # Dies ist eine Art Pytorch Eigenheit, man könnte auch im Modell auf den letzten Layer Softmax anwenden, dann funktioniert \n    # torch.nn.CrossEntropyLoss() aber nicht mehr.\n    \n    #pred_softmax = torch.softmax(pred, dim = 1)\n    \n    #print('----------*----------*----------')\n    pred_softmax = torch.log_softmax(pred.view(-1),0)\n    \n    #print(pred_softmax)\n    \n    # der folgenden Befehl könnte sicher auch einfach gestaltet werden\n    # detach() - entfernt Gradienten, welche einem Tensor zugewiesen werden\n    # reshape(-1) - plättet (flatten) einen Tensor, d.h. wirft alle Werte in einer Dimension zusammen\n    # numpy() - wandelt den Tensor in ein Numpy Array um\n    # argmax() - gibt den Index mit dem maximalen Wert zurück\n    # -> argmax gibt für die 1-Hot-Kodierung die Klasse zurück,\n    # -> argmax gibt für einen Vektor mit Klassen-Wahrscheinlichkeiten, die Klasse mit der maximalen Wahrscheinlichkeit zurück\n    \n    if target.detach().reshape(-1).numpy() == pred_softmax.detach().reshape(-1).numpy().argmax():\n        return True\n    else:\n        return False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv_lstm1.debug = False\nconv_lstm1.res = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def perf_val(): # performance auf val-set\n    conv_lstm1.eval()\n    hits = 0\n    tries = 0\n    debug = False\n    for x_val, y_val in validation_loader:\n        if debug:\n            print('----------*----------')\n            print('x_val')\n            print(x_val.shape)\n            print(x_val)\n        \n        yhat = conv_lstm1(x_val)\n    \n    \n        for target, pred in zip(y_val, yhat):\n            tries += 1\n            if hit(target, pred):\n                hits +=1\n            \n    print('Tries')\n    print(tries)\n    print('Number of hits:')\n    print(hits)\n    print('Ratio')\n    print(hits/tries)\n    # bei einem zufällig gewichteten/untrainierten Modell würde man eine Treffsicherheit von 1:(Anzahl Klassen) erwarten\n    print('Random Ratio')\n    print(1/num_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def perf_test():# performance auf dem test set\n    conv_lstm1.eval()\n    hits = 0\n    tries = 0\n    for x_val, y_val in test_loader:\n        yhat = conv_lstm1(x_val)\n        for target, pred in zip(y_val, yhat):\n            tries += 1\n            if hit(target, pred):\n                hits +=1\n            \n    print('Tries')\n    print(tries)\n    print('Number of hits:')\n    print(hits)\n    print('Ratio')\n    print(hits/tries)\n    # bei einem zufällig gewichteten/untrainierten Modell würde man eine Treffsicherheit von 1:(Anzahl Klassen) erwarten\n    print('Random Ratio')\n    print(1/num_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"num_epochs = 25\n\navg_losses = []\nconv_lstm1.train()\ndebug = False\nval_each = 1\ntest_each = 1\nfor epoch in range(num_epochs):\n    conv_lstm1.train()\n    epoch_losses = []\n    for x_batch, y_batch in train_loader:\n        output = conv_lstm1.forward(x_batch) #forward pass\n        optimizer.zero_grad()#caluclate the gradient, manually setting to 0\n        \n        if debug:\n            print(output.shape)\n            print(y_batch.shape)\n        #print(y_batch)\n        #print(torch.max(y_batch,1))\n        #loss = criterion(output.view(1,-1), y_batch)\n\n        \n        #output = torch.reshape(output, (output.shape[0], output.shape[2]))\n        loss = criterion(output, y_batch)\n        epoch_losses.append(loss.item())\n        loss.backward() #calculates the loss of the loss function\n         \n        optimizer.step() #improve from loss, i.e backprop\n        #losses.append(loss)\n    \n    epoch_avg = sum(epoch_losses)/len(epoch_losses)\n    avg_losses.append(epoch_avg)\n    print(\"Epoch: %d, avg loss: %1.5f\" % (epoch+1, epoch_avg)) \n    if epoch % val_each == 0:\n        print('----------Val---------')\n        perf_val()\n        \n    if epoch % test_each == 0:\n        print('---------Test--------')\n        perf_test()\n    \n    print('---------*---------*----------*---------*---------*----------*---------*---------*----------')\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Nach dem Training","metadata":{}},{"cell_type":"code","source":"# performance vor dem Training\nconv_lstm1.eval()\nhits = 0\ntries = 0\nfor x_val, y_val in validation_loader:\n    yhat = conv_lstm1(x_val)\n    for target, pred in zip(y_val, yhat):\n        tries += 1\n        if hit(target, pred):\n            hits +=1\n            \nprint('Tries')\nprint(tries)\nprint('Number of hits:')\nprint(hits)\nprint('Ratio')\nprint(hits/tries)\n# bei einem zufällig gewichteten/untrainierten Modell würde man eine Treffsicherheit von 1:(Anzahl Klassen) erwarten\nprint('Random Ratio')\nprint(1/num_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Performance auf dem Test-Set","metadata":{}},{"cell_type":"code","source":"# performance vor dem Training\nconv_lstm1.eval()\nhits = 0\ntries = 0\nfor x_val, y_val in test_loader:\n    yhat = conv_lstm1(x_val)\n    for target, pred in zip(y_val, yhat):\n        tries += 1\n        if hit(target, pred):\n            hits +=1\n            \nprint('Tries')\nprint(tries)\nprint('Number of hits:')\nprint(hits)\nprint('Ratio')\nprint(hits/tries)\n# bei einem zufällig gewichteten/untrainierten Modell würde man eine Treffsicherheit von 1:(Anzahl Klassen) erwarten\nprint('Random Ratio')\nprint(1/num_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}