{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Training of models for AudioSet animal sound database ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-13T19:01:32.903116Z","iopub.execute_input":"2021-06-13T19:01:32.903579Z","iopub.status.idle":"2021-06-13T19:01:32.944282Z","shell.execute_reply.started":"2021-06-13T19:01:32.903483Z","shell.execute_reply":"2021-06-13T19:01:32.943505Z"}}},{"cell_type":"markdown","source":"### Code and models are inspired by https://github.com/qiuqiangkong/audioset_tagging_cnn and *[1] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, Mark D. Plumbley. \"PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition.\" arXiv preprint arXiv:1912.10211 (2019)*","metadata":{}},{"cell_type":"markdown","source":"- *audioset-10-percent* contains the test (eval.h5) and validation (balanced_train.h5) set, and parts 00-11 of the training set\n- *audioset-20-percent* contains parts 12-21 of the training set \n- *audioset-10-percent* contains parts 22-34 of the training set\n- *audioset-10-percent* contains parts 35-41 of the training set\n- *indices* contains infromation on the classes, and the differents sets of data","metadata":{}},{"cell_type":"code","source":"pip install torchlibrosa","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:32:08.393811Z","iopub.execute_input":"2021-06-21T19:32:08.39434Z","iopub.status.idle":"2021-06-21T19:32:17.279764Z","shell.execute_reply.started":"2021-06-21T19:32:08.394298Z","shell.execute_reply":"2021-06-21T19:32:17.278593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing packages\nimport os\nimport sys\nimport numpy as np\nimport argparse\nimport time\nimport logging\nimport torch\nimport torch.nn as nn # Basic building block for graphs\nimport torch.nn.functional as F \nimport torch.optim as optim\nimport torch.utils.data\nimport logging\nimport h5py\nimport soundfile\nimport librosa\nimport pandas as pd\nfrom scipy import stats \nimport datetime\nimport pickle\nfrom torchlibrosa.stft import Spectrogram, LogmelFilterBank\nfrom torchlibrosa.augmentation import SpecAugmentation\nimport csv\nfrom sklearn import metrics    \ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:33:46.000959Z","iopub.execute_input":"2021-06-21T19:33:46.001993Z","iopub.status.idle":"2021-06-21T19:33:46.024533Z","shell.execute_reply.started":"2021-06-21T19:33:46.001924Z","shell.execute_reply":"2021-06-21T19:33:46.023424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate\nclass Evaluator(object):\n    def __init__(self, model):\n        \"\"\"Evaluator.\n\n        Args:\n          model: object\n        \"\"\"\n        self.model = model\n        \n    def evaluate(self, data_loader):\n        \"\"\"Forward evaluation data and calculate statistics.\n\n        Args:\n          data_loader: object\n\n        Returns:\n          statistics: dict, \n              {'average_precision': (classes_num,), 'auc': (classes_num,)}\n        \"\"\"\n\n        # Forward\n        output_dict = forward(\n            model=self.model, \n            generator=data_loader, \n            return_target=True)\n\n        clipwise_output = output_dict['clipwise_output']    # (audios_num, classes_num)\n        target = output_dict['target']    # (audios_num, classes_num)\n\n        average_precision = metrics.average_precision_score(\n            target, clipwise_output, average=None)\n\n        auc = metrics.roc_auc_score(target, clipwise_output, average=None)\n        \n        statistics = {'average_precision': average_precision, 'auc': auc}\n\n        return statistics","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:33:49.634793Z","iopub.execute_input":"2021-06-21T19:33:49.635152Z","iopub.status.idle":"2021-06-21T19:33:49.643618Z","shell.execute_reply.started":"2021-06-21T19:33:49.635122Z","shell.execute_reply":"2021-06-21T19:33:49.642526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Losses\ndef clip_bce(output_dict, target_dict):\n    \"\"\"Binary crossentropy loss.\n    \"\"\"\n    return F.binary_cross_entropy(\n        output_dict['clipwise_output'], target_dict['target'])\n\n\ndef get_loss_func(loss_type):\n    if loss_type == 'clip_bce':\n        return clip_bce","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:33:50.509025Z","iopub.execute_input":"2021-06-21T19:33:50.5096Z","iopub.status.idle":"2021-06-21T19:33:50.514654Z","shell.execute_reply.started":"2021-06-21T19:33:50.509552Z","shell.execute_reply":"2021-06-21T19:33:50.513726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#data generator\ndef read_black_list(black_list_csv):\n    \"\"\"Read audio names from black list. \n    \"\"\"\n    with open(black_list_csv, 'r') as fr:\n        reader = csv.reader(fr)\n        lines = list(reader)\n\n    black_list_names = ['Y{}.wav'.format(line[0]) for line in lines]\n    return black_list_names\n\n\nclass AudioSetDataset(object):\n    def __init__(self, sample_rate=32000):\n        \"\"\"This class takes the meta of an audio clip as input, and return \n        the waveform and target of the audio clip. This class is used by DataLoader. \n        \"\"\"\n        self.sample_rate = sample_rate\n    \n    def __getitem__(self, meta):\n        \"\"\"Load waveform and target of an audio clip.\n        \n        Args:\n          meta: {\n            'hdf5_path': str, \n            'index_in_hdf5': int}\n\n        Returns: \n          data_dict: {\n            'audio_name': str, \n            'waveform': (clip_samples,), \n            'target': (classes_num,)}\n        \"\"\"\n        hdf5_path = meta['hdf5_path']\n        index_in_hdf5 = meta['index_in_hdf5']\n\n        with h5py.File(hdf5_path, 'r') as hf:\n            audio_name = hf['audio_name'][index_in_hdf5].decode()\n            waveform = int16_to_float32(hf['waveform'][index_in_hdf5])\n            waveform = self.resample(waveform)\n            target = hf['target'][index_in_hdf5].astype(np.float32)\n\n        data_dict = {\n            'audio_name': audio_name, 'waveform': waveform, 'target': target}\n            \n        return data_dict\n\n    def resample(self, waveform):\n        \"\"\"Resample.\n\n        Args:\n          waveform: (clip_samples,)\n\n        Returns:\n          (resampled_clip_samples,)\n        \"\"\"\n        if self.sample_rate == 32000:\n            return waveform\n        elif self.sample_rate == 16000:\n            return waveform[0 :: 2]\n        elif self.sample_rate == 8000:\n            return waveform[0 :: 4]\n        else:\n            raise Exception('Incorrect sample rate!')\n\n\nclass Base(object):\n    def __init__(self, indexes_hdf5_path, batch_size, black_list_csv, random_seed):\n        \"\"\"Base class of train sampler.\n        \n        Args:\n          indexes_hdf5_path: string\n          batch_size: int\n          black_list_csv: string\n          random_seed: int\n        \"\"\"\n        self.batch_size = batch_size\n        self.random_state = np.random.RandomState(random_seed)\n\n        # Black list\n        if black_list_csv:\n            self.black_list_names = read_black_list(black_list_csv)\n        else:\n            self.black_list_names = []\n\n        logging.info('Black list samples: {}'.format(len(self.black_list_names)))\n\n        # Load target\n        load_time = time.time()\n\n        with h5py.File(indexes_hdf5_path, 'r') as hf:\n            self.audio_names = [audio_name.decode() for audio_name in hf['audio_name'][:]]\n            self.hdf5_paths = [hdf5_path.decode() for hdf5_path in hf['hdf5_path'][:]]\n            self.indexes_in_hdf5 = hf['index_in_hdf5'][:]\n            self.targets = hf['target'][:].astype(np.float32)\n        \n        (self.audios_num, self.classes_num) = self.targets.shape\n        logging.info('Training number: {}'.format(self.audios_num))\n        logging.info('Load target time: {:.3f} s'.format(time.time() - load_time))\n\n\nclass TrainSampler(Base):\n    def __init__(self, indexes_hdf5_path, batch_size, black_list_csv=None, \n        random_seed=1234):\n        \"\"\"Balanced sampler. Generate batch meta for training.\n        \n        Args:\n          indexes_hdf5_path: string\n          batch_size: int\n          black_list_csv: string\n          random_seed: int\n        \"\"\"\n        super(TrainSampler, self).__init__(indexes_hdf5_path, batch_size, \n            black_list_csv, random_seed)\n        \n        self.indexes = np.arange(self.audios_num)\n            \n        # Shuffle indexes\n        self.random_state.shuffle(self.indexes)\n        \n        self.pointer = 0\n\n    def __iter__(self):\n        \"\"\"Generate batch meta for training. \n        \n        Returns:\n          batch_meta: e.g.: [\n            {'hdf5_path': string, 'index_in_hdf5': int}, \n            ...]\n        \"\"\"\n        batch_size = self.batch_size\n\n        while True:\n            batch_meta = []\n            i = 0\n            while i < batch_size:\n                index = self.indexes[self.pointer]\n                self.pointer += 1\n\n                # Shuffle indexes and reset pointer\n                if self.pointer >= self.audios_num:\n                    self.pointer = 0\n                    self.random_state.shuffle(self.indexes)\n                \n                # If audio in black list then continue\n                if self.audio_names[index] in self.black_list_names:\n                    continue\n                else:\n                    batch_meta.append({\n                        'hdf5_path': self.hdf5_paths[index], \n                        'index_in_hdf5': self.indexes_in_hdf5[index]})\n                    i += 1\n\n            yield batch_meta\n\n    def state_dict(self):\n        state = {\n            'indexes': self.indexes,\n            'pointer': self.pointer}\n        return state\n            \n    def load_state_dict(self, state):\n        self.indexes = state['indexes']\n        self.pointer = state['pointer']\n\n\nclass BalancedTrainSampler(Base):\n    def __init__(self, indexes_hdf5_path, batch_size, black_list_csv=None, \n        random_seed=1234):\n        \"\"\"Balanced sampler. Generate batch meta for training. Data are equally \n        sampled from different sound classes.\n        \n        Args:\n          indexes_hdf5_path: string\n          batch_size: int\n          black_list_csv: string\n          random_seed: int\n        \"\"\"\n        super(BalancedTrainSampler, self).__init__(indexes_hdf5_path, \n            batch_size, black_list_csv, random_seed)\n        \n        self.samples_num_per_class = np.sum(self.targets, axis=0)\n        logging.info('samples_num_per_class: {}'.format(\n            self.samples_num_per_class.astype(np.int32)))\n        \n        # Training indexes of all sound classes. E.g.: \n        # [[0, 11, 12, ...], [3, 4, 15, 16, ...], [7, 8, ...], ...]\n        self.indexes_per_class = []\n        \n        for k in range(self.classes_num):\n            self.indexes_per_class.append(\n                np.where(self.targets[:, k] == 1)[0])\n            \n        # Shuffle indexes\n        for k in range(self.classes_num):\n            self.random_state.shuffle(self.indexes_per_class[k])\n        \n        self.queue = []\n        self.pointers_of_classes = [0] * self.classes_num\n\n    def expand_queue(self, queue):\n        classes_set = np.arange(self.classes_num).tolist()\n        self.random_state.shuffle(classes_set)\n        queue += classes_set\n        return queue\n\n    def __iter__(self):\n        \"\"\"Generate batch meta for training. \n        \n        Returns:\n          batch_meta: e.g.: [\n            {'hdf5_path': string, 'index_in_hdf5': int}, \n            ...]\n        \"\"\"\n        batch_size = self.batch_size\n\n        while True:\n            batch_meta = []\n            i = 0\n            while i < batch_size:\n                if len(self.queue) == 0:\n                    self.queue = self.expand_queue(self.queue)\n\n                class_id = self.queue.pop(0)\n                pointer = self.pointers_of_classes[class_id]\n                self.pointers_of_classes[class_id] += 1\n                index = self.indexes_per_class[class_id][pointer]\n                \n                # When finish one epoch of a sound class, then shuffle its indexes and reset pointer\n                if self.pointers_of_classes[class_id] >= self.samples_num_per_class[class_id]:\n                    self.pointers_of_classes[class_id] = 0\n                    self.random_state.shuffle(self.indexes_per_class[class_id])\n\n                # If audio in black list then continue\n                if self.audio_names[index] in self.black_list_names:\n                    continue\n                else:\n                    batch_meta.append({\n                        'hdf5_path': self.hdf5_paths[index], \n                        'index_in_hdf5': self.indexes_in_hdf5[index]})\n                    i += 1\n\n            yield batch_meta\n\n    def state_dict(self):\n        state = {\n            'indexes_per_class': self.indexes_per_class, \n            'queue': self.queue, \n            'pointers_of_classes': self.pointers_of_classes}\n        return state\n            \n    def load_state_dict(self, state):\n        self.indexes_per_class = state['indexes_per_class']\n        self.queue = state['queue']\n        self.pointers_of_classes = state['pointers_of_classes']\n\n\nclass AlternateTrainSampler(Base):\n    def __init__(self, indexes_hdf5_path, batch_size, black_list_csv=None,\n        random_seed=1234):\n        \"\"\"AlternateSampler is a combination of Sampler and Balanced Sampler. \n        AlternateSampler alternately sample data from Sampler and Balanced Sampler.\n        \n        Args:\n          indexes_hdf5_path: string          \n          batch_size: int\n          black_list_csv: string\n          random_seed: int\n        \"\"\"\n        self.sampler1 = TrainSampler(indexes_hdf5_path, batch_size, \n            black_list_csv, random_seed)\n\n        self.sampler2 = BalancedTrainSampler(indexes_hdf5_path, batch_size, \n            black_list_csv, random_seed)\n\n        self.batch_size = batch_size\n        self.count = 0\n\n    def __iter__(self):\n        \"\"\"Generate batch meta for training. \n        \n        Returns:\n          batch_meta: e.g.: [\n            {'hdf5_path': string, 'index_in_hdf5': int}, \n            ...]\n        \"\"\"\n        batch_size = self.batch_size\n\n        while True:\n            self.count += 1\n\n            if self.count % 2 == 0:\n                batch_meta = []\n                i = 0\n                while i < batch_size:\n                    index = self.sampler1.indexes[self.sampler1.pointer]\n                    self.sampler1.pointer += 1\n\n                    # Shuffle indexes and reset pointer\n                    if self.sampler1.pointer >= self.sampler1.audios_num:\n                        self.sampler1.pointer = 0\n                        self.sampler1.random_state.shuffle(self.sampler1.indexes)\n                    \n                    # If audio in black list then continue\n                    if self.sampler1.audio_names[index] in self.sampler1.black_list_names:\n                        continue\n                    else:\n                        batch_meta.append({\n                            'hdf5_path': self.sampler1.hdf5_paths[index], \n                            'index_in_hdf5': self.sampler1.indexes_in_hdf5[index]})\n                        i += 1\n\n            elif self.count % 2 == 1:\n                batch_meta = []\n                i = 0\n                while i < batch_size:\n                    if len(self.sampler2.queue) == 0:\n                        self.sampler2.queue = self.sampler2.expand_queue(self.sampler2.queue)\n\n                    class_id = self.sampler2.queue.pop(0)\n                    pointer = self.sampler2.pointers_of_classes[class_id]\n                    self.sampler2.pointers_of_classes[class_id] += 1\n                    index = self.sampler2.indexes_per_class[class_id][pointer]\n                    \n                    # When finish one epoch of a sound class, then shuffle its indexes and reset pointer\n                    if self.sampler2.pointers_of_classes[class_id] >= self.sampler2.samples_num_per_class[class_id]:\n                        self.sampler2.pointers_of_classes[class_id] = 0\n                        self.sampler2.random_state.shuffle(self.sampler2.indexes_per_class[class_id])\n\n                    # If audio in black list then continue\n                    if self.sampler2.audio_names[index] in self.sampler2.black_list_names:\n                        continue\n                    else:\n                        batch_meta.append({\n                            'hdf5_path': self.sampler2.hdf5_paths[index], \n                            'index_in_hdf5': self.sampler2.indexes_in_hdf5[index]})\n                        i += 1\n\n            yield batch_meta\n\n    def state_dict(self):\n        state = {\n            'sampler1': self.sampler1.state_dict(), \n            'sampler2': self.sampler2.state_dict()}\n        return state\n\n    def load_state_dict(self, state):\n        self.sampler1.load_state_dict(state['sampler1'])\n        self.sampler2.load_state_dict(state['sampler2'])\n\n\nclass EvaluateSampler(object):\n    def __init__(self, indexes_hdf5_path, batch_size):\n        \"\"\"Evaluate sampler. Generate batch meta for evaluation.\n        \n        Args:\n          indexes_hdf5_path: string\n          batch_size: int\n        \"\"\"\n        self.batch_size = batch_size\n\n        with h5py.File(indexes_hdf5_path, 'r') as hf:\n            self.audio_names = [audio_name.decode() for audio_name in hf['audio_name'][:]]\n            self.hdf5_paths = [hdf5_path.decode() for hdf5_path in hf['hdf5_path'][:]]\n            self.indexes_in_hdf5 = hf['index_in_hdf5'][:]\n            self.targets = hf['target'][:].astype(np.float32)\n            \n        self.audios_num = len(self.audio_names)\n\n    def __iter__(self):\n        \"\"\"Generate batch meta for training. \n        \n        Returns:\n          batch_meta: e.g.: [\n            {'hdf5_path': string, \n             'index_in_hdf5': int}\n            ...]\n        \"\"\"\n        batch_size = self.batch_size\n        pointer = 0\n\n        while pointer < self.audios_num:\n            batch_indexes = np.arange(pointer, \n                min(pointer + batch_size, self.audios_num))\n\n            batch_meta = []\n\n            for index in batch_indexes:\n                batch_meta.append({\n                    'audio_name': self.audio_names[index], \n                    'hdf5_path': self.hdf5_paths[index], \n                    'index_in_hdf5': self.indexes_in_hdf5[index], \n                    'target': self.targets[index]})\n\n            pointer += batch_size\n            yield batch_meta\n\n\ndef collate_fn(list_data_dict):\n    \"\"\"Collate data.\n    Args:\n      list_data_dict, e.g., [{'audio_name': str, 'waveform': (clip_samples,), ...}, \n                             {'audio_name': str, 'waveform': (clip_samples,), ...},\n                             ...]\n    Returns:\n      np_data_dict, dict, e.g.,\n          {'audio_name': (batch_size,), 'waveform': (batch_size, clip_samples), ...}\n    \"\"\"\n    np_data_dict = {}\n    \n    for key in list_data_dict[0].keys():\n        np_data_dict[key] = np.array([data_dict[key] for data_dict in list_data_dict])\n    \n    return np_data_dict","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:33:51.298377Z","iopub.execute_input":"2021-06-21T19:33:51.298744Z","iopub.status.idle":"2021-06-21T19:33:51.358469Z","shell.execute_reply.started":"2021-06-21T19:33:51.298716Z","shell.execute_reply":"2021-06-21T19:33:51.357458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Pytorch_utils\ndef move_data_to_device(x, device):\n    if 'float' in str(x.dtype):\n        x = torch.Tensor(x)\n    elif 'int' in str(x.dtype):\n        x = torch.LongTensor(x)\n    else:\n        return x\n\n    return x.to(device)\n\n\ndef do_mixup(x, mixup_lambda):\n    \"\"\"Mixup x of even indexes (0, 2, 4, ...) with x of odd indexes \n    (1, 3, 5, ...).\n\n    Args:\n      x: (batch_size * 2, ...)\n      mixup_lambda: (batch_size * 2,)\n\n    Returns:\n      out: (batch_size, ...)\n    \"\"\"\n    out = (x[0 :: 2].transpose(0, -1) * mixup_lambda[0 :: 2] + \\\n        x[1 :: 2].transpose(0, -1) * mixup_lambda[1 :: 2]).transpose(0, -1)\n    return out\n    \n\ndef append_to_dict(dict, key, value):\n    if key in dict.keys():\n        dict[key].append(value)\n    else:\n        dict[key] = [value]\n\n\ndef forward(model, generator, return_input=False, \n    return_target=False):\n    \"\"\"Forward data to a model.\n    \n    Args: \n      model: object\n      generator: object\n      return_input: bool\n      return_target: bool\n\n    Returns:\n      audio_name: (audios_num,)\n      clipwise_output: (audios_num, classes_num)\n      (ifexist) segmentwise_output: (audios_num, segments_num, classes_num)\n      (ifexist) framewise_output: (audios_num, frames_num, classes_num)\n      (optional) return_input: (audios_num, segment_samples)\n      (optional) return_target: (audios_num, classes_num)\n    \"\"\"\n    output_dict = {}\n    device = next(model.parameters()).device\n    time1 = time.time()\n\n    # Forward data to a model in mini-batches\n    for n, batch_data_dict in enumerate(generator):\n        print(n)\n        batch_waveform = move_data_to_device(batch_data_dict['waveform'], device)\n        \n        with torch.no_grad():\n            model.eval()\n            batch_output = model(batch_waveform)\n\n        append_to_dict(output_dict, 'audio_name', batch_data_dict['audio_name'])\n\n        append_to_dict(output_dict, 'clipwise_output', \n            batch_output['clipwise_output'].data.cpu().numpy())\n\n        if 'segmentwise_output' in batch_output.keys():\n            append_to_dict(output_dict, 'segmentwise_output', \n                batch_output['segmentwise_output'].data.cpu().numpy())\n\n        if 'framewise_output' in batch_output.keys():\n            append_to_dict(output_dict, 'framewise_output', \n                batch_output['framewise_output'].data.cpu().numpy())\n            \n        if return_input:\n            append_to_dict(output_dict, 'waveform', batch_data_dict['waveform'])\n            \n        if return_target:\n            if 'target' in batch_data_dict.keys():\n                append_to_dict(output_dict, 'target', batch_data_dict['target'])\n\n        if n % 10 == 0:\n            print(' --- Inference time: {:.3f} s / 10 iterations ---'.format(\n                time.time() - time1))\n            time1 = time.time()\n\n    for key in output_dict.keys():\n        output_dict[key] = np.concatenate(output_dict[key], axis=0)\n\n    return output_dict\n\n\ndef interpolate(x, ratio):\n    \"\"\"Interpolate data in time domain. This is used to compensate the \n    resolution reduction in downsampling of a CNN.\n    \n    Args:\n      x: (batch_size, time_steps, classes_num)\n      ratio: int, ratio to interpolate\n\n    Returns:\n      upsampled: (batch_size, time_steps * ratio, classes_num)\n    \"\"\"\n    (batch_size, time_steps, classes_num) = x.shape\n    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n    return upsampled\n\n\ndef pad_framewise_output(framewise_output, frames_num):\n    \"\"\"Pad framewise_output to the same length as input frames. The pad value \n    is the same as the value of the last frame.\n\n    Args:\n      framewise_output: (batch_size, frames_num, classes_num)\n      frames_num: int, number of frames to pad\n\n    Outputs:\n      output: (batch_size, frames_num, classes_num)\n    \"\"\"\n    pad = framewise_output[:, -1 :, :].repeat(1, frames_num - framewise_output.shape[1], 1)\n    \"\"\"tensor for padding\"\"\"\n\n    output = torch.cat((framewise_output, pad), dim=1)\n    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n\n    return output\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef count_flops(model, audio_length):\n    \"\"\"Count flops. Code modified from others' implementation.\n    \"\"\"\n    multiply_adds = True\n    list_conv2d=[]\n    def conv2d_hook(self, input, output):\n        batch_size, input_channels, input_height, input_width = input[0].size()\n        output_channels, output_height, output_width = output[0].size()\n \n        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups) * (2 if multiply_adds else 1)\n        bias_ops = 1 if self.bias is not None else 0\n \n        params = output_channels * (kernel_ops + bias_ops)\n        flops = batch_size * params * output_height * output_width\n \n        list_conv2d.append(flops)\n\n    list_conv1d=[]\n    def conv1d_hook(self, input, output):\n        batch_size, input_channels, input_length = input[0].size()\n        output_channels, output_length = output[0].size()\n \n        kernel_ops = self.kernel_size[0] * (self.in_channels / self.groups) * (2 if multiply_adds else 1)\n        bias_ops = 1 if self.bias is not None else 0\n \n        params = output_channels * (kernel_ops + bias_ops)\n        flops = batch_size * params * output_length\n \n        list_conv1d.append(flops)\n \n    list_linear=[] \n    def linear_hook(self, input, output):\n        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n \n        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n        bias_ops = self.bias.nelement()\n \n        flops = batch_size * (weight_ops + bias_ops)\n        list_linear.append(flops)\n \n    list_bn=[] \n    def bn_hook(self, input, output):\n        list_bn.append(input[0].nelement() * 2)\n \n    list_relu=[] \n    def relu_hook(self, input, output):\n        list_relu.append(input[0].nelement() * 2)\n \n    list_pooling2d=[]\n    def pooling2d_hook(self, input, output):\n        batch_size, input_channels, input_height, input_width = input[0].size()\n        output_channels, output_height, output_width = output[0].size()\n \n        kernel_ops = self.kernel_size * self.kernel_size\n        bias_ops = 0\n        params = output_channels * (kernel_ops + bias_ops)\n        flops = batch_size * params * output_height * output_width\n \n        list_pooling2d.append(flops)\n\n    list_pooling1d=[]\n    def pooling1d_hook(self, input, output):\n        batch_size, input_channels, input_length = input[0].size()\n        output_channels, output_length = output[0].size()\n \n        kernel_ops = self.kernel_size[0]\n        bias_ops = 0\n        \n        params = output_channels * (kernel_ops + bias_ops)\n        flops = batch_size * params * output_length\n \n        list_pooling2d.append(flops)\n \n    def foo(net):\n        childrens = list(net.children())\n        if not childrens:\n            if isinstance(net, nn.Conv2d):\n                net.register_forward_hook(conv2d_hook)\n            elif isinstance(net, nn.Conv1d):\n                net.register_forward_hook(conv1d_hook)\n            elif isinstance(net, nn.Linear):\n                net.register_forward_hook(linear_hook)\n            elif isinstance(net, nn.BatchNorm2d) or isinstance(net, nn.BatchNorm1d):\n                net.register_forward_hook(bn_hook)\n            elif isinstance(net, nn.ReLU):\n                net.register_forward_hook(relu_hook)\n            elif isinstance(net, nn.AvgPool2d) or isinstance(net, nn.MaxPool2d):\n                net.register_forward_hook(pooling2d_hook)\n            elif isinstance(net, nn.AvgPool1d) or isinstance(net, nn.MaxPool1d):\n                net.register_forward_hook(pooling1d_hook)\n            else:\n                print('Warning: flop of module {} is not counted!'.format(net))\n            return\n        for c in childrens:\n            foo(c)\n\n    # Register hook\n    foo(model)\n    \n    device = device = next(model.parameters()).device\n    input = torch.rand(1, audio_length).to(device)\n\n    out = model(input)\n \n    total_flops = sum(list_conv2d) + sum(list_conv1d) + sum(list_linear) + \\\n        sum(list_bn) + sum(list_relu) + sum(list_pooling2d) + sum(list_pooling1d)\n    \n    return total_flops","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:33:52.30094Z","iopub.execute_input":"2021-06-21T19:33:52.301314Z","iopub.status.idle":"2021-06-21T19:33:52.342478Z","shell.execute_reply.started":"2021-06-21T19:33:52.301284Z","shell.execute_reply":"2021-06-21T19:33:52.341484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Utilities\ndef create_folder(fd):\n    if not os.path.exists(fd):\n        os.makedirs(fd)\n        \n        \ndef get_filename(path):\n    path = os.path.realpath(path)\n    na_ext = path.split('/')[-1]\n    na = os.path.splitext(na_ext)[0]\n    return na\n\n\ndef get_sub_filepaths(folder):\n    paths = []\n    for root, dirs, files in os.walk(folder):\n        for name in files:\n            path = os.path.join(root, name)\n            paths.append(path)\n    return paths\n    \n    \ndef create_logging(log_dir, filemode):\n    create_folder(log_dir)\n    i1 = 0\n\n    while os.path.isfile(os.path.join(log_dir, '{:04d}.log'.format(i1))):\n        i1 += 1\n        \n    log_path = os.path.join(log_dir, '{:04d}.log'.format(i1))\n    logging.basicConfig(\n        level=logging.DEBUG,\n        format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n        datefmt='%a, %d %b %Y %H:%M:%S',\n        filename=log_path,\n        filemode=filemode)\n\n    # Print to console\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')\n    console.setFormatter(formatter)\n    logging.getLogger('').addHandler(console)\n    \n    return logging\n\n\ndef read_metadata(csv_path, classes_num, id_to_ix):\n    \"\"\"Read metadata of AudioSet from a csv file.\n\n    Args:\n      csv_path: str\n\n    Returns:\n      meta_dict: {'audio_name': (audios_num,), 'target': (audios_num, classes_num)}\n    \"\"\"\n\n    with open(csv_path, 'r') as fr:\n        lines = fr.readlines()\n        lines = lines[3:]   # Remove heads\n\n    audios_num = len(lines)\n    targets = np.zeros((audios_num, classes_num), dtype=np.bool)\n    audio_names = []\n \n    for n, line in enumerate(lines):\n        items = line.split(', ')\n        \"\"\"items: ['--4gqARaEJE', '0.000', '10.000', '\"/m/068hy,/m/07q6cd_,/m/0bt9lr,/m/0jbk\"\\n']\"\"\"\n\n        audio_name = 'Y{}.wav'.format(items[0])   # Audios are started with an extra 'Y' when downloading\n        label_ids = items[3].split('\"')[1].split(',')\n\n        audio_names.append(audio_name)\n\n        # Target\n        for id in label_ids:\n            ix = id_to_ix[id]\n            targets[n, ix] = 1\n    \n    meta_dict = {'audio_name': np.array(audio_names), 'target': targets}\n    return meta_dict\n\n\ndef float32_to_int16(x):\n    assert np.max(np.abs(x)) <= 1.2\n    x = np.clip(x, -1, 1)\n    return (x * 32767.).astype(np.int16)\n\ndef int16_to_float32(x):\n    return (x / 32767.).astype(np.float32)\n    \n\ndef pad_or_truncate(x, audio_length):\n    \"\"\"Pad all audio to specific length.\"\"\"\n    if len(x) <= audio_length:\n        return np.concatenate((x, np.zeros(audio_length - len(x))), axis=0)\n    else:\n        return x[0 : audio_length]\n\n\ndef d_prime(auc):\n    d_prime = stats.norm().ppf(auc) * np.sqrt(2.0)\n    return d_prime\n\n\nclass Mixup(object):\n    def __init__(self, mixup_alpha, random_seed=1234):\n        \"\"\"Mixup coefficient generator.\n        \"\"\"\n        self.mixup_alpha = mixup_alpha\n        self.random_state = np.random.RandomState(random_seed)\n\n    def get_lambda(self, batch_size):\n        \"\"\"Get mixup random coefficients.\n        Args:\n          batch_size: int\n        Returns:\n          mixup_lambdas: (batch_size,)\n        \"\"\"\n        mixup_lambdas = []\n        for n in range(0, batch_size, 2):\n            lam = self.random_state.beta(self.mixup_alpha, self.mixup_alpha, 1)[0]\n            mixup_lambdas.append(lam)\n            mixup_lambdas.append(1. - lam)\n\n        return np.array(mixup_lambdas)\n\n\nclass StatisticsContainer(object):\n    def __init__(self, statistics_path):\n        \"\"\"Contain statistics of different training iterations.\n        \"\"\"\n        self.statistics_path = statistics_path\n\n        self.backup_statistics_path = '{}_{}.pkl'.format(\n            os.path.splitext(self.statistics_path)[0], \n            datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n\n        self.statistics_dict = {'bal': [], 'test': []}\n\n    def append(self, iteration, statistics, data_type):\n        statistics['iteration'] = iteration\n        self.statistics_dict[data_type].append(statistics)\n        \n    def dump(self):\n        pickle.dump(self.statistics_dict, open(self.statistics_path, 'wb'))\n        pickle.dump(self.statistics_dict, open(self.backup_statistics_path, 'wb'))\n        logging.info('    Dump statistics to {}'.format(self.statistics_path))\n        logging.info('    Dump statistics to {}'.format(self.backup_statistics_path))\n        \n    def load_state_dict(self, resume_iteration):\n        self.statistics_dict = pickle.load(open(self.statistics_path, 'rb'))\n\n        resume_statistics_dict = {'bal': [], 'test': []}\n        \n        for key in self.statistics_dict.keys():\n            for statistics in self.statistics_dict[key]:\n                if statistics['iteration'] <= resume_iteration:\n                    resume_statistics_dict[key].append(statistics)\n                \n        self.statistics_dict = resume_statistics_dict","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:33:53.368084Z","iopub.execute_input":"2021-06-21T19:33:53.368462Z","iopub.status.idle":"2021-06-21T19:33:53.396861Z","shell.execute_reply.started":"2021-06-21T19:33:53.36843Z","shell.execute_reply":"2021-06-21T19:33:53.395573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Models\ndef init_layer(layer):\n    \"\"\"Initialize a Linear or Convolutional layer. \"\"\"\n    nn.init.xavier_uniform_(layer.weight)\n \n    if hasattr(layer, 'bias'): # has attribute\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n            \n    \ndef init_bn(bn):\n    \"\"\"Initialize a Batchnorm layer. \"\"\"\n    bn.bias.data.fill_(0.)\n    bn.weight.data.fill_(1.)\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        \n        super(ConvBlock, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels=in_channels, \n                              out_channels=out_channels,\n                              kernel_size=(3, 3), stride=(1, 1),\n                              padding=(1, 1), bias=False)\n                              \n        self.conv2 = nn.Conv2d(in_channels=out_channels, \n                              out_channels=out_channels,\n                              kernel_size=(3, 3), stride=(1, 1),\n                              padding=(1, 1), bias=False)\n                              \n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.init_weight()\n        \n    def init_weight(self):\n        init_layer(self.conv1)\n        init_layer(self.conv2)\n        init_bn(self.bn1)\n        init_bn(self.bn2)\n\n        \n    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n        \n        x = input\n        x = F.relu_(self.bn1(self.conv1(x)))\n        x = F.relu_(self.bn2(self.conv2(x)))\n        if pool_type == 'max':\n            x = F.max_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg':\n            x = F.avg_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg+max':\n            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n            x2 = F.max_pool2d(x, kernel_size=pool_size)\n            x = x1 + x2\n        else:\n            raise Exception('Incorrect argument!')\n        \n        return x\n\n\nclass ConvBlock5x5(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        \n        super(ConvBlock5x5, self).__init__()\n        \n        self.conv1 = nn.Conv2d(in_channels=in_channels, \n                              out_channels=out_channels,\n                              kernel_size=(5, 5), stride=(1, 1),\n                              padding=(2, 2), bias=False)\n                              \n        self.bn1 = nn.BatchNorm2d(out_channels)\n\n        self.init_weight()\n        \n    def init_weight(self):\n        init_layer(self.conv1)\n        init_bn(self.bn1)\n\n        \n    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n        \n        x = input\n        x = F.relu_(self.bn1(self.conv1(x)))\n        if pool_type == 'max':\n            x = F.max_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg':\n            x = F.avg_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg+max':\n            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n            x2 = F.max_pool2d(x, kernel_size=pool_size)\n            x = x1 + x2\n        else:\n            raise Exception('Incorrect argument!')\n        \n        return x\n\n\nclass AttBlock(nn.Module):\n    def __init__(self, n_in, n_out, activation='linear', temperature=1.):\n        super(AttBlock, self).__init__()\n        \n        self.activation = activation\n        self.temperature = temperature\n        self.att = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n        self.cla = nn.Conv1d(in_channels=n_in, out_channels=n_out, kernel_size=1, stride=1, padding=0, bias=True)\n        \n        self.bn_att = nn.BatchNorm1d(n_out)\n        self.init_weights()\n        \n    def init_weights(self):\n        init_layer(self.att)\n        init_layer(self.cla)\n        init_bn(self.bn_att)\n         \n    def forward(self, x):\n        # x: (n_samples, n_in, n_time)\n        norm_att = torch.softmax(torch.clamp(self.att(x), -10, 10), dim=-1)\n        cla = self.nonlinear_transform(self.cla(x))\n        x = torch.sum(norm_att * cla, dim=2)\n        return x, norm_att, cla\n\n    def nonlinear_transform(self, x):\n        if self.activation == 'linear':\n            return x\n        elif self.activation == 'sigmoid':\n            return torch.sigmoid(x)\n\n\nclass Cnn14(nn.Module):\n    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n        fmax, classes_num):\n        \n        super(Cnn14, self).__init__()\n\n        window = 'hann'\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        amin = 1e-10\n        top_db = None\n\n        # Spectrogram extractor\n        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n            freeze_parameters=True)\n\n        # Logmel feature extractor\n        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n            freeze_parameters=True)\n\n        # Spec augmenter\n        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n            freq_drop_width=8, freq_stripes_num=2)\n\n        self.bn0 = nn.BatchNorm2d(64)\n\n        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n        self.conv_block2 = ConvBlock(in_channels=64, out_channels=128)\n        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n\n        self.fc1 = nn.Linear(2048, 2048, bias=True)\n        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n        \n        self.init_weight()\n\n    def init_weight(self):\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        init_layer(self.fc_audioset)\n \n    def forward(self, input, mixup_lambda=None):\n        \"\"\"\n        Input: (batch_size, data_length)\"\"\"\n\n        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n\n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n        \n        if self.training:\n            x = self.spec_augmenter(x)\n\n        # Mixup on spectrogram\n        if self.training and mixup_lambda is not None:\n            x = do_mixup(x, mixup_lambda)\n\n        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = torch.mean(x, dim=3)\n        \n        (x1, _) = torch.max(x, dim=2)\n        x2 = torch.mean(x, dim=2)\n        x = x1 + x2\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = F.relu_(self.fc1(x))\n        embedding = F.dropout(x, p=0.5, training=self.training)\n        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n        \n        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n\n        return output_dict\n\n\ndef _resnet_conv3x3(in_planes, out_planes):\n    #3x3 convolution with padding\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n                     padding=1, groups=1, bias=False, dilation=1)\n\n\ndef _resnet_conv1x1(in_planes, out_planes):\n    #1x1 convolution\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, bias=False)\n\n\nclass _ResnetBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(_ResnetBasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError('_ResnetBasicBlock only supports groups=1 and base_width=64')\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in _ResnetBasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n\n        self.stride = stride\n\n        self.conv1 = _resnet_conv3x3(inplanes, planes)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = _resnet_conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.conv1)\n        init_bn(self.bn1)\n        init_layer(self.conv2)\n        init_bn(self.bn2)\n        nn.init.constant_(self.bn2.weight, 0)\n\n    def forward(self, x):\n        identity = x\n\n        if self.stride == 2:\n            out = F.avg_pool2d(x, kernel_size=(2, 2))\n        else:\n            out = x\n\n        out = self.conv1(out)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = F.dropout(out, p=0.1, training=self.training)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        \n        if self.downsample is not None:\n            identity = self.downsample(identity)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass _ResnetBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(_ResnetBottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        self.stride = stride\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = _resnet_conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = _resnet_conv3x3(width, width)\n        self.bn2 = norm_layer(width)\n        self.conv3 = _resnet_conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.conv1)\n        init_bn(self.bn1)\n        init_layer(self.conv2)\n        init_bn(self.bn2)\n        init_layer(self.conv3)\n        init_bn(self.bn3)\n        nn.init.constant_(self.bn3.weight, 0)\n\n    def forward(self, x):\n        identity = x\n\n        if self.stride == 2:\n            x = F.avg_pool2d(x, kernel_size=(2, 2))\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = F.dropout(out, p=0.1, training=self.training)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(identity)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass _ResNet(nn.Module):\n    def __init__(self, block, layers, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None):\n        super(_ResNet, self).__init__()\n\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\"replace_stride_with_dilation should be None \"\n                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            if stride == 1:\n                downsample = nn.Sequential(\n                    _resnet_conv1x1(self.inplanes, planes * block.expansion),\n                    norm_layer(planes * block.expansion),\n                )\n                init_layer(downsample[0])\n                init_bn(downsample[1])\n            elif stride == 2:\n                downsample = nn.Sequential(\n                    nn.AvgPool2d(kernel_size=2), \n                    _resnet_conv1x1(self.inplanes, planes * block.expansion),\n                    norm_layer(planes * block.expansion),\n                )\n                init_layer(downsample[1])\n                init_bn(downsample[2])\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x\n\nclass ResNet38(nn.Module):\n    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n        fmax, classes_num):\n        \n        super(ResNet38, self).__init__()\n\n        window = 'hann'\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        amin = 1e-10\n        top_db = None\n\n        # Spectrogram extractor\n        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n            freeze_parameters=True)\n\n        # Logmel feature extractor\n        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n            freeze_parameters=True)\n\n        # Spec augmenter\n        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n            freq_drop_width=8, freq_stripes_num=2)\n\n        self.bn0 = nn.BatchNorm2d(64)\n\n        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n        # self.conv_block2 = ConvBlock(in_channels=64, out_channels=64)\n\n        self.resnet = _ResNet(block=_ResnetBasicBlock, layers=[3, 4, 6, 3], zero_init_residual=True)\n\n        self.conv_block_after1 = ConvBlock(in_channels=512, out_channels=2048)\n\n        self.fc1 = nn.Linear(2048, 2048)\n        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        init_layer(self.fc_audioset)\n\n\n    def forward(self, input, mixup_lambda=None):\n        \"\"\"\n        Input: (batch_size, data_length)\"\"\"\n\n        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n        \n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n        \n        if self.training:\n            x = self.spec_augmenter(x)\n\n        # Mixup on spectrogram\n        if self.training and mixup_lambda is not None:\n            x = do_mixup(x, mixup_lambda)\n        \n        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training, inplace=True)\n        x = self.resnet(x)\n        x = F.avg_pool2d(x, kernel_size=(2, 2))\n        x = F.dropout(x, p=0.2, training=self.training, inplace=True)\n        x = self.conv_block_after1(x, pool_size=(1, 1), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training, inplace=True)\n        x = torch.mean(x, dim=3)\n        \n        (x1, _) = torch.max(x, dim=2)\n        x2 = torch.mean(x, dim=2)\n        x = x1 + x2\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = F.relu_(self.fc1(x))\n        embedding = F.dropout(x, p=0.5, training=self.training)\n        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n        \n        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n\n        return output_dict\n\nclass ConvPreWavBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        \n        super(ConvPreWavBlock, self).__init__()\n        \n        self.conv1 = nn.Conv1d(in_channels=in_channels, \n                              out_channels=out_channels,\n                              kernel_size=3, stride=1,\n                              padding=1, bias=False)\n                              \n        self.conv2 = nn.Conv1d(in_channels=out_channels, \n                              out_channels=out_channels,\n                              kernel_size=3, stride=1, dilation=2, \n                              padding=2, bias=False)\n                              \n        self.bn1 = nn.BatchNorm1d(out_channels)\n        self.bn2 = nn.BatchNorm1d(out_channels)\n\n        self.init_weight()\n        \n    def init_weight(self):\n        init_layer(self.conv1)\n        init_layer(self.conv2)\n        init_bn(self.bn1)\n        init_bn(self.bn2)\n\n        \n    def forward(self, input, pool_size):\n        \n        x = input\n        x = F.relu_(self.bn1(self.conv1(x)))\n        x = F.relu_(self.bn2(self.conv2(x)))\n        x = F.max_pool1d(x, kernel_size=pool_size)\n        \n        return x\n    \nclass Wavegram_Logmel_Cnn14(nn.Module):\n    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n        fmax, classes_num):\n        \n        super(Wavegram_Logmel_Cnn14, self).__init__()\n\n        window = 'hann'\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        amin = 1e-10\n        top_db = None\n\n        self.pre_conv0 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=11, stride=5, padding=5, bias=False)\n        self.pre_bn0 = nn.BatchNorm1d(64)\n        self.pre_block1 = ConvPreWavBlock(64, 64)\n        self.pre_block2 = ConvPreWavBlock(64, 128)\n        self.pre_block3 = ConvPreWavBlock(128, 128)\n        self.pre_block4 = ConvBlock(in_channels=4, out_channels=64)\n\n        # Spectrogram extractor\n        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n            freeze_parameters=True)\n\n        # Logmel feature extractor\n        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n            freeze_parameters=True)\n\n        # Spec augmenter\n        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n            freq_drop_width=8, freq_stripes_num=2)\n\n        self.bn0 = nn.BatchNorm2d(64)\n\n        self.conv_block1 = ConvBlock(in_channels=1, out_channels=64)\n        self.conv_block2 = ConvBlock(in_channels=128, out_channels=128)\n        self.conv_block3 = ConvBlock(in_channels=128, out_channels=256)\n        self.conv_block4 = ConvBlock(in_channels=256, out_channels=512)\n        self.conv_block5 = ConvBlock(in_channels=512, out_channels=1024)\n        self.conv_block6 = ConvBlock(in_channels=1024, out_channels=2048)\n\n        self.fc1 = nn.Linear(2048, 2048, bias=True)\n        self.fc_audioset = nn.Linear(2048, classes_num, bias=True)\n        \n        self.init_weight()\n\n    def init_weight(self):\n        init_layer(self.pre_conv0)\n        init_bn(self.pre_bn0)\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        init_layer(self.fc_audioset)\n \n    def forward(self, input, mixup_lambda=None):\n        \"\"\"\n        Input: (batch_size, data_length)\"\"\"\n\n        # Wavegram\n        a1 = F.relu_(self.pre_bn0(self.pre_conv0(input[:, None, :])))\n        a1 = self.pre_block1(a1, pool_size=4)\n        a1 = self.pre_block2(a1, pool_size=4)\n        a1 = self.pre_block3(a1, pool_size=4)\n        a1 = a1.reshape((a1.shape[0], -1, 32, a1.shape[-1])).transpose(2, 3)\n        a1 = self.pre_block4(a1, pool_size=(2, 1))\n\n        # Log mel spectrogram\n        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n        \n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n\n        if self.training:\n            x = self.spec_augmenter(x)\n\n        # Mixup on spectrogram\n        if self.training and mixup_lambda is not None:\n            x = do_mixup(x, mixup_lambda)\n            a1 = do_mixup(a1, mixup_lambda)\n        \n        x = self.conv_block1(x, pool_size=(2, 2), pool_type='avg')\n\n        # Concatenate Wavegram and Log mel spectrogram along the channel dimension\n        x = torch.cat((x, a1), dim=1)\n\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block2(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block3(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block4(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block5(x, pool_size=(2, 2), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = self.conv_block6(x, pool_size=(1, 1), pool_type='avg')\n        x = F.dropout(x, p=0.2, training=self.training)\n        x = torch.mean(x, dim=3)\n        \n        (x1, _) = torch.max(x, dim=2)\n        x2 = torch.mean(x, dim=2)\n        x = x1 + x2\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = F.relu_(self.fc1(x))\n        embedding = F.dropout(x, p=0.5, training=self.training)\n        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n        \n        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n\n        return output_dict\n    \nclass MobileNetV1(nn.Module):\n    def __init__(self, sample_rate, window_size, hop_size, mel_bins, fmin, \n        fmax, classes_num):\n        \n        super(MobileNetV1, self).__init__()\n\n        window = 'hann'\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        amin = 1e-10\n        top_db = None\n\n        # Spectrogram extractor\n        self.spectrogram_extractor = Spectrogram(n_fft=window_size, hop_length=hop_size, \n            win_length=window_size, window=window, center=center, pad_mode=pad_mode, \n            freeze_parameters=True)\n\n        # Logmel feature extractor\n        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size, \n            n_mels=mel_bins, fmin=fmin, fmax=fmax, ref=ref, amin=amin, top_db=top_db, \n            freeze_parameters=True)\n\n        # Spec augmenter\n        self.spec_augmenter = SpecAugmentation(time_drop_width=64, time_stripes_num=2, \n            freq_drop_width=8, freq_stripes_num=2)\n\n        self.bn0 = nn.BatchNorm2d(64)\n\n        def conv_bn(inp, oup, stride):\n            _layers = [\n                nn.Conv2d(inp, oup, 3, 1, 1, bias=False), \n                nn.AvgPool2d(stride), \n                nn.BatchNorm2d(oup), \n                nn.ReLU(inplace=True)\n                ]\n            _layers = nn.Sequential(*_layers)\n            init_layer(_layers[0])\n            init_bn(_layers[2])\n            return _layers\n\n        def conv_dw(inp, oup, stride):\n            _layers = [\n                nn.Conv2d(inp, inp, 3, 1, 1, groups=inp, bias=False), \n                nn.AvgPool2d(stride), \n                nn.BatchNorm2d(inp), \n                nn.ReLU(inplace=True), \n                nn.Conv2d(inp, oup, 1, 1, 0, bias=False), \n                nn.BatchNorm2d(oup), \n                nn.ReLU(inplace=True)\n                ]\n            _layers = nn.Sequential(*_layers)\n            init_layer(_layers[0])\n            init_bn(_layers[2])\n            init_layer(_layers[4])\n            init_bn(_layers[5])\n            return _layers\n\n        self.features = nn.Sequential(\n            conv_bn(  1,  32, 2), \n            conv_dw( 32,  64, 1),\n            conv_dw( 64, 128, 2),\n            conv_dw(128, 128, 1),\n            conv_dw(128, 256, 2),\n            conv_dw(256, 256, 1),\n            conv_dw(256, 512, 2),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 512, 1),\n            conv_dw(512, 1024, 2),\n            conv_dw(1024, 1024, 1))\n\n        self.fc1 = nn.Linear(1024, 1024, bias=True)\n        self.fc_audioset = nn.Linear(1024, classes_num, bias=True)\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        init_layer(self.fc_audioset)\n \n    def forward(self, input, mixup_lambda=None):\n        \"\"\"\n        Input: (batch_size, data_length)\"\"\"\n\n        x = self.spectrogram_extractor(input)   # (batch_size, 1, time_steps, freq_bins)\n        x = self.logmel_extractor(x)    # (batch_size, 1, time_steps, mel_bins)\n        \n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n        \n        if self.training:\n            x = self.spec_augmenter(x)\n\n        # Mixup on spectrogram\n        if self.training and mixup_lambda is not None:\n            x = do_mixup(x, mixup_lambda)\n        \n        x = self.features(x)\n        x = torch.mean(x, dim=3)\n        \n        (x1, _) = torch.max(x, dim=2)\n        x2 = torch.mean(x, dim=2)\n        x = x1 + x2\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = F.relu_(self.fc1(x))\n        embedding = F.dropout(x, p=0.5, training=self.training)\n        clipwise_output = torch.sigmoid(self.fc_audioset(x))\n        \n        output_dict = {'clipwise_output': clipwise_output, 'embedding': embedding}\n\n        return output_dict","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:33:54.619169Z","iopub.execute_input":"2021-06-21T19:33:54.619565Z","iopub.status.idle":"2021-06-21T19:33:55.005539Z","shell.execute_reply.started":"2021-06-21T19:33:54.619527Z","shell.execute_reply":"2021-06-21T19:33:55.004403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Config\nsample_rate = 32000\nclip_samples = sample_rate * 10     # Audio clips are 10-second\n\n# Load label\nwith open('/kaggle/input/indices/class_labels_indices_clean.csv', 'r') as f:\n    reader = csv.reader(f, delimiter=',')\n    lines = list(reader)\n\nlabels = []\nids = []    # Each label has a unique id such as \"/m/068hy\"\nfor i1 in range(1, len(lines)):\n    id = lines[i1][1]\n    label = lines[i1][2]\n    ids.append(id)\n    labels.append(label)\n\nclasses_num = len(labels)\n\nlb_to_ix = {label : i for i, label in enumerate(labels)}\nix_to_lb = {i : label for i, label in enumerate(labels)}\n\nid_to_ix = {id : i for i, id in enumerate(ids)}\nix_to_id = {i : id for i, id in enumerate(ids)}\n\nfull_samples_per_class = np.array([\n        937432,  16344,   7822,  10271,   2043,  14420,    733,   1511,\n         1258,    424,   1751,    704,    369,    590,   1063,   1375,\n         5026,    743,    853,   1648,    714,   1497,   1251,   2139,\n         1093,    133,    224,  39469,   6423,    407,   1559,   4546,\n         6826,   7464,   2468,    549,   4063,    334,    587,    238,\n         1766,    691,    114,   2153,    236,    209,    421,    740,\n          269,    959,    137,   4192,    485,   1515,    655,    274,\n           69,    157,   1128,    807,   1022,    346,     98,    680,\n          890,    352,   4169,   2061,   1753,   9883,   1339,    708,\n        37857,  18504,  12864,   2475,   2182,    757,   3624,    677,\n         1683,   3583,    444,   1780,   2364,    409,   4060,   3097,\n         3143,    502,    723,    600,    230,    852,   1498,   1865,\n         1879,   2429,   5498,   5430,   2139,   1761,   1051,    831,\n         2401,   2258,   1672,   1711,    987,    646,    794,  25061,\n         5792,   4256,     96,   8126,   2740,    752,    513,    554,\n          106,    254,   1592,    556,    331,    615,   2841,    737,\n          265,   1349,    358,   1731,   1115,    295,   1070,    972,\n          174, 937780, 112337,  42509,  49200,  11415,   6092,  13851,\n         2665,   1678,  13344,   2329,   1415,   2244,   1099,   5024,\n         9872,  10948,   4409,   2732,   1211,   1289,   4807,   5136,\n         1867,  16134,  14519,   3086,  19261,   6499,   4273,   2790,\n         8820,   1228,   1575,   4420,   3685,   2019,    664,    324,\n          513,    411,    436,   2997,   5162,   3806,   1389,    899,\n         8088,   7004,   1105,   3633,   2621,   9753,   1082,  26854,\n         3415,   4991,   2129,   5546,   4489,   2850,   1977,   1908,\n         1719,   1106,   1049,    152,    136,    802,    488,    592,\n         2081,   2712,   1665,   1128,    250,    544,    789,   2715,\n         8063,   7056,   2267,   8034,   6092,   3815,   1833,   3277,\n         8813,   2111,   4662,   2678,   2954,   5227,   1472,   2591,\n         3714,   1974,   1795,   4680,   3751,   6585,   2109,  36617,\n         6083,  16264,  17351,   3449,   5034,   3931,   2599,   4134,\n         3892,   2334,   2211,   4516,   2766,   2862,   3422,   1788,\n         2544,   2403,   2892,   4042,   3460,   1516,   1972,   1563,\n         1579,   2776,   1647,   4535,   3921,   1261,   6074,   2922,\n         3068,   1948,   4407,    712,   1294,   1019,   1572,   3764,\n         5218,    975,   1539,   6376,   1606,   6091,   1138,   1169,\n         7925,   3136,   1108,   2677,   2680,   1383,   3144,   2653,\n         1986,   1800,   1308,   1344, 122231,  12977,   2552,   2678,\n         7824,    768,   8587,  39503,   3474,    661,    430,    193,\n         1405,   1442,   3588,   6280,  10515,    785,    710,    305,\n          206,   4990,   5329,   3398,   1771,   3022,   6907,   1523,\n         8588,  12203,    666,   2113,   7916,    434,   1636,   5185,\n         1062,    664,    952,   3490,   2811,   2749,   2848,  15555,\n          363,    117,   1494,   1647,   5886,   4021,    633,   1013,\n         5951,  11343,   2324,    243,    372,    943,    734,    242,\n         3161,    122,    127,    201,   1654,    768,    134,   1467,\n          642,   1148,   2156,   1368,   1176,    302,   1909,     61,\n          223,   1812,    287,    422,    311,    228,    748,    230,\n         1876,    539,   1814,    737,    689,   1140,    591,    943,\n          353,    289,    198,    490,   7938,   1841,    850,    457,\n        814,    146,    551,    728,   1627,    620,    648,   1621,\n         2731,    535,     88,   1736,    736,    328,    293,   3170,\n          344,    384,   7640,    433,    215,    715,    626,    128,\n         3059,   1833,   2069,   3732,   1640,   1508,    836,    567,\n         2837,   1151,   2068,    695,   1494,   3173,    364,     88,\n          188,    740,    677,    273,   1533,    821,   1091,    293,\n          647,    318,   1202,    328,    532,   2847,    526,    721,\n          370,    258,    956,   1269,   1641,    339,   1322,   4485,\n          286,   1874,    277,    757,   1393,   1330,    380,    146,\n          377,    394,    318,    339,   1477,   1886,    101,   1435,\n          284,   1425,    686,    621,    221,    117,     87,   1340,\n          201,   1243,   1222,    651,   1899,    421,    712,   1016,\n         1279,    124,    351,    258,   7043,    368,    666,    162,\n         7664,    137,  70159,  26179,   6321,  32236,  33320,    771,\n         1169,    269,   1103,    444,    364,   2710,    121,    751,\n         1609,    855,   1141,   2287,   1940,   3943,    289])\n\nfull_samples_per_class = full_samples_per_class[71:137]","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:33:55.662878Z","iopub.execute_input":"2021-06-21T19:33:55.663379Z","iopub.status.idle":"2021-06-21T19:33:55.70479Z","shell.execute_reply.started":"2021-06-21T19:33:55.663343Z","shell.execute_reply":"2021-06-21T19:33:55.703842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(workspace,sample_rate,window_size,hop_size,mel_bins,fmin,fmax,model_type,loss_type,balanced,augmentation,\n      batch_size,learning_rate,early_stop,cuda,clip_samples,classes_num):\n    \"\"\"Train AudioSet tagging model. \n\n    Args:\n      dataset_dir: str\n      workspace: str\n      window_size: int\n      hop_size: int\n      mel_bins: int\n      model_type: str\n      loss_type: 'clip_bce'\n      balanced: 'none' | 'balanced' | 'alternate'\n      augmentation: 'none' | 'mixup'\n      batch_size: int\n      learning_rate: float\n      early_stop: int\n      accumulation_steps: int\n      cuda: bool\n    \"\"\"\n\n    # Arguments & parameters\n    num_workers = 0\n    clip_samples = clip_samples\n    classes_num = classes_num\n    loss_func = get_loss_func(loss_type)\n    resume_iteration = 0\n    data_type='full_train'\n\n    # Paths\n    black_list_csv = None\n    \n #   train_indexes_hdf5_path = r'/kaggle/input/indices/train_50_idx.h5'\n    train_indexes_hdf5_path = r'/kaggle/input/indices/train_100_idx.h5'\n\n    eval_bal_indexes_hdf5_path = os.path.join(workspace,\n                                              'balanced_train_idx.h5')\n\n    eval_test_indexes_hdf5_path = os.path.join(workspace, \n        'eval_idx.h5')\n\n    checkpoints_dir = os.path.join('./', 'checkpoints', \n        'sample_rate={},window_size={},hop_size={},mel_bins={},fmin={},fmax={}'.format(\n        sample_rate, window_size, hop_size, mel_bins, fmin, fmax), \n#        'data_type={}'.format(data_type), model_type, \n#        'loss_type={}'.format(loss_type), \n        #'balanced={}'.format(balanced), \n        #'augmentation={}'.format(augmentation), \n        'batch_size={}'.format(batch_size))\n    create_folder(checkpoints_dir)\n\n    \n    statistics_path = os.path.join('./', 'statistics', \n        'sample_rate={},window_size={},hop_size={},mel_bins={},fmin={},fmax={}'.format(\n        sample_rate, window_size, hop_size, mel_bins, fmin, fmax), \n#        'data_type={}'.format(data_type), model_type, \n#        'loss_type={}'.format(loss_type), \n        #'balanced={}'.format(balanced), \n        #'augmentation={}'.format(augmentation), \n        'batch_size={}'.format(batch_size), \n        'statistics.pkl')\n    create_folder(os.path.dirname(statistics_path))\n\n    logs_dir = os.path.join('./', 'logs',  \n        'sample_rate={},window_size={},hop_size={},mel_bins={},fmin={},fmax={}'.format(\n        sample_rate, window_size, hop_size, mel_bins, fmin, fmax), \n#        'data_type={}'.format(data_type), model_type, \n#        'loss_type={}'.format(loss_type), \n        #'balanced={}'.format(balanced), \n        #'augmentation={}'.format(augmentation), \n        'batch_size={}'.format(batch_size))\n\n    create_logging(logs_dir, filemode='w')\n    \n    if cuda==True:\n        logging.info('Using GPU.')\n#        device = xm.xla_device()\n        device = 'cuda'\n    else:\n        logging.info('Using CPU. Set --cuda flag to use GPU.')\n        device = 'cpu'\n    \n    # Model\n    Model = eval(model_type)\n    model = Model(sample_rate=sample_rate, window_size=window_size, \n        hop_size=hop_size, mel_bins=mel_bins, fmin=fmin, fmax=fmax, \n        classes_num=classes_num)\n     \n    params_num = count_parameters(model)\n    # flops_num = count_flops(model, clip_samples)\n    logging.info('Parameters num: {}'.format(params_num))\n    # logging.info('Flops num: {:.3f} G'.format(flops_num / 1e9))\n    \n    # Dataset will be used by DataLoader later. Dataset takes a meta as input \n    # and return a waveform and a target.\n    dataset = AudioSetDataset(sample_rate=sample_rate)\n\n    # Train sampler\n    if balanced == 'none':\n        Sampler = TrainSampler\n    elif balanced == 'balanced':\n        Sampler = BalancedTrainSampler\n    elif balanced == 'alternate':\n        Sampler = AlternateTrainSampler\n     \n    train_sampler = Sampler(\n        indexes_hdf5_path=train_indexes_hdf5_path, \n        batch_size=batch_size * 2 if 'mixup' in augmentation else batch_size,\n        black_list_csv=black_list_csv)\n    \n    # Evaluate sampler\n    eval_bal_sampler = EvaluateSampler(\n        indexes_hdf5_path=eval_bal_indexes_hdf5_path, batch_size=batch_size)\n\n    eval_test_sampler = EvaluateSampler(\n        indexes_hdf5_path=eval_test_indexes_hdf5_path, batch_size=batch_size)\n\n    # Data loader\n    train_loader = torch.utils.data.DataLoader(dataset=dataset, \n        batch_sampler = train_sampler , collate_fn=collate_fn, \n        num_workers=num_workers, pin_memory=False)\n    \n    eval_bal_loader = torch.utils.data.DataLoader(dataset=dataset, \n        batch_sampler=eval_bal_sampler, collate_fn=collate_fn, \n        num_workers=num_workers, pin_memory=False)\n\n    eval_test_loader = torch.utils.data.DataLoader(dataset=dataset, \n        batch_sampler=eval_test_sampler, collate_fn=collate_fn, \n        num_workers=num_workers, pin_memory=False)\n\n    if 'mixup' in augmentation:\n        mixup_augmenter = Mixup(mixup_alpha=1.)\n\n    # Evaluator\n    evaluator = Evaluator(model=model)\n        \n    # Statistics\n    statistics_container = StatisticsContainer(statistics_path)\n    \n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, \n        betas=(0.9, 0.999), eps=1e-08, weight_decay=0., amsgrad=True)\n\n    train_bgn_time = time.time()\n    \n    # Resume training\n    if resume_iteration > 0:\n        resume_checkpoint_path = os.path.join('./', 'checkpoints', \n            'sample_rate={},window_size={},hop_size={},mel_bins={},fmin={},fmax={}'.format(\n            sample_rate, window_size, hop_size, mel_bins, fmin, fmax), \n            'data_type={}'.format(data_type), model_type, \n            'loss_type={}'.format(loss_type), 'balanced={}'.format(balanced), \n            'augmentation={}'.format(augmentation), 'batch_size={}'.format(batch_size), \n            '{}_iterations.pth'.format(resume_iteration))\n\n        logging.info('Loading checkpoint {}'.format(resume_checkpoint_path))\n        checkpoint = torch.load(resume_checkpoint_path)\n        model.load_state_dict(checkpoint['model'])\n        train_sampler.load_state_dict(checkpoint['sampler'])\n        statistics_container.load_state_dict(resume_iteration)\n        iteration = checkpoint['iteration']\n\n    else:\n        iteration = 0\n    \n    # Parallel\n    print('GPU number: {}'.format(torch.cuda.device_count()))\n    model = torch.nn.DataParallel(model)\n\n    if 'cuda' in str(device):\n        model.to(device)\n    print('OK:', train_loader)\n     \n    time1 = time.time()\n    \n    for batch_data_dict in train_loader:\n        \"\"\"batch_data_dict: {\n            'audio_name': (batch_size [*2 if mixup],), \n            'waveform': (batch_size [*2 if mixup], clip_samples), \n            'target': (batch_size [*2 if mixup], classes_num), \n            (ifexist) 'mixup_lambda': (batch_size * 2,)}\n        \"\"\"\n        # Evaluate\n        if (iteration % 2000 == 0 and iteration > resume_iteration) or (iteration == 0): # Evaluer chaque 2000 iterations\n            train_fin_time = time.time()\n\n            bal_statistics = evaluator.evaluate(eval_bal_loader)\n            test_statistics = evaluator.evaluate(eval_test_loader)\n                            \n            logging.info('Validate bal mAP: {:.3f}'.format(\n                np.mean(bal_statistics['average_precision'])))\n\n            logging.info('Validate test mAP: {:.3f}'.format(\n                np.mean(test_statistics['average_precision'])))\n\n            statistics_container.append(iteration, bal_statistics, data_type='bal')\n            statistics_container.append(iteration, test_statistics, data_type='test')\n            statistics_container.dump()\n\n            train_time = train_fin_time - train_bgn_time\n            validate_time = time.time() - train_fin_time\n\n            logging.info(\n                'iteration: {}, train time: {:.3f} s, validate time: {:.3f} s'\n                    ''.format(iteration, train_time, validate_time))\n\n            logging.info('------------------------------------')\n\n            train_bgn_time = time.time()\n        \n        # Save model\n        if iteration % 15000 == 0:\n            checkpoint = {\n                'iteration': iteration, \n                'model': model.module.state_dict(), \n                'sampler': train_sampler.state_dict()}\n\n            checkpoint_path = os.path.join(\n                checkpoints_dir, '{}_iterations.pth'.format(iteration))\n                \n            torch.save(checkpoint, checkpoint_path)\n            logging.info('Model saved to {}'.format(checkpoint_path))\n        \n        # Mixup lambda\n        if 'mixup' in augmentation:\n            batch_data_dict['mixup_lambda'] = mixup_augmenter.get_lambda(\n                batch_size=len(batch_data_dict['waveform']))\n\n        # Move data to device\n        for key in batch_data_dict.keys():\n            batch_data_dict[key] = move_data_to_device(batch_data_dict[key], device)\n        \n        # Forward\n        model.train()\n\n        if 'mixup' in augmentation:\n            batch_output_dict = model(batch_data_dict['waveform'], \n                batch_data_dict['mixup_lambda'])\n            \"\"\"{'clipwise_output': (batch_size, classes_num), ...}\"\"\"\n\n            batch_target_dict = {'target': do_mixup(batch_data_dict['target'], \n                batch_data_dict['mixup_lambda'])}\n            \"\"\"{'target': (batch_size, classes_num)}\"\"\"\n        else:\n            batch_output_dict = model(batch_data_dict['waveform'], None)\n            \"\"\"{'clipwise_output': (batch_size, classes_num), ...}\"\"\"\n\n            batch_target_dict = {'target': batch_data_dict['target']}\n            \"\"\"{'target': (batch_size, classes_num)}\"\"\"\n\n        # Loss\n        loss = loss_func(batch_output_dict, batch_target_dict)\n\n        # Backward\n        loss.backward()\n        print(loss)\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        \n        if iteration % 10 == 0:\n            print('--- Iteration: {}, train time: {:.3f} s / 10 iterations ---'\\\n                .format(iteration, time.time() - time1))\n            time1 = time.time()\n        \n        # Stop learning\n        if iteration == early_stop:\n            break\n\n        iteration += 1\n","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:36:07.73723Z","iopub.execute_input":"2021-06-21T19:36:07.737732Z","iopub.status.idle":"2021-06-21T19:36:07.788369Z","shell.execute_reply.started":"2021-06-21T19:36:07.73769Z","shell.execute_reply":"2021-06-21T19:36:07.78694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"workspace=r'/kaggle/input/audioset-10-percent' \nsample_rate=32000 # default=32000\nwindow_size=1024 # default=1024\nhop_size=320 # default=320\nmel_bins=64 # default=64\nfmin=50 # default=50\nfmax=14000 # default=14000 \nmodel_type = 'Wavegram_Logmel_Cnn14'  \nloss_type='clip_bce'\nbalanced='balanced'  # default='balanced', choices=['none', 'balanced', 'alternate'])\naugmentation='mixup'  # default='mixup', choices=['none', 'mixup'])\nbatch_size=32 # default=32\nlearning_rate=1e-3  # default=1e-3\nearly_stop=15000 # default=20000\ncuda = True # default=False","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:36:09.366915Z","iopub.execute_input":"2021-06-21T19:36:09.367315Z","iopub.status.idle":"2021-06-21T19:36:09.373587Z","shell.execute_reply.started":"2021-06-21T19:36:09.367281Z","shell.execute_reply":"2021-06-21T19:36:09.372162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(workspace,sample_rate,window_size,hop_size,mel_bins,fmin,fmax,model_type,loss_type,balanced,augmentation,batch_size,learning_rate,early_stop,cuda,clip_samples,classes_num)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T19:36:14.475836Z","iopub.execute_input":"2021-06-21T19:36:14.47623Z","iopub.status.idle":"2021-06-21T19:36:14.517264Z","shell.execute_reply.started":"2021-06-21T19:36:14.476197Z","shell.execute_reply":"2021-06-21T19:36:14.515761Z"},"trusted":true},"execution_count":null,"outputs":[]}]}