{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#imports\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,Flatten,Dropout,MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\nimport random","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#image shape is defined for what we will feed into keras model\nIMAGE_SHAPE = (110, 110, 1)\n\n# will feed through data set via mapping\ndef load_and_preprocess_image(image):\n    image = tf.image.resize(image, IMAGE_SHAPE[0:2])\n    image = tf.cast(image, tf.float64)\n    image /= 255.0  # normalize to [0,1] range\n    return image\n\n# augmentations will feed through training data set only\ndef random_bright(image):\n    return tf.image.random_brightness(image, 0.1)\n            \ndef random_contrast(image):\n    return tf.image.random_contrast(image, 0.9, 1.1)\n\n# augmentation includes flips and bright/contrast changes\n# there still isn't tensorflow warping as a built in function\ndef augment_image(image, label):\n    if (label == 1 and random.random() < 0.8) or (label == 0 and random.random() < 0.3):\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n        image = random_contrast(image)\n        image = random_bright(image)\n    return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#read in dataframes\nx_train_df = pd.read_csv('../input/volcanoes_train/train_images.csv', header=None)\ny_train_df = pd.read_csv('../input/volcanoes_train/train_labels.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from https://www.kaggle.com/behcetsenturk/finding-volcanoes-with-cnn\ndef corruptedImages(data):\n    corruptedImagesIndex = []\n    for index, image in enumerate(np.resize(data, (data.shape[0], 12100))): # resize (7000, 110, 110, 1) to (7000,12100)\n        sum = 0;\n        for pixelIndex in range(0,len(image)):\n            sum += image[pixelIndex]\n            if pixelIndex == 10:\n                break\n        if sum == 0:\n            corruptedImagesIndex.append(index)\n        else:\n            sum = 0\n\n    for index, image in enumerate(np.resize(data, (data.shape[0], 12100))): # resize (7000, 110, 110, 1) to (7000,12100)\n        sum = 0;\n        for pixelIndex in range(0,len(image),110):\n            sum += image[pixelIndex]\n            if pixelIndex == 10:\n                break\n        if sum == 0 and index not in corruptedImagesIndex:\n            corruptedImagesIndex.append(index)\n        else:\n            sum = 0\n    return corruptedImagesIndex\n\ncorrupted_indexes = corruptedImages(x_train_df)\nx_train_df = x_train_df.drop(corrupted_indexes).reset_index(drop=True)\ny_train_df = y_train_df.drop(corrupted_indexes).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# look at distribution, we'll see there are 6 times volcano = 0 vs volcano = 1\ny_train_df.groupby('Volcano?')['Volcano?'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# initial data set size\nDS_SIZE = len(x_train_df)\nprint('ds size', DS_SIZE)\n# create initial dataset from tensor slices using the dataset api\nimage_ds = tf.data.Dataset.from_tensor_slices(np.resize(x_train_df, (DS_SIZE, 110, 110, 1)))\n\n# we will create an isolated volcano dataset to feed in 5 more times, with augmentation should distribute the labels nicely\nvolc_ind = y_train_df[y_train_df['Volcano?'] == 1].index\nvolca_image_ds = tf.data.Dataset.from_tensor_slices(np.resize(x_train_df.iloc[volc_ind], (len(volc_ind), 110, 110, 1)))\nvolca_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(y_train_df.iloc[volc_ind]['Volcano?'].values, tf.int32))\n\nrepeat_amt = 5\n# this doesn't duplicate data, but tells tensorflow how many times to repeat the data in the stream\nimage_ds = image_ds.concatenate(volca_image_ds.repeat(repeat_amt))\nDS_SIZE = DS_SIZE + (repeat_amt * len(volc_ind))\nprint('new ds size', DS_SIZE)\n\n# syncronized label dataset\nlabel_ds = tf.data.Dataset.from_tensor_slices(tf.cast(y_train_df['Volcano?'].values, tf.int32))\nlabel_ds = label_ds.concatenate(volca_label_ds.repeat(repeat_amt))\n\n# preprocess all images (original + 4*volcano)\nimage_ds = image_ds.map(load_and_preprocess_image)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# zip images and labels then shuffle\nimage_label_ds = tf.data.Dataset.zip((image_ds, label_ds)).shuffle(DS_SIZE)\n\nBATCH_SIZE = 64\n\n# split into training and validation\ntrain_size, val_size = int(0.99 * DS_SIZE), int(0.01 * DS_SIZE)\n\ntrain_ds = (image_label_ds\n    .take(train_size)\n    .cache()\n    .repeat()\n    .batch(BATCH_SIZE)\n    .map(augment_image)\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE))\n\nval_ds = (image_label_ds\n    .skip(train_size)\n    .cache()\n    .repeat()\n    .batch(BATCH_SIZE)\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\ndef get_model():\n    model = Sequential()\n    \n    model.add(Conv2D(64, (2, 2),  input_shape=IMAGE_SHAPE,  activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(rate=0.3))\n    \n    model.add(Conv2D(96, (4, 4), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(rate=0.45))\n\n    model.add(Conv2D(128, (5, 5), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(rate=0.4))\n    \n    model.add(Conv2D(128, (6, 6), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(rate=0.4))\n    \n    model.add(Flatten())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(rate=0.35))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    \n    return model\n\nmodel = get_model()\n\n# rmsprop seemed to have done better than other optimizers for me\nmodel.compile(optimizer='rmsprop', \n              loss='binary_crossentropy',\n              metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_steps = (train_size//BATCH_SIZE)+1\nval_steps = (val_size//BATCH_SIZE)+1\n\n# fit\nhistory = model.fit(train_ds,\n                    steps_per_epoch=train_steps, \n                    epochs=50,\n                    validation_data=val_ds, \n                    validation_steps=val_steps,\n                    callbacks=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['acc'], label=\"train acc\")\nplt.plot(history.history['val_acc'], label=\"val acc\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del x_train_df\ndel y_train_df\n\nx_test_df = pd.read_csv('../input/volcanoes_test/test_images.csv', header=None)\ny_test_df = pd.read_csv('../input/volcanoes_test/test_labels.csv')\n\nTEST_DS_SIZE = len(x_test_df)\n\n# run on testset\ntest_image_ds = tf.data.Dataset.from_tensor_slices(np.resize(x_test_df, (TEST_DS_SIZE, 110, 110, 1)))\ntest_image_ds = test_image_ds.map(load_and_preprocess_image)\ntest_label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(y_test_df['Volcano?'].values, tf.int32))\ntest_image_label_ds = tf.data.Dataset.zip((test_image_ds, test_label_ds)).batch(BATCH_SIZE)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# got 98% accuracy on test set, good first try, will likely look at images and see where its missing the most\nmodel.evaluate(test_image_label_ds, steps = (TEST_DS_SIZE//BATCH_SIZE)+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(test_image_label_ds, steps = (TEST_DS_SIZE//BATCH_SIZE)+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# from https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823\ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    return fig\n\nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_test_df['Volcano?'].values, np.round(preds))\nconf_matrix_plt = print_confusion_matrix(conf_matrix, [\"Not Volcano\", \"Volcano\"], figsize = (5,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}