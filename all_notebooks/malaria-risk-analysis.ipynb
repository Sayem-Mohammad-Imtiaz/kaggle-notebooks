{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from __future__ import print_function\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import rcParams\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import make_scorer,accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.base import TransformerMixin\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport lime\nimport lime.lime_tabular\n\n\n\n\nimport os\nprint(os.listdir(\"../input\"))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#loading and showing a sample from the dataset\ndata = pd.read_csv('../input/malaria_risk_dataset.csv')\ndata = data.drop(columns=\"location_extId\")\ndata.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#rcParams['figure.figsize'] = 12,4\nsns.set(style=\"whitegrid\",rc={'figure.figsize':(20,10),'axes.labelsize':16})\nsns.barplot(x=\"wallstructure\", y=\"nbofrooms\", hue=\"RDT_test_result\", data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.RDT_test_result.value_counts()/data.RDT_test_result.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataFrameImputer(TransformerMixin):\n    def __init__(self):\n        \"\"\"Imputer missing values.\n        -Columns of dtype are imputed with the most frequent value(mode) in column.\n        -Columns of other type are imputed with the mean of the column\"\"\"\n    \n    def fit(self, X,y=None):\n        self.fill = pd.Series([X[c].value_counts().index[0] \n                           if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n                          index = X.columns)\n        return self\n    \n    def transform(self,X,y=None):\n        return X.fillna(self.fill)\ndata = DataFrameImputer().fit_transform(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = list(data.columns)\nfeature_names = feature_names[:-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data preproccessing\n\n#scaler = StandardScaler()\nencoder = LabelEncoder()\n    \n#Standardizing the continous variables (nbofrooms & nbofroomsforsleeping)\n#data[data.columns[:2].tolist()] = scaler.fit_transform(data[data.columns[:2].tolist()])\n\nvalues = data.values\ndata = values[:,:-1]\nlabels = values[:,-1]\n    \n#encode the categorical values\ncategorical_features = [2,3,4,5,6,7,8,9,10]\ncategorical_names = {}\n\nfor feature in categorical_features:\n    le = encoder.fit(data[:,feature])\n    data[:,feature] = le.transform(data[:,feature])\n    categorical_names[feature] = le.classes_\n\n#One-Hot Encoding\n#data = pd.get_dummies(data, columns = features_to_encode)\n#X = data[data.columns[:-1]]\n#y = data[data.columns[-1]]\n\ndata = data.astype(float)\nlabels = labels.astype(int)\n    \nX_train,X_test,y_train,y_test = train_test_split(data, labels, test_size =0.20,random_state = 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc_score = make_scorer(accuracy_score)\n\n#Model 1 - Decision Tree\nmodel_dt= DecisionTreeClassifier(class_weight = 'balanced')\nmodel_dt.fit(X_train,y_train)\nmodel_dt_accuracy = accuracy_score(y_test,model_dt.predict(X_test))\n\n\n#Model 2 - Random Forest Classifier\nmodel_rf = RandomForestClassifier()\n    \nparameters_rf = {'n_estimators':[10,20,30,40,50,100,200],\n                 'max_features':['log2','sqrt','auto'],\n                 'criterion':['entropy','gini'],\n                 'max_depth':[2,3,4],\n                 'min_samples_split':[2,3,4],\n                 'min_samples_leaf':[2,3,4]\n                 }\n    \ngrid_obj_rf = GridSearchCV(model_rf, parameters_rf, scoring = acc_score,cv = 5)\ngrid_obj_rf = grid_obj_rf.fit(X_train, y_train)\nmodel_rf = grid_obj_rf.best_estimator_\n#print(\"the best estimator values are: \",model_rf)\nmodel_rf.fit(X_train,y_train)\nmodel_rf_accuracy = accuracy_score(y_test, model_rf.predict(X_test))\nfeature_importances_rf = model_rf.feature_importances_\n\n\n#Model 3 - AdaBoost Classifier\nmodel_adaboost = AdaBoostClassifier()\n    \nparameters_adaboost = {'base_estimator':[model_dt],\n                       'learning_rate':[k for k in np.arange(0.001,1,0.01)],\n                       'n_estimators':[k for k in range(10,100,10)]}\n    \ngrid_obj_adaboost = GridSearchCV(model_adaboost, parameters_adaboost, scoring = acc_score,cv=5)\ngrid_obj_adaboost = grid_obj_adaboost.fit(X_train, y_train)\nmodel_adaboost = grid_obj_adaboost.best_estimator_\n#print(\"the best estimator values are: \",model_adaboost)\nmodel_adaboost.fit(X_train,y_train)\nmodel_adaboost_accuracy = accuracy_score(y_test, model_adaboost.predict(X_test))\nfeature_importances_adaboost = model_adaboost.feature_importances_\n  \n#Model 4 - XGBoost Classifier\n#def xgboost(X_train,X_test,y_train,y_test,useTrainCV=True,cv_folds=5,early_stopping_rounds = 50):\nuseTrainCV = True\n\nmodel_xgboost = XGBClassifier(learning_rate = 0.1, n_estimators=1000, max_depth = 5, \n                              min_child_weight = 1,gamma = 0, subsample = 0.8,\n                              colsample_bytree = 0.8, objective = 'binary:logistic',\n                              scale_pos_weight=1, seed =27)\n    \nif useTrainCV:\n    xgb_param = model_xgboost.get_xgb_params()\n    xgtrain = xgb.DMatrix(X_train, label=y_train)\n    cvresult = xgb.cv(xgb_param,xgtrain,num_boost_round=model_xgboost.get_params()['n_estimators'],\n                    nfold = 5, metrics='auc',early_stopping_rounds = 50)\n    model_xgboost.set_params(n_estimators=cvresult.shape[0])\n    model_xgboost.fit(X_train,y_train)\n    model_xgboost_accuracy = accuracy_score(y_test, model_xgboost.predict(X_test))\n    feature_importances_xgboost = model_xgboost.feature_importances_\n  \n        \n        \n#Model 5 - Neural Net\nmodel_nn = MLPClassifier()\n  \nparameters_nn = {'hidden_layer_sizes' : [(8,),(20,),(100,)],\n                  'activation' : ['tanh','relu'],\n                  'solver' : ['lbfgs','adam'],\n                  'max_iter' : [1000]}\ngrid_obj_nn = GridSearchCV(model_nn, parameters_nn, scoring = acc_score, cv = 5,n_jobs =-1,verbose=1)\ngrid_obj_nn = grid_obj_nn.fit(X_train, y_train)\nmodel_nn = grid_obj_nn.best_estimator_\nmodel_nn.fit(X_train,y_train)\nmodel_nn_accuracy = accuracy_score(y_test, model_nn.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel_accuracies = [model_dt_accuracy, model_rf_accuracy, model_adaboost_accuracy, model_xgboost_accuracy, model_nn_accuracy]\nmodel_results = pd.DataFrame(\n    {\"Algorithm\":[\"DT\",\n                  \"RF\",\n                  \"AdaBoost\",\n                  \"XGBoost\",\n                  \"MLP\"\n                 ],\n    \"Accuracy\": model_accuracies})\n\nmodel_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ngraph = sns.barplot(\"Accuracy\",\"Algorithm\",data = model_results, palette=\"Set3\",orient = \"h\")\n#graph = graph.set_title(\"Algorithms Scores\",size = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = feature_names\nsizes = feature_importances_rf \nexplode = (0,0.1,0,0,0,0,0,0,0.2,0,0)\n\nfig, ax = plt.subplots(figsize = (20,18))\npatches, texts ,autotexts = ax.pie(sizes,labels= feature_names,explode = explode, autopct='%1.1f%%',\n                                   shadow=True, startangle=90)\nax.axis('equal') \nplt.setp(texts, size = 12, weight = \"bold\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#explaining model predictions with lime\n\npredict_fn  = lambda x : model_rf.predict_proba(x).astype(float)\n#predict_xgboost  = lambda x : model_rf.predict_proba(x).astype(float)\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names = feature_names , class_names =['0','1'], \n                                                   categorical_features = categorical_features , categorical_names = categorical_names ,\n                                                   kernel_width =3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pick the observation in the test set for which explanation is required\nobservation_1 = 6\nexp = explainer.explain_instance(X_test[observation_1], predict_fn , num_features = 5)\nexp.show_in_notebook(show_all = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above house has 0.56 probability of being positive (vulnerable to malaria) and the reasons for that is the house has :-**\n\n* Wall made by wood and mud   \n* Open eaves\n* Floor made my Earth, dung or sand\n* The insectsides was not sprayed in the house for last 12 months\n\nOn the other hand, it has 0.41 probability of being negative because of having  one ***number of rooms for sleeping***.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"observation_1 = 22\nexp = explainer.explain_instance(X_test[observation_1], predict_fn , num_features = 5)\nexp.show_in_notebook(show_all = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above house has 0. 59 probability of being negative (not vulnerable to malaria) and the reasons for that is the house has :-**\n\n*   1 room for sleeping\n*   Wall made brick and/or blocks\n*   Screened eaves\n*   Floor made of cement ."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}