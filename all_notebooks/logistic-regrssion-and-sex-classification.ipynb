{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"299891f7-4c59-5dc0-90a5-af4d7eb8949b"},"source":"This script is the second in my series of following Andrew Ng's ML course on [Coursera][1]. In my [previous kernel][2], I explored the SVM classification algorithm for detection of breast cancer. In this script I will use a logistic regression classifier to try and predict whether an athlete is a male or a female based on their height, weight and age.\n\n 1. How accurately can we guess the sex based on these features with a simple logistic regression?\n 2. Linear vs polynomial model\n 3. Accuracy vs ROC as success metrics\n\n  [1]: https://www.coursera.org/learn/machine-learning/home/welcome\n  [2]: https://www.kaggle.com/drgilermo/d/uciml/breast-cancer-wisconsin-data/exploration-of-svm"},{"cell_type":"markdown","metadata":{"_cell_guid":"c32b1230-e64b-e688-b33e-7877e0a3324e"},"source":"Load Libraries:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e5056287-d4d3-1218-61bf-b03005c3183a"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.decomposition import PCA\n\n\nplt.style.use('fivethirtyeight')\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"3e36d4ea-ec32-17c8-9ba7-e9e421efd905"},"source":"Read the data, remove NaNs and add the Age feature based on the date of birth:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"956b02ca-d78f-f328-630e-09b3e05b716c"},"outputs":[],"source":"#Read The data\ndf = pd.read_csv('../input/athletes.csv')\n\n#Add the Age feature\nfor i,row in enumerate(df.iterrows()):\n    try:\n        df.loc[i,'Age'] = 116 - float(row[1].dob[len(row[1].dob)-2:len(row[1].dob)])\n    except TypeError:\n        df.loc[i,'Age'] = 0\n        \n        \ndf['BMI'] = np.true_divide(df.weight,df.height*df.height)\n\n#Get rid of NaNs\ndf = df[np.isnan(df.height) == 0]\ndf = df[np.isnan(df.weight) == 0]\ndf = df[np.isnan(df.Age) == 0]\ndf = df[df.Age<100]\ndf = df[np.isnan(df.BMI) == 0]"},{"cell_type":"markdown","metadata":{"_cell_guid":"893bc303-dd44-4911-0b7d-f0634b9d8ffc"},"source":"Let's explore the correlations and see which features separate the females\\males populations:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d434a82d-566a-c2fa-0e54-3679a3f397c0"},"outputs":[],"source":"g = sns.PairGrid(df[['height','weight','Age','BMI','sex']],hue='sex')\ng = g.map_diag(plt.hist)\ng = g.map_offdiag(plt.scatter, s = 3, alpha = 0.2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"ac09cc01-1096-6ae4-37fd-cc4c0ee096d4"},"source":"Age and BMI aren't very good predictors, whereas weight and height, somewhat unsurprisingly, are.\n\nLet's split the data into train and validation and start classifying:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7826f58-ce54-3fb4-296d-edce00dae9ff"},"outputs":[],"source":"X = df[['height','weight']]\ny = np.zeros(len(df))\ny[df.sex.values == 'female'] = 1\nX['target'] = y\n\n\ntraindf, testdf = train_test_split(X, test_size = 0.2)"},{"cell_type":"markdown","metadata":{"_cell_guid":"74c6c8de-c957-f1d3-e061-344d4ddce037"},"source":"First let's check the effect of the regularization:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ccb3dbc4-77ee-9a9b-8103-d47d01c5367a"},"outputs":[],"source":"trainning = []\nvalidation = []\nreg = np.linspace(0.1,10,100)\nfor C in reg:\n    clf = LogisticRegression(penalty='l2',C= C )\n    clf.fit(traindf.drop(['target'], axis = 1), traindf.target)\n    trainning.append(clf.score(traindf.drop(['target'],axis = 1),traindf.target))\n    validation.append(clf.score(testdf.drop(['target'],axis = 1),testdf.target))\n    \nplt.plot(reg,trainning)\nplt.plot(reg,validation)\nplt.legend(['Trainning','Validation'])\nplt.xlabel('Regularization Strength (Inverse)')\nplt.ylabel('Accuracy')"},{"cell_type":"markdown","metadata":{"_cell_guid":"1c539a96-1816-9543-5de6-5cf3a7f6811e"},"source":"We don't see a large effect in this scale. let's pick C = 2 and visualize the decision boundary produced by the classifier on the data:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd285768-fce2-f662-c132-18d600f2afc0"},"outputs":[],"source":"clf = LogisticRegression(penalty='l2',C= 2 )\nclf.fit(traindf.drop(['target'], axis = 1), traindf.target)\n\nprint('Training Accuracy.....',clf.score(traindf.drop(['target'],axis = 1),traindf.target))\nprediction = clf.predict(testdf.drop(['target'], axis = 1))\nprint('Validation Accuracy....',clf.score(testdf.drop(['target'],axis = 1),testdf.target))\nloss = prediction - testdf['target']\naccuracy = 1 - np.true_divide(sum(np.abs(loss)),len(loss))\n\nradius = np.linspace(min(X.height), max(X.height), 100)\nline = (-clf.coef_[0][0]/clf.coef_[0][1])*radius + np.ones(len(radius))*(-clf.intercept_/clf.coef_[0][1])\nplt.plot(radius,line)\n      \nplt.plot(X['height'][X.target == 0] + np.random.normal(0,0.01,len(X[X.target == 0])),X['weight'][X.target == 0],'o',color = 'b', alpha = 0.2, markersize = 5)\nplt.plot(X['height'][X.target == 1] + np.random.normal(0,0.01,len(X[X.target == 1])),X['weight'][X.target == 1],'o',color = 'r', alpha = 0.2, markersize = 5)\nplt.legend(['Decision Line','Male','Female'])\nplt.title('Logistic Regression. Accuracy:' + str(accuracy)[0:4])\nplt.xlabel('Height')\nplt.ylabel('Weight')\nplt.xlim([min(X.height),max(X.height)])\nplt.ylim([min(X.weight),max(X.weight)])\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"f5235eab-a8aa-0e35-7199-5a567074d9a6"},"source":"With these results as a benchmark, let's see if we can improve with by adding a polynomial component: height^2:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f234765-5a67-3b0b-9bdc-ce21690b29a6"},"outputs":[],"source":"clf = LogisticRegression(penalty='l2',C= 0.1 )\n\ntraindf['height^2'] = traindf['height']*traindf['height']\ntestdf['height^2'] = testdf['height']*testdf['height']\n\nclf.fit(traindf.drop(['target'], axis = 1), traindf.target)\n\nprint('Training Accuracy.....',clf.score(traindf.drop(['target'],axis = 1),traindf.target))\nprediction = clf.predict(testdf.drop(['target'], axis = 1))\nprint('Validation Accuracy....',clf.score(testdf.drop(['target'],axis = 1),testdf.target))\nloss = prediction - testdf['target']\naccuracy = 1 - np.true_divide(sum(np.abs(loss)),len(loss))\n\nradius = np.linspace(min(X.height), max(X.height), 100)\ncurve= (-clf.coef_[0][0]/clf.coef_[0][1])*radius +(-clf.coef_[0][2]/clf.coef_[0][1])*radius**2 + np.ones(len(radius))*(-clf.intercept_/clf.coef_[0][1])\nplt.plot(radius,curve)\n      \nplt.plot(X['height'][X.target == 0] + np.random.normal(0,0.01,len(X[X.target == 0])),X['weight'][X.target == 0],'o',color = 'b', alpha = 0.2, markersize = 5)\nplt.plot(X['height'][X.target == 1] + np.random.normal(0,0.01,len(X[X.target == 1])),X['weight'][X.target == 1],'o',color = 'r', alpha = 0.2, markersize = 5)\nplt.legend(['Decision Line','Male','Female'])\nplt.title('Logistic Regression. Accuracy:' + str(accuracy)[0:4])\nplt.xlabel('Height')\nplt.ylabel('Weight')\nplt.xlim([min(X.height),max(X.height)])\nplt.ylim([min(X.weight),max(X.weight)])\n\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"e1c5b522-c300-bb39-b5bd-7471b402d0a2"},"source":"The accuracy does not improve. Maybe if we add in the other continuous features - Age, and the engineered BMI (which is simply a kind of a polynomial feature):"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5025293e-7661-5532-df95-1dbb5bf7bd45"},"outputs":[],"source":"X = df[['height','weight','Age','BMI']]\ny = np.zeros(len(df))\ny[df.sex.values == 'female'] = 1\nX['target'] = y\n\n\ntraindf, testdf = train_test_split(X, test_size = 0.2)\n\nclf = LogisticRegression(penalty='l2',C= 2 )\nclf.fit(traindf.drop(['target'], axis = 1), traindf.target)\n\nprint('Training Accuracy.....',clf.score(traindf.drop(['target'],axis = 1),traindf.target))\nprediction = clf.predict(testdf.drop(['target'], axis = 1))\nprint('Validation Accuracy....',clf.score(testdf.drop(['target'],axis = 1),testdf.target))"},{"cell_type":"markdown","metadata":{"_cell_guid":"aef59dc0-8b47-d035-9684-4fe780ca470f"},"source":"Let's try a different approach to assessing the classifier performance. the ROC (and the area under the curve) instead of accuracy:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"40baec32-5350-18e2-2110-979a1de105d9"},"outputs":[],"source":"X = df[['height','weight','Age','BMI']]\ny = np.zeros(len(df))\ny[df.sex.values == 'female'] = 1\nX['target'] = y\n\ntraindf, testdf = train_test_split(X, test_size = 0.2)\n\nprobas_1 = clf.fit(traindf.drop(['target'], axis = 1), traindf.target).predict_proba(testdf.drop(['target'],axis = 1))\nprobas_2 = clf.fit(traindf.drop(['target','Age','BMI'], axis = 1), traindf.target).predict_proba(testdf.drop(['target','Age','BMI'],axis = 1))\n\ntraindf['height^2'] = traindf['height']*traindf['height']\ntestdf['height^2'] = testdf['height']*testdf['height']\nprobas_3 = clf.fit(traindf.drop(['target','Age','BMI'], axis = 1), traindf.target).predict_proba(testdf.drop(['target','Age','BMI'],axis = 1))\n\n\nfpr, tpr, thresholds = roc_curve(testdf['target'], probas_1[:, 1])\nplt.plot(fpr, tpr, linewidth = 1)\nfpr, tpr, thresholds = roc_curve(testdf['target'], probas_2[:, 1])\nplt.plot(fpr, tpr, linewidth = 2)\nfpr, tpr, thresholds = roc_curve(testdf['target'], probas_3[:, 1])\nplt.plot(fpr, tpr, linewidth = 1)\nplt.plot([0,1],[0,1], linewidth = 1)\n\nplt.legend(['Linear - 4 features','Linear - 2 features','Deg 2 Polynomial - 2 features','Random Guess'])\nplt.title('ROC Curve for different classifiers')\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"43a34641-7797-2ab2-b2c1-d8ad175d5594"},"source":"MMM. not really. Adding polynomial or additional features doesn't outperform the linear approach with only the height and weight, both when using the accuracy or the AUC (which was not calculated but can be qualitatively deduced from the graph) as metrics.\n\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}