{"cells":[{"metadata":{},"cell_type":"markdown","source":"Nama : Ilham Maulana PT:Universitas Bina Insani Matkul:Machine Learning(program klastering)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n\n# import required libraries for dataframe and visualization\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\n\n# import required libraries for clustering\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the data on which analysis needs to be done\n\nretail = pd.read_csv('../input/online-retail-customer-clustering/OnlineRetail.csv', sep=\",\", encoding=\"ISO-8859-1\", header=0)\nretail.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape of df\n\nretail.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df info\n\nretail.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df description\n\nretail.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 2 : Data Cleansing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the Missing Values % contribution in DF\n\ndf_null = round(100*(retail.isnull().sum())/len(retail), 2)\ndf_null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Droping rows having missing values\n\nretail = retail.dropna()\nretail.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing the datatype of Customer Id as per Business understanding\n\nretail['CustomerID'] = retail['CustomerID'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 3 : Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# New Attribute : Monetary\n\nretail['Amount'] = retail['Quantity']*retail['UnitPrice']\nrfm_m = retail.groupby('CustomerID')['Amount'].sum()\nrfm_m = rfm_m.reset_index()\nrfm_m.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New Attribute : Frequency\n\nrfm_f = retail.groupby('CustomerID')['InvoiceNo'].count()\nrfm_f = rfm_f.reset_index()\nrfm_f.columns = ['CustomerID', 'Frequency']\nrfm_f.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merging the two dfs\n\nrfm = pd.merge(rfm_m, rfm_f, on='CustomerID', how='inner')\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# New Attribute : Recency\n\n# Convert to datetime to proper datatype\n\nretail['InvoiceDate'] = pd.to_datetime(retail['InvoiceDate'],format='%d-%m-%Y %H:%M')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the maximum date to know the last transaction date\n\nmax_date = max(retail['InvoiceDate'])\nmax_date","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the difference between max date and transaction date\n\nretail['Diff'] = max_date - retail['InvoiceDate']\nretail.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute last transaction date to get the recency of customers\n\nrfm_p = retail.groupby('CustomerID')['Diff'].min()\nrfm_p = rfm_p.reset_index()\nrfm_p.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extract number of days only\n\nrfm_p['Diff'] = rfm_p['Diff'].dt.days\nrfm_p.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge tha dataframes to get the final RFM dataframe\n\nrfm = pd.merge(rfm, rfm_p, on='CustomerID', how='inner')\nrfm.columns = ['CustomerID', 'Amount', 'Frequency', 'Recency']\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Outlier Analysis of Amount Frequency and Recency\n\nattributes = ['Amount','Frequency','Recency']\nplt.rcParams['figure.figsize'] = [10,8]\nsns.boxplot(data = rfm[attributes], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Range\", fontweight = 'bold')\nplt.xlabel(\"Attributes\", fontweight = 'bold')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing (statistical) outliers for Amount\nQ1 = rfm.Amount.quantile(0.05)\nQ3 = rfm.Amount.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Amount >= Q1 - 1.5*IQR) & (rfm.Amount <= Q3 + 1.5*IQR)]\n\n# Removing (statistical) outliers for Recency\nQ1 = rfm.Recency.quantile(0.05)\nQ3 = rfm.Recency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Recency >= Q1 - 1.5*IQR) & (rfm.Recency <= Q3 + 1.5*IQR)]\n\n# Removing (statistical) outliers for Frequency\nQ1 = rfm.Frequency.quantile(0.05)\nQ3 = rfm.Frequency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Frequency >= Q1 - 1.5*IQR) & (rfm.Frequency <= Q3 + 1.5*IQR)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rescaling the attributes\n\nrfm_df = rfm[['Amount', 'Frequency', 'Recency']]\n\n# Instantiate\nscaler = StandardScaler()\n\n# fit_transform\nrfm_df_scaled = scaler.fit_transform(rfm_df)\nrfm_df_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfm_df_scaled = pd.DataFrame(rfm_df_scaled)\nrfm_df_scaled.columns = ['Amount', 'Frequency', 'Recency']\nrfm_df_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 4 : Building the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# k-means with some arbitrary k\n\nkmeans = KMeans(n_clusters=4, max_iter=50)\nkmeans.fit(rfm_df_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Elbow-curve/SSD\n\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    ssd.append(kmeans.inertia_)\n    \n# plot the SSDs for each n_clusters\nplt.plot(ssd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Silhouette analysis\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(rfm_df_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final model with k=3\nkmeans = KMeans(n_clusters=3, max_iter=50)\nkmeans.fit(rfm_df_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans.labels_\n# assign the label\nrfm['Cluster_Id'] = kmeans.labels_\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot to visualize Cluster Id vs Frequency\n\nsns.boxplot(x='Cluster_Id', y='Amount', data=rfm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot to visualize Cluster Id vs Frequency\n\nsns.boxplot(x='Cluster_Id', y='Frequency', data=rfm)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box plot to visualize Cluster Id vs Recency\n\nsns.boxplot(x='Cluster_Id', y='Recency', data=rfm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Single linkage: \n\nmergings = linkage(rfm_df_scaled, method=\"single\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Complete linkage\n\nmergings = linkage(rfm_df_scaled, method=\"complete\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Average linkage\n\nmergings = linkage(rfm_df_scaled, method=\"average\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3 clusters\ncluster_labels = cut_tree(mergings, n_clusters=3).reshape(-1, )\ncluster_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assign cluster labels\n\nrfm['Cluster_Labels'] = cluster_labels\nrfm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Cluster Id vs Amount\n\nsns.boxplot(x='Cluster_Labels', y='Amount', data=rfm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Cluster Id vs Frequency\n\nsns.boxplot(x='Cluster_Labels', y='Frequency', data=rfm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot Cluster Id vs Recency\n\nsns.boxplot(x='Cluster_Labels', y='Recency', data=rfm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Step 5 : Final Analysis"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}