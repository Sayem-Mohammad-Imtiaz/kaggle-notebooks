{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#read file\nwine_data = pd.read_csv(\"../input/uci-wine-data/wine-clustering.csv\") \nwine_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wine_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#min max normalization\ndef normalize_data(data):\n    data_normalized = data.copy()\n    for col in data.columns:\n        print(col, \"max:\",data[col].max(), \"min:\",data[col].min())\n        data_normalized[col] = (data_normalized[col] - data_normalized[col].min()) / (data_normalized[col].max() - data_normalized[col].min())\n    return data_normalized\n\nwine_data_normalized = normalize_data(wine_data)\nwine_data_normalized.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#similarity matrix where, i-j entry gives dissimilarity between i and j objects\n#dissimilarity is 0 when 2 objects are similar\n# euclidean distance formula: d(i,j) = [sum for all features((Xif - Xjf)^2)]^1/2\n\ndef get_dissimilarity(data):\n    from scipy.spatial.distance import squareform, pdist\n    similarity_matrix = pd.DataFrame(squareform(pdist(data, 'euclidean')))\n    return similarity_matrix\n\nsimilarity_matrix = get_dissimilarity(wine_data_normalized)\nsimilarity_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#avg dissimilarity for each object \ndef get_avg_dissimilarity(data):\n    avg_dissimilarity = np.zeros((data.shape[0],1))\n    for i in range(data.shape[0]):\n        avg_dissimilarity[i] = data[i].mean()\n    return avg_dissimilarity\n        \navg_dissimilarity = get_avg_dissimilarity(similarity_matrix)\navg_dissimilarity[:10], avg_dissimilarity.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#forming m clusters, checking for each object the i-j pairs with dissimilarity less than avg for that object\ndef form_m_clusters(data, avg_data):\n    cluster_objects = [] \n    cluster = [] \n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            if(data[i][j]<avg_data[i]):\n                  #cluster.append(i)\n                  cluster.append(j)\n        cluster_objects.append(cluster)\n        cluster = [] \n    return cluster_objects\n\ncluster_objects = form_m_clusters(similarity_matrix, avg_dissimilarity)\nlen(cluster_objects) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preview clusters\nfor i in range(5):\n    print(\"cluster\", i, \"(\",max(cluster_objects[i]),\")\", \": \", cluster_objects[i])\n    print(\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Remove clusters that are a subset of some other cluster, to be left with p clusters\ndef remove_subset_clusters(cluster_objects):\n    for i in range(len(cluster_objects)):\n        for j in range(i+1, len(cluster_objects)):\n            if (j<len(cluster_objects) and set(cluster_objects[j]).issubset(set(cluster_objects[i]))):\n                cluster_objects = np.delete(cluster_objects, j, axis=0)\n                print(\"cluster\", j, \"subset of cluster\", i, \"deleted!\") \n    return cluster_objects\n\n# cluster_objects = remove_subset_clusters(cluster_objects)\n# len(cluster_objects)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create similarity matrix of pxp\n#where, Cij = |Ci I Cj/ Ci U Cj|\ndef get_similarity_matrix(cluster_objects):\n    p = len(cluster_objects) \n    similarity_matrix2 = np.zeros((p, p), dtype=object) \n    for i in range(p):\n        for j in range(p):\n            intersect = len(np.intersect1d(cluster_objects[i], cluster_objects[j])) \n            union = len(np.union1d(cluster_objects[i], cluster_objects[j])) \n            similarity_matrix2[i][j] = np.abs(intersect/union) \n    #pd.DataFrame(similarity_matrix2)\n    return similarity_matrix2\n\n# similarity_matrix2 = get_similarity_matrix(cluster_objects)\n# pd.DataFrame(similarity_matrix2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#find max Cij and merge Ci & Cj into one \ndef merge_max_clusters(similarity_matrix2, cluster_objects):\n    maxrow = np.argmax(np.max(similarity_matrix2, axis=0))\n    maxcol = np.argmax(np.max(similarity_matrix2, axis=1))\n    #print(\"Maxrow, Maxcol:\", maxrow, maxcol) \n\n    merged = np.unique(np.concatenate((cluster_objects[maxrow], cluster_objects[maxcol]), axis=0))\n    #print(len(cluster_objects[maxrow]) , len(cluster_objects[maxcol]), len(merged)) \n    cluster_objects[maxrow] = merged \n    cluster_objects = np.delete(cluster_objects, maxcol, axis=0)\n    return cluster_objects\n    #len(cluster_objects) \n\n# cluster_objects = merge_max_clusters(similarity_matrix2, cluster_objects)\n# len(cluster_objects)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#iterate to get k clusters \nk = 3\nwhile len(cluster_objects) > k:\n    cluster_objects = remove_subset_clusters(cluster_objects) \n    similarity_matrix2 = get_similarity_matrix(cluster_objects) \n    cluster_objects = merge_max_clusters(similarity_matrix2, cluster_objects) \nlen(cluster_objects) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(cluster_objects[0]) + len(cluster_objects[1]) + len(cluster_objects[2])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for each index from 0-177, for each cluster: find avg row, calc dissimilarity b/w avg row and index, keep index in the cluster with least dissimilarity, remove from others\n# df0 = wine_data_normalized.copy()\n# for i in range(178):\n#     if i not in cluster_objects[0]:\n#         df0 = df0.drop(i)\n# dissimilarity = get_dissimilarity(df0)\n# avg_dissimilarity = get_avg_dissimilarity(dissimilarity)\n# df0['avg_dissimilarity'] = avg_dissimilarity\n\n# df1 = wine_data_normalized.copy()\n# for i in range(178):\n#     if i not in cluster_objects[1]:\n#         df1 = df1.drop(i)\n# dissimilarity = get_dissimilarity(df1)\n# avg_dissimilarity = get_avg_dissimilarity(dissimilarity)\n# df1['avg_dissimilarity'] = avg_dissimilarity\n\n# df2 = wine_data_normalized.copy()\n# for i in range(178):\n#     if i not in cluster_objects[2]:\n#         df2 = df2.drop(i)\n# dissimilarity = get_dissimilarity(df2)\n# avg_dissimilarity = get_avg_dissimilarity(dissimilarity)\n# df2['avg_dissimilarity'] = avg_dissimilarity\n\n# # df0.index, df0.loc[41,:][\"avg_dissimilarity\"]\n# for i in range(178):\n#     a=100\n#     b=100\n#     c=100\n#     if i in df0.index:\n#         a = df0.loc[i,:]['avg_dissimilarity']\n#     if i in df1.index:\n#         b = df1.loc[i,:]['avg_dissimilarity']\n#     if i in df2.index:\n#         c = df2.loc[i,:]['avg_dissimilarity']\n#     smallest=min(a,b,c)\n#     if i in df0.index and a != smallest:\n#         df0 = df0.drop(i)\n#     if i in df1.index and b != smallest:\n#         df1 = df1.drop(i)\n#     if i in df2.index and c != smallest:\n#         df2 = df2.drop(i)\n# df0.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df1.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df2.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(df0) + len(df1) + len(df2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"our_cluster = np.zeros((178,), dtype='object')\n# for i in range(178):\n#     if i in df0.index:\n#         our_cluster[i] = 0\n#     elif i in df1.index:\n#         our_cluster[i] = 1\n#     elif i in df2.index:\n#         our_cluster[i] = 2\n#     else:\n#         our_cluster[i] = 3\nfor i in cluster_objects[0]:\n    our_cluster[i]=0;\nfor i in cluster_objects[1]:\n    our_cluster[i]=1;\nfor i in cluster_objects[2]:\n    our_cluster[i]=2;\nour_cluster","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.scatter(our_cluster, range(0,178), c=our_cluster)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3, random_state=0).fit(wine_data_normalized)\nkmeans.labels_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kmeans.predict(wine_data_normalized)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cluster0 = []\ncluster1 = []\ncluster2 = []\n\nfor i in range(178):\n    x = kmeans.labels_[i]\n    if(x == 0):\n        cluster0.append(i)\n    elif(x == 1):\n        cluster1.append(i)\n    else:\n        cluster2.append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(kmeans.labels_, range(0, 178), c=kmeans.labels_)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dunn index","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Davies Bouldin index\nfrom sklearn.metrics import davies_bouldin_score\nkmeans_dbs = davies_bouldin_score(wine_data_normalized, kmeans.labels_)\nour_dbs = davies_bouldin_score(wine_data_normalized, our_cluster)\nkmeans_dbs, our_dbs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Silhouette index\nfrom sklearn.metrics import silhouette_score\nkmeans_si = silhouette_score(wine_data_normalized, kmeans.labels_)\nour_si = silhouette_score(wine_data_normalized, our_cluster)\nkmeans_si, our_si","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}