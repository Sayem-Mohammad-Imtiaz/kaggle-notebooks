{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# import dependencies\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nimport gensim\nfrom gensim.models import LdaModel\nfrom gensim import models, corpora, similarities\nimport re\nfrom nltk.stem.porter import PorterStemmer\nimport time\nfrom nltk import FreqDist\nfrom scipy.stats import entropy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.style.use('ggplot')\n\nfrom subprocess import check_output","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df=pd.read_csv('../input/fake_or_real_news.csv')","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7787f0f0ac7ae0dbd98770cd2c93a515be984221"},"cell_type":"code","source":"df.head()","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2f24eb448a4d17c0f87aa59c0e91ad488837faf"},"cell_type":"code","source":"def initial_clean(text):\n    \"\"\"\n    Function to clean text of websites, email addresess and any punctuation\n    We also lower case the text\n    \"\"\"\n    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n    text = text.lower() # lower case the text\n    text = nltk.word_tokenize(text)\n    return text\n\nstop_words = stopwords.words('english')\ndef remove_stop_words(text):\n    \"\"\"\n    Function that removes all stopwords from text\n    \"\"\"\n    return [word for word in text if word not in stop_words]\n\nstemmer = PorterStemmer()\ndef stem_words(text):\n    \"\"\"\n    Function to stem words, so plural and singular are treated the same\n    \"\"\"\n    try:\n        text = [stemmer.stem(word) for word in text]\n        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n    except IndexError: # the word \"oed\" broke this, so needed try except\n        pass\n    return text\n\ndef apply_all(text):\n    \"\"\"\n    This function applies all the functions above into one\n    \"\"\"\n    return stem_words(remove_stop_words(initial_clean(text)))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c16bd14fc94e53b272fdfba9c99496358b8c7ec"},"cell_type":"code","source":"# clean text and title and create new column \"tokenized\"\nt1 = time.time()\ndf['tokenized'] = df['text'].apply(apply_all) + df['title'].apply(apply_all)\nt2 = time.time()\nprint(\"Time to clean and tokenize\", len(df), \"articles:\", (t2-t1)/60, \"min\")","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe1d67523df06ea44a605d6e1676804f87fb72c9"},"cell_type":"code","source":"# first get a list of all words\nall_words = [word for item in list(df['tokenized']) for word in item]\n# use nltk fdist to get a frequency distribution of all words\nfdist = FreqDist(all_words)\nlen(fdist) # number of unique words","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3cb4aedd86448a52a29740d8bd1450e366ef6bce"},"cell_type":"code","source":"k = 15000\ntop_k_words = fdist.most_common(k)\ntop_k_words[-10:]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c96bb64671038063146f0b8bd0c97e7797d64177"},"cell_type":"code","source":"# define a function only to keep words in the top k words\ntop_k_words,_ = zip(*fdist.most_common(k))\ntop_k_words = set(top_k_words)\ndef keep_top_k_words(text):\n    return [word for word in text if word in top_k_words]","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b07ffe6c3b507d8d5f02cd34a1049cd8473faa7d"},"cell_type":"code","source":"df['tokenized'] = df['tokenized'].apply(keep_top_k_words)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3d1553dfaa2fe88236842f1bfbefd92fd59682c"},"cell_type":"code","source":"# document length\ndf['doc_len'] = df['tokenized'].apply(lambda x: len(x))\ndoc_lengths = list(df['doc_len'])\ndf.drop(labels='doc_len', axis=1, inplace=True)\n\nprint(\"length of list:\",len(doc_lengths),\n      \"\\naverage document length\", np.average(doc_lengths),\n      \"\\nminimum document length\", min(doc_lengths),\n      \"\\nmaximum document length\", max(doc_lengths))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"025c4d8330b22efa63de897b9636765392a91a55"},"cell_type":"code","source":"# plot a histogram of document length\nnum_bins = 1000\nfig, ax = plt.subplots(figsize=(12,6));\n# the histogram of the data\nn, bins, patches = ax.hist(doc_lengths, num_bins, normed=1)\nax.set_xlabel('Document Length (tokens)', fontsize=15)\nax.set_ylabel('Normed Frequency', fontsize=15)\nax.grid()\nax.set_xticks(np.logspace(start=np.log10(50),stop=np.log10(2000),num=8, base=10.0))\nplt.xlim(0,2000)\nax.plot([np.average(doc_lengths) for i in np.linspace(0.0,0.0035,100)], np.linspace(0.0,0.0035,100), '-',\n        label='average doc length')\nax.legend()\nax.grid()\nfig.tight_layout()\nplt.show()","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b0903c7f0d98e29d7a4da219bb2d4567e2ae7ed"},"cell_type":"code","source":"# only keep articles with more than 30 tokens, otherwise too short\ndf = df[df['tokenized'].map(len) >= 40]\n# make sure all tokenized items are lists\ndf = df[df['tokenized'].map(type) == list]\ndf.reset_index(drop=True,inplace=True)\nprint(\"After cleaning and excluding short aticles, the dataframe now has:\", len(df), \"articles\")","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"954d8c6bcf2a1887af1fd7953c309cf89d5ccf36"},"cell_type":"code","source":"df.head()","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44df8d175bff7fb90c5117bda291cf4e66f30a1c"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size = 0.2)\n\nfrom nltk.corpus import stopwords \ntrain.columns.values\ntrain.head()","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9aaeb1a97c94c5f45056a1ac4581d3e9aa660ce6"},"cell_type":"code","source":"import re\ndef refineWords(s):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", s) \n    words = letters_only.lower().split()\n    stops = set(stopwords.words(\"english\"))\n    meaningful_words = [w for w in words if not w in stops]\n    #print( \" \".join( meaningful_words ))\n    return( \" \".join( meaningful_words ))\n\ntrain[\"text\"].fillna(\" \",inplace=True)    \ntrain[\"text\"] = train[\"text\"].apply(refineWords)\ntrain[\"title\"].fillna(\" \",inplace=True)    \ntrain[\"title\"] = train[\"title\"].apply(refineWords)\n\ntrain_two = train.copy()\ntrain.head()","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a5372f3296f0d7365bed9a1e1a007b3a267d2b3"},"cell_type":"code","source":"train = train_two.copy()\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000) \n#print(train_one[\"title\"].head())\n#temp  = (vectorizer.fit_transform(train_one[\"text\"]))\n#train_one[\"text\"] = temp.to_array()\ntrain[\"text\"] = vectorizer.fit_transform(train[\"text\"]).toarray()\ntrain[\"title\"] = vectorizer.fit_transform(train[\"title\"]).toarray()\ntrain.head()","execution_count":18,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c395f37e17c8774e941250691c5f2d6aea6f30b1"},"cell_type":"code","source":"#print(train_one[\"isSpam\"])\nfrom sklearn.ensemble import RandomForestClassifier\n#forest = RandomForestClassifier(n_estimators = 100)\nforest = RandomForestClassifier(max_depth = 10, min_samples_split=2, n_estimators = 100, random_state = 1)\nfeatures_forest = train[[\"text\", \"title\"]].values\nmy_forest = forest.fit(features_forest, train[\"label\"])","execution_count":22,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e104d0a8e8efec89ec8c7c78fc27c930b7766446"},"cell_type":"code","source":"target = train[\"label\"].values\nprint(my_forest.score(features_forest, target))","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b72d55082f3c8671682f2aaa1d14d79e183c8cee"},"cell_type":"code","source":"test[\"text\"].fillna(\" \",inplace=True)    \ntest[\"text\"] = test[\"text\"].apply(refineWords)\ntest[\"title\"].fillna(\" \",inplace=True)    \ntest[\"title\"] = test[\"title\"].apply(refineWords)\n\ntest_two = test.copy()\n\ntest[\"text\"] = vectorizer.fit_transform(test[\"text\"]).toarray()\ntest[\"title\"] = vectorizer.fit_transform(test[\"title\"]).toarray()","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8741d4cd1d9128c0130113511804689728fc0ff"},"cell_type":"code","source":"test_features = test[[\"text\", \"title\"]].values\nmy_prediction = my_forest.predict(test_features)\nprint(len(my_prediction),len(test[\"label\"]))","execution_count":25,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96cc541526656181ea0384b942fd0d658fd00a69"},"cell_type":"code","source":"count = 0\npred = my_prediction.tolist()\ntest_spam = test[\"label\"].tolist()\nfor i in range(len(pred)):\n    if pred[i] == test_spam[i]:\n        count += 1\nprint(count,float(count)/len(my_prediction))\n#print(my_prediction)\n#print(test_spam)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"610c3c75634cb969594f6f9e1bd8f8e5eb5d6f73"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}