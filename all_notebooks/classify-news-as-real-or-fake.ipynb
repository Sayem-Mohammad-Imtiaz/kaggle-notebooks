{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Classify news as real or fake"},{"metadata":{},"cell_type":"markdown","source":"### **üê±‚Äçüë§Task: Classifying the news**\n\n---\n\n## **üöõ Importing packages**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import re\nimport string\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **üß®üòç Loading the datasets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news_df = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')\nfake_news_df = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **‚úÖ True news dataframe**"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **‚ùå Fake news dataframe**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fake_news_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **üëìVisualizing uncleanded data using WordCloud**"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(STOPWORDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# True news word cloud\ntrue_news_wc = WordCloud(\n    background_color='black', \n    max_words=200, \n    stopwords=stopwords\n).generate(''.join(true_news_df['text']))\n\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 18))\nax.imshow(true_news_wc, interpolation='bilinear')\nax.axis('off')\nax.set_label('True News')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fake news word cloud\nfake_news_wc = WordCloud(\n    background_color='black', \n    max_words=200, \n    stopwords=stopwords\n).generate(''.join(fake_news_df['text']))\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\nax.imshow(fake_news_wc, interpolation='bilinear')\nax.axis('off')\nax.set_label('Fake News')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **üöøüõÅüßΩ Data preprocessing**"},{"metadata":{},"cell_type":"markdown","source":"### **ü§ñ Helper Functions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_from_list_to_text(_list):\n    text = ' '.join(_list)\n    return text\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Purples):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **üëÄ Unique words from text**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def unique(text):\n    text = text.split()\n    return list(set(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news_df['text']  = true_news_df['text'].apply(lambda x: unique(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: unique(x))\n\ntrue_news_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Getting rid of üí≤‚ùì‚ùóüò´punctuations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove punctuation from a word if it exist\ndef rm_punc_from_word(word):\n    clean_word = ''                # word without punctuation\n    for alpha in word:\n        # checking if alphabet is punctuation or not\n        if alpha in string.punctuation:\n            continue\n        clean_word += alpha\n        \n    return clean_word\n\n\n# Remove any punctuation and clean words having punctuation\ndef clean_punc(words_list):\n    for idx, word in enumerate(words_list):\n        if word in string.punctuation:\n            words_list.remove(word)\n        else:\n            words_list[idx] = rm_punc_from_word(word)\n            words_list[idx] = re.sub('[0-9]+', '', words_list[idx])\n            \n    return words_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news_df['text']  = true_news_df['text'].apply(lambda x: clean_punc(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: clean_punc(x))\n\ntrue_news_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Remove numbers ü•±**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rm_num(words_list):\n    text = ' '.join(words_list)\n    text = ''.join([i for i in text if not i.isdigit()])\n    return text.split()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news_df['text']  = true_news_df['text'].apply(lambda x: rm_num(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: rm_num(x))\n\ntrue_news_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Tokenization (‚åê‚ñ†_‚ñ†)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenization(words_list):\n    tmp = words_list.copy()\n    words_list = []\n    \n    for idx, word in enumerate(tmp):\n        for split_word in re.split('\\W+', word):\n            words_list.append(split_word)\n\n    words_list = ' '.join(words_list).split()  # removing any white spaces\n    return words_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news_df['text']  = true_news_df['text'].apply(lambda x: tokenization(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: tokenization(x))\n\ntrue_news_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **üò™ Removing URL**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_URL(words_list):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    for idx, word in enumerate(words_list):\n        words_list[idx] = url.sub(r'',word)\n    return words_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news_df['text']  = true_news_df['text'].apply(lambda x: remove_URL(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: remove_URL(x))\n\ntrue_news_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Removing üò¥ HTML tags**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_HTML(words_list):\n    html = re.compile(r'<.*?>')\n    for idx, word in enumerate(words_list):\n        words_list[idx] = html.sub(r'',word)\n    return words_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news_df['text']  = true_news_df['text'].apply(lambda x: remove_HTML(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: remove_HTML(x))\n\ntrue_news_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Remove emoji ü§¨ üòÇ**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def remove_emoji(words_list):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n\n    for idx, word in enumerate(words_list):\n        words_list[idx] = emoji_pattern.sub(r'',word)\n    return words_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news_df['text']  = true_news_df['text'].apply(lambda x: remove_emoji(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: remove_emoji(x))\n\ntrue_news_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Removing üõë stop words**"},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords = set(nltk.corpus.stopwords.words())\n\ndef clean_stopwords(words_list):\n    for word in words_list:\n        if word in stopwords:\n            words_list.remove(word)\n    return words_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news_df['text']  = true_news_df['text'].apply(lambda x: clean_stopwords(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: clean_stopwords(x))\n\ntrue_news_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **üìùLabelling the datasets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news_df['is_fake'] = 0\nfake_news_df['is_fake'] = 1\n\ntrue_news_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **üôÑ Converting list of words to text**"},{"metadata":{"trusted":true},"cell_type":"code","source":"true_news_df['text']  = true_news_df['text'].apply(lambda x: convert_from_list_to_text(x))\nfake_news_df['text']  = fake_news_df['text'].apply(lambda x: convert_from_list_to_text(x))\n\ntrue_news_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **üëìVisualizing cleanded data using WordCloud**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# True news word cloud\ntrue_news_wc = WordCloud(\n    background_color='black', \n    max_words=200, \n    stopwords=stopwords\n).generate(''.join(true_news_df['text']))\n\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 18))\nax.imshow(true_news_wc, interpolation='bilinear')\nax.axis('off')\nax.set_label('True News')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fake news word cloud\nfake_news_wc = WordCloud(\n    background_color='black', \n    max_words=200, \n    stopwords=stopwords\n).generate(''.join(fake_news_df['text']))\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\nax.imshow(fake_news_wc, interpolation='bilinear')\nax.axis('off')\nax.set_label('Fake News')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ‚ú® Concatenating true and fake news dataframes and suffling them\n\ndf = pd.concat([true_news_df, fake_news_df], axis='index')\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **üöÄ Creating model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# üî™ Splitting the dataset into train & test\n\nskf = StratifiedKFold(n_splits=10)\n\nX = df.text\nY = df.is_fake\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=212)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **üß≤ Extracting features from text**\n\nü§î Texts are actually series of words. In order to run machine learning algorithms we need to convert the text of text into numerical feature vectors.\n\nWe have used the bag of words model for our example.\n\nü§® Briefly, we segment each text into words (for English splitting by space), **and count number of times each word occurs in each document and finally assign each word an integer id**. Each unique word in our dictionary will correspond to a feature (descriptive feature).\n\nüòè Scikit-learn has a high level component CountVectorizer which will create feature vectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vec = CountVectorizer()\nX_train_counts = count_vec.fit_transform(X_train)\n\nX_train_counts.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### **TF:**\n\nJust counting the number of words in each text has 1 üò± issue: it will give more weightage to longer texts than shorter texts. To avoid this, we can use frequency **(TF - Term Frequencies) i.e. #count(word) / #Total words**, in each  text.\n\n#### **TF-IDF:**\n\nFinally, we have reduce the weightage of more common words like (the, is, an etc.) which occurs in all text. This is called as **TF-IDF i.e Term Frequency times inverse document frequency**.\n\n**To achieve this üòé TfidfTransformerfrom Scikit-learn is used**."},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\nX_train_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **üéØ Cross Validation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cross_val_score(MultinomialNB(), X_train_tfidf, y_train, cv=skf)\nprint(f'MultinomialNB mean of cross validation score: {score.mean()}')\n\nscore = cross_val_score(LogisticRegression(), X_train_tfidf, y_train, cv=skf)\nprint(f'LogisticRegression mean of cross validation score: {score.mean()}')\n\nscore = cross_val_score(LinearSVC(), X_train_tfidf, y_train, cv=skf)\nprint(f'LinearSVC mean of cross validation score: {score.mean()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **üé∞ Pipeline**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', LinearSVC())\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **üòé Fitting the model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **üîÆ Predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test_pred = model.predict(X_test)\nnp.mean(y_test_pred == y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **üéØ Metrics**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Model Score: {model.score(X_test, y_test)}')\nprint(f'f1-score: {f1_score(y_test, y_test_pred, average=\"weighted\")}')\nprint(f'precision score: {precision_score(y_test, y_test_pred, average=\"weighted\")}')\nprint(f'recall score: {recall_score(y_test, y_test_pred, average=\"weighted\")}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(y_test, y_test_pred, labels=[0,1])\nnp.set_printoptions(precision=2)\n\nprint(classification_report(y_test, y_test_pred))\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(cnf_matrix, classes=['True(0)', 'Fake(1)'],normalize= False,  title='Confusion matrix', cmap=plt.cm.RdPu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}