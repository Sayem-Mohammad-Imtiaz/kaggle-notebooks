{"cells":[{"metadata":{"_uuid":"0f13bbff-80c0-4708-b546-dfd05ae48bd8","_cell_guid":"483fb86f-2a15-4b11-999e-eab5e41693f9","trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a624129d-f617-4812-80c8-74d65ce1ca59","_cell_guid":"fad5e025-a33b-4730-8e93-45a23e3b7eae","trusted":true},"cell_type":"code","source":"from pyspark import SparkConf\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import SparkSession\n#from pyspark.sql.types import spark\nfrom pyspark.sql import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql import DataFrameReader\n#from pyspark.sql.types import sc\n#from pyspark.sql.types import sqlResultsPD\n#from pyspark.sql.types import predictionsPD\n#import pyspark.sql.DataFrame\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import RFormula\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.mllib.evaluation import RegressionMetrics\nfrom pyspark.ml.feature import StringIndexer, VectorIndexer\nfrom pyspark.ml.feature import RFormula\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.mllib.evaluation import RegressionMetrics\nfrom pylab import *\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport datetime\n\n\n#logFile = \"E:\\\\anacanda/Lib/site-packages/pyspark/bin/README.md\"  # Should be some file on your system\nsc = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n#logData = spark.read.text(logFile).cache()\n# 1. Location of training data: contains Dec 2015 trip and fare data from NYC\n#trip_file_loc = '../input/nyc-taxi/trip_data_sample.csv'\n#fare_file_loc = '../input/nyc-taxi/trip_fare_sample.csv'\n\n# 2. Location of the joined taxi+fare training file\n#taxi_valid_file_loc = 'D:\\\\Ebi secure/project spark/taxi/valid_file_loc'\n\n# 3. Set model storage directory path. This is where models will be saved.\n#modelDir = 'D:\\\\Ebi secure/project spark/taxi'; \n\n# 4. Set data storage path. This is where data is sotred on the blob attached to the cluster.\n#dataDir = 'D:\\\\Ebi secure/project spark/taxi'; # The last backslash is needed;\n\nsqlContext = SQLContext(sc)\n## READ IN TRIP DATA FRAME FROM CSV\ntrip_file_loc='../input/nyctaxinew/trip_data_new.csv'\ntrip = spark.read.csv(path=trip_file_loc, header=True, inferSchema=True)\n\n## READ IN FARE DATA FRAME FROM CSV\nfare_file_loc='../input/nyctaxi/trip_fare_sample_new.csv'\nfare = spark.read.csv(path=fare_file_loc, header=True, inferSchema=True)\ntrip.printSchema()\nfare.printSchema()\n## REGISTER DATA-FRAMEs AS A TEMP-TABLEs IN SQL-CONTEXT\ntrip.createOrReplaceTempView(\"trip\")\nfare.createOrReplaceTempView(\"fare\")\n\n## USING SQL: MERGE TRIP AND FARE DATA-SETS TO CREATE A JOINED DATA-FRAME\n## ELIMINATE SOME COLUMNS, AND FILTER ROWS WTIH VALUES OF SOME COLUMNS\nsqlStatement = \"\"\"SELECT t.medallion, t.hack_license,\n  f.total_amount, f.tolls_amount,\n  hour(f.pickup_datetime) as pickup_hour, f.vendor_id, f.fare_amount, \n  f.surcharge, f.tip_amount, f.payment_type, t.rate_code, \n  t.passenger_count, t.trip_distance, t.trip_time_in_secs \n  FROM trip t, fare f  \n  WHERE t.medallion = f.medallion AND t.hack_license = f.hack_license \n  AND t.pickup_datetime = f.pickup_datetime \n  AND t.passenger_count > 0 and t.passenger_count < 8 \n  AND f.tip_amount >= 0 AND f.tip_amount <= 25 \n  AND f.fare_amount >= 1 AND f.fare_amount <= 250 \n  AND f.tip_amount < f.fare_amount AND t.trip_distance > 0 \n  AND t.trip_distance <= 100 AND t.trip_time_in_secs >= 30 \n  AND t.trip_time_in_secs <= 7200 AND t.rate_code <= 5\n  AND f.payment_type in ('CSH','CRD')\"\"\"\ntrip_fareDF = spark.sql(sqlStatement)\n\n# REGISTER JOINED TRIP-FARE DF IN SQL-CONTEXT\ntrip_fareDF.createOrReplaceTempView(\"trip_fare\")\n\n## SHOW WHICH TABLES ARE REGISTERED IN SQL-CONTEXT\nspark.sql(\"show tables\").show()\n\n# SAMPLE 10% OF DATA, SPLIT INTO TRAIINING AND VALIDATION AND SAVE IN BLOB\ntrip_fare_featSampled = trip_fareDF.sample(False, 0.1, seed=1234)\ntrainfilename = dataDir + \"TrainData\";\ntrip_fare_featSampled.repartition(10).write.mode(\"overwrite\").parquet(trainfilename)\n\n## READ IN DATA FRAME FROM CSV\ntaxi_train_df = spark.read.parquet(trainfilename)\n\n## CREATE A CLEANED DATA-FRAME BY DROPPING SOME UN-NECESSARY COLUMNS & FILTERING FOR UNDESIRED VALUES OR OUTLIERS\ntaxi_df_train_cleaned = taxi_train_df.drop('medallion').drop('hack_license').drop('total_amount').drop('tolls_amount')\\\n    .filter(\"passenger_count > 0 and passenger_count < 8 AND tip_amount >= 0 AND tip_amount < 15 AND \\\n            fare_amount >= 1 AND fare_amount < 150 AND trip_distance > 0 AND trip_distance < 100 AND \\\n            trip_time_in_secs > 30 AND trip_time_in_secs < 7200\" )\n\n## PERSIST AND MATERIALIZE DF IN MEMORY\ntaxi_df_train_cleaned.persist()\ntaxi_df_train_cleaned.count()\n\n## REGISTER DATA-FRAME AS A TEMP-TABLE IN SQL-CONTEXT\ntaxi_df_train_cleaned.createOrReplaceTempView(\"taxi_train\")\n\ntaxi_df_train_cleaned.printSchema()\n\n#%%sql -q -o sqlResultsPD\n#SELECT fare_amount, passenger_count, tip_amount FROM taxi_train WHERE passenger_count > 0 AND passenger_count < 7 AND fare_amount > 0 AND fare_amount < 100 AND tip_amount > 0 AND tip_amount < 15\n\n#%%local\n#%matplotlib inline\n\n## %%local creates a pandas data-frame on the head node memory, from spark data-frame,\n## which can then be used for plotting. Here, sampling data is a good idea, depending on the memory of the head node\n\n# TIP BY PAYMENT TYPE AND PASSENGER COUNT\nax1 = sqlResultsPD[['tip_amount']].plot(kind='hist', bins=25, facecolor='lightblue')\nax1.set_title('Tip amount distribution')\nax1.set_xlabel('Tip Amount ($)'); ax1.set_ylabel('Counts');\nplt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()\n\n# TIP BY PASSENGER COUNT\nax2 = sqlResultsPD.boxplot(column=['tip_amount'], by=['passenger_count'])\nax2.set_title('Tip amount by Passenger count')\nax2.set_xlabel('Passenger count'); ax2.set_ylabel('Tip Amount ($)');\nplt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()\n\n# TIP AMOUNT BY FARE AMOUNT, POINTS ARE SCALED BY PASSENGER COUNT\nax = sqlResultsPD.plot(kind='scatter', x= 'fare_amount', y = 'tip_amount', c='blue', alpha = 0.10, s=2.5*(sqlResultsPD.passenger_count))\nax.set_title('Tip amount by Fare amount')\nax.set_xlabel('Fare Amount ($)'); ax.set_ylabel('Tip Amount ($)');\nplt.axis([-2, 80, -2, 20])\nplt.figure(figsize=(4,4)); plt.suptitle(''); plt.show()\n\n### CREATE FOUR BUCKETS FOR TRAFFIC TIMES\nsqlStatement = \"\"\"SELECT payment_type, pickup_hour, fare_amount, tip_amount, \n    vendor_id, rate_code, passenger_count, trip_distance, trip_time_in_secs, \n  CASE\n    WHEN (pickup_hour <= 6 OR pickup_hour >= 20) THEN 'Night'\n    WHEN (pickup_hour >= 7 AND pickup_hour <= 10) THEN 'AMRush' \n    WHEN (pickup_hour >= 11 AND pickup_hour <= 15) THEN 'Afternoon'\n    WHEN (pickup_hour >= 16 AND pickup_hour <= 19) THEN 'PMRush'\n    END as TrafficTimeBins,\n  CASE\n    WHEN (tip_amount > 0) THEN 1 \n    WHEN (tip_amount <= 0) THEN 0 \n    END as tipped\n  FROM taxi_train\"\"\"\n\ntaxi_df_train_with_newFeatures = spark.sql(sqlStatement)\n\n\n\n\n# DEFINE THE TRANSFORMATIONS THAT NEEDS TO BE APPLIED TO SOME OF THE FEATURES\nsI1 = StringIndexer(inputCol=\"vendor_id\", outputCol=\"vendorIndex\");\nsI2 = StringIndexer(inputCol=\"rate_code\", outputCol=\"rateIndex\");\nsI3 = StringIndexer(inputCol=\"payment_type\", outputCol=\"paymentIndex\");\nsI4 = StringIndexer(inputCol=\"TrafficTimeBins\", outputCol=\"TrafficTimeBinsIndex\");\n\n# APPLY TRANSFORMATIONS\nencodedFinal = Pipeline(stages=[sI1, sI2, sI3, sI4]).fit(taxi_df_train_with_newFeatures).transform(taxi_df_train_with_newFeatures);\n\ntrainingFraction = 0.75; testingFraction = (1-trainingFraction);\nseed = 1234;\n\n# SPLIT SAMPLED DATA-FRAME INTO TRAIN/TEST, WITH A RANDOM COLUMN ADDED FOR DOING CV (SHOWN LATER)\ntrainData, testData = encodedFinal.randomSplit([trainingFraction, testingFraction], seed=seed);\n\n# CACHE DATA FRAMES IN MEMORY\ntrainData.persist(); trainData.count()\ntestData.persist(); testData.count()\n\n\n\n## DEFINE REGRESSION FURMULA\nregFormula = RFormula(formula=\"tip_amount ~ paymentIndex + vendorIndex + rateIndex + TrafficTimeBinsIndex + pickup_hour + passenger_count + trip_time_in_secs + trip_distance + fare_amount\")\n\n## DEFINE INDEXER FOR CATEGORIAL VARIABLES\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=32)\n\n## DEFINE ELASTIC NET REGRESSOR\neNet = LinearRegression(featuresCol=\"indexedFeatures\", maxIter=25, regParam=0.01, elasticNetParam=0.5)\n\n## Fit model, with formula and other transformations\nmodel = Pipeline(stages=[regFormula, featureIndexer, eNet]).fit(trainData)\n\n## PREDICT ON TEST DATA AND EVALUATE\npredictions = model.transform(testData)\npredictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\ntestMetrics = RegressionMetrics(predictionAndLabels)\nprint(\"RMSE = %s\" % testMetrics.rootMeanSquaredError)\nprint(\"R-sqr = %s\" % testMetrics.r2)\n\n## PLOC ACTUALS VS. PREDICTIONS\npredictions.select(\"label\",\"prediction\").createOrReplaceTempView(\"tmp_results\");\n\nfrom pyspark.ml.regression import GBTRegressor\n\n## DEFINE REGRESSION FURMULA\nregFormula = RFormula(formula=\"tip_amount ~ paymentIndex + vendorIndex + rateIndex + TrafficTimeBinsIndex + pickup_hour + passenger_count + trip_time_in_secs + trip_distance + fare_amount\")\n\n## DEFINE INDEXER FOR CATEGORIAL VARIABLES\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=32)\n\n## DEFINE GRADIENT BOOSTING TREE REGRESSOR\ngBT = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n\n## Fit model, with formula and other transformations\nmodel = Pipeline(stages=[regFormula, featureIndexer, gBT]).fit(trainData)\n\n## PREDICT ON TEST DATA AND EVALUATE\npredictions = model.transform(testData)\npredictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\ntestMetrics = RegressionMetrics(predictionAndLabels)\nprint(\"RMSE = %s\" % testMetrics.rootMeanSquaredError)\nprint(\"R-sqr = %s\" % testMetrics.r2)\n\n## PLOC ACTUALS VS. PREDICTIONS\npredictions.select(\"label\",\"prediction\").createOrReplaceTempView(\"tmp_results\");\n\n\n## DEFINE REGRESSION FURMULA\nregFormula = RFormula(formula=\"tip_amount ~ paymentIndex + vendorIndex + rateIndex + TrafficTimeBinsIndex + pickup_hour + passenger_count + trip_time_in_secs + trip_distance + fare_amount\")\n\n## DEFINE INDEXER FOR CATEGORIAL VARIABLES\nfeatureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=32)\n\n## DEFINE RANDOM FOREST ESTIMATOR\nrandForest = RandomForestRegressor(featuresCol = 'indexedFeatures', labelCol = 'label', numTrees=20,\n                                   featureSubsetStrategy=\"auto\",impurity='variance', maxDepth=6, maxBins=100)\n\n## Fit model, with formula and other transformations\nmodel = Pipeline(stages=[regFormula, featureIndexer, randForest]).fit(trainData)\n\n## SAVE MODEL\ndatestamp = datetime.datetime.now().strftime('%m-%d-%Y-%s');\nfileName = \"RandomForestRegressionModel_\" + datestamp;\nrandForestDirfilename = modelDir + fileName;\nmodel.save(randForestDirfilename)\n\n## PREDICT ON TEST DATA AND EVALUATE\npredictions = model.transform(testData)\npredictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\ntestMetrics = RegressionMetrics(predictionAndLabels)\nprint(\"RMSE = %s\" % testMetrics.rootMeanSquaredError)\nprint(\"R-sqr = %s\" % testMetrics.r2)\n\n## PLOC ACTUALS VS. PREDICTIONS\npredictions.select(\"label\",\"prediction\").createOrReplaceTempView(\"tmp_results\");\n\n#%%sql -q -o predictionsPD\n#SELECT * from tmp_results\n\n#%%local\n\n\nax = predictionsPD.plot(kind='scatter', figsize = (5,5), x='label', y='prediction', color='blue', alpha = 0.25, label='Actual vs. predicted');\nfit = np.polyfit(predictionsPD['label'], predictionsPD['prediction'], deg=1)\nax.set_title('Actual vs. Predicted Tip Amounts ($)')\nax.set_xlabel(\"Actual\"); ax.set_ylabel(\"Predicted\");\nax.plot(predictionsPD['label'], fit[0] * predictionsPD['label'] + fit[1], color='magenta')\nplt.axis([-1, 15, -1, 15])\nplt.show(ax)\n\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n## DEFINE RANDOM FOREST MODELS\nrandForest = RandomForestRegressor(featuresCol = 'indexedFeatures', labelCol = 'label',\n                                   featureSubsetStrategy=\"auto\",impurity='variance', maxBins=100)\n\n## DEFINE MODELING PIPELINE, INCLUDING FORMULA, FEATURE TRANSFORMATIONS, AND ESTIMATOR\npipeline = Pipeline(stages=[regFormula, featureIndexer, randForest])\n\n## DEFINE PARAMETER GRID FOR RANDOM FOREST\nparamGrid = ParamGridBuilder() \\\n    .addGrid(randForest.numTrees, [10, 25, 50]) \\\n    .addGrid(randForest.maxDepth, [3, 5, 7]) \\\n    .build()\n\n## DEFINE CROSS VALIDATION\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=RegressionEvaluator(metricName=\"rmse\"),\n                          numFolds=3)\n\n## TRAIN MODEL USING CV\ncvModel = crossval.fit(trainData)\n\n## PREDICT AND EVALUATE TEST DATA SET\npredictions = cvModel.transform(testData)\nevaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\nr2 = evaluator.evaluate(predictions)\nprint(\"R-squared on test data = %g\" % r2)\n\n## SAVE THE BEST MODEL\ndatestamp = datetime.datetime.now().strftime('%m-%d-%Y-%s');\nfileName = \"CV_RandomForestRegressionModel_\" + datestamp;\nCVDirfilename = modelDir + fileName;\ncvModel.bestModel.save(CVDirfilename);\n\nfrom pyspark.ml import PipelineModel\n\nsavedModel = PipelineModel.load(randForestDirfilename)\n\npredictions = savedModel.transform(testData)\npredictionAndLabels = predictions.select(\"label\",\"prediction\").rdd\ntestMetrics = RegressionMetrics(predictionAndLabels)\nprint(\"RMSE = %s\" % testMetrics.rootMeanSquaredError)\nprint(\"R-sqr = %s\" % testMetrics.r2)\n\n## READ IN DATA FRAME FROM CSV\ntaxi_valid_df = spark.read.csv(path=taxi_valid_file_loc, header=True, inferSchema=True)\ntaxi_valid_df.printSchema()\n\n## READ IN DATA FRAME FROM CSV\ntaxi_valid_df = spark.read.csv(path=taxi_valid_file_loc, header=True, inferSchema=True)\n\n## CREATE A CLEANED DATA-FRAME BY DROPPING SOME UN-NECESSARY COLUMNS & FILTERING FOR UNDESIRED VALUES OR OUTLIERS\ntaxi_df_valid_cleaned = taxi_valid_df.drop('medallion').drop('hack_license').drop('store_and_fwd_flag').drop('pickup_datetime')\\\n    .drop('dropoff_datetime').drop('pickup_longitude').drop('pickup_latitude').drop('dropoff_latitude')\\\n    .drop('dropoff_longitude').drop('tip_class').drop('total_amount').drop('tolls_amount').drop('mta_tax')\\\n    .drop('direct_distance').drop('surcharge')\\\n    .filter(\"passenger_count > 0 and passenger_count < 8 AND payment_type in ('CSH', 'CRD') \\\n    AND tip_amount >= 0 AND tip_amount < 30 AND fare_amount >= 1 AND fare_amount < 150 AND trip_distance > 0 \\\n    AND trip_distance < 100 AND trip_time_in_secs > 30 AND trip_time_in_secs < 7200\" )\n\n## REGISTER DATA-FRAME AS A TEMP-TABLE IN SQL-CONTEXT\ntaxi_df_valid_cleaned.createOrReplaceTempView(\"taxi_valid\")\n\n### CREATE FOUR BUCKETS FOR TRAFFIC TIMES\nsqlStatement = \"\"\" SELECT *, CASE\n     WHEN (pickup_hour <= 6 OR pickup_hour >= 20) THEN \"Night\" \n     WHEN (pickup_hour >= 7 AND pickup_hour <= 10) THEN \"AMRush\" \n     WHEN (pickup_hour >= 11 AND pickup_hour <= 15) THEN \"Afternoon\"\n     WHEN (pickup_hour >= 16 AND pickup_hour <= 19) THEN \"PMRush\"\n    END as TrafficTimeBins\n    FROM taxi_valid\n\"\"\"\ntaxi_df_valid_with_newFeatures = spark.sql(sqlStatement)\n\n## APPLY THE SAME TRANSFORATION ON THIS DATA AS ORIGINAL TRAINING DATA\nencodedFinalValid = Pipeline(stages=[sI1, sI2, sI3, sI4]).fit(taxi_df_train_with_newFeatures).transform(taxi_df_valid_with_newFeatures)\n\n## LOAD SAVED MODEL, SCORE VALIDATION DATA, AND EVALUATE\nsavedModel = PipelineModel.load(CVDirfilename)\npredictions = savedModel.transform(encodedFinalValid)\nr2 = evaluator.evaluate(predictions)\nprint(\"R-squared on validation data = %g\" % r2)\n\ndatestamp = datetime.datetime.now().strftime('%m-%d-%Y-%s');\nfileName = \"Predictions_CV_\" + datestamp;\npredictionfile = dataDir + fileName;\npredictions.select(\"label\",\"prediction\").write.mode(\"overwrite\").csv(predictionfile)\nspark.stop()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"034b5f39-fd4d-431e-aaef-fd1eefd6e0f7","_cell_guid":"a0287f95-3c95-4314-9497-9d9a807b51cd","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrip_data_sample = pd.read_csv(\"../input/nyc-taxi/trip_data_sample.csv\")\ntrip_fare_sample = pd.read_csv(\"../input/nyc-yaxi/trip_fare_sample.csv\")","execution_count":0,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrip_data_sample = pd.read_csv(\"../input/sample-dataset-nyc-taxi/trip_data_sample.csv\")\ntrip_fare_sample = pd.read_csv(\"../input/sample-dataset-nyc-taxi/trip_fare_sample.csv\")","execution_count":0,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrip_data_sample_new = pd.read_csv(\"../input/nyc-taxi-new/trip_data_sample_new.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrip_fare_sample_new = pd.read_csv(\"../input/trip_fare_sample_new.csv\")","execution_count":0,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrip_data_sample_new = pd.read_csv(\"../input/trip_data_sample_new.csv\")\ntrip_fare_sample_new = pd.read_csv(\"../input/trip_fare_sample_new.csv\")","execution_count":0,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrip_data_new = pd.read_csv(\"../input/trip_data_new.csv\")","execution_count":0,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrip_data_new = pd.read_csv(\"../input/trip_data_new.csv\")\ntrip_fare_sample_new = pd.read_csv(\"../input/trip_fare_sample_new.csv\")","execution_count":0,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrip_joined_fare = pd.read_csv(\"../input/trip_joined_fare.csv\")","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}