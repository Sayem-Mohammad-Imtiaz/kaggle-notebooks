{"cells":[{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"f9e8d76b-48f8-a70f-c5d1-7ebca7b410bc","_uuid":"0ae3589fb8ddd8221b47b059b18c5c7cb607bf36"},"source":"","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"trusted":false,"_uuid":"77ea142b72ce6e64113b2a29a2f84ed8c3607e34","_execution_state":"idle","_cell_guid":"cd3102d3-e469-d89e-9242-91dc661c6da1"},"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.decomposition import PCA # Principal Component Analysis module\nfrom sklearn.cluster import KMeans # KMeans clustering \nimport matplotlib.pyplot as plt # Python defacto plotting library\nimport seaborn as sns # More snazzy plotting library\n%matplotlib inline ","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"trusted":false,"_uuid":"1d670fc0d950e196f5e2fea55fe3cfd8fc9ac2df","_execution_state":"idle","_cell_guid":"93bddab5-d5b4-40e8-9167-a4799733c495"},"source":"movie = pd.read_csv('../input/movie_metadata.csv') # reads the csv and creates the dataframe called movie\nmovie_genres = pd.concat([movie,movie.genres.str.get_dummies(\"|\")],axis=1)\n\n#movie.head()\n#print(movie.budget)\nmovie_filtered = movie[np.isfinite(movie['budget'])]\nmovie_filtered = movie_filtered[np.isfinite(movie_filtered['gross'])]\nmovie_filtered = movie_filtered[movie_filtered['country'] == 'USA']\nmovie_filtered = movie_filtered[movie_filtered['title_year'] >= 2007.0]\nmovie_filtered = movie_filtered.reset_index()\n#print(movie.budget)\n#print(movie.country)\n#print(movie.title_year)\nmovie_filtered_genres = pd.concat([movie_filtered,movie_filtered.genres.str.get_dummies(\"|\")],axis=1)\ndel movie_filtered_genres['index']\nmovie_filtered_genres.head()","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"trusted":false,"_uuid":"4dfca8e15bb81548cb9f2fa0a1c5a9f3e69c677e","_execution_state":"idle","_cell_guid":"546ba765-b576-0cf5-599e-fcd2904f67ca"},"source":"movieGenre = []\nmovieGenre_number = []\nfor genre in movie_filtered.genres.str.get_dummies(\"|\").columns.values:\n    movieGenre.append(movie_filtered[movie_filtered_genres[genre] != 0])\n    movieGenre[-1] = movieGenre[-1].reset_index()\n    #movieGenre.to_csv(\"movie_filtered_\" + genre + \".csv\")\n    str_list_filtered = [] # empty list to contain columns with strings (words)\n    for colname, colvalue in movieGenre[-1].iteritems():\n        if type(colvalue[1]) == str:\n            str_list_filtered.append(colname)\n    # Get to the numeric columns by inversion            \n    num_list =(movieGenre[-1].columns.difference(str_list_filtered))\n    movieGenre_number.append([movieGenre[-1][num_list],genre])\n    del movieGenre_number[-1][0]['level_0']\n    del movieGenre_number[-1][0]['index']","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"trusted":false,"_uuid":"b9f821f09fc871b44e9b2c07af9d51489648ea77","_cell_guid":"97757a5c-5ca0-81a1-3b33-5770a5020dde","_execution_state":"idle"},"source":"movie_filtered_Documentary = movie_filtered[movie_filtered_genres['Documentary'] == 1]\nmovie_filtered_Documentary.head()","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"e19d8aff4bf4b053abd149ff8c28ef68ce54030e","_execution_state":"idle","_cell_guid":"0657540e-6bcf-1478-d7d0-7b0c45319e72"},"source":"str_list = [] # empty list to contain columns with strings (words)\nfor colname, colvalue in movie.iteritems():\n    if type(colvalue[1]) == str:\n         str_list.append(colname)\n# Get to the numeric columns by inversion            \nnum_list = movie.columns.difference(str_list)       \n\nstr_list_filtered = [] # empty list to contain columns with strings (words)\nfor colname, colvalue in movie_filtered.iteritems():\n    if type(colvalue[1]) == str:\n         str_list_filtered.append(colname)\n# Get to the numeric columns by inversion            \nnum_list_filtered = movie_filtered.columns.difference(str_list_filtered) ","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"trusted":false,"_uuid":"187bc52a290bc2a68f97788953669beee65198fb","_execution_state":"idle","_cell_guid":"1de96952-a2f1-3683-762e-796204e051e5"},"source":"movie_num = movie[num_list]\n#del movie # Get rid of movie df as we won't need it now\nmovie_num.head()\n\nmovie_num_filtered = movie_filtered[num_list_filtered]\n#del movie # Get rid of movie df as we won't need it now\nmovie_num_filtered.head()","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"7b5040a527256fb646b92e0c7cd52d6b040d3e75","_execution_state":"idle","_cell_guid":"28b274dc-8837-2930-3997-41b255c0eaef"},"source":"movie_num = movie_num.fillna(value=0, axis=1)\nmovie_num_filtered = movie_num_filtered.fillna(value=0, axis=1)","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"trusted":false,"_uuid":"c69374294731f94d222b6cc532a39038473b9e10","_execution_state":"idle","_cell_guid":"f6ca79a8-ae86-fbde-4aac-88d1f01522c8"},"source":"X = movie_num.values\n# Data Normalization\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\n\nX_filtered = movie_num_filtered.values\n# Data Normalization\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X_filtered)\n","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"ba3faf11-dde4-44e0-7a05-5186e6443a15","_uuid":"f28478f26c19054845ec5971560efc2cd4445176"},"source":"Let's look at some hexbin visualisations first to get a feel for how the correlations between the different features compare to one another. In the hexbin plots, the lighter in color the hexagonal pixels, the more correlated one feature is to another.","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"1e83aea7019e069b184bedea9fefd7f983ae77f4","_execution_state":"idle","_cell_guid":"66078f78-ba24-48d0-ac49-0cf20614f4f0"},"source":"#movie.plot(y= 'imdb_score', x ='duration',kind='hexbin',gridsize=35, sharex=False, colormap='cubehelix', title='Hexbin of Imdb_Score and Duration',figsize=(12,8))\n#movie.plot(y= 'imdb_score', x ='gross',kind='hexbin',gridsize=45, sharex=False, colormap='cubehelix', title='Hexbin of Imdb_Score and Gross',figsize=(12,8))","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"trusted":false,"_uuid":"986bfb8952cac020157d3a960f04c42e1b71e0d5","_execution_state":"idle","_cell_guid":"6261a0c0-7ca5-f9ce-7dab-ef0ae9c4c7ac"},"source":"#movie_filtered.plot(y= 'imdb_score', x ='duration',kind='hexbin',gridsize=35, sharex=False, colormap='cubehelix', title='Hexbin of Imdb_Score and Duration',figsize=(12,8))\n#movie_filtered.plot(y= 'imdb_score', x ='gross',kind='hexbin',gridsize=45, sharex=False, colormap='cubehelix', title='Hexbin of Imdb_Score and Gross',figsize=(12,8))","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"9d2f0c46-a797-8b6e-2ae5-d5729e055f05","_uuid":"3129354328c6a30fbfe41e6c96d2d40a9c142f4a"},"source":"From the Hexbin plots, one can tell see that the correlation between IMDB score and gross is one that is quite obvious to explain while an interesting result thrown up is that of the score and the duration. ( Interesting!)","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"78c22e8b-e484-ce35-a7ef-5fb0a68540e4","_uuid":"78d11b2a8fb57461384509d1210b6e415e552290"},"source":"Anyway now - time for the customary heatmap per the tradition of most notebooks on Principal Component Analysis. The heatmap is generated to visually show how strongly correlated the values of the dataframe's columns are to one another. Therefore in this matrix the squares that are of a darker colour are more strongly correlated compared to the ones of lighter colour. ","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"trusted":false,"_uuid":"fd93b0e4a857ae2e400e9df727f1523875bc1dad","_execution_state":"idle","_cell_guid":"b56dc8a1-3de8-ce99-e1f9-ffe2576c45c4"},"source":"# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 10))\nplt.title('Pearson Correlation of Movie Features')\n# Draw the heatmap using seaborn\nsns.heatmap(movie_num.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"YlGnBu\", linecolor='black', annot=True)\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 10))\nplt.title('Pearson Correlation of Movie Features_filtered')\n# Draw the heatmap using seaborn\nsns.heatmap(movie_num_filtered.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"YlGnBu\", linecolor='black', annot=True)\n\ni = 0\nfor genre2 in movieGenre_number: \n    i = i + 1\n    f, ax = plt.subplots(figsize=(12, 10))\n    plt.title('Pearson Correlation of Movie Features ' + str(i) + \" \" + genre2[1])\n    # Draw the heatmap using seaborn\n    sns.heatmap(genre2[0].astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"YlGnBu\", linecolor='black', annot=True)\n","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"db2c9e61-5234-fa73-3493-24714456b474","_uuid":"c5fa4e97067ab515c165569f6fbe1070d411a219"},"source":"As we can see from the heatmap, there are regions (features) where we can see quite positive linear correlations amongst each other, given the darker shade of the colours - top left-hand corner and bottom right quarter. This is a good sign as it means we may be able to find linearly correlated features for which we can perform PCA projections on.","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"8d36d73d-2998-30b6-2140-ab5347855bc7","_uuid":"c7b3b46211af8b8b9557cc2ef754eb25182bc7fb"},"source":"# 2. EXPLAINED VARIANCE MEASURE\nAs alluded to in the Introduction, I will be using a particular measure called Explained Variance which will be useful in this context to help us determine the number of PCA projection components we should be looking at. Again this section heavily borrows from Sebastian Raschka's article on Principal Component Analysis so please follow his link for a much more detailed explanation on explained variance than I can do justice to : http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"b57e744b00e07abd56f033de5082ea7c0dc85103","_execution_state":"idle","_cell_guid":"045a6d1d-9b80-da80-361b-b2269e4917eb"},"source":"# Calculating Eigenvectors and eigenvalues of Cov matirx\nmean_vec = np.mean(X_std, axis=0)\ncov_mat = np.cov(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"3d16f75f-6e57-7d08-e617-4ff7ca883496","_uuid":"83201e8bbceecefb55a93d7e6b3c6f19beca5849"},"source":"Now having obtained the eigenvalues and eigenvectors, we will group them together by creating a list of eigenvalue, eigenvector tuples (immutable Python data objects). Following on from this we will sort the list  in order of Highest eigenvalue to lowest eigenvalue and then use the eigenvalues to calculate both the individual explained variance and the cumulative explained variance for visualisation.","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"bc0b7a92a3226930678e2fb26d5a0237376daec2","_execution_state":"idle","_cell_guid":"d64d8e0a-4dc6-4648-4380-9413cfdf6af7"},"source":"# Create a list of (eigenvalue, eigenvector) tuples\neig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort from high to low\neig_pairs.sort(key = lambda x: x[0], reverse= True)\n\n# Calculation of Explained Variance from the eigenvalues\ntot = sum(eig_vals)\nvar_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\ncum_var_exp = np.cumsum(var_exp) # Cumulative explained variance","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"4cdd5b6e-e08e-5048-d40b-6e697dc40326","_uuid":"da6ad23aa1a0163e8ebff60f7a469fb41d2dc0cd"},"source":"Now time to plot the explained variance graphs to see how our contributions look like. The cumulative explained variance is visualised in a blue step-plot while the individual explained variance is plotted via green bar charts as follows: ","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":false,"trusted":false,"_uuid":"bc7b022a5a4b065274d4fbb8bad4b3083d378125","_execution_state":"idle","_cell_guid":"192d6a57-09a6-167c-4afe-670d3603ffad"},"source":"# PLOT OUT THE EXPLAINED VARIANCES SUPERIMPOSED \nplt.figure(figsize=(10, 5))\nplt.bar(range(16), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\nplt.step(range(16), cum_var_exp, where='mid',label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.show()","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"5a2ea559-a5cb-1e2a-ead4-d0b28f93a3be","_uuid":"3a1dfcb448fc49a77b412b42987bd1064d30355b"},"source":"From the plot above, it can be seen that approximately 90% of the variance can be explained with the 9 principal components. Therefore for the purposes of this notebook, let's implement PCA with 9 components ( although to ensure that we are not excluding useful information, one should really go for 95% or greater variance level which corresponds to about 12 components).","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"4b18d6a4-a85b-58e8-76ab-68aac4189fac","_uuid":"9df4ae25c1feada6e415a13d616cd9bf816ecd0d"},"source":"# 3. PRINCIPAL COMPONENT ANALYSIS \nHaving roughly identified how many components/dimensions we would like to project on, let's now implement sklearn's PCA module. \n\nThe first line of the code contains the parameters \"n_components\" which states how many PCA components we want to project the dataset onto. Since we are going implement PCA with 9 components, therefore we set n_components = 9.  \n\nThe second line of the code calls the \"fit_transform\" method, which fits the PCA model with the standardised movie data X_std and applies the dimensionality reduction on this dataset. ","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"c8169d430c4ba47647058d51cbccf2af80c5981b","_execution_state":"idle","_cell_guid":"1dab4fa0-96a7-b5ac-1571-0a516ff6288b"},"source":"pca = PCA(n_components=9)\nx_9d = pca.fit_transform(X_std)","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"f2e21012-2f37-4ad7-5299-178a9f887dee","_uuid":"b7c92c6e0801eeabc950a3d4db954b524cc9ed49"},"source":"Awesome. Having now applied our specific PCA model with the movie dataset, let's visualise the first 2 projection components as a 2D scatter plot to see if we can get a quick feel for the underlying data. ","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"57765809549b0ef4816fbaa85a9eb19695113c81","_execution_state":"idle","_cell_guid":"4d30a956-ccbf-ddfd-df0e-f1234b6c98f0"},"source":"plt.figure(figsize = (9,7))\nplt.scatter(x_9d[:,0],x_9d[:,1], c='goldenrod',alpha=0.5)\nplt.ylim(-10,30)\nplt.show()","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"b175da26-a176-ae07-85fa-cd23acc839c6","_uuid":"85fc7bf7cb94492722f8b3db9240b17ea6b3cee1"},"source":"As a quick aside, my aim (or hope) in carrying out this quick and dirty plotting is to see if we can observe distinct clusters already present within the plots which would be able to tell us if our PCA-transformed data can indeed be linearly separable into different groups for later use as our new features. \n\nHowever from the 2D plot above of the first 2 PCA projections, the first visual impression is that there does not seem to be any discernible clusters. However keeping in mind that our PCA projections contain another 7 components, perhaps looking at plots with the other components may be fruitful. For now, let us assume that will be trying a 3-cluster (just as a naive guess) KMeans to see if we are able to visualise any distinct clusters.","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"b97cf478-6cb5-dee9-a742-91b3c011812e","_uuid":"12ecb0368da8a2bcdeb220a832ac44c384b7a0a5"},"source":"#4. VISUALISATIONS WITH KMEANS CLUSTERING\nA simple KMeans will now be applied to the PCA projection data. Each cluster will be visualised with a different colour so hopefully we will be able to pick out clusters by eye. \n\nTo start off, we set up a KMeans clustering with sklearn's KMeans() and call the \"fit_predict\" method to compute cluster centers and predict cluster indices for the first and third PCA projections (to see if we can observe any appreciable clusters). We then define our own colour scheme and plot the scatter diagram as follows:","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"6f24d4481e12b7f1720d6da16bbdd8ffbc44aaa0","_execution_state":"idle","_cell_guid":"4ff3c5bc-0c2f-3040-4c15-c25e848e68a7"},"source":"# Set a 3 KMeans clustering\nkmeans = KMeans(n_clusters=3)\n# Compute cluster centers and predict cluster indices\nX_clustered = kmeans.fit_predict(x_9d)\n\n# Define our own color map\nLABEL_COLOR_MAP = {0 : 'r',1 : 'g',2 : 'b'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n\n# Plot the scatter digram\nplt.figure(figsize = (7,7))\nplt.scatter(x_9d[:,0],x_9d[:,2], c= label_color, alpha=0.5) \nplt.show()","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"64dc05c3-c4bc-ecf5-0dcd-99ec1f109bbd","_uuid":"6866c8a599153b8745b74d4aabf4b9994d3107e6"},"source":"This KMeans plot looks more promising now as if our simple clustering model assumption turns out to be right, we can observe 3 distinguishable clusters via this color visualisation scheme.\n\nNow, the plot above was only for 2 PCA projections out of the 9 projections that we currently have. However I would also like to generate a KMeans visualisation for other possible combinations of the projections against one another. I will use Seaborn's convenient **pairplot** function to do the job. Basically pairplot automatically plots all the features in the dataframe (in this case our PCA projected movie data) in pairwise manner. I will pairplot the first 3 projections against one another and the resultant plot is given below:","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"524a7b6f94237d6e7a7e92024f31b62a7e524d3e","_execution_state":"idle","_cell_guid":"e66e6d3d-c5d1-4b2d-3335-7908afcb4b9e"},"source":"# Create a temp dataframe from our PCA projection data \"x_9d\"\ndf = pd.DataFrame(x_9d)\ndf = df[[0,1,2]] # only want to visualise relationships between first 3 projections\ndf['X_cluster'] = X_clustered","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"53a380c9a89d4b530117df22e1a10c91c2cbdd2f","_execution_state":"idle","_cell_guid":"f04b3784-396a-b1b1-fc46-a637da0d3524"},"source":"# Call Seaborn's pairplot to visualize our KMeans clustering on the PCA projected data\nsns.pairplot(df, hue='X_cluster', palette= 'Dark2', diag_kind='kde',size=1.85)","outputs":[]},{"execution_count":null,"cell_type":"markdown","metadata":{"_cell_guid":"73fb26f1-2231-6f5c-fe33-099b0bdb4e15","_uuid":"35737a9ae4bb45957377ce7670bb54a6e8072039"},"source":"# Conclusion","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"c93edd75d5c0627db5997c3ef9252556890d4d41","_execution_state":"idle","_cell_guid":"73575ddf-bdac-85fd-1e38-8a6757f1dc3e"},"source":"","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"357c606a6b8b7e4e59e1d717217d2ca89a417663","_execution_state":"idle","_cell_guid":"13d51a34-3b1b-4491-aab2-a494b7fc8e96"},"source":"","outputs":[]},{"execution_count":null,"cell_type":"code","metadata":{"collapsed":true,"trusted":false,"_uuid":"7268e3bd4b199d2f174dd1c1f0d98664a6619f35","_execution_state":"idle","_cell_guid":"3ee63c8e-6e0a-4e76-8923-433d280e68a9"},"source":"","outputs":[]}],"nbformat":4,"metadata":{"_change_revision":0,"_is_fork":false,"language_info":{"name":"python","version":"3.6.1","codemirror_mode":{"name":"ipython","version":3},"mimetype":"text/x-python","file_extension":".py","nbconvert_exporter":"python","pygments_lexer":"ipython3"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":0}