{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b05b33b3c037fe49414400f9b69611e9574c4b33"},"cell_type":"code","source":"#generating data sets\ntrain_data = train.copy()\ntest_data = test.copy()\n\nvectors = test_data.columns\n\ntrain_data = train_data.loc[:, vectors]\ntest_data = test_data.loc[:, vectors]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50d2eddd3239ce1f31afe4996e31c4a2cbf5dba3"},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fecc4f5e53014c8e3ea46283d5b5a36087575984"},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6167724812166c00c2f7c125f9f872890ae3db9"},"cell_type":"code","source":"drop_list = []\n\nfor i in test_data.columns:\n    if test_data[i].isna().sum() >= test_data[i].notnull().sum():\n        drop_list.append(i)\n\nfor i in train_data.columns:\n    if train_data[i].isna().sum() >= train_data[i].notnull().sum():\n        drop_list.append(i)\n        \nprint(drop_list)\n\ndrop_list = list(set(drop_list))\n\nprint(drop_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"539c5a240792a32e2e3bb756e8713ea6aaf112f5"},"cell_type":"code","source":"# dropping too void columns on both dataframes\ntrain_data.drop(drop_list, axis=1, inplace=True)\ntest_data.drop(drop_list, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1965c45ed91ce0dbce066460c094b47003edf63"},"cell_type":"code","source":"for i in train_data.columns:\n    nulls_value = train_data[i].isna().sum()\n    message = \"Column {} has {} nulls\".format(i, nulls_value)\n    print(message)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"199e157fd107fe7b0f902646f03359000f8e8e91"},"cell_type":"code","source":"for i in test_data.columns:\n    nulls_value = test_data[i].isna().sum()\n    message = \"Column {} has {} nulls\".format(i, nulls_value)\n    print(message)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84dcf9971334adfe08d4dfa0cba1e6f58c18f187"},"cell_type":"markdown","source":"With above inspections, we can cut of 3 columns with too much missing data. \nAlso, we can note that NU_NOTA_MT is missing in the same cases as NU_NOTA_LC.\n\nNow, let's explore a bit further into types, and pseudo-numbers that should be categorical or something like that"},{"metadata":{"trusted":true,"_uuid":"f490a21b208e5f58638081a47b503cb592d83cf2"},"cell_type":"code","source":"# checking correlations\n\ndef plot_correlations(data):\n    corr = data.corr()\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n    fig.colorbar(cax)\n    ticks = np.arange(0,len(data.columns),1)\n    ax.set_xticks(ticks)\n    plt.xticks(rotation=90)\n    ax.set_yticks(ticks)\n    ax.set_xticklabels(data.columns)\n    ax.set_yticklabels(data.columns)\n    plt.show()\n\nplot_correlations(test_data.copy())\nvector = plot_correlations(train_data.copy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99554c118d5450588edc658bfd25307f3743ac53"},"cell_type":"code","source":"aux = train.copy()\naux2 = train.copy()\n\naux = aux.loc[:, test_data.columns]\naux['NU_NOTA_MT'] = aux2.NU_NOTA_MT\n\nc = aux.corr()\nc.NU_NOTA_MT.sort_values()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9596f732695c49f167777d82658a379b2650794b"},"cell_type":"markdown","source":"With the code above, we can see the biggest correlated columns... Perhaps dropping the low correlations can give us better models\n\nI will keep only\n\nNU_NOTA_COMP1        0.299402\nNU_NOTA_COMP2        0.335638\nNU_NOTA_COMP4        0.342282\nNU_NOTA_COMP5        0.343337\nNU_NOTA_COMP3        0.350307\nNU_NOTA_REDACAO      0.379376\nNU_NOTA_LC           0.494695\nNU_NOTA_CH           0.529594\nNU_NOTA_CN           0.584941"},{"metadata":{"trusted":true,"_uuid":"d5c0f8bb0eccc25603c89fd258e9f9dc4fbbb917"},"cell_type":"code","source":"new_vector_training = [\n    'NU_NOTA_COMP1',\n    'NU_NOTA_COMP2',\n    'NU_NOTA_COMP4',\n    'NU_NOTA_COMP5',\n    'NU_NOTA_COMP3',\n    'NU_NOTA_REDACAO',\n    'NU_NOTA_LC',\n    'NU_NOTA_CH',\n    'NU_NOTA_CN',\n    'NU_NOTA_MT'\n]\n\nnew_vector_test = [\n    'NU_INSCRICAO',\n    'NU_NOTA_COMP1',\n    'NU_NOTA_COMP2',\n    'NU_NOTA_COMP4',\n    'NU_NOTA_COMP5',\n    'NU_NOTA_COMP3',\n    'NU_NOTA_REDACAO',\n    'NU_NOTA_LC',\n    'NU_NOTA_CH',\n    'NU_NOTA_CN'\n]\n\ntrain_data = train.copy()\ntrain_data = train_data.loc[:, new_vector_training]\ntrain_data.dropna(subset=['NU_NOTA_MT'], inplace=True)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f9d9de3a96ba3c432e1446c4860c68cf410910f"},"cell_type":"code","source":"y = train_data.NU_NOTA_MT\nX = train_data.drop(['NU_NOTA_MT'], axis=1)\n\nvalidation_data = test.copy()\nvalidation_data_1 = validation_data.loc[:, new_vector_test]\nvalidation_data_2 = validation_data.loc[:, new_vector_test]\n\ntrain_X, validation_X, train_y, validation_y = train_test_split(X, y, random_state = 0)\n\nmodel = XGBRegressor(n_estimators=200, learning_rate=0.1)\nmodel.fit(train_X, train_y, early_stopping_rounds=5, eval_set=[(validation_X, validation_y)], verbose=False)\n\nvalidation_data_1.drop(['NU_INSCRICAO'], axis=1, inplace=True)\npredicted_nota = model.predict(validation_data_1)\nanswer_df = pd.DataFrame({'NU_INSCRICAO': validation_data_2.NU_INSCRICAO, 'NU_NOTA_LC': validation_data_2.NU_NOTA_LC, 'NU_NOTA_MT_PREDICT': predicted_nota})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5442fa5b5cfaf15cc83541eb6e2bd075263f24c6"},"cell_type":"code","source":"# almost there... now let's replace any note on math with None when note on LC is NaN (this means a missing value, once both are made on the same day\n\ndef replace_notes(row):\n    if row.NU_NOTA_LC == np.NaN:\n        return np.NaN\n    return row.NU_NOTA_MT_PREDICT\n\nanswer_df['NU_NOTA_MT'] = answer_df.apply(replace_notes, axis='columns')\nanswer_df.loc[answer_df.NU_NOTA_LC.isna(), ['NU_NOTA_MT']] = np.NaN\nanswer_df_final = answer_df.loc[:, ['NU_INSCRICAO', 'NU_NOTA_MT']]\n\n# answer_df.head()\n# answer_df_final.head()\nanswer_df_final.to_csv('answer.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}