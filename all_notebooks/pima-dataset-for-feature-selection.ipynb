{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pima Indians Diabetes dataset for Feature Selection\n---\nSi vuole utilizzare il dataset PID, che è costituito da diverse variabili predittive mediche (indipendenti) e una variabile obiettivo (dipendente), Outcome. Le variabili indipendenti includono svariate caratteristiche, come ad esempio il numero di gravidanze che la paziente ha avuto, il loro BMI, il livello di insulina, l'età e così via."},{"metadata":{},"cell_type":"markdown","source":"Cominciamo con l'ìmportare le librerie di interesse (numpy per le operazioni algebriche e pandas per il data processing e per l'I/O del dataset):"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport sklearn as sk\nimport sklearn.feature_selection\nimport os\n\n#Set the log level in order to hide the unuseful warnings\nimport logging\nlogging.disable(logging.WARNING)\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\" \n\npima_ds = pd.read_csv(\"../input/pima-indians-diabetes-database/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Possiamo dare uno sguardo al dataset appena caricato invocando la funzione *head()*: questo ci permette di capire com'è fatto quel dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"pima_ds.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ad esempio, il campo **Outcome** stabilisce se alla persona è stato diagnosticato il diabete (1) oppure no (0).\n\nAlcuni valori sono lasciati a 0: significa che per quei valori non è stata disposta una misurazione. \n\nOra è il momento di convertire il dataset in un array, separando i dati in variabili differenti in modo da separare le caratteristiche di ognuna:"},{"metadata":{"trusted":true},"cell_type":"code","source":"array = pima_ds.values\nX = array[:,0:8]\nY = array[:,8]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test del Chi Quadrato"},{"metadata":{},"cell_type":"markdown","source":"Innanzitutto, verrà implementato il test del Chi Quadrato per le caratteristiche non negative: questo per selezionare le 4 migliori caratteristiche dal set di dati."},{"metadata":{"trusted":true},"cell_type":"code","source":"test = sk.feature_selection.SelectKBest(score_func=sk.feature_selection.chi2, k=4)\nfit = test.fit(X, Y)\n\n# Summarize scores\nnp.set_printoptions(precision=3)\nprint(fit.scores_)\n\nfeatures = fit.transform(X)\n# Summarize selected features\nprint(features[0:5,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Osservazione:** i pesi per ogni attributo e i 4 attributi scelti (quelli con i punteggi più alti) sono: *Glucose*, *Insuline*, *BMI* ed *Age*. Questi punteggi sono un buon indicatore per determinare le migliori caratteristiche per l'addestramento di un ipotetico modello.\n\n---\n"},{"metadata":{},"cell_type":"markdown","source":"## Recursive Feature Elimination\nLa Recursive Feature Elimination (o RFE) funziona rimuovendo in modo ricorsivo gli attributi e costruendo un modello sugli attributi che rimangono alla fine della procedura.\nRFE utilizza la precisione del modello per identificare gli attributi (e la combinazione di attributi) che contribuiscono maggiormente alla previsione dell'attributo target.\n\nVerrà utilizzata la RFE con il classificatore *Logistic Regression* per trovare le 3 caratteristiche migliori. In questo caso, la scelta dell'algoritmo non impatta significativamente."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature extraction\nmodel = sk.linear_model.LogisticRegression(max_iter=1000)\nrfe = sk.feature_selection.RFE(model, n_features_to_select=3)\nfit = rfe.fit(X, Y)\nprint(\"Num Features: %s\" % (fit.n_features_))\nprint(\"Selected Features: %s\" % (fit.support_))\nprint(\"Feature Ranking: %s\" % (fit.ranking_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Osservazione:** la RFE, come da aspettative, ha selezionato i tre attributi migliori: *Pregnancies*, *BMI* e *DiabetesPedigreeFunction*. Queste caratteristiche sono marchiate **\"True** nella matrice di supporto, con una scelta \"1\" nella matrice di classificazione. Questo, a sua volta, indica il peso di queste caratteristiche."},{"metadata":{},"cell_type":"markdown","source":"## Ridge Regression\nLa Ridge Regression è fondamentalmente una tecnica di regolarizzazione e, allo stesso tempo, anche una tecnica di selezione delle caratteristiche."},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = sk.linear_model.Ridge(alpha=1.0)\nridge.fit(X,Y)\n\nprint(str(ridge.coef_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I risultati ottenuti sono i coefficienti delle incognite $X_i$. Infatti il modello ottenuto applicando la regressione di Ridge può essere riscritto in questo modo:\n$$0.021 X_0 + 0.006 X_1 + -0.002 X_2 + 0 X_3 + 0 X_4 + 0.013 X_5 + 0.145 X_6 + 0.003 X_7$$"},{"metadata":{},"cell_type":"markdown","source":"## Conclusioni\nI metodi che sono stati implementati aiutano a comprendere le caratteristiche di un particolare set di dati in modo completo. La *Feature Selection* è essenzialmente una parte del *preprocessing* dei dati che è considerata la parte più dispendiosa in termini di tempo, in qualsiasi pipeline dei *task* di machine learning.\nLe tecniche esposte servono, quindi, ad affrontare in modo più sistematico e chiaro le problematiche legate ad alcune task di apprendimento automatico: esse, infatti, aiutano l'interpretazione delle caratteristiche, presentandole in maniera più accurata ed evidenziando quelle più rilevanti. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}