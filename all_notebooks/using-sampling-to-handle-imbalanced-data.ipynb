{"cells":[{"metadata":{},"cell_type":"markdown","source":"Many real world classification problems like anomaly detection, churn prediction or fraud detection, the class distributions are always heavily skewed as the probability of occurance of an event(like fraud) is very low.\n\nBut most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class.\n\nIn this notebook, I am using a dataset provided for a hackathon by Novartis.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install missingpy\n!pip install imblearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os, shutil\nimport pprint\nfrom collections import Counter\nimport joblib\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import recall_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\nfrom xgboost import XGBClassifier\nfrom missingpy import MissForest\nfrom imblearn import over_sampling, under_sampling\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Loading data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '/kaggle/input/novartis-data'\n\ntrain = pd.read_csv(os.path.join(DATA_PATH, 'Train.csv'))\ntest_df = pd.read_csv(os.path.join(DATA_PATH, 'Test.csv'))\nprint(\"We have {} rows and {} columns\".format(train.shape[0], train.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Target varaible distributionÂ¶\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Target value distribution\ntarget_vc = train.MULTIPLE_OFFENSE.value_counts()\n\nsns.barplot(x = target_vc.index, y = target_vc.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Filling missing values using random forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = MissForest()\nimputed_data = imputer.fit_transform(train.drop(['INCIDENT_ID', 'DATE', 'MULTIPLE_OFFENSE'], axis=1))\ntrain['X_12'] = imputed_data[:, 11]\ndel imputed_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['INCIDENT_ID', 'DATE'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Helper functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for balancing the data\n\ndef data_sampling(df, over_sampling_strategy, under_sampling_strategy, target_column='MULTIPLE_OFFENSE'):\n  #Over sampling and undersampling funcitons\n  over = over_sampling.SMOTE(sampling_strategy=over_sampling_strategy)\n  under = under_sampling.RandomUnderSampler(sampling_strategy=under_sampling_strategy)\n\n  over_sampled_data, _ = over.fit_resample(train.values, train[target_column].values)\n  sampled_data, _ = under.fit_resample(over_sampled_data, over_sampled_data[:, 15])\n  #Converting sampled data to pandas dataframe\n  sampled_df = pd.DataFrame(sampled_data)\n  sampled_df.columns = train.columns\n  return sampled_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_folds(df, n_folds, target_column='MULTIPLE_OFFENSE'):\n  df['kFold'] = -1\n  kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=18)\n  for fold, (train_idxs,val_idxs) in enumerate(kfold.split(X=df, y=df[target_column].values)):\n    df.loc[val_idxs, 'kFold'] = fold\n  return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(df, n_folds, save_model=True, booster = 'gbtree', learning_rate = 0.1, max_depth = 3, subsample = 1, target_column='MULTIPLE_OFFENSE'):\n  all_recalls = []\n  if save_model:\n    if not os.path.exists('/kaggle/working/models'):\n      os.mkdir('/kaggle/working/models')\n    else:\n      shutil.rmtree('/kaggle/working/models')\n      os.mkdir('/kaggle/working/models')\n  for fold in range(n_folds):\n    train_df = df[df.kFold.isin(FOLD_MAPPING.get(fold))]\n    val_df = df[df.kFold == fold]\n\n    train_X = train_df.drop([target_column, 'kFold'], axis=1)\n    train_y = train_df[target_column].values\n\n    val_X = val_df.drop([target_column, 'kFold'], axis=1)\n    val_y = val_df[target_column].values\n\n    #Model\n    clf = XGBClassifier(booster=booster, learning_rate=learning_rate, max_depth=max_depth, subsample=subsample)\n    clf.fit(train_X, train_y)\n    predictions = clf.predict(val_X)\n    if save_model:\n      joblib.dump(clf, f\"models/{algo}_{fold}.pkl\")\n      print(f\"Saved {algo}_{fold}.pkl\")\n    recall = recall_score(val_y, predictions)\n    all_recalls.append(recall)\n    # print(\"Recall score  for fold {} is {}\".format(fold, recall_score(val_y, predictions)))\n  return all_recalls, np.mean(all_recalls)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(test_df):\n  test_idxs = test_df.INCIDENT_ID.values\n  imputer = MissForest()\n  imputed_data = imputer.fit_transform(test_df.drop(['INCIDENT_ID', 'DATE'], axis=1))\n  test_df['X_12'] = imputed_data[:, 11]\n  del imputed_data\n  test_df = test_df.drop(['INCIDENT_ID', 'DATE'], axis=1)\n  predictions = pd.DataFrame()\n  for fold in range(n_folds):\n    clf = joblib.load(f\"/kaggle/working/models/{algo}_{fold}.pkl\")\n    predictions[f\"pred_{fold}\"] = clf.predict(test_df)\n  final_predictions = predictions.mode(axis=1)[0].values\n  submission = pd.DataFrame({'INCIDENT_ID':test_idxs, 'MULTIPLE_OFFENSE':final_predictions})\n  return submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Run","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 5\nFOLD_MAPPING = {\n    0 : [1,2,3,4],\n    1 : [0,2,3,4],\n    2 : [1,0,3,4],\n    3 : [1,2,0,4],\n    4 : [1,2,3,0],\n}\n\nparameters = {\n    'over_sampling_strategy' : np.arange(0.1, 0.6, 0.1),\n    'under_sampling_strategy' : np.arange(0.8, 0.4, -0.1),\n    'booster' : ['gbtree', 'dart'],\n    'learning_rate' : np.arange(0.1, 0.4, 0.1),\n    'max_depth' : np.arange(3, 8, 1),\n    'subsample' : np.arange(0.5, 1.25, 0.25)\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Grid search to determine oversampling and undersampling ratios","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# scores = []\n\n# for o in parameters['over_sampling_strategy']:\n#   for u in parameters['under_sampling_strategy']:\n#       sampled_df = data_sampling(train, o, u)\n#       df = create_folds(sampled_df, n_folds)\n#       recall_arr, recall_mean = train_fn(df, n_folds, False, 'gbtree', 0.1, 6)\n#       scores.append((o, u, recall_arr, recall_mean))\n#       print(o, u, recall_mean)\n\n# scores = sorted(scores, key=lambda x : x[3], reverse=True)\n# scores[0:2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Grid search to determine for boosting parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# o = 0.5\n# u = 0.5\n# scores2 = []\n# for b in parameters['booster']:\n#   for lr in parameters['learning_rate']:\n#     for d in parameters['max_depth']:\n#         sampled_df = data_sampling(train, o, u)\n#         df = create_folds(sampled_df, n_folds)\n#         recall_arr, recall_mean = train_fn(df, n_folds, False, b, lr, d)\n#         scores2.append((b, lr, d, recall_arr, recall_mean))\n\n# scores2 = sorted(scores2, key=lambda x : x[3], reverse=True)\n# scores2[0:2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Training and validation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"o = 0.5\nu = 0.5\nalgo = 'xgb_gbtree'\nsampled_df = data_sampling(train, o, u)\ndf = create_folds(sampled_df, n_folds)\nrecall_arr, recall_mean = train_fn(df, n_folds, True, booster='gbtree', learning_rate=0.2, max_depth=4)\nprint(recall_mean)\nprint(recall_arr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir('/kaggle/working/models')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Prediction on test data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test(test_df)\nsubmission.to_csv(f\"/kaggle/working/submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}