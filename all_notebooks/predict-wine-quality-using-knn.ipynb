{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi People, This notebook is for complitely beginers. Here i took the dataset and by visualise this i found some good insights about this dataset. I also perform basic algorithms to find the output. If you find this intresting please upvote."},{"metadata":{},"cell_type":"markdown","source":"# Problem Statement:"},{"metadata":{},"cell_type":"markdown","source":"The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult the reference [Cortez et al., 2009]. These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).\n\n__Here our main task is to to find the the insights abour data and make a machine leraning model.__"},{"metadata":{},"cell_type":"markdown","source":"This dataset is also available from the UCI machine learning repository, https://archive.ics.uci.edu/ml/datasets/wine+quality , I just shared it to kaggle for convenience. (If I am mistaken and the public license type disallowed me from doing so, I will take this down if requested.)"},{"metadata":{},"cell_type":"markdown","source":"## About Dataset"},{"metadata":{},"cell_type":"markdown","source":"__1.  fixed acidity:__ Most acids involved with wine or fixed or nonvolatile (do not evaporate readily).\n\n__2.  volatile acidity:__ The amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste.\n\n__3.  citric acid:__ Found in small quantities, citric acid can add 'freshness' and flavor to wines.\n\n__4.  residual sugar:__ The amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter.\n\n__5.  chlorides:__ The amount of salt in the wine.\n\n__6.  free sulfur dioxide:__ The free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents.\n\n__7.  total sulfur dioxide:__ Amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2.\n\n__8.  density:__ The density of water is close to that of water depending on the percent alcohol and sugar content.\n\n__9.  pH:__ Describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4.\n\n__10. sulphates:__ A wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial.\n\n__11. alcohol:__ The percent alcohol content of the wine.\n\n__12. quality:__ Output variable (based on sensory data, score between 0 and 10)."},{"metadata":{},"cell_type":"markdown","source":"# Implimentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import random\nimport warnings\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport seaborn as sns # visualization\nimport matplotlib.pyplot as plt # visualization\nfrom matplotlib.colors import ListedColormap\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import StandardScaler # standardization\nfrom sklearn.model_selection import train_test_split # Split dataset\nfrom sklearn.neighbors import KNeighborsClassifier # KNN Model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB # Naive Bayes Model\nfrom sklearn.metrics import accuracy_score # Accuracy measurements\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [01] Read Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"wine = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.info()\n\n# Here we can see there is no null values in this dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.isnull().sum()\n\n# No null values present in data frame.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.shape\n\n# Total data points are 1599","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.columns\n\n# List of features we have in this dataset.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can say most of wines are between 5 to 6 range which is average. 3 is the lowest qulity and 8 is the highest."},{"metadata":{},"cell_type":"markdown","source":"# [02] EDA"},{"metadata":{},"cell_type":"markdown","source":"## [2.1] Corelation Matrix: Visualise the similarity between features"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = wine.corr()\ncorr","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [2.2] Heat Map using corelation metrix:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate a mask for the upper triangle\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nwith sns.axes_style(\"white\"):\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(10, 8))\n    ax = sns.heatmap(corr, cmap=cmap, mask=mask, vmax=.3, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n\nfrom here we notice that impact of **'residual sugar'**, **'free sulfur dioxide'** and **'ph'** on **'quality'** is neglisible. So we can drop them. Also we observer that similarity between **'alcohol'** and **'quality'** is maximum."},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.drop([\"residual sugar\",'free sulfur dioxide','pH'],axis = 1,inplace = True)\nwine.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [2.3] Univariate Analysis: On Alcohol"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(wine['alcohol'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To analyze more,\n\nLet's split **'alcohol'** into three parts such as, **low, median and high**."},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = [0, 10, 12, 15]\nlabels = [\"low\",\"median\",\"high\"]\nwine['alcohol_label'] = pd.cut(wine['alcohol'], bins=bins, labels=labels)\nwine.drop('alcohol',axis =1, inplace = True)\nwine.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [2.4] Univariate Analysis: On Quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(wine['quality'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly as alcohol, \n\nWe can split **quality** also into three small chuncks such as **poor** i.e. from 0 to 4, **normal** i.e. from 5 or 6 and **excellent** i.e. from 7 to 10."},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = [0, 4, 6, 10]\nlabels = [\"poor\",\"normal\",\"excellent\"]\nwine['quality_label'] = pd.cut(wine['quality'], bins=bins, labels=labels)\nwine.drop('quality',axis =1, inplace = True)\nwine.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [2.5] Multivariate Analysis: Using Pairplots"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(wine, hue=\"quality_label\", palette=\"husl\",diag_kind=\"kde\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n\nTo betermine wine qulity, **volatile acidity and citric acid** can be our important features as the overlap of their distributions are vary less in compare to others."},{"metadata":{},"cell_type":"markdown","source":"## [2.6] Analysis On: Volatile Acidity and Citric Acid"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(wine,hue='quality_label', height=5).map(sns.distplot,'volatile acidity').add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='quality_label',y='volatile acidity', data=wine)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n\nFrom this plot we can assure that, More than 50% of our excellent catagory wine have volatile acidity in between 0.3 to 0.5."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.FacetGrid(wine,hue='quality_label', height=5).map(sns.distplot,'citric acid').add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='quality_label', y='citric acid',data=wine)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n\nSimilary from this plot we can say, More than 50% of our excellent catagory wine have citric acid in between 0.3 to 0.5."},{"metadata":{},"cell_type":"markdown","source":"# [03] Model Creation"},{"metadata":{"trusted":true},"cell_type":"code","source":"wine.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wine['alcohol_label'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert category values to numeric values by creating dummy featutes.\ndf_wine = pd.get_dummies(wine, columns=['alcohol_label'], drop_first=True)\ndf_wine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = df_wine['quality_label']\ndf_wine.drop(['quality_label'], axis=1, inplace=True)\nprint(df_wine.shape, result.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use 70% of the data for training and 30% for testing\nX_train, X_test, Y_train, Y_test = train_test_split(df_wine, result, test_size=0.30, random_state=11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we deal with distance that case we need to standadise our data other wise we can use normal data. In our case for KNN we need scdardise data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# For KNN our dataset must have to standardised.\n# No need standardised quality_label as it is the result column\n\nscaler = StandardScaler()\nscaler.fit(df_wine)\nscaled_features = scaler.transform(df_wine)\ndf_wine_sc = pd.DataFrame(scaled_features, columns=df_wine.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use 70% of the data for training and 30% for testing\nX_train_sc, X_test_sc, y_train_sc, y_test_sc = train_test_split(df_wine_sc, result, test_size=0.30, random_state=11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [3.1] Train Model:"},{"metadata":{},"cell_type":"markdown","source":"#### [3.1.2] For KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert dataframe to nd numpy array\nX_train_sc = X_train_sc.to_numpy()\ny_train_sc = y_train_sc.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_knn(neigh, weight='uniform'):\n    knn = KNeighborsClassifier(n_neighbors=neigh, weights=weight)\n    knn.fit(X_train_sc,y_train_sc)\n    pred_knn = knn.predict(X_test_sc)\n    return pred_knn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_knn_for_20 = apply_knn(20)\nprint('Accuracy of model at K=20 is', accuracy_score(y_test_sc, pred_knn_for_20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it’s time to improve the model and find out the optimal k value."},{"metadata":{},"cell_type":"markdown","source":"#### [3.1.2] For LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = LogisticRegression(random_state=0)\nclf.fit(X_train_sc, y_train_sc)\npred_lr = clf.predict(X_test_sc)\nprint('Accuracy of model is', accuracy_score(y_test_sc, pred_lr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [3.2] Apply GridSearchCV to Optomise Parameters"},{"metadata":{},"cell_type":"markdown","source":"#### [3.2.1] For KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsClassifier()\n\nparams = {'n_neighbors':list(range(1, 50, 2)), 'weights':['uniform', 'distance']}\n\ngs = GridSearchCV(model, params, cv = 5, n_jobs=-1)\n\ngs_results = gs.fit(X_train_sc, y_train_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best Accuracy: ', gs_results.best_score_)\nprint('Best Parametrs: ', gs_results.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From above we can say that the best value k is 13.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_k = 13\nbest_weights = 'distance'\npred_knn_for_Best_k = apply_knn(best_k, best_weights)\nprint('Accuracy of model at K=13 is ', accuracy_score(y_test_sc, pred_knn_for_Best_k))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### [3.2.2] For LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(max_iter=10000)\n\nparams = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4], 'penalty': ['l1', 'l2']}]\n\ngs = GridSearchCV(model, params, cv=5, n_jobs=-1)\n\ngs_results = gs.fit(X_train_sc, y_train_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best Accuracy: ', gs_results.best_score_)\nprint('Best Parametrs: ', gs_results.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From above we can say that the best parameters are C = '1' & penalty = 'l2'.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=1, penalty='l2', random_state=0)\n\nlr.fit(X_train_sc, y_train_sc)\n\npred_lr_for_Best_param = lr.predict(X_test_sc)\n\nprint('Accuracy of model at C = 1 and Penalty = l2 is', accuracy_score(y_test_sc, pred_lr_for_Best_param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [3.3] K fold Cross-Validation:"},{"metadata":{},"cell_type":"markdown","source":"#### [3.3.1] for KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=13,weights='distance')\nscores_knn = cross_val_score(knn, X_train_sc, y_train_sc, cv=10, scoring='accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores_knn.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From KNN model we can conclude that our model's best accuracy is approx 85%**"},{"metadata":{},"cell_type":"markdown","source":"#### [3.3.2] for LogisticRegression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=1, penalty='l2',random_state=0)\nscores_lr = cross_val_score(lr, X_train_sc, y_train_sc, cv=10, scoring='accuracy')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores_lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scores_lr.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From LR model we can conclude that our model's best accuracy is approx 84% which is a bit lower than KNN**"},{"metadata":{},"cell_type":"markdown","source":"## [3.4] Confusion Matrice for KNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.DataFrame(y_test_sc)['quality_label'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test_sc, pred_knn_for_Best_k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [\"excellent\",\"normal\",\"poor\"]\nprint(pd.DataFrame(cm, index=names, columns=names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [3.4] Confusion Matrice for LR"},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = confusion_matrix(y_test_sc, pred_lr_for_Best_param)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [\"excellent\",\"normal\",\"poor\"]\nprint(pd.DataFrame(cm, index=names, columns=names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# [06] Conclusion:\n\nHowever, what we should not ignore is that the wine quality classes are ordered and not balanced. There are much more normal wines than excellent or poor ones. Consequently, as we can see from the confusion matrix above, the predictive ability of the model for normal class(accuracy rate = 0.966) is much better than poor(accuracy rate = 0) and excellent class(accuracy rate = 0.265).\n\nFor the next step, we should try to find out methods to improve the model performance for unbalanced dataset."},{"metadata":{},"cell_type":"markdown","source":"## Thank for going through this notebook. If you find this notebook useful then please upvote."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}