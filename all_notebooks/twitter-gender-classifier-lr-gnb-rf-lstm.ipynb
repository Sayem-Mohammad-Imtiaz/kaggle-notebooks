{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport re\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, recall_score,f1_score, classification_report, log_loss\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-01T11:34:49.764019Z","iopub.execute_input":"2021-07-01T11:34:49.764504Z","iopub.status.idle":"2021-07-01T11:34:57.918657Z","shell.execute_reply.started":"2021-07-01T11:34:49.764424Z","shell.execute_reply":"2021-07-01T11:34:57.917701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/twitter-user-gender-classification/gender-classifier-DFE-791531.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:57.920125Z","iopub.execute_input":"2021-07-01T11:34:57.920468Z","iopub.status.idle":"2021-07-01T11:34:58.348452Z","shell.execute_reply.started":"2021-07-01T11:34:57.920428Z","shell.execute_reply":"2021-07-01T11:34:58.347455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.35002Z","iopub.execute_input":"2021-07-01T11:34:58.350289Z","iopub.status.idle":"2021-07-01T11:34:58.40546Z","shell.execute_reply.started":"2021-07-01T11:34:58.350261Z","shell.execute_reply":"2021-07-01T11:34:58.404288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.406465Z","iopub.execute_input":"2021-07-01T11:34:58.406658Z","iopub.status.idle":"2021-07-01T11:34:58.428202Z","shell.execute_reply.started":"2021-07-01T11:34:58.406637Z","shell.execute_reply":"2021-07-01T11:34:58.427341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['gender'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.429084Z","iopub.execute_input":"2021-07-01T11:34:58.429286Z","iopub.status.idle":"2021-07-01T11:34:58.442885Z","shell.execute_reply.started":"2021-07-01T11:34:58.429264Z","shell.execute_reply":"2021-07-01T11:34:58.441659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['gender'] = [1 if gender == 'male' else 0 for gender in df.gender]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.443899Z","iopub.execute_input":"2021-07-01T11:34:58.444101Z","iopub.status.idle":"2021-07-01T11:34:58.466837Z","shell.execute_reply.started":"2021-07-01T11:34:58.44408Z","shell.execute_reply":"2021-07-01T11:34:58.465755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['gender', 'description']].count()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.468022Z","iopub.execute_input":"2021-07-01T11:34:58.468307Z","iopub.status.idle":"2021-07-01T11:34:58.488034Z","shell.execute_reply.started":"2021-07-01T11:34:58.468275Z","shell.execute_reply":"2021-07-01T11:34:58.486813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = df[['gender', 'description']]\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.489939Z","iopub.execute_input":"2021-07-01T11:34:58.490148Z","iopub.status.idle":"2021-07-01T11:34:58.504578Z","shell.execute_reply.started":"2021-07-01T11:34:58.490126Z","shell.execute_reply":"2021-07-01T11:34:58.503216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.506474Z","iopub.execute_input":"2021-07-01T11:34:58.506786Z","iopub.status.idle":"2021-07-01T11:34:58.52617Z","shell.execute_reply.started":"2021-07-01T11:34:58.506735Z","shell.execute_reply":"2021-07-01T11:34:58.525634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.dropna()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.527077Z","iopub.execute_input":"2021-07-01T11:34:58.527266Z","iopub.status.idle":"2021-07-01T11:34:58.563522Z","shell.execute_reply.started":"2021-07-01T11:34:58.527245Z","shell.execute_reply":"2021-07-01T11:34:58.562964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.56443Z","iopub.execute_input":"2021-07-01T11:34:58.564811Z","iopub.status.idle":"2021-07-01T11:34:58.573432Z","shell.execute_reply.started":"2021-07-01T11:34:58.564786Z","shell.execute_reply":"2021-07-01T11:34:58.57216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets clean the description data\ndata.description = data.description.str.lower()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.574666Z","iopub.execute_input":"2021-07-01T11:34:58.57504Z","iopub.status.idle":"2021-07-01T11:34:58.602554Z","shell.execute_reply.started":"2021-07-01T11:34:58.575012Z","shell.execute_reply":"2021-07-01T11:34:58.601376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove the user handles that start with @ using regular expression\ndata.description = data.description.replace('[@+]', '', regex=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.603596Z","iopub.execute_input":"2021-07-01T11:34:58.603888Z","iopub.status.idle":"2021-07-01T11:34:58.687377Z","shell.execute_reply.started":"2021-07-01T11:34:58.603857Z","shell.execute_reply":"2021-07-01T11:34:58.686089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using regular expressions, remove URLs.a\ndata.description= data.description.replace(r\"(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)\", \"\", regex=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.689011Z","iopub.execute_input":"2021-07-01T11:34:58.689391Z","iopub.status.idle":"2021-07-01T11:34:58.994351Z","shell.execute_reply.started":"2021-07-01T11:34:58.689361Z","shell.execute_reply":"2021-07-01T11:34:58.993403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets = data.description.values\ntweets[2]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:58.99549Z","iopub.execute_input":"2021-07-01T11:34:58.996026Z","iopub.status.idle":"2021-07-01T11:34:59.002453Z","shell.execute_reply.started":"2021-07-01T11:34:58.995987Z","shell.execute_reply":"2021-07-01T11:34:59.001549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Using TweetTokenizer from NLTK, tokenize the tweets into individual terms.\nfrom nltk.tokenize import TweetTokenizer\ntokenizer = TweetTokenizer()\n#tweets1 = []\n\n#for tweet in tweets:    \n    #tweets1.append(tokenizer.tokenize(tweet))\n    \ntweets = [tokenizer.tokenize(tweet) for tweet in data.description]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:34:59.003817Z","iopub.execute_input":"2021-07-01T11:34:59.004445Z","iopub.status.idle":"2021-07-01T11:35:00.479614Z","shell.execute_reply.started":"2021-07-01T11:34:59.004407Z","shell.execute_reply":"2021-07-01T11:35:00.478937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweets[:2]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:00.480722Z","iopub.execute_input":"2021-07-01T11:35:00.481228Z","iopub.status.idle":"2021-07-01T11:35:00.488869Z","shell.execute_reply.started":"2021-07-01T11:35:00.481188Z","shell.execute_reply":"2021-07-01T11:35:00.48785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove punctuations and special chars\n\nnew_tweets =  []\nstop_words = set(stopwords.words('english'))\npunctuations = '''!()-![]{};:+'\"\\,<>./?@Ÿ‡Œ£º¦¬ÃŠ©¤€«¢œ®°$%^&*_~#Ã°ÂŸÂ“Â±!!! Ã°ÂŸÂ˜Â™Ã°ÂŸÂ˜ÂŽÃ°ÂŸÂ‘Â'''\nfor tweet in tweets:\n    new_tweets.append([i for i in tweet if not i in punctuations and not i in stop_words])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:00.490113Z","iopub.execute_input":"2021-07-01T11:35:00.490341Z","iopub.status.idle":"2021-07-01T11:35:00.661682Z","shell.execute_reply.started":"2021-07-01T11:35:00.490318Z","shell.execute_reply":"2021-07-01T11:35:00.661009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_tweets[:2]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:00.662897Z","iopub.execute_input":"2021-07-01T11:35:00.663376Z","iopub.status.idle":"2021-07-01T11:35:00.669613Z","shell.execute_reply.started":"2021-07-01T11:35:00.663337Z","shell.execute_reply":"2021-07-01T11:35:00.668505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create text again and add it as a new column in df DATA\ndata['new_description'] = new_tweets\ndata['new_description'] = [\" \".join(desc) for desc in data['new_description'].values]","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:00.670978Z","iopub.execute_input":"2021-07-01T11:35:00.671502Z","iopub.status.idle":"2021-07-01T11:35:00.715678Z","shell.execute_reply.started":"2021-07-01T11:35:00.671402Z","shell.execute_reply":"2021-07-01T11:35:00.714851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:00.716791Z","iopub.execute_input":"2021-07-01T11:35:00.717137Z","iopub.status.idle":"2021-07-01T11:35:00.738809Z","shell.execute_reply.started":"2021-07-01T11:35:00.717109Z","shell.execute_reply":"2021-07-01T11:35:00.737369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets create a word cloud\nfrom wordcloud import WordCloud\nall_words = ' '.join([text for text in data['new_description']])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10,8))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:00.74009Z","iopub.execute_input":"2021-07-01T11:35:00.740355Z","iopub.status.idle":"2021-07-01T11:35:03.791057Z","shell.execute_reply.started":"2021-07-01T11:35:00.740328Z","shell.execute_reply":"2021-07-01T11:35:03.790022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# identify X and y\nX = data['new_description']\ny = data['gender']","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:03.794924Z","iopub.execute_input":"2021-07-01T11:35:03.795233Z","iopub.status.idle":"2021-07-01T11:35:03.800515Z","shell.execute_reply.started":"2021-07-01T11:35:03.795205Z","shell.execute_reply":"2021-07-01T11:35:03.799196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:03.802145Z","iopub.execute_input":"2021-07-01T11:35:03.80244Z","iopub.status.idle":"2021-07-01T11:35:03.817363Z","shell.execute_reply.started":"2021-07-01T11:35:03.802405Z","shell.execute_reply":"2021-07-01T11:35:03.816788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split the data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:03.818103Z","iopub.execute_input":"2021-07-01T11:35:03.818291Z","iopub.status.idle":"2021-07-01T11:35:03.831658Z","shell.execute_reply.started":"2021-07-01T11:35:03.818268Z","shell.execute_reply":"2021-07-01T11:35:03.830822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:03.833272Z","iopub.execute_input":"2021-07-01T11:35:03.833649Z","iopub.status.idle":"2021-07-01T11:35:03.846955Z","shell.execute_reply.started":"2021-07-01T11:35:03.833619Z","shell.execute_reply":"2021-07-01T11:35:03.845807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#After the initial preprocessing phase, we need to transform the text into a meaningful vector (or array) of numbers. \n#The bag-of-words is a representation of text that describes the occurrence of words within a document\n# lets use TFIDF vectorizer for this purpose\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(max_features=5000)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:03.848459Z","iopub.execute_input":"2021-07-01T11:35:03.848865Z","iopub.status.idle":"2021-07-01T11:35:03.860044Z","shell.execute_reply.started":"2021-07-01T11:35:03.848831Z","shell.execute_reply":"2021-07-01T11:35:03.858924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fit and apply on train set\ntrans_X_train = vectorizer.fit_transform(X_train)\n#Apply on the test set.\ntrans_X_test = vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:03.86159Z","iopub.execute_input":"2021-07-01T11:35:03.861992Z","iopub.status.idle":"2021-07-01T11:35:04.294578Z","shell.execute_reply.started":"2021-07-01T11:35:03.861954Z","shell.execute_reply":"2021-07-01T11:35:04.293689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets apply different classification models\n# logistic regression\n\nlr_df = pd.DataFrame()\n\nlr = LogisticRegression()\nlr.fit(trans_X_train, y_train)\n\nlr_pred = lr.predict(trans_X_test)\nprint('accuracy score with logistic regression: ', accuracy_score(y_test, lr_pred))\nprint('-------------------------------------------')\nprint(classification_report(y_test, lr_pred))\n\nlr_df['description'] = X_test\nlr_df['actual values'] = y_test\nlr_df['predicted values'] = lr_pred\n\n\nlr_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:04.295889Z","iopub.execute_input":"2021-07-01T11:35:04.296244Z","iopub.status.idle":"2021-07-01T11:35:04.583268Z","shell.execute_reply.started":"2021-07-01T11:35:04.296203Z","shell.execute_reply":"2021-07-01T11:35:04.582244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random forest classififer\n\nrf_df = pd.DataFrame()\n\nrf = RandomForestClassifier()\nrf.fit(trans_X_train, y_train)\n\nrf_pred = rf.predict(trans_X_test)\nprint('accuracy score with random forest classifier: ', accuracy_score(y_test, rf_pred))\nprint('-------------------------------------------')\nprint(classification_report(y_test, rf_pred))\n\nrf_df['description'] = X_test\nrf_df['actual values'] = y_test\nrf_df['predicted values'] = rf_pred\n\n\nrf_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:04.584556Z","iopub.execute_input":"2021-07-01T11:35:04.58492Z","iopub.status.idle":"2021-07-01T11:35:31.589864Z","shell.execute_reply.started":"2021-07-01T11:35:04.584883Z","shell.execute_reply":"2021-07-01T11:35:31.588635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gaussian Naive Bayes classifier\n\ngnb_df = pd.DataFrame()\n\ngnb = GaussianNB()\ngnb.fit(trans_X_train.toarray(), y_train)\n\ngnb_pred = gnb.predict(trans_X_test.toarray())\nprint('accuracy score with Gaussian Naive Bayes classifier: ', accuracy_score(y_test, gnb_pred))\nprint('-------------------------------------------')\nprint(classification_report(y_test, gnb_pred))\n\ngnb_df['description'] = X_test\ngnb_df['actual values'] = y_test\ngnb_df['predicted values'] = gnb_pred\n\n\ngnb_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:31.591085Z","iopub.execute_input":"2021-07-01T11:35:31.591392Z","iopub.status.idle":"2021-07-01T11:35:32.509919Z","shell.execute_reply.started":"2021-07-01T11:35:31.591363Z","shell.execute_reply":"2021-07-01T11:35:32.508863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #lets try lstm into this\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n#txt_len = X_train.apply(lambda x : len(x.split(' ')))\n# txt_len = 22, considering max_txt_len = 25\nMAX_SEQ_LEN  = 25\nDEFAULT_BATCH_SIZE = 128\n\ntokenizer = Tokenizer()\n\ntokenizer.fit_on_texts(X_train)\ntrain_vec = tokenizer.texts_to_sequences(X_train)\ntest_vec = tokenizer.texts_to_sequences(X_test)\n\n# pad the sequences\ntrain_vec= pad_sequences(train_vec, maxlen=MAX_SEQ_LEN)\ntest_vec= pad_sequences(test_vec, maxlen=MAX_SEQ_LEN)\n\n# \nprint('token count:', len(tokenizer.word_index))\nprint(\"token index(max):\", train_vec.max())\n\nprint('Tweet Before tokenizing:', X_train.values[1])\nprint('Tweet After tokenizing:', tokenizer.sequences_to_texts([train_vec[1]]))\n\nprint('tokenized values sample:', train_vec[1].tolist())\n","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:54:19.257212Z","iopub.execute_input":"2021-07-01T11:54:19.257595Z","iopub.status.idle":"2021-07-01T11:54:19.802341Z","shell.execute_reply.started":"2021-07-01T11:54:19.257571Z","shell.execute_reply":"2021-07-01T11:54:19.801124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets try lstm into this\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Embedding\n\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = DEFAULT_BATCH_SIZE, \n                    input_length = MAX_SEQ_LEN))\nmodel.add(LSTM(units=128, return_sequences=True))\n#model.add(Dropout(0.02))\nmodel.add(LSTM(units=64))\nmodel.add(Dense(1, activation='softmax'))\n\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:33.805049Z","iopub.execute_input":"2021-07-01T11:35:33.805297Z","iopub.status.idle":"2021-07-01T11:35:34.457314Z","shell.execute_reply.started":"2021-07-01T11:35:33.805269Z","shell.execute_reply":"2021-07-01T11:35:34.45627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n\n# fit the data into model\nmodel.fit(train_vec, y_train, epochs=24, batch_size=32, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:35:34.459814Z","iopub.execute_input":"2021-07-01T11:35:34.460116Z","iopub.status.idle":"2021-07-01T11:44:07.272408Z","shell.execute_reply.started":"2021-07-01T11:35:34.460089Z","shell.execute_reply":"2021-07-01T11:44:07.271663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n\\n****************************\\n\\n\")\n#print('Loading Best Model...')\n#model.load_weights('./model_1.h5')\npredictions = model.predict(test_vec, verbose=1)\nprint('Validation Loss:', log_loss(y_test, predictions))\nprint('Test Accuracy', accuracy_score(y_test, predictions))\nprint('F1 Score:', f1_score(y_test, predictions))\n#plot_confusion_matrix(y_test.argmax(axis = 1), predictions.argmax(axis = 1), classes=encoder.classes_)\n#plt.show()    ","metadata":{"execution":{"iopub.status.busy":"2021-07-01T11:44:07.273303Z","iopub.execute_input":"2021-07-01T11:44:07.273524Z","iopub.status.idle":"2021-07-01T11:44:09.797594Z","shell.execute_reply.started":"2021-07-01T11:44:07.273499Z","shell.execute_reply":"2021-07-01T11:44:09.796202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**from above we see that LSTM and Naive Bayes performed very bad, instead Logistic regression and Random Forest performed decent with 70% accuracy**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}