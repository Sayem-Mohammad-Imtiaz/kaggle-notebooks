{"cells":[{"metadata":{"_uuid":"41756eea-be31-4108-b2e4-91569ae4c231","_cell_guid":"b860d3e0-5b9a-4c53-85d1-5ab8db3562b3","trusted":true},"cell_type":"markdown","source":"# Diabetes in India\n\nOver 30 million have now been diagnosed with diabetes in India. The CPR (Crude prevalence rate) in the urban areas of India is thought to be 9 per cent.\n\nIn rural areas, the prevalence is approximately 3 per cent of the total population.\n\nThe population of India is now more than 1000 million: this helps to give an idea of the scale of the problem.\n\n**The estimate of the actual number of diabetics in India is around 40 million.**\n\n**This means that India actually has the highest number of diabetics of any one country in the entire world. IGT (Impaired Glucose Tolerance) is also a mounting problem in India.**\n\n[Source Link](https://www.diabetes.co.uk/global-diabetes/diabetes-in-india.html)\n\n![Diabetes in India](https://www.thehindu.com/sci-tech/health/thu33t/article29975026.ece/alternates/FREE_615/TH15diabetescol)\n\n[Image Source](https://www.thehindu.com/sci-tech/health/india-has-second-largest-number-of-people-with-diabetes/article29975027.ece)"},{"metadata":{"_uuid":"b967bd62-35af-4590-8c0e-1d83c69cfa86","_cell_guid":"20426200-1341-44ca-90e5-300242c96b67","trusted":true},"cell_type":"markdown","source":"## As a Beginner in Data Science or into Kaggle, this Notebook will come in handy for implementing Machine Learning Models.\n\n### Topics Covered:\n- Pandas Profiling(Preferred for Data Analysis)\n- Data Scaling\n- Data Modeling\n- Hyper-parameter Tuning.\n- Cross Validation\n- Predictions"},{"metadata":{"_uuid":"a4bdd76b-c122-45a7-8933-b919b3b9256e","_cell_guid":"00502afe-a3d7-43f1-8b67-b86aec456de9","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport\nsns.set_style('darkgrid')\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69d3d306-75fe-43c1-b960-96ff82f2fa88","_cell_guid":"f63cc64a-4ef4-4911-a7cb-06dc344954ad","trusted":true},"cell_type":"markdown","source":"## 1. Daily Analysis using Pandas profiling library [Resource](https://pypi.org/project/pandas-profiling/)"},{"metadata":{"_uuid":"506cccbe-1126-4745-a9eb-cfabc79f5a86","_cell_guid":"90f6f1a7-a9d4-44e1-8037-5ac222287550","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\ndata.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e83800f-e0cc-42c1-bb9e-d5becf237f46","_cell_guid":"6d8c3dba-80b8-4eb1-9f8e-2c36342c93b4","trusted":true},"cell_type":"code","source":"profile = ProfileReport(data, title=\"Pandas Profiling Report\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14bed0e0-a3d6-4793-bd17-73d2e95ace52","_cell_guid":"2b1fa4d4-8d7f-4cec-9280-157d30cc1b29","trusted":true},"cell_type":"code","source":"## Printing the complete Data Analysis Report!\n## we have a balanced dataset\nprofile","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30ec6b40-748d-4d0f-80b0-cffcf63d52e0","_cell_guid":"738a3cbc-2bda-4cc1-b1a4-c94b21849e3e","trusted":true},"cell_type":"code","source":"## The 'Outcome' column tells us whether the person has Diabetes or not\n## 1- Diabetic\n## 0- Non Diabetic\n\nX = data.iloc[:,:-1] # Independent variables\ny = data['Outcome'] # Dependent Variables","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dcacada-47c4-4ca4-ad63-7706904c85a4","_cell_guid":"3dc5fc6c-dfbd-4b5b-8328-8dc2d146faa3","trusted":true},"cell_type":"markdown","source":"## Data Cleaning\n### Removing outliers from the Independent variables"},{"metadata":{"_uuid":"30c07aa6-fdb7-4486-a5c5-71ac615267f1","_cell_guid":"d9b2bb1d-b2df-42d8-8ce1-27cc76a0e2b6","trusted":true},"cell_type":"code","source":"data = data.drop(data[data['Pregnancies']>11].index)\ndata = data.drop(data[data['Glucose']<30].index)\ndata = data.drop(data[data['BloodPressure']>110].index)\ndata = data.drop(data[data['BloodPressure']<20].index)\ndata = data.drop(data[data['SkinThickness']>80].index)\ndata = data.drop(data[data['BMI']>55].index)\ndata = data.drop(data[data['BMI']<10].index)\ndata = data.drop(data[data['DiabetesPedigreeFunction']>1.6].index)\ndata = data.drop(data[data['Insulin']>400].index)\ndata = data.drop(data[data['Age']>80].index)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f9009e0-56c8-4c2f-92e7-ab8adcf9becd","_cell_guid":"db6f7df2-5631-4d9f-977b-be19c7a9981e","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(11,10))\ncorrelation = X.corr()\nsns.heatmap(correlation,linewidth = 0.7,cmap = 'Blues',annot = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"667d4dfe-7396-4718-8776-05bc6a4c38fc","_cell_guid":"efcd91bf-56ed-47b8-a5eb-1628485b6f4e","trusted":true},"cell_type":"code","source":"X = X.loc[data.index]\ny = y.loc[data.index]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d97bd20-1f75-45cb-bd18-f90f32aa1e6c","_cell_guid":"909799a1-938b-4781-8f12-a760bc4e865a","trusted":true},"cell_type":"markdown","source":"## Data Scaling\n\n### Note: DataScaling will not be required for this Notebook, since we are using Boosting method of Decision Trees which doesn't mind the distance between the data points\n\n**Although here are the steps to implement the same**\n\n- from sklearn.preprocessing import RobustScaler\n- cols = X.columns\n- transformer = RobustScaler().fit(X[cols])\n- X[cols] = transformer.transform(X[cols])"},{"metadata":{"_uuid":"8f24cd8a-37a2-4042-8b05-884420fa522a","_cell_guid":"e7abb329-24d7-47e1-aa2e-98d0c3559b7c","trusted":true},"cell_type":"markdown","source":"## Data Modeling\n\n- Step 1. Divide the Dataset into Test and Train set, where 75% data belongs to Train set and 25% to test size\n\n- Step 2: Using the imported library train your model and then fit it.\n\n- Using RandomSearchCV, try to get the best paramters for best results"},{"metadata":{"_uuid":"c20c2e14-abce-4958-b90c-996d136ceeb2","_cell_guid":"8fd3db22-efde-4870-b4ae-c8e6f272f92a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size = 0.25,random_state = 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b77fb2c-0e7c-45a3-af7f-870c48590051","_cell_guid":"6705b999-1752-454c-819e-aa094102ef0e","trusted":true},"cell_type":"code","source":"from sklearn.metrics import log_loss,accuracy_score,confusion_matrix,f1_score,recall_score\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold,RandomizedSearchCV\nxgb = XGBClassifier(booster ='gbtree',objective ='binary:logistic')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1be6e167-6990-4eeb-947b-000eada67e55","_cell_guid":"ba96c368-05a8-4d18-9c96-81e2b9e80c33","trusted":true},"cell_type":"markdown","source":"### You can find more information about Hyperparameter tuning using this [Github Repo](https://github.com/Lokeshrathi/HyperParamter_optimisation)"},{"metadata":{"_uuid":"c7c55b7f-c636-4989-8b73-3610581c98c4","_cell_guid":"c4af2904-928a-4fea-923a-a282b5f0585c","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nparam_lst = {\n    'learning_rate' : [0.01, 0.1, 0.15, 0.3, 0.5,0.4],\n    'n_estimators' : [100, 500, 1000,1500,2000],\n    'max_depth' : [2,3,5, 6,8, 9],\n    'min_child_weight' : [1, 5, 10],\n    'reg_alpha' : [0.001, 0.01, 0.1],\n    'reg_lambda' : [0.001, 0.01, 0.1],\n    'colsample_bytree' : [0.3,0.4,0.5,0.7],\n    'gamma' : [0.0,0.1,0.2,0.3,0.4]\n}\n\nxgb_tuning = RandomizedSearchCV(estimator = xgb, param_distributions = param_lst ,\n                          n_iter = 5,\n                        cv =6)\n       \nxgb_search = xgb_tuning.fit(X_train,y_train,\n                           early_stopping_rounds = 5,\n                           eval_set=[(X_val,y_val)],\n                           verbose = False)\n\n## checking for the best paramter values that the model took\n\nbest_param = xgb_search.best_params_\nxgb = XGBClassifier(**best_param)\nprint(best_param)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a4cc9e2-ea03-49f9-9f31-0cb19bc7d092","_cell_guid":"4856d8ad-fab2-4808-8b30-1fb8b9765df3","trusted":true},"cell_type":"code","source":"## check the best estimators\nxgb_search.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"140853b8-c579-4e4f-84fe-eca46a33ad69","_cell_guid":"5a0333dd-f067-4a4a-9b59-f62f6327e845","trusted":true},"cell_type":"code","source":"y_pred = xgb_search.predict(X_val)\nscore0 = accuracy_score(y_pred,y_val)\n#print(round(score0*100,4))\nprint('Score: {}%'.format(round(score0*100,4)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a1995e6-c0e8-4ec1-a60f-15af879ba3e8","_cell_guid":"108db759-4764-49a1-92bd-b05a735c75eb","trusted":true},"cell_type":"code","source":"## checking  'Accuracy' value using Cross Validation menthod\nacc_scores1_xgb =  cross_val_score(xgb_search,X,y,n_jobs=5,\n                                 cv = StratifiedKFold(n_splits=10),\n                                 scoring = 'accuracy')\nacc_scores1_xgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e02966a8-861f-4393-a57e-4b2adbecbcf0","_cell_guid":"06f01f55-3aa0-4d98-866b-c34034a4bcd8","trusted":true},"cell_type":"code","source":"from sklearn.metrics import RocCurveDisplay\nfrom sklearn.metrics import roc_auc_score,roc_curve,auc\nfrom sklearn import metrics\n\nfpr, tpr, threshold = metrics.roc_curve(y_val, y_pred)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'red', label = 'ROC AUC score = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'b--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35df90af-fa5b-4697-9fa2-cfc04ba9c77e","_cell_guid":"a1865c9c-8f29-4e0a-bb92-632865822417","trusted":true},"cell_type":"markdown","source":"ROC-AUC Curve:\n\n- An ROC curve (Receiver Operating Characteristic curve) is a graph showing the performance of a classification model at all classification thresholds.\n\n- Y-Axis shows the True Positive Rate\n- X-Axis shows the False Positive Rate\n\nTrue postive Rate is also known as Recall and is given by True Postive/(True Positive+False Negative)\n\nFalse Postive Rate is given by False Postive/(False Positive+True Negative)\n\n![Confusion Matrix](https://glassboxmedicine.files.wordpress.com/2019/02/confusion-matrix.png?w=816)"},{"metadata":{"_uuid":"94586249-d750-4907-b369-62338916e62c","_cell_guid":"329fbbfc-89d0-4e01-8fa3-0d32f42df81a","trusted":true},"cell_type":"markdown","source":"## Using Cross Validation with Logistic Regression"},{"metadata":{"_uuid":"4f8d174f-4ebc-4e87-a209-16c7f51e41ad","_cell_guid":"e9d0a881-986e-4792-ba7e-73234c837d39","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr= LogisticRegression()\nlr.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6745a71d-cc85-445c-97d2-c8f041cfbf7b","_cell_guid":"97ce58f5-cb33-473f-befa-b824eb4ff5ee","trusted":true},"cell_type":"code","source":"log_scores_logi = -1 * cross_val_score(lr, X, y,\n                              cv=5,\n                              scoring='neg_log_loss')\nacc_scores1_logi =  cross_val_score(lr,X,y,\n                                 cv = 5,\n                                 scoring = 'accuracy')\nf_score_logi =  cross_val_score(lr,X,y,\n                                 cv = 5,\n                                 scoring = 'f1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"784189d5-3ab3-44f2-8732-439af1462e22","_cell_guid":"afd0954a-7ca4-4d74-8788-e60168e84ea0","trusted":true},"cell_type":"code","source":"print(\"log_loss scores:\\n\", log_scores_logi)\nprint(\"Accuracy scores:\\n\", acc_scores1_logi)\nprint(\"f1_score scores:\\n\", f_score_logi)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46974b1f-4158-4c00-a9d8-c03935549cc7","_cell_guid":"485f3197-6aad-403f-9a0e-8c31fc186e6a","trusted":true},"cell_type":"code","source":"print(acc_scores1_logi.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a5181de-7b35-47f3-b0bf-4b41d515a9aa","_cell_guid":"5e16ad78-e51e-4199-bfbf-22dc75227f36","trusted":true},"cell_type":"markdown","source":"### This Notebook will come in handy to anyone who wants to revise the important concepts such as  \n- **Hyperparamter Tuning**\n- **Using Cross Validation**\n- **Boosting method like XGBClassifier**\n\n- References:\n - [Tuning](https://www.kaggle.com/angqx95/data-science-workflow-top-2-with-tuning)\n - [GridSearchCV vs Random](https://github.com/Lokeshrathi/HyperParamter_optimisation)\n \n## Do **Upvote** and comment your thoughts on this! "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}