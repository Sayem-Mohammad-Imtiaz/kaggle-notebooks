{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\ndata=pd.read_csv('/kaggle/input/play-tennis/play_tennis.csv')\ndata","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PART 1: NUMERICAL DATA"},{"metadata":{},"cell_type":"markdown","source":"**Let P(Py) be the probability of playing and P(Pn) be the probability of not playing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the no of people playing i.e Pyes and not playing i.e Pno\nPyes=data['play'].value_counts()[0]\nPno=data['play'].value_counts()[1]\n\n#Finding the total number of people who played and didnt play\ntotal=data.shape[0]\n\n#Finding the probability of people playing i.e P_Pyes and of people not playing i.e P_Pno\nP_Pyes=Pyes/total\nP_Pno=Pno/total\nprint(\"Probability of playing:\", P_Pyes)\nprint(\"Probability of not playing:\", P_Pno)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding Entropy"},{"metadata":{},"cell_type":"markdown","source":"**Let E(P) be the Entropy of playing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"E_P = -(P_Pyes * math.log(P_Pyes)) - (P_Pno * math.log(P_Pno))\nprint(\"Entropy:\", E_P)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding Weighted Entropy and Information Gain for each column"},{"metadata":{},"cell_type":"markdown","source":"# FOR OUTLOOK:"},{"metadata":{"trusted":true},"cell_type":"code","source":"outlook=data['outlook'].value_counts()\nprint(outlook)\npd.crosstab(data['outlook'],data['play'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating number of people played in Sunny i.e OS , in Rain OR and in Overcast OO\nOS=data['outlook'].value_counts()[0]\nOR=data['outlook'].value_counts()[1]\nOO=data['outlook'].value_counts()[2]\n\n#Finding their respective probabilities\nP_OSyes=2/5\nP_OSno=3/5\nP_ORyes=3/5\nP_ORno=2/5\nP_OOyes=4/4\nP_OOno=0/4\n\n#Calculating Entropy of each child dataset i.e for Sunny, Rain and Overcast\nE_OS=-(P_OSyes * math.log(P_OSyes)) - (P_OSno * math.log(P_OSno))\nE_OR=-(P_ORyes * math.log(P_ORyes)) - (P_ORno * math.log(P_ORno))\nE_OO=-(P_OOyes * math.log(P_OOyes)) - 0\nprint(\"Entropy for Sunny:\", E_OS)\nprint(\"Entropy for Rain:\", E_OR)\nprint(\"Entropy for Overcast:\", E_OO)\n\n#Weighted Entropy for column Outlook\nWE_outlook= OS/total * E_OS + OR/total * E_OR + OO/total * E_OO\nprint(\"Weighted Entropy of Outlook:\", WE_outlook)\n\n#Information Gain for column Outlook\nIG_outlook=E_P-WE_outlook\nprint(\"Information Gain of Outlook:\",IG_outlook)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FOR TEMP:"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp=data['temp'].value_counts()\nprint(temp)\npd.crosstab(data['temp'],data['play'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating number of people played in Mild i.e TM, in Hot i.e TH and in Cool i.e TC\nTM=data['temp'].value_counts()[0]\nTH=data['temp'].value_counts()[1]\nTC=data['temp'].value_counts()[2]\n\n#Finding their respective probabilities\nP_TMyes=4/6\nP_TMno=2/6\nP_THyes=2/4\nP_THno=2/4\nP_TCyes=3/4\nP_TCno=1/4\n\n\n#Calculating Entropy of each child dataset i.e for Mild, Hot and Cool\nE_TM=-(P_TMyes * math.log(P_TMyes)) - (P_TMno * math.log(P_TMno))\nE_TH=-(P_THyes * math.log(P_THyes)) - (P_THno * math.log(P_THno))\nE_TC=-(P_TCyes * math.log(P_TCyes)) - (P_TCno * math.log(P_TCno))\n\nprint(\"Entropy for Mild:\", E_TM)\nprint(\"Entropy for Hot:\", E_TH)\nprint(\"Entropy for Cool:\", E_TC)\n\n#Weighted Entropy for column Temp\nWE_temp= TM/total * E_TM + TH/total * E_TH + TC/total * E_TC\nprint(\"Weighted Entropy of Temp:\", WE_temp)\n\n#Information Gain for column Temp\nIG_temp=E_P-WE_temp\nprint(\"Information Gain of Temp:\",IG_temp)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FOR HUMIDITY"},{"metadata":{"trusted":true},"cell_type":"code","source":"humidity=data['humidity'].value_counts()\nprint(humidity)\npd.crosstab(data['humidity'],data['play'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating number of people played in High Humidity i.e HH and in Normal Humidity i.e HN\nHH=data['humidity'].value_counts()[0]\nHN=data['humidity'].value_counts()[1]\n\n#Finding their respective probabilities\nP_HHyes=3/7\nP_HHno=4/7\nP_HNyes=6/7\nP_HNno=1/7\n\n#Calculating Entropy of each child dataset i.e for High and Normal\nE_HH=-(P_HHyes * math.log(P_HHyes)) - (P_HHno * math.log(P_HHno))\nE_HN=-(P_HNyes * math.log(P_HNyes)) - (P_HNno * math.log(P_HNno))\n\nprint(\"Entropy for High:\", E_HH)\nprint(\"Entropy for Normal:\", E_HN)\n\n\n#Weighted Entropy for column Humidity\nWE_humidity= HH/total * E_HH + HN/total * E_HN\nprint(\"Weighted Entropy of Humidity:\", WE_humidity)\n\n#Information Gain for column Humidity\nIG_humidity=E_P-WE_humidity\nprint(\"Information Gain of Humidity:\",IG_humidity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# FOR WIND"},{"metadata":{"trusted":true},"cell_type":"code","source":"wind=data['wind'].value_counts()\nprint(wind)\npd.crosstab(data['wind'],data['play'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating number of people played in Strong Wind i.e WS and in Weak Wind i.e WW\nWW=data['wind'].value_counts()[0]\nWS=data['wind'].value_counts()[1]\n\n#Finding their respective probabilities\nP_WWyes=6/8\nP_WWno=2/8\nP_WSyes=3/6\nP_WSno=3/6\n\n#Calculating Entropy of each child dataset i.e for Strong Wind and Weak Wind\nE_WW=-(P_WWyes * math.log(P_WWyes)) - (P_WWno * math.log(P_WWno))\nE_WS=-(P_WSyes * math.log(P_WSyes)) - (P_WSno * math.log(P_WSno))\n\nprint(\"Entropy for Strong Wind:\", E_WW)\nprint(\"Entropy for Weak Wind:\", E_WS)\n\n\n#Weighted Entropy for column Wind\nWE_wind= WW/total * E_WW + WS/total * E_WS\nprint(\"Weighted Entropy of Wind:\", WE_wind)\n\n#Information Gain for column Wind\nIG_wind=E_P-WE_wind\nprint(\"Information Gain of Wind:\",IG_wind)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data[\"outlook\"] == \"Overcast\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion:\n\n\n**It has been predicted that Overcast of Outlook column has the highest gain out of every other columns. So, we select this column as an inital node.**"},{"metadata":{},"cell_type":"markdown","source":"# Part 2: NUMERICAL COLUMN"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/iris/Iris.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sorting the data on the basis of PetalLengthCm\ndf.sort_values(\"PetalLengthCm\", inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding Entropy of Species in Iris-Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Species'].value_counts()\ntotal=df['Species'].shape[0]\nprint(\"Total is:\",total)\n\n#Probaility of being Iris-setosa i.e P_S, Iris-versicolor i.e P_VC and Iris-virginica i.e P_V\nP_S=50/total\nP_VC=50/total\nP_V=50/total\n\nE_S = -(P_S * math.log(P_S))-(P_VC * math.log(P_VC))-(P_V * math.log(P_V))\nprint(\"Entropy of Species:\", E_S)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To get the Maximum Breaking Point i.e B_P"},{"metadata":{},"cell_type":"markdown","source":" **To Store the MAXIMUM ENTROPY and BREAKING POINT i.e M_E and B_P**"},{"metadata":{"trusted":true},"cell_type":"code","source":"M_E = -1\nB_P = -1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To get Weighted Entropy and Information Gain i.e W_E and I_G"},{"metadata":{"trusted":true},"cell_type":"code","source":"#First we split the datas on the basis of every rows of PetalLengthCm and make Yes or No table for storing it\n\ndef entropy(group):\n    df = group.groupby(\"Species\").count()[\"Id\"] # Getting Number Of Occurence Of Each Species\n    total = df.sum() # Total Number Of Rows\n    entropy_sum = 0 # To store entropy\n    for i in df :\n        p = i/total\n        entropy_sum = entropy_sum - (p*math.log(p))\n    return(entropy_sum)\n\n\nfor i in df[\"PetalLengthCm\"].unique():\n    \n    NO = df[(df[\"PetalLengthCm\"] > i)]\n    YES = df[~(df[\"PetalLengthCm\"] > i)]\n    \n    len_NO = NO.shape[0]\n    len_YES = YES.shape[0]\n    \n    E_NO= entropy(NO)\n    E_YES= entropy(YES)\n    \n    WE = (len_NO/total * E_NO) + (len_YES/total * E_YES) #Finding Weighted Entropy\n    IG = E_S - WE #Finding Information Gain\n    \n    print(\"Breaking Point is:\", i)\n    print(\"Weighted Entropy is:\", WE)\n    print(\"Information Gain is:\", IG)\n    print()\n    \n    if(IG > M_E): #Finding Maximum Breakpoint \n        M_E = IG\n        B_P = i","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Finding where Maximum Information Gain occurs"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Maximum Information Gain:\", B_P)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# To see which species can be identified from this BreakPoint"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df[df[\"PetalLengthCm\"] < 1.9][\"Species\"].tolist():\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"**Therefore, it has been predicted that out of all the Species, probability of data points falling under Iris-setosa is higher than Iris-versicolor and Iris-virginica from this breakpoint **"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}