{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#8F003C','#eb3446','Tourney','Smokum',45,10\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https://fonts.googleapis.com/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';</style>\n    <h4 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s</h4>\"\"\"%string))\n    \nfrom IPython.display import HTML\nHTML(\"\"\"\n<style>\nh1,h2,h3 {\n\tmargin: 1em 0 0.5em 0;\n\tfont-weight: 600;\n\tfont-family: 'Titillium Web', sans-serif;\n\tposition: relative;  \n\tfont-size: 36px;\n\tline-height: 40px;\n\tpadding: 15px 15px 15px 2.5%;\n\tcolor: #00018D;\n\tbox-shadow: \n\t\tinset 0 0 0 1px rgba(97,0,45, 1), \n\t\tinset 0 0 5px rgba(53,86,129, 1),\n\t\tinset -285px 0 35px #F2D8FF;\n\tborder-radius: 0 10px 0 15px;\n\tbackground: #FFD8B2\n    \n},\n\nh4 {\n\tmargin: 1em 0 0.5em 0;\n\tfont-weight: 600;\n\tfont-family: 'Titillium Web', sans-serif;\n\tposition: relative;  \n\tfont-size: 36px;\n\tline-height: 40px;\n\tpadding: 15px 15px 15px 2.5%;\n\tcolor: #00018D;\n\n\tborder-radius: 0 10px 0 15px;\n\tbackground: #FFD8B2\n    \n}\n</style>\n\"\"\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-05T08:30:00.180897Z","iopub.execute_input":"2021-09-05T08:30:00.181647Z","iopub.status.idle":"2021-09-05T08:30:00.201832Z","shell.execute_reply.started":"2021-09-05T08:30:00.181509Z","shell.execute_reply":"2021-09-05T08:30:00.200776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LGBM** \n\nthere was a Golden age of XGBoost and  was champion on kaggle, Now LGBM have taken over. LGBM outperform XGBoost.\nwhy LGBM is a great choice:\n1 LGBM was developed and maintained by Microsoft himself.\n2 easier to implement \n3 faster than typical gradient boosting algorithm \n\nhowever: But to XGBoost‚Äôs credit, XGBoost has been around the block longer than either LightGBM and CatBoost, so it has better learning resources and a more active developer community. It also doesn‚Äôt hurt that XGBoost is substantially faster and more accurate than its predecessors and other competitors such as Scikit-learn.","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:lightblue;font-size:20px;color:#00033E;font-weight : bold\"> üëç   1. INTRODUCTION:</h1>","metadata":{}},{"cell_type":"markdown","source":"<h4 style=\"background-color:white;font-size:16px;color:black;\">it's the first algorithm i try when solving a tabular problem. because of its handy nature.\n\nLightGBM is a light version of gradient boosting framework based on decision trees to increases the efficiency of the model and reduces memory usage.\n</h4>\n\n* [LightGBM Github Documentation](https://github.com/microsoft/LightGBM/tree/master/python-package)\n    \n* [check out feature of LightGBM](https://lightgbm.readthedocs.io/en/latest/Features.html)\n\n<h4 style=\"background-color:white;font-size:16px;color:black;\">To get most out of your LightGBM model, its necessary to understand the dynamics of the algorithm</h4>\n\n* [Official Documentation](https://lightgbm.readthedocs.io/en/latest/index.html)\n\n<h4 style=\"background-color:white;font-size:16px;color:black;\">\nThis are the question to be answered in this notebook\n1. Which Gradient Boosting methods are implemented in LightGBM.\n2. Which parameters are important in general?\n3. Which regularization parameters need to be tuned?\n\n</h4>\n\n","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:lightblue;font-size:20px;color:#00033E;font-weight : bold\"> üëç   2. Gradient Boosting methods:</h1>","metadata":{}},{"cell_type":"markdown","source":"<h4 style=\"background-color:white;font-size:16px;color:black;\">\nXGBoost and LGBM are cousine and they both implement the same idea Gradient Boosting Decision Tree\n</h4>\n\nThere are different kinds of Gradient Boosting, methods available in LGBM are:GBDT, DART, and GOSS and they can be controlled by boosting parameter\n\n**gbdt (gradient boosted decision trees)** It is based on three important principles:\n\n * Weak learners (decision trees) \n \n * Gradient Optimization \n \n * Boosting Technique\n \n * So in the gbdt method we have a lot of decision trees(weak learners). Those trees are built sequentially:\n \n\n<h4 style=\"background-color:white;font-size:16px;color:black;\">\nfirst tree learns how to fit to the target variable\nsecond tree learns how to fit to the residual (difference) between the predictions of the first tree and the ground truth\nThe third tree learns how to fit the residuals of the second tree and so on.\nAll those trees are trained by propagating the gradients of errors throughout the system.\n   </h4>\n","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:lightblue;font-size:20px;color:#00033E;font-weight : bold\"> üëç   3. Read this Instead Hit and Trail parameter search:</h1>\n\n<h4 style=\"background-color:white;font-size:16px;color:black;\">\n<b>Parameter that control decision trees in lgbm</b></h4>\n\n* num_leaves: controls the number of decision leaves in a single tree. there will be multiple trees in pool.\n* max_depth: this the height of a decision tree. if its more possibility of overfitting but too low may underfit.\n* min_data_in_leaf: the minimum number of data/sample/count per leaf (default is 20; lower min_data_in_leaf means less conservative/control, potentially overfitting).\n**NOTE: max_depth have direct impact on:**\n1 The best value for the num_leaves parameter\n2 Model Performance\n3 Training Time\n\n\n<h4 style=\"background-color:white;font-size:16px;color:black;\">\n<b>For Better Accuracy</b></h4>\n\n* Use large max_bin (may be slower)\n\n* Use small learning_rate with large num_iterations\n\n* Use large num_leaves (may cause over-fitting)\n\n* Use bigger training data\n\n* Try dart\n\n<h4 style=\"background-color:white;font-size:16px;color:black;\">\nDeal with Over-fitting</h4>\n\n* Use small max_bin\n\n* Use small num_leaves\n\n* Use min_data_in_leaf and min_sum_hessian_in_leaf\n\n* Use bagging by set bagging_fraction and bagging_freq\n\n* Use feature sub-sampling by set feature_fraction\n\n* Use bigger training data\n\n* Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n\n* Try max_depth to avoid growing deep tree\n\n* Try extra_trees\n\n* Try increasing path_smooth\n\n    \n      \n","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:lightblue;font-size:20px;color:#00033E;font-weight : bold\"> IMPORTANT PARAMETERS OF LGBM:</h1>\n\n**objective**\n * When you change it affects other parameters\tSpecify the type of ML model\n * default- value regression\n * aliases- Objective_type\n\n\n**boosting**\n * If you set it RF, that would be a bagging approach\n * default- gbdt\n * Range- [gbdt, rf, dart, goss]\n * aliases- boosting_type\n\n**lambda_l1**\n * regularization parameter\n * default- 0.0\n * Range- [0, ‚àû]\n * aliases- reg_alpha\n * constraints- lambda_l1 >= 0.0\n\n**bagging_fraction**\n * randomly select part of data without resampling\n * default-1.0\n * range- [0, 1]\n * aliases- Subsample\n * constarints- 0.0 < bagging_fraction <= 1.0\n\n**bagging_freq**\n * default- 0.0\n * range- [0, ‚àû]\n * aliases- subsample_freq\n * bagging_fraction should be set to value smaller than 1.0 as well 0 means disable bagging\n\n**num_leaves**\n * max number of leaves in one tree\n * default- 31\n * Range- [1, ‚àû]\n * Note- 1 < num_leaves <= 131072\n\n**feature_fraction**\n * if you set it to 0.8, LightGBM will select 80% of features\n * default- 1.0\n * Range- [0, 1]\n * aliases- sub_feature\n * constarint- 0.0 < feature_fraction <= 1.0\n\n**max_depth**\n * default- [-1]\n * range- [-1, ‚àû]m\n * Larger is usually better, but overfitting speed increases.\n * limit the max depth Forr tree model\n\n**max_bin**\n * deal with over-fitting\n * default- 255\n * range- [2, ‚àû]\n * aliases- Histogram Binning\n * max_bin > 1\n\n**num_iterations**\n * number of boosting iterations\n * default- 100\n * range- [1, ‚àû]\n * AKA- Num_boost_round, n_iter\n * constarints- num_iterations >= 0\n\n**learning_rate**\n * default- 0.1\n * range- [0 1]\n * aliases- eta\n * general values- learning_rate > 0.0Typical: 0.05.\n\n**early_stopping_round**\n * will stop training if validation doesn‚Äôt improve in last early_stopping_round\n * Model Performance, Number of Iterations, Training Time\n * default- 0\n * Range- [0, ‚àû]\n\n**categorical_feature** \n * to sepecify or Handle categorical features\n * i.e LGBM automatically handels categorical variable we dont need to one hot encode them.\n\n**bagging_freq**\n * default-0.0\n * Range-[0, ‚àû]\n * aliases- subsample_freq\n * note- 0 means disable bagging; k means perform bagging at every k iteration\n * enable    bagging, bagging_fraction should be set to value smaller than 1.0 as well\n\n**verbosity**\n * default- 0\n * range- [-‚àû, ‚àû]\n * aliases- verbose\n * constraints- {< 0: Fatal, = 0: Error (Warning), = 1: Info, > 1}\n\n**min_data_in_leaf**\n * Can be used to deal with over-fitting:\n * default- 20\n * constarint-min_data_in_leaf >= 0","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"background-color:lightblue;font-size:20px;color:#00033E;font-weight : bold\"> Parameter Optimization:</h1>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ntrain=pd.read_pickle(\"../input/optiver006/train.pkl\")\n# replace by order sum (tau)\ntrain['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n#test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\ntrain['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n#test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\ntrain['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n#test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\ntrain['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n#test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )\ntrain['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n#test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\ntrain['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n#test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n#test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\ntrain['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n#test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n\n# delta tau\ntrain['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n#test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-12T08:37:46.494767Z","iopub.execute_input":"2021-09-12T08:37:46.49515Z","iopub.status.idle":"2021-09-12T08:37:49.679083Z","shell.execute_reply.started":"2021-09-12T08:37:46.495115Z","shell.execute_reply":"2021-09-12T08:37:49.678091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n# making agg features\n\n\ntrain_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\n\n\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\n\nnnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     'total_volume_sum_6c1',\n     'trade_size_sum_0c1',\n     'trade_size_sum_1c1', \n     'trade_size_sum_3c1',\n     'trade_size_sum_4c1', \n     'trade_size_sum_6c1',\n     'trade_order_count_sum_0c1',\n     'trade_order_count_sum_1c1',\n     'trade_order_count_sum_3c1',\n     'trade_order_count_sum_4c1',\n     'trade_order_count_sum_6c1',      \n     'price_spread_sum_0c1',\n     'price_spread_sum_1c1',\n     'price_spread_sum_3c1',\n     'price_spread_sum_4c1',\n     'price_spread_sum_6c1',   \n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     'bid_spread_sum_6c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     'ask_spread_sum_6c1',   \n     'volume_imbalance_sum_0c1',\n     'volume_imbalance_sum_1c1',\n     'volume_imbalance_sum_3c1',\n     'volume_imbalance_sum_4c1',\n     'volume_imbalance_sum_6c1',       \n     'bid_ask_spread_sum_0c1',\n     'bid_ask_spread_sum_1c1',\n     'bid_ask_spread_sum_3c1',\n     'bid_ask_spread_sum_4c1',\n     'bid_ask_spread_sum_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] \ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-12T08:37:49.68046Z","iopub.execute_input":"2021-09-12T08:37:49.680716Z","iopub.status.idle":"2021-09-12T08:37:56.422549Z","shell.execute_reply.started":"2021-09-12T08:37:49.68069Z","shell.execute_reply":"2021-09-12T08:37:56.421801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T08:37:56.424253Z","iopub.execute_input":"2021-09-12T08:37:56.424784Z","iopub.status.idle":"2021-09-12T08:37:56.637497Z","shell.execute_reply.started":"2021-09-12T08:37:56.424743Z","shell.execute_reply":"2021-09-12T08:37:56.636395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport glob\nfrom joblib import Parallel, delayed\n\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, KFold\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\n\nimport optuna\n#N_TRIALS = 100\nTIME = 1800*2\nN_SPLITS = 5\nRANDOM_STATE = 99\nkfold = KFold(N_SPLITS, random_state=RANDOM_STATE, shuffle=True)\n\nFIXED_PARAMS = {\n                'learning_rate':0.07,\n                'metric': 'rmse',\n                'verbosity': -1,\n                'n_jobs': -1,\n                #'max_bin': 127,\n                'seed': RANDOM_STATE}\n\ndef rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) / y_true))))\n\n    \ndef objective(trial, cv=kfold):\n    \n    params = {\n        'n_estimators': trial.suggest_int('n_estimators',500,2000),\n        'num_leaves': trial.suggest_int('num_leaves', 400, 800),\n        'max_depth': trial.suggest_int('max_depth', -1, 3),\n        'max_bin':trial.suggest_int('max_bin', 50, 200),\n        'min_data_in_leaf':trial.suggest_int('min_data_in_leaf',400,700),\n        #'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 5),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 5),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1),\n        'subsample': trial.suggest_float('subsample', 0.4, 1),\n        'subsample_freq': trial.suggest_int('subsample_freq',1,10)\n        \n     \n        #'cat_smooth': trial.suggest_float('cat_smooth', 10, 100.0),  \n        #'feature_fraction': trial.suggest_float('feature_fraction',0.3,0.99),\n        \n       \n    }\n    \n    params.update(FIXED_PARAMS)\n\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\", valid_name='valid_1')\n    rmspe_list = []\n    \n    for kfold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_val = y.iloc[val_idx]\n\n        d_train = lgb.Dataset(X_train, label=y_train)\n        d_valid = lgb.Dataset(X_val, label=y_val)\n\n        model = lgb.train(params,\n                      train_set=d_train,\n                      valid_sets=[d_train, d_valid],\n                      verbose_eval=0,\n                      early_stopping_rounds=100,\n                      callbacks=[pruning_callback])\n\n        preds = model.predict(X_val)\n        score = rmspe(y_val, preds)\n        \n        rmspe_list.append(score)\n        \n    \n    return np.mean(rmspe_list)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T08:37:56.639001Z","iopub.execute_input":"2021-09-12T08:37:56.6393Z","iopub.status.idle":"2021-09-12T08:37:56.651761Z","shell.execute_reply.started":"2021-09-12T08:37:56.639269Z","shell.execute_reply":"2021-09-12T08:37:56.650976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_display = train.drop(['row_id', 'time_id', 'target'], axis = 1)\nX = X_display\ny = train['target']","metadata":{"execution":{"iopub.status.busy":"2021-09-12T08:37:57.832192Z","iopub.execute_input":"2021-09-12T08:37:57.832809Z","iopub.status.idle":"2021-09-12T08:37:58.40929Z","shell.execute_reply.started":"2021-09-12T08:37:57.832763Z","shell.execute_reply":"2021-09-12T08:37:58.40825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X= X.interpolate(method='index')","metadata":{"execution":{"iopub.status.busy":"2021-09-12T08:37:58.83207Z","iopub.execute_input":"2021-09-12T08:37:58.832646Z","iopub.status.idle":"2021-09-12T08:38:00.221518Z","shell.execute_reply.started":"2021-09-12T08:37:58.832608Z","shell.execute_reply":"2021-09-12T08:38:00.220777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T08:38:02.942107Z","iopub.execute_input":"2021-09-12T08:38:02.94249Z","iopub.status.idle":"2021-09-12T08:38:03.131005Z","shell.execute_reply.started":"2021-09-12T08:38:02.942459Z","shell.execute_reply":"2021-09-12T08:38:03.130268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nstudy = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=25))\nstudy.optimize(objective, timeout=TIME)\n    \nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T08:38:05.512103Z","iopub.execute_input":"2021-09-12T08:38:05.512466Z","iopub.status.idle":"2021-09-12T09:12:56.34472Z","shell.execute_reply.started":"2021-09-12T08:38:05.512436Z","shell.execute_reply":"2021-09-12T09:12:56.343835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}