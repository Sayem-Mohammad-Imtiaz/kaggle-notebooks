{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nfrom nltk.tokenize import WordPunctTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom nltk.collocations import TrigramCollocationFinder\nfrom nltk.metrics import TrigramAssocMeasures\nfrom nltk.tokenize import MWETokenizer\nfrom nltk.stem.snowball import SnowballStemmer\nimport string\nimport re\n\nbigram_measures = BigramAssocMeasures()\ntrigram_measures = TrigramAssocMeasures()\n\nfrom collections import Counter\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nstop_words = set(stopwords.words('english'))\nstop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) # remove it if you need punctuation\n\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt \n%matplotlib inline  \n\nimport multiprocessing, logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)   \nlogging.getLogger(\"py4j\").setLevel(logging.ERROR)\n\nfrom nltk.data import find\nfrom gensim.models import word2vec\nimport gensim\n\nimport spacy\nnlp = spacy.load('en_core_web_lg')\n# nlp = spacy.load('../input/spacyen-vectors-web-lg/spacy-en_vectors_web_lg/en_vectors_web_lg/')\n\nword2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\nw2v_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"b11ef8e73452f929c8ea93089f8b6933996c2be5"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"135640a894a3a44637b64e48d5c575f4d326e756"},"cell_type":"code","source":"from tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"path = '../input/'\ndata = pd.read_json(path+'news-headlines-dataset-for-sarcasm-detection/Sarcasm_Headlines_Dataset.json', lines=True)\nprint(data['is_sarcastic'].value_counts())\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1205b25a5c50c40c5676a45b7572b0afd37227d9"},"cell_type":"code","source":"[print(item, '\\n') for item in data['headline'][3:7]]\nprint()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"345bbbb8151970e3ff1893adc9724930708ae521"},"cell_type":"code","source":"obj = data['headline'][4]\nprint(obj)\ndoc = nlp(obj)\n\nfor token in doc:\n    print(token.string, token.pos_, token.tag_, token.dep_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"310639e54a67334bf1248a682f6464ff905de251"},"cell_type":"markdown","source":"looks interesting!!"},{"metadata":{"_uuid":"0ae5da980bc5a10f8f241b0db109f82ae8176647"},"cell_type":"markdown","source":"THe sample set is nearly balanced"},{"metadata":{"trusted":true,"_uuid":"826c359d29ae3b8fe00e6675e1dcf45768f57e18"},"cell_type":"code","source":"def clean_text(text):\n    \n    ## Remove puncuation\n    text = text.translate(string.punctuation)\n    \n    ## Convert words to lower case and split them\n    text = text.lower().split()\n    \n    ## Remove stop words\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops and len(w) >= 3]\n    \n    text = \" \".join(text)\n    ## Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    ## Stemming\n    text = text.split()\n    stemmer = SnowballStemmer('english')\n    stemmed_words = [stemmer.stem(word) for word in text]\n    text = \" \".join(stemmed_words)\n    return text\n# apply the above function to df['text']\ndata['headline_text'] = data['headline'].map(lambda x: clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3319b5c1c79ccac8268ee98a9e1664a14db9bf6","scrolled":true},"cell_type":"code","source":"#vectorize\nlist_news = data['headline_text'].tolist()\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(list_news)\nsequences = tokenizer_obj.texts_to_sequences(list_news)\n\nmax_length = max([len(s.split()) for s in list_news])\nprint('Maximum length of sentences %s' %max_length)\n#pad sequence\nword_index = tokenizer_obj.word_index\nprint('number of unique words', len(word_index))\n\nreview_pad = pad_sequences(sequences, maxlen=max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07ba97c22585e851d7fdecbeb6e26af2a5d469cd","scrolled":true},"cell_type":"code","source":"embedding_index = {}\nfor item in w2v_model.wv.vocab:\n    embedding_index[item.lower()] = w2v_model[item]\n\nEMBEDDING_DIM = embedding_index['adam'].shape[0]\n#initializing with zero vector for Unknown word\nembedding_index['<UNK>'] = np.zeros(EMBEDDING_DIM)\n\nnum_words = len(word_index)+1\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n\nfor word, ind in word_index.items():\n    if ind>num_words:\n        continue\n    embedding_vector = embedding_index.get(word, embedding_index['<UNK>'])\n    if embedding_vector is not None:\n        embedding_matrix[ind] = embedding_vector\nprint(embedding_matrix.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfad00f7c6cf282e02128e8ba330fca99ba0b351"},"cell_type":"code","source":"model = Sequential()\nembedding_layer = Embedding(input_dim=num_words, output_dim=EMBEDDING_DIM,\n                           embeddings_initializer=Constant(embedding_matrix),\n                           input_length=max_length,\n                           trainable=False,\n                           name='Embedding')\nmodel.add(embedding_layer)\nmodel.add(LSTM(units=max_length, dropout=0.2, name='lstm' ))\nmodel.add(Dense(units=1, activation='sigmoid', name='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"c97ee763374e345992f73ed0c7bea7b307c0eb65"},"cell_type":"code","source":"VALIDATION_SPLIT = 0.2\nindices = np.arange(review_pad.shape[0])\nnp.random.shuffle(indices)\n\nreview_pad = review_pad[indices]\nsarcasm_flag = np.array(data['is_sarcastic'].tolist())[indices]\n\nno_validation_sample = int(VALIDATION_SPLIT*review_pad.shape[0])\n\nX_train_pad = review_pad[:-no_validation_sample]\ny_train = sarcasm_flag[:-no_validation_sample]\n\nx_test_pad = review_pad[-no_validation_sample:]\ny_test = sarcasm_flag[-no_validation_sample:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1dbf9f86339716cb8cf75dbfdf2629999d89e0e"},"cell_type":"code","source":"print('Let\\'s Train...')\nmodel.fit(X_train_pad, y_train, batch_size=64, epochs=32, validation_data=(x_test_pad, y_test), verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"754402058db38752f8344db02590fbdb574e0c0d"},"cell_type":"code","source":"#finding bias vector \n#person vector with top 5000 baby names\nperson_names = pd.read_csv('../input/us-baby-names/NationalNames.csv')['Name'].head(5000).tolist()\nperson_vector = []\nfor item in person_names:\n    try:\n        person_vector.append(embedding_matrix[item.lower()].tolist())\n    except:\n        pass\nperson_matrice = np.array(person_vector)\nprint('Person matched', person_matrice.shape[0])\nembedding_matrix['[NAME]'] = np.mean(person_matrice, axis=0)\n\n#location vector with city names\nlocation_names = pd.read_csv('../input/store-locations/directory.csv')['City'].unique().tolist()\nlocation_vector = []\nfor item in location_names:\n    try:\n        location_vector.append(embedding_matrix[item.lower()].tolist())\n    except:\n        pass\nlocation_matrice = np.array(location_vector)\nprint('Location matched', location_matrice.shape[0])\nembedding_matrix['[PLACE]'] = np.mean(location_matrice, axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a705c32b70f1b95a969a48f8fb5a6bfeef760d15"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}