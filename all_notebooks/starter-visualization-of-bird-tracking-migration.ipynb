{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction\nGreetings from the Krishna! This is an starter kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing by creating your own copy.\n\nPS: Need Suggestions for improvements as this is my first notebook\nThankyou all.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Analysis\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made. (Hey, I'm just a simple starter, not a Kaggle Competitions Grandmaster!)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Basic Imports","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true,"scrolled":false},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is 1 csv file in the current version of the dataset:\n","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true,"scrolled":false},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"# Distribution graphs (histogram/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation Matrix","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scatter & Density plots","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false},"cell_type":"code","source":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now you're ready to read in the data and use the plotting functions to visualize the data.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Let's check 1st file: /kaggle/input/bird_tracking.csv\n\nnRowsRead = None which means all the rows of dataset will be imported.","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true,"scrolled":false},"cell_type":"code","source":"nRowsRead = None  # specify 'None' if want to read whole file\n# bird_tracking.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\ndf1 = pd.read_csv('/kaggle/input/bird_tracking.csv',index_col= 0, delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'bird_tracking.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a quick look at what the data looks like:","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true,"scrolled":false},"cell_type":"code","source":"df1.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Distribution graphs (histogram/bar graph) of sampled columns:","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true,"scrolled":false},"cell_type":"code","source":"plotPerColumnDistribution(df1, 10, 5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Correlation matrix:","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true,"scrolled":false},"cell_type":"code","source":"plotCorrelationMatrix(df1, 8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scatter and density plots:","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true,"scrolled":false},"cell_type":"code","source":"plotScatterMatrix(df1, 18, 10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So far what we have seen is the KaggleBot i.e\n\nit loads data and does some visualisation and processing to make it easy to work here\n\nlet's start from here!\n\ndf1 is the name of our dataframe.\n\nFor more DataFrames related to bird tracking and related aspects u can visit: \n\nhttps://www.gbif.org/dataset/83e20573-f7dd-4852-9159-21566e1e691e","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df1.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"info gives us the col names of df1 & how many nonnull ie 'not nan'\n\nif the row conts and non-nullcount are same which means there is no missing data, but here are some missing values in the speed_2d and direction.\n\nit also tells about the datatypeof the columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data preprocessing","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df1.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"describe() - gives us the basic statistics of all the columns which are of Data type int and float ie numeric data only.\nwe can get min, 1,2, 3 quartile and max.\n\nhere u can understand that std of altitue is 136 feet. which means there is a change of height aorung 136 feet per timeperiod/ frequency of data logging in device, in here we are taking approximately around  1 hour to collect the data. 1 hr is the inerval. \n\nwe can observe the device_info_seriel whose basic stats are of no use as it's of no value and is only used for identification of devices from mixing up with other device data. so we can convert it to object or delete it.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df1.bird_name.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"each bird data is collected approx 20k times.\n\nlet's do some basic imports of seaborn and matplotlib","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's have a basic plot of latitude and long of each bird\n\nTo do that we need to collect data of each bird seperately and give those as points for plotting or u can go with seaborn for quick visualisation and with less hassle. seaborn is a visualisation package which is built on top of matplotlib or u can say it's an advaned version of matplotlib\n\nplot longitude on x axis and latitude on y axis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# line plot & scatter plot","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize= (15,15))\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('Latitude vs Longitude')\nsns.lineplot(x='longitude' , y= 'latitude', hue = 'bird_name', data= df1, legend= 'full', alpha =  0.7)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.figure(figsize= (10,10))\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\nplt.title('Latitude vs Longitude')\nsns.scatterplot(x='longitude' , y= 'latitude', hue = 'bird_name', data= df1, legend= 'full', alpha = 0.3 )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the graph there is a lot of scatterdness and all the three birds have simillar flight pattern u can play with several visualisation and can understand data in a different ways\n\nNow lets deal with understanding Speed_2d it gives us the average speed of the bird in a 2D plane which is an local approximation of the earth curved surface\n\nlet's start dealing with null or nan valus in this column","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Dealing with null values","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df1.speed_2d.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 443 null values or mising data from the df1\n\nNow it will be a problem to plot missing values so we consider the values and we ignore the missing values\n\nlet's define x which gives us the speed component of bird Eric for only of it's non null values.\n\nand repeat the process for the other two birds and plot and histogram because it's a continous data.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Distribution plot","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"x = df1.speed_2d[df1.bird_name == 'Eric']\nx1 = df1.speed_2d[df1.bird_name == 'Nico']\nx2 = df1.speed_2d[df1.bird_name == 'Sanne']\n#print(x,x1,x2)\nplt.figure(figsize=(15,9))\nsns.distplot(x,bins =30,rug = True )\nplt.show();\nsns.distplot(x1,bins =20,rug = True )\nplt.show();\nsns.distplot(x2,bins =10,rug = True )\nplt.show();\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** 1. Here we can see all the 3 graphs, and bins give u the light blue square one in the graph which means that if N no. of bins are given then the range of x is taken and it does make that range into N-1 parts and bars are drawn with breath of the cal value,more bins means less widthh of the bar which means the each partition value decreases\n\n\nsuppose we have range 0-10x  and want to make into 10 parts then we need 11 bins each bin corresponds to change of 1x\n\nsame range and if we give 21 bins each bin corresponds to change of 0.5x\n\nwhich inturn results in decreasing the breath of the rectangle in graph.\n\nrug = true : which gives a small blue line for all the values\n\nthe dark blue lines are dense/more at starting of graph and scattered at the right side meaning more observations on left side and less on the right side and even the length of light blue rectangle means the same ie if length\\ height of rect is more which means more obs of data are there\n\nlength dir proportional to no. of observations.\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let's import datetime so that we can convert our date_time clumn which is an object will be converted to datetime so we can subtract two rows value to get the time elapsed in between the observations ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Working with Date Time","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"import datetime\n#convert column to Date time format\ndf1.date_time = pd.to_datetime(df1.date_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df1.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2013     -08     -15     00      :18     :08             +00:00\n\nyear    month    day    hours    mins   second           Timezone it's in UTC so +00: 00 if it's supposed u want to convert it to IST(indian standard time u need to +05:30 i.e 5 hrs and 30 mins.\n\nNow let's subtract the first and last value of date_time col to know the complete elapsed time.\n\nas both the values are in the same time zone we have no problem seperating the values and getting the time elapsed in btwn the observed data.\n\nsuppose if they are in different time zones then we need to either convert them to same timezone or we need to delete the timezone to do the subtraction and get the difference btwn them.\n","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"df1.date_time.iloc[-1] - df1.date_time.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so that's roughly around 259 days from first data recording to the last in our dataset.","execution_count":null},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"time_Eric = (df1.date_time[df1.bird_name == 'Eric']).astype('datetime64[ns]')\ntime_Eric.iloc[-1] - time_Eric.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"time_Nico = (df1.date_time[df1.bird_name == 'Nico']).astype('datetime64[ns]')\n\ntime_Nico.iloc[-1] - time_Nico.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now let's get the time_date of each individual bird and save it in a variable as a **\"datetime64[ns]\"** so that the variable will still be in the datime format and later we can use that to plot for better understanding.\n\nAfter getting the Variable now we calcualte the time taken by each bird from starting of their jourey to the end of their journey.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"time_Sanne = (df1.date_time[df1.bird_name == 'Sanne']).astype('datetime64[ns]')\ntime_Sanne.head()\n#time_Sanne.iloc[-1] - time_Sanne.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualisation of Time period and number of observations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.title('Eric')\nplt.xlabel('Time taken in DateTime format')\nplt.ylabel('Number of observations')\nplt.plot(time_Eric)\nplt.show()\n\nplt.title('Nico')\nplt.xlabel('Time taken in DateTime format')\nplt.ylabel('Number of observations')\nplt.plot(time_Nico)\nplt.show()\n\n\nplt.title('Sanne')\nplt.xlabel('Time taken in DateTime format')\nplt.ylabel('Number of observations')\nplt.plot(time_Sanne)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the graph we can infer that all the three birds data has been started collecting from the same time, date and there are no deviations in the data which means the data has been collected over periodic time and it looks like there is little iregularity (ie some data might be missed or the device has sompe problem storing the data or any othr reasons) in the bird named Eric of observations btwn 6500-7500 which can be neglected considering the majority of data following an upward trend or we could say that there are observations that are further apart from one other than the remaining obsrvations.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Visualization of Avg speed for every 30 mins and Datetime","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,12))\nplt.plot( time_Eric, x, linestyle= '-', marker = 'o')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Difference of time between two Data points**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#time_Eric.iloc[1] - time_Eric.iloc[0]\n#time_Eric.iloc[2] - time_Eric.iloc[1]\ntime_Eric.iloc[3] - time_Eric.iloc[2]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well as we have seen the graph we can't get much understanding out of it.so let's look at why we can't get inference out of it and where did we go wrong.\n\nif we see at the data there is a less standard deviation in the speed_2d but why did the graph shows us that there is more scatterdness/ noise.\n\nwhen we look at the datetime of each bird we notice that each observation is made on an average of 30 minutes, which means that the speed_2d is getting noted down for each bird over a period of 30 mins or half an hour, let's think of stock market for a sec, when we see the stock price of certain company over a time interval of let's say 30 mins then we get a more noisier graph but when we increase the timeperiod say 4 hrs we get less noise than before and as we increase the timeperiod let's say 1 day we will get less noise because it only shows us the day's starting price and end of day closing price.\n\nso as we plot graphs of various timeperiod we get less and less noisy grahs and after certain timeperiod the stocks generally follow a trend an some call it as seasonality which occurs due to weather seasons and festivals associated with and less sales at financial year end and so on...\n\nIn this bird data we will try to find the seasonal migration period of the birds and preict the same in the future.\n\nSo now let's try to take the average of speed_2d along each day and plot them along no. of days for better understanding.\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Converting elapsed time i.e. 30 mins into whole day to calculate Avg Speed of each day.\n* *All observations with in 24 hrs i.e of same day are taken and their speed_2d values are taken and summed up and their Average spped of particular day is calculated.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"elapsed_time = [time - time_Eric[0] for time in time_Eric]\nelapsed_time[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elapsed_days= np.array(elapsed_time) / datetime.timedelta(days=1)\nelapsed_days[:5]\n# it will convert the datetime series into number of days as we need to iterate we converted it to a array as we can't iterate over list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''for (i,t) in enumerate(elapsed_days):\n    print(i,t)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next_day = 1\nindeces=[]\ndaily_mean_speed = []\nfor (i,t) in enumerate(elapsed_days):\n    # Here i is the count or index we use for later gtting the spped_2d and \n    # t is the datetime in the time delta RATIO OF timedata so it says \n    if t< next_day:\n        indeces.append(i)\n        # we get a list of indeces and those speed_2d are with in the same day\n        #we get multiple list of each day indeces as a list\n        #print(indeces)\n    else:\n        daily_mean_speed.append(np.mean(df1.speed_2d[indeces]))\n        #using the list of indeces of day 1 and getting their values from Speed_2d and\n        #cal mean of that day and storing in daiy_mean_speed by append\n        # so now we get mean speed of day 1 as one value and the ssame continues for all days\n        #print(indeces)\n        next_day += 1\n        indeces = []\ndaily_mean_speed[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have seen now we have obtained the mean speed of Eric over it's entire journey so let's plot it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Plot of Avg Speed of each day vs Days","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,9))\nplt.plot(daily_mean_speed)\nplt.xlabel('Days')\nplt.ylabel('Mean Speed per Day')\nplt.title('Mean speed per Day of Eric')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can now get a less noise graph from before which helps us to inference that the mean avg speed per day is around 2 to 3 and max speed of 9 has been observed.\n\nThe anormaly i.e the max speed per days are the days where migration are taking place from one location to another.\n\nso now let's look at to where does the migraton happens.\n\nu can similarly get the graphs for other2 birds using the same code and replacing their components.\n\nhttps://scitools.org.uk/cartopy/docs/latest/\n\ncartopy - Cartopy is a Python package designed for geospatial data processing in order to produce maps and other geospatial data analyses.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# pip install cartopy \n# For visualisation of flight of bird over a MAP","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#!pip install cartopy\nimport cartopy.crs as ccrs\nimport cartopy.feature as cft","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"proj = ccrs.Mercator()\nplt.figure(figsize=(12,12))\nax = plt.axes(projection = proj)\nax.set_extent((-25.0, 20.0, 52.0, 10.0))\n#to set the long and lat take the min and max of lat and long and add some degree to max \n#and subtract some degrees to min and give the range by trial and error method.\nax.add_feature(cft.LAND)\nax.add_feature(cft.OCEAN)\nax.add_feature(cft.COASTLINE)\nax.add_feature(cft.BORDERS)\n# adding features to show on the map like land, ocean, coastline, borders\nfor name in df1.bird_name:\n    ix = df1.bird_name == name\n    x, y = df1.longitude.loc[ix], df1.latitude.loc[ix] \nax.plot(x,y, '.', transform = ccrs.Geodetic(), label = name )\nplt.legend(loc= 'upper left' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Instead of for loops and all basic functions u can use groupby and other pandas functions to get the data more easily.\n\nif u can understand the content give it a like if anything need to be updated type in the comment section below.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](http://)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Conclusion\nThis concludes your starter Visualisation! To go forward from here, click the blue \"Fork Notebook\" button at the top of this kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}