{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf #magical gradient computation\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/data.csv')\ndata = data.sample(frac = 1)\ndata.head()","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"00d377c66d300b0dd8d1473d3f84fdad54485977","scrolled":true},"cell_type":"code","source":"data['Length'] = [len(headline) for headline in data['Headline']]\ndata.head()","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb895872926d946aef85cdc9935e504e854876c9","scrolled":true},"cell_type":"code","source":"data['Length'].describe()","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"315362f60e88dc3dfad5df619312fdaf1a091512","collapsed":true},"cell_type":"code","source":"real_text = ' '.join(data[data['Label'] == 1]['Headline'])\nfake_text = ' '.join(data[data['Label'] == 0]['Headline'])\nfake_words = [word for word in nltk.tokenize.word_tokenize(fake_text) if word not in stopwords.words('english') and len(word) > 3]\nreal_words = [word for word in nltk.tokenize.word_tokenize(real_text) if word not in stopwords.words('english') and len(word) > 3]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6189d400c7e0043b67c56d8220e29cc244222a6d"},"cell_type":"code","source":"common_fake = nltk.FreqDist(fake_words).most_common(25)\ncommon_real =nltk.FreqDist(real_words).most_common(25)\nfake_ranks = []\nfake_counts = []\nreal_ranks = []\nreal_counts = []\n\nfor ii, word in enumerate(reversed(common_fake)):\n    fake_ranks.append(ii)\n    fake_counts.append(word[1])\n\nfor ii, word in enumerate(reversed(common_real)):\n    real_ranks.append(ii)\n    real_counts.append(word[1])\n\nplt.figure(figsize=(20, 7))\n\nplt.scatter(fake_ranks, fake_counts)\n\nfor labels, fake_rank, fake_count in zip(common_fake, fake_ranks, fake_counts):\n    plt.annotate(\n        labels[0],\n        xy = (fake_rank, fake_count)\n    )\n\nplt.scatter(real_ranks, real_counts)\nplt.title('Real vs Fake Headlines')\n\nfor labels, real_rank, real_count in zip(common_real, real_ranks, real_counts):\n    plt.annotate(\n        labels[0],\n        xy = (real_rank, real_count)\n    )\n    \nreal_patch = mpatches.Patch(color='orange', label='Real')\nfake_patch = mpatches.Patch(color='blue', label='Fake')\nplt.legend(handles=[real_patch, fake_patch])\n\nplt.show()\n\n    ","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec1847d7b77eb95f4de64ba2f2861101f1f9a4ea","scrolled":true},"cell_type":"code","source":"def pad(x):\n    \n    if len(x) < 69:\n        \n        return x + ' ' * (69 - len(x))\n    \n    return x\n\ndef trim(x):\n    \n    if len(x) > 69:\n        \n        return x[:69]\n    \n    return x\n\ndata['Headline'] = data['Headline'].apply(pad)\ndata['Headline'] = data['Headline'].apply(trim)\ndata['Length'] = [len(headline) for headline in data['Headline']]\ndata.describe()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e29784f185dd0560c1f20b368e3ca15c64fd0d2","collapsed":true},"cell_type":"code","source":"text = ' '.join(data['Headline'])\ndictionary_size = len(set(text))\ndictionary = sorted(set(text))\ncharacter_map = { k:v for v, k in enumerate(dictionary) }\nmax_length = 69\nbatch_size = 50","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4026e96ee06a9451da1fd92b1970a233e00daf39","collapsed":true},"cell_type":"code","source":"def to_input(sentence, character_map, dictionary_size):\n    \n    sentence = np.array([character_map[char] for char in sentence])\n    one_hot = np.zeros((len(sentence), dictionary_size))\n    one_hot[np.arange(len(sentence)), sentence] = 1\n    return one_hot\n\ndef batch(sentences, labels, start, batch_size):\n    \n    if start + batch_size < len(sentences):\n        \n        inputs = [to_input(sentence, character_map, dictionary_size) for sentence in sentences[start: start + batch_size ]]\n        labels = [label for label in labels[start: start + batch_size ]]\n        start = start + batch_size\n    \n    else:\n        \n        inputs = [to_input(sentence, character_map, dictionary_size) for sentence in sentences[start:]]\n        labels = [label for label in labels[start:]]\n        start = 0\n    \n    return np.array(inputs), np.array(labels) , start\n\ndef test_batch(sentences, labels):\n    \n        \n    inputs = [to_input(sentence, character_map, dictionary_size) for sentence in sentences]\n    labels = [label for label in labels]\n\n    return np.array(inputs), np.array(labels)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a72fd6865f53e855a1d3cd0ab9adbfc27dc48138","collapsed":true},"cell_type":"code","source":"inputs = tf.placeholder(tf.float32, [None, max_length, dictionary_size])\nlabels = tf.placeholder(tf.int64, [None])\nhidden_size = 512\ncell = tf.contrib.rnn.LSTMCell(hidden_size, state_is_tuple = True)\ninitial_state = tf.contrib.rnn.LSTMStateTuple(tf.placeholder(tf.float32, [None, hidden_size], name = \"c_init\"), \n                                             tf.placeholder(tf.float32, [None, hidden_size], name = \"h_init\"))\n\nwith tf.variable_scope(\"softmax\") as softmax:\n    \n    W_h = tf.get_variable(\"Wh\", \n        shape = [hidden_size, 2], \n        dtype = tf.float32,\n        initializer = tf.random_normal_initializer(.1)\n    )\n\n    b_h = tf.get_variable(\"bh\", \n        shape = 2,\n        dtype = tf.float32,\n        initializer = tf.ones_initializer()\n    )\n\noutputs, state = tf.nn.dynamic_rnn(cell, inputs, initial_state = initial_state)\n\nlogits = tf.matmul(outputs[:,-1, :], W_h) + b_h\n\nloss = tf.reduce_mean(\n    tf.nn.sparse_softmax_cross_entropy_with_logits(\n        labels = labels, logits = logits\n    )\n)\n\naccuracy = tf.reduce_mean(\n    tf.cast(tf.equal(\n        tf.argmax(logits, 1),\n        tf.cast(labels, tf.int64)\n    ), \n    tf.float32)\n)\n\noptimizer = tf.train.AdamOptimizer(.001)\n\ntrain_step = optimizer.minimize(loss)\n\n\n\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a32cda7a8cf4cbd8e64ba3578740c0b1e748b835","scrolled":false,"collapsed":true},"cell_type":"code","source":"with tf.Session() as sess:\n    \n    init = tf.global_variables_initializer()\n    sess.run(init)\n    \n    initial = sess.run(cell.zero_state(50, tf.float32))\n\n    start = 0\n    inputs_, targets_, start = batch(data[\"Headline\"][:3350], data[\"Label\"][:3350], start, batch_size)\n    \n    for step in range(1000):\n        _, initial = sess.run([train_step, initial_state], feed_dict = {\n            inputs:inputs_, \n            labels:targets_,\n            initial_state:initial\n        })\n        \n        inputs_, targets_, start = batch(data[\"Headline\"][:3350], data[\"Label\"][:3350], start, batch_size)\n        \n            \n        if step % 100 == 0:\n            \n            test_inputs, test_targets = test_batch(data[\"Headline\"][3350:], data[\"Label\"][3350:])\n            cost, acc = sess.run([loss, accuracy], feed_dict = {\n            inputs:test_inputs, \n            labels:test_targets,\n            initial_state:sess.run(cell.zero_state(len(data[\"Label\"][3350:]), tf.float32))\n            })\n            \n            print(cost, acc)\n            \n            ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}