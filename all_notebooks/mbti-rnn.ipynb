{"cells":[{"metadata":{},"cell_type":"markdown","source":"# MBTI (Myers-Briggs Type Indicator) RNN"},{"metadata":{"_cell_guid":"e73d57ef-5ffe-4d4d-8888-4f8840138c1d","_uuid":"2a784050600a9de23bf507d4f1bbe017f8980a29","trusted":true},"cell_type":"code","source":"# Import Libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"660418a8-9ce6-4e67-afd8-6d0861a9fe1e","_uuid":"ff666f98b54785518bb8aa250524f59facbd50ad"},"cell_type":"markdown","source":"## Load dataset\n\nThe dataset is a 'csv' file, so we'll use pandas to load it. We shall print the shape and the first few entries of the dataset to understand what we're working with. Accordingly, we need to choose what strategy to use to clean the data."},{"metadata":{"_cell_guid":"781ff4d7-bdfe-4e5d-935d-601e2a2966a5","_uuid":"ac3059a5ebc7fcdd9b38883037d7478363fac32e","trusted":true},"cell_type":"code","source":"# Load Dataset\ntext=pd.read_csv(\"../input/mbti_1.csv\" ,index_col='type')\nprint(text.shape)\nprint(text[0:5])\nprint(text.iloc[2])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2d9d8cbf-9871-42c7-9dd8-1c5d5bc47ec2","_uuid":"667feaca9519e2e2d16ce648c258fde2fd3ce247"},"cell_type":"markdown","source":"## Preprocessing labels\nThe neural letwork cannot understand string labels, so we one-hot-encode them using sklearn.preprocessing.LabelBinarizer. I'm displaying the first few labels to see if everything's okay."},{"metadata":{"_cell_guid":"bd2c1550-c3e6-4501-9c23-724ccd03b35a","_uuid":"2830e929576f2b7cc0b84faa8cd955dd66dcb655","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\n\n# One hot encode labels\nlabels=text.index.tolist()\nencoder=LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\nlabels=encoder.fit_transform(labels)\nlabels=np.array(labels)\nprint(labels[50:55])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4f7b312c-36ca-41de-80a0-94e382482597","_uuid":"4a7fb67e54ec002ea73368b7bd0895fdaebab778","trusted":true},"cell_type":"code","source":"mbti_dict={0:'ENFJ',1:'ENFP',2:'ENTJ',3:'ENTP',4:'ESFJ',5:'ESFP',6:'ESTJ',7:'ESTP',8:'INFJ',9:'INFP',10:'INTJ',11:'INTP',12:'ISFJ',13:'ISFP',14:'ISFP',15:'ISTP'}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f9133062-a98d-4f1b-96d7-d54006241c58","_uuid":"5eddfa6a3100e465ffde1d86b7da0791da3a4d44"},"cell_type":"markdown","source":"### Preprocessing posts\n\nWe can see that the posts are very noisy, so they need to be cleaned. For this I'm doing the following:\n\n1. Converting all letters to lowercase.\n2. Remove '|||'\n3. Removing punctuation.\n4. Removing URLs, links etc..\n5. Convert words to integers\n\nWe'll leave unicode emojis alone."},{"metadata":{"_cell_guid":"eac653a1-f0da-4f38-96bd-d2be016c1ba0","_uuid":"5918da8ec665950e8133ba22ccbc505ee2a3890f","trusted":true},"cell_type":"code","source":"import re\n\n# Function to clean data ... will be useful later\ndef post_cleaner(post):\n    \"\"\"cleans individual posts`.\n    Args:\n        post-string\n    Returns:\n         cleaned up post`.\n    \"\"\"\n    # Covert all uppercase characters to lower case\n    post = post.lower() \n    \n    # Remove |||\n    post=post.replace('|||',\"\") \n\n    # Remove URLs, links etc\n    post = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', '', post, flags=re.MULTILINE) \n    # This would have removed most of the links but probably not all \n\n    # Remove puntuations \n    puncs1=['@','#','$','%','^','&','*','(',')','-','_','+','=','{','}','[',']','|','\\\\','\"',\"'\",';',':','<','>','/']\n    for punc in puncs1:\n        post=post.replace(punc,'') \n\n    puncs2=[',','.','?','!','\\n']\n    for punc in puncs2:\n        post=post.replace(punc,' ') \n    # Remove extra white spaces\n    post=re.sub( '\\s+', ' ', post ).strip()\n    return post","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8739bc1c-f158-4f10-9130-67a8ac6415a8","_uuid":"1a9248e49f19347c38b33c0d3f5612fb56e9ff7b","trusted":true},"cell_type":"code","source":"# Clean up posts\n# Covert pandas dataframe object to list. I prefer using lists for prepocessing. \nposts=text.posts.tolist()\nposts=[post_cleaner(post) for post in posts]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c5189ad3-ab59-4eee-a33a-82b7bdd398ea","_uuid":"e086424b857f002373c5eabe0fb2b6f7b57970e7","trusted":true},"cell_type":"code","source":"# Count total words\nfrom collections import Counter\n\nword_count=Counter()\nfor post in posts:\n    word_count.update(post.split(\" \"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f07ebbd-72e1-47c8-9919-1739d570e29d","_uuid":"2e6a8dcc20e16775f1dfa2e1e5eec1c8e658074c","trusted":true},"cell_type":"code","source":"# Size of the vocabulary available to the RNN\nvocab_len=len(word_count)\nprint(vocab_len)\n\nprint(len(posts[0]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9ee928c4-efc4-4e9d-86dc-86e1ba0f33ca","_uuid":"47c81752a05c0ec56f879ff4b7c5222d584f4d16"},"cell_type":"markdown","source":"### Convert words to integers"},{"metadata":{"_cell_guid":"88818f1e-a2fa-4da9-bb7d-3cc395e8d8af","_uuid":"3010bfa3dbf515ac65bbdb6c46f87e5d50a9e10b","trusted":true},"cell_type":"code","source":"# Create a look up table \nvocab = sorted(word_count, key=word_count.get, reverse=True)\n# Create your dictionary that maps vocab words to integers here\nvocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n\nposts_ints=[]\nfor post in posts:\n    posts_ints.append([vocab_to_int[word] for word in post.split()])\n\nprint(posts_ints[0])\nprint(len(posts_ints[0]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cd4a1482-b863-4c26-ae92-ce20b8ff3766","_uuid":"2d640b383ddf3d75caffc2dc7cef95c19a009aac"},"cell_type":"markdown","source":" ### Make posts uniform\nWe can see that the lengths of the posts aren't uniform, so we'll limit number of words in each post to 1000.For posts with less than 1000 words, we'll pad with zeros."},{"metadata":{"_cell_guid":"8709d94d-b98e-49dc-8b35-4a3fffca1673","_uuid":"64538831d8bb2a7422be33997b1dda3207d21b9f","trusted":true},"cell_type":"code","source":"posts_lens = Counter([len(x) for x in posts])\nprint(\"Zero-length reviews: {}\".format(posts_lens[0]))\nprint(\"Maximum review length: {}\".format(max(posts_lens)))\nprint(\"Minimum review length: {}\".format(min(posts_lens)))\n\nseq_len = 500\nfeatures=np.zeros((len(posts_ints),seq_len),dtype=int)\nfor i, row in enumerate(posts_ints):\n    features[i, -len(row):] = np.array(row)[:seq_len]\nprint(features[:10])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12148723-9ab3-4dc6-8971-4d8ac41f7b58","_uuid":"7a643d0c8443884ef03c9b618369098b606d51bc"},"cell_type":"markdown","source":"### Preparing tranining, test and validation datasets"},{"metadata":{"_cell_guid":"aacacf26-4ab3-45bb-9f6f-f5353afda760","_uuid":"6c29d68316001a6cf4d64acd4f0f932e4fcce804","trusted":true},"cell_type":"code","source":"# Split data into training, test and validation\n\nsplit_frac = 0.8\n\nnum_ele=int(split_frac*len(features))\nrem_ele=len(features)-num_ele\ntrain_x, val_x = features[:num_ele],features[num_ele:int(rem_ele/2)+num_ele]\ntrain_y, val_y = labels[:num_ele],labels[num_ele:int(rem_ele/2)+num_ele]\n\ntest_x =features[num_ele+int(rem_ele/2):]\ntest_y = labels[num_ele+int(rem_ele/2):]\n\nprint(\"\\t\\t\\tFeature Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(train_x.shape), \n      \"\\nValidation set: \\t{}\".format(val_x.shape),\n      \"\\nTest set: \\t\\t{}\".format(test_x.shape))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c21961f3-e8b3-42de-b4d2-5542226036f4","_uuid":"72a85f118ed44f53fffa6f83465ca80b54cfdf1a"},"cell_type":"markdown","source":"## The RNN"},{"metadata":{"_cell_guid":"3a82d5ed-0199-4e7f-9b18-2ea953bfc195","_uuid":"c25c324b578e7d8e4faf31d6eb9265759bf8663d","trusted":true},"cell_type":"code","source":"lstm_size = 256\nlstm_layers = 1\nbatch_size = 256\nlearning_rate = 0.01\nembed_dim=250","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6cc632c5-174f-459d-be3a-511f1d6e89ec","_uuid":"70ba0c6ea9268fcdb4024de80797b24711f46cb1","trusted":true},"cell_type":"code","source":"n_words = len(vocab_to_int) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1\n\n# Create the graph object\ngraph = tf.Graph()\n# Add nodes to the graph\nwith graph.as_default():\n    input_data = tf.placeholder(tf.int32, [None, None], name='inputs')\n    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n    keep_prob = tf.placeholder(tf.float32, name='keep_prob')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c176a7f-863a-4ad5-b982-d90f2a657edd","_uuid":"8a3df7fee93677bf1deea55566cdeb792c8bb9ab","trusted":true},"cell_type":"code","source":"# Embedding\nwith graph.as_default():\n    embedding= tf.Variable(tf.random_uniform(shape=(n_words,embed_dim),minval=-1,maxval=1))\n    embed=tf.nn.embedding_lookup(embedding,input_data)\n    print(embed.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4541e896-1fd1-4c87-abf6-84434984ca28","_uuid":"169693e69c9fd6399bf0cfb2e63793b0f1d11bf8","trusted":true},"cell_type":"code","source":"# LSTM cell\nwith graph.as_default():\n    # basic LSTM cell\n    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    \n    # Add dropout to the cell\n    drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n    \n    # Stack up multiple LSTM layers, for deep learning\n    cell = tf.contrib.rnn.MultiRNNCell([drop]* lstm_layers)\n    \n    # Getting an initial state of all zeros\n    initial_state = cell.zero_state(batch_size, tf.float32)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ea851977-2d38-4570-b8e6-92b9a3285b7a","_uuid":"09ed7b0f4435c6b6011d89b43cb59bafaef60727","trusted":true},"cell_type":"code","source":"with graph.as_default():\n    outputs,final_state=tf.nn.dynamic_rnn(cell,embed,dtype=tf.float32 )","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81b02a11-40c2-4784-a781-21e081e21711","_uuid":"11e30ca0ef0c3936e792c0d2b47a9a2898ef8d59","trusted":true},"cell_type":"code","source":"with graph.as_default():\n    \n    pre = tf.layers.dense(outputs[:,-1], 16, activation=tf.nn.relu)\n    predictions=tf.layers.dense(pre, 16, activation=tf.nn.softmax)\n    \n    cost = tf.losses.mean_squared_error(labels_, predictions)\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2b622b23-98c4-430e-8b62-298c75774ad5","_uuid":"c48275ff698a96edb1b9440315fc67a228666a3f","trusted":true},"cell_type":"code","source":"with graph.as_default():\n    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"332bbb71-84bb-4a45-93da-a9f28d48245d","_uuid":"ed6008ad6bf6d88724523e948aba01a4f649aad7","trusted":true},"cell_type":"code","source":"def get_batches(x, y, batch_size=100):    \n    n_batches = len(x)//batch_size\n    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n    for ii in range(0, len(x), batch_size):\n        yield x[ii:ii+batch_size], y[ii:ii+batch_size]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2c58430c-e36d-4170-88fc-eb6468818d94","_uuid":"aeaadce1ec744f315b4edf025e4e752b21e066e0"},"cell_type":"markdown","source":"## Training"},{"metadata":{"_cell_guid":"f27e94a0-d21f-4885-8aba-133079a61352","_uuid":"0110900f97830864943c05a48b6998bcc3afb74d","trusted":true},"cell_type":"code","source":"epochs = 3\n\nwith graph.as_default():\n    saver = tf.train.Saver()\n\nwith tf.Session(graph=graph) as sess:\n    sess.run(tf.global_variables_initializer())\n    iteration = 1\n    for e in range(epochs):\n        state = sess.run(initial_state)\n        \n        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n            feed = {input_data: x,\n                    labels_: y,\n                    keep_prob: 1.0,\n                    initial_state: state}\n            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n            \n            if iteration%5==0:\n                print(\"Epoch: {}/{}\".format(e, epochs),\n                      \"Iteration: {}\".format(iteration),\n                      \"Train loss: {:.3f}\".format(loss))\n\n            if iteration%25==0:\n                val_acc = []\n                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n                for x, y in get_batches(val_x, val_y, batch_size):\n                    feed = {input_data: x,\n                            labels_: y,\n                            keep_prob: 1,\n                            initial_state: val_state}\n                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n                    val_acc.append(batch_acc)\n                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n            iteration +=1\n    saver.save(sess, \"checkpoints/mbti.ckpt\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0c39f046-f8a1-47a2-8029-71be84beabea","_uuid":"2bd79323f22ae74baf66c695c7ff65c426eadaaa"},"cell_type":"markdown","source":"## Testing"},{"metadata":{"_cell_guid":"3f2aba5c-346e-4c06-ba21-40d0b31d3b05","_uuid":"b23cef1239627fd2333091ee5a131772df7d95e4","trusted":true},"cell_type":"code","source":"test_acc = []\nwith tf.Session(graph=graph) as sess:\n    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n    for ii, (x, y) in enumerate(get_batches(test_x, test_y, batch_size), 1):\n        feed = {input_data: x,\n                labels_: y,\n                keep_prob: 1,\n                initial_state: test_state}\n        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n        test_acc.append(batch_acc)\n    print(\"Test accuracy: {:.3f}\".format(np.mean(test_acc)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"86513a17-0d1f-447c-b66e-80d741b932d0","_uuid":"8fb9ea36415a03d44412bcc7686f92fe0b8249fe","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","version":"3.6.1"},"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}