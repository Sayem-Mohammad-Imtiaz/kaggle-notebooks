{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Will You Get a Job or Not ?"},{"metadata":{},"cell_type":"markdown","source":"# Welcome To This Notebook\n1. I tried my best to explain every tiny details in this notebook in between as I can.\n2. I have compared all famous machine learning models in data modelling section to find the best model as I can :)\n3. Don't Forget to Upvote this Notebook, If you really liked it :) and that encourage me to upload more interesting notebooks like this in the future."},{"metadata":{},"cell_type":"markdown","source":"# Required Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import auc, roc_curve, classification_report, confusion_matrix, roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optional Settings"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# %matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# plt.rcdefaults()\n# sns.set_style()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# plt.rc(\"figure\", figsize=[9, 5])\n# plt.style.use(\"seaborn\")\n# sns.set(rc={\"figure.figsize\": [9, 5]})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Let's Look at the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv\")\ndata.drop(labels=[\"sl_no\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Take a Sneak Peek :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### The data types of the attributes are in the right form "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### As you can see, among 14 attributes (columns) there exist one attribute with 67 (215 - 67 = 148) missing values :("},{"metadata":{},"cell_type":"markdown","source":"### Optional"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"data.isna().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"data.loc[data[\"salary\"].isna(), :]    # You can also use this code :) -->  data[data[\"salary\"].isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Descriptive Analysis - Numeric Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### Class Imbalance Check !"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"status\", data=data, ax=ax)\nplt.title(\"Target Class Distribution\")\nplt.xlabel(\"Class Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### It says 148 (69%) people have been placed and 67 (31%) people have not placed. It concludes that our dataset is imbalanced but not that much :("},{"metadata":{},"cell_type":"markdown","source":"### Does Gender Impacts Placement ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"gender\", hue=\"status\", data=data, ax=ax)\nplt.xlabel(\"Gender\")\nplt.title(\"Gender vs Placement\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### The answer is Yes. Because more number of male have been placed than female. But when it comes to not placed people, it's not that much difference.  If you think it is unfair, feel free to post your opinion in the comment section."},{"metadata":{},"cell_type":"markdown","source":"### Do Specialisation Matters ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"specialisation\", hue=\"status\", data=data, ax=ax)\nplt.xlabel(\"Specialisation\")\nplt.title(\"Specialisation vs Placement\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### The chances of getting placed for students who have taken \"Mkt&Fin\" specialisation is more than the students with \"Mkt&HR\" specialisation. But don't worry 53 (36 %) out of 148 placed students are from \"Mkt&HR\" :)"},{"metadata":{},"cell_type":"markdown","source":"### Do Work Experience Helps You Get Placed ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"workex\", hue=\"status\", data=data, ax=ax)\nplt.xlabel(\"Work Experience\")\nplt.title(\"Work Experience vs Placement\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Really it looks weird but you gotta accept that. If you are a student, mostly you might have heard from your professor or some other saying that \"work experience through internship or any peoject really helps you to get placed\". But here you can see that most of students who have been placed are not having any work experience :("},{"metadata":{},"cell_type":"markdown","source":"### Which Degree has More Placements ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"degree_t\", hue=\"status\", data=data, ax=ax)\nplt.title(\"Degree Priority for Placement\")\nplt.xlabel(\"Degree\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### The \"Comm&Mgmt\" degree has more priority for recruiters to recruit. But they also need some \"Sci&Tech\" students than \"Others\" category students which is good sign for Science and Technology students :)"},{"metadata":{},"cell_type":"markdown","source":"### Does Board of Education Matters ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"ssc_b\", hue=\"status\", data=data, ax=ax)\nplt.title(\"Board of Education vs Placement\")\nplt.xlabel(\"Board of Education\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Looks like both board of education is having more equal (same but not exactly) chances of getting placed :)"},{"metadata":{},"cell_type":"markdown","source":"### Do Higher Secondary Group matters ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.countplot(x=\"hsc_s\", hue=\"status\", data=data, ax=ax)\nplt.xlabel(\"HSC Groups\")\nplt.title(\"HSC Groups vs Placement\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### I guess the recruiters are mostly choosing \"Commerce\" students because in the above \"Degree Priority\" Chart, the recruiters were mostly selected the students who have completed \"Comm&Mgmt\" degree. But don't worry, they have also recruited more \"Science\" group students which is a good sign for me :)"},{"metadata":{},"cell_type":"markdown","source":"### Do Employability Test Helps Getting Job?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure()\nax = fig.add_subplot()\n\nsns.barplot(x=\"status\", y=\"etest_p\", data=data, ax=ax, ci=None)\nplt.title(\"Employability Test vs Placement\")\nplt.xlabel(\"Status\")\nplt.ylabel(\"Employability Test\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Again you can't be confident if you have taken Employability Test to get placed :("},{"metadata":{},"cell_type":"markdown","source":"### Do High Percentage Holders Got Placed More Than Low Percentage Holders ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=[11, 6])\nax = fig.add_subplot()\n\nsns.scatterplot(x=\"ssc_p\", y=\"hsc_p\", hue=data[\"status\"].tolist(),\n                style=data[\"ssc_b\"].tolist(), size=data[\"hsc_s\"].tolist(), data=data, ci=None, ax=ax)\nplt.xlabel(\"Secondary School Percentage\")\nplt.ylabel(\"Higher Secondary School Percentage\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Now it is clear that students who have scored more than 60% in both secondary and higher secondary and have chosen commerce as their group in higher secondary have been placed more than the other students :)"},{"metadata":{},"cell_type":"markdown","source":"Note:\n1. Here we are only concerned about predicting whether a student will get placed or not which is a classification problem in our case.\n2. That's why I haven't included salary attribute (column) in EDA.\n3. However If you are interested in predicting the salary of placed and not placed student you can take it as a homework :)"},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Categorical Data Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_variables = data.select_dtypes(include=\"object\").columns.tolist()\n\ntreat_not_as_same = [\"degree_t\", \"hsc_s\"]\n\ntreat_as_same = [var for var in categorical_variables if not var in treat_not_as_same]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Why I'm treating \"degree_t\" and \"hsc_s\" as not same ? The answer is, they are not like all categorical variable they need some order to arange them which is nothing but one degree is bigger or smaller than the other or it has higher value than the others. For example Sci&Tech > Comm&Mgmt > Others"},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in treat_as_same[:-1]:\n    dict_to_map = {j:i for i, j in enumerate(data[var].unique())}\n    data[var] = data[var].map(dict_to_map)\n\ndata[\"status\"] = data[\"status\"].map({\"Not Placed\": 0, \"Placed\": 1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in treat_not_as_same:\n    data = pd.concat(objs=[data, pd.get_dummies(data=data[var])], axis=1)\n    data.drop(labels=var, axis=1, inplace=True)\n    \ndata.drop(labels=\"salary\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=[11, 6])\nax = fig.add_subplot()\n\nsns.heatmap(data=data.corr(), cmap=\"RdYlGn\", annot=True, fmt=\".2f\", ax=ax)\nplt.title(\"Correlation Among all Variables\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### As you can see there are some negative and positive correlations among one hot encoded variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(labels=\"status\", axis=1).values\ny = data[\"status\"].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Cross Validation"},{"metadata":{},"cell_type":"markdown","source":"Note:\n1. I'm doing Cross Validation before building the model because I want to know which algorithm is giving good result.\n2. So that I can give more effort to that model to get best result as I can :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [(\"Logistic Regression\", LogisticRegression(random_state=0, n_jobs=-1)),\n         (\"Linear SVM\", SVC(kernel=\"linear\", random_state=0)),\n         (\"RBF SVM\", SVC(kernel=\"rbf\", random_state=0)),\n         (\"Decision Tree\", DecisionTreeClassifier(random_state=0)),\n         (\"Random Forest\", RandomForestClassifier(n_jobs=-1, random_state=0)),\n         (\"Adaboost RF\", AdaBoostClassifier(base_estimator=RandomForestClassifier(n_jobs=-1, random_state=0), random_state=0, learning_rate=0.1)),\n         (\"Adaboost DT\", AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=0), learning_rate=0.1)),\n         (\"Gradient Boosting\", GradientBoostingClassifier(random_state=0))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stratified = StratifiedKFold()\nmodel_details = {name: [] for name, _ in models}\n\nfor train_index, test_index in stratified.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for name, model in models:\n        if name in [\"Logistic Regression\", \"Linear SVM\", \"RBF SVM\"]:\n            std = StandardScaler()\n            X_train = std.fit_transform(X_train)\n            X_test = std.transform(X_test)\n\n        model.fit(X_train, y_train)\n        train_accuracy = model.score(X_train, y_train)\n        test_accuracy = model.score(X_test, y_test)\n        model_details[name].append((train_accuracy, test_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_df = pd.DataFrame(index=[\"Train Score\", \"Test Score\"])\n\nfor model, accuracy in zip(model_details.keys(), model_details.values()):\n    train_accuracy = [train_accuracy for train_accuracy, _ in accuracy]\n    test_accuracy = [test_accuracy for _, test_accuracy in accuracy]\n    summary_df[model] = [np.mean(train_accuracy), np.mean(test_accuracy)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"summary_df.T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### From the above model we can conclude that the Support Vector Machine is doing better than the other models with \"RBF\" kernel. Note we can't stop right here by getting good accuracy, we need to evaluate our model using other metrics too."},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression"},{"metadata":{},"cell_type":"markdown","source":"Note:\n1. You can also use train_test_split here to get train and test dataset.\n2. But why I'm using StratifiedShuffleSplit here ?\n3. The answer is that our dataset is somehow imbalanced not fully and this ensure that our train and test will be more representive of both classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"stratified_split = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n\nfor train_index, test_index in stratified_split.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"std = StandardScaler()\nscaled_train = std.fit_transform(X_train)\nscaled_test = std.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logistic = LogisticRegression(random_state=0, n_jobs=-1)\nlogistic.fit(scaled_train, y_train)\nprint(\"Logistic Regression Test Score :\", logistic.score(scaled_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"con_mat = pd.DataFrame(confusion_matrix(y_test, logistic.predict(scaled_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"Logistic Regression Confusion Matrix\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Logistic Regression has proved to be a good classifier. However we have 4 False Positive (classified students who are not eligible for placement as eligible) and 4 False Negative (classified students who are eligible for placement as not eligible). But don't worry we will try to reduce this Type 1 and Type 2 error :)"},{"metadata":{},"cell_type":"markdown","source":"###### Note: In this problem we have to reduce False Negatives more than False Positives because we can't miss any student who is eligible for placement but our model predicted as not eligible :("},{"metadata":{},"cell_type":"markdown","source":"### ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresshold = roc_curve(y_test, logistic.predict_proba(scaled_test)[:, 1])\nauc_score = auc(fpr, tpr)\n\nfig = plt.figure(figsize=[9, 5])\nax = fig.add_subplot()\n\nplt.plot(fpr, tpr, c=\"darkred\", lw=2, label=\"AUC = {}\".format(round(auc_score, 2)))\nplt.plot([0, 1], [0, 1], c='black', lw=2, ls='--', label=\"AUC = {}\".format(0.5))\nplt.title(\"ROC Curve - Logistic Regression\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Tuning "},{"metadata":{"trusted":true},"cell_type":"code","source":"C = [100, 10, 1.0, 0.1, 0.01]\npenalty = ['l2', 'l1']\nparam = {\"C\": C, \"penalty\": penalty}\n\ngrid = GridSearchCV(estimator=LogisticRegression(random_state=0, n_jobs=-1), param_grid=param, n_jobs=-1)\ngrid.fit(scaled_train, y_train)\n\ngrid_model = grid.estimator.fit(scaled_train, y_train)\ntest_score = grid_model.score(scaled_test, y_test)\nprint(\"Tuned Logistic Regression Test Score :\", test_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, grid_model.predict(scaled_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### After doing hyperparameter tuning, the model is producing the same accuracy as we got before without tuning parameter."},{"metadata":{},"cell_type":"markdown","source":"## Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC(kernel=\"rbf\", random_state=0, probability=True)\nsvc.fit(scaled_train, y_train)\nprint(\"SVM Test Score :\", svc.score(scaled_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"con_mat = pd.DataFrame(confusion_matrix(y_test, svc.predict(scaled_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"SVM Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Wow, our SVM model has reduced the False Positive count to 1 which is pretty amazing :)"},{"metadata":{},"cell_type":"markdown","source":"### ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresshold = roc_curve(y_test, svc.predict_proba(scaled_test)[:, 1])\nauc_score = auc(fpr, tpr)\n\nfig = plt.figure(figsize=[9, 5])\nax = fig.add_subplot()\n\nplt.plot(fpr, tpr, c=\"darkred\", lw=2, label=\"AUC = {}\".format(round(auc_score, 2)))\nplt.plot([0, 1], [0, 1], c='black', lw=2, ls='--', label=\"AUC = {}\".format(0.5))\nplt.title(\"ROC Curve - SVM\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"C = [100, 10, 1.0, 0.1, 0.01]\nkernel = [\"linear\", \"RBF\", \"poly\"]\nparam = {\"C\": C, \"kernel\":kernel}\n\ngrid = GridSearchCV(estimator=SVC(random_state=0), param_grid=param, n_jobs=-1)\ngrid.fit(scaled_train, y_train)\n\ngrid_model = grid.estimator.fit(scaled_train, y_train)\ntest_score = grid_model.score(scaled_test, y_test)\nprint(\"Tuned SVM Test Score :\", test_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, grid_model.predict(scaled_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"dec = DecisionTreeClassifier(random_state=0)\ndec.fit(X_train, y_train)\nprint(\"Decision Tree Classifier Test Score :\", dec.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"con_mat = pd.DataFrame(confusion_matrix(y_test, dec.predict(X_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"Decision Tree Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### More or less but same as Logistic Regression performance :( Because Decision Trees are more likely to overfit to training data. So we need to tune it's parameter"},{"metadata":{},"cell_type":"markdown","source":"### ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresshold = roc_curve(y_test, dec.predict_proba(X_test)[:, 1])\nauc_score = auc(fpr, tpr)\n\nfig = plt.figure(figsize=[9, 5])\nax = fig.add_subplot()\n\nplt.plot(fpr, tpr, c=\"darkred\", lw=2, label=\"AUC = {}\".format(round(auc_score, 2)))\nplt.plot([0, 1], [0, 1], c='black', lw=2, ls='--', label=\"AUC = {}\".format(0.5))\nplt.title(\"ROC Curve - Decision Tree\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"depth = list(range(1, 11))\nmin_sample_split = np.arange(5, 30, 5)\nmin_leaf_sample = np.arange(3, 16, 3)\nfeatures = [\"auto\", \"sqrt\", \"log2\"]\nmax_leaf_nodes = [4, 6, 8, 10]\nparam = {\"max_depth\": depth, \"min_samples_split\": min_sample_split, \"min_samples_leaf\": min_leaf_sample,\n         \"max_features\": features, \"max_leaf_nodes\": max_leaf_nodes}\n\ngrid = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0), param_grid=param, n_jobs=-1)\ngrid.fit(X_train, y_train)\n\ngrid_model = grid.estimator.fit(X_train, y_train)\ntest_score = grid_model.score(X_test, y_test)\nprint(\"Tuned Decision Tree Test Score :\", test_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Always don't trust too much on Decision Trees but try to trust it using Random Forest and Boosting models."},{"metadata":{},"cell_type":"markdown","source":"### Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, grid_model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"ran = RandomForestClassifier(random_state=0)\nran.fit(X_train, y_train)\nprint(\"Random Forest Classifier Test Score :\", ran.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"con_mat = pd.DataFrame(confusion_matrix(y_test, ran.predict(X_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"Random Forest Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### That's the power of Random Forest :)  It proved to be a good classifier than SVM in our case. SVM has 7 False Positives but Random Forest has only 3 and False Negatives also good when compared overall :)"},{"metadata":{},"cell_type":"markdown","source":"### ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresshold = roc_curve(y_test, ran.predict_proba(X_test)[:, 1])\nauc_score = auc(fpr, tpr)\n\nfig = plt.figure(figsize=[9, 5])\nax = fig.add_subplot()\n\nplt.plot(fpr, tpr, c=\"darkred\", lw=2, label=\"AUC = {}\".format(round(auc_score, 2)))\nplt.plot([0, 1], [0, 1], c='black', lw=2, ls='--', label=\"AUC = {}\".format(0.5))\nplt.title(\"ROC Curve - Random Forest\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Yes, AUC for Random Forest is 95 :) That's great isn't it !"},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = np.arange(100, 140, 10)\nmax_depth = np.arange(3, 10, 2)\nmax_features = [\"auto\", \"sqrt\"]\nmax_leaf_nodes = np.arange(3, 10, 2)\nparam = {\"n_estimators\": n_estimators, \"max_depth\": max_depth, \"max_features\": max_features, \"max_leaf_nodes\": max_leaf_nodes}\n\ngrid = GridSearchCV(estimator=RandomForestClassifier(random_state=0, n_jobs=-1), param_grid=param, n_jobs=-1)\ngrid.fit(X_train, y_train)\n\ngrid_model = grid.estimator.fit(X_train, y_train)\ntest_score = grid_model.score(X_test, y_test)\nprint(\"Tuned Random Forest Test Score :\", test_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"AUC afetr parameter tuning : \", roc_auc_score(y_test, grid_model.predict_proba(X_test)[:, 1]))\n\n#confusion matrix\ncon_mat = pd.DataFrame(confusion_matrix(y_test, grid_model.predict(X_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"Random Forest Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, grid_model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Don't worry it's doing same as before :)"},{"metadata":{},"cell_type":"markdown","source":"## AdaBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"ada = AdaBoostClassifier(base_estimator=RandomForestClassifier(random_state=0, n_jobs=-1), learning_rate=0.1, random_state=0)\nada.fit(X_train, y_train)\nprint(\"AdaBoost Classifier Test Score :\", ada.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"con_mat = pd.DataFrame(confusion_matrix(y_test, ada.predict(X_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"AdaBoost Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fpr, tpr, thresshold = roc_curve(y_test, ada.predict_proba(X_test)[:, 1])\nauc_score = auc(fpr, tpr)\n\nfig = plt.figure(figsize=[9, 5])\nax = fig.add_subplot()\n\nplt.plot(fpr, tpr, c=\"darkred\", lw=2, label=\"AUC = {}\".format(round(auc_score, 2)))\nplt.plot([0, 1], [0, 1], c='black', lw=2, ls='--', label=\"AUC = {}\".format(0.5))\nplt.title(\"ROC Curve - AdaBoost\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### When comparing Random Forest with AdaBoost, I recommend to use Random Forest than AdaBoost :)"},{"metadata":{},"cell_type":"markdown","source":"### Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, ada.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GradientBoosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"grad = GradientBoostingClassifier(learning_rate=0.1, random_state=0)\ngrad.fit(X_train, y_train)\nprint(\"GradientBoost Classifier Test Score :\", grad.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"con_mat = pd.DataFrame(confusion_matrix(y_test, grad.predict(X_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"GardientBoost Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Again I recommend Random Forest :)"},{"metadata":{},"cell_type":"markdown","source":"### Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, grad.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Votting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = [(\"LR\", LogisticRegression(random_state=0, n_jobs=-1)),\n             (\"SVC\", SVC(random_state=0)),\n             (\"RF\", RandomForestClassifier(random_state=0, n_jobs=-1)),\n             (\"Ada\", AdaBoostClassifier(RandomForestClassifier(random_state=0, n_jobs=-1), learning_rate=0.1, random_state=0)),\n             (\"Dec\", DecisionTreeClassifier(random_state=0))]\nvot = VotingClassifier(estimators, n_jobs=-1)\nvot.fit(X_train, y_train)\nprint(\"Votting Classifier Test Score :\", vot.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"con_mat = pd.DataFrame(confusion_matrix(y_test, vot.predict(X_test)))\nfig = plt.figure(figsize=[6, 4])\nax = fig.add_subplot()\n\nsns.heatmap(con_mat, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", cbar=False, ax=ax)\nplt.title(\"Votting Classifier Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Classification Report"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, vot.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### Again Random Forest is good"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion :)\n1. From the mdoels we have built, Random Forest is doing better than the other models.\n2. You can also use Votting Classifier to predict the same as than Random Forest :(\n3. You can also use SVM other than Ensemble models.\n4. It is possible to select important categorical features using Chi-Square test but I have not implemented here."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}