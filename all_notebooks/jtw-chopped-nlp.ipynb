{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Chopped NLP\nData courtesy of Jeffrey Braun (https://www.kaggle.com/jeffreybraun/chopped-10-years-of-episode-data)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Import necessary modules","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Read CSV of Chopped Episode data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"chopped_data = pd.read_csv(\"/kaggle/input/chopped-10-years-of-episode-data/chopped.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"chopped_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating list of every appetizer ingredient","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"round_type = \"entree\"\nround_list = chopped_data[round_type].to_list()\nprint(round_list[:5])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating a modified sequence of data\nOne which decomposes a sentence so that a sentence like [\"My name is Jordan\"]  turns into [\"My name\", \"My name is\", \"My name is Jordan\"]","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"text = []\n\nfor selection in round_list:                     # For each basket selection in appetizers\n    ingredients_comma =  selection.split(\",\")         # Convert each basket selection into a list of strings\n    ingredients = []\n    for ingredient in ingredients_comma:\n        for element in ingredient.strip().split(\" \"):\n            ingredients.append(element)\n    for i in range(1, len(ingredients)):              # For as long as the list of strings is\n        n_gram = ingredients[:i + 1]                  # create a fragment of the sentence\n        text.append(n_gram)                           # and append it to text\n\nprint(text[:15])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenizing our words","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words = 2000, oov_token = \"<OOV>\")    # Generate a tokenizer\ntokenizer.fit_on_texts(text)                                    # and fit it on our text\nsequences = tokenizer.texts_to_sequences(text)                  # Turn all of our text into texts\nword_index = tokenizer.word_index                               \nword_count = len(word_index) - 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Padding each element so that they are all the same length","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_len = max([len(sequence) for sequence in sequences])\nsequences = np.array(pad_sequences(sequences, maxlen = max_len, padding = \"pre\"))\nprint(sequences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating our inputs and labels for our training data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xs = sequences[:,:-1]\nlabels = sequences[:,-1]\n\nys = tf.keras.utils.to_categorical(labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating and compiling a model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(word_count, 64, input_length = max_len - 1),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n    tf.keras.layers.Dense(word_count +2, activation = \"softmax\"),\n])\n\nmodel.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training our model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(xs, ys, epochs=300, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Making Predictions with our model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_ingredients = \"chicken\"\nnum_words = 6\n\nfor _ in range(num_words):\n    token_list = tokenizer.texts_to_sequences([seed_ingredients])[0]\n    token_list = pad_sequences([token_list], maxlen = max_len - 1, padding = \"pre\")\n    predicted = model.predict_classes(token_list, verbose = 2)\n    output_word = \"\"\n    for word, i in tokenizer.word_index.items():\n        if i == predicted:\n            output_word = word\n            break\n    seed_ingredients += f\" {output_word}\"\n\nprint(seed_ingredients)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}