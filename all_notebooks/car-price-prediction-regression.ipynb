{"cells":[{"metadata":{},"cell_type":"markdown","source":"> # Car Price Prediction - Regression\n### Business Objective\nThe objective of this analysis is to provide a reliable regression model to predict the price of a car based on the variables provided as accurately as possible. The idea is for this to be used in the future for any new cars that would added to the dataset going forward.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Initial Data Pull and Exploration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Read in the Data Dictionary provided to give clearer insight into the dataset's variables\ndf_dict = pd.read_excel('../input/car-data/Data Dictionary - carprices.xlsx')\ndf_dict.rename(columns={\"Unnamed: 7\": \"Column_Name\", \"Unnamed: 11\": \"Description\"}, inplace=True)\ndf_dict = df_dict.drop(df_dict.index[0:3])\ndf_dict = df_dict.drop(df_dict.index[26:28])\ndf_dict = df_dict.filter([\"Column_Name\",\"Description\"])\npd.set_option('display.max_colwidth', -1)\ndf_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read in CarPrice_Assignment csv dataset\ndf = pd.read_csv('../input/car-data/CarPrice_Assignment.csv')\nprint(df.shape)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Look closer at the CarPrices dataset using describe() and info()\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#When looking at the CarName column, you can see that the Manufacturer is at the beginning of the values. We want to extract this as a new column\nManufacturer = df['CarName'].apply(lambda x : x.split(' ')[0])\nManufacturer.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking through the Manufacturer names pulled from the CarName column, there appears to be misspelling and variations on a few of the names. These will be updated/corrected.\n\n'maxda' -> 'mazda'\n\n'Nissan' -> 'nissan'\n\n'porcshce' -> 'porcshe'\n\n'toyouta' -> 'toyota'\n\n'vokswagen' -> 'volkswagen'\n\n'vm' -> 'volkswagen'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Manufacturer = Manufacturer.replace('maxda', 'mazda')\nManufacturer = Manufacturer.replace('Nissan', 'nissan')\nManufacturer = Manufacturer.replace('porcshce', 'porsche')\nManufacturer = Manufacturer.replace('toyouta', 'toyota')\nManufacturer = Manufacturer.replace('vokswagen', 'volkswagen')\nManufacturer = Manufacturer.replace('vw', 'volkswagen')\nManufacturer.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.insert(1,'manufacturer',Manufacturer)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since there are so many unique Car Names compared to the overall size of the dataset, I will remove 'CarName' now that we have Manufacturer to be used instead.\ndf.drop(['CarName'],axis=1,inplace=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking into the dependent variable \"price\"\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nplt.title('Price Boxplot')\nsns.boxplot(y=\"price\", data=df)\n\nplt.subplot(1,2,2)\nplt.title('Price Distribution')\nsns.distplot(df[\"price\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking at how the different manufacturers compare with against their prices\nax  = sns.boxplot(x=\"manufacturer\", y=\"price\", data=df)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting counts of each manufacturer as well\nax = sns.countplot(x=\"manufacturer\", data=df)\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking into the Categorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.select_dtypes(include=['object'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Countplots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\n\nplt.subplot(1,3,1)\nplt.title('Fuel Type')\nsns.countplot(x=\"fueltype\", data=df)\n\nplt.subplot(1,3,2)\nplt.title('Aspiration')\nsns.countplot(x=\"aspiration\", data=df)\n\nplt.subplot(1,3,3)\nplt.title('Number of Doors')\nsns.countplot(x=\"doornumber\", data=df)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\n\nplt.subplot(1,3,1)\nplt.title('Carbody')\nsns.countplot(x=\"carbody\", data=df)\n\nplt.subplot(1,3,2)\nplt.title('Drivewheel')\nsns.countplot(x=\"drivewheel\", data=df)\n\nplt.subplot(1,3,3)\nplt.title('Engine Location')\nsns.countplot(x=\"enginelocation\", data=df)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\n\nplt.subplot(1,3,1)\nplt.title('Engine Type')\nsns.countplot(x=\"enginetype\", data=df)\n\nplt.subplot(1,3,2)\nplt.title('Cylinder Number')\nsns.countplot(x=\"cylindernumber\", data=df)\n\nplt.subplot(1,3,3)\nplt.title('Fuel System')\nsns.countplot(x=\"fuelsystem\", data=df)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Boxplots","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,5))\n\nplt.subplot(1,3,1)\nplt.title('Fuel Type')\nsns.boxplot(x=\"fueltype\", y=\"price\", data=df)\n\nplt.subplot(1,3,2)\nplt.title('Aspiration')\nsns.boxplot(x=\"aspiration\", y=\"price\", data=df)\n\nplt.subplot(1,3,3)\nplt.title('Number of Doors')\nsns.boxplot(x=\"doornumber\", y=\"price\", data=df)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,5))\n\nplt.subplot(1,3,1)\nplt.title('Carbody')\nsns.boxplot(x=\"carbody\", y=\"price\", data=df)\n\nplt.subplot(1,3,2)\nplt.title('Drivewheel')\nsns.boxplot(x=\"drivewheel\", y=\"price\", data=df)\n\nplt.subplot(1,3,3)\nplt.title('Engine Location')\nsns.boxplot(x=\"enginelocation\", y=\"price\", data=df)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,5))\n\nplt.subplot(1,3,1)\nplt.title('Engine Type')\nsns.boxplot(x=\"enginetype\", y=\"price\", data=df)\n\nplt.subplot(1,3,2)\nplt.title('Cylinder Number')\nsns.boxplot(x=\"cylindernumber\", y=\"price\", data=df)\n\nplt.subplot(1,3,3)\nplt.title('Fuel System')\nsns.boxplot(x=\"fuelsystem\", y=\"price\", data=df)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical Variable Conclusions\nAfter looking at the countplots and boxplots for the categorical variables, the number of doors (\"doornumber\") does not seem to play much of a role in the price. Also, engine location (\"enginelocation\") only has a few records with values as \"rear\" and the \"front\" engine location records have prices across the board. So I will not be including either in the model later.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Looking into the Numerical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.select_dtypes(include=['int64', 'float64'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Looking to understand the symboling variable\nplt.figure(figsize=(18,5))\n\nplt.subplot(1,3,1)\nsns.boxplot(x=\"symboling\", y=\"price\", data=df)\n\nplt.subplot(1,3,2)\nsns.countplot(x=\"symboling\", data=df)\n\nplt.subplot(1,3,3)\nsns.scatterplot(x=\"symboling\", y=\"price\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparing the other numerical variables to price\nplt.figure(figsize=(20,5))\n\nplt.subplot(1,3,1)\nplt.title('Wheel Base')\nsns.scatterplot(x=\"wheelbase\", y=\"price\", data=df)\n\nplt.subplot(1,3,2)\nplt.title('Engine Size')\nsns.scatterplot(x=\"enginesize\", y=\"price\", data=df)\n\nplt.subplot(1,3,3)\nplt.title('Curb Weight')\nsns.scatterplot(x=\"curbweight\", y=\"price\", data=df)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\n\nplt.subplot(1,3,1)\nplt.title('Car Length')\nsns.scatterplot(x=\"carlength\", y=\"price\", data=df)\n\nplt.subplot(1,3,2)\nplt.title('Car Width')\nsns.scatterplot(x=\"carwidth\", y=\"price\", data=df)\n\nplt.subplot(1,3,3)\nplt.title('Car Height')\nsns.scatterplot(x=\"carheight\", y=\"price\", data=df)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,5))\n\nplt.subplot(1,3,1)\nplt.title('Bore Ratio')\nsns.scatterplot(x=\"boreratio\", y=\"price\", data=df)\n\nplt.subplot(1,3,2)\nplt.title('Stroke')\nsns.scatterplot(x=\"stroke\", y=\"price\", data=df)\n\nplt.subplot(1,3,3)\nplt.title('Compression Ratio')\nsns.scatterplot(x=\"compressionratio\", y=\"price\", data=df)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(23,5))\n\nplt.subplot(1,4,1)\nplt.title('Horsepower')\nsns.scatterplot(x=\"horsepower\", y=\"price\", data=df)\n\nplt.subplot(1,4,2)\nplt.title('Peak RPM')\nsns.scatterplot(x=\"peakrpm\", y=\"price\", data=df)\n\nplt.subplot(1,4,3)\nplt.title('City MPG')\nsns.scatterplot(x=\"citympg\", y=\"price\", data=df)\n\nplt.subplot(1,4,4)\nplt.title('Highway MPG')\nsns.scatterplot(x=\"highwaympg\", y=\"price\", data=df)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Numeric Variable Conclusions\nAfter looking at the scatter plots, the numeric variables symboling, carheight, compressionratio and peakrpm do not seem to have a significant correlation with price and will be removed in the restructuring step before modeling.\n\nI also want to see how these variables are correlated with each other.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df[['price','wheelbase','enginesize','curbweight','carlength','carwidth','boreratio','stroke','horsepower','citympg','highwaympg']])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df[['price','wheelbase','enginesize','curbweight','carlength','carwidth','boreratio','stroke','horsepower','citympg','highwaympg']].corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the pairplots and heatmap graph related to the correlation, you can see that there are numerous variables that appear to have moderate to high correlations. Things like carlength and wheelbase seem to be positively correlated but citympg and highwaympg are the 2 that jump out to me. Since they are so similar, I will perform some feature engineering to combine these variables.\n\n## Develop new variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#As mentioned above, since citympg and highwaympg are so strongly correlated and when you think about it logically these 2 variables \n    #are both related to the car's fuel economy and how many mpg the car can drive.\n    \n#I will combine these 2 variables into one by taking the adding the values and dividing by 2. This will give a rough \"average mpg\".\n\ndf[\"avgmpg\"] = (df[\"citympg\"]+df[\"highwaympg\"])/2\ndf[\"avgmpg\"].head(10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Restructure the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cars = df[['price', 'manufacturer', 'fueltype', 'aspiration','carbody','drivewheel','enginetype','cylindernumber','fuelsystem','wheelbase','enginesize','curbweight','carlength','carwidth','boreratio','stroke','horsepower','avgmpg']]\ndf_cars","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Setting up dummy variables from the categorical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummy = pd.get_dummies(df_cars['manufacturer'])\ndf_cars = pd.concat([df_cars, df_dummy], axis = 1)\ndf_cars.drop('manufacturer', axis = 1, inplace=True)\n\ndf_dummy = pd.get_dummies(df_cars['fueltype'])\ndf_cars = pd.concat([df_cars, df_dummy], axis = 1)\ndf_cars.drop('fueltype', axis = 1, inplace=True)\n\ndf_dummy = pd.get_dummies(df_cars['aspiration'])\ndf_cars = pd.concat([df_cars, df_dummy], axis = 1)\ndf_cars.drop('aspiration', axis = 1, inplace=True)\n\ndf_dummy = pd.get_dummies(df_cars['carbody'])\ndf_cars = pd.concat([df_cars, df_dummy], axis = 1)\ndf_cars.drop('carbody', axis = 1, inplace=True)\n\ndf_dummy = pd.get_dummies(df_cars['drivewheel'])\ndf_cars = pd.concat([df_cars, df_dummy], axis = 1)\ndf_cars.drop('drivewheel', axis = 1, inplace=True)\n\ndf_dummy = pd.get_dummies(df_cars['enginetype'])\ndf_cars = pd.concat([df_cars, df_dummy], axis = 1)\ndf_cars.drop('enginetype', axis = 1, inplace=True)\n\ndf_dummy = pd.get_dummies(df_cars['cylindernumber'])\ndf_cars = pd.concat([df_cars, df_dummy], axis = 1)\ndf_cars.drop('cylindernumber', axis = 1, inplace=True)\n\ndf_dummy = pd.get_dummies(df_cars['fuelsystem'])\ndf_cars = pd.concat([df_cars, df_dummy], axis = 1)\ndf_cars.drop('fuelsystem', axis = 1, inplace=True)\n\nprint(df_cars.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Splitting dataset into Training and Testing sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\ny = df_cars['price']\nX = df_cars.drop(['price'], axis = 1)\n\nX_train_org, X_test_org, y_train, y_test = train_test_split(X, y, random_state = 0)\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train_org)\n#.fit_transform first fits the original data and then transforms it\nX_test = scaler.transform(X_test_org)\n\nprint(\"X_train shape: \", X_train.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"X_test shape: \", X_test.shape)\nprint(\"y_test shape: \", y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building a Model\n\n### Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\n\nlreg = LinearRegression()\nlreg.fit(X_train, y_train)\nprint(\"R2 Training Score: \", lreg.score(X_train, y_train))\nprint(\"R2 Testing Score: \", lreg.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lreg.intercept_)\nlreg.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, looking at the above coefficients and intercepts aren't necessarily practical, but can still be interesting to look at. We will use this model though to run a prediction on the test set.\n\n### Linear Regression Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predict = lreg.predict(X_test)\ntest_predict = pd.DataFrame(test_predict,columns=['Predicted_Price'])\ntest_predict['Predicted_Price'] = round(test_predict['Predicted_Price'],2)\n\ny_test_index = y_test.reset_index()\ny_test_index = y_test_index.drop(columns='index', axis = 1)\ntest_predict = pd.concat([y_test_index, test_predict], axis = 1)\ntest_predict.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the predictions compared to the listed price, you can see there are examples where the values are very similar, but there are also values that are quite different. The biggest example I can see is index 3, where the predicted price is -4707.10 which is completely invalid. Lets look at how these predictions vs the price look in a regplot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,5))\n\nplt.subplot(1,2,1)\nsns.scatterplot(x=\"price\", y=\"Predicted_Price\", data=test_predict)\n\nplt.subplot(1,2,2)\nsns.regplot(x=\"price\", y=\"Predicted_Price\", data=test_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that the records with prices/prediction between 5K-20K are fairly close to the prediction line for the most part while values >$20K appear further away. To me this makes sense as the vast majority of prices in the current dataset are in this range while there are fewer cars/records with higher prices.\n\nWhile this is not a bad model, I wanted to see how using L1 and L2 regularization could affect the outcome of the model. Below I will run Lasso and Ridge Regression to see how they compare to the base Linear Regression.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Lasso Regression\nLasso regression is a linear least squares model with L1 regularization. This creates a loss function where the model minimizes the sum of the least absolute errors. i.e minimizing the sum of the absolute value differences (errors) between the true y values and estimated y values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from  sklearn.linear_model import Lasso\n\n#Testing different alpha values for the L1 regularization\nalpha_range = [0.01, 0.1, 1, 10, 100]\ntrain_score_list = []\ntest_score_list = []\n\nfor alpha in alpha_range: \n    lasso = Lasso(alpha)\n    lasso.fit(X_train,y_train)\n    train_score_list.append(lasso.score(X_train,y_train))\n    test_score_list.append(lasso.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparing different alpha values to see which produces the best scores\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.plot(alpha_range, train_score_list, c = 'g', label = 'Train Score')\nplt.plot(alpha_range, test_score_list, c = 'b', label = 'Test Score')\nplt.xscale('log')\nplt.legend(loc = 3)\nplt.xlabel(r'$\\alpha$')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The alpha values between [0.01, 0.1, 1, 10] seem to be overfitting the data, so I will try alpha = 100.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso(100)\nlasso.fit(X_train,y_train)\nprint(lasso.score(X_train,y_train))\nprint(lasso.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These R Squared scores are quite similar to the linear regression model (fairly lower training score but slightly higher test score). We can see how running a prediction using this model will look compared to the linear regression prediction.\n\n### Lasso Regression Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predict = lasso.predict(X_test)\ntest_predict = pd.DataFrame(test_predict,columns=['Predicted_Price'])\ntest_predict['Predicted_Price'] = round(test_predict['Predicted_Price'],2)\ny_test_index = y_test.reset_index()\ny_test_index = y_test_index.drop(columns='index', axis = 1)\ntest_predict = pd.concat([y_test_index, test_predict], axis = 1)\nprint(test_predict.head(15))\nsns.regplot(x=\"price\", y=\"Predicted_Price\", data=test_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Again not a bad model, it seems to have fixed the prediction that was giving a negative value, but when looking at the predictions compared to the price there are larger error terms.\n\n### Ridge Regression\nRidge regression is a linear least squares model with L2 regularization. This creates a loss function where the model minimizes the sum of the least squares errors. i.e minimizing the sum of the squared differences (errors) between the true y values and estimated y values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from  sklearn.linear_model import Ridge\n\n#Testing different alpha values for the L2 regularization\nx_range = [0.01, 0.1, 1, 10, 100]\ntrain_score_list = []\ntest_score_list = []\n\nfor alpha in x_range: \n    ridge = Ridge(alpha)\n    ridge.fit(X_train,y_train)\n    train_score_list.append(ridge.score(X_train,y_train))\n    test_score_list.append(ridge.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.plot(x_range, train_score_list, c = 'g', label = 'Train Score')\nplt.plot(x_range, test_score_list, c = 'b', label = 'Test Score')\nplt.xscale('log')\nplt.legend(loc = 3)\nplt.xlabel(r'$\\alpha$')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_score_list)\nprint(test_score_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alpha = 1 give the best results for test while still having a high score for train so I will use that alpha value for the model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Ridge Regression Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge(1)\nridge.fit(X_train,y_train)\ntest_predict = ridge.predict(X_test)\ntest_predict = pd.DataFrame(test_predict,columns=['Predicted_Price'])\ntest_predict['Predicted_Price'] = round(test_predict['Predicted_Price'],2)\ny_test_index = y_test.reset_index()\ny_test_index = y_test_index.drop(columns='index', axis = 1)\ntest_predict = pd.concat([y_test_index, test_predict], axis = 1)\ntest_predict.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x=\"price\", y=\"Predicted_Price\", data=test_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This appears to give the best model and prediction. It still has higher error terms once the prices go beyond ~25K, but that is a current limitation of this dataset. \n\n## Conclusion\n\nAs with any model, as additional data is collected or new variables become available we could re-evaluate the model and see what happens, but for the variables provided in the dataset, once cleaned and preprocessed, a Ridge Regression model with alpha value = 1 provides the best price prediction on the cars. It gives an R-Squared for the training split of 95.9% and the testing split of 87.9%. \n\n#### Next Steps & Future Analysis\n\nAn interesting exercise going beyond this analysis could be seeing how feature reduction could affect the models above or seeing how different regression techniques/algorithms could be used to create better predictions. If you have any suggestions please feel free to comment, I would love to hear any feedback or ideas.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Further Analysis utilizing Cross Validation\n\nBeyond linear, lasso and ridge regression on the train and test split, I want to test how Lasso, Ridge and K Neighbors Regressor perform using cross validation against the entire dataset. This is the simulate another type of scenario where you want to generate a model against your entire dataset while validating with cross validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Re-show X and y (y = price, X = all other variables from df_cars)\nprint(X.shape)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scale the X dataset\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y.shape)\ny[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K Neighbors Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nnn_list = list(range(1,51))\n\nparam_grid = {'n_neighbors': nn_list}\n\ngrid_search = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5)\ngrid_search.fit(X_scaled, y)\nprint(grid_search.score(X_scaled,y))\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using cross validation on the entire dataset (X being scaled), you find that n_neighbors = 23 provide the best results. Below I will fit the algorithm with the found n_neighbors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn_reg = KNeighborsRegressor(n_neighbors=23)\nknn_reg.fit(X_scaled, y)\nknn_reg.score(X_scaled, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'alpha': [0.01,0.1,1,10,100]}\n\ngrid_search = GridSearchCV(Lasso(), param_grid, cv=5)\ngrid_search.fit(X_scaled, y)\nprint(\"Best parameters: {}\".format(grid_search.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso(alpha=100)\nlasso.fit(X_scaled, y)\nlasso.score(X_scaled, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {'alpha': [0.01,0.1,1,10,100]}\n\ngrid_search = GridSearchCV(Ridge(), param_grid, cv=5)\ngrid_search.fit(X_scaled, y)\nprint(\"Best parameters: {}\".format(grid_search.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge(alpha=1)\nridge.fit(X_scaled, y)\nridge.score(X_scaled,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cross Validation Conclusion\n\nAfter utilizing cross validation and grid search to find the best hyperparameters for each of the respective models, Ridge regression still provides the best results as previously found when running with the train and test split.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}