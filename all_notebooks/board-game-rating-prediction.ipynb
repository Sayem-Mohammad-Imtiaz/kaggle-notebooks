{"cells":[{"metadata":{"_uuid":"f6c87d62-77cf-4c69-afb7-6d20f02afaa6","_cell_guid":"f9284412-f412-4d77-bb77-7a9fa7199a55","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preprocess\n### For this part, we load our dataset, and then drop useless column, and we also drop contain miss value line, and we convert the uppercase letters to lowercase, and the below is shown the dataset we have processed, it contains the rating and comment column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(file_path):\n    \n    comment_rating = pd.read_csv(file_path, encoding = \"ISO-8859-1\")\n    comment_rating = comment_rating.drop(columns=[\"Unnamed: 0\", 'user', 'ID', 'name'])\n    comment_rating = comment_rating.dropna(axis = 0, how = 'any')    \n    comment_rating = comment_rating.reset_index(drop = True)\n    comment_rating[\"comment\"] = comment_rating[\"comment\"].apply(lambda x: x.lower())\n    \n    return comment_rating\n\ncomment_rating = preprocess('/kaggle/input/boardgamegeek-reviews/bgg-13m-reviews.csv')   \n\ncomment_rating\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### And then, producing our rating part, let the rating number round to integer, it's benefit to the next part we predict the comment.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generation_new_set(comment_rating):\n    rating_num_set = {}\n    for rating in range(1, 11, 1):\n        new_comment_rating = comment_rating.loc[comment_rating['rating'] >= (rating - 0.5)]\n        new_comment_rating = new_comment_rating.loc[new_comment_rating['rating'] <= (rating + 0.5)]\n        new_comment_rating = new_comment_rating.sample(frac = 1).reset_index(drop = True)\n        rating_num_set[rating] = new_comment_rating\n    return rating_num_set\n\nrating_num_set = generation_new_set(comment_rating)\n\nfor rating in rating_num_set:\n    print(\"rating: \", rating, \"rating num:\",  len(rating_num_set[rating]))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_list = []\nfor rating in rating_num_set: \n    rating_list.append(len(rating_num_set[rating]))\nplt.bar(range(len(rating_list)), rating_list)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_train_dev_test(comment_rating):\n   \n    train_set = comment_rating[:int(0.7 * len(comment_rating))]\n    test = comment_rating[int(0.7 * len(comment_rating)):]\n    test_set = test[:int(0.5 * len(test))]\n    dev_set = test[int(0.5 * len(test)):]\n    \n    dev_set = dev_set.sample(frac = 1).reset_index(drop = True)\n    test_set = test_set.sample(frac = 1).reset_index(drop = True)\n    \n    train_set = train_set.copy()\n    train_set['reating'] = [round(rating) for rating in train_set['rating']]\n    test_set['reating'] = [round(rating) for rating in test_set['rating']]\n    dev_set['reating'] = [round(rating) for rating in dev_set['rating']]\n\n    return train_set, test_set, dev_set\n\ntrain_set, dev_set, test_set = split_train_dev_test(comment_rating)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"length of train_set: \", len(train_set))\nprint(\"length of dev_set: \", len(dev_set))\nprint(\"length of test_set: \", len(test_set))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Designed two vectorizer function: tfidf and count, and use this function to process the train set and development set, to get the result for next prediction, and then compared the accuracy of the two vectorizer function handal the modul efficiency.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def vectorizer_tfidf(train_set, test_set, dev_set):\n    tfidf_model = TfidfVectorizer()\n    tfidf_model.fit(train_set['comment'])\n    \n    train_tfidf = tfidf_model.transform(train_set['comment'])\n    test_tfidf = tfidf_model.transform(test_set['comment'])\n    dev_tfidf = tfidf_model.transform(dev_set['comment'])\n    \n    train_tag = train_set['reating'].astype(int)\n    test_tag = test_set['reating'].astype(int)\n    dev_tag = dev_set['reating'].astype(int)\n    \n    return train_tfidf, train_tag, test_tfidf, test_tag, dev_tfidf, dev_tag\n\ntrain_tfidf, train_tag, test_tfidf, test_tag, dev_tfidf, dev_tag = vectorizer_tfidf(train_set, test_set, dev_set)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vectorizer_count(train_set, test_set, dev_set):\n    count_model = CountVectorizer()\n    count_model.fit(train_set['comment'])\n    \n    train_count = count_model.transform(train_set['comment'])\n    test_count = count_model.transform(test_set['comment'])\n    dev_count = count_model.transform(dev_set['comment'])\n    \n    train_tag = train_set['reating'].astype(int)\n    test_tag = test_set['reating'].astype(int)\n    dev_tag = dev_set['reating'].astype(int)\n    \n    return train_count, train_tag, test_count, test_tag, dev_count, dev_tag\n\ntrain_count, train_tag, test_count, test_tag, dev_count, dev_tag = vectorizer_count(train_set, test_set, dev_set)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Naive bayes modul.\n### Using naive bayes modul to train our dataset and predict the rating, the bayes theorem p(y|x1x2x3...xn) = p(x1x2x3...xn|y)p(y) / p(x1x2x3...xn). to get the predict, we useing two method to compared the prediction, and drow the bar graph to visualization.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"show_figure = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_tiidf_bayes(train_tfidf, train_tag, dev_tfidf, dev_tag):\n    naive_bayes = MultinomialNB()\n    naive_bayes.fit(train_tfidf, train_tag)\n    dev_predict = naive_bayes.predict(dev_tfidf)\n    accuracy = accuracy_score(dev_tag, dev_predict)\n    show_figure['tfidf_bayes'] = accuracy\n    print('tfidfvectorizer on naive bayes accuracy: ', accuracy * 100)\n    return show_figure\n\nshow_figure = apply_tiidf_bayes(train_tfidf, train_tag, dev_tfidf, dev_tag)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_count_bayes(train_count, train_tag, dev_count, dev_tag):\n    naive_bayes = MultinomialNB()\n    naive_bayes.fit(train_count, train_tag)\n    dev_predict = naive_bayes.predict(dev_count)\n    accuracy = accuracy_score(dev_tag, dev_predict)\n    show_figure['count_bayes'] = accuracy\n    \n    print('countvectorizer on naive bayes accuracy: ', accuracy * 100)\n    return show_figure, naive_bayes\n\nshow_figure, naive_bayes = apply_count_bayes(train_count, train_tag, dev_count, dev_tag)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure = plt.figure(figsize=(6, 4)).add_subplot()        \nfigure.set_title('countvectorizer and tfidfvectorizer on bayes  accuracy')\nfigure.set_xticklabels(['tfidf_bayes', 'count_bayes']) \nfigure.set_ylabel('accuracy')\nplt.bar('tfidf_bayes', show_figure['tfidf_bayes'])\nfigure = plt.bar('count_bayes', show_figure['count_bayes'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM modul\n### Using svm to train our modul, the svm theorem: yi(w/||w||* xi + b / ||w||). and get the prediction, compared the two method, and get the best modul.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"show_figure = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_tiidf_svm(train_tfidf, train_tag, dev_tfidf, dev_tag):\n    svm_modul = LinearSVC()\n    svm_modul.fit(train_tfidf, train_tag)\n    dev_predict = svm_modul.predict(dev_tfidf)\n    accuracy = accuracy_score(dev_tag, dev_predict)\n\n    show_figure['tfidf_svm'] = accuracy\n    print('tfidfvectorizer on svm accuracy: ', accuracy * 100)\n    return show_figure\n\nshow_figure = apply_tiidf_svm(train_tfidf, train_tag, dev_tfidf, dev_tag)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_count_bayes(train_count, train_tag, dev_count, dev_tag):\n    svm_modul = LinearSVC()\n    svm_modul.fit(train_count, train_tag)\n    dev_predict = svm_modul.predict(dev_count)\n    accuracy = accuracy_score(dev_tag, dev_predict)\n    show_figure['count_svm'] = accuracy\n    \n    print('countvectorizer on svm accuracy: ', accuracy * 100)\n    return show_figure\n\nshow_figure = apply_count_bayes(train_count, train_tag, dev_count, dev_tag)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"figure = plt.figure(figsize=(6, 4)).add_subplot()        \nfigure.set_title('countvectorizer and tfidfvectorizer on svm  accuracy')\nfigure.set_xticklabels(['tfidf_svm', 'count_svm']) \nfigure.set_ylabel('accuracy')\nplt.bar('tfidf_svm', show_figure['tfidf_svm'])\nfigure = plt.bar('count_svm', show_figure['count_svm'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Experiment: hyperparameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"show_figure = {}\nhyper_list = [0.001, 0.01, 0.1, 0.5, 0.7, 0.9]\n\ndef find_svm_hyper(train_tfidf, train_tag, dev_tfidf, dev_tag, hyper_list):\n    for index in hyper_list:\n        svm_modul = LinearSVC(C = index)\n        svm_modul.fit(train_tfidf, train_tag)\n        dev_predict = svm_modul.predict(dev_tfidf)\n        accuracy = accuracy_score(dev_tag, dev_predict)\n\n        show_figure[index] = accuracy\n        print('hyper: ', index, 'tfidfvectorizer on svm accuracy: ', accuracy * 100)\n    return show_figure\n\nshow_figure = find_svm_hyper(train_tfidf, train_tag, dev_tfidf, dev_tag, hyper_list)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rating_list = []\nrow_list = []\nfor rating in show_figure:\n    rating_list.append(show_figure[rating])\n    row_list.append(rating)\nplt.plot(row_list, rating_list, color='red')\nplt.title('SVM Hyper of Accuracy')\nplt.xlabel('Hyper')\nplt.ylabel('Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Challenge\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"1. Data preprocessing:\nFind whitch data is useless and missing, and how to vectorization trainset, development set and testset ? \nUseless data such as username, movie id, we drop it, and for missing value, we delete that data row. Compared the two vectorization method, tfidf vectorization and count vectorization, and Get the proformanc of count vectorization is better.\n\n2. Modul selection:\nHow to find the modul satisfy this dataset, and how to train modul, get accuracy ? \nTest the naive bayes and svm modul, find it can perform well, and then training our trainset and test it on testset.\n\n3. Hyper parameter tuning:\nHow to find the best hyper? we set different hyper list and train on svm modul, depend on different accuracy, judge whtch hyper is beeter.\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Reference","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n1. http://www.tfidf.com/\n\n2. https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n\n3. https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\n\n4. https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\n\n5. https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/\n\n6. https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/\n\n\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}