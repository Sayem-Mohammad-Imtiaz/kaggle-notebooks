{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Australia Rain Prediction using a Neural Network Model"},{"metadata":{},"cell_type":"markdown","source":"### 1. What do we want to achieve?"},{"metadata":{},"cell_type":"markdown","source":"In this notebook, I will create a classification model using an Artificial Neural Network (ANN) to determine whether or not it will rain tomorrow in Australia.\n\nI've used the \"Rain in Australia\" dataset for this project.\n\n"},{"metadata":{},"cell_type":"markdown","source":"### 2. Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom keras.layers import Dense, BatchNormalization, Dropout, LSTM\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\nfrom keras import callbacks\nfrom keras.optimizers import Adam\n\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Import Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = ('../input/weather-dataset-rattle-package/weatherAUS.csv')\n\ndf = pd.read_csv(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. EDA (Exploratory Data Analysis)"},{"metadata":{},"cell_type":"markdown","source":"#### Finding out the general format of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = df.columns\n\ncol_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Things to note:\n- We can see that the dataset contains a mixture of categorical and numerical variables.\n- Categorical variables are type 'object'\n- Numerical variables are type 'float64'\n- There are quite a lot of missing values in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5. Univariate Analysis\n\n##### Explore 'RainTomorrow' variable\n\nCheck for missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['RainTomorrow'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check for, and then view unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['RainTomorrow'].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['RainTomorrow'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two unique values are 'No' and 'Yes'.\n\nLet's visualise this."},{"metadata":{"trusted":true},"cell_type":"code","source":"q = sns.countplot(x = df['RainTomorrow'], palette = 'crest')\nq.set(xlabel = 'Value')\nq.set(ylabel = 'Count')\nq.set(title = 'Count of each unique value')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6. Data Visualisation and Cleanup\n\n\n**Now I will parse Dates into datetime**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parsing datetime\n# exploring the length of date objects\nlengths = df[\"Date\"].str.len()\nlengths.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As far as we can see from the value above, there are no errors. Now we can parse into datetime\ndf['Date']= pd.to_datetime(df['Date'])\n# Creating a 'Year' column\ndf['year'] = df.Date.dt.year\n\n# Now we will create a function to encode datetime into cyclic parameters.\n# This data will be used in a neural network, therefore having months and days in a cyclic continuous feature will make things easier for us.\n\ndef encode(df, col, max_val):\n    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_val)\n    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_val)\n    return df\n\ndf['month'] = df.Date.dt.month\ndf = encode(df, 'month', 12)\n\ndf['day'] = df.Date.dt.day\ndf = encode(df, 'day', 31)\n\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, I will deal with missing values in categorical and numeric attributes separately."},{"metadata":{},"cell_type":"markdown","source":"**Categorical Variables**\n\nWe're going to fill missing values with the mode of the column value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# View list of categorical variables\ns = (df.dtypes == \"object\")\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Missing values in categorical variables\n\nfor i in object_cols:\n    print(i, df[i].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling missing values with the mode\n\nfor i in object_cols:\n    df[i].fillna(df[i].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Numerical variables**\n\nFilling missing numerical values with the median of the column value"},{"metadata":{"trusted":true},"cell_type":"code","source":"# View list of numerical variables\nt = (df.dtypes == 'float64')\nnum_cols = list(t[t].index)\n\nprint('Numerical Variables:')\nprint(num_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Amount of missing values\n\nfor i in num_cols:\n    print(i, df[i].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling missing values with the median of the column value\n\nfor i in num_cols:\n    df[i].fillna(df[i].median(), inplace=True)\n    \ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Data Preprocessing\n**Steps involved:**\n- Label encoding columns with categorical data\n- Perform the scaling of the features\n- Detecting outliers\n- Dropping the outliers based on data analysis"},{"metadata":{},"cell_type":"markdown","source":"**Label encoding the categorical variable**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor i in object_cols:\n    df[i] = label_encoder.fit_transform(df[i])\n    \ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing attributes of scale data\n# Dropping extra columns\nfeatures = df.drop(['RainTomorrow', 'Date','day', 'month'], axis=1)\n\n# Defining our target columns\n\ntarget = df['RainTomorrow']\n\n# Set up a standard scaler for the features\ncol_names = list(features.columns)\ns_scaler = preprocessing.StandardScaler()\nfeatures = s_scaler.fit_transform(features)\nfeatures = pd.DataFrame(features, columns=col_names)\n\nfeatures.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detecting outliers in the data by looking at the scaled features\n\nplt.figure(figsize=(20,10))\nsns.boxenplot(data = features,palette = 'pastel')\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features['RainTomorrow'] = target\n\n# Dropping outliers\n\nfeatures = features[(features[\"MinTemp\"]<2.3)&(features[\"MinTemp\"]>-2.3)]\nfeatures = features[(features[\"MaxTemp\"]<2.3)&(features[\"MaxTemp\"]>-2)]\nfeatures = features[(features[\"Rainfall\"]<4.5)]\nfeatures = features[(features[\"Evaporation\"]<2.8)]\nfeatures = features[(features[\"Sunshine\"]<2.1)]\nfeatures = features[(features[\"WindGustSpeed\"]<4)&(features[\"WindGustSpeed\"]>-4)]\nfeatures = features[(features[\"WindSpeed9am\"]<4)]\nfeatures = features[(features[\"WindSpeed3pm\"]<2.5)]\nfeatures = features[(features[\"Humidity9am\"]>-3)]\nfeatures = features[(features[\"Humidity3pm\"]>-2.2)]\nfeatures = features[(features[\"Pressure9am\"]< 2)&(features[\"Pressure9am\"]>-2.7)]\nfeatures = features[(features[\"Pressure3pm\"]< 2)&(features[\"Pressure3pm\"]>-2.7)]\nfeatures = features[(features[\"Cloud9am\"]<1.8)]\nfeatures = features[(features[\"Cloud3pm\"]<2)]\nfeatures = features[(features[\"Temp9am\"]<2.3)&(features[\"Temp9am\"]>-2)]\nfeatures = features[(features[\"Temp3pm\"]<2.3)&(features[\"Temp3pm\"]>-2)]\n\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at the scaled features without outliers\n\nplt.figure(figsize = (20,10))\nsns.boxenplot(data = features, palette = 'pastel')\nplt.xticks(rotation = 90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We're now looking at a consistent dataframe which is perfect for building a neural network."},{"metadata":{},"cell_type":"markdown","source":"## Model Building"},{"metadata":{},"cell_type":"markdown","source":"**Method of approach** \n- Assigning X and y the status of 'Attributes' and 'Tags'\n- Splitting test and training sets\n- Initialising the neural network\n- Defining by adding layers to the network\n- Compiling the neural network\n- Training the neural network"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = features.drop(['RainTomorrow'], axis=1)\ny = features[\"RainTomorrow\"]\n\n# Splitting the test and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Early stopping\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, #minimum amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\n# Initialising the NN\nmodel = Sequential()\n\n# Layers\n\nmodel.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = 26))\nmodel.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nopt = Adam(learning_rate=0.00009)\nmodel.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Train the ANN\n\nhistory = model.fit(X_train, y_train, batch_size = 32, epochs = 150, callbacks=[early_stopping], validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\n\nplt.plot(history_df.loc[:, ['loss']], \"#BDE2E2\", label='Training loss')\nplt.plot(history_df.loc[:, ['val_loss']],\"#C2C4E2\", label='Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc=\"best\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_df = pd.DataFrame(history.history)\n\nplt.plot(history_df.loc[:, ['accuracy']], \"#BDE2E2\", label='Training accuracy')\nplt.plot(history_df.loc[:, ['val_accuracy']], \"#C2C4E2\", label='Validation accuracy')\n\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusions"},{"metadata":{},"cell_type":"markdown","source":"**Concluding the model with:**\n- Testing on the test set\n- Evaluating the confusion matrix\n- Evaluating the classification report"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting the test set results\ny_pred = model.predict(X_test)\ny_pred = (y_pred > 0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\ncmap1 = sns.diverging_palette(260,-10,s=50,l=75,n=5, as_cmap=True)\nplt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_test,y_pred)\nsns.heatmap(cf_matrix/np.sum(cf_matrix), cmap = cmap1, annot = True, annot_kws = {'size':15})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}