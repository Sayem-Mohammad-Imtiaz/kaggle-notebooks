{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Classical credit scoring modeling tutorial","metadata":{}},{"cell_type":"markdown","source":"## Fields\n\n- **ID**: ID of each client\n- **LIMIT_BAL:** Amount of given credit in NT dollars (includes individual and family/supplementary credit\n- **SEX:** Gender (1=male, 2=female)\n- **EDUCATION:** (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n- **MARRIAGE:** Marital status (1=married, 2=single, 3=others)\n- **AGE:** Age in years\n- **PAY_0 to PAY_6:** Repayment status at last month until 6 months ago, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, â€¦ 8=payment delay for eight months, 9=payment delay for nine months and above)\n- **BILL_AMT1 to BILL_AMT6:** Amount of bill statement at last month until 6 months ago, 2005 (NT dollar)\n- **PAY_AMT1 to PAY_AMT6:** Amount of previous payment at last month until 6 months ago, 2005 (NT dollar)\n\n- **default.payment.next.month:** Default payment (1=yes, 0=no)","metadata":{}},{"cell_type":"markdown","source":"# Modeling steps\n\n## Reading data:\n * For all types:\n  * Check missing values: null, none, blanket,  9999..99 (are also common), -1 and so on.\n * Categorical variables:\n  * STRING: Clean string variables: trim, normalize cases etc.;\n  * INTEGER: are integer variables continuous ou categoric codes.\n * Date and Time:\n  * They MUST NOT enter in the model. See the feature engineering bellow.\n  \n## Population and database design\n\n* Observation definition:\n * You should define which field is the primary-key / key of you observation (or row of the database). \n * Define your population. Should you considere all the documents/clients/entities or a subset of it is enough?\n  * Many models are influenced by the proportion of a certain class in the training set and tries to predict the same proportion on the testing set.\n  * Example: Are all the people in CRM database the target of your model or only the subset that has already bought someting?\n  * Considere undersampling and upsample cautionly because it would require an extra step to calibrate the probabilities.\n  \n## Feature engineering\n\n* Main tips:\n * All models perform better on variables with linear relationship with the target (not exponential, not unbalanced, not with long tail, and so on).\n * All businesses need to trust your model. Yesterday, to understand the what happened, today, to make decisions, and tomorrow, to forecast risk/revenue/demand/infected etc.\n * The goal in doing feat. engineering is to stabilize the model across time and subsamples.\n\n* For data-time variables:\n * REFDATE: **First** don't mess with the snapshot/reference/month on book/whatever field. ALL DATABASEs must have the data of extration, or the snapshot. This information is part of primary key. They are important for audit, reporting, stability checks, model validation and so on.\n  * The refdate is used to create reports.\n * Transform into timedeltas like years (ages), months (time of relationship, month until brankrupcy).\n * When combined with other variables can produce good attributes like (expenses in the last 3 months).\n \n* For String/categorical Data\n * Do you have many categories ? why don't use pareto's law and use only those relevants and group the rest on the \"others\" category? \n * Do they have similars meaning: \"single\", \"divorced\"?  Why not group then \"single-divord\"? \n * Do they have same odds ratio? group them.\n * Do they have order: low, medium, high ? why don't replace them by the mean of the target on a sample?\n * Keep the number o categories manageable, around 3 to 10.\n * Do you have only 2 categs ? why not create only one binary var: ex. gender (male, female) ==> (isFemale)\n * Pseudo-categorical: ex. education (high school, college, etc) can hide a continous variables, years of education.\n \n \n* For real/floating variables:\n * **Monetary values**: They are prices, income, payments, costs, rents, exchange rate, etc. **IN 99% of cases DON'T USE** them direct in your model. They fragile and oscilate acording to macro-economic movements, like inflation, demand, etc. Create relative variables  like BALANCE_INCOME = BALANCE / BASIC_INCOME_AT_MONTH.\n * Take care with fake precision.  Does 5.009943323 dollars mean something? why not truncated it to 5.01?\n * What is a good range for percentages variables ?\n \n\n* For integer variables:\n * Counting variables usually have very assymetric distribuition. Sometimes grouping them into 1, 2, >=3 leads to good results.\n * **Especial Cases**:\n  * **Age**: age are usually measured in years and are highly corrected with marriage status, education and so on. It's distribuion should be checked against the country's demography distribuition to prevent unwanted biaes. In many case, considere create a categorial variable = 19-23 (college age), 23-27 (first job), 27-32 (senior posions), 60-high (retired) etc.\n  \n \n* Derived attributes:\n * Lagging attributes:\n  * Ex: What was the client last status?  Has the machine failed in the last 10 days (for maintainance prediction)?\n * Ratios, variations and deltas attributes: They usually helps to stabilize the model.\n  * Ex: ratio between the load and the truck strengh ( Load_Stress = boad/strengh). Ratios can be greater than one.\n  * Ex: Balanced_used = (expences/balanced). Proportions should be less than one, specially when they express probabilities.\n  \n \n* External attributes:\n * Other scores - bureau score: should be used as last resource due to its cost and vunerable to providers errors.\n  \n  \n  \nConcepts:\n* **Stable model**: a model the is stable across time and subsamples.\n* **Future data**: an attribute that uses informations only available after the Snapshot, therefore will break your model.","metadata":{}},{"cell_type":"markdown","source":"# Comments\n\n-  Real credit datasets have at least 12 months and not one month because people behave different after christimas, vantines day etc.\n--  The variable YMS will be created to **S**imulate **Y**ear**M**onth\n\n- The lagging variable PAY_0  seems to be PAY_1 like the others variables\n\n","metadata":{}},{"cell_type":"code","source":"!pip install sweetviz","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\n#basic data analysis libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pylab as plt\nimport seaborn as sns\n\nfrom IPython.display  import HTML\nimport sweetviz","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#machine learning libraries\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\n\nimport shap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading data and doing preprocessing","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/default-of-credit-card-clients-dataset/UCI_Credit_Card.csv')\ndf.rename(columns={'default.payment.next.month':'target','PAY_0':'PAY_1'}, inplace=True)\n\n#year month simulation creating 10 groups\ndf['YMS'] = np.ceil(10*np.random.RandomState(seed=42).rand(len(df)))\n# YMS <7  is the training sample and YMS >=7 is the test sample.","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe(percentiles=np.linspace(0,1,11)).T.drop(columns=['count']).style.background_gradient(axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature enginering\n\nLimit_bal  and bill_amt can vary by income, region, time/inflation etc  and these informations are not included in the dataset.  So, the best way of creating stable feature is by calculate relative metrics","metadata":{}},{"cell_type":"code","source":"#binarizing\ndf['SEX'] = df['SEX'] - 1\n\ndf['MARRIED'] = df['MARRIAGE'].apply(lambda x: 1 if x == 1 else 0);\n\n#converting education into years of study\ndf['EDUCATION'] = df['EDUCATION'].map({1:5,2:15, 3:9, 4:0, 5:0, 6:0, 0:0}) #1 = graduate school; 2 = university; 3 = high school; 4 = other\n\n\n# IU = BILL_AMT/LIMIT  =>  percentage of the limit used\n# PER_PAYED = pay / bill\nfor i in range(1,7):\n    df['IU'+str(i)]= df['BILL_AMT'+str(i)] / df['LIMIT_BAL'];\n    df['PER_PAYED'+str(i)]= (df['PAY_AMT'+str(i)] / df['BILL_AMT'+str(i)]).replace([np.inf, -np.inf], np.nan).fillna(0);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['IU'+str(i) for i in range(1,7)]+['YMS']]\\\n    .groupby('YMS').mean().round(2).plot(figsize=(10,3));\nplt.legend(loc='best');","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Are there any trend from 6th to the last month ?  the correlation of the percentages with the vector [1,2,3,4,6] is positive?","metadata":{}},{"cell_type":"code","source":"df['TREND_PER_PAYED'] = 1- pairwise_distances(\n    df[['PER_PAYED'+str(i) for i in range(1,7)]],\n    (6 - np.arange(6)).reshape(1,-1),\n    metric='correlation');\ndf['TREND_PER_PAYED'] = df['TREND_PER_PAYED'].replace([np.inf, -np.inf], np.nan).fillna(0)\n\ndf['TREND_PER_IU'] = 1- pairwise_distances(\n    df[['IU'+str(i) for i in range(1,7)]],\n    (6 - np.arange(6)).reshape(1,-1),\n    metric='correlation')\ndf['TREND_PER_IU'] = df['TREND_PER_IU'].replace([np.inf, -np.inf], np.nan).fillna(0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The **EDA_continuous_v1** function encodes the following design/business informations:\n- the univarate analisys - Stats table and the histogram - where outliers can be seem.\n- Continuous variavels are grouped by percentile with each bucket containing 1% of the data.\n- the bivariate analysis contains the relationship between the mean of the bucket with mean of the target.\n-- desireable attribute has linear relationship / correlation","metadata":{}},{"cell_type":"code","source":"def EDA_continuous_v1(df, var,target='target', datetime='YMS', preprocess=None, hist_bins=100, figsize=(10,5)):\n    display(HTML(\"EDA %s\"%var))\n    display(df[var].to_frame().describe(percentiles=np.linspace(0,1,11)).round(2).T)\n    \n    plt.figure(figsize=figsize);\n    \n    ### plot 1\n    plt.subplot(121);\n    plt.title(\" Histogram - %s\"%var);\n    df[var].hist(bins=hist_bins, density=True);\n    plt.xlabel(var);\n    \n    ### plot 2\n    plt.subplot(122);\n    temp =pd.DataFrame({        \n        'rank': np.ceil(hist_bins*df[var].rank(pct=True)),\n        'rank10': KMeans(n_clusters=5,random_state=42).fit_predict(df[var].values.reshape(-1, 1)),\n        #'rank10': np.ceil(5*df[var].rank(pct=True))*2,\n        var: preprocess(df[var].values) if preprocess is not None else df[var],\n        target:df[target],\n        datetime:df[datetime]\n    })    \n    sns.regplot(x=var,y=target,ax=plt.gca(), data=temp.groupby('rank').mean());\n    \n    plt.title(\"Correlation %0.4f\" % temp.groupby('rank').mean().corr().values[0,1]);\n    plt.tight_layout();\n    plt.show();\n    \n    #plot 3\n    plt.figure(figsize=(figsize[0],3))\n    ax = plt.gca();\n    temp.pivot_table(index=datetime, columns='rank10',values=target).plot(ax=ax);\n    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.10), fancybox=True, shadow=True, ncol=10);\n    ax.set_xlabel(\"YMS\");\n    ax.set_ylabel(\"Mean target\");\n    ax.set_title(\"target per cluster of %s over time\"%var);\n\n    plt.tight_layout();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EDA_continuous_v1(df.query('YMS < 7'),'AGE', preprocess = lambda x:1-np.abs(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TREND_PER_IU** relationship with the target contains a inverted \"v\" shape in the lower valeus what may indicated this variable should be used and categorical (each category representing parts of the curve)","metadata":{}},{"cell_type":"code","source":"EDA_continuous_v1(df.query('YMS < 7'),'TREND_PER_IU')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#applying the transformation seen above that make the relationship more linear\ndf['TREND2_PER_PAYED'] = 1-np.abs(df['TREND_PER_PAYED'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Age var","metadata":{"trusted":true}},{"cell_type":"code","source":"EDA_continuous_v1(df.query('YMS < 7'),'AGE', preprocess=lambda x: abs(x-32)/32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['AGE_NORM'] = np.abs(df['AGE']-32)/32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating a categorical variables from non-linear or unstable variables.","metadata":{}},{"cell_type":"code","source":"def temp(x):\n    if x < -0.8:\n        return 'a';\n    elif x < -0.5:\n        return 'b';\n    elif x < 0.5:\n        return 'c';\n    elif x < 0.8:\n        return 'd';\n    else:\n        return 'x';\n\ndf['TREND2_IU']=df['TREND_PER_IU'].apply(temp)\npd.DataFrame({\n        'rank': df['TREND_PER_IU'].apply(temp),\n        'YMS':df['YMS'],\n        'var': df['TREND_PER_IU'],\n        'target':df['target']\n    }).pivot_table(index='YMS',columns='rank',values='target').round(2).plot(figsize=(15,4));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def income2Range(x):\n    # income are generaly exponential.\n    x = x/1000;\n    \n    return str(int(np.log(x)))\n\n# Assuming the limit gave to the client has to something to do with his income.\n# in the practice the risk + income defines the limit. So in the real world using the limit as input would break your model. \ndf['PROXY_INCOME']=df['LIMIT_BAL'].apply(income2Range)\npd.DataFrame({\n        'rank': df['LIMIT_BAL'].apply(income2Range),\n        'YMS':df['YMS'],\n        'var': df['LIMIT_BAL'],\n        'target':df['target']\n    }).pivot_table(index='YMS',columns='rank',values='target').round(2).plot(figsize=(15,4));\n\ndisplay(HTML(\"We can see that different limits/incomes has different risks\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Auxiliary functions / reports","metadata":{}},{"cell_type":"code","source":"def sort_score(df):\n    \"\"\"\n    if the score was created using kmeans it will not come ordered.  This function reorder the score to match the target.\n    \"\"\"\n    temp = df.groupby('score').apply(lambda x:x['prob'].mean());\n    temp = temp.reset_index().sort_values(0).reset_index(drop=True).reset_index()\n    temp = dict(zip(temp['score'],temp['index']));\n    df['score'] = df['score'].map(temp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_report(df, query_test, target='target',prob='prob'):\n    def model_stats(x):\n        s = pd.Series({\n            'prob':x[prob].mean(),\n            target:x[target].mean(),\n            'count':len(x),\n            'event':x[target].sum(),\n            'non_event': len(x) - x[target].sum()\n        });\n        return s\n\n    temp = df.query(query_test).groupby('score').apply(model_stats).round(2);\n    temp['per_event']     = temp['event'] / temp['event'].sum();\n    temp['non_per_event'] = temp['non_event'] / temp['non_event'].sum();\n    temp['odds']          = temp['per_event']/temp['non_per_event']\n    temp['per_pop_acc']   = 1-temp['count'].cumsum()/temp['count'].sum();\n    temp['per_event_acc'] = 1-temp['event'].cumsum()/temp['event'].sum();\n    temp['lift']          = temp[target] / ( temp['event'].sum()/ temp['count'].sum());\n    \n    display(temp.reset_index().round(2))\n    temp_total = temp.mean();\n    display(temp_total.to_frame().T.round(2))\n\n    plt.figure(figsize=(12,8))\n    plt.subplot(221);\n    temp.plot.scatter(x='prob',y=target,ax=plt.gca());\n    plt.xlabel('Prob');\n    plt.xlabel('Mean target');\n\n    plt.subplot(222);\n    plt.title('Strategy plot')\n    temp[['per_pop_acc','per_event_acc']].plot(ax=plt.gca());\n    plt.grid();\n    #print(temp.reset_index().columns)\n    \n    plt.subplot(223);\n    plt.title('KS %0.2f'% np.abs( temp['per_event'].cumsum() -temp['non_per_event'].cumsum()).max())\n    temp.reset_index().plot.bar(x='prob',y=['per_event','non_per_event'],ax=plt.gca());\n    \n    #ROC\n    fpr, tpr, _ = roc_curve(df[target],df[prob])\n    roc_auc = auc(fpr, tpr)\n    \n    plt.subplot(224);\n    plt.plot(fpr, tpr, color='darkorange',label='ROC curve (area = %0.2f)' % roc_auc);\n    plt.plot([0, 1], [0, 1], color='navy',  linestyle='--');\n    plt.xlim([0.0, 1.0]);\n    plt.ylim([0.0, 1.05]);\n    plt.xlabel('False Positive Rate');\n    plt.ylabel('True Positive Rate');\n    plt.title('ROC curve');\n    plt.legend(loc=\"lower right\");\n    plt.tight_layout();\n    plt.show();\n    \n    \n    # stability \n    plt.figure(figsize=(12,4))\n    plt.subplot(121);\n    df.pivot_table(index='YMS',columns='score',values=target).plot(ax=plt.gca());\n    plt.title(\"Score stability - good scores must not cross each other\");\n    \n    plt.subplot(122);\n    df.pivot_table(index='YMS',columns='score',values=target, aggfunc='count').plot(ax=plt.gca(), kind='bar', stacked=True);\n    plt.title(\"Score stability - good scores must not cross each other\");\n    plt.tight_layout();\n    plt.show();\n    \n#model_report(df,'YMS >= 7 ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.formula.api import logit\n\nvars = [\n    'PROXY_INCOME',\n    'TREND2_IU',\n    'TREND_PER_PAYED' ,\n    'SEX',\n    'MARRIED',\n    'IU1',\n    'AGE_NORM'\n];\n\ndf2 = df.copy().replace([np.inf, -np.inf], np.nan).fillna(0);\n\nlogit_mod = logit('target ~ ' + (' + '.join(vars)), df2.query('YMS < 7'))\nlogit_res = logit_mod.fit(disp=0)\ndisplay(logit_res.summary())\n\ndf['prob'] = logit_res.predict(df2)\n#df['score'] = np.ceil(df['prob'].rank(pct=True)*10)\ndf['score']= KMeans(n_clusters=7,random_state=42).fit_predict(df['prob'].values.reshape(-1, 1))\nsort_score(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_report(df,'YMS >= 7 ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's try some machine learning models\n    \n    \n    \n    ","metadata":{"trusted":true}},{"cell_type":"code","source":"df2 = df.copy().replace([np.inf, -np.inf], np.nan).fillna(0);\n\n# creating the dummies manually\ndummy = pd.get_dummies(df[['PROXY_INCOME','TREND2_IU']],drop_first=True,prefix=['PROXY_INCOME','TREND2_IU'])\ndf2 = pd.concat([\n    df,\n    dummy\n], axis=1)\n\nvars = [\n    'SEX',\n    'MARRIED',\n    'TREND_PER_PAYED' ,\n    'IU1',\n] + dummy.columns.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's try regularized logistic regression","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(C=0.2,random_state=42, max_iter=1000)\\\n        .fit(df2.query('YMS < 7')[vars],df2.query('YMS < 7')['target'])\n#display(logit_res.summary())\n#display(pd.DataFrame({'var':vars, 'importance':clf.feature_importances_}))\n\ndf['prob']  = 1- clf.predict_proba(df2[vars])\n#df['score'] = np.ceil(df['prob'].rank(pct=True)*10)\ndf['score']= KMeans(n_clusters=7,random_state=42).fit_predict(df['prob'].values.reshape(-1, 1))\nsort_score(df)\n\n\ndisplay(HTML(\"<h2>Model with %s </h2>\"% clf.__class__.__name__))\nmodel_report(df,'YMS >= 7 ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's try vanilla neural nets","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.neural_network import MLPClassifier\n\nclf = MLPClassifier(hidden_layer_sizes=[20,10,5], random_state=42)\\\n        .fit(df2.query('YMS < 7')[vars],df2.query('YMS < 7')['target'])\n#display(logit_res.summary())\n#display(pd.DataFrame({'var':vars, 'importance':clf.feature_importances_}))\n\ndf['prob']  = 1- clf.predict_proba(df2[vars])\n#df['score'] = np.ceil(df['prob'].rank(pct=True)*10)\ndf['score']= KMeans(n_clusters=7,random_state=42).fit_predict(df['prob'].values.reshape(-1, 1))\nsort_score(df)\n\n\ndisplay(HTML(\"<h2>Model with %s </h2>\"% clf.__class__.__name__))\nmodel_report(df,'YMS >= 7 ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's try simple decision trees","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\nclf = DecisionTreeClassifier(\n        criterion='gini',\n        min_samples_split=0.05,\n        #max_depth=4,\n        max_leaf_nodes=20,\n        random_state=42)\\\n        .fit(df2.query('YMS < 7')[vars],df2.query('YMS < 7')['target'])\n\ndf['prob']  = 1- clf.predict_proba(df2[vars])\n#df['score'] = np.ceil(df['prob'].rank(pct=True)*10)\ndf['score']= KMeans(n_clusters=7,random_state=42).fit_predict(df['prob'].values.reshape(-1, 1))\nsort_score(df)\n\n\ndisplay(HTML(\"<h2>Model with %s </h2>\"% clf.__class__.__name__))\nmodel_report(df,'YMS >= 7 ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplot_tree(clf,filled=True,proportion=True, feature_names=vars);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import BaggingClassifier\n\nclf = BaggingClassifier(base_estimator=LogisticRegression(), n_estimators=15, random_state=42)\\\n        .fit(df2.query('YMS < 7')[vars],df2.query('YMS < 7')['target'])\n\ndf['prob']  = 1- clf.predict_proba(df2[vars])\n#df['score'] = np.ceil(df['prob'].rank(pct=True)*10)\ndf['score']= KMeans(n_clusters=8,random_state=42).fit_predict(df['prob'].values.reshape(-1, 1))\nsort_score(df)\n\n\ndisplay(HTML(\"<h2>Model with %s </h2>\"% clf.__class__.__name__))\nmodel_report(df,'YMS >= 7 ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Let's try a boosting algoritm","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(min_samples_split=0.05,random_state=42)\\\n        .fit(df2.query('YMS < 7')[vars],df2.query('YMS < 7')['target'])\n\npd.DataFrame({'var':vars, 'importance':clf.feature_importances_})\\\n    .sort_values('importance',ascending=False)\\\n    .set_index('var').plot(kind='bar', title='Feature Importance')\n\ndf['prob']  = 1- clf.predict_proba(df2[vars])\n#df['score'] = np.ceil(df['prob'].rank(pct=True)*10)\ndf['score']= KMeans(n_clusters=7,random_state=42).fit_predict(df['prob'].values.reshape(-1, 1))\nsort_score(df)\n\ndisplay(HTML(\"<h2>Model with %s </h2>\"% clf.__class__.__name__))\nmodel_report(df,'YMS >= 7 ')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model quality check","metadata":{}},{"cell_type":"markdown","source":"Checking if the model has bias by income level","metadata":{}},{"cell_type":"code","source":"df.pivot_table(index='score',columns='PROXY_INCOME',values='BILL_AMT1').round(0).style.background_gradient(axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({\n        'rank': np.ceil(df['score'].rank(pct=True)*10),\n        'mean_age': df['AGE'],\n        'per_female':df['SEX'],\n        'MARRIED': df['MARRIED'],\n    }).groupby('rank').mean().round(2).style.background_gradient(axis=0)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}