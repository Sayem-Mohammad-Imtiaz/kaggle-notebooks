{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch as tr\n\nimdb = pd.read_csv('../input/imdb-review-dataset/imdb_master.csv', encoding='ISO-8859-1')\ndev_df = imdb[(imdb.type == 'train') & (imdb.label != 'unsup')]\n\ntest_df = imdb[(imdb.type == 'test')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\ntrain_df, val_df = model_selection.train_test_split(dev_df, test_size=0.1, stratify=dev_df.label)\n\nimport re\nfrom nltk.corpus import stopwords \n    \ndef word_extraction(sentence):   \n    ignore = stopwords.words('english')\n    words = re.sub(\"[^\\w]\", \" \", sentence).split()    \n    cleaned_text = [w.lower() for w in words if w not in ignore and not w.isdigit()]   \n    return cleaned_text\n\ndef build_vocab(sentences):    \n    for i, sentence in enumerate(sentences):\n        words = word_extraction(sentence)\n        for w in words:\n            if w in vocab:\n                vocab[w] = vocab[w]+1\n            else:\n                vocab[w] = 1\n            \n    return vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_max_seq_len(train_data, val_data, test_data):\n    maxl = 0\n    for sentence in train_data:  \n        words = word_extraction(sentence)\n        if maxl<len(words):\n            maxl = len(words)\n            \n    for sentence in val_data:  \n        words = word_extraction(sentence)\n        if maxl<len(words):\n            maxl = len(words)\n            \n    for sentence in test_data:  \n        words = word_extraction(sentence)\n        if maxl<len(words):\n            maxl = len(words)\n    \n    return maxl\n\n\ndef generate_bow(t_vocab, allsentences):\n    vocab_vector = list()\n    size = len(allsentences)\n    for cur_i, sentence in enumerate(allsentences):  \n        if cur_i % 2000==0:\n            print(cur_i,\"/\",size)\n        cur_i += 1\n        words = word_extraction(sentence)\n        #print(words)\n        bag_vector = np.full((2000), 0)\n        for i, word in enumerate(words):\n            try:\n                v_i = vocab.index(word)\n            except:\n                v_i = 0\n            #print(word,\"-\",int(v_i), \"-\", i)\n            bag_vector[i] = v_i\n        #print(bag_vector.tolist())\n        vocab_vector.append(bag_vector)\n    return vocab_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab = {\".\":1}\nvocab_dic = build_vocab(train_df[\"review\"])\nprint(\"Vocabulary with words count length \", len(vocab_dic.keys()))\n#print(vocab_dic)\n\ndelete = [key for key, value in vocab_dic.items() if value < 80 and key!='.'] \nfor key in delete: del vocab_dic[key] \n\nvocab = [key for key in vocab_dic.keys()]\nprint(\"Vocabulary length \", len(vocab))\n#print(vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train_data\")\ntrain_data = generate_bow(vocab, train_df[\"review\"])\nprint(\"val_data\")\nval_data = generate_bow(vocab, val_df[\"review\"])\nprint(\"test_data\")\ntest_data = generate_bow(vocab, test_df[\"review\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset,DataLoader\nfrom torch.utils import data\n\ndef build_features(token_ids, label, max_seq_len, pad_index, label_encoding):\n    if len(token_ids) >= max_seq_len:\n        ids = token_ids[:max_seq_len]\n    else:\n        ids = token_ids + [0 for _ in range(max_seq_len - len(token_ids))]\n    return [ids, [i for i,el in enumerate(label_encoding) if el == label][0]]\n\ndef features_to_tensor(list_of_features):\n    text_tensor = tr.tensor([example[0] for example in list_of_features], dtype=tr.long)\n    labels_tensor = tr.tensor([example[1] for example in list_of_features], dtype=tr.long)\n    return text_tensor, labels_tensor\n    \nclasses = ['neg', 'pos']\n\nmax_seq_len = get_max_seq_len(train_df[\"review\"], val_df[\"review\"], test_df[\"review\"])\n\ntrain_features = [build_features(review.tolist(), label, max_seq_len, 0, classes) for review, label in zip(train_data, train_df['label'])]\nval_features = [build_features(review.tolist(), label, max_seq_len, 0, classes) for review, label in zip(val_data, val_df['label'])]\ntest_features = [build_features(review.tolist(), label, max_seq_len, 0, classes) for review, label in zip(test_data, test_df['label'])]\n\ntrain_tensor, train_labels = features_to_tensor(train_features)\nval_tensor, val_labels = features_to_tensor(val_features)\ntest_tensor, test_labels = features_to_tensor(test_features)\n\nbatch_size = 128\ntrain_loader = DataLoader(TensorDataset(train_tensor, train_labels), batch_size = batch_size)\nval_loader = DataLoader(TensorDataset(val_tensor, val_labels), batch_size = batch_size)\ntest_loader = DataLoader(TensorDataset(test_tensor, test_labels), batch_size = batch_size, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\ndf = pd.read_csv('../input/imdb-review-dataset/imdb_master.csv',encoding=\"latin-1\")\ndf = df.drop(['Unnamed: 0','file'],axis=1)\ndf.columns = ['type',\"review\",\"sentiment\"]\ndf.head()\nprint(df.head())\ndf = df[df.sentiment != 'unsup']\ndf['sentiment'] = df['sentiment'].map({'pos': 1, 'neg': 0})\n\ndf_train = df[df.type == 'train']\ndf_test = df[df.type == 'test']\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef ngram_vectorize(train_texts, train_labels, val_texts):\n    kwargs = {\n        'ngram_range' : (1, 2),\n        'dtype' : 'int32',\n        'strip_accents' : 'unicode',\n        'decode_error' : 'replace',\n        'analyzer' : 'word',\n        'min_df' : 2,\n    }\n    \n    tfidf_vectorizer = TfidfVectorizer(**kwargs)\n    x_train = tfidf_vectorizer.fit_transform(train_texts)\n    x_val = tfidf_vectorizer.transform(val_texts)\n    \n    selector = SelectKBest(f_classif, k=min(6000, x_train.shape[1]))\n    selector.fit(x_train, train_labels)\n    x_train = selector.transform(x_train).astype('float32')\n    x_val = selector.transform(x_val).astype('float32')\n    return x_train, x_val\n\ndf_bag_train, df_bag_test = ngram_vectorize(df_test['review'], df_test['sentiment'], df_train['review'])\ndf_bag_train, df_bag_test = ngram_vectorize(df_test['review'], df_test['sentiment'], df_train['review'])\nfrom sklearn import metrics\n\nnb = MultinomialNB()\nnb.fit(df_bag_train, df_train['sentiment'])\nnb_pred = nb.predict(df_bag_test)\nprint(metrics.classification_report(df_test['sentiment'], nb_pred))\ncm = metrics.confusion_matrix(nb_pred, df_test['sentiment'])\nprint('Accuracy ',metrics.accuracy_score(df_test['sentiment'], nb_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\nfrom sklearn import metrics\n\nclass TextClassifier(nn.Module):\n    def __init__(self, vocab_size, emb_size, num_filters, num_classes):\n        super(TextClassifier, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.dropout_big = tr.nn.Dropout(p = 0.4)\n        self.dropout_small = tr.nn.Dropout(p = 0.05)\n\n        self.convs = nn.ModuleList([\n            nn.Conv1d(1, num_filters, [3, emb_size], padding = (2, 0)),\n            nn.Conv1d(1, num_filters, [4, emb_size], padding = (3, 0)),\n            nn.Conv1d(1, num_filters, [5, emb_size], padding = (4, 0)),\n        ])\n\n        self.fc = nn.Linear(num_filters * 3, num_classes)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.dropout_big(x)\n\n        x = tr.unsqueeze(x, 1)\n        xs = []\n        for i in range(len(self.convs)):\n            conv = self.convs[i]\n            \n            x2 = F.relu(conv(x))\n            x2 = tr.squeeze(x2, -1)\n            x2 = F.max_pool1d(x2, x2.size(2))\n            \n            if i == 1:\n                x2 = self.dropout_small(x2)\n            \n            xs.append(x2)\n        x = tr.cat(xs, 2)\n\n        x = x.view(x.size(0), -1)\n        logits = self.fc(x)\n        \n        return logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(conv, loader, val_loader, epochs):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(conv.parameters(), lr = 0.001)\n\n    for epoch in range(epochs):\n        r_loss = 0.0\n        for i, data in enumerate(loader, 0):\n            inputs, labels = data\n\n            optimizer.zero_grad()\n\n            outputs = conv(inputs.cuda())\n            loss = criterion(outputs, labels.cuda())\n            loss.backward()\n            optimizer.step()\n\n            r_loss += loss.item()\n            \n            every_n = 100\n            if i % every_n == every_n - 1:\n                validation_loss = get_validation_loss(conv, val_loader)\n                print('Epoch %d: loss: %.3f, validation loss: %.3f' %\n                      (epoch + 1, r_loss / every_n, validation_loss))\n                r_loss = 0.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_validation_loss(conv, val_loader):\n    criterion = nn.CrossEntropyLoss()\n    \n    validation_loss = 0.0\n    for i, data in enumerate(val_loader, 0):\n        inputs, labels = data\n\n        outputs = conv(inputs.cuda())\n        loss = criterion(outputs, labels.cuda())\n\n        validation_loss += loss.item()\n    \n    return validation_loss / len(val_loader)\n\ndef test_predictions(conv, data_loader):\n    y_true = []\n    y_pred = []\n    \n    for data in data_loader:\n        d, labels = data\n        d = d.cuda()\n        labels = labels.cuda()\n        \n        outputs = conv(d)\n        _, predicted = tr.max(outputs.data, 1)\n        y_true += labels.tolist()\n        y_pred += predicted.tolist()\n    \n    return [y_true, y_pred]\n\n\ndef get_metrics(conv, test_loader, classes):\n    y_true, y_pred = test_predictions(conv, test_loader)\n    \n    classification = metrics.classification_report(y_true, y_pred, target_names = classes)\n    \n    y_true = [classes[a] for a in y_true]\n    y_pred = [classes[a] for a in y_pred]\n    \n    confusion = metrics.confusion_matrix(y_true, y_pred, labels = classes)\n    return [classification, confusion]\n\ndef validate(conv, test_loader):\n    y_true, y_pred = test_predictions(conv, test_loader)\n    \n    correct = (np.array(y_true) == np.array(y_pred)).sum()\n    total = len(y_true)\n    validation = correct / total\n    \n    return validation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(conv, test_loader):\n    dataiter = iter(test_loader)\n\n    for i in range(10):\n        review, labels = next(dataiter)\n        outputs = conv(review.cuda())\n        _, predicteds = tr.max(outputs, 1)\n        predicted = classes[predicteds[0]]\n        groundtruth = classes[labels[0]]\n        \n        words = []\n        for x in review[0]:\n            if vocab[x]!=\".\":\n                words.append(vocab[x])\n            \n        print()\n        print(words)\n        print(\"Prediction: {}\".format(predicted))\n        print(\"Real: {}\".format(groundtruth))\n        \n        \nemb_size = 100\nnum_filters = 500\nnum_classes = 2\n\nlen_vocab = len(vocab)\ntc = TextClassifier(len_vocab, emb_size, num_filters, num_classes)\ntc.cuda()\n\nprint('Train')\ntrain(tc, train_loader, val_loader, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification, confusion = get_metrics(tc, test_loader, classes)\n\nprint()\nprint('Classification report:')\nprint(classification)\n\nprint()\nprint('Confusion report:')\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print()\nprint(\"Test\")\ntest(tc, test_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tc_validation = validate(tc, val_loader)\nprint('Validation score')\nprint(tc_validation)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}