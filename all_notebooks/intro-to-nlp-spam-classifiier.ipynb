{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I wrote this work during the training, and I will be glad if it turns out to be useful to someone :)    \n\nDataset was loaded from here:\nhttps://www.kaggle.com/wanderfj/enron-spam","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nfrom sklearn.datasets import load_files\n\nimport time\n\n# Text cleaning and precprcessing\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-15T14:19:05.001071Z","iopub.execute_input":"2021-06-15T14:19:05.001565Z","iopub.status.idle":"2021-06-15T14:19:05.008186Z","shell.execute_reply.started":"2021-06-15T14:19:05.001527Z","shell.execute_reply":"2021-06-15T14:19:05.006809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This dataset have 7 emails, but reading, cleaning and preprocessing all of them take to much time. Using one of them is more than enough.","metadata":{}},{"cell_type":"code","source":"X, y = [], []\nemail = load_files(\"../input/enron-spam/enron1\")\nX = np.append(X, email.data)\ny = np.append(y, email.target)    ","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:19:05.009812Z","iopub.execute_input":"2021-06-15T14:19:05.010114Z","iopub.status.idle":"2021-06-15T14:19:13.09728Z","shell.execute_reply.started":"2021-06-15T14:19:05.010085Z","shell.execute_reply":"2021-06-15T14:19:13.09633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's create Dataframe with text and target feature","metadata":{}},{"cell_type":"code","source":"df_all = pd.DataFrame(columns=['text', 'target'])\ndf_all['text'] = [x for x in X]\ndf_all['target'] = [t for t in y]","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:19:13.098906Z","iopub.execute_input":"2021-06-15T14:19:13.099194Z","iopub.status.idle":"2021-06-15T14:19:13.213571Z","shell.execute_reply.started":"2021-06-15T14:19:13.099168Z","shell.execute_reply":"2021-06-15T14:19:13.212787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:19:13.214819Z","iopub.execute_input":"2021-06-15T14:19:13.21521Z","iopub.status.idle":"2021-06-15T14:19:13.229454Z","shell.execute_reply.started":"2021-06-15T14:19:13.215183Z","shell.execute_reply":"2021-06-15T14:19:13.228196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_X = df_all.drop(['target'], axis=1)\ndf_y = df_all['target']","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:19:13.23129Z","iopub.execute_input":"2021-06-15T14:19:13.231717Z","iopub.status.idle":"2021-06-15T14:19:13.244032Z","shell.execute_reply.started":"2021-06-15T14:19:13.231672Z","shell.execute_reply":"2021-06-15T14:19:13.242898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:19:13.245832Z","iopub.execute_input":"2021-06-15T14:19:13.246369Z","iopub.status.idle":"2021-06-15T14:19:13.257642Z","shell.execute_reply.started":"2021-06-15T14:19:13.246324Z","shell.execute_reply":"2021-06-15T14:19:13.256456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have list of texts, that encoded binary. Using 'decode' is one of possible solutions, but some texts don't allow us to apply decoding. This can be solved by deleting these texts, but because of this, we can lose important information. Instead of this we can do following:\n\n1. We should remove all special symbols.\n1. Remove 'b' in beginning of each text\n1. Replace all gaps (\\t, \\n, \\r, \\f) between words with spaces\n1. Remove all non-letters characters","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n\n# Create corpus\ncorpus = []\nfor i in range(0, len(df_X)):\n    # Remove special symbols\n    review = re.sub(r'\\\\r\\\\n', ' ', str(df_X['text'][i]))\n    # Remove all symbols except letters\n    review = re.sub('[^a-zA-Z]', ' ', review)\n    # Replacing all gaps with spaces \n    review = re.sub(r'\\s+', ' ', review)                    \n    # Remove 'b' in the beginning of each text\n    review = re.sub(r'^b\\s+', '', review)       \n\n    review = review.lower()\n    review = review.split()\n    review = [stemmer.stem(word) for word in review if word not in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:19:13.258926Z","iopub.execute_input":"2021-06-15T14:19:13.2594Z","iopub.status.idle":"2021-06-15T14:21:03.290085Z","shell.execute_reply.started":"2021-06-15T14:19:13.259355Z","shell.execute_reply":"2021-06-15T14:21:03.289047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n#tf = TfidfVectorizer()\n\n# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX = cv.fit_transform(corpus).toarray()","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:21:03.292154Z","iopub.execute_input":"2021-06-15T14:21:03.292492Z","iopub.status.idle":"2021-06-15T14:21:04.368132Z","shell.execute_reply.started":"2021-06-15T14:21:03.292459Z","shell.execute_reply":"2021-06-15T14:21:04.367148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting data on train and test dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,  random_state=9, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:21:04.36948Z","iopub.execute_input":"2021-06-15T14:21:04.369779Z","iopub.status.idle":"2021-06-15T14:21:05.187768Z","shell.execute_reply.started":"2021-06-15T14:21:04.369749Z","shell.execute_reply":"2021-06-15T14:21:05.186756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The likelihood of whether an email is spam or ham is a aposterior probability. So let's try bayes models. They usually shows high performance in spam detection","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix\n\nmodel = MultinomialNB().fit(X_train, y_train)\npred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, pred)\nprecision = precision_score(y_test, pred)\nrecall = recall_score(y_test, pred)\nconf_m = confusion_matrix(y_test, pred)\n\nprint(f\"accuracy: %.3f\" %accuracy)\nprint(f\"precision: %.3f\" %precision)\nprint(f\"recall: %.3f\" %recall)\nprint(f\"confusion matrix: \")\nprint(conf_m)\nprint(\"--- %s seconds ---\" % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2021-06-15T14:21:05.189079Z","iopub.execute_input":"2021-06-15T14:21:05.189406Z","iopub.status.idle":"2021-06-15T14:21:08.264831Z","shell.execute_reply.started":"2021-06-15T14:21:05.189369Z","shell.execute_reply":"2021-06-15T14:21:08.263659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With stemming and using Bag of words I became this results:\n* accuracy: 0.978\n* precision: 0.961\n* recall: 0.964\n* It took 116.1 sec","metadata":{}},{"cell_type":"markdown","source":"**Let's try different combinations of models (Bag of Words or Tf-Idf) with different preprocessing techniques (Stemming or Lemmatization)**","metadata":{}},{"cell_type":"markdown","source":"With lemmatizer and using Bag of words:\n* accuracy: 0.979\n* precision: 0.970\n* recall: 0.957\n* It took 102.9 sec","metadata":{}},{"cell_type":"markdown","source":"With stemming and using Tf-Idf:\n* accuracy: 0.905\n* precision: 0.995\n* recall: 0.680\n* It took 116.6 sec","metadata":{}},{"cell_type":"markdown","source":"With lemmatizer and using Tf-Idf:\n* accuracy: 0.907\n* precision: 0.995\n* recall: 0.686\n* It took 116.6 sec","metadata":{}},{"cell_type":"markdown","source":"As seen, Tf-Idf gives us very high preccision, but recall is bad. On the other hand Bag of words demonstrate high results in both cases. I think, that lemmatizer+Bag-of-words is the better solution. It took least of all time, what will become even more noticeable if you increase the size of dataset. It shows the highest accuracy compared to other solutions. Precision and recall are high too.","metadata":{}}]}