{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Introduction**\nMachine learning is a branch of AI that learns from data to make future predictions. This project will look at how we can apply machine learning on titanic data which results in a model that predicts survival rates based on certain features. This is a follow up on my titanic data analysis notebook and will use findings from there."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nimport keras\nfrom keras.optimizers import SGD\nimport graphviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the data\n\ntrain_data = pd.read_csv('../input/titanic-machine-learning-from-disaster/train.csv')\ntest_data = pd.read_csv('../input/titanic-machine-learning-from-disaster/test.csv')\n\nprint(train_data.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Preprocessing data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing duplicates\n\ntrain_data = train_data.drop_duplicates()\ntest_data = test_data.drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for missing values in the data\n\ntrain_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Filling missing values for features 'Fare' and 'Age' using imputation\n\ntrain_data['Age'].fillna(train_data['Age'].mean(), inplace = True)\ntest_data['Age'].fillna(test_data['Age'].mean(), inplace = True)\n\ntest_data['Fare'].fillna(test_data['Fare'].mean(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating a new feature \"Relatives\" by combining the features 'Sibsp' and 'Parch'\n\ntrain_data['Relatives'] = train_data['SibSp'] + train_data['Parch']\ntest_data['Relatives'] = test_data['SibSp'] + test_data['Parch']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Following my previous data analysis, I chose the following features to train the model.Since Pclass and Fare depict similar data regarding their socio-economic status I chose to keep Pclass.\n\ninput_features = ['Pclass', 'Sex', 'Age', 'Embarked', 'Relatives']\nX_train = train_data[input_features]\nX_test = test_data[input_features]\nY_train = train_data['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Converting categorical variables into dummy/indicator variables.\n\nX_train = pd.get_dummies(X_train).astype(np.float64, copy=False)\nX_test = pd.get_dummies(X_test).astype(np.float64, copy=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalizing data \n\nfrom sklearn import preprocessing\nscale = preprocessing.MinMaxScaler()\n\nX_train = scale.fit_transform(X_train)\nX_train = pd.DataFrame(X_train)\n\nX_test = scale.fit_transform(X_test)\nX_test = pd.DataFrame(X_test)\n\nX_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # **Model compilation,visualization and training**\n We have created a neural Network that has three layers. There are two layers with 16 nodes and the last one with one. The output layer uses the sigmoid activation function.One output unit is used since for each record values in X, a probability will be predicted. For binary classification problems that give output in the form of probability, binary_crossentropy is usually the optimizer of choice.Then we use the model to predict y values based on learnings by feeding in the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential([\n    keras.layers.Dense(16, activation= 'relu'),\n\tkeras.layers.Dense(16, activation= 'relu'),\n    keras.layers.Dense(1, activation='sigmoid')])\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n\nmodel.fit(X_train, Y_train, epochs=50, batch_size=20)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model.predict(X_test)\nY_final = (Y_pred > 0.5).astype(int).reshape(X_test.shape[0])\nprint(Y_final)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # **Model evaluation**\nThe accuracy of results obtained above can be improved by implementing various techniques such as the following:\n\n* Increasing number of epochs - This helps tune the model parameters better, thereby minimizing loss and increasing accuracy\n* Increasing the number of nodes in each layer\n* Adding dropout - the model chooses which neurons to shut off based on the probability specified which triggers other neurons to work actively and therfore makes sure weights across different parts of the network are consistent\n* Validation - Part of the train set data is chosen to be the validation data set. The model is trained using the remaining training data and then run on the validation data to check how accurate the model outputs. This helps reduce overfitting in train set data too.\n* Reducing the batch size"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1 = keras.Sequential([\n    keras.layers.Dense(60,activation= 'relu'),(Dropout(0.5)),\n    keras.layers.Dense(55,activation= 'relu'),(Dropout(0.5)),\n    keras.layers.Dense(50,activation= 'relu'),(Dropout(0.5)),\n\tkeras.layers.Dense(30,activation='relu'),(Dropout(0.5)),\n    keras.layers.Dense(25,activation= 'relu'),( Dropout(0.5)),\n    keras.layers.Dense(1, activation= 'sigmoid')\n])\n\nmodel_1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n\nmodel_1.fit(X_train, Y_train, epochs=120, batch_size=32, validation_split=0.1)\nmodel_1.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally the model's performance can be evaluated according to how accurate results are and as per the results obtained above, accuracy has increased by approximately 3%."},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model_1.predict(X_test)\nY_final = (Y_pred > 0.5).astype(int).reshape(X_test.shape[0])\nprint(Y_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_2 = keras.Sequential([\n    keras.layers.Dense(40,activation= 'relu'),(Dropout(0.4)),\n    keras.layers.Dense(35,activation= 'relu'),(Dropout(0.4)),\n    keras.layers.Dense(30,activation= 'relu'),(Dropout(0.4)),\n    keras.layers.Dense(25,activation= 'relu'),(Dropout(0.4)),\n    keras.layers.Dense(20,activation= 'relu'),(Dropout(0.4)),\n\tkeras.layers.Dense(15,activation='relu'),(Dropout(0.4)),\n    keras.layers.Dense(15,activation='relu'),(Dropout(0.4)),\n    keras.layers.Dense(10,activation= 'relu'),( Dropout(0.4)),\n    keras.layers.Dense(10,activation='relu'),(Dropout(0.4)),\n    keras.layers.Dense(5,activation= 'relu'),(Dropout(0.4)),\n    keras.layers.Dense(1, activation= 'sigmoid')\n])\n\nmodel_2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n\nmodel_2.fit(X_train, Y_train, epochs=100, batch_size=32, validation_split=0.2)\nmodel_2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred = model_2.predict(X_test)\nY_final = (Y_pred > 0.5).astype(int).reshape(X_test.shape[0])\nprint(Y_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data=pd.read_csv('../input/titanic-machine-learning-from-disaster/test.csv')\nprint(len(test_data['PassengerId'].tolist()))\nresult_df=pd.DataFrame()\nresult_df['PassengerId']=test_data['PassengerId'].tolist()\nresult_df['Survived']= Y_final\nprint(result_df)\n\nresult_df.to_csv('/kaggle/working/submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}