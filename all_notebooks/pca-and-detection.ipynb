{"cells":[{"metadata":{"_cell_guid":"5840cefa-b618-427d-969c-0a3d9b72fd4f","_uuid":"9b7518bfbbb23162d813783719a1cb7cc219d980"},"cell_type":"markdown","source":"This dataset consists of 1558 features. Before starting with ads detection, can we reduce the dimensionality of the dataset without lose informations?\nIn this kernel I try to reduce the dimensionality of the dataset by applying PCA, and I'll use the algorithm provided by sklearn for this task."},{"metadata":{"_cell_guid":"722b5216-a6bf-4f7e-9c9b-e1722b37b28c","_uuid":"9a606a57a50f99dfe2d3757f52e1e9e42fa6a5a0","trusted":false,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import subplots, show\n\ndf = pd.read_csv('../input/add.csv',low_memory=False)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"19c688b8-f191-4ae8-960b-0d699b8fbad5","_uuid":"c6b40599e567408c16fe0a6145bb0f99a900e941"},"cell_type":"markdown","source":"## Pre processing \n\nPCA requires non-null numbers."},{"metadata":{"_cell_guid":"0e734b4a-9475-4558-82b2-d61db165aede","collapsed":true,"_uuid":"e3ba33ee94a3864f1668c89f67d33706bd47374e","trusted":false},"cell_type":"code","source":"df = df.applymap(lambda val: np.nan if str(val).strip() == '?' else val)\ndf = df.dropna()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b08bc7f7-37e6-4a23-92d6-a6da92a3fd25","_uuid":"16c3e351a426bcfa79760744a044c17d9ed90067"},"cell_type":"markdown","source":"And don't forget to standardize it ;)"},{"metadata":{"_cell_guid":"dc86e4d7-69e4-467a-b487-ed6fc48973a3","_uuid":"5d5f3413911248f2e857ed1ae585d5305e691372","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Define a standard scaler\nsc = StandardScaler()\n\n# Remove the first column, it's useless\ndata = df.iloc[:,1:].reset_index(drop=True)\n\n# Factorization \ndata.loc[data['1558'] == 'ad.', '1558'] = 1\ndata.loc[data['1558'] == 'nonad.', '1558'] = 0\n\n# Scale features and extract targets\nx = data.iloc[:,:-1]\nx = pd.DataFrame(sc.fit_transform(x), index=x.index, columns=x.columns)\ny = data.iloc[:, -1]\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7995c5e1-9398-4961-9052-84c1661a8d75","_uuid":"b21b1cc7f859b605a2cc3b42c75812c6693017b6"},"cell_type":"markdown","source":"Now I should be ready to perform PCA."},{"metadata":{"_cell_guid":"a4675db1-c7e3-475b-84f1-539f780977bc","_uuid":"55e40b1121709bdcc71ecd02c250f89c94b87281","trusted":false,"collapsed":true},"cell_type":"code","source":"n_components = 250;\npca = PCA(n_components=n_components)\npca.fit(x)\nPCA(copy=True, iterated_power='auto', n_components=n_components, random_state=None, svd_solver='auto', tol=0.0, whiten=False)\n\n# variance explained \nfig, ax = subplots()\nplt.plot( pca.explained_variance_ratio_*100)\nax.set_xlabel(\"#Component\")\nax.set_ylabel(\"Explained variance ratio\")\nshow()\n\n# cumulative variance explained\nfig, ax = subplots()\nplt.plot( pca.explained_variance_ratio_.cumsum()*100)\nax.set_xlabel(\"#Component\")\nax.set_ylabel(\"Cumulative explained variance ratio\")\n\nshow()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c950dacd-a7df-4760-a001-5dcea630bf92","_uuid":"a58142c58c6f7052c7f8d33ef80fa455ec661ef0"},"cell_type":"markdown","source":"And finally a score plot of the first two components. "},{"metadata":{"_cell_guid":"39bf5bdd-1839-436c-8a32-d7735596379f","_uuid":"d7f83b42cae20051d464f12544c066f42a0a0e6a","trusted":false,"collapsed":true},"cell_type":"code","source":"idx_ad = y[y==1].index # ad indexes\nidx_nonad = y[y==0].index # non ad indexes\n\nxs_ad = pca.transform(x)[idx_ad,0] # scores 1st component (ads)\nys_ad = pca.transform(x)[idx_ad,1] # scores 2nd component (ads)\nxs_nad = pca.transform(x)[idx_nonad,0] # scores 1st component (non ads)\nys_nad = pca.transform(x)[idx_nonad,1] # scores 2nd component (non ads)\n\nd = pd.DataFrame({'x':[],'y':[],'type':[]})\nd=d.append(pd.DataFrame({'x':xs_ad, 'y':ys_ad, 'type' : 'ad.'}))\nd=d.append(pd.DataFrame({'x':xs_nad, 'y':ys_nad, 'type' : 'nonad.'}))\nd = d.reset_index(drop=True);\n\n# scatterplot \ng = sns.lmplot('x', 'y', data=d, hue='type', fit_reg=False)\ng.set(xlabel='1st component', ylabel='2st component')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4d92c7cc-656b-42fc-8379-e1dd9994913d","_uuid":"a23c1d44ecf6d7ea7a4233eee447217bae6f1d0f"},"cell_type":"markdown","source":"With 100 components we explain the 80% of the variance. It's still a huge number, but a good improvement :-) "},{"metadata":{"_cell_guid":"c2048519-5ea9-4930-94e7-67508162f327","_uuid":"0eaf858ba9e671a894a52a52ba576c4b4a87f77d"},"cell_type":"markdown","source":"# Detection"},{"metadata":{"_cell_guid":"1d180d03-c777-481a-8e11-a44e4149ec02","_uuid":"e1a1778de98e75359c43d3448bf256e3afca0445"},"cell_type":"markdown","source":"TODO : here I used all the features, next time I'll try what happens if I use only 100-150 of the components provided by PCA."},{"metadata":{"_cell_guid":"c5c3150d-30ee-4d6c-b4d9-413c2acfeaf5","_uuid":"e0c76916696fe235b61c74b76ce976265a0b1d86","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\n\n# Split predictors and targets\ny = df.iloc[:, -1]\nx = df.iloc[:,1:-1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a805b390-f10d-47c5-b8e0-eed1bd565431","_uuid":"da76b40e9ad16debd06624fa6f54c67813bd25c5"},"cell_type":"markdown","source":"## Logistic regression"},{"metadata":{"_cell_guid":"3b99c6a5-c062-49ff-b86a-834dfa13ac4c","_uuid":"72ba48ec5b99c46e0f07e1b7bba13a391a7ddc64"},"cell_type":"markdown","source":"### 5-fold Cross Validation"},{"metadata":{"_cell_guid":"28337a91-5c74-4954-a188-4f274f1330fc","_uuid":"4fc6457e3211c0239a5179510faad034ce936c12","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn import linear_model\n\npres_rec = pd.DataFrame( columns=['precision','recall','c','out']);\nacc = pd.DataFrame( columns=['accuracy','c']);\nkf = KFold(n_splits=5, random_state=True, shuffle=True)\n\n\nC = [0.1,1,100,300,500,700];\nfor c in C:\n    for train_index, test_index in kf.split(x):\n\n        xtrain, xtest = x.iloc[train_index], x.iloc[test_index]\n        ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n        logreg = linear_model.LogisticRegression(C=c)\n        logreg.fit(xtrain, ytrain)\n        predicted = logreg.predict(xtest)\n        cm = confusion_matrix(ytest,predicted)\n\n        # precision\n        ad_precision = cm[0][0] / ( cm[0][0] + cm[1][0] )\n        nonad_precision = cm[1][1] / ( cm[0][1] + cm[1][1] )\n\n        #recall\n        ad_recall = cm[0][0] / ( cm[0][0] + cm[0][1] )\n        nonad_recall = cm[1][1] / ( cm[1][0] + cm[1][1] )\n        \n        #accuracy\n        accuracy = (cm[0][0] + cm[1][1]) / ( cm[0][0] + cm[1][1] + cm[0][1] + cm[1][0]);\n\n        pres_rec = pres_rec.append([{'precision': ad_precision, 'recall' : ad_recall, 'c' : c , 'out' : 'ad'}]);\n        pres_rec = pres_rec.append([{'precision': nonad_precision, 'recall' : nonad_recall, 'c' : c , 'out' : 'nonad'}]);\n        acc = acc.append([{'accuracy' : accuracy, 'c' : c}])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"baa35da2-c4f7-4c48-bee8-30f51b0450ce","_uuid":"9dd35886e6bc60d12390d1b9f40918515a23dafe"},"cell_type":"markdown","source":"Here I try to create a model exploiting logistic regression: I run it with several times in order to tune C, the penalty parameter of the error term.\n\nThe following graph shows the accuracy, that is, the percentage of well predicted images. But it is not enough to understand the efficiency of the model, I plot also sensitivity and specificity: the first one represent the percentage of well classified non-ad, the second one represent the percentage of well classified ad."},{"metadata":{"_cell_guid":"92125844-aa4d-482a-a0d7-cbad2d411a7e","_uuid":"115db5695c604a082cb2f3b0d1476f3446769730","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharey=True, figsize=(11,5))\n\ng = sns.factorplot(x=\"c\", y=\"accuracy\", data=acc,capsize=.2, ax = ax1)\nplt.close(g.fig)\n\ng = sns.factorplot(x=\"c\", y=\"precision\", hue=\"out\", data=pres_rec,capsize=.2, palette=\"YlGnBu_d\", ax = ax2)\nplt.close(g.fig)\n\ng = sns.factorplot(x=\"c\", y=\"recall\", hue=\"out\", data=pres_rec,capsize=.2, palette=\"YlGnBu_d\", ax = ax3)\nplt.close(g.fig)\n\nfig.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"93a69a31-c878-4639-a22f-4350d4ed23f5","_uuid":"85b5997f8aec435e50d90aa7a3bf14663f710d3a"},"cell_type":"markdown","source":"This model is pretty good, all three measures are very high. Since we're more interested in ads, C=1 seems to be a good compromise between accuracy and recall.\n\nBut can we do better? Let's try with SVM."},{"metadata":{"_cell_guid":"63c1fc63-d5c9-42ee-a325-5dd9f6862b10","_uuid":"328add826619723bb8b97d976a62996cfe694f48"},"cell_type":"markdown","source":"## SVM"},{"metadata":{"_cell_guid":"511a4260-8682-4f60-8f58-8efdc9e19597","_uuid":"39724e6bb0f467de9083ba490268b3c1ff2f6bd0"},"cell_type":"markdown","source":"### 3-fold Cross Validation"},{"metadata":{"_cell_guid":"e0fed6b2-c568-4f0d-8773-90e70f454eba","_uuid":"fd04b79d1ef4a9beae2ab0a24942497daa864683"},"cell_type":"markdown","source":"Let's try to build a SVM using different kernels and different values of C. In this case I run a cross validation with 3 folds instead of 5: it's an heavy task. "},{"metadata":{"_cell_guid":"7dfe7378-1830-4656-bc77-7c552dd11522","collapsed":true,"_uuid":"16018d9164cd41a29484e7f8bcbb86fb78e621b7","trusted":false},"cell_type":"code","source":"from sklearn.svm import SVC\n\npres_rec = pd.DataFrame( columns=['precision','recall','c','kernel','out']);\nacc = pd.DataFrame( columns=['accuracy','c', 'kernel']);\nkf = KFold(n_splits=3, random_state=True, shuffle=True)\n\nC = [0.1,1,50,100]\nkernels = ['linear','poly','rbf','sigmoid']\n\nfor kernel in kernels:\n    for c in C:\n        for train_index, test_index in kf.split(x):\n\n            xtrain, xtest = x.iloc[train_index], x.iloc[test_index]\n            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n            clf = SVC(kernel=kernel, C=c)\n            clf.fit(xtrain, ytrain)\n            predicted = clf.predict(xtest)\n            cm = confusion_matrix(ytest,predicted)\n\n            # precision\n            ad_precision = cm[0][0] / ( cm[0][0] + cm[1][0] )\n            nonad_precision = cm[1][1] / ( cm[0][1] + cm[1][1] )\n\n            #recall\n            ad_recall = cm[0][0] / ( cm[0][0] + cm[0][1] )\n            nonad_recall = cm[1][1] / ( cm[1][0] + cm[1][1] )\n\n            #accuracy\n            accuracy = (cm[0][0] + cm[1][1]) / ( cm[0][0] + cm[1][1] + cm[0][1] + cm[1][0]);\n\n            pres_rec = pres_rec.append([{'precision': ad_precision, 'recall' : ad_recall, 'c' : c , 'kernel' : kernel, 'out' : 'ad'}]);\n            pres_rec = pres_rec.append([{'precision': nonad_precision, 'recall' : nonad_recall, 'c' : c , 'kernel' : kernel, 'out' : 'nonad'}]);\n            acc = acc.append([{'accuracy' : accuracy, 'c' : c, 'kernel' : kernel}])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"16706f49-3474-4c92-aa97-3c30e8989fa9","_uuid":"8e2977099292f72caeceac8bbdbc53ede40a4edd"},"cell_type":"markdown","source":"First of all let's plot the accuracy. Again, this will give us just a rough idea."},{"metadata":{"_cell_guid":"c66efdfb-85f0-48f1-9ecb-af06a99fb406","_uuid":"74ae1d798946e67f90af10fac05da66558ae6385","trusted":false,"collapsed":true},"cell_type":"code","source":"g = sns.factorplot(x=\"c\", y=\"accuracy\", col=\"kernel\", data=acc, capsize=.2, size=4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"18252eb2-d672-42c7-9abf-b5ce17de3ccf","_uuid":"24b05c87ba81c0d75bdd5f3f7e2aba50c0c8b2fa"},"cell_type":"markdown","source":"All kernels but sigmoid perform very well, let's see if precision and recall highlight strangenesses."},{"metadata":{"_cell_guid":"ead82159-67cd-458b-a01d-4a9d010a46a0","_uuid":"8539ad3f183d06b76fda1bb69c55b40242a7039e","trusted":false,"collapsed":true},"cell_type":"code","source":"g = sns.factorplot(x=\"c\", y=\"precision\", hue=\"out\", col=\"kernel\", data=pres_rec, capsize=.2, palette=\"YlGnBu_d\", size=4)\ng.despine(left=True)\nplt.show()\n\ng = sns.factorplot(x=\"c\", y=\"recall\", hue=\"out\", col=\"kernel\", data=pres_rec, capsize=.2, palette=\"YlGnBu_d\", size=4)\ng.despine(left=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a026c3d5-9b44-4e36-92d1-bdce3b53fc46","_uuid":"f4d28fa3f498326f73b252f41407882098884654"},"cell_type":"markdown","source":"The previous idea was true: 'sigmoid' kernel is absolutely the wrong choise if we want to classify ads (0 ads well classified).\nOn the other hand, linear and poly produce very similar results. C = 1 seems to be the best choise.\n\nIn conclusion, I obtained good result in predicting both ad and non-ad. \n\n**TODO: I fed this classifiers with all features, I'll try to use 100-150 components produced by the PCA to see if I get worse results.**\n\nAny suggestion is appreciated."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}