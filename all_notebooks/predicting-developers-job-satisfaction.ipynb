{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import pearsonr\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The Stack Overflow Developer Survey from a Human Resource Managers Point of View\n\n## Idea\n\nThe following notebook analyses the stack overflow developer survey dataset of 2017 from a human resource managers perspective. We will ask four different questions related to employee retention and job satisfaction of software developers and try to answer them with different data science methods based on this dataset.\n\nThe analysis follows broadly the CRISP-DM (cross-industry standard process for data mining) model. The six major phases of this model are:\n\n* Business Understanding\n* Data Understanding\n* Data Preparation\n* Modeling\n* Evaluation\n* Deployment\n\nWhile the first five steps are roughly reflected in this notebook, the results of the analysis are deployed as a blog post on medium.com.\n\n## Business Understanding\n\nEmployee retention and job satisfaction are important things for a human resource manager to consider. Since IT resources are rare to find these days, this is especially true today. It would be a great opportunity if data science techniques could help here. Therefore, we ask and try to answer four questions:\n\n* Is it possible to predict whether a developer is looking for a new job or not?\n* If so, what are the most important features for such a prediction?\n* Is job satisfaction related to other features recorded in the survey – like salaries?\n* Do these aspects change from country to country?\n\n## Data understanding\n\nKaggle characterizes the Stack Overflow survey in the following way: \"Every year, Stack Overflow conducts a massive survey of people on the site, covering all sorts of information like programming languages, salary, code style and various other information. This year, they amassed more than 64,000 responses fielded from 213 countries.\""},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_orig = pd.read_csv('/kaggle/input/so-survey-2017/survey_results_public.csv')\ndf_schema = pd.read_csv('/kaggle/input/so-survey-2017/survey_results_schema.csv')\ndf_orig.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_orig.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The final dataset consists of about 50.000 entries with about 150 features. An additional CSV file shows the exact questions the developer were asked. Next to a few numerical data features like salary there are more than a hundred features consisting of categorical data.\n\n### First question: Is it possible to predict whether a developer is looking for a new job or not?\n\nTo answer the questions formulated above we first create a labeled dataset for the machine learning model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_orig\n\n\n# What was the question Stack Overflow asked?\n\nprint(\"Question: \" + df_schema[df_schema['Column'] == \"JobSeekingStatus\"]['Question'].tolist()[0])\n\n# What are the possible answers for JobSeekingStatus?\n\nprint(\"Answers:\")\nprint(df['JobSeekingStatus'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reduce Dataframe to professional, full-time developers\n\ndf = df.loc[df['EmploymentStatus'] == 'Employed full-time']\ndf = df.loc[df['Professional'] == 'Professional developer']\ndf = df.drop('Respondent', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete rows without a JobSeekingStatus\ndf = df.dropna(subset=['JobSeekingStatus'], axis=0)\n\n# Delete columns with only NaNs\ndf = df.dropna(how='all', axis=1)\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create two categories of developers: those who are not interested in a new job (1) and those who are (0)\nX = df.drop('JobSeekingStatus', axis=1)\ny = pd.get_dummies(df['JobSeekingStatus'], prefix=\"JobSearch\")\ny = y['JobSearch_I am not interested in new job opportunities']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation\n\nSince the first and most ambitious question should be answered by creating a machine learning predictor for the data, we need to prepare the data to be used by a classifier. Therefore, we convert numerical NaNs to the mean of the column and convert categorical data to dummy data first."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill the NaNs in numerical columns with the mean\n\nnum_cols = X.select_dtypes(include=['float','int']).columns\n\nfor col in num_cols:\n    X[col].fillna(X[col].mean(), inplace=True)\n\n# Create dummy columns for categorical columns (takes a while...)\n\ncat_cols = X.select_dtypes(include=['object']).columns\n\nfor col in cat_cols:\n    X = pd.concat([X.drop(col, axis=1), pd.get_dummies(X[col], prefix=col, drop_first=True)], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling\n\nAfter several tests we decided to use a Random Forest classifier for the prediction. It is either fast and has the useful capability that it can show the most important features used for the prediction – which is helpful for answering the next question."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Do the prediction\n    \n# Step 1: test train sample\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Step 2: create and train a classifier (may take a while, too...)\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\n\n# Step 3: How well does it predict?\n\ny_pred = clf.predict(X_test)\nprint(\"Classification report:\")\nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this pretty high F1 score we can say that it is generally possible to predict the two types of developer pretty well.\n\n### Next question: what are the most important features for such a prediction?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the 10 most important features?\n\nimportances = clf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(10):\n    print(\"%d. %s (%f)\" % (f + 1, X_train.columns[indices[f]-1], importances[indices[f]]))\n\n# Plot the feature importances\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(10), importances[indices[:10]], color=\"r\", yerr=std[indices[:10]], align=\"center\")\nplt.xticks(range(10), X_train.columns[indices[:10]-1], rotation='vertical')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence, the clearly most important feature is JobSatisfaction.\n\n### Next question: Is job satisfaction related to other features recorded in the survey – like salaries?\n\nTherefore, we analyze the correlation between job satisfaction and salary."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_orig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Again, reduce Dataframe to professional, full-time developers\n\ndf = df.loc[df['EmploymentStatus'] == 'Employed full-time']\ndf = df.loc[df['Professional'] == 'Professional developer']\ndf = df.drop(['Respondent','JobSeekingStatus','ExpectedSalary'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are possible values for job satisfaction?\n\ndf['JobSatisfaction'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Delete rows with no value for job satisfaction or salary\n\ndf = df.dropna(subset=['JobSatisfaction','Salary'], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Are both features linear correlated?\n\ncorr, _ = pearsonr(df['Salary'], df['JobSatisfaction'])\nprint('Pearsons correlation: %.3f' % corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot a correlation matrix for a deeper look\n## Source: https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n\n# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the correlation numbers are altogether pretty low, we cannot really find a correlation here.\n\n### Last question: Do these aspects change from country to country?\n\nTo answer this question we analyze how job satisfaction (and salary) vary from country to country."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate average job satisfaction and salary for major countries\n\ndf = df.dropna(subset=['JobSatisfaction','Salary'], axis=0)\nmajor_countries = df['Country'].value_counts()[:15].keys()\n\nsal_mean = []\nsat_mean = []\n\nfor i in range(len(major_countries)):\n    sat_mean.append(df.loc[df['Country'] == major_countries[i]]['JobSatisfaction'].mean())\n    sal_mean.append(df.loc[df['Country'] == major_countries[i]]['Salary'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare these values by a scatter plot\n\nplt.title(\"Job Satisfaction and Salary for 15 Countries\")\nplt.xlabel(\"Avg. Salary\")\nplt.ylabel(\"Avg. Job Satisfaction\")\nplt.scatter(sal_mean, sat_mean)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Are they linear correlated?\n\ncorr, _ = pearsonr(sal_mean, sat_mean)\nprint('Pearsons correlation: %.3f' % corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the scatter plot and the increased Pearsons number suggests we have at least a moderate correlation.\n\n## Deployment\n\nPlease see the following blog post for a deeper discussion on the topics outlined above:\n\nhttps://medium.com/@cornel_77788/how-data-science-could-help-a-human-resource-manager-5d6e95c87c95"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}