{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"cd ../working","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://github.com/luke-dinh/Image-Super-Resolution.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cd Image-Super-Resolution/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from models import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install argparse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python '../input/data-for-training'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nimport random\nimport os\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n# Normalization parameters for pre-trained PyTorch models\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\n\nclass ImageDataset(Dataset):\n    def __init__(self, root, hr_shape):\n        hr_height, hr_width = hr_shape\n        # Transforms for low resolution images and high resolution images\n        self.lr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height // 4, hr_height // 4), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n        self.hr_transform = transforms.Compose(\n            [\n                transforms.Resize((hr_height, hr_height), Image.BICUBIC),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std),\n            ]\n        )\n\n        self.files = sorted(glob.glob(root + \"/*.*\"))\n\n    def __getitem__(self, index):\n        img = Image.open(self.files[index % len(self.files)])\n        img_lr = self.lr_transform(img)\n        img_hr = self.hr_transform(img)\n\n        return {\"lr\": img_lr, \"hr\": img_hr}\n\n    def __len__(self):\n        return len(self.files)\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom torchvision.models import vgg19\nimport math\n\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        vgg19_model = vgg19(pretrained=True)\n        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])\n\n    def forward(self, img):\n        return self.feature_extractor(img)\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n            nn.PReLU(),\n            nn.Conv2d(in_features, in_features, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(in_features, 0.8),\n        )\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n\n\nclass GeneratorResNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, n_residual_blocks=16):\n        super(GeneratorResNet, self).__init__()\n\n        # First layer\n        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4), nn.PReLU())\n\n        # Residual blocks\n        res_blocks = []\n        for _ in range(n_residual_blocks):\n            res_blocks.append(ResidualBlock(64))\n        self.res_blocks = nn.Sequential(*res_blocks)\n\n        # Second conv layer post residual blocks\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.BatchNorm2d(64, 0.8))\n\n        # Upsampling layers\n        upsampling = []\n        for out_features in range(2):\n            upsampling += [\n                # nn.Upsample(scale_factor=2),\n                nn.Conv2d(64, 256, 3, 1, 1),\n                nn.BatchNorm2d(256),\n                nn.PixelShuffle(upscale_factor=2),\n                nn.PReLU(),\n            ]\n        self.upsampling = nn.Sequential(*upsampling)\n\n        # Final output layer\n        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())\n\n    def forward(self, x):\n        out1 = self.conv1(x)\n        out = self.res_blocks(out1)\n        out2 = self.conv2(out)\n        out = torch.add(out1, out2)\n        out = self.upsampling(out)\n        out = self.conv3(out)\n        return out\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        self.input_shape = input_shape\n        in_channels, in_height, in_width = self.input_shape\n        patch_h, patch_w = int(in_height / 2 ** 4), int(in_width / 2 ** 4)\n        self.output_shape = (1, patch_h, patch_w)\n\n        def discriminator_block(in_filters, out_filters, first_block=False):\n            layers = []\n            layers.append(nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1))\n            if not first_block:\n                layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Conv2d(out_filters, out_filters, kernel_size=3, stride=2, padding=1))\n            layers.append(nn.BatchNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        layers = []\n        in_filters = in_channels\n        for i, out_filters in enumerate([64, 128, 256, 512]):\n            layers.extend(discriminator_block(in_filters, out_filters, first_block=(i == 0)))\n            in_filters = out_filters\n\n        layers.append(nn.Conv2d(out_filters, 1, kernel_size=3, stride=1, padding=1))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, img):\n        return self.model(img)\n    \n    \nimport argparse\nimport os\nimport numpy as np\nimport math\nimport itertools\nimport sys\n# os.path.isdir('../input/data-for-training')\n# os.path.isdir('../input/models-for-training')\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image, make_grid\n\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\n\n# from models import *\n# from datasets import *\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nos.makedirs(\"images\", exist_ok=True)\nos.makedirs(\"saved_models\", exist_ok=True)\n\n# parser = argparse.ArgumentParser()\n# parser.add_argument(\"--epoch\", type=int, default=0, help=\"epoch to start training from\")\n# parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n# parser.add_argument(\"--dataset_name\", type=str, default=\"img_align_celeba\", help=\"name of the dataset\")\n# parser.add_argument(\"--batch_size\", type=int, default=4, help=\"size of the batches\")\n# parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n# parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n# parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n# parser.add_argument(\"--decay_epoch\", type=int, default=100, help=\"epoch from which to start lr decay\")\n# parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n# parser.add_argument(\"--hr_height\", type=int, default=256, help=\"high res. image height\")\n# parser.add_argument(\"--hr_width\", type=int, default=256, help=\"high res. image width\")\n# parser.add_argument(\"--channels\", type=int, default=3, help=\"number of image channels\")\n# parser.add_argument(\"--sample_interval\", type=int, default=100, help=\"interval between saving image samples\")\n# parser.add_argument(\"--checkpoint_interval\", type=int, default=-1, help=\"interval between model checkpoints\")\n# opt = parser.parse_args()\n# print(opt)\n\nepoch = 0\nn_epochs = 50\ndataet_name = 'img_align_data'\nbatch_size = 4\nlr = 0.0002\nb1 = 0.5\nb2 = 0.999\ndecay_epoch = 25\nn_cpu = 8\nhr_height = 256\nhr_width = 256\nchannels = 3\nsample_interval = 10\ncheckpoint_interval = -1\n\ncuda = torch.cuda.is_available()\n\nhr_shape = (hr_height, hr_width)\n\n# Initialize generator and discriminator\ngenerator = GeneratorResNet()\ndiscriminator = Discriminator(input_shape=(channels, *hr_shape))\nfeature_extractor = FeatureExtractor()\n\n# Set feature extractor to inference mode\nfeature_extractor.eval()\n\n# Losses\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_content = torch.nn.L1Loss()\n\nif cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    feature_extractor = feature_extractor.cuda()\n    criterion_GAN = criterion_GAN.cuda()\n    criterion_content = criterion_content.cuda()\n\nif epoch != 0:\n    # Load pretrained models\n    generator.load_state_dict(torch.load(\"saved_models/generator_%d.pth\"))\n    discriminator.load_state_dict(torch.load(\"saved_models/discriminator_%d.pth\"))\n\n# Optimizers\noptimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n\nTensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n\ndataloader = DataLoader(\n    ImageDataset(\"../input/sourceimg100/img/img\", hr_shape=hr_shape),\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=n_cpu,\n)\n\n# ----------\n#  Training\n# ----------\n\nfor epoch in range(epoch, n_epochs):\n    for i, imgs in enumerate(dataloader):\n\n        # Configure model input\n        imgs_lr = Variable(imgs[\"lr\"].type(Tensor))\n        imgs_hr = Variable(imgs[\"hr\"].type(Tensor))\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((imgs_lr.size(0), *discriminator.output_shape))), requires_grad=False)\n\n        # ------------------\n        #  Train Generators\n        # ------------------\n\n        optimizer_G.zero_grad()\n\n        # Generate a high resolution image from low resolution input\n        gen_hr = generator(imgs_lr)\n\n        # Adversarial loss\n        loss_GAN = criterion_GAN(discriminator(gen_hr), valid)\n\n        # Content loss\n        gen_features = feature_extractor(gen_hr)\n        real_features = feature_extractor(imgs_hr)\n        loss_content = criterion_content(gen_features, real_features.detach())\n\n        # Total loss\n        loss_G = loss_content + 1e-3 * loss_GAN\n\n        loss_G.backward()\n        optimizer_G.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        optimizer_D.zero_grad()\n\n        # Loss of real and fake images\n        loss_real = criterion_GAN(discriminator(imgs_hr), valid)\n        loss_fake = criterion_GAN(discriminator(gen_hr.detach()), fake)\n\n        # Total loss\n        loss_D = (loss_real + loss_fake) / 2\n\n        loss_D.backward()\n        optimizer_D.step()\n\n        # --------------\n        #  Log Progress\n        # --------------\n\n        sys.stdout.write(\n            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n            % (epoch, n_epochs, i, len(dataloader), loss_D.item(), loss_G.item())\n        )\n\n        batches_done = epoch * len(dataloader) + i\n        if batches_done % sample_interval == 0:\n            # Save image grid with upsampled inputs and SRGAN outputs\n            imgs_lr = nn.functional.interpolate(imgs_lr, scale_factor=4)\n            gen_hr = make_grid(gen_hr, nrow=1, normalize=True)\n            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n            img_grid = torch.cat((imgs_lr, gen_hr), -1)\n            save_image(img_grid, \"images/%d.png\" % batches_done, normalize=False)\n\n    if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n        # Save model checkpoints\n        torch.save(generator.state_dict(), \"../input/output/saved_models/generator_%d.pth\" % epoch)\n        torch.save(discriminator.state_dict(), \"../input/output/saved_models/discriminator_%d.pth\" % epoch)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}