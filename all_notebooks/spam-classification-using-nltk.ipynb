{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Are you getting spam messages in your inbox???\n<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTLLx3jilwIExEUyOBnj_pRKGS0E-IEel_jTVE8eSzVgBsYeH1Q\" width=\"500px\" align=\"left\">"},{"metadata":{},"cell_type":"markdown","source":"**What are Spam messages ? **\n\nSpam messages are unsolicited, usually commercial messages (such as e-mails, text messages, or Internet postings) sent to a large number of recipients or posted in a large number of places.\n\n**Problem Statement : ** Accurately classifing spam messages by building a predictive model. \n\n**Solution : ** By doing data preprocessing and predictive analysis using naive bayes, we can achieve the goal of accurately classifing spam messages.\n\n**Agenda :**\n1. [Import libraries](#1)\n2. [Load the data](#2)\n3. [Data Cleaning and Data Preprocessing](#3)\n    * [Dropping null data](#4)\n    * [Renaming columns](#5)    \n    * [Counting values in `label`](#6)\n    * [Checking shape of the dataset](#7)\n    * [Dropping duplicates](#8)\n    * [Checking shape of the dataset after dropping duplicates](#9)\n    * [Mapping `label`](#10)\n    * [Dropping `label` column](#11)\n    * [Removing Punctuations](#12)\n    * [Removing Stopwords](#12)\n4. [Data Visualization](#13)\n    * [WordCloud for spam messages](#14)\n    * [WordCloud for ham messages](#15)\n5. [Feature Extraction](#16)\n6. [Train and Test split](#17)\n7. [Predictive Analysis](#18)\n8. [Model Evaluation](#19)\n9. [Test on random data](#20)\n10. [Conclusion](#21)\n\nLet's start here!"},{"metadata":{},"cell_type":"markdown","source":"## 1. Import libraries<a id=\"1\"></a>"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport string\nimport pandas as pd\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Load the data<a id=\"2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading data\n\nmails = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\",encoding = 'latin-1')\nmails.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Cleaning and Data preprocessing<a id=\"3\"></a>\nData cleaning - In this step, we'll drop columns, rename columns, map columns or drop duplicate values.\n\nPreprocessing - Preprocessing is one of the major steps when we are dealing with any kind of text models. During this stage we have to look at the distribution of our data, what techniques are needed and how deep we should clean."},{"metadata":{},"cell_type":"markdown","source":"### Dropping null data <a id=\"4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the null columns namely Unnamed: 2, Unnamed: 3 and Unnamed: 4\n\nmails.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1,inplace=True)\nmails.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Renaming columns <a id=\"5\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# renaming the columns v1 and v2 as labels and message\n\nmails.rename(columns = {'v1':'labels', 'v2':'message'}, inplace=True)\nmails.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Counting values in `label`<a id=\"6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count of labels\n\nmails['labels'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking shape of the data<a id=\"7\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"mails.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping duplicates<a id=\"8\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# now, we will see if our dataset contains duplicates, we will drop the duplicates\n\nmails.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking for the shape of the data after dropping duplicates<a id=\"9\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# after droping duplicates let's see the shape of dataset again\n\nmails.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see our dataset had contained 403 duplicates."},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for any null values in the dataset\n\nmails.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thus, there is no null value in the data."},{"metadata":{},"cell_type":"markdown","source":"### Mapping `label`<a id=\"10\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# mapping the labels as 0 or 1\n# 0 for ham and 1 for spam\n\nmails['label'] = mails['labels'].map({'ham': 0, 'spam': 1})\nmails.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping `label` column<a id=\"11\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# now, labels column is of no use so we will drop the labels columns\n\nmails.drop(['labels'], axis=1, inplace=True)\nmails.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenzation<a id='12'></a>\nWe will tokenize our dataset using some techniques. Tokenization basically refers to splitting up a larger body of text into smaller lines, words or even creating words.\n\nPunctuations are the unnecessary symbols that are in our corpus documents.\n\nStop words are the most commonly occurring words which don’t give any additional value to the document vector. in-fact removing these will increase computation and space efficiency."},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_text(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)\n    \n    # Now just remove any stopwords\n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#show the tokenization \n\nmails['message'].head().apply(process_text)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Data Visualization<a id = \"13\"></a>\n\nNow, we will see frequently used words using WordCloud."},{"metadata":{},"cell_type":"markdown","source":"### WordCloud for spam messages<a id='14'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"spam_words = ' '.join(list(mails[mails['label'] == 1]['message']))\nspam_wc = WordCloud(width = 512,height = 512).generate(spam_words)\nplt.figure(figsize = (10, 8), facecolor = 'k')\nplt.imshow(spam_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WordCloud for ham messages<a id='15'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"ham_words = ' '.join(list(mails[mails['label'] == 0]['message']))\nham_wc = WordCloud(width = 512,height = 512).generate(ham_words)\nplt.figure(figsize = (10, 8), facecolor = 'k')\nplt.imshow(ham_wc)\nplt.axis('off')\nplt.tight_layout(pad = 0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Feature Extraction<a id = \"16\"></a>\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert a collection of text documents to a matrix of token counts\n# message_bow stands for bag of words\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nx = mails['message']\ny = mails['label']\ncv = CountVectorizer()\nx= cv.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Train and Test split<a id = \"17\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data into 80% training and 20% testing\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shape of data after Vectorization\n\nx.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 7. Predictive analysis<a id = \"18\"></a>\n**Q.** Why we are using Naive Bayes?\n\nNaive Bayes is commonly applied to text classification. Naive Bayes classifiers work by correlating the use of tokens with spam and non-spam e-mails and then using Bayes' theorem to calculate a probability that an email is or is not spam.\n\nThe “Naive” assumption that the Naive Bayes classifier makes is that the probability of observing a word is independent of each other. The result is that the “likelihood” is the product of the individual probabilities of seeing each word in the set of Spam or Ham emails."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create and train the naive Bayes classifier\n# The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification)\n\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB().fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 8. Model Evaluation<a id = \"19\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the model on the test data set\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\npred = classifier.predict(X_test)\nprint(classification_report(y_test, pred))\nprint()\nprint('Confusion Matrix:\\n',confusion_matrix(y_test, pred))\nprint()\nprint('Accuracy : ',accuracy_score(y_test, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the predictions\nprint(classifier.predict(X_test))\n\n# print the actual values\nprint(y_test.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 9. Test on random data<a id='20'></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def sms(text):\n    \n    # creating a list of labels\n    lab = ['not spam','spam'] \n    \n    # perform tokenization\n    x = cv.transform(text).toarray()\n    \n    # predict the text\n    p = classifier.predict(x)\n    \n    # convert the words in string with the help of list\n    s = [str(i) for i in p]\n    a = int(\"\".join(s))\n    \n    # show out the final result\n    res = str(\"This message is looking: \"+ lab[a])\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms(['Congratulations, your entry into our contest last month made you a WINNER! goto our website to claim your price! You have 24 hours to claim.'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms(['Your mobile number has won 1,615,000 million pounds in Apple iPhone UK. For claim email your name, country, occupation.'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms(['Our Summer Sale is live from 15th to 17th june! Get 40% off on select products. visit the store now.'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms(['Hey there! I am using kaggle'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms(['Did you here about the new tv show?'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sms(['I am free after 11:00 in morning, meet you soon!'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 10. Conclusion<a id='21'></a>\nWe’ve come a long way in your journey to learn about Spam Classification using NLTK(natural language toolkit). Here we are using the Naive Bayes Classification and extracting the word using word-count algorithm. After calculation we find that  Naive Bayes' Classification has more accuracy support. The error rate is very low when we are using the Naive Bayes Classification. Thus, using naive bayes algorithm we can classify ham and spam messages with the accuracy of 97%."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}