{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Feature Selection\n#### Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Pearson Correlation\n_______\n#### Pearson's correlation coefficient is the test statistics that measures the statistical relationship, or association, between two continuous variables. It gives information about the magnitude of the association, or correlation, as well as the direction of the relationship.\n#### There are two types of correlations. Positive Correlation: means that if feature A increases then feature B also increases or if feature A decreases then feature B also decreases. Both features move in tandem and they have a linear relationship. Negative Correlation: means that if feature A increases then feature B decreases and vice versa.\n\n![](https://www.researchgate.net/profile/Ivan-Nikolov/publication/345362331/figure/fig11/AS:954798429978645@1604653093872/Correlation-matrix-of-the-used-metrics-together-with-the-dependent-variable-For-easier.ppm)\n\n#### We will be choosing our features after calculations based on correlation matrix.\n#### If 2 or more independent features are highly correlated then they can be considered as duplicate features and can be dropped. When independent variables are highly correlated, change in one variable would cause change to another and so the model results fluctuate significantly. The model results will be unstable and vary a lot given a small change in the data or model. Both positive and negative correlations are taken into consideration.  \n     \n_______","metadata":{}},{"cell_type":"markdown","source":"### Importing required libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nfrom sklearn.model_selection import train_test_split\ndef split(df,label):\n    X_tr, X_te, Y_tr, Y_te = train_test_split(df, label, test_size=0.25, random_state=42)\n    return X_tr, X_te, Y_tr, Y_te\n\n\ndef correlation(dataset, cor):\n    df = dataset.copy()\n    col_corr = set()  # For storing unique value\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > cor: # absolute values to handle positive and negative correlations\n                colname = corr_matrix.columns[i]  \n                col_corr.add(colname)\n    df.drop(col_corr,axis = 1,inplace = True)\n    return df\n\n\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold, cross_val_score\n\nclassifiers = ['LinearSVM', 'RadialSVM', \n               'Logistic',  'RandomForest', \n               'AdaBoost',  'DecisionTree', \n               'KNeighbors','GradientBoosting']\n\nmodels = [svm.SVC(kernel='linear'),\n          svm.SVC(kernel='rbf'),\n          LogisticRegression(max_iter = 1000),\n          RandomForestClassifier(n_estimators=200, random_state=0),\n          AdaBoostClassifier(random_state = 0),\n          DecisionTreeClassifier(random_state=0),\n          KNeighborsClassifier(),\n          GradientBoostingClassifier(random_state=0)]\n\n\ndef acc_score(df,label):\n    Score = pd.DataFrame({\"Classifier\":classifiers})\n    j = 0\n    acc = []\n    X_train,X_test,Y_train,Y_test = split(df,label)\n    for i in models:\n        model = i\n        model.fit(X_train,Y_train)\n        predictions = model.predict(X_test)\n        acc.append(accuracy_score(Y_test,predictions))\n        j = j+1     \n    Score[\"Accuracy\"] = acc\n    Score.sort_values(by=\"Accuracy\", ascending=False,inplace = True)\n    Score.reset_index(drop=True, inplace=True)\n    return Score\n\n\ndef acc_score_cor(df,label,cor_list):\n    Score = pd.DataFrame({\"Classifier\":classifiers})\n    for k in range(len(cor_list)):\n        df2 = correlation(df, cor_list[k])\n        X_train,X_test,Y_train,Y_test = split(df2,label)\n        j = 0\n        acc = []\n        for i in models:\n            model = i\n            model.fit(X_train,Y_train)\n            predictions = model.predict(X_test)\n            acc.append(accuracy_score(Y_test,predictions))\n            j = j+1  \n        feat = str(cor_list[k])\n        Score[feat] = acc\n    return Score\n\n        \ndef plot2(df,l1,l2,p1,p2,c = \"b\"):\n    feat = df.columns.tolist()\n    feat = feat[1:]\n    plt.figure(figsize = (16, 18))\n    for j in range(0,df.shape[0]):\n        value = []\n        k = 0\n        for i in range(1,len(df.columns.tolist())):\n            value.append(df.iloc[j][i])\n        plt.subplot(4, 4,j+1)\n        ax = sns.pointplot(x=feat, y=value,color = c )\n        plt.text(p1,p2,df.iloc[j][0])\n        plt.xticks(rotation=90)\n        ax.set(ylim=(l1,l2))\n        k = k+1\n        \n\ndef highlight_max(data, color='aquamarine'):\n    attr = 'background-color: {}'.format(color)\n    if data.ndim == 1:  \n        is_max = data == data.max()\n        return [attr if v else '' for v in is_max]\n    else: \n        is_max = data == data.max().max()\n        return pd.DataFrame(np.where(is_max, attr, ''),\n                            index=data.index, columns=data.columns)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:33:53.693184Z","iopub.execute_input":"2021-06-26T11:33:53.693501Z","iopub.status.idle":"2021-06-26T11:33:53.722981Z","shell.execute_reply.started":"2021-06-26T11:33:53.693472Z","shell.execute_reply":"2021-06-26T11:33:53.722241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________\n### Function Description\n\n#### 1. split():\nSplits the dataset into training and test set.\n\n#### 2. correlation():\nReturns the dataframe after dropping features with greater correlation than the given value.\n\n#### 3. acc_score():\nReturns accuracy for all the classifiers.\n\n#### 4. acc_score_cor():\nReturns accuracy for all the classifiers after dropping features for the respective correlation value.\n\n#### 5. plot2():\nFor plotting the results.\n_____\n### The following 3 datasets are used:\n1. Breast Cancer\n2. Parkinson's Disease\n3. PCOS\n______\n### Plan of action:\n* Looking at dataset (includes a little preprocessing)\n* Heatmap (Plotting the heatmap)\n* Checking Accuracy (comparing accuracies with the new dataset)\n* Visualization (Plotting the graphs)\n_______","metadata":{}},{"cell_type":"markdown","source":"__________\n# Breast Cancer\n_________________","metadata":{}},{"cell_type":"markdown","source":"### 1. Looking at dataset","metadata":{}},{"cell_type":"code","source":"data_bc = pd.read_csv(\"../input/breast-cancer-wisconsin-data/data.csv\")\nlabel_bc = data_bc[\"diagnosis\"]\nlabel_bc = np.where(label_bc == 'M',1,0)\ndata_bc.drop([\"id\",\"diagnosis\",\"Unnamed: 32\"],axis = 1,inplace = True)\n\nprint(\"Breast Cancer dataset:\\n\",data_bc.shape[0],\"Records\\n\",data_bc.shape[1],\"Features\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:32:22.85215Z","iopub.execute_input":"2021-06-26T11:32:22.852744Z","iopub.status.idle":"2021-06-26T11:32:22.891793Z","shell.execute_reply.started":"2021-06-26T11:32:22.852689Z","shell.execute_reply":"2021-06-26T11:32:22.890763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(data_bc.head())\nprint(\"All the features in this dataset have continuous values\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:32:26.671641Z","iopub.execute_input":"2021-06-26T11:32:26.671971Z","iopub.status.idle":"2021-06-26T11:32:26.70964Z","shell.execute_reply.started":"2021-06-26T11:32:26.671941Z","shell.execute_reply":"2021-06-26T11:32:26.708627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Heatmap","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,18))\ncor1 = data_bc.corr()\nsns.heatmap(cor1, annot=True, cmap=\"viridis\",annot_kws={\"size\":8})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:32:29.776213Z","iopub.execute_input":"2021-06-26T11:32:29.776534Z","iopub.status.idle":"2021-06-26T11:32:33.415567Z","shell.execute_reply.started":"2021-06-26T11:32:29.776501Z","shell.execute_reply":"2021-06-26T11:32:33.414497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Checking Accuracy","metadata":{}},{"cell_type":"code","source":"score1 = acc_score(data_bc,label_bc)\nscore1","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:32:33.41727Z","iopub.execute_input":"2021-06-26T11:32:33.417668Z","iopub.status.idle":"2021-06-26T11:32:36.40529Z","shell.execute_reply.started":"2021-06-26T11:32:33.41762Z","shell.execute_reply":"2021-06-26T11:32:36.404331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrate_bc = [0.6,0.7,0.8,0.9,0.95,0.99]\nclassifiers = score1[\"Classifier\"].tolist()\nscore_bc = acc_score_cor(data_bc,label_bc,corrate_bc)\nscore_bc.style.apply(highlight_max, subset = score_bc.columns[1:], axis=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:34:02.709335Z","iopub.execute_input":"2021-06-26T11:34:02.709828Z","iopub.status.idle":"2021-06-26T11:34:10.004789Z","shell.execute_reply.started":"2021-06-26T11:34:02.709785Z","shell.execute_reply":"2021-06-26T11:34:10.003797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best Accuracy with all features : RandomForest Classifier - 0.972\n#### Best Accuracy after applying with correlation() : LinearSVM - for = (0.9,0.99) - 0.972  and DecisionTree Classifier - for = (0.9) - 0.972\n#### Here we see no improvement.","metadata":{}},{"cell_type":"markdown","source":"### 4. Visualization","metadata":{}},{"cell_type":"code","source":"plot2(score_bc,0.85,1,2.4,0.86,c = \"gold\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:36:26.941168Z","iopub.execute_input":"2021-06-26T11:36:26.941502Z","iopub.status.idle":"2021-06-26T11:36:27.783225Z","shell.execute_reply.started":"2021-06-26T11:36:26.941474Z","shell.execute_reply":"2021-06-26T11:36:27.782218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________\n# Parkinson's Disease\n___________","metadata":{}},{"cell_type":"markdown","source":"### 1. Looking at dataset","metadata":{}},{"cell_type":"code","source":"data_pd = pd.read_csv(\"../input/parkinson-disease-detection/Parkinsson disease.csv\")\nlabel_pd = data_pd[\"status\"]\ndata_pd.drop([\"status\",\"name\"],axis = 1,inplace = True)\n\nprint(\"Parkinson's disease dataset:\\n\",data_pd.shape[0],\"Records\\n\",data_pd.shape[1],\"Features\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:45:02.380793Z","iopub.execute_input":"2021-06-26T11:45:02.381076Z","iopub.status.idle":"2021-06-26T11:45:02.404972Z","shell.execute_reply.started":"2021-06-26T11:45:02.381042Z","shell.execute_reply":"2021-06-26T11:45:02.404135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(data_pd.head())\nprint(\"All the features in this dataset have continuous values\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:45:02.406381Z","iopub.execute_input":"2021-06-26T11:45:02.406628Z","iopub.status.idle":"2021-06-26T11:45:02.432576Z","shell.execute_reply.started":"2021-06-26T11:45:02.406604Z","shell.execute_reply":"2021-06-26T11:45:02.431673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Heatmap","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,18))\ncor3 = data_pd.corr()\nsns.heatmap(cor3, annot=True, cmap=\"seismic\",annot_kws={\"size\":8})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:45:02.434074Z","iopub.execute_input":"2021-06-26T11:45:02.434455Z","iopub.status.idle":"2021-06-26T11:45:04.584253Z","shell.execute_reply.started":"2021-06-26T11:45:02.434413Z","shell.execute_reply":"2021-06-26T11:45:04.583557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Checking Accuracy","metadata":{}},{"cell_type":"code","source":"score3 = acc_score(data_pd,label_pd)\nscore3","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:45:04.585229Z","iopub.execute_input":"2021-06-26T11:45:04.58561Z","iopub.status.idle":"2021-06-26T11:45:05.284438Z","shell.execute_reply.started":"2021-06-26T11:45:04.585571Z","shell.execute_reply":"2021-06-26T11:45:05.283376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrate_pd = [0.5,0.6,0.7,0.8,0.9,0.95]\nclassifiers = score3[\"Classifier\"].tolist()\nscore_pd = acc_score_cor(data_pd,label_pd,corrate_pd)\nscore_pd.style.apply(highlight_max, subset = score_pd.columns[1:], axis=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:45:05.285572Z","iopub.execute_input":"2021-06-26T11:45:05.285848Z","iopub.status.idle":"2021-06-26T11:45:09.554674Z","shell.execute_reply.started":"2021-06-26T11:45:05.285815Z","shell.execute_reply":"2021-06-26T11:45:09.553579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best Accuracy with all features : DecisionTree Classifier - 0.918\n#### Best Accuracy after applying with correlation() : RandomForest Classifier - for = (0.9) - 0.938\n#### Here we can see an improvement of 2%.","metadata":{}},{"cell_type":"markdown","source":"### 4. Visualization","metadata":{}},{"cell_type":"code","source":"plot2(score_pd,0.75,1,2.5,0.765,c = \"orange\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:45:09.556694Z","iopub.execute_input":"2021-06-26T11:45:09.556962Z","iopub.status.idle":"2021-06-26T11:45:10.328381Z","shell.execute_reply.started":"2021-06-26T11:45:09.556932Z","shell.execute_reply":"2021-06-26T11:45:10.327255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________\n# PCOS\n________","metadata":{}},{"cell_type":"markdown","source":"### 1. Looking at dataset","metadata":{}},{"cell_type":"code","source":"data_pcos = pd.read_csv(\"../input/pcos-dataset/PCOS_data.csv\")\nlabel_pcos = data_pcos[\"PCOS (Y/N)\"]\ndata_pcos.drop([\"Sl. No\",\"Patient File No.\",\"PCOS (Y/N)\",\"Unnamed: 44\",\"II    beta-HCG(mIU/mL)\",\"AMH(ng/mL)\"],axis = 1,inplace = True)\ndata_pcos[\"Marraige Status (Yrs)\"].fillna(data_pcos['Marraige Status (Yrs)'].describe().loc[['50%']][0], inplace = True) \ndata_pcos[\"Fast food (Y/N)\"].fillna(1, inplace = True) \n\nprint(\"PCOS dataset:\\n\",data_pcos.shape[0],\"Records\\n\",data_pcos.shape[1],\"Features\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:46:24.138513Z","iopub.execute_input":"2021-06-26T11:46:24.138877Z","iopub.status.idle":"2021-06-26T11:46:24.174552Z","shell.execute_reply.started":"2021-06-26T11:46:24.138842Z","shell.execute_reply":"2021-06-26T11:46:24.173517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(data_pcos.head())\nprint(\"The features in this dataset have both discrete and continuous values\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:46:24.559842Z","iopub.execute_input":"2021-06-26T11:46:24.560199Z","iopub.status.idle":"2021-06-26T11:46:24.589272Z","shell.execute_reply.started":"2021-06-26T11:46:24.560163Z","shell.execute_reply":"2021-06-26T11:46:24.58772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Heatmap","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18,18))\ncor4 = data_pcos.corr()\nsns.heatmap(cor4, annot=True, cmap=\"YlOrBr\",annot_kws={\"size\":8})\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:46:25.880422Z","iopub.execute_input":"2021-06-26T11:46:25.880809Z","iopub.status.idle":"2021-06-26T11:46:31.928725Z","shell.execute_reply.started":"2021-06-26T11:46:25.880775Z","shell.execute_reply":"2021-06-26T11:46:31.927813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Checking Accuracy","metadata":{}},{"cell_type":"code","source":"score4 = acc_score(data_pcos,label_pcos)\nscore4","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:46:31.930261Z","iopub.execute_input":"2021-06-26T11:46:31.930537Z","iopub.status.idle":"2021-06-26T11:47:29.821354Z","shell.execute_reply.started":"2021-06-26T11:46:31.93051Z","shell.execute_reply":"2021-06-26T11:47:29.820331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corrate_pcos = [0.4,0.5,0.6,0.8,0.9,0.95]\nclassifiers = score4[\"Classifier\"].tolist()\nscore_pcos = acc_score_cor(data_pcos,label_pcos,corrate_pcos)\nscore_pcos.style.apply(highlight_max, subset = score_pcos.columns[1:], axis=None)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:47:29.823337Z","iopub.execute_input":"2021-06-26T11:47:29.823752Z","iopub.status.idle":"2021-06-26T11:55:08.903284Z","shell.execute_reply.started":"2021-06-26T11:47:29.823696Z","shell.execute_reply":"2021-06-26T11:55:08.902312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best Accuracy with all features : RandomForest Classifier - 0.889\n#### Best Accuracy after applying with correlation() : DecisionTree Classifier - for = (0.9) - 0.904\n#### Here we can see an improvement of ~1.5%.","metadata":{}},{"cell_type":"markdown","source":"### 4. Visualization","metadata":{}},{"cell_type":"code","source":"plot2(score_pcos,0.60,1,2.5,0.975,c = \"limegreen\")","metadata":{"execution":{"iopub.status.busy":"2021-06-26T11:55:08.904817Z","iopub.execute_input":"2021-06-26T11:55:08.905215Z","iopub.status.idle":"2021-06-26T11:55:09.736041Z","shell.execute_reply.started":"2021-06-26T11:55:08.905172Z","shell.execute_reply":"2021-06-26T11:55:09.734966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__________\n","metadata":{}},{"cell_type":"markdown","source":"#### From looking at these results we can see that there is a possibility of slight improvement in the accuracy after removing features that are correlated.\n#### Link to other feature selection methods:\n##### [Genetic Algorithm](https://www.kaggle.com/tanmayunhale/genetic-algorithm-for-feature-selection)\n##### [Variance Threshold](https://www.kaggle.com/tanmayunhale/feature-selection-variance-threshold)\n##### [F-score](https://www.kaggle.com/tanmayunhale/feature-selection-f-score)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}