{"cells":[{"metadata":{"_uuid":"108a9f6ae089a97c442efb22c7c5fb064aa98016"},"cell_type":"markdown","source":"## Overview\nFor this project, I wanted to explore image recognition through supervised machine learning. When creating an image classifier, I found a popular solution was utilizing a Convolutional Neural Network with the Keras library. Keras is a deep learning library written in Python that is used for easy and fast prototyping.  \n\nThis project is two fold:\n1. Creating an image classifier \n2. Explore practical uses for model \n                         \nFor this project, I will be using fashion product images. A common business question as it relates to products that can be solved machine learning is \"how might we make recommendations based on what user's like or dislike?\" While this project does not go as far as creating a complete solution to solve for this case, it tackles the beginning steps towards it."},{"metadata":{"_uuid":"0a5dd1bb4db47e3313cc9957857eb3f4684dd11f"},"cell_type":"markdown","source":"## Method\n\nSince I had planned on training a image classifier model, this would required a lot of images and I did not want to compile these myself. I was able to find this [Fashion Product Data Set](https://www.kaggle.com/paramaggarwal/fashion-product-images-small) on Kaggle. This data set includes over 44,000 images with a variety of types or products. Additionally, it includes multiple attributes and categories that the model can be trained on. \n\nThis dataset also came with [Starter Code](https://www.kaggle.com/paramaggarwal/fashion-product-images-classifier) for an image classifier that was automatically created with a Kaggle bot. I decided to utilize this code while making modifications and additions based on other example solutions for image classifiers across the web. In particular, I wanted to follow some best practices in process such as futher segmenting the data into training, testing and evaluation sets. I've added another step of evalution and prediting the output.\n"},{"metadata":{},"cell_type":"markdown","source":"## Analysis\n\n"},{"metadata":{},"cell_type":"markdown","source":"#### Load Libraries\nThis first step is identifying and loading all the libraries for this project. This included essentials such as numpy and pandas, as well as machine learning specific libraries in keras and sklearn. "},{"metadata":{"_kg_hide-input":false,"_uuid":"875b42ec5baee5274279d8a7b7a72159f3a586de","trusted":true},"cell_type":"code","source":"\n#setting up all the essentials\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport matplotlib.image as mpimg\n\n\n#machine learning libraries \nfrom keras.models import Sequential, Model\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras import regularizers, optimizers\nfrom keras.applications.mobilenet_v2 import MobileNetV2\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\n\nimport os # accessing directory structure","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Importing Data\n\nNext, was setting up the data for further use. There was two folders provided in the Fashion Product dataset. One was the training data and the other was the testing data. This next set of code imports the CSV file. Since each image file name is connected by the row ID of the csv, creating a new column with the ID and .jpg extention makes it easily access in future code. This was done to both training and test datasets. Then we view each table to see that it's correct, as well as what other information this dataset has."},{"metadata":{"_kg_hide-input":false,"_uuid":"7c96d605282a65cebf83737bbf0a3386c5c3f19e","trusted":true},"cell_type":"code","source":"#import and setup training dataset\nTRAINING_DATASET_PATH = \"/kaggle/input/myntradataset/\"\ntraining_data = pd.read_csv(TRAINING_DATASET_PATH + \"styles.csv\", error_bad_lines=False)\ntraining_data['image'] = training_data.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\ntraining_data = training_data.sample(frac=1).reset_index(drop=True)\ntraining_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f92cc76567188bdfe9dfa0d720d163f2702ab4fb"},"cell_type":"code","source":"#import and setup test dataset\nTEST_DATASET_PATH = \"/kaggle/input/\"\ntest_data = pd.read_csv(TEST_DATASET_PATH + \"styles.csv\", nrows=128, error_bad_lines=False)\ntest_data['image'] = test_data.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\ntest_data = test_data.sample(frac=1).reset_index(drop=True)\ntest_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Image Pre-Processing\nIn this section, we pre-process the images to be ready for the neural network. \n\nAs if our dataset wasn't big enough, a very common practice is to create additonal images for training through data augmentation. We generate images by rotation and flipping them. This gives the model additional training material to increase the accuracy and general makes for a better model.\n\nWhile we already have a training and test dataset, we are going to further split the training set into two segements: training and validation. There have been various school's of thought in the idea ratio, 80/20, 70/30, etc, however it seems that it really depends on your data and finding the best fit, so we're going to start with 80/20. The rational between having these 3 segements is to have one for training, one for validating the model and another for further evalation on how this model world work on brand new images.   \n\nLastly, we are going to identify what the images are going to be trained on. For starting out, I've indicated the masterCategory column as the class that they will training for. The masterCategory has 5 broad fashion product types: Accessories, Apparel, Footwear, Free Items, and Personal Care. I thought this would be easiest as a starting point since there aren't too many categories and they are distinct categories, as opposed to other columns in the dataset that may be more ambiguous and harder to train. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#create additional images for training, while splitting dataset into two datasets for training and validation data\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255,\n        validation_split=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\n#process training dataset images\ntraining_generator = train_datagen.flow_from_dataframe(\n    dataframe=training_data,\n    directory=TRAINING_DATASET_PATH + \"images\",\n    x_col=\"image\",\n    y_col=\"masterCategory\",\n    target_size=(96,96),\n    batch_size=32,\n    subset=\"training\"\n)\n\n#process validation dataset images\nvalid_generator = train_datagen.flow_from_dataframe(\n    dataframe=training_data,\n    directory=TRAINING_DATASET_PATH + \"images\",\n    x_col=\"image\",\n    y_col=\"masterCategory\",\n    target_size=(96,96),\n    batch_size=32,\n    subset=\"validation\"\n)\n\n#process testing dataset images\ntest_datagen = ImageDataGenerator(rescale=1./255)\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=test_data,\n    directory=TEST_DATASET_PATH + \"images\",\n    x_col=\"image\",\n    y_col=\"masterCategory\",\n    target_size=(96,96),\n    batch_size=32\n)\n\n#create classes (categories to be sorted into)\nclasses = len(training_generator.class_indices)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Build the model\n\nOnce all images have been prepped, its time to build and train the model. "},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"# # create the base pre-trained model\nbase_model = MobileNetV2(input_shape=(96, 96, 3), include_top=False, weights='imagenet')\n\n# add a global spatial average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\npredictions = Dense(classes, activation='softmax')(x)\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#training the model\nmodel.fit_generator(\n    generator=training_generator,\n    steps_per_epoch=training_generator.n/training_generator.batch_size,\n\n    validation_data=valid_generator,\n    validation_steps=valid_generator.n/valid_generator.batch_size,\n\n    epochs=3 #the number of times to repeat training for higher accuracy\n)\n\n#save model to file\nmodel.save('/kaggle/working/model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluating the model\nNow that the model has been built and trained. This next section takes the model and evaluates it against the test dataset. The method returns scalar test loss and will print out the model's loss and accuracy. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate model with validation data set\nSTEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n\nevaluation = model.evaluate_generator(generator=valid_generator,\nsteps=STEP_SIZE_TEST)\n\n# Print out final values of all metrics\nkey2name = {'acc':'Accuracy', 'loss':'Loss', \n    'val_acc':'Validation Accuracy', 'val_loss':'Validation Loss'}\nresults = []\nfor i,key in enumerate(model.metrics_names):\n    results.append('%s = %.2f' % (key2name[key], evaluation[i]))\nprint(\", \".join(results))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To get an idea of what the loss and accuracy looks like through the training process, these numbers are plotted in the following graph. "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,5))\n\n# Plot loss function\nplt.subplot(222)\nplt.plot(history.history['loss'],'bo--', label = \"loss\")\nplt.plot(history.history['val_loss'], 'ro--', label = \"val_loss\")\nplt.title(\"train_loss vs val_loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epochs\")\n\n# Plot accuracy\nplt.subplot(221)\nplt.plot(history.history['acc'],'bo--', label = \"acc\")\nplt.plot(history.history['val_acc'], 'ro--', label = \"val_acc\")\nplt.title(\"train_acc vs val_acc\")\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"epochs\")\nplt.legend()\n\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With pretty low loss and high accuracy, I think the model looks pretty good to move onto the next step! "},{"metadata":{},"cell_type":"markdown","source":"#### Predicting Output\nFor a more practical use of this model, it's time to test out how well it makes predictions. When given an image, can it successfully assign it the correct category? In this section, we'll go through the list test dataset and use the model assign what it thinks the category should be then export a csv file comparing what the correct caterization should have been according to the original dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#reset the test generator to make sure the order is correct \ntest_generator.reset()\n\n#make predictions\npred=model.predict_generator(test_generator,\nsteps=STEP_SIZE_TEST,\nverbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#map out the indices to the category names to make more legible \npredicted_class_indices=np.argmax(pred,axis=1)\n\nlabels = (training_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\nlabels\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compare the prediction to the original test dataset\nfilenames=test_generator.filenames\nactual = test_data['masterCategory']\n\nmatch = []\ncount = 0\ncorrect = 0\nfor x in predictions: \n    if predictions[count] == actual[count]:\n        match.append(\"True\")\n        correct = correct + 1\n    else: \n        match.append(\"False\")\n    count = count + 1\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results=pd.DataFrame({\"Filename\":filenames,\n                      \"Predictions\":predictions,\n                      \"Actual\":actual,\n                      \"Correct\":match})\n\n#expore results to csv and print accuracy\nresults.to_csv(\"results.csv\",index=False)\nprint(\"Number Predictions Correct: \" + str(correct) + \" out of \" + str(count) + \" Percent Correct: \" + str(correct/count))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c9244be20f6bcc2228d0da2584371b11d1b48dc"},"cell_type":"markdown","source":"## Conclusion\n\nI was able to create a model that had pretty low loss and high accuracy. I am surprised to see that when making predictions, which would be the primary use case, it only had ~37% accuracy. I think that I most likely over fitted my model - where it was very accurate in the training and validation dataset, but it was not generalizable enough for brand new data. I tried to change the training data where originally I only pulled first 5000 entries for managable coding to train it with the entire ~40k, but it still had pretty low correct prediction rates. I also tried reducing the number of epoch (from originally 10) to see if that would help with the generalizability but that also did very little. If I had more time, I would definately be exploring how to tweek the model for better prediction accuracy. \n\nOnce the model is able to predict with better accuracy, I can see this further adapted in multiple applications, especially if the model was trained on multiple categories that this dataset provides. An example would be if given an image of a product, it would identify its category and attributes. Based on those, it could recommend similar products as replacements or even give complimentary items to create a full outfit.  \n"}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}