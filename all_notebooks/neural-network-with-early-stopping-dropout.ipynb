{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Description:\n### Using predictive models can be benefial to the healthcare industry. New technologies, such as deep neural network, can help healthcare professionals to predict cells, if they were maglignant or benign. It takes years of training for physicians to be able to diagnose cancer. Having a reliable predictive model can positively impact the efficiency of healthcare. By no means can the model replace physicians, predictive models can help physicians to prioritize their workload and focus on high-risk patients. This interesting project will use deep neural network with early stopping and dropout on the cancer dataset. ","metadata":{}},{"cell_type":"markdown","source":"# Project Objective:\n### To develop a predictive model using deep neural network to predict if the cell is malignant or benign. The target feature or the y-variable is \"benign_0__mal_1\". ","metadata":{}},{"cell_type":"markdown","source":"# Process:\n### This interesting project will start off with basic descriptive analysis, followed by exploratory data analysis, data visualization, preprocessing the data using a scaler, setting up train & test data, finally fit the data into the model. This project will be concluded with metrics, such as classification report and confusion matrix to evaluate the model's performance. ","metadata":{}},{"cell_type":"markdown","source":"# Potential Impact\n### Potential positive impact included healthcare resouce allocation strategy, improve efficiency and the process of diagnosing a patient, and help physicians to prioritize their workload. Early recognition and diagnosis can significantly improve the chance of survival and recovery. ","metadata":{}},{"cell_type":"markdown","source":"### Importing libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/cancer-classification/cancer_classification.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### An overview of the type of data","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Basic descriptive analysis","metadata":{}},{"cell_type":"code","source":"data.describe().transpose()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,7))\nsns.set_context(\"paper\", font_scale=1.5)\nsns.countplot(\"benign_0__mal_1\", data=data, color=\"pink\")\nplt.title(\"Malignancy vs Benign\")\nplt.xlabel(\"Malignancy - No:0, Yes:1\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A quick glance at the correlations between features","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,7))\nsns.set_context(\"paper\")\nsns.heatmap(data.corr(), cmap=\"coolwarm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,8))\ndata.corr()[\"benign_0__mal_1\"].sort_values().plot(kind=\"bar\", color=\"pink\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Getting ready for modeling. Setting X variable and y variable for train and test data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dropping the \"benign_0__mal_\" in X variable as it is the target feature in this predictive model. ","metadata":{}},{"cell_type":"code","source":"X = data.drop(\"benign_0__mal_1\", axis=1).values\ny = data[\"benign_0__mal_1\"].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test size is set at 25% of the data. Random state will be used so the random sequence will be the same each time. It is set at 101, which is an arbitrary number. ","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing the data using scaler. ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = MinMaxScaler()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = scaler.fit_transform(X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = scaler.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing from tensorflow.keras libraries ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense,Dropout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Since this is a small dataset, 30 neurons will be used and then reduced to half for the second layer. Rectified linear unit will be used as the activation and Sigmoid as the final layer. Binary_crossentropy will be used as loss function as this is a binary classification. Adam will be used, which is the common optimizer. ","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(30, activation=\"relu\"))\nmodel.add(Dense(15, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x=X_train, y=y_train,epochs=600, validation_data=(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing the loss values","metadata":{}},{"cell_type":"code","source":"losses = pd.DataFrame(model.history.history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"losses.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adding early stopping to the model","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(30, activation=\"relu\"))\nmodel.add(Dense(15, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=2, patience=25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x=X_train, y=y_train,epochs=600, validation_data=(X_test, y_test), callbacks=[early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loss = pd.DataFrame(model.history.history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### With early stopping added to the model, the validation losses is better than the previous","metadata":{}},{"cell_type":"code","source":"model_loss.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Adding dropput setting at 0.5, which is arbitrary.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dropout","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Dense(30, activation=\"relu\"))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(15, activation=\"relu\"))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x=X_train, y=y_train,epochs=600, validation_data=(X_test, y_test), callbacks=[early_stop])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loss = pd.DataFrame(model.history.history)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The graph below shows the significant impact on validation loss when early stopping and dropout included","metadata":{}},{"cell_type":"code","source":"model_loss.plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict_classes(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, prediction))\nprint(confusion_matrix(y_test, prediction))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}