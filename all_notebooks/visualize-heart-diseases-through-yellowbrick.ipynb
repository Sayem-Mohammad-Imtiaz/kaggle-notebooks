{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://www.incimages.com/uploaded_files/image/970x450/getty_584203352_200013282000928014_380350.jpg)"},{"metadata":{},"cell_type":"markdown","source":"**Note:**  \nKindly upvote the kernel if you find it useful. Suggestions are always welome. Let me know your thoughts in the comment if any.\n\n**Reference:**  \n[Analyzing Machine Learning Models with Yellowbrick by \nParul Pandey](https://heartbeat.fritz.ai/analyzing-machine-learning-models-with-yellowbrick-37795733f3ee)"},{"metadata":{},"cell_type":"markdown","source":"**Context**  \nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.  \n\n**Attribute Information:**  \n* Age\n* Sex\n* Chest pain type (4 values)\n* Resting blood pressure\n* Serum cholestoral in mg/dl\n* Fasting blood sugar > 120 mg/dl\n* Resting electrocardiographic results (values 0,1,2)\n* Maximum heart rate achieved\n* Exercise induced angina\n* Oldpeak = ST depression induced by exercise relative to rest\n* The slope of the peak exercise ST segment\n* Number of major vessels (0-3) colored by flourosopy\n* Thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values. One file has been \"processed\", that one containing the Cleveland database. All four unprocessed files also exist in this directory.  \n\n**Acknowledgements - Creators:**  \n* Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.  \n* University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.  \n* University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.  \n* V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.  \n* Donor: David W. Aha (aha '@' ics.uci.edu) (714) 856-8779  \n\n**Inspiration**  \nExperiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0)."},{"metadata":{},"cell_type":"markdown","source":"**About Yellowbrick**  \nThe Yellowbrick library is a diagnostic visualization platform for machine learning that allows data scientists to steer the model selection process and assist in diagnosing problems throughout the machine learning workflow. In short, it tries to find a model described by a triple composed of features, an algorithm, and hyperparameters that best fit the data.  \n\nYellowbrick is an open source, Python project that extends the scikit-learn API with visual analysis and diagnostic tools. The Yellowbrick API also wraps matplotlib to create interactive data explorations.  \n\nIt extends the scikit-learn API with a new core object: the Visualizer. Visualizers allow visual models to be fit and transformed as part of the scikit-learn pipeline process, providing visuals throughout the transformation of high-dimensional data.\n\n**Advantages**  \nYellowbrick isn’t a replacement for other data visualization libraries but helps to achieve the following:  \n* Model Visualization  \n* Data visualization for machine learning  \n* Visual Diagnostics  \n* Visual Steering  \n\nFor additional information on Yellowbrick visit the below link:  \n[Yellowbrick](https://www.scikit-yb.org/en/latest/)"},{"metadata":{},"cell_type":"markdown","source":"**Global Options**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Listing the files**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"!ls ../input/","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Reading the Dataset**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nht_dt = pd.read_csv(\"../input/heart.csv\", header = 'infer')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Viewing the shape of the dataset**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print(\"The heart dataset has {0} rows and {1} columns\".format(ht_dt.shape[0], ht_dt.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sample of the dataset**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"ht_dt.head()  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Specifying the feature and target column**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"feature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n                 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n\ntarget_name = 'target'\n\nX = ht_dt[feature_names]\ny = ht_dt[target_name]\n\nprint(\"Features of the dataset are {0}\".format(X.columns.values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Analysis in Yellowbrick**  \nThe Yellowbrick feature analysis visualizers focus on aggregation, optimization, and other techniques to give overviews of the data.  \n\nFeature analysis visualizers implementation includes\n* Rank Features\n* Manifold visualization\n* Radviz Visualizer\n* Feature Importance\n* Parallel coordinates\n* Recursive feature elimination\n* PCA Projection\n* Joint Plots\n\n "},{"metadata":{},"cell_type":"markdown","source":"**Rank Features**  \nRank Features rank single and pairs of features to detect covariance. Ranking can be 1D or 2D depending on the number of features utilized for ranking. \n\n**Rank 1D**  \nRank 1D utilizes a ranking algorithm that takes into account only a single feature at a time. By default, the Shapiro-Wilk algorithm is used to assess the normality of the distribution of instances with respect to the feature."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from yellowbrick.features import Rank1D\n# Instantiate the 1D visualizer with the Sharpiro ranking algorithm\nvisualizer = Rank1D(features=feature_names, algorithm='shapiro')\n\n# Fit the data to the visualizer\nvisualizer.fit(X, y)  \n\n# Transform the data\nvisualizer.transform(X) \n\n# visualise\nvisualizer.poof()                   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Rank 2D**  \nRank 2D, on the other hand, performs pairwise feature analysis as a heatmap. The default ranking algorithm is covariance, but we can also use the Pearson score."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from yellowbrick.features import Rank2D\n# covariance\nvisualizer = Rank2D(features=feature_names, algorithm='covariance') \nvisualizer.fit(X, y)                \nvisualizer.transform(X)             \nvisualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#pearson\nvisualizer = Rank2D(features=feature_names, algorithm='pearson')\nvisualizer.fit(X, y)                \nvisualizer.transform(X)             \nvisualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RadViz**  \nRadViz is a multivariate data visualization algorithm that plots each feature dimension uniformly around the circumference of a circle and then plots data points on the interior of the circle. This allows many dimensions to easily fit on a circle, greatly expanding the dimensionality of the visualization.  "},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#Feature set\nfeat_1 = ['age', 'trestbps', 'chol', 'thalach']    \nfeat_2 = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n\nfrom yellowbrick.features import RadViz\n# Specify the features of interest and the classes of the target \nfeatures = feat_1\nclasses = [0, 1]\n\n# Instantiate the visualizer\nvisualizer = RadViz(classes=classes, features=features,size = (800,300))\nvisualizer.fit(X, y)      \nvisualizer.transform(X)  \nvisualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Specify the features of interest and the classes of the target \nfeatures = feat_2\nclasses = [0, 1]\n\n# Instantiate the visualizer\nvisualizer = RadViz(classes=classes, features=features,size = (800,300))\nvisualizer.fit(X, y)      \nvisualizer.transform(X)  \nvisualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Parallel Coordinates**  \nThis technique is useful when we need to detect clusters of instances that have similar classes, and to note features that have high variance or different distributions. Points that tend to cluster will appear closer together."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from yellowbrick.features import ParallelCoordinates\nclasses = [0, 1]\n# Instantiate the visualizer for feat_1\nvisualizer = visualizer = ParallelCoordinates(\n    classes=classes, features=feature_names,\n    normalize='standard', size = (1200,500))\n\nvisualizer.fit(X, y)     \nvisualizer.transform(X)   \nvisualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Parallel coordinates is a visualization technique used to plot individual data elements across many dimensions. Each of the dimensions corresponds to a vertical axis, and each data element is displayed as a series of connected points along the dimensions/axes.  \n\nThe groups of similar instances are called ‘braids’, and when there are distinct braids of different classes, it suggests there’s enough separability that a classification algorithm might be able to discern between each class.  "},{"metadata":{},"cell_type":"markdown","source":"**Model Evaluation Visualizers**  \nModel evaluation signifies how well the values predicted by the model match the actual labeled ones. Yellowbrick has visualizers for classification, regression, and clustering algorithms.  \n\n**Evaluating Classifiers**  \nClassification models try to assign the dependent variables one or more categories. The sklearn.metrics module implements a function to measure classification performance.  \n\n![Classifiers Metrics](https://cdn-images-1.medium.com/max/1600/1*U35S7hZqKSZ8DZlxcTl1Bg.png)  \n\nYellowbrick implements has 7 classifier evaluation metrics.\n* ROCAUC\n* Class Prediction Error\n* Discrimination Error\n* Class Balance\n* Confusion Matrix\n* Classification Report\n* Precision - Recall Curves"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Classifier Evaluation Imports\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n#Yellowbrick\nfrom yellowbrick.classifier import ClassificationReport,ConfusionMatrix\n\n#Training & Test dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Classification Report**  \nThe classification report visualizer displays the precision, recall, and F1 scores for the model.  \n\n* precision = true positives / (true positives + false positives)\n* recall = true positives / (false negatives + true positives)\n* F1 score = 2 * ((precision * recall) / (precision + recall))  \n\nLet's try to visualize the classification report for 2 model's and decide which is better."},{"metadata":{},"cell_type":"markdown","source":"**Classification report using Gaussian NB**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Instantiate the classification model and visualizer \nbayes = GaussianNB()\nvisualizer = ClassificationReport(bayes, classes=classes)\nvisualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)  \ng = visualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Classification report using Logistic Regression**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"bayes = LogisticRegression()\nvisualizer = ClassificationReport(bayes, classes=classes)\nvisualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)  \ng = visualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visual classification reports are used to compare classification models to select models that are **“redder”**, e.g. have stronger classification metrics or that are more balanced."},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**  \nThe ConfusionMatrix visualizer displays the accuracy score of the model, i.e. it shows how each of the predicted classes compares to their actual classes. Let’s check out the confusion matrix for the Logistic Regression Model."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"logReg = LogisticRegression()\nvisualizer = ConfusionMatrix(logReg)\nvisualizer.fit(X_train, y_train)  \nvisualizer.score(X_test, y_test)\ng = visualizer.poof()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Will try the Evaluating Regressors on a seperate kernel**  \n\n**Stay connected**  \n\n**Happy Learning**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}