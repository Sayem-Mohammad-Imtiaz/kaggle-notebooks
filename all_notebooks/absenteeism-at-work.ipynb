{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nimport imageio\n\nfrom pandas_profiling import ProfileReport\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file = os.path.join(dirname, filename)\n        \n\nfrom sklearn.svm import SVR, SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nfrom sklearn.tree import plot_tree\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nfrom imblearn.over_sampling import SMOTE\n\nDO_PCA = False\nDO_PLOT = True\nDO_CLASSIFY = True\n\ndf = pd.read_csv(file)\ndf.head()\n\ndef plot_confusion_matrix(test,pred):\n    matrix = confusion_matrix(y_test, y_pred)\n    plt.matshow(matrix)\n    plt.title('Confusion matrix')\n    plt.colorbar()\n    plt.ylabel('GroundTruth')\n    plt.xlabel('Predicted')\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"df.columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Social drinker'] = df['Social drinker'].astype('int')\ndf['Social smoker'] = df['Social smoker'].astype('int')\ndf['Disciplinary failure'] = df['Disciplinary failure'].astype('int')\ndf['Seasons'] = df['Seasons'].astype('category')\ndf['Education'] = df['Education'].astype('category')\ndf['Day of the week'] = df['Day of the week'].astype('category')\ndf['Month of absence'] = df['Month of absence'].astype('category')\ndf['Reason for absence'] = df['Reason for absence'].astype('category')\ndf[\"Work load Average/day\"] = df[\"Work load Average/day \"]\n\ndf = df.drop([\"ID\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot_encode(df, features):\n    for feature in features:\n        df = pd.concat([df,pd.get_dummies(df[feature], \n                                      prefix=feature)],axis=1).drop([feature],axis=1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df['Month of absence'].value_counts())\ndf = df[df['Month of absence'] != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df[\"is_weekend\"] = df[\"Day of the week\"].map (lambda val: 1 if val==6 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#g = sns.FacetGrid(data=df,col='Day of the week')\n#g.map(plt.hist,'Absenteeism time in hours')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if DO_PLOT:\n#    sns.pairplot(df.select_dtypes(exclude='bool'), vars=[\"Absenteeism time in hours\", \"Month of absence\", \"Reason for absence\"], height=4, aspect=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#if DO_PLOT:\n#    with sns.axes_style(\"white\"):\n#        sns.jointplot(x=\"Absenteeism time in hours\", y=\"Reason for absence\", data=df);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#if DO_PLOT:\n#    cat_cols = df.select_dtypes(include='category').columns\n#    fig, axes = plt.subplots(1, len(cat_cols))\n\n#    for i, col in enumerate(cat_cols):\n#        sns.countplot(y=col, orient=\"h\", data=df, ax=axes[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#profile = ProfileReport(df, title=\"Pandas Profiling Report\")\n#profile.to_notebook_iframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#profile.to_file(\"report.html\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_PLOT:\n\n    age_abs = df.groupby('Age')[['Absenteeism time in hours']].mean()\n    ax = age_abs.plot(kind='bar', figsize=(8,6), legend=False)\n    for i, v in enumerate(age_abs.values):\n        ax.text(i-.25, v + 1, str(np.int(np.round(v))), color='red')\n    ax.set_ylabel('Absenteeism time in hours')\n    #ax.set_title('Average Absenteeism time in hours by age')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_PLOT:\n\n    dis_abs = df.groupby('Distance from Residence to Work')[['Absenteeism time in hours']].mean()\n    ax = dis_abs.plot(kind='bar', figsize=(8,6), legend=False)\n    for i, v in enumerate(dis_abs.values):\n        ax.text(i-.25, v + 1, str(np.int(np.round(v))), color='red')\n    ax.set_xlabel('Distance from Residence to Work (km)')\n    ax.set_ylabel('Absenteeism time in hours')\n    #ax.set_title('Average Absenteeism time in hours by distance')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_PLOT:\n    fig, ax = plt.subplots(figsize=(11,11))\n    sns.heatmap(df.corr(), square = True, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_dpi=96\nplt.figure(figsize=(480/my_dpi, 480/my_dpi), dpi=my_dpi)\n \npca = PCA(n_components=3)\npca.fit(df)\n \n# Store results of PCA in a data frame\nresult = pd.DataFrame(pca.transform(df), columns=['PCA%i' % i for i in range(3)], index=df.index)\n\nangle = 70\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(result['PCA0'], result['PCA1'], result['PCA2'], cmap=\"Set2_r\", s=60)\n\n# make simple, bare axis lines through space:\nxAxisLine = ((min(result['PCA0']), max(result['PCA0'])), (0, 0), (0,0))\nax.plot(xAxisLine[0], xAxisLine[1], xAxisLine[2], 'r')\nyAxisLine = ((0, 0), (min(result['PCA1']), max(result['PCA1'])), (0,0))\nax.plot(yAxisLine[0], yAxisLine[1], yAxisLine[2], 'r')\nzAxisLine = ((0, 0), (0,0), (min(result['PCA2']), max(result['PCA2'])))\nax.plot(zAxisLine[0], zAxisLine[1], zAxisLine[2], 'r')\n\nax.view_init(30,angle)\n\n# label the axes\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_zlabel(\"PC3\")\n#ax.set_title(\"PCA\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pca_expvar(pca):\n    \"\"\"Plot the variance explained by the given PCA model \n    as a function of the number of principal components.\n    \"\"\"\n    plt.figure(figsize=(8, 4))\n    variance_ratio = pca.explained_variance_ratio_\n    xticks = np.arange(pca.n_components_) + 1\n    #plt.title('Explained variance')\n    plt.grid(True)\n    plt.xticks(xticks)\n    plt.ylim(0, 1)\n    plt.ylabel('Ratio')\n    plt.plot(xticks, variance_ratio, label='Explained variance')\n    plt.plot(xticks, np.cumsum(variance_ratio), label='Cumulative')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=3)\nplot_pca_expvar(pca.fit(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = one_hot_encode(df, [\"Reason for absence\", \"Seasons\", \"Month of absence\", \"Day of the week\", \"Education\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_normalize = [\"Transportation expense\", \"Distance from Residence to Work\", \n                \"Service time\", \"Work load Average/day\", \"Weight\", \"Height\", \"Body mass index\"]\n\n\nfor column in to_normalize:\n    x = df[column].values \n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x.reshape(-1,1))\n    df[column] = x_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#bins = [25, 35, 45, 55, np.inf]\n#names = [25, 35, 45, 55]\n#df['Age'] = pd.cut(df['Age'], bins, labels=names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_CLASSIFY:\n    \n        abs_bins = [-1, 0, 10, np.inf]\n        abs_names = ['0', '0-10', '>10']\n        \n        df[\"Absenteeism time in hours\"] = pd.cut(df[\"Absenteeism time in hours\"], abs_bins ,include_lowest=True, labels=abs_names)\ndf[\"Absenteeism time in hours\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df[\"Absenteeism time in hours\"]\nif not DO_PCA:\n    X = df.drop([\"Absenteeism time in hours\"], axis=1)\n\nelse:\n    pca = PCA(n_components=3)\n    X = pca.fit_transform(df.drop([\"Absenteeism time in hours\"], axis=1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_CLASSIFY:\n    smote = SMOTE(sampling_strategy='auto',random_state=0)\n    X,y = smote.fit_sample(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    axes : array of 3 axes, optional (default=None)\n        Axes to use for plotting the curves.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    #axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    #axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    #axes[2].set_title(\"Performance of the model\")\n\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\n\nif DO_CLASSIFY:\n\n    clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n    scores = cross_val_score(clf, X, y, cv=3)\n\n    plt.plot(range(1,4),scores)\n    plt.xticks(range(1,4))\n    plt.grid()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nclf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nplot_confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_CLASSIFY:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    clf.fit(X_train, y_train)\n    print(\"Accuracy: \", accuracy_score(y_test, clf.predict(X_test)))\n\n    ax = (pd.Series(clf.feature_importances_, index=X.columns)\n           .nlargest(19)\n           .plot(kind='barh', figsize=(8,6), color='lightgreen'))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_CLASSIFY:\n    clf.fit(X,y)\n\n    fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)\n    plot_tree(clf.estimators_[0], feature_names = X.columns, filled=True)\n    fig.savefig('imagename.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if DO_CLASSIFY:\n    svc = SVC(C=1.0,kernel='rbf')\n    scores = cross_val_score(svc, X, y, cv=3)\n    plt.plot(range(1,4),scores)\n    plt.xticks(range(1,4))\n    plt.grid()\n    plot_learning_curve(svc, \"title\", X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nif DO_CLASSIFY:\n    nb = GaussianNB(priors=None)\n    scores = cross_val_score(nb, X, y, cv=3)\n    plt.plot(range(1,4),scores)\n    plt.xticks(range(1,4))\n    plt.grid()\n    plot_learning_curve(nb, \"NB\", X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nif DO_CLASSIFY:\n    knn  = KNeighborsClassifier(n_neighbors=3)\n    scores = cross_val_score(knn, X, y, cv=3)\n    plt.plot(range(1,4),scores)\n    plt.xticks(range(1,4))\n    plt.grid()\n    plot_learning_curve(knn, \"KNN\", X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\n\nif DO_CLASSIFY:\n    lr  = LogisticRegression(random_state=0, max_iter=1000)\n    scores = cross_val_score(lr, X, y, cv=3)\n    plt.plot(range(1,4),scores)\n    plt.xticks(range(1,4))\n    plt.grid()\n    plot_learning_curve(lr, \"LR\", X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\n\nif not DO_CLASSIFY:\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    clf = SVR(C=1.0)\n    clf.fit(X_train,y_train)\n    \n    y_test = clf.predict(X_test)\n    print(\"MSE: \", mean_squared_error(y_test, y_pred))\n    print(\"R2 Score: \", r2_score(y_test, y_pred))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not DO_CLASSIFY:\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    reg = GradientBoostingRegressor(random_state=0)\n    scores = cross_val_score(reg, X, y, cv=5)\n    print(scores)\n\n    reg.fit(X_train, y_train)\n    print(r2_score(y_test, reg.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not DO_CLASSIFY:\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    reg = RandomForestRegressor(n_estimators=100, random_state=0)\n    scores = cross_val_score(reg, X, y, cv=5)\n    print(scores)\n\n    reg.fit(X_train, y_train)\n    print(r2_score(y_test, reg.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import  VotingClassifier\n\nrf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\nlr  = LogisticRegression(random_state=0, max_iter=1000)\nlda =  LinearDiscriminantAnalysis()\n\neclf1 = VotingClassifier(estimators=[('lr', lr), ('rf', rf), ('lda', lda)], voting='hard')\neclf1 = eclf1.fit(X_train, y_train)\naccuracy_score(y_test,eclf1.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curve(eclf1, \"ensemble\", X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}