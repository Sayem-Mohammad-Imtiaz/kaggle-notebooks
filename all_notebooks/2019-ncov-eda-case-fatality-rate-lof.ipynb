{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 2019 Novel Corona Virus Outbreak: EDA with estimation of Case Fatality Rate (CFR)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's read the data and get a glimpse of how it looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"ncov = pd.read_csv('/kaggle/input/novel-corona-virus-2019-dataset/2019_nCoV_data.csv')\nprint(ncov.head(7))\nprint('\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ncov.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The data has 434 rows and 7 columns. The first column 'Sno' doesn't appear to be informative. So we will remove it. Also, let's rename the columns to something that doesn't have any special characters or spaces. Also, the column 'Last Update' is a date variable, but its not in the proper datetime format and so let's convert it to proper datetime format as well"},{"metadata":{"trusted":true},"cell_type":"code","source":"ncov = ncov.drop('Sno', axis = 1)\nncov.columns = ['State', 'Country', 'Date', 'Confirmed', 'Deaths', 'Recovered']\nncov['Date'] = ncov['Date'].apply(pd.to_datetime).dt.normalize() #convert to proper datetime object","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ncov.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ncov[['State','Country','Date']].drop_duplicates().shape[0] == ncov.shape[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### The above code simply checks if the number of distinct rows of the dataset containing only the three columns State, Country and Date is equal to the number of rows of the parent dataset. This check returned 'True' and it means that every row in the dataset is unique per each Country per State per Date. This is important information to have while performing grouping and aggregating operations"},{"metadata":{},"cell_type":"markdown","source":"### Now let's view some summary statistics of all the 6 columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"ncov.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From the above table, we know that there are missing values for 'State', but rest all columns doesn't contain any missing values. Let's explore 'State' variable to see if its missing is something dubious"},{"metadata":{"trusted":true},"cell_type":"code","source":"ncov[['Country','State']][ncov['State'].isnull()].drop_duplicates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ncov[ncov['Country'].isin(list(ncov[['Country','State']][ncov['State'].isnull()]['Country'].unique()))]['State'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ok, it looks the 'State' names are missing for certain nations only. In case of Australia, there are records where 'State' is recorded and there are also records where Australia's 'State' name is missing. The plausible reason could be that the epicenter of 2019-nCoV outbreak is China and hence data is likely to be more complete from China. Nonetheless, the missingness of 'State' doesn't appear to be something that's dubious. "},{"metadata":{"trusted":true},"cell_type":"code","source":"ncov.State.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ncov.Country.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Looking at the list of countries, there appear to be two values that sounds conflicting. These are 'China' and 'Mainland China'. These two entities are supposed to be representing a single country, but in the data it's two different entities. Let's see what's going on.."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ncov[ncov['Country'].isin(['China', 'Mainland China'])].groupby('Country')['State'].unique())\nprint(ncov[ncov['Country'].isin(['China', 'Mainland China'])].groupby('Country')['Date'].unique())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### It looks like the value of 'Country' is set to 'China' for observations on 22 Jan 2020 and from 23 Jan 2020 onwards its recorded as 'Mainland China'. Both the entities have same distinct states as well. This means that they both represent 'China', a single entity. We should correct the name or else having them as two different values could cause trouble in the EDA downstream"},{"metadata":{"trusted":true},"cell_type":"code","source":"ncov['Country'] = ncov['Country'].replace(['Mainland China'], 'China') #set 'Mainland China' to 'China'\nsorted(ncov.Country.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ncov.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"china = ncov[ncov['Country']=='China']\nchina.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.rcParams[\"figure.figsize\"] = (7,5)\nax1 = china[['Date','Confirmed']].groupby(['Date']).sum().plot()\nax1.set_ylabel(\"Total Number of Confirmed Cases\")\nax1.set_xlabel(\"Date\")\n\nax2 = china[['Date','Deaths', 'Recovered']].groupby(['Date']).sum().plot()\nax2.set_ylabel(\"Total N\")\nax2.set_xlabel(\"Date\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The above two line plots is looking only at the data from 'China' to explore the trends of 2019-nCoV outbreak. The number of confirmed cases are on the rise. However, there doesn't seem to be any explosion in the number of confirmed infections in China with time. A similar trend is seen with respect to the number of 'deaths' and 'recovered' cases as well. Number of people who is dying from 2019-nCoV is rising day by day with no plateau reached as of 30 Jan 2020. The number of people who are 'recovering' from the infection is also increasing day by day."},{"metadata":{},"cell_type":"markdown","source":"### Total confirmed cases per each state/province in China"},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (17,10)\nnums = china.groupby([\"State\"])['Confirmed'].aggregate(sum).reset_index().sort_values('Confirmed', ascending= False)\nax = sns.barplot(x=\"Confirmed\", y=\"State\", order = nums['State'], data=china, ci=None) \nax.set_xlabel(\"Total Confirmed Cases\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"markdown","source":"#### Looking at the barplot above, Hubei clearly has more confirmed cases than anywhere else in China. Wuhan is the capital of Hubei province. There also seems to be a province named 'Taiwan', this is likely an error (typo) in the data."},{"metadata":{},"cell_type":"markdown","source":"## Case Fatality Rate\nThe case fatality rate or CFR is the proportion of deaths among people diagnosed with a disease. This is a measure of the fatality and is a measure of the risk of death if you happen to have get infected with an infectious agent. For infectious agents like Ebola and Nipah this number is quite high. 2019-nCoV's CFR rates are estimated to be less than 5% in most literature. We could compute the same using this data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#a custom function to return the lower and upper bounds of 95% confidence interval of a proportion\ndef get_ci(N,p):\n    lci = (p - 1.96*(((p*(1-p))/N) ** 0.5))*100\n    uci = (p + 1.96*(((p*(1-p))/N) ** 0.5))*100\n    return str(np.round(lci,3)) + \"% - \" + str(np.round(uci,3)) + '%'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = ncov[ncov.Date==np.max(ncov.Date)]\nfinal = final.copy()\n\nfinal['CFR'] = np.round((final.Deaths.values/final.Confirmed.values)*100,3)\nfinal['CFR 95% CI'] = final.apply(lambda row: get_ci(row['Confirmed'],row['CFR']/100),axis=1)\nglobal_cfr = np.round(np.sum(final.Deaths.values)/np.sum(final.Confirmed.values)*100, 3)\nfinal.sort_values('CFR', ascending= False).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tops = final.sort_values('CFR', ascending= False)\ntops = tops[tops.CFR >0]\ndf = final[final['CFR'] != 0]\nplt.rcParams[\"figure.figsize\"] = (10,5)\nax = sns.barplot(y=\"CFR\", x=\"State\", order = tops['State'], data=df, ci=None) \nax.axhline(global_cfr, alpha=.5, color='r', linestyle='dashed')\nax.set_title('Case Fatality Rates (CFR) as of 30 Jan 2020')\nax.set_ylabel('CFR %')\nprint('Average CFR % = ' + str(global_cfr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#### The average CFR is represented by the dotted red line. The CFR of 2019-nCoV as of now is ~ 2-4%. Note that there are many infectious agents in the world whose CFR is many times higher than 2019-nCoV."},{"metadata":{},"cell_type":"markdown","source":"## Local Outlier Factor (LOF)\n> > In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.\n> https://en.wikipedia.org/wiki/Local_outlier_factor"},{"metadata":{},"cell_type":"markdown","source":"#### The numerical columns of the data with confirmed number of cases, number of deaths and number of recovered cases are the interesting variables here for outlier detection. Variables denoting geocordinates of each state could be a value addition, but we don't have that in the data. These variables are standardized and then inputted into the LOF algorithm to get LOF scores for each state. A LOF score in and around 1 is characteristic of an inlier, while a LOF score >>1 is typical for outliers "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\nscaler = StandardScaler()\nscd = scaler.fit_transform(final[['Confirmed','Deaths','Recovered']])\nclf = LocalOutlierFactor(n_neighbors=20, contamination=0.1) #LOF is very sensitive to the choice of n_neighbors. Generally, n_neighbors = 20 works better\nclf.fit(scd)\nlofs = clf.negative_outlier_factor_*-1\nfinal['LOF Score'] = lofs\ntops = final.sort_values('LOF Score', ascending= False)\nplt.rcParams[\"figure.figsize\"] = (20,12)\nax = sns.barplot(x=\"LOF Score\", y=\"State\", order = tops['State'], data=final, ci=None) \nax.axvline(1, alpha=.5, color='g', linestyle='dashed')\nax.axvline(np.median(lofs), alpha=.5, color='b', linestyle='dashed')\nax.axvline(np.mean(lofs) + 3*np.std(lofs), alpha=.5, color='r', linestyle='dashed')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Hubei is indeed in a league of its when it comes to nCoV. It's LOF score is 70, which is kind of astronomically high! The bar plot above has three dotted lines representing a LOF score of 1 (green), the median LOF score (blue) and the LOF score that's 3 standard deviations away (red). It is to be noted that LOF score does not translate to a bad epidemiological condition in that region and that it only represents whether that region is an outlier or not as compared to others. For example, New South Wales also has a very high LOF score (~12), but this was because 50% of the cases in New South Wales got recovered already."},{"metadata":{"trusted":true},"cell_type":"code","source":"final.sort_values('LOF Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## K-Means Clustering \nThe outlierness of Hubei is clearly seen in K-Means clustering too"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nplt.rcParams[\"figure.figsize\"] = (5,5)\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=1897)\n    kmeans.fit(scd)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1, 11), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within Cluster Sum of Squares')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### num_clusters = 2 seems to be the best choice by looking at the elbow analysis above"},{"metadata":{"trusted":true},"cell_type":"code","source":"kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10, random_state=1897)\nclusters = np.where(kmeans.fit_predict(scd) == 0, 'Cluster 1', 'Cluster 2')\nclusters","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Let's do Principal Component Analysis (PCA). This makes it easier to visualize high-dimensional data in 2D-space"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import decomposition\npca = decomposition.PCA(n_components=3)\npca.fit(scd)\nX = pca.transform(scd)\nprint(pca.explained_variance_ratio_.cumsum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### The first principal component itself explains ~99% of the variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (7,7)\nax = sns.scatterplot(X[:,0], X[:,1], marker = 'X', s = 80, hue=clusters)\nax.set_title('K-Means Clusters of States/Provinces')\nax.set_xlabel('Principal Component 1')\nax.set_ylabel('Principal Component 2')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Hubei is clustered into 'Cluster 2' and is quite far from any other states."},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(final.State.values, clusters)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}