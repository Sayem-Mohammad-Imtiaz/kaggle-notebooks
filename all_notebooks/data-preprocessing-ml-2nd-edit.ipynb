{"cells":[{"metadata":{"_uuid":"1460811827e23931d9559bb9d7d5c805e37ee4ea"},"cell_type":"markdown","source":"*Python Machine Learning 2nd Edition* by [Sebastian Raschka](https://sebastianraschka.com), Packt Publishing Ltd. 2017\n\nCode Repository: https://github.com/rasbt/python-machine-learning-book-2nd-edition\n\nCode License: [MIT License](https://github.com/rasbt/python-machine-learning-book-2nd-edition/blob/master/LICENSE.txt)"},{"metadata":{"_uuid":"a4a0278f54f9df4675f8d78477411aec1419a653"},"cell_type":"markdown","source":"# Python Machine Learning - Code Examples"},{"metadata":{"_uuid":"9cd76f9592284fc5268091bfd4add3ad921eaeaf"},"cell_type":"markdown","source":"# Chapter 4 - Building Good Training Sets – Data Preprocessing"},{"metadata":{"_uuid":"12f3b635cb5a3ceef017e3cdf060355b54b164db"},"cell_type":"markdown","source":"Note that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s)."},{"metadata":{"trusted":false,"_uuid":"f6b7000c5a8052583dc1f0a628d32c5aa0dbdbe5"},"cell_type":"code","source":"# %load_ext watermark\n# %watermark -a \"Sebastian Raschka\" -u -d -p numpy,pandas,matplotlib,sklearn","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6cc719b4ddc6bbdf30783800e0e166d39900b9bb"},"cell_type":"markdown","source":"*The use of `watermark` is optional. You can install this IPython extension via \"`pip install watermark`\". For more information, please see: https://github.com/rasbt/watermark.*"},{"metadata":{"_uuid":"80ed1fe1a1b9985cea2a4d62de86a5c50701e42a"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"_uuid":"1070ad923231bdfd006681d6361531e730986068"},"cell_type":"markdown","source":"### Overview"},{"metadata":{"_uuid":"2cf37ea4633e90da9fc2771a0a7e89dfe9540b0a"},"cell_type":"markdown","source":"- [Dealing with missing data](#Dealing-with-missing-data)\n  - [Identifying missing values in tabular data](#Identifying-missing-values-in-tabular-data)\n  - [Eliminating samples or features with missing values](#Eliminating-samples-or-features-with-missing-values)\n  - [Imputing missing values](#Imputing-missing-values)\n  - [Understanding the scikit-learn estimator API](#Understanding-the-scikit-learn-estimator-API)\n- [Handling categorical data](#Handling-categorical-data)\n  - [Nominal and ordinal features](#Nominal-and-ordinal-features)\n  - [Mapping ordinal features](#Mapping-ordinal-features)\n  - [Encoding class labels](#Encoding-class-labels)\n  - [Performing one-hot encoding on nominal features](#Performing-one-hot-encoding-on-nominal-features)\n- [Partitioning a dataset into a separate training and test set](#Partitioning-a-dataset-into-seperate-training-and-test-sets)\n- [Bringing features onto the same scale](#Bringing-features-onto-the-same-scale)\n- [Selecting meaningful features](#Selecting-meaningful-features)\n  - [L1 and L2 regularization as penalties against model complexity](#L1-and-L2-regularization-as-penalties-against-model-omplexity)\n  - [A geometric interpretation of L2 regularization](#A-geometric-interpretation-of-L2-regularization)\n  - [Sparse solutions with L1 regularization](#Sparse-solutions-with-L1-regularization)\n  - [Sequential feature selection algorithms](#Sequential-feature-selection-algorithms)\n- [Assessing feature importance with Random Forests](#Assessing-feature-importance-with-Random-Forests)\n- [Summary](#Summary)"},{"metadata":{"_uuid":"b7b54d041bfc108bbc5fbc26c5ef14f5cbab8c07"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"trusted":false,"_uuid":"cf058859b4702cd2926403c0d9ff622397fdcea2"},"cell_type":"code","source":"from IPython.display import Image\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73edf312e96bdf4658a300b923adbb78b19f5ca0"},"cell_type":"markdown","source":"# Dealing with missing data"},{"metadata":{"_uuid":"9f44b471c7c306aafb7754b02e5356bf012c79cc"},"cell_type":"markdown","source":"## Identifying missing values in tabular data"},{"metadata":{"trusted":true,"_uuid":"1fe80c5b8055c754e0f9969a95a798bc374e666e"},"cell_type":"code","source":"import pandas as pd\nfrom io import StringIO\nimport sys\nimport numpy as np\ndf = pd.DataFrame([[1,2,3,4],[5,6,np.nan,8],[10,11,12,np.nan]], columns=['A','B','C','D'])\n# '''A,B,C,D\n# 1.0,2.0,3.0,4.0\n# 5.0,6.0,,8.0\n# 10.0,11.0,12.0,'''\n\n# # If you are using Python 2.7, you need\n# # to convert the string to unicode:\n\n# if (sys.version_info < (3, 0)):\n#     csv_data = unicode(csv_data)\n\n# df = pd.read_csv(StringIO(csv_data))\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d7313caf56b3047f51e18a56ea17cf4e6f20e58"},"cell_type":"code","source":"df.isnull()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"288d48d59c96a9a54a94002dc31aa8e3454ff4f2"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26a326748ec0a0ef5a277769b8cb177573818ca5"},"cell_type":"code","source":"df.loc[:,df.isnull().sum() == 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2008c7da6af3f215f547c76ab6b41c857802ca30"},"cell_type":"code","source":"df.isnull().sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"71e881523fb79f5e26b3571894bef3eb04faf526"},"cell_type":"code","source":"# access the underlying NumPy array\n# via the `values` attribute\ndf.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21329a9a42671bc3c8c1cef95027f8d6f28a0c73"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"_uuid":"c6ca21dd5fc83c4f2af19e70523f43e4dbd30033"},"cell_type":"markdown","source":"## Eliminating samples or features with missing values"},{"metadata":{"trusted":true,"_uuid":"7c4ed472d567db12083c614f5faa518dee34cf8f"},"cell_type":"code","source":"# remove rows that contain missing values\n\ndf.dropna(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed3f8a3f17aa281c06a48deb58ce11bbd4254833"},"cell_type":"code","source":"# remove columns that contain missing values\n\ndf.dropna(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce8b310bba51f4fd358b5fcb774ea7676c92ac3b"},"cell_type":"code","source":"# remove columns that contain missing values\n\ndf.dropna(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2193ec1be3068c570ae16c991738fe20bac884a3"},"cell_type":"code","source":"# only drop rows where all columns are NaN\n\ndf.dropna(how='all')  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c4c3600b5013dbe9aa44c2f3268ad0e02f5fa1f"},"cell_type":"code","source":"# drop rows that have less than 3 real values \n\ndf.dropna(thresh=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a8dcef7b449672f8b40626b8960c5f0dc1e7fa3"},"cell_type":"code","source":"# only drop rows where NaN appear in specific columns (here: 'C')\n\ndf.dropna(subset=['C'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff98aea96eeaab5289764f3618bdda44f42954ff"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"_uuid":"05f9f954c0b9f94f2cc7360835d8df3821520fb3"},"cell_type":"markdown","source":"## Imputing missing values"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"107090ee7aff26db76418ec0a292b49f9b270783"},"cell_type":"code","source":"# again: our original array\ndf.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db291a77d00ef358d0d3c750b8bbaf9fd892d99a"},"cell_type":"code","source":"# impute missing values via the column mean\n# next time we have a sklearn functionality tutorial :D\nfrom sklearn.preprocessing import Imputer \n\nimr = Imputer(missing_values='NaN', strategy='mean', axis=0)\nimr = imr.fit(df.values)\nimputed_data = imr.transform(df.values)\nimputed_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2debc077f219dec4099d69b8d7f2ba8cea24c94"},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n# impute along each column. sklearn 0.20.2 \ndf_copy = df.copy()\nprint(df_copy)\nimr_c = SimpleImputer(missing_values=np.nan, strategy='mean') # strategy median, constant, most_frequent\nimr_c = imr_c.fit(df_copy.values)\nimputed_data = imr_c.transform(df_copy.values)\nimputed_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4845c64920b5203e1d746a70b0395054beb9720c"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"_uuid":"e6d9f06d34ad920523e9c8fd07d40da42035e65e"},"cell_type":"markdown","source":"## Understanding the scikit-learn estimator API"},{"metadata":{"trusted":false,"_uuid":"351a07e4678b5890f9ef51c4679699c961a49e2a"},"cell_type":"code","source":"# Image(filename='images/04_01.png', width=400) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3a0f49531914f0c276be9119ed85d6d06d17c3de"},"cell_type":"code","source":"# Image(filename='images/04_02.png', width=300) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a32251b178a50a65041c6839fa22da5ad948999e"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"_uuid":"56761852378f3a7e81d0eb3f8347a8f50e5896c2"},"cell_type":"markdown","source":"# Handling categorical data"},{"metadata":{"_uuid":"3f149b36a8cb62828b42ceb9d9c44c0a15a4472d"},"cell_type":"markdown","source":"## Nominal and ordinal features"},{"metadata":{"trusted":true,"_uuid":"39e78200550c565f5cdbad1393fee7ea77666236"},"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame([['green', 'M', 10.1, 'class2'],\n                   ['red', 'L', 13.5, 'class1'],\n                   ['blue', 'XL', 15.3, 'class2']])\n\ndf.columns = ['color', 'size', 'price', 'classlabel']\ndf","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d7abdc3419ad090eac6f8a8ef22edc6036481d9"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"_uuid":"9148ff5d6a2d4a25349af87a8dbbbd6514af502e"},"cell_type":"markdown","source":"## Mapping ordinal features"},{"metadata":{"trusted":true,"_uuid":"d463a7d3c9e72f414795bd15acb499776389e69d"},"cell_type":"code","source":"# ordered labels \nsize_mapping = {'XL': 3,\n                'L': 2,\n                'M': 1}\n\ndf['size'] = df['size'].map(size_mapping)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bc8f748f5f5f07a83f6be0e9fa8d9453ebfcd5a"},"cell_type":"code","source":"?map","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11169c6f8c15cd73a37e4422fae2a3d0ff32b7c6"},"cell_type":"markdown","source":"map() function returns a list of the results after applying the given function to each item of a given iterable"},{"metadata":{"trusted":true,"_uuid":"557cab997e7744a012bbfad2992fa0572ccf4ca4"},"cell_type":"code","source":"def addition(n): \n    return n + n \n  \n# We double all numbers using map() \nnumbers = (1, 2, 3, 4) \nresult = map(addition, numbers) \nprint(list(result)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0d4f2405c2960870a198d417b85f109508f34e0"},"cell_type":"code","source":"\n# Double all numbers using map and lambda \n  \nnumbers = (1, 2, 3, 4) \nresult = map(lambda x: x + x, numbers) \nprint(list(result)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c162a1c31f71763947e052b42f90b22f4f359b0f"},"cell_type":"code","source":"# inverse label encoding\ninv_size_mapping = {v: k for k, v in size_mapping.items()}\ndf['size'].map(inv_size_mapping)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d03852cd1e9321907f3ce008c6d804ab64898d48"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"_uuid":"a66d75badc3c5cb3da5a5827acd0c4abad1c3728"},"cell_type":"markdown","source":"## Encoding class labels"},{"metadata":{"trusted":true,"_uuid":"22a9412065645116ab2ee7faf5b622875966417a"},"cell_type":"code","source":"import numpy as np\n\n# create a mapping dict\n# to convert class labels from strings to integers\nclass_mapping = {label: idx for idx, label in enumerate(np.unique(df['classlabel']))}\nclass_mapping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcb16e23013c0aa32a1789a5b13b7983634fc619"},"cell_type":"code","source":"# to convert class labels from strings to integers\ndf['classlabel'] = df['classlabel'].map(class_mapping)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c150596b3651f846728c241dea2ebe650bb08a8f"},"cell_type":"code","source":"# reverse the class label mapping\ninv_class_mapping = {v: k for k, v in class_mapping.items()}\ndf['classlabel'] = df['classlabel'].map(inv_class_mapping)\ndf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4b0cf196818d3341f59bf9dc1b95d4530cd5a4d"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Label encoding with sklearn's LabelEncoder\nclass_le = LabelEncoder()\ny = class_le.fit_transform(df['classlabel'].values)\ny","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30629ecd5badaeb32bbac231143e7bbc0ea56e1e"},"cell_type":"code","source":"# reverse mapping\nclass_le.inverse_transform(y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4050eb5e377e486ebeff8377efc45483a84b0ef"},"cell_type":"markdown","source":"Note: The deprecation warning shown above is due to an implementation detail in scikit-learn. It was already addressed in a pull request (https://github.com/scikit-learn/scikit-learn/pull/9816), and the patch will be released with the next version of scikit-learn (i.e., v. 0.20.0)."},{"metadata":{"_uuid":"1f37c9421e4bf054147d67c9a380552abb30d911"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"_uuid":"06738fcc0a22aea18b0a172b7e383ddbb96d91dd"},"cell_type":"markdown","source":"## Performing one-hot encoding on nominal features"},{"metadata":{"trusted":true,"_uuid":"800d47141d90b8d4006c73b54e0ba70452cd00c0"},"cell_type":"code","source":"# color \nX = df[['color', 'size', 'price']].values\n\ncolor_le = LabelEncoder()\nX[:, 0] = color_le.fit_transform(X[:, 0])\nX","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce4e1562729d5cb6155c527fb41d351390a42e14"},"cell_type":"markdown","source":"What do you think about encoding colors with numbers?  \nblue = 0\ngreen = 1\nred = 2"},{"metadata":{"_uuid":"48cb2bdef7dc52d68049ac3a7920304e04d1d3c1"},"cell_type":"markdown","source":"When we are using one-hot encoding datasets, we have to keep in mind that it introduces multicollinearity, which can be an issue for certain methods (for instance, methods that require matrix inversion). If features are highly correlated, matrices are computationally difficult to invert, which can lead to numerically unstable estimates. To reduce the correlation among variables, we can simply remove one feature column from the one-hot encoded array. Note that we do not lose any important information by removing a feature column, though; for example, if we remove the column color_blue, the feature information is still preserved since if we observe color_green=0 and color_red=0, it implies that the observation must be blue.If we use the get_dummies function, we can drop the first column by passing a True argument to the drop_first parameter, as shown in the following code example:"},{"metadata":{"trusted":true,"_uuid":"88978d063c7bb0e69d30b213ddab54b141e3f65f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b6e92ed9e61a4a721f08a79d38d05d48fc9a68b"},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(categorical_features=[0])\nohe.fit_transform(X).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03d3e4bc9f79a5e69c193a147206e7674b3d3dc8"},"cell_type":"code","source":"# return dense array so that we can skip\n# the toarray step\n\nohe = OneHotEncoder(categorical_features=[0], sparse=False)\nohe.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dc0b67d0bd661cf43b732f432ddcde8396d0396"},"cell_type":"code","source":"# one-hot encoding via pandas\n\npd.get_dummies(df[['price', 'color', 'size']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"673ed2b274b1262849f585b2c31eea6f5335cbe1"},"cell_type":"code","source":"# multicollinearity guard in get_dummies\n\npd.get_dummies(df[['price', 'color', 'size']], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2b82a5f5ce170e44ae944da70db7cf9f1fbece9"},"cell_type":"code","source":"# multicollinearity guard for the OneHotEncoder\n\nohe = OneHotEncoder(categorical_features=[0])\nohe.fit_transform(X).toarray()[:, 1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a7b141b9670e2256904e3972f4c3c6b6573461c"},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\nX = [['Male', 1], ['Female', 3], ['Female', 2]]\nenc.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0f2e5eea1609da36a6b758e9af1c864b3a48caa"},"cell_type":"code","source":"print(enc.categories_)\n\nprint(enc.transform([['Female', 1], ['Male', 4]]).toarray())\n\n\nprint(enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]]))\n\n\nenc.get_feature_names()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ce7c3ff77c1f77a32b55ce2e3dafbd3394e9483"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"_uuid":"8657807c93a50c1d8dcef143bf6a99809d9684f0"},"cell_type":"markdown","source":"# Partitioning a dataset into a seperate training and test set"},{"metadata":{"trusted":true,"_uuid":"410674cfc652fe9680af968ad6510d379178b6e5"},"cell_type":"code","source":"# df_wine = pd.read_csv('https://archive.ics.uci.edu/'\n#                       'ml/machine-learning-databases/wine/wine.data',\n#                       header=None)\n\ndf_wine = pd.read_csv('../input/wine-quality-public/wine_data.csv', header=None)\n\n\n\n# if the Wine dataset is temporarily unavailable from the\n# UCI machine learning repository, un-comment the following line\n# of code to load the dataset from a local path:\n\n# df_wine = pd.read_csv('wine.data', header=None)\n\n\ndf_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n                   'Proline']\n\nprint('Class labels', np.unique(df_wine['Class label']))\ndf_wine.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d4a47ed7166a7b5e6a78ec958990410b24f2891"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n\nX_train, X_test, y_train, y_test =\\\n    train_test_split(X, y, \n                     test_size=0.3, \n                     random_state=0, \n                     stratify=y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d73807a9a28ac8f8874241b352772d08589969a5"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"_uuid":"de50ad8f44f9bcefc0b0919229f3495e9cb75092"},"cell_type":"markdown","source":"# Bringing features onto the same scale"},{"metadata":{"trusted":true,"_uuid":"aed6e2e49eac2465ed8afee1fe436769f4f38fe7"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nmms = MinMaxScaler()\nX_train_norm = mms.fit_transform(X_train)\nX_test_norm = mms.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69397e5cc5f9d5b616a7306a4d01ac629ed84507"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nstdsc = StandardScaler()\nX_train_std = stdsc.fit_transform(X_train)\nX_test_std = stdsc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ee6d9ecea2ebc69ac3f1cada06d7d6b4bbdb28b"},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\nstdsc = RobustScaler()  \nX_train_std = stdsc.fit_transform(X_train)\nX_test_std = stdsc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b61345a20cdcf3877c2c4bdb7a1c4faad2c20aff"},"cell_type":"markdown","source":"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n\nDifferent types of normalizers\nhttp://cs231n.github.io/neural-networks-2/\n"},{"metadata":{"_uuid":"49acbc27defbdda0f23c1876056c1f20113f77b3"},"cell_type":"markdown","source":"A visual example:"},{"metadata":{"trusted":true,"_uuid":"225366cb765b3346a223431dfa047e8f7b54a750"},"cell_type":"code","source":"ex = np.array([0, 1, 2, 3, 4, 5])\n\nprint('standardized:', (ex - ex.mean()) / ex.std())\n\n# Please note that pandas uses ddof=1 (sample standard deviation) \n# by default, whereas NumPy's std method and the StandardScaler\n# uses ddof=0 (population standard deviation)\n\n# normalize\nprint('normalized:', (ex - ex.min()) / (ex.max() - ex.min()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de21fb25b42b39bfdf25219b6ea55a727663d8ab"},"cell_type":"markdown","source":"\nKaggle workshop\nhttps://www.kaggle.com/agrawaladitya/step-by-step-data-preprocessing-eda\n<br>\n<br>"},{"metadata":{"_uuid":"532217910ed13944f0b187507521d4b00b7349a3"},"cell_type":"markdown","source":"# Selecting meaningful features"},{"metadata":{"_uuid":"eb584ebfa42d8a5e690856069071bcd33c60f2b4"},"cell_type":"markdown","source":"..."},{"metadata":{"_uuid":"ac9e279de7e337e96e267562401f58076113d9c0"},"cell_type":"markdown","source":"## L1 and L2 regularization as penalties against model complexity"},{"metadata":{"_uuid":"6f9e44bdee7af0d4c9254f92dc61df635b44c8c5"},"cell_type":"markdown","source":"## A geometric interpretation of L2 regularization"},{"metadata":{"trusted":false,"_uuid":"408db089494c8797e5c321e2fa15ea7c2ad8b896"},"cell_type":"code","source":"# Image(filename='images/04_04.png', width=500) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"76affd2b3beabcedd2ca3c03d185d6bdb06d2f7d"},"cell_type":"code","source":"# Image(filename='images/04_05.png', width=500) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b98646f31f73d9df0244d740268e145315caa333"},"cell_type":"markdown","source":"## Sparse solutions with L1-regularization"},{"metadata":{"trusted":false,"_uuid":"2f1b8fef091bb80766a3c141676f7e78a29603eb"},"cell_type":"code","source":"# Image(filename='images/04_06.png', width=500) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"470769227fbd935957a53585a6239ce0611e8507"},"cell_type":"markdown","source":"For regularized models in scikit-learn that support L1 regularization, we can simply set the `penalty` parameter to `'l1'` to obtain a sparse solution:"},{"metadata":{"trusted":false,"_uuid":"092aad751576049c022aa318645babd67941fff5"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nLogisticRegression(penalty='l1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97447e145a1d810b76e7ce618569b8bf1f960dab"},"cell_type":"markdown","source":"Applied to the standardized Wine data ..."},{"metadata":{"trusted":false,"_uuid":"a8bcc62407f8d6ca0d4efd48a51c5d077c55122c"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(penalty='l1', C=1.0)\n# Note that C=1.0 is the default. You can increase\n# or decrease it to make the regulariztion effect\n# stronger or weaker, respectively.\nlr.fit(X_train_std, y_train)\nprint('Training accuracy:', lr.score(X_train_std, y_train))\nprint('Test accuracy:', lr.score(X_test_std, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"63e7a1fa803d329127dd78fc6253e691f68c168e"},"cell_type":"code","source":"lr.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d9c2d1b8bc5816013e7b5e4950940c1f91529d90"},"cell_type":"code","source":"np.set_printoptions(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"09ccbb07257c5eba989e0945dfbe9bdba6f94d85"},"cell_type":"code","source":"lr.coef_[lr.coef_!=0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8c2e71e6dbce2021e710c5dfcddc3f911d3b8dd9"},"cell_type":"code","source":"lr.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b8d71bbf412f231d4bacbe018f4aa29d3bae00ff"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = plt.subplot(111)\n    \ncolors = ['blue', 'green', 'red', 'cyan', \n          'magenta', 'yellow', 'black', \n          'pink', 'lightgreen', 'lightblue', \n          'gray', 'indigo', 'orange']\n\nweights, params = [], []\nfor c in np.arange(-4., 6.):\n    lr = LogisticRegression(penalty='l1', C=10.**c, random_state=0)\n    lr.fit(X_train_std, y_train)\n    weights.append(lr.coef_[1])\n    params.append(10**c)\n\nweights = np.array(weights)\n\nfor column, color in zip(range(weights.shape[1]), colors):\n    plt.plot(params, weights[:, column],\n             label=df_wine.columns[column + 1],\n             color=color)\nplt.axhline(0, color='black', linestyle='--', linewidth=3)\nplt.xlim([10**(-5), 10**5])\nplt.ylabel('weight coefficient')\nplt.xlabel('C')\nplt.xscale('log')\nplt.legend(loc='upper left')\nax.legend(loc='upper center', \n          bbox_to_anchor=(1.38, 1.03),\n          ncol=1, fancybox=True)\n#plt.savefig('images/04_07.png', dpi=300, \n#            bbox_inches='tight', pad_inches=0.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e5c63da3abf15f3a128cbb7edfa19b0d143dde0"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"_uuid":"19c504a9a4e866ad5c2268e77766acf95ebdf28c"},"cell_type":"markdown","source":"## Sequential feature selection algorithms"},{"metadata":{"trusted":false,"_uuid":"07c42dffc66e668c903688a659fb8c978a1e77d2"},"cell_type":"code","source":"from sklearn.base import clone\nfrom itertools import combinations\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n\nclass SBS():\n    def __init__(self, estimator, k_features, scoring=accuracy_score,\n                 test_size=0.25, random_state=1):\n        self.scoring = scoring\n        self.estimator = clone(estimator)\n        self.k_features = k_features\n        self.test_size = test_size\n        self.random_state = random_state\n\n    def fit(self, X, y):\n        \n        X_train, X_test, y_train, y_test = \\\n            train_test_split(X, y, test_size=self.test_size,\n                             random_state=self.random_state)\n\n        dim = X_train.shape[1]\n        self.indices_ = tuple(range(dim))\n        self.subsets_ = [self.indices_]\n        score = self._calc_score(X_train, y_train, \n                                 X_test, y_test, self.indices_)\n        self.scores_ = [score]\n\n        while dim > self.k_features:\n            scores = []\n            subsets = []\n\n            for p in combinations(self.indices_, r=dim - 1):\n                score = self._calc_score(X_train, y_train, \n                                         X_test, y_test, p)\n                scores.append(score)\n                subsets.append(p)\n\n            best = np.argmax(scores)\n            self.indices_ = subsets[best]\n            self.subsets_.append(self.indices_)\n            dim -= 1\n\n            self.scores_.append(scores[best])\n        self.k_score_ = self.scores_[-1]\n\n        return self\n\n    def transform(self, X):\n        return X[:, self.indices_]\n\n    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n        self.estimator.fit(X_train[:, indices], y_train)\n        y_pred = self.estimator.predict(X_test[:, indices])\n        score = self.scoring(y_test, y_pred)\n        return score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"6e61f8f0fca61654cbcd2ebca0a6eaaf02930828"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# selecting features\nsbs = SBS(knn, k_features=1)\nsbs.fit(X_train_std, y_train)\n\n# plotting performance of feature subsets\nk_feat = [len(k) for k in sbs.subsets_]\n\nplt.plot(k_feat, sbs.scores_, marker='o')\nplt.ylim([0.7, 1.02])\nplt.ylabel('Accuracy')\nplt.xlabel('Number of features')\nplt.grid()\nplt.tight_layout()\n# plt.savefig('images/04_08.png', dpi=300)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e2cb474ef96968f330a76cc82ae6aff55e5c1dc7"},"cell_type":"code","source":"k3 = list(sbs.subsets_[10])\nprint(df_wine.columns[1:][k3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"16204414679ef1f41c279c625d47cb8e6f9966ce"},"cell_type":"code","source":"knn.fit(X_train_std, y_train)\nprint('Training accuracy:', knn.score(X_train_std, y_train))\nprint('Test accuracy:', knn.score(X_test_std, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5b0270b4abb6068d29200e4be31034f7e5cf8f4d"},"cell_type":"code","source":"knn.fit(X_train_std[:, k3], y_train)\nprint('Training accuracy:', knn.score(X_train_std[:, k3], y_train))\nprint('Test accuracy:', knn.score(X_test_std[:, k3], y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"791fb3ff47dabbb4ef85b4015b1781428d6e246d"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"_uuid":"30a9aa4340c2c64a16879b9454a789d7f6a16495"},"cell_type":"markdown","source":"# Assessing feature importance with Random Forests"},{"metadata":{"trusted":false,"_uuid":"1cee56f1f135fb14ea4c76407a73e5909d06f3f8"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nfeat_labels = df_wine.columns[1:]\n\nforest = RandomForestClassifier(n_estimators=500,\n                                random_state=1)\n\nforest.fit(X_train, y_train)\nimportances = forest.feature_importances_\n\nindices = np.argsort(importances)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, \n                            feat_labels[indices[f]], \n                            importances[indices[f]]))\n\nplt.title('Feature Importance')\nplt.bar(range(X_train.shape[1]), \n        importances[indices],\n        align='center')\n\nplt.xticks(range(X_train.shape[1]), \n           feat_labels[indices], rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.tight_layout()\n#plt.savefig('images/04_09.png', dpi=300)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a26a08451dc9571afdf453e9f928df0eca6ff836"},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\n\nsfm = SelectFromModel(forest, threshold=0.1, prefit=True)\nX_selected = sfm.transform(X_train)\nprint('Number of features that meet this threshold criterion:', \n      X_selected.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"74e372649643713d17a1eef0d378a3cca158906a"},"cell_type":"markdown","source":"Now, let's print the 3 features that met the threshold criterion for feature selection that we set earlier (note that this code snippet does not appear in the actual book but was added to this notebook later for illustrative purposes):"},{"metadata":{"trusted":false,"_uuid":"428405481b7ba18523f37b40fab435bedd4ffc87"},"cell_type":"code","source":"for f in range(X_selected.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, \n                            feat_labels[indices[f]], \n                            importances[indices[f]]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b6600d3921e7ae753a4c11f60e9bd6cca145af8"},"cell_type":"markdown","source":"<br>\n<br>"},{"metadata":{"collapsed":true,"_uuid":"964167376c25dfc0006769ebdcb84e8a507388ca"},"cell_type":"markdown","source":"# Summary"},{"metadata":{"_uuid":"5c349ba58c5295976b8ddff148b0bcb9acd9140a"},"cell_type":"markdown","source":"..."},{"metadata":{"_uuid":"d9e2d1f098dc7b2d249f00603611b913925e8411"},"cell_type":"markdown","source":"---\n\nReaders may ignore the next cell."},{"metadata":{"trusted":false,"_uuid":"2d3afd5ce3f2b5657687538b2e58385948382b91"},"cell_type":"code","source":"# ! python ../.convert_notebook_to_script.py --input ch04.ipynb --output ch04.py","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7a1c7e2c7953eb034571f4f7b4eee7b52161304b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}