{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Text Classification Models - An Extensive List \n\nI won't go into the details & bore you'll with the information about \"what is text classification?\". Instead I shall go straight to implementing various models for text classification [assuming thats what you're here for :-)]. \n\nI will keep the notebook fairely organised & well commented for easy reading, please do **UPVOTE** if you find it helpful.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Setup\n\n### Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os, warnings, gc, string\nwarnings.filterwarnings(\"ignore\")\n\n# SKLearn\nfrom sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\n\n# Tensorflow / Keras\nimport tensorflow as tf\nfrom keras.preprocessing import text,sequence\nfrom keras import layers,models,optimizers\nimport tensorflow_hub as hub\n\n# XGBoost & Textblob\nimport xgboost\n\n#Gensim Library for Text Processing\nimport gensim.parsing.preprocessing as gsp\nfrom gensim import utils","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Setup","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"'''Load'''\n\n#train\nurl = '../input/analytics-vidhya-identify-the-sentiments/train.csv'\ndf = pd.read_csv(url, header='infer')\n\n#Drop Columns\ndf.drop('id', inplace=True, axis=1)\n\n#Inspect\nprint(\"Total Records (training dataset): \", df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Tweet Data Cleaning Utility Function'''\n\nprocesses = [\n               gsp.strip_tags, \n               gsp.strip_punctuation,\n               gsp.strip_multiple_whitespaces,\n               gsp.strip_numeric,\n               gsp.remove_stopwords, \n               gsp.strip_short, \n               gsp.stem_text\n            ]\n\ndef proc_txt(txt):\n    text = txt.lower()\n    text = utils.to_unicode(text)\n    for p in processes:\n        text = p(text)\n    return text\n\n\n# Training Dataset\ndf['tweet_cln'] = df['tweet'].apply(lambda x: proc_txt(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Dataset\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" '''Data Split (training dataset)'''\ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['tweet_cln'], df['label'])\n\n\n\n'''Feature Engineering of Training Dataset [TF-IDF Vectors] - Basic Classifiers'''\ntfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(df['tweet_cln'])\nxtrain_tfidf =  tfidf_vect.transform(train_x)\nxvalid_tfidf =  tfidf_vect.transform(valid_x)\n\n\n\n'''Feature Engineering of Training Dataset [Word Embedding] - Deep Neural'''\nembeddings_index = {}\n\nfor i, line in enumerate(open('../input/wikinews300d1mvec/wiki-news-300d-1M.vec')):  #Pretrained Word Embedding Vectors\n    values = line.split()\n    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n\n# Tokenizer \ntoken = text.Tokenizer()\ntoken.fit_on_texts(df['tweet_cln'])\nword_index = token.word_index\n\n# Text to Sequence \ntrain_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\nvalid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n\n# Token-embedding Mapping\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build Model","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"'''Utility Function'''\n\ndef model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n   \n    #free memory\n    gc.collect()\n    \n    return metrics.accuracy_score(predictions, valid_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_acc = model(naive_bayes.MultinomialNB(),xtrain_tfidf, train_y, xvalid_tfidf)\nprint(\"Naive Bayes(multinomial) Accuracy Achieved: \", '{:.2%}'.format(nb_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Reg Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ln_acc = model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint(\"Logistic Reg Accuracy Achieved: \", '{:.2%}'.format(ln_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_acc = model(ensemble.RandomForestClassifier(random_state=42), xtrain_tfidf, train_y, xvalid_tfidf)\nprint(\"Random Forest Accuracy Achieved: \", '{:.2%}'.format(rf_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_acc = model(xgboost.XGBClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\nprint(\"XGBoost Accuracy Achieved: \", '{:.2%}'.format(xgb_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CNN (Keras)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# Convolutional Layer\nconv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n\n# Pooling Layer\npooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\ncnn_model = models.Model(inputs=input_layer, outputs = output_layer2)\ncnn_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\ncnn_acc = model(cnn_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"CNN Model Accuracy Achieved: \", '{:.2%}'.format(cnn_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RNN - LSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# LSTM Layer\nlstm_layer = layers.LSTM(100)(embedding_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrnn_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrnn_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrnn_acc = model(rnn_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RNN(LSTM) Model Accuracy Achieved: \", '{:.2%}'.format(rnn_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RNN - GRU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# GRU Layer\ngru_layer = layers.GRU(100)(embedding_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(gru_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrnngru_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrnngru_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrnngru_acc = model(rnngru_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RNN(GRU) Model Accuracy Achieved: \", '{:.2%}'.format(rnngru_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RNN - BiDirectional(GRU)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# BiDirectional Layer\nbi_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(bi_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrnnbi_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrnnbi_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrnnbi_acc = model(rnnbi_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RNN(BiDirectional-GRU) Model Accuracy Achieved: \", '{:.2%}'.format(rnnbi_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RNN - BiDirectional(LSTM)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# BiDirectional Layer\nbi_layer = layers.Bidirectional(layers.LSTM(100))(embedding_layer)\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(bi_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrnnbil_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrnnbil_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrnnbil_acc = model(rnnbil_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RNN(BiDirectional-LSTM) Model Accuracy Achieved: \", '{:.2%}'.format(rnnbil_acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### RCNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Create Model'''\n\n# Input Layer\ninput_layer = layers.Input((70, ))\n\n# Word Embedding Layer\nembedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\nembedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n\n# Recurrent Layer\nrnn_layer = layers.Bidirectional(layers.GRU(100,return_sequences=True))(embedding_layer)\n    \n# Convolutional Layer\nconv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(rnn_layer)\n\n# Pooling Layer\npooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n\n\n# Output Layers\noutput_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\noutput_layer1 = layers.Dropout(0.25)(output_layer1)\noutput_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n\n# Compile\nrcnn_model = models.Model(inputs=input_layer, outputs = output_layer2)\nrcnn_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n\n\nrcnn_acc = model(rcnn_model, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\nprint(\"RCNN Model Accuracy Achieved: \", '{:.2%}'.format(rcnn_acc))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}