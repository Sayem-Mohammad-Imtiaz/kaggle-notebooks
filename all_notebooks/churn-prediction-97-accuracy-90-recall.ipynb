{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook we will try to find the most important reasons that a customer would churn and also devise multiple models that would predict churning customers"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/credit-card-customers/BankChurners.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.iloc[:,:-2]#deleting last two rows as mentioned in database\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[data.select_dtypes(['object']).columns] = data.select_dtypes(['object']).apply(lambda x: x.astype('category'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Attrition_Flag.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(x='Attrition_Flag', data=data, hue='Gender')\nplt.title(\"Distribution of Gender Among Attrited and Existing Customers\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Attrition_Flag', data=data, hue='Marital_Status')\nplt.title(\"Distribution of Marital Status Among Attrited and Existing Customers\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Attrition_Flag', data=data, hue='Income_Category')\nplt.title(\"Distribution of Income Category Among Attrited and Existing Customers\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Attrition_Flag', data=data, hue='Card_Category')\nplt.title(\"Distribution of Card_Category Among Attrited and Existing Customers\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing**\n <br>Here the most important metric is recall since we want most of if not all the customers who want to churn so that the bank manager can propose plans to minimise the churning rate. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX=data.drop(\"Attrition_Flag\",axis=1)\ny=data.Attrition_Flag\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_num=X_train[X_train.select_dtypes(['int64',\"float64\"]).columns]\nX_test_num=X_test[X_test.select_dtypes(['int64',\"float64\"]).columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train_num)\nX_train_scaled=pd.DataFrame(scaler.transform(X_train_num),columns=X_train_num.columns,index=X_train_num.index)\nX_test_scaled=pd.DataFrame(scaler.transform(X_test_num),columns=X_test_num.columns,index=X_test_num.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_dum=pd.get_dummies(X_train[X_train.select_dtypes(['category']).columns],drop_first=True)\nX_test_dum=pd.get_dummies(X_test[X_test.select_dtypes(['category']).columns],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pd=pd.concat([X_train_scaled, X_train_dum], axis=1)\nX_test_pd=pd.concat([X_test_scaled, X_test_dum], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_pd.drop(\"CLIENTNUM\",axis=1,inplace=True)\nX_test_pd.drop(\"CLIENTNUM\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **1. Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train_pd,y_train)\nlrpredictions = logmodel.predict(X_test_pd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,lrpredictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **2. Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nnbmodel = MultinomialNB().fit(X_train_pd, y_train)\nnbpredictions=nbmodel.predict(X_test_pd)\n\nprint(classification_report(y_test,nbpredictions))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **3. KNN Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknnmodel = KNeighborsClassifier(n_neighbors=3)\nknnmodel.fit(X_train_pd,y_train)\nknnpredictions=knnmodel.predict(X_test_pd)\nprint(classification_report(y_test,knnpredictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **4. Decision Tree Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtmodel = DecisionTreeClassifier(random_state=0)\ndtmodel.fit(X_train_pd,y_train)\ndtpredictions=dtmodel.predict(X_test_pd)\nprint(classification_report(y_test,dtpredictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **5. Random Forest Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfmodel = RandomForestClassifier(random_state=0,max_depth=100,n_estimators=50)\nrfmodel.fit(X_train_pd,y_train)\nrfpredictions=rfmodel.predict(X_test_pd)\nprint(classification_report(y_test,rfpredictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **6. Gradient Boosting Classifier**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbmodel = GradientBoostingClassifier(random_state=0)\ngbmodel.fit(X_train_pd,y_train)\ngbpredictions=gbmodel.predict(X_test_pd)\nprint(classification_report(y_test,gbpredictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **7. Extreme Gradient Boosting Classifier(XGBoost)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nxgbmodel = xgb.XGBClassifier(random_state=0)\nxgbmodel.fit(X_train_pd,y_train)\nxgbpredictions=xgbmodel.predict(X_test_pd)\nprint(classification_report(y_test,xgbpredictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 8. ANN"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dummyfying the target variable since Deep Learning doesn't accept categorical variables\ny_train_dum=pd.get_dummies(y_train,drop_first=True)\ny_test_dum=pd.get_dummies(y_test,drop_first=True)\n\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 42)#performing SMOTE to resolve class imbalances\nX_train_oversampled, y_train_oversampled = sm.fit_sample(X_train_pd, y_train_dum)\nX_train_dumsmote = pd.DataFrame(X_train_oversampled, columns=X_train_pd.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras import regularizers\nimport keras\n\nannmodel = Sequential()\nannmodel.add(Dense(64, input_dim=32, activation='relu'))\nannmodel.add(Dense(32,kernel_regularizer=regularizers.l2(0.01), activation='relu'))\nannmodel.add(Dropout(0.1))\nannmodel.add(Dense(1, activation='sigmoid'))\n\n# compile the keras model\nannmodel.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n# fit the keras model on the dataset\nannmodel.fit(X_train_dumsmote, y_train_oversampled, epochs=150, batch_size=32,verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"annpredictions=annmodel.predict(X_test_pd)\nannpredictions=[1 if x>0.7 else 0 for x in annpredictions]\nprint(classification_report(y_test_dum,annpredictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see that even after performing various pre processing and adjusting various hyperparameters, the maximum recall and accuracy we are able to achieve is only 86% and 90% respectively. After a lot of adjustments, I have come to the conclusion that there isn't enough data to acheive better results than the XGBoost Algorithm here for deep learning to perform better. Hence, there seems to be no point going any further. If anyone has any better algorithms, please write it down in the comments and I'll try that out. "},{"metadata":{},"cell_type":"markdown","source":"# Feature Importances\nNow Let's look at which features contributed most to the classification algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_important = xgbmodel.get_booster().get_score(importance_type='weight')\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\ndata.plot(kind='barh',figsize=(5,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above Graph we notice that the most important feature that decides between the churning customer and the existing customer is the Total_Trans_Amt. It makes logical sense since anyone who is planning on churning will try out another bank's services before closing down the current account. "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}