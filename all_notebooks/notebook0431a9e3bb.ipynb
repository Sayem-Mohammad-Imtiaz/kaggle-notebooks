{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Step 1: Reading and Understanding the Data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Description"},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata=pd.read_csv('../input/boom-bike-dataset/bike_sharing_data.csv')\nbikedata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step 2: Check - Null Values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate Percentage of null values in each clolumn\nround(100*(bikedata.isnull().sum()/len(bikedata.index)),2).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculate Percentage of null values in each row\nround((bikedata.isnull().sum(axis=1)/122)*100,2).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From above observation there is no null values in rows and columns."},{"metadata":{},"cell_type":"markdown","source":"## Step2.1. Check Duplicate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copy of original bikedata dataframe for duplicate check\nbike_dp = bikedata\n\n# Checking for duplicates and dropping the entire duplicate row if any\nbike_dp.drop_duplicates(subset=None, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bike_dp.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Original dataframe is same after performing drop duplicate and shape size is same as previous. Hence conclude that there were no duplicate values in dataset."},{"metadata":{},"cell_type":"markdown","source":"## Step 2.2 Droping unwanted columns"},{"metadata":{},"cell_type":"markdown","source":"The following variables can be removed from further analysis: \n\n- instant : It is just an index value \n- dteday : This has the date, Since we already have seperate columns for 'year' & 'month'  \n- casual & registered : These columns contains the count of bike booked by different categories of customers.Ignoring these two columns. \n\nSaveing the new dataframe as bikedata_new."},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata_new=bikedata[['season','yr','mnth','holiday','weekday','workingday','weathersit','temp','atemp','hum','windspeed','cnt']]\nbikedata_new.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Converting 4 categorical variables(season,weathersit,mnth,weekday) into 'category' data types."},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata_new['season']=bikedata_new['season'].astype('category')\nbikedata_new['weathersit']=bikedata_new['weathersit'].astype('category')\nbikedata_new['mnth']=bikedata_new['mnth'].astype('category')\nbikedata_new['weekday']=bikedata_new['weekday'].astype('category')\n\nbikedata_new.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Dummy Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Droping orginal variable for which dummy was created\n#Droping first dummy variable for each set of dummies created\n\nbikedata_new=pd.get_dummies(bikedata_new,drop_first=True)\n\nbikedata_new.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata_new.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SPLITTING THE DATA"},{"metadata":{},"cell_type":"markdown","source":"### Splitting the data to Train and Test: - Split the data into TRAIN and TEST (70:30 ratio) - We will use train_test_split method from sklearn package for this ---"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the shape before spliting\n\nbikedata_new.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the info before spliting\n\nbikedata_new.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Specifying 'random_state'- the train and test data set always have the same rows, respectively\n\nnp.random.seed(0)\ndf_train, df_test = train_test_split(bikedata_new, train_size = 0.70, test_size = 0.30, random_state = 333)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EXPLORATORY DATA ANALYSIS\n\n* Performing EDA on df_train dataset"},{"metadata":{},"cell_type":"markdown","source":"### Makeing a pairplot of all the numeric variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Creating newdataframe for numeric variable\n\nbikenum=df_train[[ 'temp', 'atemp', 'hum', 'windspeed','cnt']]\n\nsns.pairplot(bikenum,diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### From above pairplot, there is liner relation between temp, atemp and cnt. "},{"metadata":{},"cell_type":"markdown","source":"### Building boxplot of all categorical variables against target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25, 10))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'season', y = 'cnt', data = bikedata)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'mnth', y = 'cnt', data = bikedata)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = bikedata)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'holiday', y = 'cnt', data = bikedata)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'weekday', y = 'cnt', data = bikedata)\nplt.subplot(2,3,6)\nsns.boxplot(x = 'workingday', y = 'cnt', data = bikedata)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:6 categorical variables in the dataset and used Box plot to observe their effect on the target variable (‘cnt’).\n\n- season: Almost 32% of the bike booking were happening in season3 with a median of over 5000 booking (for the period of 2 years). This was followed by season2 & season4 with 27% & 25% of total booking. This indicates, season can be a good predictor for the dependent variable. \n- mnth: Almost 10% of the bike booking were happening in the months 5,6,7,8 & 9 with a median of over 4000 booking per month. This indicates, mnth has some trend for bookings and can be a good predictor for the dependent variable. \n- weathersit: Almost 67% of the bike booking were happening during ‘weathersit1 with a median of close to 5000 booking (for the period of 2 years). This was followed by weathersit2 with 30% of total booking. This indicates, weathersit does show some trend towards the bike bookings can be a good predictor for the dependent variable. \n- holiday: Almost 97.6% of the bike booking were happening when it is not a holiday. This indicates, holiday CANNOT be a good predictor for the dependent variable. \n- weekday: weekday variable shows very close trend (between 13.5%-14.8% of total booking on all days of the week) having their independent medians between 4000 to 5000 bookings. This variable can have some or no influence towards the predictor. I will let the model decide if this needs to be added or not. \n- workingday: Almost 69% of the bike booking were happening in ‘workingday’ with a median of close to 5000 booking (for the period of 2 years). This indicates, workingday can be a good predictor for the dependent variable."},{"metadata":{},"cell_type":"markdown","source":"# Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking the correlation coefficients -which variables are highly correlated. \n\n\nplt.figure(figsize = (25,20))\nsns.heatmap(bikedata_new.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: \n- The heatmap clearly shows which all variable are multicollinear in nature, and which variable have high collinearity with the target variable. \n- validate different correlated values along with VIF & p-value, for identifying the correct variable to select/eliminate from the model. ---"},{"metadata":{},"cell_type":"markdown","source":"# RESCALING "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before scaling, check the values\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Applying scaler() to all the numeric variables\n\nnum_vars = ['temp', 'atemp', 'hum', 'windspeed','cnt']\n\ndf_train[num_vars] = scale.fit_transform(df_train[num_vars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Linear Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.pop('cnt')\nX_train = df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# RFE"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#RFE with the output number of the variable equal to 15\nlin = LinearRegression()\nlin.fit(X_train, y_train)\n\nrfe = RFE(lin, 15)             #RFE\nrfe = rfe.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = X_train.columns[rfe.support_]\ncol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.columns[~rfe.support_]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Model using 'STATS MODEL"},{"metadata":{},"cell_type":"markdown","source":"## Model 1\n\n### Observing VIF\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Createing a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n\n# Add a constant\nX_train_lm1 = sm.add_constant(X_train_rfe)\n\n# Create a first fitted model\nlinr1 = sm.OLS(y_train, X_train_lm1).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the parameters obtained\n\nlinr1.params\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the linear regression model obtained\nprint(linr1.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note : atemp\" has high P- value and High VIF - Dropping 'atemp'\n\n## Model 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_rfe.drop([\"atemp\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking VIF"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant\nX_train_lm2 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlinr2 = sm.OLS(y_train, X_train_lm2).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the parameters obtained\n\nlinr2.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# summary of the linear regression model obtained\nprint(linr2.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note- Dropping 'hum', as it has second highest VIF. Tempeature can be important factor .\n\n## Model 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop([\"hum\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Checking VIF"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant\nX_train_lm3 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlinr3 = sm.OLS(y_train, X_train_lm3).fit()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the parameters obtained\n\nlinr3.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Summary of the linear regression model obtained\nprint(linr3.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note- Dropping 'seacon-2', as it has second highest VIF. Tempeature can be important factor .\n\n## Model 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop([\"season_2\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### VIF Checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant\nX_train_lm4 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlinr4 = sm.OLS(y_train, X_train_lm4).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the parameters obtained\n\nlinr4.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summary of the linear regression model obtained\nprint(linr4.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note- Dropping 'month-10', as it has high P- Value. \n\n## Model 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop([\"mnth_10\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### VIF Checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant\nX_train_lm5 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlinr5 = sm.OLS(y_train, X_train_lm5).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the parameters obtained\n\nlinr5.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summary of the linear regression model obtained\nprint(linr5.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Note- Dropping 'month-8', as it has high P- Value. \n\n## Model 6"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = X_train_new.drop([\"mnth_8\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### VIF Checking"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add a constant\nX_train_lm6 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlinr6 = sm.OLS(y_train, X_train_lm6).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the parameters obtained\n\nlinr6.params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summary of the linear regression model obtained\nprint(linr6.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Note: - We will consider this as our final model (unless the Test data metrics are not significantly close to this number). ---Considering this is a final model as seems to be very low muliticollinerity between predictors and p-values for all the predictors seems to be significant."},{"metadata":{},"cell_type":"markdown","source":"# Final Model Interpretation\n\n## Hypothesis Testing:\n\n#### Hypothesis testing states that:\n##### H0:B1=B2=...=Bn=0 \n##### H1:  at least one  Bi!=0\n\n### linr6 model coefficient values\n\n* const           0.082570\n* yr              0.231870\n* temp            0.580437\n* windspeed      -0.152419\n* season_4        0.129183\n* mnth_3          0.065965\n* mnth_4          0.083857\n* mnth_5          0.072684\n* mnth_6          0.061821\n* mnth_9          0.090881\n"},{"metadata":{},"cell_type":"markdown","source":"### Consultion: As all above coefficients are not equl to zero. Which means Rejecting the NULL Hypothesis"},{"metadata":{},"cell_type":"markdown","source":"## F Statistics\nF-Statistics is used for testing the overall significance of the Model: Higher the F-Statistics, more significant the Model is.\n\nF-statistic: 194.5\nProb (F-statistic): 4.74e-165"},{"metadata":{},"cell_type":"markdown","source":"# VALIDATE ASSUMPTIONS"},{"metadata":{},"cell_type":"markdown","source":"### Error terms are normally distributed with mean zero (not X, Y)\nResidual Analysis Of Training Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_pred = lr6.predict(X_train_lm6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = y_train-y_train_pred\n\n# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((res), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                   \nplt.xlabel('Errors', fontsize = 18)                         ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  From the above histogram,  the Residuals are normally distributed. Hence our assumption for Linear Regression is valid."},{"metadata":{},"cell_type":"markdown","source":"### There is a linear relationship between X and Y"},{"metadata":{"trusted":true},"cell_type":"code","source":"bikedata_new=bikedata_new[[ 'temp', 'atemp', 'hum', 'windspeed','cnt']]\n\nsns.pairplot(bikenum, diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From above the pair plot,  see  that there is a linear relation between temp and atemp variable with the predictor ‘cnt’. ---"},{"metadata":{},"cell_type":"markdown","source":"## There is No Multicollinearity between the predictor variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Note - From the VIF calculation, concluted that there is no multicollinearity existing between the predictor variables, as all the values are within permissible range of below 5."},{"metadata":{},"cell_type":"markdown","source":"# MAKING PREDICTION USING FINAL MODEL\n\n#### Now that we have fitted the model and checked the assumptions, it's time to go ahead and make predictions using the final model (lr6)\n\nApplying the scaling on the test sets¶"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply scaler() to all numeric variables in test dataset. Note: we will only use scaler.transform, \n# as want to use the metrics that the model learned from the training data to be applied on the test data. \n\n\nnum_vars = ['temp', 'atemp', 'hum', 'windspeed','cnt']\n\ndf_test[num_vars] = scale.transform(df_test[num_vars])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dividing  X_test and y_test"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test = df_test.pop('cnt')\nX_test = df_test\n\nX_test.info()\n\n\n\n# y_test = df_test.pop('cnt')\n# X_test = df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting the variables that were part of final model.\ncol1=X_train_new.columns\n\nX_test=X_test[col1]\n\n# Adding constant variable to test dataframe\nX_test_lm6 = sm.add_constant(X_test)\n\nX_test_lm6.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Making predictions using the final model (lr6)\n\ny_pred = linr6.predict(X_test_lm6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MODEL EVALUATION"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting y_test and y_pred to understand the spread\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred, alpha=.5)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16)      ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### R^2 Value for TEST"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adjusted R^2 Value for TEST\nFormula for Adjusted R^2\n\nR2adj.=1−(1−R2)∗n−1n−p−1"},{"metadata":{"trusted":true},"cell_type":"code","source":"r2=0.7768676080828206","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the shape of X_test\n\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# n is number of rows in X\n\nn = X_test.shape[0]\n\n\n# Number of features (predictors, p) is the shape along axis 1\np = X_test.shape[1]\n\n# We find the Adjusted R-squared using the formula\n\nadjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)\nadjusted_r2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Result "},{"metadata":{},"cell_type":"markdown","source":"#### Train R^2 :0.77986.. - Train Adjusted R^2 :0.76614 .\n##### This seems to be a really good model that can very well 'Generalize' various datasets. ---"},{"metadata":{},"cell_type":"markdown","source":"# FINAL REPORT"},{"metadata":{},"cell_type":"markdown","source":"##### As per our final Model, the top 3 predictor variables that influences the bike booking are: --- - Temperature (temp) - A coefficient value of ‘0.580437’ indicated that a unit increase in temp variable increases the bike hire numbers by 0.5804 units. - Weather Situation 3 (weathersit_3) - A coefficient value of ‘-0.276877’ indicated that, w.r.t Weathersit1, a unit increase in Weathersit3 variable decreases the bike hire numbers by 0.276877 units. - Year (yr) - A coefficient value of ‘0.231870’ indicated that a unit increase in yr variable increases the bike hire numbers by 0.276877 units. --- \n\n##### SO IT IS RECOMMENDED TO GIVE THESE VARIABLES UTMOST IMPORTANCE WHILE PLANNING, TO ACHIEVE MAXIMUM BOOKING. --- \n \n##### The next best features that can also be considered are - - season_4: - A coefficient value of ‘0.129183’ indicated that w.r.t season_1, a unit increase in season_4 variable increases the bike hire numbers by 0.128744 units. - windspeed: - A coefficient value of ‘-0.152419’ indicated that, a unit increase in windspeed variable decreases the bike hire numbers by 0.152419 units. ---\n\n* NOTE: - The details of weathersit_1 & weathersit_3 - weathersit_1: Clear, Few clouds, Partly cloudy, Partly cloudy - weathersit_3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds - The details of season1 & season4 - season1: spring - season4: winter"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}