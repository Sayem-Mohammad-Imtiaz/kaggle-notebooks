{"cells":[{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"# HR Analytics Job Change of Data Scientists. Accuracy = 85%"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"## About this Dataset\n\n### Context and Content\n\nA company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company. Many people signup for their training. Company wants to know which of these candidates are really wants to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates. Information related to demographics, education, experience are in hands from candidates signup and enrollment.\n\nThis dataset designed to understand the factors that lead a person to leave current job for HR researches too. By model(s) that uses the current credentials,demographics,experience data you will predict the probability of a candidate to look for a new job or will work for the company, as well as interpreting affected factors on employee decision.\n\nThe whole data divided to train and test . Target isn't included in test but the test target values data file is in hands for related tasks. A sample submission correspond to enrollee_id of test set provided too with columns : enrollee _id , target\n\n### Note:\n\n*The dataset is imbalanced.*\nMost features are categorical (Nominal, Ordinal, Binary), some with high cardinality.\nMissing imputation can be a part of your pipeline as well.\n\n### Goal\n\nPredict the probability of a candidate will work for the company\nInterpret model(s) such a way that illustrate which features affect candidate decision\n\n### The source of the dataset\nhttps://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists/tasks?taskId=3015\n\n"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"## Imports"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"### Import libraries"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"#load packages\nimport os           # Operating system dependent functionality\nimport sys\nprint(\"Python version: {}\". format(sys.version))\n\nimport numpy  as np # Numerical computing tools.\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport pandas as pd # Data analysis and manipulation tool\nprint(\"pandas version: {}\". format(pd.__version__))\n\nfrom scipy.stats import gamma\n\nimport matplotlib\nimport matplotlib.pyplot as plt     # Provides a MATLAB-like plotting framework. \n# The source: https://matplotlib.org/api/pyplot_api.html\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport matplotlib.mlab   as mlab    # Numerical python functions \n# written for compatibility with MATLAB commands with the same names. \n# The source: https://matplotlib.org/3.3.3/api/mlab_api.html \n\nimport seaborn           as sns     # Python data visualization library based on matplotlib\n\nimport cufflinks as cf              # Python data visualization library for dynamical plots\n# Source: https://github.com/santosjorge/cufflinks\n# Some useful examples: https://www.kaggle.com/kyleos/cufflinks\ncf.set_config_file(offline=True)\n\nimport plotly.io as pio             # Themes for iplot\n# Some useful examples: https://plotly.com/python/templates/\n\nimport plotly.express as px         # \n\nimport colorama     # ANSI escape character sequences have long been used \n# to produce colored terminal text and cursor positioning on Unix and Macs\n# The source: https://pypi.org/project/colorama/\n\nimport warnings    # alert the user of some condition in a program, where that condition \n#(normally) doesnâ€™t warrant raising an exception and terminating the program.\n# The source: https://docs.python.org/3/library/warnings.html\nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import classification_report,confusion_matrix\n\n\nprint('-'*40)\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"-L\"]).decode(\"utf8\"))\n\n\nfrom sklearn import metrics\n\n\n#  show plots in Jupyter Notebook browser\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"### Import data"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Import data from aug_train.csv. \n# It will be separated into 2 parts later. \n\n# files\nfile_train       = '../input/hr-analytics-job-change-of-data-scientists/aug_train.csv'\nfile_validation  = '../input/hr-analytics-job-change-of-data-scientists/aug_test.csv'\n\n# check if datafile exists and load file dependently on the file type.\n# used functions: read_csv \n# read_csv https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n# or read_csv \n# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html\nif (not os.path.exists(file_train)): \n    if str(__name__) == '__main__':\n        raise SystemExit(\"There is no file with data!\")\nelse: \n    file_name, file_extension = os.path.splitext(file_train)\n    if (file_extension  == '.csv'):\n        train_set_init = pd.read_csv(file_train)\n    elif(file_extension == '.xlsx'):\n        train_set_init = pd.read_excel(file_train)\n        \nif (not os.path.exists(file_validation)): \n    if str(__name__) == '__main__':\n        raise SystemExit(\"There is no file with data!\")\nelse: \n    file_name, file_extension = os.path.splitext(file_validation)\n    if (file_extension  == '.csv'):\n        valid_set = pd.read_csv(file_validation)\n    elif(file_extension == '.xlsx'):\n        valid_set = pd.read_excel(file_validation)\n\n# Make a copy         \ntrain_set     = train_set_init.copy()\n\nn_rows        =  train_set.shape[0]    \nn_columns     =  train_set.shape[1]\n\nn_rows_val    =  valid_set.shape[0]    \nn_columns_val =  valid_set.shape[1]\n\n# To process datasets together, they will be collected into one array.\ntwo_sets_data = [train_set, valid_set]\n\n# information about train dataset\n# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html\ntrain_set.info()\n\n# # selection of numeric columns \ndf_numeric   = train_set.select_dtypes(include=[np.number])\nnumeric_cols = df_numeric.columns.values\nprint(\"\\n Numeric columns: \")\nprint(numeric_cols)\n\n# # selection of non-numeric columns \ndf_non_numeric   = train_set.select_dtypes(exclude=[np.number])\nnon_numeric_cols = df_non_numeric.columns.values\nprint(\"\\n Non-numeric columns: \")\nprint(non_numeric_cols)\n\n# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html\ntrain_set.head() ","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"# Data processing"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"Let's have a look at the data in general view: on the amount of unique values of attributes and missed values."},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"### Unique elements and non-null elements"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"''' \nThe solution how to draw a table in the Jupyter Notebook \nis presented here:\nhttps://stackoverflow.com/questions/35160256/how-do-i-output-lists-as-a-table-in-jupyter-notebook\n'''\nfrom IPython.display import HTML, display\ndef display_table(data):\n    html = \"<table>\"\n    for row in data:\n        html += \"<tr>\"\n        for field in row:\n            html += \"<td><h4>%s</h4><td>\"%(field)\n        html += \"</tr>\"\n    html += \"</table>\"\n    display(HTML(html))\n\ntable = [] # Name , Unique elements, Non-null Elements\ntable.append(['Name', 'Unique elements','Null Elements'])\nfor (columnName, columnData) in train_set.iteritems():\n    counted_number = train_set[columnName].isnull().sum()\n    line = [columnName, len(columnData.unique()), counted_number]\n    table.append(line)\n        \nprint(\"The total number of rows in the training dataset is {}.\".format(n_rows))\ndisplay_table(table)\n\nprint('-'*40)\n\ntable = [] # Name , Unique elements, Non-null Elements\ntable.append(['Name', 'Unique elements','Null Elements'])\nfor (columnName, columnData) in valid_set.iteritems():\n    counted_number = valid_set[columnName].isnull().sum()\n    line = [columnName, len(columnData.unique()), counted_number]\n    table.append(line)\n        \nprint(\"The total number of rows in the validation dataset is {}.\".format(n_rows_val))\ndisplay_table(table)","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"***Correcting***\n\n***Completing***\nThe are null values in gender, enrolled university, education level, major discipline, experience, company size, company type and time gap between last and new job fields. \nThe numerical attributes are supposed to be filled by a median value, the categorical attributes - don't change for Decision Tree technique and combine as a special category - for other algorithms.\n\n***Creating***\n\n***Converting***"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"# Clean data"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"### Missed values with the heat map"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Construct a heat map\ncols = train_set.columns\ncolours = ['#000099', '#ffff00'] \nfig, ax0 = plt.subplots(1,1, figsize = (5, 5))\nsns.heatmap(train_set[cols].isnull(), cmap=sns.color_palette(colours), vmin=0, vmax=1)\n\n# To write the percentage of missed values\nprint(\"The percentage of missing values\")\nfor col in train_set.columns:\n    pct_missing = np.mean(train_set[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100))) \n    \n# indicator for attributes with missed data\ntrain_set_copy = train_set.copy()\nfor col in train_set_copy.columns:\n    missing     = train_set_copy[col].isnull()\n    num_missing = np.sum(missing)\n    \n    if num_missing > 0:  \n        train_set_copy['{}_ismissing'.format(col)] = missing\n\n# hist for \nismissing_cols = [col for col in train_set_copy.columns if 'ismissing' in col]\ntrain_set_copy['num_missing'] = train_set_copy[ismissing_cols].sum(axis=1)\ntrain_set_copy['num_missing'].value_counts().reset_index().sort_values(by='index').plot.bar(x='index', y='num_missing',\n                                                                                           title='The number of rows vs number of missed values')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"In the graph one can see the missed values denoted by yellow color, the precented values are denoted by blue color. \n\nIn the next parts we well look at data atribbutes, their  distributions and fill the missed values and prepare data set for model using."},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"## Gender"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Replace nan values by a string value 'No gender'\nfor dataset in two_sets_data:\n    dataset[\"gender\"].fillna(\"No gender\", inplace = True)\n# calculate the number of unique genders \nn_genders = len(train_set.gender.unique()) \n\n# Create a subset to plot the data presentation and write \n# the total number of different values of the attribute\ndf = pd.DataFrame(train_set.groupby('gender').size())\nprint(df)\n\ndf.columns = ['Count']\n# one additional column for plotting\ndf['gender'] = df.index\nfig = px.pie(df, values='Count', names='gender', color='gender',\n             title='Gender distribution in the training dataset',\n             color_discrete_sequence=px.colors.sequential.Plasma)\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()    ","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"The dominant gender in dataset is Male. The missed value are a big part of presented data, hence we will fill as by a category \"No gender\" in meaning that the gender gap was not filled."},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"## Cities"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### Unique cities"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Write the number of null elements\ncounted_number = train_set['city'].isnull().sum()\nprint(\"The number of null elements is {}.\".format(counted_number))\n\nn_cities = len(train_set.city.unique())\ncities = []\nnumber = []\nfor i in range(n_cities):\n    city           = train_set.city.unique()[i]\n    counted_number = train_set[train_set.city == city].city.count()\n    cities.append(city)\n    number.append(counted_number)\n\nfig, ax0 = plt.subplots(1,1, figsize = (18, 5))\nax0.bar(cities, number, facecolor='g', alpha=0.5)\nplt.title('Distributions of the people in cities')\nplt.xlabel('Unique city')\nplt.ylabel('The number on people')\nplt.show()\n\n# max in the distribution\nindex = number.index(max(number))\nprint (str(cities[index]) + ' \\t: ' + str(max(number)))","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"The distribution looks similar to the double-Gaussian with one narrow dome or delta-distribution. That means that in the set there are mostly people from one city."},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### Unique city_development_index"},{"metadata":{"hideCode":true,"hidePrompt":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"x = train_set.city_development_index\nfig, ax0 = plt.subplots(1,1, figsize = (18, 5))\nsns.distplot(x)\nplt.title('Distributions of the people in cities')\nplt.xlabel('City development index')\nplt.ylabel('The number on people')\nplt.show()\n\nfig, axes = plt.subplots(1,1, figsize = (5, 5))\nx = train_set.city_development_index\nsns.boxplot(data=x)\nplt.show()\n\ncities = []\nnumber = []\nn_cities = len(train_set.city_development_index.unique())\n\nfor i in range(n_cities):\n    city         = train_set.city_development_index.unique()[i]\n    number_count = train_set[train_set.city_development_index == city].city_development_index.count()\n    cities.append(city)\n    number.append(number_count)\n\n# max in the distribution\nindex = number.index(max(number))\nprint (str(cities[index]) + ' \\t: ' + str(max(number)))","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"One can see on the box diagram that there is an outlier."},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"## Education and qualification of employee"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### Education"},{"metadata":{"hideCode":true,"hidePrompt":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"def plot_hist_ed_level():\n    x = train_set.education_level\n    fig, ax0 = plt.subplots(1,1, figsize = (18, 5))\n    sns.countplot(x)\n    plt.title('Education')\n    plt.xlabel('Education')\n    plt.ylabel('The number on people')\n    plt.show()\n\n# Write the number of null elements\ncounted_number = train_set['education_level'].isnull().sum()\nprint(\"The number of null elements is {}.\".format(counted_number))\n\n# Fill missed values to 'Undetermined'\nfor dataset in two_sets_data:\n    dataset[\"education_level\"].fillna(\"Undetermined\", inplace = True)\n\nplot_hist_ed_level()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"To make the dataset more balanced, there is a suggestion to make 2 groups for this category: 'Graduate' and 'Others', contained all other values. "},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Change all unigue values except of 'Graduate' to 'Others'.\nn_ed_levels = train_set.education_level.unique()\n\nfor education_level in n_ed_levels:\n    if education_level != 'Graduate':\n        train_set['education_level'] = train_set['education_level'].replace([education_level],'Others')\n        \nplot_hist_ed_level()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### Major discipline"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"def plot_hist_major_disc():\n    x = train_set.major_discipline\n    fig, ax0 = plt.subplots(1,1, figsize = (18, 5))\n    sns.countplot(x)\n    plt.title(\"Major discipline\")\n    plt.xlabel('Discipline')\n    plt.ylabel('The number on people')\n    plt.show()\n\n# Write the number of null elements\ncounted_number = train_set['major_discipline'].isnull().sum()\nprint(\"The number of null elements is {}.\".format(counted_number))\n\n# Fill missed values to 'Undetermined'\nfor dataset in two_sets_data:\n    dataset[\"major_discipline\"].fillna(\"Undetermined\", inplace = True)\n\nplot_hist_major_disc()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"It is also visible that the prevailing value is 'STEM'. It means that people in Datascience are mostly have a technical education like engineering, math etc. It is suggested to create 2 groups to make the dataset more balanced: 'STEM' and 'Not STEM', which contains all other groups."},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Change all unigue values except of 'STEM' to 'Others'.\nn_mj_disc = train_set.major_discipline.unique()\n\nfor major_discipline in n_mj_disc:\n    if major_discipline != 'STEM':\n        train_set['major_discipline'] = train_set['major_discipline'].replace([major_discipline],'Others')\n        \nplot_hist_major_disc()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### Type of University course enrolled if any"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Write the number of null elements\ncounted_number = train_set['enrolled_university'].isnull().sum()\nprint(\"The number of null elements is {}.\".format(counted_number))\n\n# Fill missed values to 'Undetermined'\nfor dataset in two_sets_data:\n    dataset[\"enrolled_university\"].fillna(\"Undetermined\", inplace = True)\n    \nn_univ = len(train_set.enrolled_university.unique())\nx = train_set.enrolled_university\nfig, ax0 = plt.subplots(1,1, figsize = (18, 5))\nsns.set(style=\"whitegrid\")\nsns.countplot(x, palette='Spectral')\nplt.title('Education')\nplt.xlabel('Education')\nplt.ylabel('The number on people')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### Training hours completed"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"n_training = len(train_set.training_hours.unique())\nn_training\nx = train_set.training_hours\nfig, axes = plt.subplots(1,1, figsize = (18, 5))\nsns.set(style=\"whitegrid\")\nsns.countplot(x, palette='Spectral')\nplt.title('Training hours')\nplt.xlabel('Hours')\nplt.ylabel('The number on people')\nplt.show()\n\nfig, axes = plt.subplots(1,1, figsize = (18, 5))\nsns.distplot(x)\nplt.title('Training hours')\nplt.xlabel('Hours')\nplt.ylabel('The number on people')\nplt.show()\n\nfig, axes = plt.subplots(1,1, figsize = (5, 5))\nx = train_set.training_hours\nsns.boxplot(data=x)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"## Experience"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### relevent_experience: Relevant experience of candidate"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Write the number of null elements\ncounted_number = train_set['enrolled_university'].isnull().sum()\nprint(\"The number of null elements is {}.\".format(counted_number))\n\ntrain_set.relevent_experience.unique()\nx = train_set.relevent_experience\nfig, ax0 = plt.subplots(1,1, figsize = (18, 5))\nsns.set(style=\"whitegrid\")\nsns.countplot(x, palette='Spectral')\nplt.title('Relevant experience')\nplt.xlabel('Experience')\nplt.ylabel('The number on people')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### Experience: Candidate total experience in years"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Write the number of null elements\ncounted_number = train_set['experience'].isnull().sum()\nprint(\"The number of null elements is {}.\".format(counted_number))\n\n# Fill missed values to '-1'.\nfor dataset in two_sets_data:\n    dataset[\"experience\"].fillna(\"-1\", inplace = True)\n\nsizes = []\ntrain_set_sorted = train_set.sort_values(by=['experience'])\nlabels     = train_set_sorted.experience.unique()\nexperience = len(train_set_sorted.experience.unique())\n\n# check if there are nan values\n# n_nan = train_set_sorted[train_set_sorted.experience == 'nan'].enrollee_id.count()\n\nfor i in range(len(train_set_sorted.experience.unique())):\n        exp              = train_set_sorted.experience.unique()[i]\n        counted_number   = train_set_sorted[train_set_sorted.experience == exp].experience.count()\n        sizes.append(counted_number)\n        # nan is float, hence\n        if type(experience) != 'str':\n            exp_str = str(exp)\n        else:\n            exp_str = exp\n\nfig1, ax1 = plt.subplots(1,1, figsize = (7, 7))\n\nfrom palettable.colorbrewer.qualitative import Pastel1_9\n# Create a circle for the center of the plot\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\nplt.pie(sizes, labels=labels, colors=Pastel1_9.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\n# plt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"## Complanies"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### Company size"},{"metadata":{"hideCode":true,"hidePrompt":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"# Write the number of null elements\ncounted_number = train_set['company_size'].isnull().sum()\nprint(\"The number of null elements is {}.\".format(counted_number))\n\n# # Fill missed values to '-1'.\n# Fill missed values to '-1'.\nfor dataset in two_sets_data:\n    dataset[\"company_size\"].fillna(\"-1\", inplace = True)\n\ntrain_set.company_size.unique()\nx = train_set.company_size\nfig, ax0 = plt.subplots(1,1, figsize = (18, 5))\nsns.set(style=\"whitegrid\")\nsns.countplot(x, palette='Spectral')\nplt.title('Company size')\nplt.xlabel('Companies')\nplt.ylabel('The number on people')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### Company type"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Write the number of null elements\ncounted_number = train_set['company_type'].isnull().sum()\nprint(\"The number of null elements is {}.\".format(counted_number))\n\n# # Fill missed values to 'Undetermined'\nfor dataset in two_sets_data:\n    dataset[\"company_type\"].fillna(\"Undetermined\", inplace = True)\n\nsizes = []\nlabels = train_set.company_type.unique()\nn_company_type = len(train_set.company_type.unique())\nfor i in range(len(train_set.company_type.unique())):\n    c_type           = train_set.company_type.unique()[i]\n    counted_number   = train_set[train_set.company_type == c_type].company_type.count()\n    sizes.append(counted_number)\n    # nan is float, hence\n    if type(c_type) != 'str':\n        c_type_str = str(c_type)\n    else:\n        c_type_str = c_type\n    print (c_type_str + \" \\t: \", counted_number)\n    \na = 0.25\nexplode = (0, a, 0, 0, 0, 0, 0)  \n\ncmap = plt.get_cmap('Spectral')\ncolors = [cmap(i) for i in np.linspace(0, 1, n_company_type)]\n\nfig1, ax1 = plt.subplots(1,1, figsize = (8, 8))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=180)\nax1.axis('equal')  \nplt.title(\"Company type\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"## Job"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### Difference in years between previous job and current job (last_new_job)"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Write the number of null elements\ncounted_number = train_set['last_new_job'].isnull().sum()\nprint(\"The number of null elements is {}.\".format(counted_number))\n\n# Fill missed values to 'Undetermined'\nfor dataset in two_sets_data:\n    dataset[\"last_new_job\"].fillna(\"-1\", inplace = True)\n\ntrain_set.last_new_job.unique()\nx = train_set.last_new_job\nfig, ax0 = plt.subplots(1,1, figsize = (18, 5))\nsns.set(style=\"whitegrid\")\nsns.countplot(x, palette='Spectral')\nplt.title('Relevant experience')\nplt.xlabel('Time gap between last job and new job')\nplt.ylabel('The number on people')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(train_set.last_new_job.unique())\n\n# fit = preprocessing.LabelEncoder()\n# fit.fit(['1', '>4', 'never', '4', '3', '2', '-1'])\n# train_set['last_new_job'].replace('>4', '5', inplace=True)\n# train_set['last_new_job'].replace('never', '0', inplace=True)\n# train_set['last_new_job'].replace('-1', '6', inplace=True)\n# train_set['last_new_job'] = train_set['last_new_job'].astype(int)\n# train_set['last_new_job'].dtypes\n\n# print(train_set_droped2.last_new_job.unique())","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### target: 0 â€“ Not looking for job change, 1 â€“ Looking for a job change"},{"metadata":{"hideCode":true,"hidePrompt":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"x = train_set.target\nfig, ax0 = plt.subplots(1,1, figsize = (10, 10))\nsns.set(style=\"whitegrid\")\nsns.countplot(x, palette='Spectral')\nplt.title('Target')\nplt.xlabel('')\nplt.ylabel('The number on people')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"---"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"## Intersections of the data and correlations"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"#### Intersections\nOne can see that the dataset is unbalanced, in other words, one type of considered variants is dominant. That fact can lead to misprediction in the future. \nAnyway, let's look at the crossing of dominant characteristics.\n\nThe highly dominant characteristics are :\n1. Gender : man\n\n2. graduated\n\n3. city_103 (city dev. index = 0.920)\n\n4. STEM\n\n5. no_enrollnent\n\n6. Pvt Ltd\n\n7. 1 year of exp\n\nLet's have a look at the intersections of these creteria. \nThe criteria of separation are the dominant characteristics. For that goal we will construct Venn diagrams."},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"gender            = 'Male'\nsubset_gender     = train_set[train_set.gender == gender].enrollee_id\nprint('subset_gender : ', len(subset_gender))\n\nmajor_discipline  = 'STEM'\nsubset_discipline = train_set[train_set.major_discipline == major_discipline].enrollee_id\nprint('subset_discipline : ', len(subset_discipline))\n\ncompany_type      = 'Pvt Ltd'\nsubset_comp_type  = train_set[train_set.company_type == company_type].enrollee_id\nprint('subset_comp_type : ', len(subset_comp_type))\n\nfrom matplotlib_venn import venn3\nfig, ax0 = plt.subplots(1,1, figsize = (7, 7))\nvenn3([set(subset_gender), set(subset_discipline), set(subset_comp_type)], \n      set_labels = ('Male', 'STEM', 'Pvt Ltd'))\nplt.title('Intersection')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"city              = 'city_103'\nsubset_city_103   = train_set[train_set.city == city].enrollee_id\nprint('subset_city_103 : ', len(subset_city_103))\n\neducation         = 'Graduate'\nsubset_education  = train_set[train_set.education_level == education].enrollee_id\nprint('subset_education : ', len(subset_education))\n\nuniversity        = 'no_enrollment'\nsubset_univ       = train_set[train_set.enrolled_university == university].enrollee_id\nprint('subset_univ : ', len(subset_univ))\n\nfig, ax0 = plt.subplots(1,1, figsize = (7, 7))\nvenn3([set(subset_city_103), set(subset_education), set(subset_univ)], \n      set_labels = ('city_103', 'Graduate', 'no_enrollment'))\nplt.title('Intersection')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Correlations\n\nTo consider correlations between the criterias in the dataset, we will construct the correlation heat map."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#correlation heatmap of dataset\nmatrix = train_set[[\"city\",\"city_development_index\",\"gender\",\"relevent_experience\",\"enrolled_university\",\"education_level\",\n                   \"major_discipline\",\"experience\",\"company_size\",\"company_type\",\"last_new_job\",\"training_hours\",\"target\"]].corr(method ='pearson')\n\nsns.set(font_scale=1.10)\nplt.figure(figsize=(15, 10))\nsns.heatmap(matrix,  linewidths=0.05,\n            square=True,annot=True,cmap='Purples_r',linecolor=\"white\")\nplt.title('Correlation');","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":""},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"---"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"## Data quality check\nHere we analyse how much data is not suitable for further work.\n\nThe things which we check:\n\n1. Absence of data (null, nan).\n\n2. Untypical data (outliers).\n\n3. Dublicates.\n\n4. Inconsistent data (the same data, presented in different formats or registers)."},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"---"},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"# Model and training"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"train_set.info()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"print(train_set.relevent_experience.unique())\nfit = preprocessing.LabelEncoder()\nfit.fit(['Has relevent experience', 'No relevent experience'])\ntrain_set['relevent_experience'] = fit.transform(train_set.relevent_experience)\nprint(train_set.relevent_experience.unique())","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"print(train_set.enrolled_university.unique())\nfit = preprocessing.LabelEncoder()\nfit.fit([ 'no_enrollment', 'Full time course', 'Undetermined', 'Part time course'])\ntrain_set['enrolled_university'] = fit.transform(train_set.enrolled_university)\nprint(train_set.enrolled_university.unique())","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"print(train_set.education_level.unique())\nfit = preprocessing.LabelEncoder()\nfit.fit(['Graduate', 'Others'])\ntrain_set['education_level'] = fit.transform(train_set.education_level)\nprint(train_set.education_level.unique())","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"print(train_set.major_discipline.unique())\nfit = preprocessing.LabelEncoder()\nfit.fit(['STEM', 'Others'])\ntrain_set['major_discipline'] = fit.transform(train_set.major_discipline)\nprint(train_set.major_discipline.unique())","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"print(train_set.experience.unique())\nfit = preprocessing.LabelEncoder()\nfit.fit(['>20', '15', '5', '<1', '11', '13', '7', '17', '2', '16', '1', '4', \n         '10', '14', '18', '19', '12', '3', '6', '9', '8', '20'])\n\ntrain_set['experience'].replace('>20', '21', inplace=True)\ntrain_set['experience'].replace('<1', '0', inplace=True)\ntrain_set['experience'] = train_set['experience'].astype(int)\ntrain_set['experience'].dtypes\nprint(train_set.experience.unique())","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"print(train_set.last_new_job.unique())\nfit = preprocessing.LabelEncoder()\nfit.fit(['1', '>4', 'never', '4', '3', '2', '-1'])\ntrain_set['last_new_job'].replace('>4', '5', inplace=True)\ntrain_set['last_new_job'].replace('never', '0', inplace=True)\ntrain_set['last_new_job'] = train_set['last_new_job'].astype(int)\ntrain_set['last_new_job'].dtypes\nprint(train_set.last_new_job.unique())","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"n = len(train_set.city.unique())\nuniq_array = train_set.city.unique()\nfor i in range(n):\n    string = uniq_array[i]\n    train_set['city'].replace(uniq_array[i], string.strip('city_'), inplace=True)\ntrain_set['city'] = train_set['city'].astype(int)\nprint(train_set.city.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(train_set.gender.unique())\nuniq_array = train_set.gender.unique()\nfit = preprocessing.LabelEncoder()\nfit.fit(['Male', 'No gender', 'Female', 'Other'])\ntrain_set['gender'] = fit.transform(train_set.gender)\nprint(train_set.gender.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(train_set.company_size.unique())\nuniq_array = train_set.company_size.unique()\nprint(uniq_array)\nfit = preprocessing.LabelEncoder()\nfit.fit(['-1', '50-99', '<10', '10000+', '5000-9999', '1000-4999', '10/49', '100-500', '500-999'])\ntrain_set['company_size'] = fit.transform(train_set.company_size)\nprint(train_set.company_size.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = len(train_set.company_size.unique())\nuniq_array = train_set.company_type.unique()\nprint(uniq_array)\nfit = preprocessing.LabelEncoder()\nfit.fit(['Undetermined', 'Pvt Ltd', 'Funded Startup', 'Early Stage Startup', 'Other', 'Public Sector', 'NGO'])\ntrain_set['company_type'] = fit.transform(train_set.company_type)\nprint(train_set.company_type.unique())","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"train_set.info()","execution_count":null,"outputs":[]},{"metadata":{"hideCode":true,"hidePrompt":true},"cell_type":"markdown","source":"# Models"},{"metadata":{},"cell_type":"markdown","source":"### Preparation data"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Importing packages for SMOTE\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# separate dataset\nX_dataset = train_set.drop(columns=['target']).values\nY_dataset = train_set[['target']].values\n\n# oversampling\n\n# 1 \noversampling = SMOTE()\nX_dataset, Y_dataset = oversampling.fit_resample(X_dataset, Y_dataset)\n\n# 2\n# oversampling = RandomOverSampler(random_state=42)\n# oversampling.fit(X_dataset, Y_dataset)\n# X_dataset1, Y_dataset1 = oversampling.fit_resample(X_dataset, Y_dataset)\n\n# 3 UnderSa\n# rus = RandomUnderSampler(random_state=0)\n# rus.fit(X_dataset, Y_dataset)\n# X_dataset1, Y_dataset1 = rus.fit_resample(X_dataset, Y_dataset)\n\n# devision of the thain and test parts\nX_trainset, X_testset, Y_trainset, Y_testset = train_test_split(X_dataset, Y_dataset, test_size=0.3, random_state=3)\n\n#Check if the shape is the same \nif (X_testset.shape[0] != Y_testset.shape[0]):\n    print ('The array don\\'t much!')\n    print (X_testset.shape)\n    print (Y_testset.shape)\nelse: \n    print('The array do much.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"hideCode":true,"hidePrompt":true,"trusted":true},"cell_type":"code","source":"# Learning\nmodel_tree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 8)\nmodel_tree.fit(X_trainset,Y_trainset)\n\n# Prediction\npredTree = model_tree.predict(X_testset)\n\n# Metrics and accuracy\nprint(classification_report(Y_testset, predTree))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Learning\nforest = RandomForestClassifier( max_depth = 9)\nforest.fit(X_trainset,Y_trainset)\n\n# Prediction\npredForest = forest.predict(X_testset)\n\n# Metrics and accuracy\nprint(classification_report(Y_testset, predForest))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}