{"cells":[{"metadata":{},"cell_type":"markdown","source":"Using the HR Analytics data set available to find Logistic Regression, Random Forest Model, and Decision Tree and check the accuracy of the model\n\n\nUnderstanding why and when employees are most likely to leave can lead to actions to improve employee retention as well as possibly planning new hiring in advance. This model will help predicting the employee retention in a company usimg different algorithms."},{"metadata":{},"cell_type":"markdown","source":"In the most recent many years, having the best machines were sufficient to be serious or to rule a mechanical area. These days, the organization that has more connected with and gainful workers will have a superior possibility of winning business sector rivalry. Therefore, organizations can not lose significant workers and when that starts to happen you need to get why to keep this from occurring. \n\nThe Human Resources Analytics dataset is utilized to clarify the initial phases in the Da way. In this initial segment are introduced to how to get acclimated with the informational collection by playing out the illustrative investigation. Strategies, for example, exploratory Data Analysis (EDA) permit us to introduce the information in a more important manner, applying general measurable techniques and exploratory designs, that permit a less complex translation before the draw in an machine Learning calculations. \n\nThe Human Resources Analytics is a reproduced dataset from Kaggle and the center is to comprehend why the best and most experienced workers are leaving the organization. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.read_csv('../input/hr-analytics/HR_comma_sep.csv')\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['Department'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['salary'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"table = data.pivot_table(values='satisfaction_level', index='Department', columns='salary', aggfunc=np.count_nonzero)\ntable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"left = data[data.left==1]\nleft.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\ncorr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(style='white')\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(13,8))\ncmap = sns.diverging_palette(10,220, as_cmap=True)\nax = sns.heatmap(corr, mask=mask, cmap=cmap, vmax= .5, annot=True, annot_kws= {'size':11}, square=True, xticklabels=True, yticklabels=True, linewidths=.5, \n           cbar_kws={'shrink': .5}, ax=ax)\nax.set_title('Correlation between variables', fontsize=20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"retained = data[data.left==0]\nleft.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby('left').mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.salary, data.left).plot(kind='bar') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(data.Department, data.left).plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subdf = data[['satisfaction_level','average_montly_hours','promotion_last_5years','salary']]\nsubdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dummies = pd.get_dummies(data.salary, prefix='salary')\ndf3 = pd.concat([subdf, dummies], axis=1)\ndf3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4 = df3.drop(['salary', 'salary_high'], axis='columns')\ndf4.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df4\ny = data.left","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg.score(X_test, y_test)\nprint(\"Accuracy on Training Set is: \", reg.score(X_train, y_train))\nprint(\"Accuracy on Testing Set is: \", reg.score(X_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Logistic Regression Model Error Table"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\ny_pred = reg.predict(X_test)\nprint('\\t\\tError Table')\nprint('Mean Absolute Error is: ', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error is: ', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error is: ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('Root Squared Error is: ', metrics.r2_score(y_test, y_pred))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\nrandommodel = RandomForestClassifier()\nrandommodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Random Model Accuracy on train data is:', randommodel.score(X_train, y_train))\nprint('Random Model Accuracy on test data is:', randommodel.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = randommodel.predict(X_test)\nprint('\\t\\tError Table')\nprint('Mean Absolute Error       :', metrics.mean_absolute_error(y_test,y_pred))\nprint('Mean Squared Error        :', metrics.mean_squared_error(y_test,y_pred))\nprint('Root Mean Squared Error   :', np.sqrt(metrics.mean_squared_error(y_test,y_pred)))\nprint('Mean Absolute Error       :', metrics.r2_score(y_test,y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Decision Tree classifier Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import tree\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nmodeltree = tree.DecisionTreeClassifier()\nmodeltree.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model Accuracy on train data:', modeltree.score(X_train,y_train))\nprint('Model Accuracy on test data :', modeltree.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = modeltree.predict(X_test)\nprint(\"\\t\\tError Table\")\nprint('Mean Absolute Error     : ', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error      : ', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error : ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('R Squared Error         : ', metrics.r2_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Area Under-Receiving Operating Characteristic Curve Evaluation Metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve, roc_auc_score\n\n# Getting predicted probabilities\ny_score1 = reg.predict_proba(X_test)[:,1]\ny_score2 = modeltree.predict_proba(X_test)[:,1]\ny_score3 = randommodel.predict_proba(X_test)[:,1]\n# Creating true and false positive rate\nfalse_positive_rate1, true_positive_rate1, threshold1 = roc_curve(y_test, y_score1)\nfalse_positive_rate2, true_positive_rate2, threshold2 = roc_curve(y_test, y_score2)\nfalse_positive_rate3, true_positive_rate3, threshold3 = roc_curve(y_test, y_score3)\n\nreg_roc_auc    = roc_auc_score(y_test, y_score1)\ntree_roc_auc   = roc_auc_score(y_test, y_score2)\nrandom_roc_auc = roc_auc_score(y_test, y_score3)\n\nprint('roc_auc_score for Logistic Regression: ', reg_roc_auc)\nprint('roc_auc_score for DecisionTree: ', tree_roc_auc)\nprint('roc_auc_score for RandomForest: ', random_roc_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(9, 6))\n\n# Plot Logistic Regression ROC\nplt.plot(false_positive_rate1,true_positive_rate1,linestyle= '--',label='Logistic Regression(area = %0.3f)'\n         % reg_roc_auc)\n\n# Plot Decision Tree ROC\nplt.plot(false_positive_rate2,true_positive_rate2,linestyle= '--',label='Decision Tree (area = %0.3f)'\n         % tree_roc_auc)\n\n# Plot Random Forest ROC\nplt.plot(false_positive_rate3,true_positive_rate3,linestyle= '--',label='Random Forest (area = %0.3f)'\n         % random_roc_auc)\n\n#Plotting Base Rate ROC\nplt.plot([0,1], [0,1], linestyle='--', label='Base Rate')\n\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Area Under-Receiving Operating Characteristic Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion\n\nThe higher the AUC, the better is the performance of the model at distiniguishing between the positive and negative classes.\n\nFrom the above graph we can see that AUC for Decision Tree ROC curve(Area=97.7%) which is higher than Logistic model(Area=77.5%) and Random Forest model(Area=96.6%)\n\nTo conclude, Decision Tree did a better job in classifying the positive class in the dataset and gives the most accuracy in result."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}