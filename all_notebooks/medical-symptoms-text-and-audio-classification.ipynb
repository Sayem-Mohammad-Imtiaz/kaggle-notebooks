{"cells":[{"metadata":{"_uuid":"57a086870bc52aaf6b03da01cffbaa9133ca7369"},"cell_type":"markdown","source":"**Medical Text and Audio Classification with Fastai**"},{"metadata":{"_uuid":"7c3a125da19eb5dbc755571865e76ebb057f4d52"},"cell_type":"markdown","source":"I stumbled across an interesting dataset containing verbal descriptions of medical symptoms (.wav, audio data) paired with text transcriptions (.csv, text data) and labeled according to the category of the ailment.  I have never worked with audio data before and so I decided to explore this dataset.\n\nHere I use the fastai library to classify medical text and audio according to the category of the ailment being described.\n"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"bd086b8af02dc596bf765541e29940eda3366ca5"},"cell_type":"code","source":"# **Step 1: Import Python Packages** \n\n# Fastai, Librosa, Spacy, Scispacy, PySound, Seaborn, etc","execution_count":1,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"55af58d2ae1e018a19db5043b9795b79181f0b6d"},"cell_type":"code","source":"!pip install scispacy\n!pip install pysoundfile\n!apt-get install libav-tools -y\n!apt-get install zip\n!pip freeze > '../working/dockerimage_snapshot.txt'","execution_count":null,"outputs":[{"output_type":"stream","text":"Collecting scispacy\n  Downloading https://files.pythonhosted.org/packages/b2/68/33d18f448dfddda2392ffd9f4ef349c3627a9bf91806f55e1bf91ed64e75/scispacy-0.1.0-py3-none-any.whl\nCollecting awscli (from scispacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/5b/ca70b0804813dda500736b0854ba15145442fa0a3ce3382d7688359fdd27/awscli-1.16.116-py2.py3-none-any.whl (1.5MB)\n\u001b[K    100% |████████████████████████████████| 1.5MB 17.3MB/s ta 0:00:01\n\u001b[?25hCollecting conllu (from scispacy)\n  Downloading https://files.pythonhosted.org/packages/ca/82/b02495f1c594cfb4af9b1eb8f404e35c1298a1448fc950b37f14c3e83317/conllu-1.2.3-py2.py3-none-any.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from scispacy) (1.16.1)\nRequirement already satisfied: spacy==2.0.18 in /opt/conda/lib/python3.6/site-packages (from scispacy) (2.0.18)\nRequirement already satisfied: PyYAML<=3.13,>=3.10 in /opt/conda/lib/python3.6/site-packages (from awscli->scispacy) (3.12)\nRequirement already satisfied: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from awscli->scispacy) (0.14)\nCollecting botocore==1.12.106 (from awscli->scispacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/27/ec2c22fdc556c142c1cdf37a7335156482e5298db71980567961ab299ea4/botocore-1.12.106-py2.py3-none-any.whl (5.3MB)\n\u001b[K    100% |████████████████████████████████| 5.3MB 8.0MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from awscli->scispacy) (0.2.0)\nRequirement already satisfied: colorama<=0.3.9,>=0.2.5 in /opt/conda/lib/python3.6/site-packages (from awscli->scispacy) (0.3.9)\nCollecting rsa<=3.5.0,>=3.1.2 (from awscli->scispacy)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n\u001b[K    100% |████████████████████████████████| 51kB 20.8MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (1.0.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (2.0.2)\nRequirement already satisfied: preshed<2.1.0,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (2.0.1)\nRequirement already satisfied: thinc<6.13.0,>=6.12.1 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (6.12.1)\nRequirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (0.9.6)\nRequirement already satisfied: ujson>=1.35 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (1.35)\nRequirement already satisfied: dill<0.3,>=0.2 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (0.2.9)\nRequirement already satisfied: regex==2018.01.10 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (2018.1.10)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.6/site-packages (from spacy==2.0.18->scispacy) (2.21.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore==1.12.106->awscli->scispacy) (2.6.0)\nRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from botocore==1.12.106->awscli->scispacy) (0.9.3)\nRequirement already satisfied: urllib3<1.25,>=1.20; python_version >= \"3.4\" in /opt/conda/lib/python3.6/site-packages (from botocore==1.12.106->awscli->scispacy) (1.22)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa<=3.5.0,>=3.1.2->awscli->scispacy) (0.4.5)\nRequirement already satisfied: msgpack<0.6.0,>=0.5.6 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (0.5.6)\nRequirement already satisfied: msgpack-numpy<0.4.4 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (0.4.3.2)\nRequirement already satisfied: cytoolz<0.10,>=0.9.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (0.9.0.1)\nRequirement already satisfied: wrapt<1.11.0,>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (1.10.11)\nRequirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (4.31.1)\nRequirement already satisfied: six<2.0.0,>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (1.12.0)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->scispacy) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->scispacy) (2.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.18->scispacy) (2018.11.29)\nRequirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy==2.0.18->scispacy) (0.9.0)\n","name":"stdout"}]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"from fastai.text import *\nfrom fastai.vision import *\nimport spacy\nfrom spacy import displacy\nimport scispacy\nimport librosa\nimport librosa.display\nimport soundfile as sf\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nimport IPython\nimport os\nfrom glob import glob\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pylab\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"aecf19b48d10c2a80ed7d4d04943c9b670363178"},"cell_type":"code","source":"# **Step 3: Define Helper Functions**\n\n# Create spectrograms and word frequency plots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90ef211283f429d80a37ea45db0b1e0127772851","_kg_hide-input":true},"cell_type":"code","source":"def get_wav_info(wav_file):\n    data, rate = sf.read(wav_file)\n    return data, rate\n\ndef create_spectrogram(wav_file):\n    # adapted from Andrew Ng Deep Learning Specialization Course 5\n    data, rate = get_wav_info(wav_file)\n    nfft = 200 # Length of each window segment\n    fs = 8000 # Sampling frequencies\n    noverlap = 120 # Overlap between windows\n    nchannels = data.ndim\n    if nchannels == 1:\n        pxx, freqs, bins, im = plt.specgram(data, nfft, fs, noverlap = noverlap)\n    elif nchannels == 2:\n        pxx, freqs, bins, im = plt.specgram(data[:,0], nfft, fs, noverlap = noverlap)\n    return pxx\n\ndef create_melspectrogram(filename,name):\n    # adapted from https://www.kaggle.com/devilsknight/sound-classification-using-spectrogram-images\n    plt.interactive(False)\n    clip, sample_rate = librosa.load(filename, sr=None)\n    fig = plt.figure(figsize=[0.72,0.72])\n    ax = fig.add_subplot(111)\n    ax.axes.get_xaxis().set_visible(False)\n    ax.axes.get_yaxis().set_visible(False)\n    ax.set_frame_on(False)\n    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n    filename  = Path('/kaggle/working/spectrograms/' + name + '.jpg')\n    plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n    plt.close()    \n    fig.clf()\n    plt.close(fig)\n    plt.close('all')\n    del filename,name,clip,sample_rate,fig,ax,S\n\ndef wordBarGraphFunction(df,column,title):\n    # adapted from https://www.kaggle.com/benhamner/most-common-forum-topic-words\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n    plt.title(title)\n    plt.show()\n\ndef wordCloudFunction(df,column,numWords):\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n    word_string=str(popular_words_nonstop)\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                          background_color='white',\n                          max_words=numWords,\n                          width=1000,height=1000,\n                         ).generate(word_string)\n    plt.clf()\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"overview = pd.read_csv('../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/overview-of-recordings.csv')\noverview = overview[['file_name','phrase','prompt','overall_quality_of_the_audio','speaker_id']]\noverview=overview.dropna()\noverviewAudio = overview[['file_name','prompt']]\noverviewAudio['spec_name'] = overviewAudio['file_name'].str.rstrip('.wav')\noverviewAudio = overviewAudio[['spec_name','prompt']]\noverviewText = overview[['phrase','prompt']]\nnoNaNcsv = '../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/overview-of-recordings.csv'\nnoNaNcsv = pd.read_csv(noNaNcsv)\nnoNaNcsv = noNaNcsv.dropna()\nnoNaNcsv = noNaNcsv.to_csv('overview-of-recordings.csv',index=False)\nnoNaNcsv","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Part 1 of 3: Exploratory Data Analysis and Data Visualization**\n\n\nThe dataset consists of verbal descriptions of medical symptoms (.wav, audio data) paired with text transcriptions (.csv, text data) and labeled according to the category of the ailment.\n\nHere is a sample of the .csv file that accompanies the .wav audio files."},{"metadata":{"trusted":true,"_uuid":"bc830faf4b4972acbc45183f63b5abc7e89bf998","_kg_hide-input":true},"cell_type":"code","source":"overview[110:120]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cd046574b577158450e887eadd80f22f375ac89"},"cell_type":"markdown","source":"The categories of ailments and the quality of the audio descriptions are described below:"},{"metadata":{"trusted":true,"_uuid":"7d3297820b22d0916799af32fa6e704f1acaa736","_kg_hide-input":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\npromptsPlot = sns.countplot(y='prompt',data=overview)\npromptsPlot\n\nqualityPlot = sns.FacetGrid(overview,aspect=2.5)\nqualityPlot.map(sns.kdeplot,'overall_quality_of_the_audio',shade= True)\nqualityPlot.set(xlim=(2.5, overview['overall_quality_of_the_audio'].max()))\nqualityPlot.set_axis_labels('overall_quality_of_the_audio', 'Proportion')\nqualityPlot","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38dbfec4ce35d060073532a0e374a18455211eeb"},"cell_type":"markdown","source":"And here we zoom in on one specific example:\n"},{"metadata":{"trusted":true,"_uuid":"f985884a4638ad4f2db519535f5ca37406238ace","_kg_hide-input":true},"cell_type":"code","source":"overview[62:63]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d80b33d158ee958f04b82ce5c2b4130203883904","_kg_hide-input":true},"cell_type":"code","source":"en_core_sci_sm = '../input/scispacy-pretrained-models/scispacy pretrained models/Scispacy Pretrained Models/en_core_sci_sm-0.1.0/en_core_sci_sm/en_core_sci_sm-0.1.0'\nnlp = spacy.load(en_core_sci_sm)\ntext = overview['phrase'][62]\ndoc = nlp(text)\nprint(list(doc.sents))\nprint(doc.ents)\ndisplacy.render(next(doc.sents), style='dep', jupyter=True,options = {'compact': True, 'word_spacing': 45, 'distance': 90})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76a92eb78edad64d3a4f7002c1b75ee5583cc387","_kg_hide-input":true},"cell_type":"code","source":"IPython.display.Audio('../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/test/1249120_20518958_23074828.wav')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"134214fc15be86c89894db2a1d13af7c30de5ae1"},"cell_type":"markdown","source":"Here is another example:\n"},{"metadata":{"trusted":true,"_uuid":"fcc3c7640d4c0ab364e2a8e3644cb70c68c3d48f","_kg_hide-input":true},"cell_type":"code","source":"overview[118:119]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b7d746b6ff37bebeee99ff3845ba35b64c2bb86","_kg_hide-input":true},"cell_type":"code","source":"en_core_sci_sm = '../input/scispacy-pretrained-models/scispacy pretrained models/Scispacy Pretrained Models/en_core_sci_sm-0.1.0/en_core_sci_sm/en_core_sci_sm-0.1.0'\nnlp = spacy.load(en_core_sci_sm)\ntext = overview['phrase'][118]\ndoc = nlp(text)\nprint(list(doc.sents))\nprint(doc.ents)\ndisplacy.render(next(doc.sents), style='dep', jupyter=True,options = {'compact': True, 'word_spacing': 45, 'distance': 90})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ae08669d65e4aa895792c62318613df7840a0d1","_kg_hide-input":true},"cell_type":"code","source":"IPython.display.Audio('../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/test/1249120_43788827_53247832.wav')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ccf40d9df5ad1256d880b9fea6df224c0d1346c"},"cell_type":"markdown","source":"These are the most common words that are described in the text descriptions:\n"},{"metadata":{"trusted":true,"_uuid":"b037d5660ddd3af691f74feddb3f97d4f8269538","_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nwordCloudFunction(overview,'phrase',10000000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9593f576f5211a9f41a3e57884956fad1d13d6d","_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nwordBarGraphFunction(overview,'phrase',\"Most Common Words in Medical Text Transcripts\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"933ab5502145b6ae5d2ce436ec8606a8aa3c5038"},"cell_type":"markdown","source":"**Part 2 of 3: Classify Ailment from Text Description**\n\nNext I will use the [fastai.text_classifier_learner()](https://docs.fast.ai/text.learner.html) functions to categorize the text descriptions according to the ailment category being described."},{"metadata":{"trusted":true,"_uuid":"154f1eb6aa52f6ec02d934f7d5f3242612204265"},"cell_type":"code","source":"np.random.seed(7)\npath = Path('../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/')\ndata_clas = (TextList.from_csv(path, 'overview-of-recordings.csv', \n                               cols='phrase')\n                   .random_split_by_pct(.2)\n                   .label_from_df(cols='prompt')\n                   .databunch(bs=42))\nMODEL_PATH = \"/tmp/model/\"\nlearn = text_classifier_learner(data_clas,model_dir=MODEL_PATH,arch=AWD_LSTM)\nlearn.fit_one_cycle(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60359f7e02a5ceb236c1f44ee175dab3337e2749"},"cell_type":"code","source":"learn.unfreeze()\nlearn.fit_one_cycle(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d99271c65e6633f7181cebd102b4d5a17d93197c"},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(10,10), dpi=60)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c27a7f68c8a92ae02a669ca477dacf2dd8935b5"},"cell_type":"markdown","source":"It worked! We were able to classify the category of the ailment being described from the text description of the symptoms and we were able to do so with a high accuracy.  \n\nNow let's try to do the same thing but with the audio descriptions."},{"metadata":{"_uuid":"b2fd5cca72f10635ccaca631900a04f3d17eeab1"},"cell_type":"markdown","source":"**Part 3 of 3: Classify Ailment from Audio Description**\n\nNext I will convert the .wav files into .jpg spectrograms and then again I will attempt to classify the audio descriptions according to the category of the ailment that is being described."},{"metadata":{"_uuid":"900ed4e80bf24f2138caafd81ed2b1fb2dcc3a92"},"cell_type":"markdown","source":"A spectrogram is a visual representation of a sound. The x-axis represents time, the y-axis represents frequency, and the third dimension (intensity or color) represents the amplitutde of a specific frequency at a specific point in time.\n"},{"metadata":{"trusted":true,"_uuid":"faa1da3670f01d5590540714e3085b7d53ce5afa","_kg_hide-input":true},"cell_type":"code","source":"testAudio = \"../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/train/1249120_44176037_58635902.wav\"\nx = create_spectrogram(testAudio)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fec698fbf098f6dccfbc3f5d787fdb6630500be"},"cell_type":"markdown","source":"Prior work has shown that it can be advantageous to transform the spectrogram into a melspectrogram before proceeding with computer vision applications.  For more information, see: https://en.wikipedia.org/wiki/Mel-frequency_cepstrum.\n\nHere is a representative melspectrogram:"},{"metadata":{"trusted":true,"_uuid":"782e4b2fe4d629bb43ab770a5584bda852b07070","_kg_hide-input":true},"cell_type":"code","source":"filename = \"../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/train/1249120_44176037_58635902.wav\"\nclip, sample_rate = librosa.load(filename, sr=None)\nfig = plt.figure(figsize=[5,5])\nS = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\nlibrosa.display.specshow(librosa.power_to_db(S, ref=np.max))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73b08a90f5ef425211a446060c5953a850626451"},"cell_type":"markdown","source":"Next I convert all of the .wav audio files into .jpg melspectrogram files."},{"metadata":{"trusted":true,"_uuid":"bb39040f2c7ea3d08eb044acd48571f705a9490a","_kg_hide-input":true},"cell_type":"code","source":"!mkdir /kaggle/working/spectrograms\n\nData_dir_train=np.array(glob(\"../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/train/*\"))\nData_dir_test=np.array(glob(\"../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/test/*\"))\nData_dir_val=np.array(glob(\"../input/medical-speech-transcription-and-intent/medical speech transcription and intent/Medical Speech, Transcription, and Intent/recordings/validate/*\"))\n\nfor file in tqdm(Data_dir_train):\n    filename,name = file,file.split('/')[-1].split('.')[0]\n    create_melspectrogram(filename,name)\nfor file in tqdm(Data_dir_test):\n    filename,name = file,file.split('/')[-1].split('.')[0]\n    create_melspectrogram(filename,name)\nfor file in tqdm(Data_dir_val):\n    filename,name = file,file.split('/')[-1].split('.')[0]\n    create_melspectrogram(filename,name)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"894843d2a55007e2db847b8c93e8c4ca3928e6ab"},"cell_type":"markdown","source":"Then I use the [fastai.create_cnn()](https://docs.fast.ai/vision.learner.html) to classify the melspectrogram images according to the category of the ailment that is being described in the audio description."},{"metadata":{"trusted":true,"_uuid":"15d97644e5e50dbadfe25d1fe83e5176388d9c35"},"cell_type":"code","source":"path = Path('/kaggle/working/')\nnp.random.seed(7)\ndata = ImageDataBunch.from_df(path,df=overviewAudio, folder=\"spectrograms\", valid_pct=0.2, suffix='.jpg',\n        ds_tfms=get_transforms(), size=299, num_workers=0).normalize(imagenet_stats)\nlearn = create_cnn(data, models.resnet50, metrics=accuracy)\nlearn.fit_one_cycle(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5aa034a7ad40224b706dce25193a5e93bec69bba"},"cell_type":"code","source":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c11a008d5fd0b762fefbe7e340789a44b6f2886"},"cell_type":"code","source":"learn.fit_one_cycle(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5592d40b7cf53cced14a182918c0d70f108abac"},"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(10,10), dpi=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a127b25c1413460b8c2f9f0446a754422cc39d20","_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"!zip -r spectrograms.zip /kaggle/working/spectrograms/\n!rm -rf spectrograms/*","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4f49f4ad12b3a0222f61dc34e157b733a8f8b0d"},"cell_type":"markdown","source":"In the end we were able to classify the category of the ailment being described from the audio description of the symptoms and we were able to do so with an accuracy that was much better than random chance (albeit much less accurate than earlier when we performed this same classification task using the text transcriptions instead of the audio files).\n"},{"metadata":{"_uuid":"6b6a97efd4cd17008bbf06f691376fdf269059f8"},"cell_type":"markdown","source":"**Summary** \n\nHere we use the [fastai.text_classifier_learner()](https://docs.fast.ai/text.learner.html) functions to classify text descriptions of medical symptoms according to the category of the ailment  being described.  Likewise, we use the [fastai.create_cnn()](https://docs.fast.ai/vision.learner.html) functions to classify melspectrogram audio descriptions of medical symptoms according to the category of the ailment being described in the audio file.\n\n\nPlease note that some of the labels are incorrect and some of the audio files have poor quality.  To improve the models that are produced by this kernel I would recommend cleaning the dataset in much more detail."},{"metadata":{"_uuid":"8a98497d4ebda7a366d55be294e815bfe0562553"},"cell_type":"markdown","source":"**Credit:**"},{"metadata":{"_uuid":"e242cb3c8bd62d4bde820ebc85e9817deac3d754"},"cell_type":"markdown","source":"Inspired by:\n* [Jeremey Howard's Deep Learning Course](https://course.fast.ai/) (Lesson 1: Fastai and Convolutional Neural Networks; Lesson 4: NLP; Tabular data; Collaborative filtering; Embeddings)\n* [Andrew Ng's Deep Learning Course](https://www.coursera.org/specializations/deep-learning) (Lesson 5: Spectrograms and Audio Data)\n\nWith select functions adapted from:\n\n* [most-common-forum-topic-words](https://www.kaggle.com/benhamner/most-common-forum-topic-words) (plot word frequencies)\n* [play-audio-read-the-files-create-a-spectrogram](https://www.kaggle.com/vbookshelf/play-audio-read-the-files-create-a-spectrogram) (preview audio files)\n* [sound-classification-using-spectrogram-images](https://www.kaggle.com/devilsknight/sound-classification-using-spectrogram-images) (create spectrograms)\n\n\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}