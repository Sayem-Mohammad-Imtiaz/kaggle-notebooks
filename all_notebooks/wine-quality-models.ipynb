{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas\n\ndf = pandas.read_csv('/kaggle/input/cusersmarildownloadswinecsv/wine.csv',  delimiter=';')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Codes by Nataliia Rostoropova \n\n\nhttps://medium.com/analytics-vidhya/step-by-step-guide-for-predicting-wine-quality-using-scikit-learn-de5869f8f91a\n\nhttps://gist.githubusercontent.com/NataliiaRastoropova/1d861775c47516d99e8dcb444eaa7049/raw/da82bdcd1ba4d50451bdf837755457fc21aca043/Wine_preferences_note_book.ipynb\n\nhttps://gist.github.com/NataliiaRastoropova/1d861775c47516d99e8dcb444eaa7049"},{"metadata":{"trusted":true},"cell_type":"code","source":"colum_names = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol']\ndf_pivot_table = df.pivot_table(colum_names,['quality'], aggfunc='median')\n\ndf_pivot_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(6, 5))\nsns.barplot(x='quality', y='sulphates', data=df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df['quality'], hist=True, kde=True,\n             bins='auto', color = 'darkblue',\n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n#fill in mean for floats\nfor c in df.columns:\n    if df[c].dtype=='float16' or  df[c].dtype=='float32' or  df[c].dtype=='float64':\n        df[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf = df.fillna(-999)\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n        \nprint('Labelling done.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Data Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://gist.githubusercontent.com/NataliiaRastoropova/1d861775c47516d99e8dcb444eaa7049/raw/da82bdcd1ba4d50451bdf837755457fc21aca043/Wine_preferences_note_book.ipynb\n# Dividing wine as good and bad by giving the limit for the quality\nbins = (2, 6, 8)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"label_quality = LabelEncoder()\n# Bad becomes 0 and good becomes 1\ndf['quality'] = label_quality.fit_transform(df['quality'])\ndf['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.drop('quality', axis=1)\ny = df['quality']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Applying Standard scaling to get optimized result\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)\n\n#model_selection was a \"pain in the ass\"\n\n#https://blog.csdn.net/qq_43457383/article/details/109466459\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import model_selection","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed = 7\n# prepare models\nmodels = []\nmodels.append(('SupportVectorClassifier', SVC()))\nmodels.append(('StochasticGradientDecentC', SGDClassifier()))\nmodels.append(('RandomForestClassifier', RandomForestClassifier()))\nmodels.append(('DecisionTreeClassifier', DecisionTreeClassifier()))\nmodels.append(('GaussianNB', GaussianNB()))\nmodels.append(('KNeighborsClassifier', KNeighborsClassifier()))\nmodels.append(('AdaBoostClassifier', AdaBoostClassifier()))\nmodels.append(('LogisticRegression', LogisticRegression()))\n\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=None)\n\tcv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.xticks(rotation=45)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##Setting a random_state has no effect since shuffle is False. You should leave random_state to its default (None), or set shuffle=True.\n\nRandom state was random_state=seed I changed to None."},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC()\nsvc.fit(X_train, y_train)\npred_svc = svc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def svc_param_selection(X, y, nfolds):\n    param = {\n        'C': [0.1, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4],\n        'kernel': ['linear', 'rbf'],\n        'gamma': [0.1, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4]\n    }\n    grid_search = GridSearchCV(svc, param_grid=param, scoring='accuracy', cv=nfolds)\n    grid_search.fit(X,  y)\n    return grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_param_selection(X_train, y_train,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc2 = SVC(C = 1.3, gamma =  1.3, kernel= 'rbf')\nsvc2.fit(X_train, y_train)\npred_svc2 = svc2.predict(X_test)\nprint('Confusion matrix')\nprint(confusion_matrix(y_test, pred_svc2))\nprint('Classification report')\nprint(classification_report(y_test, pred_svc2))\nprint('Accuracy score',accuracy_score(y_test, pred_svc2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Stochastic Gradient Decent Classifier"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"sgd = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=60)\nsgd.fit(X_train, y_train)\npred_sgd = sgd.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = df.corr()\nprint(corr_matrix[\"quality\"].sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colum_names = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n# Correlation matrix\ncorrelations = df.corr()\n# Plot figsize\nfig, ax = plt.subplots(figsize=(10, 10))\n# Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n# Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(correlations, cmap=colormap, annot=True, fmt=\".2f\")\nax.set_xticklabels(\n    colum_names,\n    rotation=45,\n    horizontalalignment='right'\n);\nax.set_yticklabels(colum_names);\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\n# Scatterplot Matrix\nsm = scatter_matrix(df, figsize=(6, 6), diagonal='kde')\n#Change label rotation\n[s.xaxis.label.set_rotation(40) for s in sm.reshape(-1)]\n[s.yaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n#May need to offset label when rotating to prevent overlap of figure\n[s.get_yaxis().set_label_coords(-0.6,0.5) for s in sm.reshape(-1)]\n#Hide all ticks\n[s.set_xticks(()) for s in sm.reshape(-1)]\n[s.set_yticks(()) for s in sm.reshape(-1)]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Prepare the data to better expose the underlying data patterns to Machine Learning algorithms"},{"metadata":{},"cell_type":"markdown","source":"#Fine-tune your models and combine them into a great solution"},{"metadata":{"trusted":true},"cell_type":"code","source":"svc = SVC()\nsvc.fit(X_train, y_train)\npred_svc = svc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def svc_param_selection(X, y, nfolds):\n    param = {\n        'C': [0.1, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4],\n        'kernel': ['linear', 'rbf'],\n        'gamma': [0.1, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4]\n    }\n    grid_search = GridSearchCV(svc, param_grid=param, scoring='accuracy', cv=nfolds)\n    grid_search.fit(X,  y)\n    return grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_param_selection(X_train, y_train,10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc2 = SVC(C = 1.3, gamma =  1.3, kernel= 'rbf')\nsvc2.fit(X_train, y_train)\npred_svc2 = svc2.predict(X_test)\nprint('Confusion matrix')\nprint(confusion_matrix(y_test, pred_svc2))\nprint('Classification report')\nprint(classification_report(y_test, pred_svc2))\nprint('Accuracy score',accuracy_score(y_test, pred_svc2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Stochastic Gradient Decent Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=60)\nsgd.fit(X_train, y_train)\npred_sgd = sgd.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=200, max_depth=20,\n                             random_state=0)\nrfc.fit(X_train, y_train)\npred_rfc = rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#KNeighborsClassifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors = 2\n\nfor weights in ['uniform', 'distance']:\n    nbrs = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n    nbrs.fit(X_train,y_train)\n    pred_nbrs = nbrs.predict(X_test)\n    print('KNeighborsClassifier', weights)\n    print(classification_report(y_test, pred_nbrs))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_classifier = AdaBoostClassifier(n_estimators=100)\nada_classifier.fit(X_train, y_train)\npred_ada = ada_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(mean_absolute_error(test_labels,predictions)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy_score(test_labels,predictions)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\nevaluate(svc,X_test,y_test)\nevaluate(svc2,X_test,y_test)\nevaluate(sgd,X_test,y_test)\nevaluate(rfc,X_test,y_test)\nevaluate(ada_classifier,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = cross_val_score(ada_classifier,X_test,y_test, cv=5)\nscores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance=ada_classifier.feature_importances_\n\nstd = np.std([tree.feature_importances_ for tree in ada_classifier.estimators_],\n             axis=0)\nindices = np.argsort(importance)\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.barh(range(X.shape[1]), importance[indices],\n       color=\"b\",  align=\"center\")\n\nplt.yticks(range(X.shape[1]), colum_names)\nplt.ylim([0, X.shape[1]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Codes by  Nataliia Rasttoropova   https://gist.github.com/NataliiaRastoropova/1d861775c47516d99e8dcb444eaa7049"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}