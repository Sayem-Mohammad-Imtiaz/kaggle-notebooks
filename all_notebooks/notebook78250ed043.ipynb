{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visulaisation\nfrom matplotlib.pyplot import xticks\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data display coustomization\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data loading\nlead = pd.read_csv(r\"/kaggle/input/lead-scoring-dataset/Lead Scoring.csv\")\nlead.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word=pd.read_excel(r\"/kaggle/input/lead-scoring-dataset/Leads Data Dictionary.xlsx\")\nword.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_colwidth', -1)\nword.drop('Unnamed: 0',inplace=True,axis=1)\nword.columns = word.iloc[1]\nword = word.iloc[2:]\nword.reset_index(drop=True, inplace=True)\nword.head(len(word))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead_dub = lead.copy()\n\n# Checking for duplicates and dropping the entire duplicate row if any\nlead_dub.drop_duplicates(subset=None, inplace=True)\nlead_dub.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we can observe that there are select values for many column.\n#This is because customer did not select any option from the list, hence it shows select.\n# Select values are as good as NULL.\n\n# Converting 'Select' values to NaN.\nlead = lead.replace('Select', np.nan)\nlead.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will drop the columns having more than 60% NA values.\nlead = lead.drop(lead.loc[:,list(round(100*(lead.isnull().sum()/len(lead.index)), 2)>60)].columns, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping Lead Number and Prospect ID since they have all unique values\n\nlead.drop(['Prospect ID', 'Lead Number'], 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lead Quality: Indicates the quality of lead based on the data and intuition the the employee who has been assigned to the lead","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As Lead quality is based on the impression employee & the lead, \n#if anything is left blank we can impute 'Not Sure' in NaN safely.\n\nlead['Lead Quality'] = lead['Lead Quality'].replace(np.nan, 'Not Sure')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# There is too much variation in thes parameters so its not reliable to impute any value in it. \n# 45% null values means we need to drop these columns.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead = lead.drop(['Asymmetrique Activity Index','Asymmetrique Activity Score',\n                  'Asymmetrique Profile Index','Asymmetrique Profile Score'],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Around 57.8% of the data available  is Mumbai so we can impute Mumbai in the missing values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['City'] = lead['City'].replace(np.nan, 'Mumbai')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It maybe the case that lead has not entered any specialization if his/her option is not availabe on the list,\n#  may not have any specialization or is a student.\n# Hence we can make a category \"Others\" for missing values. \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['Specialization'] = lead['Specialization'].replace(np.nan, 'Others')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Blanks in the tag column may be imputed by 'Will revert after reading the email'.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['Tags'] = lead['Tags'].replace(np.nan, 'Will revert after reading the email')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Blanks in the this column may be imputed by 'Better Career Prospects'.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['What matters most to you in choosing a course'] = lead['What matters most to you in choosing a course'].replace(np.nan, 'Better Career Prospects')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 86% entries are of Unemployed so we can impute \"Unemployed\" in it.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['What is your current occupation'] = lead['What is your current occupation'].replace(np.nan, 'Unemployed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Country is India for most values so let's impute the same in missing values.\nlead['Country'] = lead['Country'].replace(np.nan, 'India')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Rest missing values are under 1.5% so we can drop these rows.\nlead.dropna(inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_retailed= len(lead)* 100 / len(lead_dub)\nprint(\"{} % of original rows is available for EDA\".format(round(data_retailed,2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['Lead Source'] = lead['Lead Source'].replace(['google'], 'Google')\nlead['Lead Source'] = lead['Lead Source'].replace(['Click2call', 'Live Chat', 'NC_EDM', 'Pay per Click Ads', 'Press_Release',\n  'Social Media', 'WeLearn', 'bing', 'blog', 'testone', 'welearnblog_Home', 'youtubechannel'], 'Others')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's keep considerable last activities as such and club all others to \"Other_Activity\"\nlead['Last Activity'] = lead['Last Activity'].replace(['Had a Phone Conversation', 'View in browser link Clicked', \n                                                       'Visited Booth in Tradeshow', 'Approached upfront',\n                                                       'Resubscribed to emails','Email Received', 'Email Marked Spam'],\n                                                      'Other_Activity')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['Specialization'] = lead['Specialization'].replace(['Others'], 'Other_Specialization')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead['What is your current occupation'] = lead['What is your current occupation'].replace(['Other'], 'Other_Occupation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's keep considerable last activities as such and club all others to \"Other_Activity\"\nlead['Tags'] = lead['Tags'].replace(['In confusion whether part time or DLP', 'in touch with EINS','Diploma holder (Not Eligible)',\n                                     'Approached upfront','Graduation in progress','number not provided', 'opp hangup','Still Thinking',\n                                    'Lost to Others','Shall take in the next coming month','Lateral student','Interested in Next batch',\n                                    'Recognition issue (DEC approval)','Want to take admission but has financial problems',\n                                    'University not recognized'], 'Other_Tags')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead = lead.drop(['What matters most to you in choosing a course','Search',\n                  'Magazine','Newspaper Article','X Education Forums','Newspaper',\n           'Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses',\n                  'Update me on Supply Chain Content',\n           'Get updates on DM Content','I agree to pay the amount through cheque',\n                  'A free copy of Mastering The Interview','Country'],1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Original Columns {} % Retained\".format(round((100* len(lead.columns)/len(lead_dub.columns)),2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Original Data {} % Retained\".format(round((len(lead) * \n                                                     len(lead.columns))*100/(len(lead_dub.columns)*len(lead_dub)),2)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting some binary variables (Yes/No) to 1/0","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of variables to map\n\nvarlist =  ['Do Not Email', 'Do Not Call']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\n# Applying the function to the housing list\nlead[varlist] = lead[varlist].apply(binary_map)\nlead.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(lead[['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation',\n                              'Tags','Lead Quality','City','Last Notable Activity']], drop_first=True)\ndummy1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adding the results to the master dataframe\nlead = pd.concat([lead, dummy1], axis=1)\nlead.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead = lead.drop(['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization',\n                  'What is your current occupation','Tags','Lead Quality','City','Last Notable Activity'], axis = 1)\nlead.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lead.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Putting feature variable to X\nX = lead.drop(['Converted'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Putting response variable to y\ny = lead['Converted']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=75)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the Converted Rate\nConverted = round((sum(lead['Converted'])/len(lead['Converted'].index))*100,2)\nprint(\"We have almost {} %  Converted rate after successful data manipulation\".format(Converted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nmodel = LogisticRegression(random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model with the training data\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\npredict_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainaccuracy = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', trainaccuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VIF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_remove = vif.loc[vif['VIF'] >= 4.99,'Features'].values\nfeatures_to_remove = list(features_to_remove)\nprint(features_to_remove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.drop(columns=features_to_remove, axis = 1)\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test.drop(columns=features_to_remove, axis = 1)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model with the training data\nmodel.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\npredict_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracytrain = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', accuracytrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# VIF","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The NaN, in this case, is interpretted as no correlation between the two variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train, predict_train )\nprint(confusion)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our model\ntrainsensitivity= TP / float(TP+FN)\ntrainsensitivity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\ntrainspecificity= TN / float(TN+FP)\ntrainspecificity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate false postive rate - predicting Converted when customer does not have Converted\nprint(FP/ float(TN+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Positive predictive value \nprint (TP / float(TP+FP))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Negative predictive value\nprint(TN / float(TN+ FN))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plotting the ROC Curve","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_roc(y_train,predict_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Precision and Recall","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using sklearn utilities for the same","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train,predict_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"recall_score(y_train,predict_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making predictions on the test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict the target on the test dataset\npredict_test = model.predict(X_test)\nprint('Target on test data\\n\\n',predict_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confusion2 = metrics.confusion_matrix(y_test, predict_test )\nprint(confusion2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's check the overall accuracy.\ntestaccuracy= accuracy_score(y_test,predict_test)\ntestaccuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see the sensitivity of our lmodel\ntestsensitivity=TP / float(TP+FN)\ntestsensitivity","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us calculate specificity\ntestspecificity= TN / float(TN+FP)\ntestspecificity","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Observation:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us compare the values obtained for Train & Test:\nprint(\"Train Data Accuracy    :{} %\".format(round((trainaccuracy*100),2)))\nprint(\"Train Data Sensitivity :{} %\".format(round((trainsensitivity*100),2)))\nprint(\"Train Data Specificity :{} %\".format(round((trainspecificity*100),2)))\nprint(\"Test Data Accuracy     :{} %\".format(round((testaccuracy*100),2)))\nprint(\"Test Data Sensitivity  :{} %\".format(round((testsensitivity*100),2)))\nprint(\"Test Data Specificity  :{} %\".format(round((testspecificity*100),2)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}