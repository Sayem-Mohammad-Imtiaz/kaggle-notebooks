{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Our approch on chronological order\n\n* Exploratory Data analysis\n* Exploratory Data analysis - Job descriptions\n* Models without text variables\n* Models with text variables\n* Final Output"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import all required modules for the analysis(make sure that you installed all these modules prior to importing)\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\nimport math\nfrom IPython.display import display,HTML\nfrom patsy import dmatrices\nimport seaborn as sns; sns.set()\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%pylab inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading the train data\ntrain_df = pd.read_csv('../input/trainrev1/Train_rev1.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (train_df.info())\n\n# Let's look at the unique values present in the data frame to have a general understanding of the data\nnames = train_df.columns.values\nuniq_vals = {}\nfor name in names:\n    uniq_vals[name] = train_df.loc[:,name].unique()\n    print(\"Count of %s : %d\" %(name,uniq_vals[name].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of salaries based on the train data\npylab.rcParams['figure.figsize'] = (20,10)\nplt.hist(train_df['SalaryNormalized'], bins='auto')\nplt.xlabel('Salaries')\nplt.ylabel('Number of postings')\nplt.title('Histogram of Salaries')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe that the job descriptions are skewed mostly towards the lower end(mostly < 50,000) showing that most of the jobs are on the lower end of the job salary spectrum. This distribution might be useful as we move further into the analysis, as this might help us detect if there is any bias in our final analysis.\n\nlimitation:\nAs you can see, there are about ~240 k rows in the dataset. As i am currently running the analysis on my personal system with 8 GB, i will randomly take a small number of rows to train my classifier and incrementally add data to train untill its feasible."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Randomly selecting 2500 rows to train the classifier\nimport random\nrandom.seed(1)\nindices = list(train_df.index.values)\nrandom_2500 = random.sample(indices,2500)\n\n# Subsetting the train data based on the random indices\ntrain_df1 = train_df.loc[random_2500].reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's see the salary distribution in this data and compare it with the original data\npylab.rcParams['figure.figsize'] = (20,10)\nplt.hist(train_df1['SalaryNormalized'], bins='auto')\nplt.xlabel('Salaries')\nplt.ylabel('Number of postings')\nplt.title('Histogram of Salaries')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution seemingly holds good and is comparably similar to the orginial distribution. With this, let's move forward with the actual analysis of cleaning the data and creating the features for the analysis.\n\n## Exploratory Data analysis - Job descriptions\nLet's look into the job descriptions and try to answer some of the questions to have more clarity\n\nWhat are the top 5 parts of speech in the job descriptions? How frequently do they appear?\nHow do these numbers change if you exclude stopwords?\nWhat are the 10 most common words after removing stopwords and lemmatization?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize, sent_tokenize \nstop_words = set(stopwords.words('english')) \nfrom string import punctuation\nimport collections","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While looking at the data, you can observe that the numbers are masked as *** and they turn out to be of no value for us in the analysis. In addition to that, there are a few data cleaning steps that i have performed in the below code\n\n1. Remove website links from the data\n1. Remove punctuations\n1. Removing numbers\n\nBy running these steps, we can achieve a higher accuracy as the data becomes more cleaned and the predictive power of the algorithm increases because of that."},{"metadata":{"trusted":true},"cell_type":"code","source":"# To obtain the full width of a cell in a dataframe\npd.set_option('display.max_colwidth', -1)\ndesc = train_df1.loc[1,'FullDescription']\n\n# Creating a list of words from all the job descriptions in train_df1 data\nall_desc = []\nfor i in range(0,train_df1.shape[0]):\n    desc = train_df1.loc[i,'FullDescription']\n    desc1 = desc.lower()\n    # Removing numbers, *** and www links from the data\n    desc2 = re.sub('[0-9]+\\S+|\\s\\d+\\s|\\w+[0-9]+|\\w+[\\*]+.*|\\s[\\*]+\\s|www\\.[^\\s]+','',desc1)\n    # Removing punctuation\n    for p in punctuation:\n        desc2 = desc2.replace(p,'')\n    all_desc.append(desc2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating word tokens for all the descriptions\nfinal_list = []\nfor desc in all_desc:\n    word_list = word_tokenize(desc)\n    final_list.extend(word_list)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see that Noun(NN) , adjective(JJ), preposition(IN), determiner(DT), plural nouns(NNS) are the most common parts of speech from the job descriptions. Refer this link to look at the descriptions of the parts of speech for the above result"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Excluding stopwords from the analysis\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english')) \n\nlist_wo_stopwords = []\nfor w in final_list:\n    if w not in stop_words:\n        list_wo_stopwords.append(w)\n        \n# 3. Tagging parts of speech\npos_tagged_wo_sw = nltk.pos_tag(list_wo_stopwords)\n\n# 4. Identifying the most common parts of speech\ntag_fd_wo_sw = nltk.FreqDist(tag for (word, tag) in pos_tagged_wo_sw)\ntag_fd_wo_sw.most_common()[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After removing stopwords, there are two important observations in comparison to the previous result\n\n1. Prepositions and determiners disappeared from the top 5 set as most of these are present in the stopwords imported from NLTK\n2. The counts of nouns and plural nouns have decreased and the adjectives have increased.\n3. Verb, gerund or present participle(VBG) and Verb, non-3rd person singular present(VBP) moved to the top 5 list\n\nAs we have already removed stopwords and create a dataframe list_wo_stopwords earlier, our first step here would be to perform lemmatization and then identify the 10 most common words. I have also plotted the wordcloud of all the words to visualize these words."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Lemmatization without specifying parts of speech\nlist_lemmatized = []\nfor word in list_wo_stopwords:\n    list_lemmatized.append(lemmatizer.lemmatize(word))\n\nword_freq_lem = dict(collections.Counter(list_lemmatized))\nkeys = list(word_freq_lem.keys())\nvalues = list(word_freq_lem.values())\ndf_lem = pd.DataFrame({'words':keys,'freq':values})\ndisplay(df_lem.sort_values(by = 'freq',ascending = False)[:10])\n\nfrom wordcloud import WordCloud\nfrom collections import Counter\nword_could_dict=Counter(word_freq_lem)\nwordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(word_could_dict)\n\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model without text variables\nAs i have told, we will perform the analysis with 2 types of variables\n\n1. Non-text variables\n2. Text variables\n\nJust to revisit, our objective is to predict the salaries based on the information posted on the website including variables such as location, company and job description.\n\nWith this in mind, let's proceed with the analysis\n\nFor this analysis, let's define the target variable based on the salary normalized. This converts the problem into a classfication problem and reduces the complexity. Further, based on the requirement we can perform a regression analysis to predict a number for the salary."},{"metadata":{"trusted":true},"cell_type":"code","source":"p_75 = np.percentile(train_df1['SalaryNormalized'], 75)\ntrain_df1['target'] = train_df1['SalaryNormalized'].apply(lambda x: 1 if x>=p_75 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"costly_cities = ['London','Brighton','Edinburgh','Bristol','Southampton','Portsmouth','Exeter','Cardiff','Manchester',\n                 'Birmingham','Leeds','Aberdeen','Glasgow','Newcastle','Sheffield','Liverpool']\ncostly_cities_lower = [x.lower() for x in costly_cities]\n\n# More robust if lower() is applied\ntrain_df1['location_flag'] = train_df1['LocationNormalized'].apply(lambda x: 1 if x in costly_cities else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x = train_df1.drop(['FullDescription','index','Id','LocationRaw','Title','Company','LocationNormalized','SalaryRaw','SalaryNormalized',\n                    'target'],axis=1)\n\ntrain_x1 = pd.get_dummies(train_x,drop_first=True)\nX_n = np.array(train_x1)\ny_n = np.array(train_df1['target'])\n\nfrom sklearn.model_selection import train_test_split\nX_train_num, X_val_num, y_train_num, y_val_num = train_test_split(X_n, y_n, test_size=0.3, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bernoulli\nfrom sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB()\nclf.fit(X_train_num, y_train_num)\n\nfrom sklearn import metrics\nprediction_train = clf.predict(X_val_num)\nmat_n = metrics.confusion_matrix(y_val_num, prediction_train)\nmat_n\nprint (metrics.accuracy_score(y_val_num, prediction_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Baseline accuracy\n1-(sum(y_val_num)/len(y_val_num))\n# sum(prediction_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def models(l):\n    # Counting the occurence of each word in the corpus\n    from sklearn.feature_extraction.text import CountVectorizer\n    count_vect = CountVectorizer()\n    X_train_counts = count_vect.fit_transform(l)\n    count_vect.get_feature_names()\n    X_matrix= X_train_counts.todense()\n\n    y = np.array(train_df1['target'])\n\n    # Creating the train and test split\n    from sklearn.model_selection import train_test_split\n    X_train_m, X_val_m, y_train_m, y_val_m = train_test_split(X_train_counts, y, test_size=0.3, random_state=1)\n\n    #Multinomial\n\n    from sklearn.naive_bayes import MultinomialNB\n    clf = MultinomialNB().fit(X_train_m, y_train_m)\n    labels_m = clf.predict(X_val_m)\n\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import accuracy_score\n    mat_m = confusion_matrix(y_val_m, labels_m)\n\n    # Bernoulli\n    # Changing the data to binary to input BernoulliNB\n    x_train_b1 = X_train_counts.todense()\n    X_train_counts_ber = np.where(x_train_b1 >=1 ,1,0)\n\n    # Creating the train and test split for bernoulli\n    from sklearn.model_selection import train_test_split\n    X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(X_train_counts_ber, y, test_size=0.3, random_state=1)\n\n    from sklearn.naive_bayes import BernoulliNB\n    clf = BernoulliNB().fit(X_train_b, y_train_b)\n    labels_b = clf.predict(X_val_b)\n\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import accuracy_score\n    mat_b = confusion_matrix(y_val_b, labels_b)\n    print ('Confusion matrix:',mat_b)\n    print ('Accuracy using BernoulliNB:',accuracy_score(y_val_b, labels_b))\n\n\n    print ('Confusion matrix:',mat_m)\n    print ('Accuracy using MultinomialNB:',accuracy_score(y_val_m, labels_m))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without removing stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"models(all_desc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After removing stopwords from the data****"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing stopwords\ndef remove_stopwords(s):\n    big_regex = re.compile(r'\\b%s\\b' % r'\\b|\\b'.join(map(re.escape, stop_words)))\n    return big_regex.sub('',s)\n\nall_desc_wo_sw = [remove_stopwords(s) for s in all_desc]\nmodels(all_desc_wo_sw)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nfrom nltk.corpus import wordnet\n\ndef get_wordnet_pos(treebank_tag):\n\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return None \n\n# Lemmatizing the data\nall_desc_lemm = []\nfor i in range(0,len(all_desc_wo_sw)):\n    desc = all_desc_wo_sw[i]\n    desc2 = re.sub('[0-9]+\\S+|\\s\\d+\\s|\\w+[0-9]+|\\w+[\\*]+.*|\\s[\\*]+\\s|www\\.[^\\s]+','',desc)\n    for p in punctuation:\n        desc2 = desc2.replace(p,'')\n    tagged = nltk.pos_tag(word_tokenize(desc2))\n    list_lemmatized = []\n    for word, tag in tagged:\n        wntag = get_wordnet_pos(tag)\n        if wntag is None:# not supply tag in case of None\n            list_lemmatized.append(lemmatizer.lemmatize(word)) \n        else:\n            list_lemmatized.append(lemmatizer.lemmatize(word, pos=wntag))\n    k = ' '.join(list_lemmatized)   \n    all_desc_lemm.append(k)\n\nmodels(all_desc_lemm)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}