{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Heart Disease Machine Learning Project.\n\n**Problem Definition**\n### A project aimed at modelling heart disease based on biometric factors in order to predict its incidence.\n * labels are binary 0(no heart disease) and 1(heart disease)\n    \n**Data**\nTaken from Kaggle.com, this data set is curated by UCI in Cleveland\nhttps://www.kaggle.com/ronitf/heart-disease-uci\n\n*Evaluation:\nWith only 303 rows, this is a small dataset,but the spread between target labels 0(no heart disease) and 1(heart disease) is fairly even.\nAlso, with 14 columns there is a lot of depth to analyze our dataset with.\n\nOur benchmark goal will be 95% accuracy  \n    \n**Features**\n* age \n* sex\n* chest pain type (4 values)(cp)\n    - 0 = typical angina (chest pain related to lack of blood in heart)\n    - 1 = Atypical angina (chest pain not related to heart)\n    - 2 = non-anginal pain (typical esophageal spasm not related to heart)\n    - 4 = Asymptomatic  \n* resting blood pressure (trestbps)\n    - one of the key risk factors outlined by the CDC)\n    - Over 120 is cause for concern\n* serum cholestoral in mg/dl (chol)\n    - Another key CDC risk factor \n    - Over 200 is high and over 240 is irregular\n* fasting blood sugar > 120 mg/dl (fbs)\n    - high blood sugar can damage blood vessels and the nerves that control your heart\n    - Over 200 is irregular\n* resting electrocardiographic results (values 0,1,2) (restecg)\n    - can tell where the heart's blood supply is blocked or interrupted by a build-up of fatty substances.\n    - Over 100 is irregular\n    - 0  = No signs of irregularity\n    - 1 = ST-T wave abnormality (signals non-normal heart beat)\n    - 2 = definite sign of left-ventricular hypertrophy\n* maximum heart rate achieved (thalach)\n    - 220 minus age is the threshold\n* exercise induced angina\n    - heart pain based on exercies\n* oldpeak = ST depression induced by exercise relative to rest\n* slope =the slope of the peak exercise ST segment\n* number of major vessels (0-3) colored by flourosopy (ca)\n* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:55.387819Z","iopub.execute_input":"2021-05-20T23:37:55.388082Z","iopub.status.idle":"2021-05-20T23:37:55.395873Z","shell.execute_reply.started":"2021-05-20T23:37:55.388059Z","shell.execute_reply":"2021-05-20T23:37:55.394865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration (EDA)\n\n#### In this phase we want to learn as much about the data set as possible and become a subject matter expert.\n* What are the questions and goals of the research\n* What is the data like? \n* Are there relationships between features?\n* Is there a missing component to the data?\n* Are there outliers? Are they worth keeping?\n* Do any features need augmentation, removal, etc?\n\n\n","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/heart-disease-dataset/heart.csv')\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:55.404641Z","iopub.execute_input":"2021-05-20T23:37:55.404959Z","iopub.status.idle":"2021-05-20T23:37:55.439976Z","shell.execute_reply.started":"2021-05-20T23:37:55.40493Z","shell.execute_reply":"2021-05-20T23:37:55.439052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#more detail about target distribution\ndf.target.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:55.441537Z","iopub.execute_input":"2021-05-20T23:37:55.441804Z","iopub.status.idle":"2021-05-20T23:37:55.44759Z","shell.execute_reply.started":"2021-05-20T23:37:55.441778Z","shell.execute_reply":"2021-05-20T23:37:55.446962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Are there null values\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:55.448407Z","iopub.execute_input":"2021-05-20T23:37:55.44863Z","iopub.status.idle":"2021-05-20T23:37:55.466875Z","shell.execute_reply.started":"2021-05-20T23:37:55.448606Z","shell.execute_reply":"2021-05-20T23:37:55.465554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It looks clean. Let's look at the rough relationship between target and features\n\ntarget_breakdown = df.groupby('target').mean()\n\nfor column in target_breakdown.columns:\n    target_breakdown[column] = target_breakdown[column]/target_breakdown[column].max()\ntransposed_for_graph = target_breakdown.T\ntransposed_for_graph.plot.bar(title='Comparison of Features by Target Label Group', xlabel='Feature', \n                              ylabel='Proportional Percentage of mean')\nplt.savefig('Feature-and-Label-Comparison.jpg')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-05-20T23:37:55.468089Z","iopub.execute_input":"2021-05-20T23:37:55.468333Z","iopub.status.idle":"2021-05-20T23:37:55.771112Z","shell.execute_reply.started":"2021-05-20T23:37:55.468309Z","shell.execute_reply":"2021-05-20T23:37:55.770395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### There are some clear relationships between the target and features: oldpeak, ca, cp, sex, and slope in particular.\n* Women are much more likely to have heart disease in this sample \n* Incidence of chest pain appears highly correlated to heart disease.\n* Exang appears to be a sign of good health, as does the number of vessels colored by flouroscopy.\n* High ECG, Slope and Maximum heart rate appear to be indicative of heart disease probability\n* We can see that Several factors may simply be adding noise. Resting blood pressure, cholesterol and age may not be as correlated to the target labels, as odd as that may seem. \n\nAt 13 features, we should have enough depth to discard the 3 that aren't as strongly correlated. Why would we not create a separate test group or drop out these features altogether? \n\nWe have only seen one line of correlation. The features themselves could be correlated in ways that, together, they helped predict heart disease. We now have to see how all features correlate to all other features.","metadata":{}},{"cell_type":"code","source":"#Matrix of variable correlation\ncorrelation_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(20,20))\nax = sns.heatmap(correlation_matrix, annot=True, fmt='.2f')\nax.set(title= 'Correlation Between Heart Disease Variables')\nax.set_yticklabels(ax.get_yticklabels(),rotation=45)\nplt.rc('axes', titlesize=24)\nplt.rc('axes', labelsize=14)\nplt.rc('xtick', labelsize=13)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=13)\nplt.savefig('Variable-Correlation-Matrix.jpg')\n;","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:55.773444Z","iopub.execute_input":"2021-05-20T23:37:55.773676Z","iopub.status.idle":"2021-05-20T23:37:57.135845Z","shell.execute_reply.started":"2021-05-20T23:37:55.773651Z","shell.execute_reply":"2021-05-20T23:37:57.134662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see that age has some of the highest correlation with cholesterol and resting blood pressure. \n\nIt is possible that some combination of these factors will play a part in predicting heart disease.\n\nIt is also possible that a certain subset of the datapoints could be of interest, in a more limited scope that general correlation.\n\nHowever, Machine makes it so easy to test this hypothesis. We will create a second test group without the three features which have the least correlation to our target label","metadata":{}},{"cell_type":"markdown","source":"## Modelling our Data\n\n** We are going to do a preliminary run with 3 models ","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:14.495009Z","iopub.execute_input":"2021-05-20T23:37:14.495293Z","iopub.status.idle":"2021-05-20T23:37:14.50165Z","shell.execute_reply.started":"2021-05-20T23:37:14.495261Z","shell.execute_reply":"2021-05-20T23:37:14.499812Z"}}},{"cell_type":"code","source":"X = df.drop('target', axis=1)\ny = df['target']\n\ntest_group2 = df.drop(['age', 'chol', 'trestbps'], axis=1)\nX2 = test_group2.drop('target', axis=1)\ny2 = test_group2['target']\n\nnp.random.seed(42) # Enables reproducibility and consistency between models and tests.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:57.138608Z","iopub.execute_input":"2021-05-20T23:37:57.138977Z","iopub.status.idle":"2021-05-20T23:37:57.150771Z","shell.execute_reply.started":"2021-05-20T23:37:57.138951Z","shell.execute_reply":"2021-05-20T23:37:57.149782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y2_test.value_counts(), y_test.value_counts(), X_train[:3], X2_train[:3]","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:57.151918Z","iopub.execute_input":"2021-05-20T23:37:57.152986Z","iopub.status.idle":"2021-05-20T23:37:57.182507Z","shell.execute_reply.started":"2021-05-20T23:37:57.152949Z","shell.execute_reply":"2021-05-20T23:37:57.181756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"They are not identical sets, but the label composition of the two test groups are similar, so we can assume our results will be comparable.\n","metadata":{}},{"cell_type":"code","source":"# For the function we will set up in the next cell, we need dictionaries of the models and the tests each will face\nmodels_dict = {'KNeighbors': KNeighborsClassifier(),\n              'RandomForest': RandomForestClassifier(),\n              'LogisticRegression': LogisticRegression()}","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:57.183729Z","iopub.execute_input":"2021-05-20T23:37:57.184048Z","iopub.status.idle":"2021-05-20T23:37:57.188643Z","shell.execute_reply.started":"2021-05-20T23:37:57.183979Z","shell.execute_reply":"2021-05-20T23:37:57.187793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_test_results(X, y, X_train, X_test, y_train, y_test):\n    \"\"\"\n    X: the complete set of feature data\n    y: the complete set of target data\n    X_train: feature labels for model training\n    X_test: feature labels for model testing\n    y_train: target labels for the training set\n    y_test: target labels for the test set\n    \"\"\"\n    test_result_repository = []\n    for k, v in models_dict.items():\n\n        v.fit(X_train, y_train)\n        y_preds = v.predict(X_test)\n        model_test ={\n                     f'{k} accuracy_score':accuracy_score(y_test, y_preds),\n                     f'{k} precision_score':precision_score(y_test, y_preds),\n                     f'{k} recall_score': recall_score(y_test, y_preds),\n                     f'{k} f1_score': f1_score(y_test, y_preds),\n                     f'{k} confusion_matrix': confusion_matrix(y_test, y_preds),\n                     f'{k} classification_report': classification_report(y_test, y_preds),\n                     f'{k} plot_roc_curve': plot_roc_curve(v, X, y),\n                     f'{k} cross_validate': cross_validate(v, X,y, cv=5,\n                                                     scoring=['accuracy','recall', 'precision'])}\n        model_test_copy = model_test.copy()\n        test_result_repository.append(model_test_copy)\n    return test_result_repository\nresults = model_test_results(X,y,X_train,X_test,y_train,y_test)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:57.189993Z","iopub.execute_input":"2021-05-20T23:37:57.190359Z","iopub.status.idle":"2021-05-20T23:37:59.004923Z","shell.execute_reply.started":"2021-05-20T23:37:57.190327Z","shell.execute_reply":"2021-05-20T23:37:59.003864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like Random Forest and Logistic Regression far outperformed K Neighbor in terms of false positives. Let's see some of the other metrics we measured ","metadata":{}},{"cell_type":"code","source":"for k, v in results[0].items():\n    print(k)\n    print(v, '\\n\\n')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:59.007977Z","iopub.execute_input":"2021-05-20T23:37:59.008217Z","iopub.status.idle":"2021-05-20T23:37:59.015249Z","shell.execute_reply.started":"2021-05-20T23:37:59.00819Z","shell.execute_reply":"2021-05-20T23:37:59.014423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k, v in results[1].items():\n    print(k)\n    print(v, '\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:59.01658Z","iopub.execute_input":"2021-05-20T23:37:59.016896Z","iopub.status.idle":"2021-05-20T23:37:59.036716Z","shell.execute_reply.started":"2021-05-20T23:37:59.016866Z","shell.execute_reply":"2021-05-20T23:37:59.03584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k, v in results[2].items():\n    print(k)\n    print(v, '\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:59.037843Z","iopub.execute_input":"2021-05-20T23:37:59.038142Z","iopub.status.idle":"2021-05-20T23:37:59.051105Z","shell.execute_reply.started":"2021-05-20T23:37:59.03811Z","shell.execute_reply":"2021-05-20T23:37:59.050113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### While it looks like Random Forest Classifier and Logistic Regression both performed well, Logistic Regression outperformed its counterpart in all measures, and was more consistent in the cross validation test.","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:59.054275Z","iopub.execute_input":"2021-05-20T23:37:59.054556Z","iopub.status.idle":"2021-05-20T23:37:59.062039Z","shell.execute_reply.started":"2021-05-20T23:37:59.054529Z","shell.execute_reply":"2021-05-20T23:37:59.061013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_stats_plt = pd.DataFrame(data = [\n                                       [results[0]['KNeighbors accuracy_score'],results[0]['KNeighbors recall_score']],\n                                       [results[1]['RandomForest accuracy_score'], results[1]['RandomForest recall_score']],\n                                       [results[2]['LogisticRegression accuracy_score'], results[2]['LogisticRegression recall_score']]\n                                       ],\n                               index=['KNeighbors', 'RandomForest', 'LogisticRegression'],\n                               columns=['Accuracy', 'Recall'])\nfigure = model_stats_plt.plot.bar(figsize=(10,10), title='Model Test Comparison', xlabel='Model');","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:59.062936Z","iopub.execute_input":"2021-05-20T23:37:59.063159Z","iopub.status.idle":"2021-05-20T23:37:59.223871Z","shell.execute_reply.started":"2021-05-20T23:37:59.063138Z","shell.execute_reply":"2021-05-20T23:37:59.222719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We still have another test set. Luckily we can put the function we created before to quick use.\n\nresults2 = model_test_results(X2,y2, X2_train, X2_test, y2_train, y2_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:37:59.225142Z","iopub.execute_input":"2021-05-20T23:37:59.225456Z","iopub.status.idle":"2021-05-20T23:38:00.888668Z","shell.execute_reply.started":"2021-05-20T23:37:59.225424Z","shell.execute_reply":"2021-05-20T23:38:00.888167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The ROC curves are almost identical, let's look closer at the data\n\nfor k, v in results2[0].items():\n    print(k)\n    print(v, '\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:00.889417Z","iopub.execute_input":"2021-05-20T23:38:00.889715Z","iopub.status.idle":"2021-05-20T23:38:00.898956Z","shell.execute_reply.started":"2021-05-20T23:38:00.889692Z","shell.execute_reply":"2021-05-20T23:38:00.897176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# KNeighbors seems to have improved. Let's see about RandomForest\nfor k, v in results2[1].items():\n    print(k)\n    print(v, '\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:00.900215Z","iopub.execute_input":"2021-05-20T23:38:00.900503Z","iopub.status.idle":"2021-05-20T23:38:00.914691Z","shell.execute_reply.started":"2021-05-20T23:38:00.900475Z","shell.execute_reply":"2021-05-20T23:38:00.913397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for k, v in results2[2].items():\n    print(k)\n    print(v, '\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:00.915849Z","iopub.execute_input":"2021-05-20T23:38:00.916164Z","iopub.status.idle":"2021-05-20T23:38:00.934185Z","shell.execute_reply.started":"2021-05-20T23:38:00.916136Z","shell.execute_reply":"2021-05-20T23:38:00.933403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results are a wash, with slight decrease in performance. One point of interest is that in all models, the percentage of false negatives dropped, and recalls rose despite this decrease. The recall scores showed a 8%, 2% and 2% increase for K Neighbor, Random Forest, and Logistic Regression models respectively. \n\nThe implication is significant. If we could test for general accuracy with one test, and then test negatives again with a model that excels in picking out false negatives, we could reduce the risk of error.\n\nSince our first test group outperformed in most measures, lets focus on hyper parameter tuning on this data set first.\nWe will only be using Logistic Regression and Random Forest Models from here on out.","metadata":{}},{"cell_type":"code","source":"random_forest_grid = {'n_estimators': np.arange(10,1000,50),\n                     'max_depth':[None,1,2,4,10],\n                     'min_samples_split': np.arange(2,10,1),\n                     'min_samples_leaf': np.arange(2,10,1)}\nlogistic_regression_grid = {'C':np.logspace(-4, 4,20),\n                           'solver': ['liblinear']}\n#Random Forest object\nrfc = RandomForestClassifier()\n#Logistic Regression Object\nlrc = LogisticRegression()\n\n#Randomized Search for Random Forest\nrf_RandomizedSearchCV = RandomizedSearchCV(rfc, random_forest_grid, cv=5, verbose=True, n_iter=20)\n# This does not even cover .01& of the possible combinations\n\n#Randomized Search for Logistic Regression\nlrc_RandomizedSearchCV = RandomizedSearchCV(lrc, logistic_regression_grid, cv=5, verbose=True, n_iter=20)\n# For the logistic regression grid, 20 tests will exhaust all possibilities","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:00.935238Z","iopub.execute_input":"2021-05-20T23:38:00.935454Z","iopub.status.idle":"2021-05-20T23:38:00.946899Z","shell.execute_reply.started":"2021-05-20T23:38:00.935432Z","shell.execute_reply":"2021-05-20T23:38:00.945631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\nlrc_RandomizedSearchCV.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:00.948517Z","iopub.execute_input":"2021-05-20T23:38:00.948825Z","iopub.status.idle":"2021-05-20T23:38:01.981231Z","shell.execute_reply.started":"2021-05-20T23:38:00.948798Z","shell.execute_reply":"2021-05-20T23:38:01.980524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lrc_RandomizedSearchCV.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:01.982259Z","iopub.execute_input":"2021-05-20T23:38:01.982489Z","iopub.status.idle":"2021-05-20T23:38:01.987335Z","shell.execute_reply.started":"2021-05-20T23:38:01.982466Z","shell.execute_reply":"2021-05-20T23:38:01.986635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lrc_RandomizedSearchCV.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:01.988537Z","iopub.execute_input":"2021-05-20T23:38:01.988787Z","iopub.status.idle":"2021-05-20T23:38:02.006306Z","shell.execute_reply.started":"2021-05-20T23:38:01.988764Z","shell.execute_reply":"2021-05-20T23:38:02.005513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.random.seed(42)\nrf_RandomizedSearchCV.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:02.007457Z","iopub.execute_input":"2021-05-20T23:38:02.007808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_RandomizedSearchCV.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_RandomizedSearchCV.score(X_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both have not changed significantly between the SearchCV iterations and their baseline scores. Also, despite Random Forest seeing much more variation in hyperparameters, it is still below the accuracy of the Logistic Regression model.","metadata":{}}]}