{"cells":[{"metadata":{"_uuid":"41e7d814a567a822b4e155788cc247c6ac697e7b"},"cell_type":"markdown","source":"# PIMA INDIAN DIABETES EDA\nIn this Kernel I have predicted the chances of diabetes using PIMA Indian dataset.This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on."},{"metadata":{"_uuid":"80e6f6223839de768064ba50f0146d145697042d"},"cell_type":"markdown","source":"# About Dataset\n\nThe Pima Indians Diabetes Dataset involves predicting the onset of diabetes within 5 years in Pima Indians given medical details.\n\nIt is a binary (2-class) classification problem. The number of observations for each class is not balanced. There are 768 observations with 8 input variables and 1 output variable. The variable names are as follows:\n\n**Features:**\n\n- **Pregnancies -** Number of times pregnant.\n- **Glucose  -** Plasma glucose concentration a 2 hours in an oral glucose tolerance testPlasma glucose concentration a 2 hours in an oral glucose tolerance test.\n- **BloodPressure -** Diastolic blood pressure (mm Hg).\n- **SkinThickness -** Triceps skinfold thickness (mm).\n- **Insulin -** 2-Hour serum insulin (mu U/ml).\n- **BMI -** Body mass index (weight in kg/(height in m)^2).\n- **DiabetesPedigreeFunction -** Diabetes pedigree function.\n- **Age -** Age in years.\n\n**Target Variable :**\n\n- **Outcome -** Class variable 1 if patient has diagnosed diabetes and 0 if not.\n\n## Steps to be Followed :\nFollowing steps I have taken to apply machine learning models:\n\n1. Importing Essential Libraries.\n2. Data Preparation & Data Cleaning.\n3. Data Visualization \n4. Feature Engineering to discover essential features in the process of applying machine learning.\n5. Encoding Categorical Variables.\n6. Train Test Split\n7. Apply Machine Learning Algorithm\n8. Cross Validation\n9. Model Evaluation\n\n## Model Evaluation :\n- [Cross Validation Score] (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n- [Confusion Matrix] (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n- [Plotting ROC-AUC Curve] (https://en.wikipedia.org/wiki/Receiver_operating_characteristic)\n- [Sensitivity and Specitivity] (https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n- [Classification Error] (https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)"},{"metadata":{"_uuid":"ea9eb5cc4a8237319a888ae549284d0b4adf5aa7"},"cell_type":"markdown","source":"# Importing Essential Libraries"},{"metadata":{"trusted":true,"_uuid":"218184a9169803099eb25a2d1ac971dbd7fc778e"},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nimport itertools\nplt.style.use('fivethirtyeight')\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import  accuracy_score, f1_score, precision_score,confusion_matrix, recall_score, roc_auc_score\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c1cfd56bc8b596aebddb1f48c382e52dcfd10ba"},"cell_type":"code","source":"df=pd.read_csv('../input/diabetes.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4d57e57839f96218e9fe8885deffddd4300a5af"},"cell_type":"code","source":"# Lets look at some of the sample data \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6eb16985b69abdd0500e85c8d22e8f2ff9255851"},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e258e8cbbb4f8e61c6184c622d9ee76c2fc9bfd1"},"cell_type":"markdown","source":"This dataset is known to have missing values.Specifically, there are missing observations for some columns that are marked as a zero value."},{"metadata":{"_uuid":"306f4cdfb95a97231db75670b11b5e764cf1ebca"},"cell_type":"markdown","source":"## Data Cleaning & Data Preparation\nIn this step we will find missing entries, if there then fill them with median or mean values, checking data types of all the features to find any inconsistency."},{"metadata":{"trusted":true,"_uuid":"ee9700fee002053cc6f0e3c6436acca270788f71"},"cell_type":"code","source":"df.isna().any() # checking No. of Missing Values.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8bd1d9cdbcd9322842f30fa7ba460f31d9202d42"},"cell_type":"code","source":"print(df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c2acbcac2b098d80ef14fcaa17702f4b06864e3"},"cell_type":"code","source":"df.head(50)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77bc9b10a9508fa0fdc6c4c788471029c3269da0"},"cell_type":"markdown","source":"It seems from the above table that there are zero entries in BMI, Blood Pressure,Glucose, Skin Thickness and Insulin which are meaningless so we will fill it with their median values before fitting it into the machine learning models.\n\n**Replacing zero entries BMI, Blood Pressure,Glucose, Skin Thickness and Insulin with their median values**"},{"metadata":{"trusted":true,"_uuid":"9fdc88bce3ac6f7984efc7b75241f25749354d5a"},"cell_type":"code","source":"# Calculate the median value for BMI\nmedian_bmi = df['BMI'].median()\n# Substitute it in the BMI column of the\n# dataset where values are 0\ndf['BMI'] = df['BMI'].replace(\n    to_replace=0, value=median_bmi)\n\nmedian_bloodp = df['BloodPressure'].median()\n# Substitute it in the BloodP column of the\n# dataset where values are 0\ndf['BloodPressure'] = df['BloodPressure'].replace(\n    to_replace=0, value=median_bloodp)\n\n# Calculate the median value for PlGlcConc\nmedian_plglcconc = df['Glucose'].median()\n# Substitute it in the PlGlcConc column of the\n# dataset where values are 0\ndf['Glucose'] = df['Glucose'].replace(\n    to_replace=0, value=median_plglcconc)\n\n# Calculate the median value for SkinThick\nmedian_skinthick = df['SkinThickness'].median()\n# Substitute it in the SkinThick column of the\n# dataset where values are 0\ndf['SkinThickness'] = df['SkinThickness'].replace(\n    to_replace=0, value=median_skinthick)\n\n# Calculate the median value for SkinThick\nmedian_skinthick = df['Insulin'].median()\n# Substitute it in the SkinThick column of the\n# dataset where values are 0\ndf['Insulin'] = df['Insulin'].replace(\n    to_replace=0, value=median_skinthick)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c83581bdd4582507840d142f1a784f68ed348aa9"},"cell_type":"code","source":"df.head(50)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b364647cf8eed0b7bdd6c5fc2ccfc7c3823e252c"},"cell_type":"markdown","source":"Now, all the zero entries are now filled with the median values."},{"metadata":{"_uuid":"333eed78a2c3eb0438e20efc23de618b2d38f44a"},"cell_type":"markdown","source":"## Data Visualization"},{"metadata":{"trusted":true,"_uuid":"3bd984d9e34e7c250f6664c0fa4a21c584c16342"},"cell_type":"code","source":"sns.countplot(data=df, x = 'Outcome', label='Count')\n\nDB, NDB = df['Outcome'].value_counts()\nprint('Number of patients diagnosed with Diabtetes disease: ',DB)\nprint('Number of patients not diagnosed with Diabtetes disease: ',NDB)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8ac0d8e2c5ad2d7bc7756dd00b22040573c62fa"},"cell_type":"markdown","source":"# Brief Analysis of the Data"},{"metadata":{"trusted":true,"_uuid":"ff4fec2697022a79d8cc9b259c54838291b2d494"},"cell_type":"code","source":"columns=df.columns[:8]\nplt.subplots(figsize=(18,15))\nlength=len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    df[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0eee5165981fbff90477d04b258126c76f7cad14"},"cell_type":"markdown","source":"It seems to have even distribution of data in all the features of the dataset."},{"metadata":{"_uuid":"29f1f8519e395024cdba588fc3ea87dbab05dc8e"},"cell_type":"markdown","source":"# Analysis of Diabetic Cases"},{"metadata":{"trusted":true,"_uuid":"0515450ef9c1989de48d3bb2b8dcc9772e6b856a"},"cell_type":"code","source":"df1=df[df['Outcome']==1]\ncolumns=df.columns[:8]\nplt.subplots(figsize=(18,15))\nlength=len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    df1[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"763cd088f8e12b9ab05722312eab764efb45af15"},"cell_type":"code","source":"sns.pairplot(df, hue = 'Outcome', vars = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI','DiabetesPedigreeFunction','Age'] )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cadb5d18bea21b18f6bb3692bf7c9e3a035bd178"},"cell_type":"code","source":"sns.jointplot(\"Pregnancies\", \"Insulin\", data=df, kind=\"reg\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ef9280c40a074c686ed870ae9133625702173a9"},"cell_type":"markdown","source":"# Feature Engineering\nNow, its time to add important features to the dataset and see their effect by visualizing them."},{"metadata":{"_uuid":"3a1c16c16ce84edea0151e7438784c008f19fe41"},"cell_type":"markdown","source":"**Feature 1 : BMI Indicator**<br>\nI m adding BMI Indicator feature as we know :\nIf you have a BMI of:\n- Under 18.5 – you are considered underweight and possibly malnourished.\n- 18.5 to 24.9 – you are within a healthy weight range for young and middle-aged adults.\n- 25.0 to 29.9 – you are considered overweight.\n- Over 30 – you are considered obese."},{"metadata":{"trusted":true,"_uuid":"efe0f7e9d70ea72414b1e4999d8ec4427cc77d81"},"cell_type":"code","source":"def set_bmi(row):\n    if row[\"BMI\"] < 18.5:\n        return \"Under\"\n    elif row[\"BMI\"] >= 18.5 and row[\"BMI\"] <= 24.9:\n        return \"Healthy\"\n    elif row[\"BMI\"] >= 25 and row[\"BMI\"] <= 29.9:\n        return \"Over\"\n    elif row[\"BMI\"] >= 30:\n        return \"Obese\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d51f65e22ab6cf31d5755df36cab14188baed554"},"cell_type":"code","source":"df = df.assign(BM_DESC=df.apply(set_bmi, axis=1))\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14b95bd69e021e7a89130bebcaa59b8c8225bd2c"},"cell_type":"markdown","source":"**Feature 2: Insulin Indicative Range** <br>\nIf insulin level (2-Hour serum insulin (mu U/ml)) is >= 16 and <= 166, then it is normal range\nelse it is considered as Abnormal"},{"metadata":{"trusted":true,"_uuid":"bfe92301003706147c1d56c1fe076dc0ae6f96df"},"cell_type":"code","source":"def set_insulin(row):\n    if row[\"Insulin\"] >= 16 and row[\"Insulin\"] <= 166:\n        return \"Normal\"\n    else:\n        return \"Abnormal\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57525bdfbc119c58075715af2f598a580d79140c"},"cell_type":"code","source":"df = df.assign(INSULIN_DESC=df.apply(set_insulin, axis=1))\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a25a716c2a7a96732da7675b4a4acb2bc7e4ebfc"},"cell_type":"code","source":"sns.countplot(data=df, x = 'INSULIN_DESC', label='Count')\n\nAB, NB = df['INSULIN_DESC'].value_counts()\nprint('Number of patients Having Abnormal Insulin Levels: ',AB)\nprint('Number of patients Having Normal Insulin Levels: ',NB)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d34a43ddac9722d37c5000f05cb098c3a2dcb78"},"cell_type":"markdown","source":"It seems from the above plot that more than 500 patients have Abnormal Insulin Levels where as around 250 patients have Normal Insulin Levels."},{"metadata":{"trusted":true,"_uuid":"1b4dde5db0ea1c9646e756137f2b5d6275ab266f"},"cell_type":"code","source":"sns.countplot(data=df, x = 'BM_DESC', label='Count')\n\nUD,H,OV,OB = df['BM_DESC'].value_counts()\nprint('Number of patients Having Underweight BMI Index: ',UD)\nprint('Number of patients Having Healthy BMI Index: ',H)\nprint('Number of patients Having Overweigth BMI Index: ',OV)\nprint('Number of patients Having Obese BMI Index: ',OB)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"430ac896a36d67d1288043e9bb509aaa7cf5b668"},"cell_type":"code","source":"g = sns.FacetGrid(df, col=\"INSULIN_DESC\", row=\"Outcome\", margin_titles=True)\ng.map(plt.scatter,\"Glucose\", \"BloodPressure\",  edgecolor=\"w\")\nplt.subplots_adjust(top=1.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aabccc1fce649645214e670d1ee556fcec0e2583"},"cell_type":"code","source":"g = sns.FacetGrid(df, col=\"Outcome\", row=\"INSULIN_DESC\", margin_titles=True)\ng.map(plt.hist, \"Age\", color=\"red\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Disease by INSULIN and Age');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce187f5988c6e4b7b353305fb5b1c997f540e3c7"},"cell_type":"code","source":"sns.boxplot(x=\"Age\", y=\"INSULIN_DESC\", hue=\"Outcome\", data=df);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5926371d713997c6065ad743dc5cbba4fa5d331d"},"cell_type":"markdown","source":"It seems from the above plot that patients having normal insulin levels are more diabetic within the age range from 25 and 42\nwhere as patients having anormal insulin levels are more diabetic in the age range of late 20's to mid 40's."},{"metadata":{"trusted":true,"_uuid":"9cf92d02cd403fd749849f7204fd390564fa6558"},"cell_type":"code","source":"sns.boxplot(x=\"Age\", y=\"BM_DESC\", hue=\"Outcome\", data=df);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"522f9f44ad00dd82f2891146453a976926751d2f"},"cell_type":"markdown","source":"From the above plot it is evident that patients who are obese as per BMI index are more diabetic in early age of 25 where as patients who are overweight are prone to diabetes in early 30's\n\nAs far as data is concerned it is the data of all women patients at least 21 years old of Pima Indian heritage.So, the findings may differ in other cases."},{"metadata":{"_uuid":"ace9ed043937eaca50511c8d6f0f932293c27f8a"},"cell_type":"markdown","source":"## Label Encoding"},{"metadata":{"_uuid":"6483048897fb6f288bfbf3c5bbdcac88f51321fd"},"cell_type":"markdown","source":"In this step we will encode the categorical variables BM_DESC,INSULIN_DESC into numerical values before fitting it into machine learning models."},{"metadata":{"trusted":true,"_uuid":"5c10763acc717c756c66921224c4af45894abccd"},"cell_type":"code","source":"df[\"INSULIN_DESC\"] = df.INSULIN_DESC.apply(lambda  x:1 if x==\"Normal\" else 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb1f5f7ba60cf80658b10f901f1f31e9fe851724"},"cell_type":"markdown","source":"Segregating Features and Target Variable.\n\nI have taken X as Feature variable and y as target variable."},{"metadata":{"trusted":true,"_uuid":"fde8dc0591b696301430df2c59c7f340f4b861f5"},"cell_type":"code","source":"X=pd.get_dummies(df,drop_first=True)\nX=X.drop(['Outcome'],axis=1)\ny = df['Outcome']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b67beeda5d7a0a02f7e418e2244a00efb9b4f62b"},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aab5048e2d25eecea17021f41f9d487a42fb5d98"},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"069cff37106e4237869b868c525db20c175af30f"},"cell_type":"markdown","source":"## Splitting data into Training & Testing Set\nThe training dataset and test dataset must be similar, usually have the same predictors or variables. They differ on the observations and specific values in the variables. If you fit the model on the training dataset, then you implicitly minimize error or find correct responses. The fitted model provides a good prediction on the training dataset. Then you test the model on the test dataset. If the model predicts good also on the test dataset, you have more confidence. You have more confidence since the test dataset is similar to the training dataset, but not the same nor seen by the model. It means the model transfers prediction or learning in real sense.\n\nSo,by splitting dataset into training and testing subset, we can efficiently measure our trained model since it never sees testing data before.Thus it's possible to prevent overfitting.\n\nI am just splitting dataset into 20% of test data and remaining 80% will used for training the model.\n\nI have used stratify parameter.This stratify parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter stratify.\n\nFor example, if variable y is a binary categorical variable with values 0 and 1 and there are 25% of zeros and 75% of ones, stratify=y will make sure that your random split has 25% of 0's and 75% of 1's."},{"metadata":{"trusted":true,"_uuid":"7cf79cbee67748499cef43b75415ecbb976eb79d"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,stratify=y, random_state = 1234)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37b464fb098928e52c784689c60337033bc2377d"},"cell_type":"markdown","source":"## Feature Scaling\nMost of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations, this is a problem.\n\nIf left alone, these algorithms only take in the magnitude of features neglecting the units. The results would vary greatly between different units, 5kg and 5000gms. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.\nTo supress this effect, we need to bring all features to the same level of magnitudes. This can be acheived by scaling."},{"metadata":{"trusted":true,"_uuid":"2501b4c062f4a317be1623e1d5caf8cdae36917f"},"cell_type":"code","source":"sc_X = StandardScaler()\nX_train_scaled = pd.DataFrame(sc_X.fit_transform(X_train))\nX_test_scaled = pd.DataFrame(sc_X.transform(X_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"054797b124af806170f11945eb91875571175a72"},"cell_type":"markdown","source":"## Applying Machine Learning Models"},{"metadata":{"trusted":true,"_uuid":"d3e685874a8a75fe2aaed53f9a0f7483edd9ecb6"},"cell_type":"code","source":"logi = LogisticRegression(random_state = 0, penalty = 'l1')\nlogi.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85736cc200707bd995a85d4fd37bbe5a6fb37dd5"},"cell_type":"code","source":"xgb_classifier = XGBClassifier()\nxgb_classifier.fit(X_train_scaled, y_train, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5279344046aca8400c4441bcc11cc406e7650adf"},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators = 100,criterion='gini', random_state = 47)\nrandom_forest.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5714cdacb540a6956d11e9219b11df09ad16c28a"},"cell_type":"code","source":"svc_model_l = SVC(kernel='linear',probability=True)\nsvc_model_l.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb57ff046212e2c8b462582b99bf1396d7686c23"},"cell_type":"code","source":"svc_model_r = SVC(kernel='rbf',probability=True)\nsvc_model_r.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f2a277afc4fffba6a01289944ae20313da38421"},"cell_type":"markdown","source":"## Cross validation"},{"metadata":{"trusted":true,"_uuid":"29dea208e150c56b62930994952b94d1202a42b9"},"cell_type":"code","source":"kfold = model_selection.KFold(n_splits=10, random_state=7)\nscoring = 'accuracy'\n\nacc_logi = cross_val_score(estimator = logi, X = X_train_scaled, y = y_train, cv = kfold,scoring=scoring)\nacc_logi.mean()\n\nacc_xgb = cross_val_score(estimator = xgb_classifier, X = X_train_scaled, y = y_train, cv = kfold,scoring=scoring)\nacc_xgb.mean()\n\nacc_rand = cross_val_score(estimator = random_forest, X = X_train_scaled, y = y_train, cv = kfold, scoring=scoring)\nacc_rand.mean()\n\nacc_svc_l = cross_val_score(estimator = svc_model_l, X = X_train_scaled, y = y_train, cv = kfold,scoring=scoring)\nacc_svc_l.mean()\n\nacc_svc_r = cross_val_score(estimator = svc_model_r, X = X_train_scaled, y = y_train, cv = kfold,scoring=scoring)\nacc_svc_r.mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80376a3bfb19dab69f4fb49f295715d6f241c6c3"},"cell_type":"markdown","source":"## Model Evaluation\nIn this step we will compare different performance metric such as cross validation accuracy, Precision,Recall,F1 Score, ROC etc."},{"metadata":{"trusted":true,"_uuid":"46e72aa428200d5a2d03caa389591884c5acffae"},"cell_type":"code","source":"y_predict_logi = logi.predict(X_test_scaled)\nacc= accuracy_score(y_test, y_predict_logi)\nroc=roc_auc_score(y_test, y_predict_logi)\nprec = precision_score(y_test, y_predict_logi)\nrec = recall_score(y_test, y_predict_logi)\nf1 = f1_score(y_test, y_predict_logi)\n\nresults = pd.DataFrame([['Logistic Regression',acc, acc_logi.mean(),prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy','Cross Val Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4842fcdb8422a180260564583a63caab568b9534"},"cell_type":"code","source":"y_predict_x = xgb_classifier.predict(X_test_scaled)\nroc=roc_auc_score(y_test, y_predict_x)\nacc = accuracy_score(y_test, y_predict_x)\nprec = precision_score(y_test, y_predict_x)\nrec = recall_score(y_test, y_predict_x)\nf1 = f1_score(y_test, y_predict_x)\n\nmodel_results = pd.DataFrame([['XG Boost',acc, acc_xgb.mean(),prec,rec, f1,roc]],\n               columns = ['Model','Accuracy', 'Cross Val Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a831a79f462952032d5e76a477ac797b66fad252"},"cell_type":"code","source":"y_predict_r = random_forest.predict(X_test_scaled)\nroc=roc_auc_score(y_test, y_predict_r)\nacc = accuracy_score(y_test, y_predict_r)\nprec = precision_score(y_test, y_predict_r)\nrec = recall_score(y_test, y_predict_r)\nf1 = f1_score(y_test, y_predict_r)\n\nmodel_results = pd.DataFrame([['Random Forest',acc, acc_rand.mean(),prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy','Cross Val Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"400d8ce94605bdf3dff92b6f327384256945f17a"},"cell_type":"code","source":"y_predict_s = svc_model_l.predict(X_test_scaled)\nroc=roc_auc_score(y_test, y_predict_s)\nacc = accuracy_score(y_test, y_predict_s)\nprec = precision_score(y_test, y_predict_s)\nrec = recall_score(y_test, y_predict_s)\nf1 = f1_score(y_test, y_predict_s)\n\nmodel_results = pd.DataFrame([['SVC Linear',acc, acc_svc_l.mean(),prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy','Cross Val Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26743f9248c5a0eb6b8b9fafc4d31b204aaea736"},"cell_type":"code","source":"y_predict_s1 = svc_model_r.predict(X_test_scaled)\nroc=roc_auc_score(y_test, y_predict_s1)\nacc = accuracy_score(y_test, y_predict_s1)\nprec = precision_score(y_test, y_predict_s1)\nrec = recall_score(y_test, y_predict_s1)\nf1 = f1_score(y_test, y_predict_s1)\n\nmodel_results = pd.DataFrame([['SVC RBF',acc, acc_svc_r.mean(),prec,rec, f1,roc]],\n               columns = ['Model', 'Accuracy','Cross Val Accuracy', 'Precision', 'Recall', 'F1 Score','ROC'])\nresults = results.append(model_results, ignore_index = True)\nresults","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4300633dff4be5f5d78897384aaec26e78e0cb2"},"cell_type":"markdown","source":"# Plotting ROC Curve\nAUC(Area Under Curve) - ROC (Receiver Operating Characterstics) curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\n\nThe ROC curve is plotted with TPR(True Positive Rate) against the FPR (False Positive Rate) where TPR is on y-axis and FPR is on the x-axis."},{"metadata":{"trusted":true,"_uuid":"23573ddf45945b668d43eb00be03d07e9d9efe30"},"cell_type":"code","source":"from sklearn import metrics\nimport matplotlib.pyplot as plt\n\nplt.figure()\n\n# Add the models to the list that you want to view on the ROC plot\nmodels = [\n{\n    'label': 'Logistic Regression',\n    'model': LogisticRegression(random_state = 0, penalty = 'l1'),\n},\n{\n    'label': 'XG Boost',\n    'model': XGBClassifier(),\n},\n    {\n    'label': 'Random Forest Gini',\n    'model': RandomForestClassifier(n_estimators = 100,criterion='gini', random_state = 47),\n},\n    {\n    'label': 'Support Vector Machine-L',\n    'model': SVC(kernel='linear',probability=True)} ,\n        {\n    'label': 'Support Vector Machine-RBF',\n    'model': SVC(kernel='rbf',probability=True) ,\n}\n]\n\n# Below for loop iterates through your models list\nfor m in models:\n    model = m['model'] # select the model\n    model.fit(X_train_scaled, y_train) # train the model\n    y_pred=model.predict(X_test_scaled) # predict the test data\n# Compute False postive rate, and True positive rate\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test_scaled)[:,1])\n# Calculate Area under the curve to display on the plot\n    auc = metrics.roc_auc_score(y_test,model.predict(X_test_scaled))\n# Now, plot the computed values\n    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], auc))\n# Custom settings for the plot \nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('1-Specificity(False Positive Rate)')\nplt.ylabel('Sensitivity(True Positive Rate)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2734f91cc88e97c8cb82f5fcc6d87674d24c00d7"},"cell_type":"markdown","source":"# Confusion Matrix"},{"metadata":{"trusted":true,"_uuid":"6fff239e4d548fbf23d5247c17e2b81ba724eb7f"},"cell_type":"code","source":"cm_logi = confusion_matrix(y_test, y_predict_logi)\nplt.title('Confusion matrix of the Logistic classifier')\nsns.heatmap(cm_logi,annot=True,fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"715c24e1f4a4faf2dd4c176edbaf85775cdbe911"},"cell_type":"code","source":"cm_x = confusion_matrix(y_test, y_predict_x)\nplt.title('Confusion matrix of the XGB classifier')\nsns.heatmap(cm_x,annot=True,fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f937cc192550cdd6aef867c72d3c4e722a86df53"},"cell_type":"code","source":"cm_r = confusion_matrix(y_test, y_predict_r)\nplt.title('Confusion matrix of the Random Forest classifier')\nsns.heatmap(cm_r,annot=True,fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78402a7621fb2db54f436d3f7286bed35f6a06e4"},"cell_type":"code","source":"cm = confusion_matrix(y_test, y_predict_s)\nplt.title('Confusion matrix of the SVC Linear classifier')\nsns.heatmap(cm,annot=True,fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d86bb866416c0a27d3832a35015975d4fae48a7d"},"cell_type":"markdown","source":"As we have seen from the above model evaluation, Logistic Regression and SVC Linear are best model for this dataset. so we will perform further Model evaluation of Logistic Regression."},{"metadata":{"_uuid":"aa9124c1057d6c5d61681b4e4fb6d1fdab46f58b"},"cell_type":"markdown","source":"# Model Evaluation Part 2\nIn this part we will further find Classification error,sensitivity and specifitivity of our logistic regression model."},{"metadata":{"trusted":true,"_uuid":"9fe765c770c7053d524308996d4dff53e2320817"},"cell_type":"code","source":"TP = cm_logi[1, 1]\nTN = cm_logi[0, 0]\nFP = cm_logi[0, 1]\nFN = cm_logi[1, 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"033306db8f287984eac9bb5fa62c5317727e43a1"},"cell_type":"code","source":"classification_error = (FP + FN) / float(TP + TN + FP + FN)\n\nprint(classification_error)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8f502d12f442d709f23b6723105f006677a5571"},"cell_type":"markdown","source":"The model has 18.83% of classification error."},{"metadata":{"_uuid":"0044356a00441c920d02ce32a48f9d031a3f3914"},"cell_type":"markdown","source":"# Sensitivity & Specifitivity\nBefore finding sensitivity and specifitivity we must know what these terms are :\n\nSensitivity and specificity are statistical measures of the performance of a binary classification test, also known in statistics as a classification function:\n\n- **Sensitivity** (also called the true positive rate, the recall, or probability of detection[1] in some fields) measures the proportion of actual positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition).\n- **Specificity** (also called the true negative rate) measures the proportion of actual negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition)."},{"metadata":{"trusted":true,"_uuid":"7700f10457256c57d5d64a5e4ee4e17df56ab85e"},"cell_type":"code","source":"sensitivity = TP / float(FN + TP)\n\nprint(sensitivity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b6ba1d687f06e774625d688e631af4c10bc24b5"},"cell_type":"code","source":"specificity = TN / (TN + FP)\n\nprint(specificity)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eace26424dd70490792b0b0e0638b54545143995"},"cell_type":"markdown","source":"The model is highly specific and less sensitive model."},{"metadata":{"trusted":true,"_uuid":"31b730e9a56a0fd044a91ab0ad792d771e9aacd0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}