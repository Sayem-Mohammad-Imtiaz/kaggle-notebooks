{"cells":[{"metadata":{},"cell_type":"markdown","source":"If any one had problems with data files I have uploaded, below there is original link I used to get them:\nhttps://archive.ics.uci.edu/ml/datasets/Adult\n\nAfter downloading please only change name of the file \"adult.names\" into \"adult_descr\".\nWe start with uploading basic libraries."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Data Import"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/adult-census-income-data/adult_data.csv')   ##import data\ndata_descr = pd.read_csv('../input/adult-census-income-data/adult_descr.csv', sep=':')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see how the data looks like. Firstly one can see we do not have correct headers by default in the below. To get correct ones we can use the file data_descr."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_descr   ## data descrption.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We take last 15 rows of data_descr to get headers. Moreover the first element from the list has to be moved on the last position to fit with the order of columnns in the data file."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_names = data_descr.tail(15)\nnames = list(data_names.index)\n\n## move the first column on the last position\nnames.append(names[0])\nnames = names[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can import the data again using known headers and start processing."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/adult-census-income-data/adult_data.csv', names=names)\ndata = data.rename(columns={'>50K, <=50K.': 'money'})\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. The first touch of our dataset"},{"metadata":{},"cell_type":"markdown","source":"We start with the first observation, to find out what is the frequency of our labels. "},{"metadata":{"trusted":true},"cell_type":"code","source":"money_group = data.groupby(by='money').count()\nless_than_50k = str(np.round(money_group.age[0]/len(data) * 100, 2))+' %'\ngreater_than_50k = str(np.round(money_group.age[1]/len(data) * 100, 2))+' %'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Due to the data', less_than_50k, 'of people earn less than 50K, and only', greater_than_50k, 'do more than 50K.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see how looks the distribution of the age in our database. The more it looks like the natural distribution, the more is possibility the data is representative."},{"metadata":{"trusted":true},"cell_type":"code","source":"age_count = data.groupby(by='age').count()\nage_unique_list = list(age_count.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(x=age_unique_list, y=age_count.values[:, 0], height=10, marginal_kws=dict(bins=len(age_count), rug=True), kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No we can start observe how each feature implies on the amount of money one can generate annualy. We start with comparing features from the Education category using a simple barplot. We take concerned columns, then group and count vaules with the respect to education categories and belonging into one of two groups of earning."},{"metadata":{"trusted":true},"cell_type":"code","source":"educ_money = data[['education', 'money']]\nless = educ_money[educ_money['money']==' <=50K'].groupby('education', as_index=False).count()\nmore = educ_money[educ_money['money']==' >50K'].groupby('education', as_index=False).count()\nmerge = more.merge(less, on='education', how='right', sort=True)\nmerge = merge.rename(columns={'education': 'Education', 'money_x': 'More than 50K', 'money_y': 'Less than 50K'})\n#merge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merge_melted = merge.melt(id_vars='Education').rename(columns=str.title)\n#merge_melted","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20, 8))\nsns.barplot(x='Education', y='Value', data=merge_melted, hue='Variable', palette='vlag')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the above plot one can see that people with education higher or equal to high school are able to earn more than 50K money. Furthermore only three groups: Doctorate, Masters and Prof-school, contain more pepole earning more than 50K, than people who generate less.\n\nBelow there are also plots concerned marital status, race and relationship. Instead of barplot we use vilolinplots to see all dependencies in more geometrical way. "},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(28, 10))\nf.suptitle('Capital - Marital Status - Race', fontsize=20)\nsns.violinplot(data=data,\n               x=\"marital-status\",\n               y=\"age\",\n               inner=\"quartile\",\n               hue='money',\n               scale=\"count\",\n               split=True, palette='vlag', linewidth=.8, ax=ax1, cut=2)\nsns.violinplot(data=data,\n               x=\"race\",\n               y=\"age\",\n               inner=\"quartile\",\n               hue='money',\n               scale=\"count\",\n               split=True, palette='vlag', linewidth=.8, ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, figsize=(26, 10))\nf.suptitle('Capital - Relationship', fontsize=20)\nsns.violinplot(data=data,\n               x=\"relationship\",\n               y=\"age\",\n               inner=\"quartile\",\n               hue='money',\n               scale=\"count\",\n               split=True, palette='vlag', linewidth=.8, ax=ax, cut=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Processing (1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data.drop(columns=['relationship', 'education', 'education-num', 'marital-status', 'workclass'])\nlabels = data['money']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We create new columns as below to get numerical values for each category. I tried to simplify that, therefore there are categories with few feateures assigned to the one numerical value. For example in the Education column, all feateures from 1st to 12th are assigned to zero. Moreover "},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Relationship_Num'] = data['relationship'].map(dict(zip(list(np.unique(data['relationship'])), [0, 1, 2, 3, 4, 0])))\ntrain['Education_Num'] = data['education'].map(dict(zip(list(np.unique(data['education'])), [0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 0, 4, 0, 5, 0])))\ntrain['Marital_Status_Num'] = data['marital-status'].map(dict(zip(list(np.unique(data['marital-status'])), [0, 1, 1, 1, 2, 0, 3])))\ntrain['Workclass_Num'] = data['workclass'].map(dict(zip(list(np.unique(data['workclass'])), [np.nan, 0, 0, 1, 2, 2, 2, 0, 1])))\ntrain['sex'] = data['sex'].map(dict(zip(list(np.unique(data['sex'])), [0, 1])))  #1 - male, 0 - female\ntrain['race'] = data['race'].map(dict(zip(list(np.unique(data['race'])), list(range(len(data.groupby(by=['race']).count().iloc[:,0]))))))\ntrain['Occupation_Num'] = data['occupation'].map(dict(zip(list(np.unique(data['occupation'])), list(range(len(data.groupby(by=['occupation']).count().iloc[:,0]))))))\ntrain['Occupation_Num'] = train['Occupation_Num'].replace({0: np.nan})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some numerical features are grouped as below to get better interpretation at the end. That is easier to classified that three men are between 40 and 50 years old, then looking on three independent amounts in same range."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['HrsBand'] = pd.cut(data['hours-per-week'], 5)\ntrain['HrsBand_Num'] = pd.cut(data['hours-per-week'], 5, labels=list(range(len(train.groupby(by=['HrsBand']).count().iloc[:,0]))))\ntrain['AgeBand'] = pd.cut(data['age'], 5)\ntrain['Age_Num'] = pd.cut(data['age'], 5, labels=list(range(len(train.groupby(by=['AgeBand']).count().iloc[:,0]))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we can take all countries from the native-country column and assign them into six categories: A - Asia, Am-S - South America, Am-N - North America, Am-M - Middle America, E - Europe, O - Other,"},{"metadata":{"trusted":true},"cell_type":"code","source":"countries= np.unique(train['native-country'])\ncountries = countries.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes = ['O', 'A', 'Am-N', 'A', 'Am-S', 'Am-M', 'Am-M', 'Am-S', 'Am-M', 'E', 'E', 'E', 'E', 'Am-M', 'Am-M', 'E', 'Am-M', 'A', 'E', 'A', 'A', 'E', 'E', 'Am-M', 'A', 'A', 'Am-M', 'Am-M', 'O', 'Am-S', 'A', 'E', 'E', 'Am-M', 'E', 'O', 'A', 'A', 'Am-S', 'Am-N', 'A', 'E']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## quick check that we did not miss anything\nprint(len(countries)==len(indexes))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create dictionary that assign each country to its continent\nnative_countries_dictionary = {a: b for a, b in zip(countries, indexes)}   \n\n## crate new column with assigned continents\ntrain['Continents'] = train['native-country'].map(native_countries_dictionary)  \n\n## create new column with numerical values - each number represents assinged continent\ntrain['Continents_Num'] = train['Continents'].map(dict(zip(list(np.unique(train['Continents'])), list(range(len(train.groupby(by=['Continents']).count().iloc[:,0]))))))    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are almost ready to split the data into training, validation subsets."},{"metadata":{"trusted":true},"cell_type":"code","source":"## we delete all non-numerical columns\ntrain_drop = train.drop(columns=['occupation', 'native-country', 'Continents', 'AgeBand', 'age', 'hours-per-week', 'HrsBand'])\n\n## check length of the data before deleting NaN's\nlen(train_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## drop NaN's and check the lenght again\ntrain_drop_na = train_drop.dropna()\nlen(train_drop_na)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## change money features into numerical values: 0 - less than 50K, 1 - more than 50K\ntrain_drop_na.money = train_drop_na.money.map(dict(zip(list(np.unique(labels)), [0, 1])))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## change numerical values to int32 type\ntrain_drop_na = train_drop_na.astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_drop_na.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Data vs. Searching Characteristics"},{"metadata":{},"cell_type":"markdown","source":"We can stop for the moment here before further processing. Now the data is of the form that helps to find best atributies that characterize people that are able to gain more money. Let us see what is the basic result once comparing only sex feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"male_money = train_drop_na[train_drop_na.sex==1].groupby(by='money').count()\nfemale_money = train_drop_na[train_drop_na.sex==0].groupby(by='money').count()\n\nmale_percent_low = np.round(male_money.sex.values[0]/male_money.sex.sum(),2)\nmale_percent_high = np.round(male_money.sex.values[1]/male_money.sex.sum(),2)\nfemale_percent_low = np.round(female_money.sex.values[0]/female_money.sex.sum()*100,2)\nfemale_percent_high = np.round(female_money.sex.values[1]/female_money.sex.sum()*100,2)\n\ntitle = str(male_percent_high) + '% of males gain more than 50K, ' + str(male_percent_low) + '& males gain less. For women ' + str(female_percent_low) + '% of them gain less than 50K, ' + str(female_percent_high) + '% gain more.'\nprint(title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let the above result will be a point of start. Only 11% of females earns more once taking all data."},{"metadata":{"trusted":true},"cell_type":"code","source":"best_attributies = train_drop_na[train_drop_na.sex==0]\nbest_attributies.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"## take only wifes and husbands - they are assigned to 0 in column Relationship_Num\nbest_attributies = best_attributies[best_attributies.Relationship_Num==0]\n\n## take only Masters - they are assigned to 4\nbest_attributies = best_attributies[best_attributies.Education_Num==4]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's quite easy: wifes (can split outcomes on themselves and husbands) with Master Degree seem to be one of the best group to start looking for rich women. As one can see below, more than 83% of females gain more than 50K in this group."},{"metadata":{"trusted":true},"cell_type":"code","source":"female_money = best_attributies.groupby(by='money').count()\n\nif len(female_money.sex.values)==2:\n    female_percent_low = np.round(female_money.sex.values[0]/female_money.sex.sum() * 100,2)\n    female_percent_high = np.round(female_money.sex.values[1]/female_money.sex.sum() *100 ,2)\n    title = str(female_percent_low) + '% of women gain less than 50K, ' + str(female_percent_high) + '% gain more.'\n    print(title)\nelse:\n    print('100% of women gain more than 50K/year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax) = plt.subplots(1, figsize=(20, 10))\nf.suptitle('Capital - Race', fontsize=20)\nsns.violinplot(data=best_attributies,\n               x=\"Workclass_Num\",\n               y=\"Age_Num\",\n               inner=\"quartile\",\n               hue='money',\n               scale=\"area\",\n               split=True, palette='vlag', linewidth=.8, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.unique(data.workclass)[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_attributies = best_attributies[best_attributies.Workclass_Num==2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(26, 10))\nf.suptitle('Capital - Occupation - Race', fontsize=20)\nsns.violinplot(data=best_attributies,\n               x=\"Occupation_Num\",\n               y=\"Age_Num\",\n               inner=\"quartile\",\n               hue='money',\n               scale=\"count\",\n               split=True, palette='vlag', linewidth=.8, ax=ax1, cut=2)\nsns.violinplot(data=best_attributies,\n               x=\"race\",\n               y=\"Age_Num\",\n               inner=\"quartile\",\n               hue='money',\n               scale=\"count\",\n               split=True, palette='vlag', linewidth=.8, ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.unique(data.occupation)[10])\nbest_attributies = best_attributies[best_attributies.Occupation_Num==10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"female_money = best_attributies.groupby(by='money').count()\n\nif len(female_money.sex.values)==2:\n    female_percent_low = np.round(female_money.sex.values[0]/female_money.sex.sum() * 100,2)\n    female_percent_high = np.round(female_money.sex.values[1]/female_money.sex.sum() *100 ,2)\n    title = str(female_percent_low) + '% of women gain less than 50K, ' + str(female_percent_high) + '% gain more.'\n    print(title)\nelse:\n    print('100% of women gain more than 50K/year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax) = plt.subplots(1, figsize=(26, 10))\nf.suptitle('Capital - Race', fontsize=20)\nsns.violinplot(data=best_attributies,\n               x=\"race\",\n               y=\"Age_Num\",\n               inner=\"quartile\",\n               hue='money',\n               scale=\"count\",\n               split=True, palette='vlag', linewidth=.8, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.unique(data.race)[4])\nbest_attributies = best_attributies[best_attributies.race==4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"female_money = best_attributies.groupby(by='money').count()\n\nif len(female_money.sex.values)==2:\n    female_percent_low = np.round(female_money.sex.values[0]/female_money.sex.sum() * 100,2)\n    female_percent_high = np.round(female_money.sex.values[1]/female_money.sex.sum() *100 ,2)\n    title = str(female_percent_low) + '% of women gain less than 50K, ' + str(female_percent_high) + '% gain more.'\n    print(title)\nelse:\n    print('100% of women gain more than 50K/year')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax) = plt.subplots(1, figsize=(26, 10))\nf.suptitle('Capital - Hrs/Week', fontsize=20)\nsns.violinplot(data=best_attributies,\n               x=\"HrsBand_Num\",\n               y=\"Age_Num\",\n               inner=\"quartile\",\n               hue='money',\n               scale=\"count\",\n               split=True, palette='vlag', linewidth=.8, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.unique(train.AgeBand)[1])\nbest_attributies = best_attributies[best_attributies.HrsBand_Num==1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"female_money = best_attributies.groupby(by='money').count()\n\nif len(female_money.sex.values)==2:\n    female_percent_low = np.round(female_money.sex.values[0]/female_money.sex.sum() * 100,2)\n    female_percent_high = np.round(female_money.sex.values[1]/female_money.sex.sum() *100 ,2)\n    title = str(female_percent_low) + '% of women gain less than 50K, ' + str(female_percent_high) + '% gain more.'\n    print(title)\nelse:\n    print('100% of women gain more than 50K/year')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It would mean, that 92.86& of white wifes who work in local government, spend there between 32 and 46 hours per week, has master degree and present proficiency level are able to gain more than 50K annualy. Obviously there is many ways to manipulate data and they depend on the goal one would achive, I only roughly presented how one could handle with that task."},{"metadata":{},"cell_type":"markdown","source":"# 5. Data Processing (2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"## standarization for fnlwgt column\nfnlwgt_norm = (train_drop_na.fnlwgt - train_drop_na.fnlwgt.mean())/train_drop_na.fnlwgt.std()\ntrain_drop_na['fnlwgt'] = train_drop_na['fnlwgt'].map(dict(zip(list(train_drop_na.fnlwgt), fnlwgt_norm)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## standarization for capital-gain column\ncapital_gain_norm = (train_drop_na['capital-gain'] - train_drop_na['capital-gain'].mean())/train_drop_na['capital-gain'].std()\ntrain_drop_na['capital-gain'] = train_drop_na['capital-gain'].map(dict(zip(list(train_drop_na['capital-gain']), capital_gain_norm)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## standarization for capital-loss column\ncapital_loss_norm = (train_drop_na['capital-loss'] - train_drop_na['capital-loss'].mean())/train_drop_na['capital-loss'].std()\ntrain_drop_na['capital-loss'] = train_drop_na['capital-loss'].map(dict(zip(list(train_drop_na['capital-loss']), capital_gain_norm)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_drop_na.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## splitting features and labels\ntrain_only = train_drop_na.drop(columns=['money'])\nlabels_only = train_drop_na['money']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train_only.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n#cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nf, ax = plt.subplots(figsize=(11, 9))\nsns.heatmap(corr, cbar=True, annot=True, mask=mask, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see above, mostly correlated features are ones that connect age, sex, relationship, marital-status and education number. It seems to be true, that relationship is correlated with sex somehow - it's hardly possible that husband can be a female. Same with the age and education; there is really small chance that prof-school could be younger than 35 etc. Finally I decided to leave all columns as they are."},{"metadata":{"trusted":true},"cell_type":"code","source":"## create a dictionary with integers and assined race features\nrace_dict = dict(zip(list(range(len(data.groupby(by=['race']).count().iloc[:,0]))), list(np.unique(data['race']))))\nrelationship_dict = dict(zip(list(range(len(train_drop_na.groupby(by=['Relationship_Num']).count().iloc[:,0]))), list(np.unique(data['relationship']))))\neducation_dict = dict(zip(list([0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 0, 4, 0, 5, 0]), list(np.unique(data['education'])) ))\nmarital_dict = dict(zip(list([0, 1, 1, 1, 2, 0, 3]), list(np.unique(data['marital-status'])) ))\nworkclass_dict = dict(zip(list([0, 0, 1, 2, 2, 2, 0, 1]), list(np.unique(data['workclass'])) ))\noccupation_dict = dict(zip(list(range(len(data.groupby(by=['occupation']).count()))), list(np.unique(data['occupation']))))\ncontinents_dict = dict(zip(list(range(len(train_drop_na.groupby(by=['Continents_Num']).count().iloc[:,0]))), list(np.unique(train['Continents']))))\nprint(race_dict)\nprint(relationship_dict)\nprint(education_dict)\nprint(marital_dict)\nprint(workclass_dict)\nprint(occupation_dict)\nprint(continents_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## race\nrace_dummy = pd.get_dummies(train_drop_na.race)\nrace_dummy = race_dummy.rename(columns=race_dict)\n## relationship\nrelationship_dummy = pd.get_dummies(train_drop_na.Relationship_Num)\nrelationship_dummy = relationship_dummy.rename(columns=relationship_dict)\n## education\neducation_dummy = pd.get_dummies(train_drop_na.Education_Num)\neducation_dummy = education_dummy.rename(columns=education_dict)\n## marital-status\nmarital_dummy = pd.get_dummies(train_drop_na.Marital_Status_Num)\nmarital_dummy = marital_dummy.rename(columns=marital_dict)\n## workclass\nworkclass_dummy = pd.get_dummies(train_drop_na.Workclass_Num)\nworkclass_dummy = workclass_dummy.rename(columns=workclass_dict)\n## occupation\noccupation_dummy = pd.get_dummies(train_drop_na.Occupation_Num)\noccupation_dummy = occupation_dummy.rename(columns=occupation_dict)\n## continents\ncontinents_dummy = pd.get_dummies(train_drop_na.Continents_Num)\ncontinents_dummy = continents_dummy.rename(columns=continents_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concat_list = [train_only, race_dummy, relationship_dummy, education_dummy, marital_dummy, workclass_dummy, occupation_dummy, continents_dummy]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_with_dummies = pd.concat(concat_list, axis=1)\ntrain_with_dummies = train_with_dummies.drop(columns=['race', 'Relationship_Num', 'Education_Num', 'Marital_Status_Num', 'Workclass_Num', 'Occupation_Num', 'Continents_Num'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_with_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train_with_dummies, labels_only, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready to start train some models and choose the best one."},{"metadata":{},"cell_type":"markdown","source":"# 6. Models, training, prediction and evaluation "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classificator_names, acc_values = [], []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## we start with basic classification algorithm that is logistic regression\nclassificator_names.append('Logistic Regression')\n\nlogreg_clf = LogisticRegression(random_state=42)\nlogreg_clf.fit(X_train, y_train)\nacc_logreg = round(logreg_clf.score(X_val, y_val) * 100, 2)\nprint (acc_logreg)\nacc_values.append(acc_logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## next try is taking Logistic Regression model with Stochastic Gradient Decent optimization\nclassificator_names.append('Logistic Regression with Stochastic Gradient Decent Optimizer')\n\nsgd_log_clf = SGDClassifier(loss='log', random_state=42)\nsgd_log_clf.fit(X_train, y_train)\nacc_log_sgd = round(sgd_log_clf.score(X_val, y_val) * 100, 2)\nprint (acc_log_sgd)\nacc_values.append(acc_log_sgd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## DecisionTreeClassifier with default hyperparameters\nclassificator_names.append('Decision Tree')\n\ndec_tree_clf = DecisionTreeClassifier(random_state=42)\ndec_tree_clf.fit(X_train, y_train)\nacc_decision_tree = round(dec_tree_clf.score(X_val, y_val) * 100, 2)\nprint (acc_decision_tree)\nacc_values.append(acc_decision_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## using GridSearch model we can find best hyperparameters:\nclassificator_names.append('Tuned Decision Tree')\n\ntuned_dec_tree_clf = DecisionTreeClassifier(random_state=42)\ngrid_values = {'max_depth': [2, 4, 8, 10, 12], 'max_leaf_nodes': [2, 6, 8, 10, 12]}\ngrid_clf_acc = GridSearchCV(tuned_dec_tree_clf, param_grid = grid_values, scoring = 'accuracy', cv=5)\ngrid_clf_acc.fit(X_train, y_train)\nacc_grid_decision_tree = round(grid_clf_acc.score(X_val, y_val) * 100, 2)\nprint('The accuracy using hyperparameters:', grid_clf_acc.best_params_, 'is equal to:', acc_grid_decision_tree)\nacc_values.append(acc_grid_decision_tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## RandomForestClassifier with default hyperparameters\nclassificator_names.append('Random Forest')\n\nrand_for_clf = RandomForestClassifier(random_state=42, n_estimators=100)\nrand_for_clf.fit(X_train, y_train)\nacc_random_forest = round(rand_for_clf.score(X_val, y_val) * 100, 2)\nprint (acc_random_forest)\nacc_values.append(acc_random_forest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## again we are looking for best parameters\nclassificator_names.append('Tuned Random Forest')\n\ntuned_rand_for_clf = RandomForestClassifier(random_state=42, n_estimators=100)\ngrid_values = {'max_depth': [5, 10, 15], 'max_leaf_nodes': [8, 12, 16]}\ngrid_clf_acc = GridSearchCV(tuned_rand_for_clf, param_grid = grid_values, scoring = 'accuracy', cv=5)\ngrid_clf_acc.fit(X_train, y_train)\nacc_grid_random_forest = round(grid_clf_acc.score(X_val, y_val) * 100, 2)\nprint('The accuracy using hyperparameters:', grid_clf_acc.best_params_, 'is equal to:', acc_grid_random_forest)\nacc_values.append(acc_grid_random_forest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Support Vector Machine with defauls hyperparamteres\nclassificator_names.append('Support Vector Machine')\n\nsvm_clf = SVC(random_state=42, gamma='scale')\nsvm_clf.fit(X_train, y_train)\nacc_svm = round(svm_clf.score(X_val, y_val) * 100, 2)\nprint(acc_svm)\nacc_values.append(acc_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM with GridSearch tuning\nclassificator_names.append('Tuned Support Vector Machine')\n\ntuned_svm_clf = SVC(kernel='rbf', random_state=42, gamma='scale')\ngrid_values = {'C': [1, 10, 100]}\ngrid_clf_acc = GridSearchCV(tuned_svm_clf, param_grid = grid_values, scoring = 'accuracy', cv=5)\ngrid_clf_acc.fit(X_train, y_train)\nacc_grid_svm = round(grid_clf_acc.score(X_val, y_val) * 100, 2)\nprint('The accuracy using hyperparameters:', grid_clf_acc.best_params_, 'is equal to:', acc_grid_svm)\nacc_values.append(acc_grid_svm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## SVM with SGD Optimizer\nclassificator_names.append('Support Vector Machine with SGD Optimizer')\n\nsgd_svm_clf = SGDClassifier(loss='hinge', random_state=42)\nsgd_svm_clf.fit(X_train, y_train)\nacc_svm_sgd = round(sgd_svm_clf.score(X_val, y_val) * 100, 2)\nprint (acc_svm_sgd)\nacc_values.append(acc_svm_sgd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## K-Nearest Neighbors\nclassificator_names.append('K-Nearest Neighbors')\n\nknn_clf = KNeighborsClassifier(algorithm='auto', n_neighbors=5)\nknn_clf.fit(X_train, y_train)\nacc_knn = round(knn_clf.score(X_val, y_val) * 100, 2)\nprint(acc_knn)\nacc_values.append(acc_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Gaussian Naive Bayes\nclassificator_names.append('Gaussian Naive Bayes')\n\ngauss_clf = GaussianNB()\ngauss_clf.fit(X_train, y_train)   \nacc_gauss = round(gauss_clf.score(X_val, y_val) * 100, 2)\nprint (acc_gauss)\nacc_values.append(acc_gauss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below table presents all trained models with accuracy results."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = pd.DataFrame({\n    'Model': classificator_names, 'Score': acc_values\n})\nclassifiers = classifiers.sort_values(by='Score', ascending=False).reset_index(drop=True)\nclassifiers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below there are results from boosting models, that I've tried to get better score."},{"metadata":{"trusted":true},"cell_type":"code","source":"boosting_list, boosting_acc = [], []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boosting_list.append('Gradient Boosting Classifier')\n\ngbrt = GradientBoostingClassifier(max_depth = 2, n_estimators = 150, learning_rate = .2)\ngbrt.fit(X_train, y_train)\nacc_gbrt = round(gbrt.score(X_val, y_val) * 100, 2)\nprint(acc_gbrt)\nboosting_acc.append(acc_gbrt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boosting_list.append('AdaBoost Classifier')\n\nada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 2, max_leaf_nodes=9), n_estimators = 150, algorithm = 'SAMME.R', learning_rate = 0.2)\nada_clf.fit(X_train, y_train)\nacc_ada = round(ada_clf.score(X_val, y_val) * 100, 2)\nprint(acc_ada)\nboosting_acc.append(acc_ada)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boosting_list.append('XGBoost Classifier')\n\nmodel = xgb.XGBClassifier(objective='binary:logistic', learning_rate= 1, colsample_bynode= 0.8, subsample= 0.8, num_parallel_tree=100, eval_metric = 'auc')\nmodel.fit(X_train, y_train)\nacc_xgb = round(model.score(X_val, y_val) * 100, 2)\nprint(acc_xgb)\nboosting_acc.append(acc_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## for XGBoost we can create plot of most important features\nf, ax = plt.subplots(1, figsize=(26, 10))\nxgb.plot_importance(model, importance_type='weight', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boosting_list.append('Bagging Classifier')\n\nbag_clf = BaggingClassifier(DecisionTreeClassifier(splitter = 'random', max_leaf_nodes = 16), n_estimators = 500, bootstrap = True, n_jobs = -1)\nbag_clf.fit(X_train, y_train)\nacc_bag_clf = round(bag_clf.score(X_val, y_val) * 100, 2)\nprint(acc_bag_clf)\nboosting_acc.append(acc_bag_clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boosting_classifiers = pd.DataFrame({\n    'Model': boosting_list, 'Score': boosting_acc\n})\nboosting_classifiers = boosting_classifiers.sort_values(by='Score', ascending=False).reset_index(drop=True)\nboosting_classifiers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. adult_test file - processing and testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/adult-census-income-data/adult_test.csv', names=names)\ntest_data = test_data.rename(columns={'>50K, <=50K.': 'money'})\ntest_data = test_data.iloc[1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_test = test_data.drop(columns=['relationship', 'education', 'education-num', 'marital-status', 'workclass'])\ntest_labels = test_data['money']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_test['Relationship_Num'] = test_data['relationship'].map(dict(zip(list(np.unique(test_data['relationship'])), [0, 1, 2, 3, 4, 0])))\ntrain_on_test['Education_Num'] = test_data['education'].map(dict(zip(list(np.unique(test_data['education'])), [0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 3, 0, 4, 0, 5, 0])))\ntrain_on_test['Marital_Status_Num'] = test_data['marital-status'].map(dict(zip(list(np.unique(test_data['marital-status'])), [0, 1, 1, 1, 2, 0, 3])))\ntrain_on_test['Workclass_Num'] = test_data['workclass'].map(dict(zip(list(np.unique(test_data['workclass'])), [np.nan, 0, 0, 1, 2, 2, 2, 0, 1])))\ntrain_on_test['sex'] = test_data['sex'].map(dict(zip(list(np.unique(test_data['sex'])), [0, 1])))  #0 - male, 1 - female\ntrain_on_test['race'] = test_data['race'].map(dict(zip(list(np.unique(test_data['race'])), list(range(len(test_data.groupby(by=['race']).count().iloc[:,0]))))))\ntrain_on_test['Occupation_Num'] = test_data['occupation'].map(dict(zip(list(np.unique(test_data['occupation'])), list(range(len(test_data.groupby(by=['occupation']).count().iloc[:,0]))))))\ntrain_on_test['Occupation_Num'] = train_on_test['Occupation_Num'].replace({0: np.nan})\ntrain_on_test['HrsBand'] = pd.cut(test_data['hours-per-week'], 5)\ntrain_on_test['HrsBand_Num'] = pd.cut(test_data['hours-per-week'], 5, labels=list(range(len(train_on_test.groupby(by=['HrsBand']).count().iloc[:,0]))))\ntrain_on_test['AgeBand'] = pd.cut(test_data.age.astype('int32'), 5)  ## in the test file values of age column have string type\ntrain_on_test['Age_Num'] = pd.cut(test_data.age.astype('int32'), 5, labels=list(range(len(train_on_test.groupby(by=['AgeBand']).count().iloc[:,0]))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_test['Continents'] = train_on_test['native-country'].map(native_countries_dictionary)  \ntrain_on_test['Continents_Num'] = train_on_test['Continents'].map(dict(zip(list(np.unique(train_on_test['Continents'])), list(range(len(train_on_test.groupby(by=['Continents']).count().iloc[:,0]))))))    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_test_drop = train_on_test.drop(columns=['occupation', 'native-country', 'Continents', 'AgeBand', 'age', 'hours-per-week', 'HrsBand'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_test_drop_na = train_on_test_drop.dropna()\nprint('There is', len(train_on_test_drop) - len(train_on_test_drop_na),\"Nan rows in the test dataset, it's\", np.round((len(train_on_test_drop) - len(train_on_test_drop_na))/len(train_on_test_drop) * 100, 2),'% that needs to be deleted.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_test_drop_na.money = train_on_test_drop_na.money.map(dict(zip(list(np.unique(test_labels)), [0, 1])))\ntrain_on_test_drop_na = train_on_test_drop_na.astype('int32')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## standarization for fnlwgt column\nfnlwgt_norm = (train_on_test_drop_na.fnlwgt - train_on_test_drop_na.fnlwgt.mean())/train_on_test_drop_na.fnlwgt.std()\ntrain_on_test_drop_na['fnlwgt'] = train_on_test_drop_na['fnlwgt'].map(dict(zip(list(train_on_test_drop_na.fnlwgt), fnlwgt_norm)))\n## standarization for capital-gain column\ncapital_gain_norm = (train_on_test_drop_na['capital-gain'] - train_on_test_drop_na['capital-gain'].mean())/train_on_test_drop_na['capital-gain'].std()\ntrain_on_test_drop_na['capital-gain'] = train_on_test_drop_na['capital-gain'].map(dict(zip(list(train_on_test_drop_na['capital-gain']), capital_gain_norm)))\n## standarization for capital-loss column\ncapital_loss_norm = (train_on_test_drop_na['capital-loss'] - train_on_test_drop_na['capital-loss'].mean())/train_on_test_drop_na['capital-loss'].std()\ntrain_on_test_drop_na['capital-loss'] = train_on_test_drop_na['capital-loss'].map(dict(zip(list(train_on_test_drop_na['capital-loss']), capital_gain_norm)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## race\ntest_race_dummy = pd.get_dummies(train_on_test_drop_na.race)\ntest_race_dummy = test_race_dummy.rename(columns=race_dict)\n## relationship\ntest_relationship_dummy = pd.get_dummies(train_on_test_drop_na.Relationship_Num)\ntest_relationship_dummy = test_relationship_dummy.rename(columns=relationship_dict)\n## education\ntest_education_dummy = pd.get_dummies(train_on_test_drop_na.Education_Num)\ntest_education_dummy = test_education_dummy.rename(columns=education_dict)\n## marital-status\ntest_marital_dummy = pd.get_dummies(train_on_test_drop_na.Marital_Status_Num)\ntest_marital_dummy = test_marital_dummy.rename(columns=marital_dict)\n## workclass\ntest_workclass_dummy = pd.get_dummies(train_on_test_drop_na.Workclass_Num)\ntest_workclass_dummy = test_workclass_dummy.rename(columns=workclass_dict)\n## occupation\ntest_occupation_dummy = pd.get_dummies(train_on_test_drop_na.Occupation_Num)\ntest_occupation_dummy = test_occupation_dummy.rename(columns=occupation_dict)\n## continents\ntest_continents_dummy = pd.get_dummies(train_on_test_drop_na.Continents_Num)\ntest_continents_dummy = test_continents_dummy.rename(columns=continents_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_test_only = train_on_test_drop_na.drop(columns=['money'])\ntest_labels_only = train_on_test_drop_na['money']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_concat_list = [train_on_test_only, test_race_dummy, test_relationship_dummy, test_education_dummy, test_marital_dummy, test_workclass_dummy, test_occupation_dummy, test_continents_dummy]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_test_with_dummies = pd.concat(test_concat_list, axis=1)\ntrain_on_test_with_dummies = train_on_test_with_dummies.drop(columns=['race', 'Relationship_Num', 'Education_Num', 'Marital_Status_Num', 'Workclass_Num', 'Occupation_Num', 'Continents_Num'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_on_test_with_dummies.columns)==len(train_with_dummies.columns)) ## check if number of columns in train and test datasets are same","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('We can see that boosting classifiers work better during the training than the previous ones, the best boosting classifier is', \n      boosting_classifiers.values[0][0],'while the worst regular one is',classifiers.values[len(classifiers)-1][0],'. Let us check how they behave on the test one.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc_ada = round(ada_clf.score(train_on_test_with_dummies, test_labels_only) * 100, 2)\nprint('Accuracy for AdaBoost Classifier on the test set is:', test_acc_ada)\ntest_acc_gauss = round(gauss_clf.score(train_on_test_with_dummies, test_labels_only) * 100, 2)\nprint ('Accuracy for Gaussian Naive Bayes on the test set is:', test_acc_gauss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As excpected there is huge difference on accuracy for two above models. However, it does not mean that AdaBoost Classifier is the best possible here. Once we pass from the training set onto the test one, some factors, like overfitting, may occur. Due to computational and data processing capacity on my home workstation I could not find all best possible parameters during GridSearch tuning. Finally, I guess there are better ways of preparing data, that could also improve final results. I used tools I have already learned, so that maybe this kernel will be helpful somehow. Once I tried run all models on the test data, the best accuracy I got from the XGBoost model, which (again, as excpected) should reduce overfitting, is shown below."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc_gbrt = round(gbrt.score(train_on_test_with_dummies, test_labels_only) * 100, 2)\nprint('Best accuracy using XGBoost model:',test_acc_gbrt)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}