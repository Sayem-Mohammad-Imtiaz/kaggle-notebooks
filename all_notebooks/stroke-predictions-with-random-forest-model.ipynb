{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stroke predictions","metadata":{}},{"cell_type":"markdown","source":"### Introduction","metadata":{}},{"cell_type":"markdown","source":"In this notebook we investigate the occurrence of strokes based on anonymized data of patients. We will train various machine learning algorithms to predict strokes based on input parameters such as age, body mass index, and smoking status. There are many metrics to assess model perfomance. However, in this analysis we will select the best model based on the F1-score only.","metadata":{}},{"cell_type":"markdown","source":"### Imports and settings","metadata":{}},{"cell_type":"code","source":"# Standard Python libraries:\nimport sys\nimport time\nfrom typing import List, Dict, Any, Union\nimport warnings\n\n# Data processing and modeling:\nfrom imblearn.over_sampling import SMOTE\nimport numpy as np\nimport pandas as pd\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Data visualization:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Settings:\nwarnings.filterwarnings(\"ignore\")\nnp.set_printoptions(threshold=sys.maxsize)\npd.set_option(\"display.max_colwidth\", None)\nsns.set_theme()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:33.116015Z","iopub.execute_input":"2021-06-02T20:36:33.116536Z","iopub.status.idle":"2021-06-02T20:36:34.710807Z","shell.execute_reply.started":"2021-06-02T20:36:33.116396Z","shell.execute_reply":"2021-06-02T20:36:34.709673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data parsing","metadata":{}},{"cell_type":"markdown","source":"The variable *input_path* corresponds to the path of the input CSV file (the healtcare dataset). This file is converted to a pandas dataframe where each row corresponds to a unique patient.","metadata":{}},{"cell_type":"code","source":"input_path = \"../input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\"\ndf = pd.read_csv(input_path)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-06-02T20:36:34.712184Z","iopub.execute_input":"2021-06-02T20:36:34.712515Z","iopub.status.idle":"2021-06-02T20:36:34.751497Z","shell.execute_reply.started":"2021-06-02T20:36:34.712484Z","shell.execute_reply":"2021-06-02T20:36:34.750306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our dataframe contains the following columns:","metadata":{}},{"cell_type":"code","source":"df.columns = df.columns.str.lower()\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:34.75322Z","iopub.execute_input":"2021-06-02T20:36:34.753611Z","iopub.status.idle":"2021-06-02T20:36:34.786733Z","shell.execute_reply.started":"2021-06-02T20:36:34.753577Z","shell.execute_reply":"2021-06-02T20:36:34.785281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:34.788115Z","iopub.execute_input":"2021-06-02T20:36:34.788474Z","iopub.status.idle":"2021-06-02T20:36:34.82459Z","shell.execute_reply.started":"2021-06-02T20:36:34.788444Z","shell.execute_reply":"2021-06-02T20:36:34.823593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The meanings of the columns are rather self-explanatory. There are eleven features in total, plus the label. The label is represented by the *stroke* column. Note that the *bmi* column has missing values, which is something that we will tackle later. Furthermore, a number of features are categorical rather than numerical. As most machine learning algorithms require numeric input, we will address this issue for each categorical feature individually.","metadata":{}},{"cell_type":"markdown","source":"### Data exploration and preprocessing","metadata":{}},{"cell_type":"markdown","source":"In the following we will study each column in more detail. Besides exploratory analyses, we will also immediately perform most of the required preprocessing.","metadata":{}},{"cell_type":"markdown","source":"#### ID feature","metadata":{}},{"cell_type":"markdown","source":"Let's start with the *id* column. Since this column will not be relevant to predict strokes, we will simply drop it. However, before we remove it let us make sure that the dataset does not contain any duplicate IDs.","metadata":{}},{"cell_type":"code","source":"df = df.drop_duplicates(subset=\"id\")\ndf = df.drop([\"id\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:34.82584Z","iopub.execute_input":"2021-06-02T20:36:34.826124Z","iopub.status.idle":"2021-06-02T20:36:34.839642Z","shell.execute_reply.started":"2021-06-02T20:36:34.826096Z","shell.execute_reply":"2021-06-02T20:36:34.838555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Gender feature","metadata":{}},{"cell_type":"markdown","source":"The *gender* column shows the following distribution:","metadata":{}},{"cell_type":"code","source":"df[\"gender\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:34.841097Z","iopub.execute_input":"2021-06-02T20:36:34.841637Z","iopub.status.idle":"2021-06-02T20:36:34.850092Z","shell.execute_reply.started":"2021-06-02T20:36:34.841598Z","shell.execute_reply":"2021-06-02T20:36:34.849332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For simplicity, we will only consider two gender options. The *Other* value can be replaced by the majority vote, which is *Female*. Furthermore, we will convert gender into a numerical variable using binary encoding:","metadata":{}},{"cell_type":"code","source":"df[\"gender\"] = df[\"gender\"].replace([\"Other\"], \"Female\")\ngender_conversion = {\"Male\": 0, \"Female\": 1}\ndf[\"gender\"] = df[\"gender\"].map(gender_conversion)\ndf[\"gender\"] = df[\"gender\"].astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:34.851178Z","iopub.execute_input":"2021-06-02T20:36:34.851679Z","iopub.status.idle":"2021-06-02T20:36:34.867679Z","shell.execute_reply.started":"2021-06-02T20:36:34.851647Z","shell.execute_reply":"2021-06-02T20:36:34.866732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the plot below we can see how gender affects the probability of getting a stroke:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nsns.barplot(x=\"gender\", y=\"stroke\", data=df)\nplt.xticks(list(gender_conversion.values()), list(gender_conversion.keys()))\nplt.xlabel(\"Gender\", fontweight=\"bold\")\nplt.ylabel(\"Stroke (mean)\", fontweight=\"bold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:34.868994Z","iopub.execute_input":"2021-06-02T20:36:34.86951Z","iopub.status.idle":"2021-06-02T20:36:35.186844Z","shell.execute_reply.started":"2021-06-02T20:36:34.869467Z","shell.execute_reply":"2021-06-02T20:36:35.186033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The wide, colored bars represent mean values and the black lines provide the boundaries of the 95% confidence interval. We note that there is no big differene between males and females.","metadata":{}},{"cell_type":"markdown","source":"#### Age feature","metadata":{}},{"cell_type":"markdown","source":"Next up is the *age* column:","metadata":{}},{"cell_type":"code","source":"df[\"age\"].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:35.189239Z","iopub.execute_input":"2021-06-02T20:36:35.189716Z","iopub.status.idle":"2021-06-02T20:36:35.200771Z","shell.execute_reply.started":"2021-06-02T20:36:35.18967Z","shell.execute_reply":"2021-06-02T20:36:35.199555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The correlation between age and having a stroke is as follows:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(df[df[\"stroke\"] == 0][\"age\"], binwidth=5, binrange=[0, 85], stat=\"probability\", color=\"limegreen\", label=\"No stroke\")\nsns.histplot(df[df[\"stroke\"] == 1][\"age\"], binwidth=5, binrange=[0, 85], stat=\"probability\", color=\"firebrick\", label=\"Stroke\")\nplt.xlabel(\"Age\", fontweight=\"bold\")\nplt.ylabel(\"Stroke (probability)\", fontweight=\"bold\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:35.202957Z","iopub.execute_input":"2021-06-02T20:36:35.203337Z","iopub.status.idle":"2021-06-02T20:36:35.689586Z","shell.execute_reply.started":"2021-06-02T20:36:35.203303Z","shell.execute_reply":"2021-06-02T20:36:35.688522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The sum of the bar heights for each scenario (stroke vs. no stroke) equals unity. We can infer that the chances of having a stroke significantly increase with age.","metadata":{}},{"cell_type":"markdown","source":"#### Hypertension feature","metadata":{}},{"cell_type":"markdown","source":"The *hypertension* field takes on the values 0 (no hypertension) and 1 (hypertension): ","metadata":{}},{"cell_type":"code","source":"df[\"hypertension\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:35.691032Z","iopub.execute_input":"2021-06-02T20:36:35.691446Z","iopub.status.idle":"2021-06-02T20:36:35.70145Z","shell.execute_reply.started":"2021-06-02T20:36:35.691401Z","shell.execute_reply":"2021-06-02T20:36:35.700191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The relation with *stroke* can be visualized as follows:","metadata":{}},{"cell_type":"code","source":"hypertension_conversion = {\"No hypertension\": 0, \"Hypertension\": 1}\nplt.figure(figsize=(6,4))\nsns.barplot(x=\"hypertension\", y=\"stroke\", data=df)\nplt.xticks(list(hypertension_conversion.values()), list(hypertension_conversion.keys()))\nplt.xlabel(\"Hypertension\", fontweight=\"bold\")\nplt.ylabel(\"Stroke (mean)\", fontweight=\"bold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:35.702969Z","iopub.execute_input":"2021-06-02T20:36:35.703255Z","iopub.status.idle":"2021-06-02T20:36:35.979082Z","shell.execute_reply.started":"2021-06-02T20:36:35.703229Z","shell.execute_reply":"2021-06-02T20:36:35.978024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We note that patients with hypertension are more likely to experience a stroke.","metadata":{}},{"cell_type":"markdown","source":"#### Heart disease feature","metadata":{}},{"cell_type":"markdown","source":"The variable *heart_disease* is either 0 (no heart disease) or 1 (heart disease):","metadata":{}},{"cell_type":"code","source":"df[\"heart_disease\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:35.982214Z","iopub.execute_input":"2021-06-02T20:36:35.982768Z","iopub.status.idle":"2021-06-02T20:36:35.991172Z","shell.execute_reply.started":"2021-06-02T20:36:35.982722Z","shell.execute_reply":"2021-06-02T20:36:35.990084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot below shows the correlation between heart disease and strokes:","metadata":{}},{"cell_type":"code","source":"heart_conversion = {\"No heart disease\": 0, \"Heart disease\": 1}\nplt.figure(figsize=(6,4))\nsns.barplot(x=\"heart_disease\", y=\"stroke\", data=df)\nplt.xticks(list(heart_conversion.values()), list(heart_conversion.keys()))\nplt.xlabel(\"Heart disease\", fontweight=\"bold\")\nplt.ylabel(\"Stroke (mean)\", fontweight=\"bold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:35.992389Z","iopub.execute_input":"2021-06-02T20:36:35.992672Z","iopub.status.idle":"2021-06-02T20:36:36.252388Z","shell.execute_reply.started":"2021-06-02T20:36:35.992644Z","shell.execute_reply":"2021-06-02T20:36:36.251341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this plot we infer that heart disease increases the chances of experiencing a stroke.","metadata":{}},{"cell_type":"markdown","source":"#### Ever married feature","metadata":{}},{"cell_type":"markdown","source":"The column *ever_married* shows whether the patient has ever been married:","metadata":{}},{"cell_type":"code","source":"df[\"ever_married\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:36.253558Z","iopub.execute_input":"2021-06-02T20:36:36.253838Z","iopub.status.idle":"2021-06-02T20:36:36.263876Z","shell.execute_reply.started":"2021-06-02T20:36:36.253811Z","shell.execute_reply":"2021-06-02T20:36:36.262653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To convert this feature to a numerical one, we can apply binary encoding:","metadata":{}},{"cell_type":"code","source":"married_conversion = {\"No\": 0, \"Yes\": 1}\ndf[\"ever_married\"] = df[\"ever_married\"].map(married_conversion)\ndf[\"ever_married\"] = df[\"ever_married\"].astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:36.264932Z","iopub.execute_input":"2021-06-02T20:36:36.265222Z","iopub.status.idle":"2021-06-02T20:36:36.279114Z","shell.execute_reply.started":"2021-06-02T20:36:36.265196Z","shell.execute_reply":"2021-06-02T20:36:36.277978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution of this variable with respect to strokes is as follows:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nsns.barplot(x=\"ever_married\", y=\"stroke\", data=df)\nplt.xticks(list(married_conversion.values()), list(married_conversion.keys()))\nplt.xlabel(\"Ever married\", fontweight=\"bold\")\nplt.ylabel(\"Stroke (mean)\", fontweight=\"bold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:36.280854Z","iopub.execute_input":"2021-06-02T20:36:36.281179Z","iopub.status.idle":"2021-06-02T20:36:36.555735Z","shell.execute_reply.started":"2021-06-02T20:36:36.281133Z","shell.execute_reply":"2021-06-02T20:36:36.554601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that marriage significantly increases the chances of having a stroke.","metadata":{}},{"cell_type":"markdown","source":"#### Work type feature","metadata":{}},{"cell_type":"markdown","source":"Next, we have the column *work_type*. There are five possible values:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.barplot(x=\"work_type\", y=\"stroke\", data=df)\nplt.xlabel(\"Work type\", fontweight=\"bold\")\nplt.ylabel(\"Stroke (mean)\", fontweight=\"bold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:36.556975Z","iopub.execute_input":"2021-06-02T20:36:36.557244Z","iopub.status.idle":"2021-06-02T20:36:36.979304Z","shell.execute_reply.started":"2021-06-02T20:36:36.557217Z","shell.execute_reply":"2021-06-02T20:36:36.978261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that *Private* and *Govt_job* have a similar impact on strokes. The same is true for *children* and *Never_worked*. For this reason, we can create three categories instead without loosing much information. Moreover, the fact that a patient is a child is actually redundant as it is already captured by the *age* field.","metadata":{}},{"cell_type":"code","source":"df[\"work_type\"] = df[\"work_type\"].replace([\"Self-employed\"], \"self-employed\")\ndf[\"work_type\"] = df[\"work_type\"].replace([\"Private\", \"Govt_job\"], \"employed\")\ndf[\"work_type\"] = df[\"work_type\"].replace([\"children\", \"Never_worked\"], \"never_employed\")\n\n# Sanity check that children (up to age 12) are never employed:\ndf.loc[df[\"age\"] < 13, \"work_type\"] = \"never_employed\"\n\ndf[\"work_type\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:36.980295Z","iopub.execute_input":"2021-06-02T20:36:36.9806Z","iopub.status.idle":"2021-06-02T20:36:37.001699Z","shell.execute_reply.started":"2021-06-02T20:36:36.98057Z","shell.execute_reply":"2021-06-02T20:36:36.999829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the work type categories are nominal, we can benefit from one-hot encoding (we  will drop one column to remove redundant information) to create a numerical variable:","metadata":{}},{"cell_type":"code","source":"df_work_ohe = pd.get_dummies(\n    df[\"work_type\"], \n    prefix=\"work_ohe\", \n    drop_first=True,\n)\ndf = pd.concat([df, df_work_ohe], axis=1)\ndf = df.drop([\"work_type\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:37.003409Z","iopub.execute_input":"2021-06-02T20:36:37.003762Z","iopub.status.idle":"2021-06-02T20:36:37.018048Z","shell.execute_reply.started":"2021-06-02T20:36:37.003731Z","shell.execute_reply":"2021-06-02T20:36:37.016803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Residence type feature","metadata":{}},{"cell_type":"markdown","source":"For the *residence_type* field, the distribution is as follows:","metadata":{}},{"cell_type":"code","source":"df[\"residence_type\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:37.019678Z","iopub.execute_input":"2021-06-02T20:36:37.020063Z","iopub.status.idle":"2021-06-02T20:36:37.031376Z","shell.execute_reply.started":"2021-06-02T20:36:37.020028Z","shell.execute_reply":"2021-06-02T20:36:37.030188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can convert this categorical field to a numerical one using binary encoding:","metadata":{}},{"cell_type":"code","source":"residence_conversion = {\"Rural\": 0, \"Urban\": 1}\ndf[\"residence_type\"] = df[\"residence_type\"].map(residence_conversion)\ndf[\"residence_type\"] = df[\"residence_type\"].astype(int)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:37.032638Z","iopub.execute_input":"2021-06-02T20:36:37.032915Z","iopub.status.idle":"2021-06-02T20:36:37.049127Z","shell.execute_reply.started":"2021-06-02T20:36:37.032889Z","shell.execute_reply":"2021-06-02T20:36:37.047855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot below shows the relation of this variable with *stroke*:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nsns.barplot(x=\"residence_type\", y=\"stroke\", data=df)\nplt.xticks(list(residence_conversion.values()), list(residence_conversion.keys()))\nplt.xlabel(\"Residence type\", fontweight=\"bold\")\nplt.ylabel(\"Stroke (mean)\", fontweight=\"bold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:37.050668Z","iopub.execute_input":"2021-06-02T20:36:37.05101Z","iopub.status.idle":"2021-06-02T20:36:37.323188Z","shell.execute_reply.started":"2021-06-02T20:36:37.050942Z","shell.execute_reply":"2021-06-02T20:36:37.322285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Residence type does seem to affect strokes much.","metadata":{}},{"cell_type":"markdown","source":"#### Glucose level feature","metadata":{}},{"cell_type":"markdown","source":"The following column, *avg_glucose_level*, describes the average glucose level in mg/dL. Its statistical details read:","metadata":{}},{"cell_type":"code","source":"df[\"avg_glucose_level\"].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:37.324489Z","iopub.execute_input":"2021-06-02T20:36:37.324768Z","iopub.status.idle":"2021-06-02T20:36:37.336323Z","shell.execute_reply.started":"2021-06-02T20:36:37.324739Z","shell.execute_reply":"2021-06-02T20:36:37.335367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the [Mayo Clinic](https://www.mayoclinic.org/diseases-conditions/diabetes/diagnosis-treatment/drc-20371451) we learn the following in relation to diabetes:\n\n> A blood sugar level less than 140 mg/dL is normal. A reading of more than 200 mg/dL after two hours indicates diabetes. A reading between 140 and 199 mg/dL indicates prediabetes.\n\nGlucose level and strokes are related as follows:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(df[df[\"stroke\"] == 0][\"avg_glucose_level\"], binwidth=10, binrange=[50, 280], stat=\"probability\", color=\"limegreen\", label=\"No stroke\")\nsns.histplot(df[df[\"stroke\"] == 1][\"avg_glucose_level\"], binwidth=10, binrange=[50, 280], stat=\"probability\", color=\"firebrick\", label=\"Stroke\")\nplt.xlabel(\"Average glucose level (mg/dL)\", fontweight=\"bold\")\nplt.ylabel(\"Stroke (probability)\", fontweight=\"bold\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:37.338005Z","iopub.execute_input":"2021-06-02T20:36:37.338473Z","iopub.status.idle":"2021-06-02T20:36:37.738047Z","shell.execute_reply.started":"2021-06-02T20:36:37.338425Z","shell.execute_reply":"2021-06-02T20:36:37.736949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot we infer that there is a (positive) relation between strokes and being (pre)diabetic.","metadata":{}},{"cell_type":"markdown","source":"#### Smoking status feature","metadata":{}},{"cell_type":"markdown","source":"The next feature column is *smoking_status*. There are four possible values:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.barplot(x=\"smoking_status\", y=\"stroke\", data=df)\nplt.xlabel(\"Smoking status\", fontweight=\"bold\")\nplt.ylabel(\"Stroke (mean)\", fontweight=\"bold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:37.739826Z","iopub.execute_input":"2021-06-02T20:36:37.740256Z","iopub.status.idle":"2021-06-02T20:36:38.118751Z","shell.execute_reply.started":"2021-06-02T20:36:37.740211Z","shell.execute_reply":"2021-06-02T20:36:38.117635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although smoking does not seem beneficial for your health, the relationship with strokes is not so clear from this plot due to the large confidence intervals.\n\nThe value *Unknown* means that information on smoking status is unavailable. To decide on what to do with this, let us look at the number of occurrences:","metadata":{}},{"cell_type":"code","source":"df[\"smoking_status\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.123973Z","iopub.execute_input":"2021-06-02T20:36:38.12432Z","iopub.status.idle":"2021-06-02T20:36:38.134217Z","shell.execute_reply.started":"2021-06-02T20:36:38.124289Z","shell.execute_reply":"2021-06-02T20:36:38.1331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since *Unknown* occurs 1544 times in our dataset, it is best to leave it as a separate category rather than to replace it by guesses.\n\nWe consider this variable to be nominal. For that reason we can apply one-hot encoding to convert it into a numerical feature:","metadata":{}},{"cell_type":"code","source":"df_smoking_ohe = pd.get_dummies(\n    df[\"smoking_status\"], \n    prefix=\"smoking_ohe\", \n    drop_first=True,\n)\ndf_smoking_ohe = df_smoking_ohe.rename(columns={\n    \"smoking_ohe_never smoked\": \"smoking_ohe_never_smoked\", \n    \"smoking_ohe_formerly smoked\": \"smoking_ohe_formerly_smoked\",\n})\ndf = pd.concat([df, df_smoking_ohe], axis=1)\ndf = df.drop([\"smoking_status\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.136079Z","iopub.execute_input":"2021-06-02T20:36:38.136417Z","iopub.status.idle":"2021-06-02T20:36:38.152098Z","shell.execute_reply.started":"2021-06-02T20:36:38.136377Z","shell.execute_reply":"2021-06-02T20:36:38.150778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### BMI feature","metadata":{}},{"cell_type":"markdown","source":"The next column is *bmi*, the body mass index (BMI) in kg/m$^2$. Its statistical details are as follows:","metadata":{}},{"cell_type":"code","source":"df[\"bmi\"].describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.1541Z","iopub.execute_input":"2021-06-02T20:36:38.154655Z","iopub.status.idle":"2021-06-02T20:36:38.172453Z","shell.execute_reply.started":"2021-06-02T20:36:38.154616Z","shell.execute_reply":"2021-06-02T20:36:38.171288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the [CDC](https://www.cdc.gov/healthyweight/assessing/bmi/adult_bmi/index.html) we learn the following in relation to obesity:\n\n| BMI | Weight status |\n| ---: | :--- |\n| < 18.5 | Underweight |\n| 18.5 - 24.9 | Normal weight |\n| 25.0 - 29.9 | Overweight |\n| > 30.0 | Obese |\n\nFrom earlier we know that there are 201 missing values for this field. There are various ways to deal with this. The best option might be to infer those missing values from the other features using a regression model. For convenience, let us pick the easiest regression algorithm for this, i.e. linear regression:","metadata":{}},{"cell_type":"code","source":"train_data = df.dropna()\nX_train = train_data.drop(\"bmi\", axis=1)\ny_train = train_data[\"bmi\"]\n\ntest_data = df[df[\"bmi\"].isnull()]\nX_test = test_data.drop(\"bmi\", axis=1)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_test = model.predict(X_test)\n\nbmi_slice = df[\"bmi\"].copy()\nbmi_slice[np.isnan(bmi_slice)] = y_test\ndf[\"bmi\"] = bmi_slice","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.173977Z","iopub.execute_input":"2021-06-02T20:36:38.174332Z","iopub.status.idle":"2021-06-02T20:36:38.219559Z","shell.execute_reply.started":"2021-06-02T20:36:38.174298Z","shell.execute_reply":"2021-06-02T20:36:38.218377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that our BMI values are complete, let us study the relationship with strokes:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.histplot(df[df[\"stroke\"] == 0][\"bmi\"], binwidth=2, binrange=[10, 100], stat=\"probability\", color=\"limegreen\", label=\"No stroke\")\nsns.histplot(df[df[\"stroke\"] == 1][\"bmi\"], binwidth=2, binrange=[10, 100], stat=\"probability\", color=\"firebrick\", label=\"Stroke\")\nplt.xlabel(\"BMI (kg/m2)\", fontweight=\"bold\")\nplt.ylabel(\"Stroke (probability)\", fontweight=\"bold\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.221174Z","iopub.execute_input":"2021-06-02T20:36:38.221842Z","iopub.status.idle":"2021-06-02T20:36:38.821638Z","shell.execute_reply.started":"2021-06-02T20:36:38.221788Z","shell.execute_reply":"2021-06-02T20:36:38.820664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distributions for stroke and no stroke are fairly similar. However, strokes seem to be relatively common in the BMI range of 26-32 kg/m$^2$.","metadata":{}},{"cell_type":"markdown","source":"#### Stroke label","metadata":{}},{"cell_type":"markdown","source":"Finally, we have the target column *stroke*. Its values refer to whether the patient has experienced a stroke (1) or not (0).","metadata":{}},{"cell_type":"code","source":"df[\"stroke\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.822921Z","iopub.execute_input":"2021-06-02T20:36:38.823201Z","iopub.status.idle":"2021-06-02T20:36:38.833915Z","shell.execute_reply.started":"2021-06-02T20:36:38.823175Z","shell.execute_reply":"2021-06-02T20:36:38.832816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We note that this dataset is extremely imbalanced. In fact, if our model would always predict 0 it would be correct 95% of the time! For this reason, accuracy will not be a suitable metric to measure model performance. Furthermore, in the next section we will try to balance our dataset more to avoid any model bias.","metadata":{}},{"cell_type":"markdown","source":"#### Wrap-up","metadata":{}},{"cell_type":"markdown","source":"To conclude the preprocessing, let us verify that there are no longer any missing values and that all fields are numeric of nature:","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.835587Z","iopub.execute_input":"2021-06-02T20:36:38.836134Z","iopub.status.idle":"2021-06-02T20:36:38.863711Z","shell.execute_reply.started":"2021-06-02T20:36:38.83609Z","shell.execute_reply":"2021-06-02T20:36:38.862706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.86493Z","iopub.execute_input":"2021-06-02T20:36:38.865215Z","iopub.status.idle":"2021-06-02T20:36:38.882909Z","shell.execute_reply.started":"2021-06-02T20:36:38.865187Z","shell.execute_reply":"2021-06-02T20:36:38.881654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model building","metadata":{}},{"cell_type":"markdown","source":"Now that we have almost fully preprocessed our dataset, we can try to model it with various machine learning algorithms. First, we will split the data into training and test sets. After the split we will perform a few more preprocessing steps. Then quickly train and test various different algorithms to see which perform best. The best ones will be examined in more detail to further improve the predictions.","metadata":{}},{"cell_type":"markdown","source":"#### Train-test split","metadata":{}},{"cell_type":"markdown","source":"First, we make a train-test split of the data:","metadata":{}},{"cell_type":"code","source":"X = df.drop([\"stroke\"], axis=1)\ny = df[\"stroke\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.88425Z","iopub.execute_input":"2021-06-02T20:36:38.884557Z","iopub.status.idle":"2021-06-02T20:36:38.898594Z","shell.execute_reply.started":"2021-06-02T20:36:38.884528Z","shell.execute_reply":"2021-06-02T20:36:38.897542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us take a look at the distribution of the target label:","metadata":{}},{"cell_type":"code","source":"y_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.899866Z","iopub.execute_input":"2021-06-02T20:36:38.900189Z","iopub.status.idle":"2021-06-02T20:36:38.911837Z","shell.execute_reply.started":"2021-06-02T20:36:38.900158Z","shell.execute_reply":"2021-06-02T20:36:38.910664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since our dataset is highly imbalanced, there is a risk that our models will be biased toward predicting no stroke. To combat this issue, we can apply an oversampling technique called SMOTE, which is short for Synthetic Minority Oversampling Technique. This technique makes use of the K-nearest neighbors algorithm to synthesize more data for the minority class. After applying SMOTE to the training set, the stroke vs. no-stroke rows are more balanced.","metadata":{}},{"cell_type":"code","source":"oversampler = SMOTE(sampling_strategy=0.65, random_state=0)\nX_train, y_train = oversampler.fit_resample(X_train, y_train)\ny_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.913591Z","iopub.execute_input":"2021-06-02T20:36:38.914076Z","iopub.status.idle":"2021-06-02T20:36:38.940905Z","shell.execute_reply.started":"2021-06-02T20:36:38.914031Z","shell.execute_reply":"2021-06-02T20:36:38.939881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we apply standardization (meaning zero mean and unit variance) to all features (i.e. *X_train* and *X_test*) to ensure that all data 'lives' at the same scale:","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nfeature_sets = [scaler.fit_transform(feature_set) for feature_set in [X_train, X_test]]    \nX_train = feature_sets[0]\nX_test = feature_sets[1]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.942125Z","iopub.execute_input":"2021-06-02T20:36:38.942738Z","iopub.status.idle":"2021-06-02T20:36:38.960439Z","shell.execute_reply.started":"2021-06-02T20:36:38.942693Z","shell.execute_reply":"2021-06-02T20:36:38.959548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we start training, it is good practice to verify the dimensions of the data:","metadata":{}},{"cell_type":"code","source":"y_train = np.array(y_train)\nprint(\"Dimensions of the training set:\\n\")\nprint(\"Features:\\t\", X_train.shape, \"\\nLabels:\\t\\t\", y_train.shape)\n\ny_test = np.array(y_test)\nprint(\"\\nDimensions of the test set:\\n\")\nprint(\"Features:\\t\", X_test.shape, \"\\nLabels:\\t\\t\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.962134Z","iopub.execute_input":"2021-06-02T20:36:38.962603Z","iopub.status.idle":"2021-06-02T20:36:38.971655Z","shell.execute_reply.started":"2021-06-02T20:36:38.962555Z","shell.execute_reply":"2021-06-02T20:36:38.970304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Naive modeling","metadata":{}},{"cell_type":"markdown","source":"Using the Scikit-learn library, we can quickly try many different machine learning algorithms and select the best one. To this end, we define a function that we can invoke for each algorithm that calculates the mean F1-score that results from cross validation:","metadata":{}},{"cell_type":"code","source":"def cross_validation(algorithm: Any, X_train: Any, y_train: Any) -> Dict[str, float]:\n    \"\"\"Performs and assesses cross validation for a given machine learning algorithm.\n    \n    The performance metric is the F1-score.\n    \n    Args:\n        algorithm: The Scikit-learn algorithm class.\n        X_train: The input for training.\n        y_train: The labels for training.\n    \n    Returns:\n        The mean and standard deviation for the F1-score.\n    \"\"\"\n    \n    cross_val_scores = cross_val_score(algorithm, X_train, y_train, cv=5, scoring=\"f1\")\n    f1_mean = round(cross_val_scores.mean(), 3)\n    f1_std = round(cross_val_scores.std(), 3)\n    results = {\"Mean\": f1_mean, \"Standard deviation\": f1_std}\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.973485Z","iopub.execute_input":"2021-06-02T20:36:38.973952Z","iopub.status.idle":"2021-06-02T20:36:38.99646Z","shell.execute_reply.started":"2021-06-02T20:36:38.973895Z","shell.execute_reply":"2021-06-02T20:36:38.994859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us now collect all algorithms that we want to try out. In this subsection we perform a 'quick and dirty' analysis using the Scikit-learn default settings for the algorithms.","metadata":{}},{"cell_type":"code","source":"algorithms = {\n    \"Gaussian naive Bayes\": GaussianNB(),\n    \"K-nearest neighbors\": KNeighborsClassifier(),\n    \"Support vector machine\": SVC(),\n    \"Logistic regression\": LogisticRegression(),\n    \"Multilayer perceptron\": MLPClassifier(random_state=0),\n    \"Decision tree\": DecisionTreeClassifier(random_state=0),\n    \"Random forest\": RandomForestClassifier(random_state=0),\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:38.998026Z","iopub.execute_input":"2021-06-02T20:36:38.998782Z","iopub.status.idle":"2021-06-02T20:36:39.013257Z","shell.execute_reply.started":"2021-06-02T20:36:38.998733Z","shell.execute_reply":"2021-06-02T20:36:39.012354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we can perform cross validation for all these algorithms. We collect the mean and standard deviation for all F1-scores. The results are as follows:","metadata":{}},{"cell_type":"code","source":"cv_results = []\nfor name, algorithm in algorithms.items():\n    start_time = time.time()\n    results = cross_validation(algorithm, X_train, y_train)\n    elapsed_time = round(time.time() - start_time, 2)\n    f1_mean = results[\"Mean\"]\n    f1_std = results[\"Standard deviation\"]\n    cv_results.append((name, f1_mean))\n    print(\"\\n{}\\nDuration: \\t{} seconds\\nF1-score: \\t{} ± {}\".format(name.upper(), elapsed_time, f1_mean, f1_std))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:36:39.014729Z","iopub.execute_input":"2021-06-02T20:36:39.015448Z","iopub.status.idle":"2021-06-02T20:37:34.435104Z","shell.execute_reply.started":"2021-06-02T20:36:39.0154Z","shell.execute_reply":"2021-06-02T20:37:34.434086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To select the most suitable algorithms, let us sort them by their (mean) F1-scores:","metadata":{}},{"cell_type":"code","source":"sorted_cv_results = sorted(cv_results, key=lambda x: x[1], reverse=True)\npd.DataFrame(sorted_cv_results, columns=[\"Model\", \"F1-score\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:37:34.436548Z","iopub.execute_input":"2021-06-02T20:37:34.436888Z","iopub.status.idle":"2021-06-02T20:37:34.450611Z","shell.execute_reply.started":"2021-06-02T20:37:34.43685Z","shell.execute_reply":"2021-06-02T20:37:34.449236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The four best models are the random forest (RF), decision tree (DT), K-nearest neighbors (KNN), and multilayer perceptron (MLP) models. An RF in general performs better than a DT as an RF essentially averages a number of DTs through ensemling to improve generalizability. For this reason, we will discard the DT model and only continue with the RF, KNN, and MLP models.","metadata":{}},{"cell_type":"markdown","source":"#### Improved modeling","metadata":{}},{"cell_type":"markdown","source":"In this subsection we aim to improve the RF, KNN, and MLP models. To this end, we can perform hyperparameter grid searches to optimize the models. For convenience, let us define the following functoin that facilitates grid searches:","metadata":{}},{"cell_type":"code","source":"def grid_search(\n        algorithm: Any, \n        parameters: Dict[str, List[Any]],\n        X_train: Any, \n        y_train: Any, \n) -> Dict[str, Union[float, Dict[str, Any]]]:\n    \"\"\"Performs a grid search for a given algorithm to find the best hyperparameters.\n    \n   All parameter combinations are considered. The best parameters are those that \n   maximize the F1-score using cross validation.\n    \n    Args:\n        algorithm: The Scikit-learn algorithm class.\n        parameters: A grid of hyperparameters that the algorithm will loop through.\n        X_train: The input for training.\n        y_train: The labels for training.\n    \n    Returns:\n        The best F1-score and the corresponding hyperparameter settings.\n    \"\"\"\n    \n    clf = GridSearchCV(algorithm, parameters, cv=5, scoring=\"f1\")\n    clf.fit(X_train, y_train)\n    best_f1 = round(clf.best_score_, 3)\n    best_parameters = clf.best_params_\n    results = {\"F1-score\": best_f1, \"Parameters\": best_parameters}\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:37:34.452673Z","iopub.execute_input":"2021-06-02T20:37:34.453237Z","iopub.status.idle":"2021-06-02T20:37:34.463663Z","shell.execute_reply.started":"2021-06-02T20:37:34.453187Z","shell.execute_reply":"2021-06-02T20:37:34.462239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us now define the hyperparameters that we would like to try:","metadata":{}},{"cell_type":"code","source":"knn_parameters = {\n    \"n_neighbors\": [2, 3, 5, 8, 13],\n    \"weights\": [\"uniform\", \"distance\"],\n    \"algorithm\": [\"ball_tree\", \"kd_tree\", \"brute\"],\n    \"p\": [1, 2],\n}  # Number of combinations: 5*2*3*2 = 60\nmlp_parameters = {\n    \"hidden_layer_sizes\": [(8,), (13,), (21,), (8, 8,), (13, 13,), (21, 21,)],\n    \"solver\": [\"adam\", \"lbfgs\"],\n    \"alpha\": [0.01, 0.1, 1.0],\n    \"random_state\": [0],\n}  # Number of combinations: 6*2*3 = 36\nrf_parameters = {\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"max_features\": [\"auto\", 5, 8],\n    \"max_depth\": [8, 13],\n    \"min_samples_split\": [5, 8],\n    \"ccp_alpha\": [0.001, 0.01, 0.1],\n    \"random_state\": [0],\n}  # Number of combinations: 2*3*2*2*3 = 72\n\nsettings = {\n    \"K-nearest neighbors\": (KNeighborsClassifier(), knn_parameters),\n    \"Multilayer perceptron\": (MLPClassifier(), mlp_parameters),\n    \"Random forest\": (RandomForestClassifier(), rf_parameters),\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:37:34.465693Z","iopub.execute_input":"2021-06-02T20:37:34.466115Z","iopub.status.idle":"2021-06-02T20:37:34.482582Z","shell.execute_reply.started":"2021-06-02T20:37:34.466076Z","shell.execute_reply":"2021-06-02T20:37:34.481395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are ready to perform the grid search. For each algorithm, we collect the best F1-scores and corresponding hyperparameters. The results are as follows:","metadata":{}},{"cell_type":"code","source":"search_results = []\nfor name, grid in settings.items():\n    algorithm = grid[0]\n    parameters = grid[1]\n    start_time = time.time()\n    results = grid_search(algorithm, parameters, X_train, y_train)\n    elapsed_time = round(time.time() - start_time, 2)\n    best_f1_score = results[\"F1-score\"]\n    best_parameters = results[\"Parameters\"]\n    search_results.append((name, best_f1_score, best_parameters))\n    print(\"\\n{}\\nDuration: \\t\\t{} seconds\\nBest F1-score: \\t\\t{}\\nHyperparameters: \\t{}\".format(name.upper(), elapsed_time, best_f1_score, best_parameters))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:37:34.484081Z","iopub.execute_input":"2021-06-02T20:37:34.484558Z","iopub.status.idle":"2021-06-02T20:50:57.823569Z","shell.execute_reply.started":"2021-06-02T20:37:34.484516Z","shell.execute_reply":"2021-06-02T20:50:57.822512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To select the most suitable algorithm, let us sort them by their F1-scores:","metadata":{}},{"cell_type":"code","source":"sorted_search_results = sorted(search_results, key=lambda x: x[1], reverse=True)\npd.DataFrame(sorted_search_results, columns=[\"Model\", \"F1-score\", \"Hyperparameters\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:50:57.825171Z","iopub.execute_input":"2021-06-02T20:50:57.825533Z","iopub.status.idle":"2021-06-02T20:50:57.841961Z","shell.execute_reply.started":"2021-06-02T20:50:57.8255Z","shell.execute_reply":"2021-06-02T20:50:57.840824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All three models perform well during cross validation based on the F1-score. However, from this improved analysis we learn that the RF model with the given hyperparameters is most suitable for our use case. Note that the F1-score is slightly lower than before. The reason for this is that we enforce pruning, which is controlled by the parameter *ccp_alpha*. ","metadata":{}},{"cell_type":"markdown","source":"### Conclusions","metadata":{}},{"cell_type":"markdown","source":"In this final section we will look at some characteristics of our final RF model. Furthermore, we will see how it performs on our test set.","metadata":{}},{"cell_type":"markdown","source":"#### Model characteristics","metadata":{}},{"cell_type":"markdown","source":"A nice thing about RFs is that we can easily infer what the relative importance is of the different features regarding stroke predictions. Studying the features this way also provides a good check with respect to the exploratory analysis we performed earlier.","metadata":{}},{"cell_type":"code","source":"best_rf_params = sorted_search_results[0][2]\nrandom_forest = RandomForestClassifier(**best_rf_params)\nrandom_forest.fit(X_train, y_train)\n\nweights = list(random_forest.feature_importances_.round(3))\nfeature_names = list(X.columns)\nrelevancies = list(zip(feature_names, weights))\nsorted_relevancies = sorted(relevancies, key=lambda x: x[1], reverse=True)\npd.DataFrame(sorted_relevancies, columns=[\"Feature\", \"Relative importance\"])","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:50:57.844289Z","iopub.execute_input":"2021-06-02T20:50:57.844651Z","iopub.status.idle":"2021-06-02T20:50:59.428189Z","shell.execute_reply.started":"2021-06-02T20:50:57.844605Z","shell.execute_reply":"2021-06-02T20:50:59.427303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this overview, we conclude that *age*, *avg_glucose_level*, and *bmi* are the most important features to predict whether a patient is susceptible to a stroke. These results are fairly consistent with our exploratory analysis, with the possible exception of the bottom three features, *ever_married*, *heart_disease*, and *hypertension*. Earlier, those features showed significantly different distributions for stroke vs. no stroke. Apparently they do not play a very important role in our final RF model.\n\nRecall that an RF model is build up from an ensemble of DTs. To get a better feel for our model, let us visualize a random tree:","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(dpi=2048)\ntree.plot_tree(\n    random_forest.estimators_[0],\n    feature_names=feature_names, \n    class_names=[\"no stroke\", \"stroke\"],\n    filled=True,\n)\nfig.savefig('rf_example_tree.png')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:50:59.429496Z","iopub.execute_input":"2021-06-02T20:50:59.429794Z","iopub.status.idle":"2021-06-02T20:51:45.567823Z","shell.execute_reply.started":"2021-06-02T20:50:59.429765Z","shell.execute_reply":"2021-06-02T20:51:45.565834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each node (except leaves) splits into two nodes. The left/right node satisties/dissatisfies the condition prescribed by its parent node. The colors provide information about the class label. The bluer the node, the stronger it predicts a stroke. Likewise, the redder the node, the higher the probability that it corresponds to no stroke.","metadata":{}},{"cell_type":"markdown","source":"#### Test results","metadata":{}},{"cell_type":"markdown","source":"We have optimized our model by performing grid searches and cross validation. Now that we have selected our final model, we need to test its performance on the test set that we defined earlier. One important difference with cross validation is that in contrast to the training set, the test set has not been oversampled. This means that the test set reflects the true, imbalanced situation where strokes are rare.","metadata":{}},{"cell_type":"code","source":"y_pred = random_forest.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = round(accuracy_score(y_test, y_pred), 3)\nprecision = round(precision_score(y_test, y_pred), 3)\nrecall = round(recall_score(y_test, y_pred), 3)\nf1 = round(f1_score(y_test, y_pred), 3)    \nprint(\"Confusion matrix: \\n\\n{}\\n\\nAccuracy: \\t{}\\nPrecision: \\t{}\\nRecall: \\t{}\\nF1-score: \\t{}\".format(conf_matrix, accuracy, precision, recall, f1))","metadata":{"execution":{"iopub.status.busy":"2021-06-02T20:51:45.569497Z","iopub.execute_input":"2021-06-02T20:51:45.56998Z","iopub.status.idle":"2021-06-02T20:51:45.612288Z","shell.execute_reply.started":"2021-06-02T20:51:45.569928Z","shell.execute_reply":"2021-06-02T20:51:45.611325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From left to right, the first row of the confusion matrix contains the True Negatives (TNs) and the False Positives (FPs). The second row, again from left to right, contains the False Negatives (FNs) and the True Positives (TPs). As the diagonal contains the correct predictions, our goal has been to make this matrix as diagonal as possible. A high precision means that the FPs are suppressed, while a high recall corresponds to a low number of FNs. Ideally, both of these metrics are large. Both quantities are combined in the F1-score, which is the harmonic mean of precision and recall. For this reason, the F1-score is probably the most suitable metric to assess our model performance.","metadata":{}},{"cell_type":"markdown","source":"#### Summary and discussion","metadata":{}},{"cell_type":"markdown","source":"In this notebook we have considered various machine learning algorithms to predict strokes in patients. Before modeling, we performed some exploratory data analyses (mainly visual) and converted the categorical data into numeric data using either binary encoding or one-hot encoding. The missing BMI values were imputed from a linear regression model that was trained on the remaining data. To tackle the issue of significantly imbalanced class labels, we appealed to the SMOTE algorithm to oversample the stroke labels, creating a more balanced training set. Subsequently, we standardized all features, i.e. we enforced a zero mean and unit variance.\n\nAfter having fully preprocessed the data, we applied ten-fold cross validation to seven different machine learning algorithms using the Scikit-learn default settings. We selected the top three algorithms based on the F1-score. For these three algorithms we then performed a grid search to find optimal sets of hyperparameters. A random forest model came out best from cross validation. Perhaps surprisingly, only a marginal amount of pruning (controlled by the *ccp_alpha* parameter) was applied.\n\nFinally, we investigated the obtained random forest model in more detail. In particular, we looked at the relative importance of the different features and we visualized a decision tree. To assess the performance of this model in a more realistic setting, we had it predict on a hold-out test set, without oversampling. Unfortunately, the F1-score came out relatively low, much lower than during cross validation. Although it is not shown explicitly in this notebook, varying the oversampling factor would not resolve this issue. As it turned out, no oversampling at all would make the results much worse. Of course, one could again search for better hyperparameters to improve the results, but then one is essentially fitting to the test set, which is not a preferred workflow.","metadata":{}},{"cell_type":"markdown","source":"Any feedback on this notebook is more than welcome! I have also shared this project on my [GitHub](https://github.com/tvdaal). Finally, a big thank you to [fedesoriano](https://www.kaggle.com/fedesoriano) for sharing this interesting dataset.","metadata":{}}]}