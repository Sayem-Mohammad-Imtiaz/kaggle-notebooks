{"cells":[{"metadata":{"_cell_guid":"c989e1fa-d458-4bad-9681-58a3e47a643e","_uuid":"1aea1cab5f353ef985abc4a060140ea035dee747","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom IPython.display import display \nimport matplotlib.pyplot as plt\n%matplotlib inline\ndata=pd.read_csv('../input/train.csv')    ","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"0d931082-ec03-46ad-af2d-1719cc0a13a8","_uuid":"dcf245cddf4965e204774ced8d89ac431ea82895","trusted":true},"cell_type":"code","source":"display(data.head())","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"66bb4d9f-cfa5-4792-a912-da5b063afd59","_uuid":"5960005bb3551a3287b21748e32babd212795ab2","trusted":true,"collapsed":true},"cell_type":"code","source":"                                    ##### DATA PREPROCESSING #####\n# Check if columns have NA entries #####\ncount=dict() # Keys are feature names, and items are missing entries counts.\ndef missing_entries(data):\n    for ii in data.columns:\n     count[ii]=np.sum(data[ii].isnull())\n     miss_entr=np.sum(data[ii].isnull())\n     print('The feature {} has {} missing entries.'.format(ii,miss_entr))\n    return count\n","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"73ae531f-8468-4037-8a73-5b6123d8e91b","_uuid":"51886c89d6e07999380d5a5949dcd58a2f7fb1bc","trusted":true},"cell_type":"code","source":"count=missing_entries(data)\n    ","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"7dd6813d-5736-4457-9e98-cf9ff899257a","_uuid":"80ffe8f2879569c6be47e50d16a97142bc42bbdf","trusted":true,"collapsed":true},"cell_type":"code","source":"######## Extracting designation of each passenger #####\ndef extract_desig(input_data):\n    import re\n    designation=list()\n    for ii in (input_data['Name']):\n        t=re.findall(r\",(.*)\\.\",ii)[0]\n        designation.append(t)\n    \n    designation=pd.DataFrame(designation,columns=['Designation'])  \n    df1=input_data.copy()\n    df1=df1.drop(['Name','Ticket','Cabin','PassengerId'],axis=1)\n    df1=pd.concat([df1,designation],axis=1)\n    df1.head()\n    return df1","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"fa13b899-f223-4fb0-a3a9-9c197b505cca","_uuid":"e4468b03b4cfe0336f1fbe525b43e4842a5fabbe","trusted":true,"collapsed":true},"cell_type":"code","source":"############# Replacing missing Ages with it's corrosponding designation's mean\ndef Age_meanimpute(df2):\n    \n    Age_ind=df2['Age'].isnull()\n    df=df2.copy()\n    temp=pd.DataFrame(df2.groupby(['Designation'])['Age'].mean())\n    df=df[Age_ind]\n    for ii in df.index.tolist(): \n        for jj in temp.index.tolist(): \n            if df.loc[ii,'Designation']==jj: \n                df.loc[ii,'Age']=temp.loc[jj,'Age']\n    df2[Age_ind]=df\n    return(df2)    ","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"ec19ed6d-6d50-4a56-bd12-78847eeec5fb","_uuid":"48cc9fcf304508fbd2e20433aa26dcfa932975e9","trusted":true,"collapsed":true},"cell_type":"code","source":"###### Replacing the 2 Embarked NAN positions with most frequently embarked port S\ndef Embark_impute(df3):\n    df3['Embarked'].describe()\n    Emb_ind=df3['Embarked'].isnull()\n    df3.loc[Emb_ind,'Embarked']='S'\n    return df3","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"67e03d79-b3a4-48a9-9fbf-a98617530219","collapsed":true,"_uuid":"57b182ade2a547cece45637d3e55c42cae84fd38","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"94dded54-2237-48e7-9e83-709a402567a4","_uuid":"a838feef1b88e473e4b0a6dfdf5cc47a4acffced","trusted":true,"collapsed":true},"cell_type":"code","source":"########### Designation into 5 different buckets #########\n'''\nThere are a total of 19 designations, we divide them into 5 buckets based on the age range \nplot 'plt.hist(data_cpy.groupby(['Designation'])['Age'].mean(),bins=5)'\n\n'''\ndef desig_bin_divide(df4):\n\n\n    def desig_bucket(df2,input_arr): \n        desig_ind=df['Designation'].isin(input_arr)\n        df2.loc[desig_ind,'Designation']=input_arr[0]\n        #print(input_arr[0])\n        return df2\n    df=df4.copy()\n\n    arr=dict()\n    arr={0:[' Master'],\n         1:[' Ms',' Mr',' Mme',' Mlle',' Miss'],\n         2:[' Don',' Dr',' Jonkheer',' Rev',' the Countess',' Mrs'],\n         3:[' Lady',' Major',' Mrs. Martin (Elizabeth L',' Sir'],\n         4:[' Col',' Capt']\n         }\n    for ii in arr.keys():\n        df4=desig_bucket(df4,arr[ii])\n    df4['Designation'].unique() \n    return df4","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"152599d6-35e3-4105-81ce-9865a6477dca","_uuid":"33fa5b8419eb11535e879e25bfbb8229fe82683d","trusted":true,"collapsed":true},"cell_type":"code","source":"data_cpy=extract_desig(data)\nAge_groupedby_desig=pd.DataFrame(data_cpy.groupby(['Designation'])['Age'].mean())\ndata_cpy=Age_meanimpute(data_cpy)\ndata_cpy=Embark_impute(data_cpy)\ndata_cpy=desig_bin_divide(data_cpy)","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"48c22cba-90c6-46c6-a941-ff56cd18c0f6","_uuid":"824d61cf73ac97e1afc4de4ab24a681b349b4398","trusted":true},"cell_type":"code","source":"'''\nWe are converting designation feature with 18 entries to a categorical variable of 5 entries based on the age feature. \nA dependency can be assumed between Designation and Age features. \nDesignation buckets w.r.t Age are given in desig_bin_divide function.\n'''\nplt.hist(Age_groupedby_desig['Age'],bins=5)\nplt.xlabel('Mean Age')\nplt.title('Mean Age grouped by designation')\nplt.ylabel('Frequency')\nplt.show()","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"d6a861da-3923-4869-80e5-46f0e3918549","_uuid":"8c12a9a596e899bc99354fabc3b13752d089ded7","trusted":true,"collapsed":true},"cell_type":"code","source":"#### Function for splitting data, log transforming skewed features, normalizing features with extreme values\ndef split_data(df1,output_var,frac,norm_feat,log_feat):\n    target=df1[output_var]\n    df1=df1.drop(output_var,axis=1)\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test=train_test_split(df1,target,test_size=frac,random_state=0)\n    from sklearn.preprocessing import MinMaxScaler \n    scaler=MinMaxScaler()\n    ##### Log transform of train and test data\n    df1[log_feat]=df1[log_feat].apply(lambda x:np.log(x + 1))\n    X_train[log_feat]=X_train[log_feat].apply(lambda x:np.log(x + 1)) # Apply log transform to skewed data, show it is skewed\n    X_test[log_feat]=X_test[log_feat].apply(lambda x:np.log(x + 1))\n    ##### Normalization of train and test data\n    df1[norm_feat]=scaler.fit_transform(df1[norm_feat])\n    X_train[norm_feat]=scaler.fit_transform(X_train[norm_feat])\n    X_test[norm_feat]=scaler.fit_transform(X_test[norm_feat])\n\n    return df1,X_train,X_test,y_train,y_test\n    ","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"c164a915-47d2-4fd1-8808-65410200434f","_uuid":"394bf2798b561edce38e498001f532a96a6365fe","trusted":true},"cell_type":"code","source":"'''\nSome continous features could have skewed values, i.e., some of their entries could be completely in the extremes\n'''\nplt.figure(1)\nplt.subplot(211)\nplt.hist(data_cpy['Fare'])\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.title('Initial Distribution')\nplt.figure(2)\nplt.subplot(212)\nplt.hist(data_cpy['Age'])\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Initial Distribution')\nplt.show()","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"9e434286-27db-4a03-bdbe-26f56ff18c24","_uuid":"ad66c530e555240976b3714aeb7c453286a8322b","trusted":true,"collapsed":true},"cell_type":"code","source":"pd.options.mode.chained_assignment = None   #### Command for removing any warning for chaining operations\ndata_cpy=pd.get_dummies(data_cpy)\ndata_cpy.head()\nfeature_tobe_normalized=['Fare','Age']\nfeature_tobe_log=['Fare']\nfinal_data_cpy,features_train,features_test,out_train,out_test=split_data(data_cpy,'Survived',0.2,\n                                                           feature_tobe_normalized,feature_tobe_log)","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"f9417ab3-49ad-41b2-9ae6-9b7586eb5d57","_uuid":"b495140e6ae7acf4734861d9c70fa41b1b48c252","trusted":true},"cell_type":"code","source":"'''\nLog, norm transformations of Fare feature and norm transformation of Age feature\n\n'''\nplt.figure(1)\nplt.subplot(211)\nplt.hist(final_data_cpy['Fare'])\nplt.xlabel('Fare')\nplt.ylabel('Frequency')\nplt.title('Final Distribution of Fare')\nplt.figure(2)\nplt.subplot(212)\nplt.hist(final_data_cpy['Age'])\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Final Distribution of Age')\nplt.show()","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"216e4167-c252-49be-871f-8e8757102228","_uuid":"a45b10bd3f5905c4a4a874b1889e4c27564f893b","trusted":true},"cell_type":"code","source":"'''\nTo get an understanding of any classifier, it is good to calculate the naive predictor performance, \nwhere in predicted values for a person is always 1, i.e., no training is present!\n'''\nnaive_pred=np.ones(len(out_train))\nfrom sklearn.metrics import accuracy_score, fbeta_score,make_scorer\nprint('Naive predictor accuracy is {:.2f}% and f_beta score is {:.2f}.'.format(accuracy_score(out_train,naive_pred)*100,\n                                                                               fbeta_score(out_train,naive_pred,beta=0.5)))\n    \n\n","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"a1e1474a-5f90-441d-97d5-c3067ec8136a","_uuid":"6a33c3bf8fe0d3431a53b57386ea42a091ca8076","trusted":true},"cell_type":"code","source":"from sklearn import linear_model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_validate\n\nclf_A=linear_model.LogisticRegression(random_state=0)\nclf_B=RandomForestClassifier(random_state=0)\nclf_C=SVC(random_state=0)\nclf_D = AdaBoostClassifier(random_state=0)\n\nfor clf in [clf_A,clf_B,clf_C,clf_D]:\n    clf_name = clf.__class__.__name__\n    acc_scorer=make_scorer(accuracy_score)\n    acc_scores = cross_validate(clf, features_train, out_train, scoring=acc_scorer,\n                        cv=5, return_train_score=True)\n    f_scorer=make_scorer(fbeta_score,beta=0.5)\n    fbeta_scores = cross_validate(clf, features_train, out_train, scoring=f_scorer,\n                        cv=5, return_train_score=True)\n    print('For classifier:{}, CV_accuracy:{:.9f}, f_beta:{:.2f}'.format(clf_name,\n                                                                        np.mean(acc_scores['test_score']),\n                                                                        np.mean(fbeta_scores['test_score'])\n                                                                       ))\n","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"18bc2c65-ae8a-4d28-a7d6-cbeb8327e3ac","_uuid":"c2f49713a2ff674a934e6f39546f7a5e4245efdb","trusted":true},"cell_type":"code","source":"######### Using SVC  as the final optimizer #########\ndef train_pred(input_features,output_features):\n    from sklearn import grid_search \n    from sklearn.metrics import accuracy_score\n    from sklearn.svm import SVC\n    clf=SVC(random_state=6)\n    parameters={'kernel':['linear','rbf'],\n                'C':[1e-2, 1, 1e2],\n                'gamma':[1e-1, 1, 1e1]\n               }\n    scorer=make_scorer(accuracy_score)\n    grid_obj=grid_search.GridSearchCV(estimator=clf,param_grid=parameters,scoring=scorer)\n    \n    grid_fit=grid_obj.fit(input_features,output_features)\n    best_clf = grid_fit.best_estimator_\n    \n    return clf,best_clf\n\n    ##### Comparing unoptimized model to optimzied model \nclf_train,best_clf_train=train_pred(features_train,out_train)\nunoptimized_pred=clf_train.fit(features_train,out_train).predict(features_test)\noptimized_pred=best_clf_train.predict(features_test)\n\nprint('accuracy score for unoptimized model:{:.8f}'.format(accuracy_score(out_test,unoptimized_pred)))\nprint('accuracy score for optimized model:{:.8f}'.format(accuracy_score(out_test,optimized_pred)))\n    ","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"9973f349-dbaf-4a68-bbf2-5142f02e02ec","_uuid":"3e8cc92ac18a079d5c0a861ddec46ffb7ea0e052","trusted":true,"collapsed":true},"cell_type":"code","source":"########### Importing actual test data #########\ntest_data=pd.read_csv('../input/test.csv') \ntest_data.head()\npassenger_id=test_data['PassengerId']","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"98d8cbcc-571f-418b-88e4-dfe9470233e7","_uuid":"08f4ad713af607b8c069a0c4329ed8a49812b653","trusted":true,"collapsed":true},"cell_type":"code","source":"test_df=extract_desig(test_data)\ntest_df=Age_meanimpute(test_df)\ntest_df=Embark_impute(test_df)\ntest_df=desig_bin_divide(test_df)\n'''\ntwo observations in the test test have NAN's imputed manually\n'''\ntest_df.loc[test_df['Fare'].isnull(),'Fare']=np.min(test_df['Fare'])\ntest_df.loc[test_df['Age'].isnull(),'Age']=28.0 ### Imputing the mean Age for designation 'Ms'\nfrom sklearn.preprocessing import MinMaxScaler \nscaler=MinMaxScaler()\nnorm_feats=['Fare','Age']\nlog_feats=['Fare']\n##### Log transform of train and test data\ntest_df[log_feats]=test_df[log_feats].apply(lambda x:np.log(x + 1)) # Apply log transform to skewed data, show it is skewed\n##### Normalization of train and test data\ntest_df[norm_feats]=scaler.fit_transform(test_df[norm_feats])\n\ntest_df=pd.get_dummies(test_df) \n#### Training over the entire data set\nfinal_feat_train=final_data_cpy\nfinal_out_train=data_cpy['Survived']","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"0906117e-3028-40b1-9249-500ecca84ffc","_uuid":"70e14c0d17e0e9cc4cfd9f869c137a368e0320ff","trusted":true,"collapsed":true},"cell_type":"code","source":"test_pred=best_clf_train.fit(final_feat_train,final_out_train).predict(test_df)","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"5b02f013-9396-41a8-a045-9e24af94c34a","_uuid":"dc8a41de1908f08dfa0a32619785cc090f229e8c","trusted":true},"cell_type":"code","source":"best_clf_train","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"d844a6e9-f768-4ac3-9213-ff2d7210a090","_uuid":"400bb9e82a0c443250aa3241bb7af7f1aa598ea1","trusted":true,"collapsed":true},"cell_type":"code","source":"final_df=pd.DataFrame(pd.concat([passenger_id,pd.Series(test_pred)],axis=1))\nfinal_df.to_csv('final_submission.csv', sep=',', encoding='utf-8')","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"25cc9d0e-5617-4a10-93bc-46f9df39b59e","_uuid":"656c1c024ceefa408ee2092c0585a98ca73b57ae","trusted":true},"cell_type":"code","source":"'''\nThe final survival rate accuracy on the unseen test data from Kaggle is 79.425% using the above feature transformations,\nmodel selection and model optimization using hyper parameters.\n'''","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"70e80a41-5837-4f89-aab2-e4686ddea21a","collapsed":true,"_uuid":"e1ce90cd835c5a4170c6f02bd2974f1c45e3b766","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}