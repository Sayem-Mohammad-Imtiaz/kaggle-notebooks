{"cells":[{"metadata":{"_uuid":"40a32f23fe2a44602b26baa43e1dcf1a3983e093"},"cell_type":"markdown","source":"## Importing all the libraries that we will need"},{"metadata":{"_uuid":"4f7fc23b599b5352a709ec8ffbd4f4b3c2f7fd87","trusted":true,"collapsed":true},"cell_type":"code","source":"import warnings\nimport gc\nwarnings.filterwarnings(\"ignore\")\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 15, 10\n#rcParams[]\nimport seaborn as sns\nimport pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"655b7fcaf42cd609d0ee2a731ab9defdd7cf8d55"},"cell_type":"code","source":"rcParams['figure.figsize'] = 15, 10","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fca1d9ee44bbf522ff27f74793ceab5f0c232b97","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nimport statsmodels.stats.api as ssm\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0de94da27e5b678cefa4184e0141105e880f645e","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fe203b4ad519e426034e10542a5019bdd446fd2","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV, LinearRegression, SGDRegressor\nfrom sklearn.metrics import mean_squared_error, log_loss, r2_score\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9078fa2a191668479bf0a591a7e732facce18159"},"cell_type":"markdown","source":"## Загрузка данных"},{"metadata":{"_uuid":"27fc569a9e1966e6d5ef5b82575e78fd93062a68"},"cell_type":"markdown","source":"Загружаю данные.\nSales - таблица всех продаж с детализацией по Дате, магазину и департаменту.\nFeatures - много разных признаков с детализацией по дням\nStores - данные магазинов\nЗаодно сразу смотрю, сколько пропущенных значений в таблицах"},{"metadata":{"_uuid":"fcccb563a7138db55d25b060b603b2f5db776d3e","trusted":true},"cell_type":"code","source":"features = pd.read_csv('../input/Features data set.csv', parse_dates=['Date'], dayfirst=True)\n\nsales = pd.read_csv('../input/sales data-set.csv', parse_dates=['Date'], dayfirst=True)\n\nstores = pd.read_csv('../input/stores data-set.csv') #(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f528150ee42cf8dd17f3cb024371946be037853b","trusted":true},"cell_type":"code","source":"features.shape, sales.shape, stores.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef172f255974708dc026faa435de465cf5512e02","scrolled":true,"trusted":true},"cell_type":"code","source":"print('Количество уникальных записей по Store и Date: {}'.format(len(sales.groupby(['Date', 'Store']).groups)))\n\nsales.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c531eb211544d6953efb166c1ee0af73dc0ea50d","trusted":true},"cell_type":"code","source":"\nprint('Количество уникальных записей по Store и Date: {}'.format(len(features.groupby(['Date', 'Store']).groups)))\n\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30d7e1b14025bf377a66fb6886d06a929284bc07","trusted":true},"cell_type":"code","source":"features.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fac6197a85af3df1d2c64aa904c9594eea44d656","trusted":true},"cell_type":"code","source":"stores.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01437218171a7e9aea059e57083722ef7d0089f9"},"cell_type":"markdown","source":"## Missing values\n    Надо избавиться от пропущенных значений.\n    В данных отсутствуют строки, соответствующие нулевым значениям продаж. \n    \n    Кроме того, в описании к датасету указано, что столбцы Markdown1-5 были добавлены в ноябре 2011 года, все более ранние значения этих колонок не заполнены. Но эти столбцы я решила пока не заполнять.\n\n    Я сделала итератор по всем значениям Даты, Магазина и Департамента, аналогичный функции product из библиотеки itertools, в последней версии которой почему-то этой функции нет D=\n    После этого объединяю все таблицы между собой"},{"metadata":{"_uuid":"9a78261870c2ba88eb613740747d8c061e77d3a8","trusted":true,"collapsed":true},"cell_type":"code","source":"arr =[]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f74f8412888fc3f089f96816509c0ee37f6fcb7","trusted":true,"collapsed":true},"cell_type":"code","source":"for d in sales.Date.unique():\n    for dept in sales.Dept.unique():\n        for s in sales.Store.unique():\n            arr.append([d, dept, s])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb5f2c01ee87c4ab8ca182dd6b1f7b3de1e093ca","trusted":true,"collapsed":true},"cell_type":"code","source":"all_values = pd.DataFrame.from_records(arr, columns=['Date', 'Dept', 'Store'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72ee9390e196b29deaa8eafdd7413d9947a28e3d","trusted":true,"collapsed":true},"cell_type":"code","source":"whole_sales = pd.merge(all_values, sales.drop('IsHoliday', axis=1), on=['Date', 'Dept', 'Store'], how = 'left')\nwhole_sales.Weekly_Sales.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2acadb4ebe2914e6ecb2ad5cca43c8a8e339ad96","trusted":true},"cell_type":"code","source":"whole_sales.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1a33c172e7fc2c1fa9a75d3653ff15be31d9fb2","trusted":true},"cell_type":"code","source":"whole_sales.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7211662a815b91cfd936e39fece6f608d3dc0209"},"cell_type":"markdown","source":"## Generating Feature"},{"metadata":{"_uuid":"687033ea3c18c60d0ae60743f318d3862af4e036","trusted":true,"collapsed":true},"cell_type":"code","source":"def create_new_features(whole_sales, features, stores):\n    \n    datas = pd.to_datetime(pd.unique(sales.Date), format=\"%d/%m/%Y\")\n    \n    date_dict = {k:v for k,v in zip(datas, datas.week)}\n\n    whole_sales['week'] = whole_sales.Date.apply(lambda x: date_dict[x])\n\n    whole_sales['year'] = whole_sales.Date.apply(lambda x: x.year)\n\n    whole_sales['month'] = whole_sales.Date.apply(lambda x: x.month)\n\n    whole_sales['day'] = whole_sales.Date.apply(lambda x: x.day)\n    \n    # Флаги пред- и постпраздничных недель\n    df_of_holidays = sales.groupby(['Date', 'IsHoliday']).Store.nunique().reset_index()[['Date', 'IsHoliday']]\n\n    df_of_holidays['post_holiday'] = df_of_holidays.IsHoliday.shift(1)\n    df_of_holidays['pred_holiday'] = df_of_holidays.IsHoliday.shift(-1)\n    df_of_holidays = df_of_holidays.fillna(False)\n\n    whole_sales = pd.merge(whole_sales, df_of_holidays.drop('IsHoliday', axis=1), on='Date')\n    whole_data_with_markdown = whole_sales.merge(features, on=['Date', 'Store'])\n    whole_data_with_markdown = whole_data_with_markdown.merge(stores, on=['Store'])\n    \n    #закодируем булевы переменные\n    encoder = LabelEncoder()\n    \n    whole_data_with_markdown['Type'] = encoder.fit_transform(whole_data_with_markdown['Type'])\n\n    whole_data_with_markdown['IsHoliday'] = encoder.fit_transform(whole_data_with_markdown['IsHoliday'])\n    whole_data_with_markdown['post_holiday'] = encoder.fit_transform(whole_data_with_markdown['IsHoliday'])\n    whole_data_with_markdown['pred_holiday'] = encoder.fit_transform(whole_data_with_markdown['IsHoliday'])\n    \n    return whole_data_with_markdown","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d48cda27518d40e16c577441b10121aef85a764d","trusted":true,"collapsed":true},"cell_type":"code","source":"whole_data_with_markdown = create_new_features(whole_sales, features, stores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4057e026136ccdf0f4cbc6379a7327b405a7036","trusted":true},"cell_type":"code","source":"whole_data_with_markdown.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e24e37ac307b48b0b13fb09df441c927c5dc70bf"},"cell_type":"code","source":"whole_data_with_markdown.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"193c4c7593d49a3e6dae6ccb79dc0e0b41027ec6"},"cell_type":"markdown","source":"## Предварительный анализ"},{"metadata":{"_uuid":"22f8ac6c80b15a66160987b84f7c45797209fc0b"},"cell_type":"markdown","source":"### Средние продажи за день по всем департаментам всех мазагинов.\nВидно, что продажи имеют очень яркие пики в преддверии января (рождество и новый год)"},{"metadata":{"trusted":true,"_uuid":"dca376904c8be98db7ecc72fa7a9f490f018d3c8"},"cell_type":"code","source":"fig, ax = plt.subplots()\nwhole_data_with_markdown.groupby('Date').Weekly_Sales.mean().plot(x='Date', y='Weekly_Sales', ax=ax);\nax.xticks = whole_data_with_markdown.groupby('Date')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b33136128503ad9a76d5aedd90ffc6a9d9ae39bc"},"cell_type":"markdown","source":"### Объемы продаж за все время с разбивкой по магазинам и департаментам\nДанные очень шумные. Нет однозначных магазинов/департаментов, у которых все значения были бы максимальными/минимальными"},{"metadata":{"trusted":true,"_uuid":"a456f2f1c051f2e1793dcf066492b91f2f67cc57"},"cell_type":"code","source":"df_stores_depts = whole_data_with_markdown.groupby(['Store', 'Dept']).sum().reset_index()\n\ndf_stores_depts = df_stores_depts.pivot(index='Store', columns='Dept', values='Weekly_Sales')\n\n\nax = sns.heatmap(df_stores_depts.apply(lambda col: (col-min(col))/(max(col)-min(col)), axis=0), cbar_kws={'label': 'Normalized Sale'})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44f8a7b141efbb20a0236232d401da71e75e05a3"},"cell_type":"markdown","source":"### Анализ данных Markdown\nЯ хочу проанализировать данные на предмет того, влияют ли значения столбцов Markdown на объем продаж.\n\nИз данных видно, что значения колонок постоянны у каждого магазина в течение дня."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7bf4682ac64cda4809a4b8ee875ee8c23febe1d7"},"cell_type":"code","source":"whole_data_with_markdown_new = whole_data_with_markdown[(whole_data_with_markdown.Date >= pd.datetime(2011,11,5))]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"489526c5a05466d74ecdbb689847b97125406de5","trusted":true,"collapsed":true},"cell_type":"code","source":"markdown_cols = [x for x in features.columns if 'MarkDown' in x]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"60aafe76dd4b52a8d264526e12526fd19bedbe11"},"cell_type":"code","source":"whole_data_with_markdown_new['markdown_sum'] = whole_data_with_markdown[markdown_cols].sum(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fc960233d78d856230a9e11400af5826fd4976e"},"cell_type":"code","source":"whole_data_with_markdown_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9a4afb93dfd346ee6b65342a45ddf3154314b38"},"cell_type":"code","source":"whole_data_with_markdown_new.groupby(['Date', 'Store']).markdown_sum.nunique().reset_index().groupby('markdown_sum').Store.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"36bf9f73ec31fe2f1b40f88ea78de0e3f076f060"},"cell_type":"code","source":"average_promo_sales = whole_data_with_markdown_new.groupby(['Date'])['Weekly_Sales','markdown_sum'].mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb6ad9a401ad94e21ab5e38e6bc3b91e02fc2722"},"cell_type":"code","source":"fig, (axis1, axis2) = plt.subplots(2,1)\nax1 = average_promo_sales.plot(y='Weekly_Sales', x='Date', legend=True,ax=axis1,marker='o',title=\"Average Sales\")\nax2 = average_promo_sales.plot(y='markdown_sum', x='Date', legend=True,ax=axis2,marker='o',rot=90,colormap=\"summer\",title=\"Average Promo\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d0c63b4539b00358432d098ad3e7657a8d9a16e"},"cell_type":"markdown","source":"### Постороение гистограммы значений целевой переменной\n\nРаспределение очень отличается от нормального, поэтому я решила логарифмировать значения продаж"},{"metadata":{"trusted":true,"_uuid":"9739a5fcb7c98bb1ed98a4b553c2effd98dc160d"},"cell_type":"code","source":"fig, axis = plt.subplots(1,2)\n\nwhole_data_with_markdown.Weekly_Sales.hist(bins=20, ax=axis[0])\n\nwhole_data_with_markdown.Weekly_Sales.apply(lambda x: 0 if x<=0 else np.log(x)).hist(bins=20, ax=axis[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"51633607bd127dbae90d64dcb8ed553229d84af1"},"cell_type":"code","source":"whole_data_with_markdown.Weekly_Sales = whole_data_with_markdown.Weekly_Sales.apply(lambda x: 0 if x==0 else np.log(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"232cfc00be21b34cbd86274852bd66f68682186b"},"cell_type":"code","source":"whole_data_with_markdown.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"846571b081817a9342fd40903831ea9411bdcc42"},"cell_type":"markdown","source":"### Матрица корреляции признаков"},{"metadata":{"trusted":true,"_uuid":"75c503d4e56d3aa6ab89f2ad57f028386f57823d"},"cell_type":"code","source":"corrmat = whole_data_with_markdown.corr()\nsns.heatmap(corrmat, vmax=.8, square=True, annot=True);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"229909831c6b3e05f9bf0e3704694ac4f62064e4"},"cell_type":"markdown","source":"# Моделирование\nПосле визуального анализа стало понятно, что данные имеют очень четкую структуру во времени, и, видимо, сильно автокоррелированы.\n\nЯ предприняла попытку использовать прогнозную модель ARIMA для решения поставленной задачи. Я построила модель для всех продаж всех магазинов, без разбивки по магазинам и департаментам, а также для одного из магазинов. \nОказалось, что динамика продаж отличается у разных магазинов (и, вероятно, у их департаментов),\nи вместо того, чтобы настраивать для каждого из них модель вручную, было решено использовать регрессионный подход к прогнозированию.\n\nЯ ориентировалась на две метрики - r2 и rmse.\nRMSE легко минимизировать, но она слишком штрафует за большие отклонения от правильного ответа.\nИз-за того, что наши данные имеют много сильных (и очень сильных) выбросов, она не очень подходит..\n\nR2 может быть интерпретируема как доля дисперсии, объясненной используемой моделью.\n\nЯ попробовала использовать для обучения два набора данных:\n\n1) данные за весь период, удалив оттуда столбцы Markdown1-5\n\n2) данные, начиная с ноября 2011 года, не удаляя эти столбцы\n"},{"metadata":{"_uuid":"1326adb1532490924df577e4d8bd8542104a66cd"},"cell_type":"markdown","source":"## Регрессионный подход к прогнозированию"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a8d983d2ce031e4dd1bbd5bec7411e2111de109b"},"cell_type":"code","source":"whole_data = whole_data_with_markdown.drop(markdown_cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb63a7abc20385b4d1e1058017d5aa662a77e1c4","trusted":true},"cell_type":"code","source":"whole_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b80609912f9133c0011cf4c0ba34fb84ba6d8249"},"cell_type":"code","source":"whole_data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5c535b511f8630911ca3f04e2dbed7b866e3c0e","trusted":true,"collapsed":true},"cell_type":"code","source":"with open('whole_data.pkl', 'wb') as f:\n    pickle.dump(whole_data, f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58955c309113c7f1e7ce004b946acb235e47864a"},"cell_type":"markdown","source":"### Отделяю отложенную выборку"},{"metadata":{"_uuid":"bf289c35df6cdb33dc06e74bad1a32047888963f","trusted":true,"collapsed":true},"cell_type":"code","source":"def train_test_spl(whole_data, test_len=16):\n    whole_data = whole_data.fillna(0)\n    whole_data = whole_data.sort_values(by='Date')\n    \n    unique_date = whole_data.Date.unique()[-test_len:]\n    \n    train = whole_data[~whole_data.Date.isin(unique_date)]\n    test = whole_data[whole_data.Date.isin(unique_date)]\n    \n    return [train, test]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d86b7a386f5bf30d156b2be547845f1eff206ee7","trusted":true,"collapsed":true},"cell_type":"code","source":"train, test = train_test_spl(whole_data, 16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"df90919824ad27a5e21282c3fcbdc74e78c05c0e"},"cell_type":"code","source":"train_m, test_m = train_test_spl(whole_data_with_markdown_new, 16)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"157dbddd996586210559341229a6b4b79e48db2d"},"cell_type":"markdown","source":"## Разбиение датасета на фолды"},{"metadata":{"_uuid":"4303655b9de49bdcd77223934961c6a78cc9ebc0","trusted":true,"collapsed":true},"cell_type":"code","source":"cv_splits = TimeSeriesSplit(n_splits=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fffcdba078c8d55fc4e174a90d56f14c03c337f4","trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82bb48c84b7f66ec18e6c90ea30cda4673864694","trusted":true,"collapsed":true},"cell_type":"code","source":"estimators =[RandomForestRegressor(n_estimators=100, max_features ='sqrt'),\n             KNeighborsRegressor(n_neighbors=6),\n             ExtraTreesRegressor(n_estimators=20, criterion='mse', bootstrap=True, n_jobs=-1, random_state=17)\n            ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07de1c666c98be8bf918365efd90e0eb8b98c573","trusted":true,"collapsed":true},"cell_type":"code","source":"def plot_scores(test, group_cols = 'Date'):\n    \n    for col in group_cols:\n        if col not in test.columns:\n            return 'group columns not exist in test dataframe'\n    \n    ttest = test.groupby(by=group_cols)[['Weekly_Sales', 'predict_y']].mean()\n\n    fig, ax = plt.subplots()\n    ttest.plot(y='Weekly_Sales', ax=ax)\n    ttest.plot(y='predict_y', ax=ax)\n    ax.set_title('Mean squared error: {}. r2-score : {}'.format(mean_squared_error(test.Weekly_Sales, test.predict_y), r2_score(test.Weekly_Sales, test.predict_y)))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c4f73089275cb1e95445e27fa73ebdc0bf3247c9"},"cell_type":"code","source":"def scale_set(x):\n    scaler = StandardScaler()\n    return scaler.fit_transform(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"081abd8b243322d37adf6dd2ec61376421a58476"},"cell_type":"code","source":"def x_y_split(x_y):\n    X = x_y.drop(['Weekly_Sales', 'Date'], axis=1)\n    y = x_y['Weekly_Sales']\n    return [X,y]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"058382430ab838c460e32e0c6f4e269ec69e01cc","trusted":true},"cell_type":"code","source":"scores = pd.DataFrame()\ntmp = {}\nfor m,est in zip(['RandomForestRegressor', 'KNeighborsRegressor', 'ExtraTreesRegressor'], estimators):\n    tmp['Model'] = m\n    for j,i in enumerate([train, train_m]):\n        X_train, y_train = x_y_split(i)\n        cv_scores = cross_val_score(est, scale_set(X_train), y_train, cv=cv_splits, scoring='r2')\n        tmp['R2_Y%s'%str(j+1)] = np.mean(cv_scores)\n    scores = scores.append([tmp])\n    scores.set_index('Model', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e39da3d6b19ce598aa1f623328fadf3e42e606a"},"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(10,4))\nscores.R2_Y1.plot(ax=axes[0], kind='bar', title='R2_Y1')\nscores.R2_Y2.plot(ax=axes[1], kind='bar', color='green', title='R2_Y2')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16d0a72d45f945a45627022a7b59cf6fe88792a3"},"cell_type":"markdown","source":"### Лучше всего с задачей справился ExtraTreesRegressor при обучении на данных без столбцов Markdown"},{"metadata":{"trusted":true,"_uuid":"081f26083bb1d7bd1ad0ff61ba03cebee30c2b01"},"cell_type":"code","source":"etr = ExtraTreesRegressor(n_estimators=20, criterion='mse', bootstrap=True, n_jobs=-1, random_state=17)\nX_train, y_train = x_y_split(train)\nX_test, y_test = x_y_split(test)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\netr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5b1a76c81e6182df2671c368e3e70af6b87227cf"},"cell_type":"code","source":"test['Weekly_Sales'] = test.Weekly_Sales.apply(lambda x: np.e**x)\npred_y = list(map(lambda x: np.e**x, etr.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4404c03146001d0ee3dc8a24a78c4fe9c0502a54"},"cell_type":"code","source":"plot_scores(test.assign(predict_y=pred_y), ['Date', 'Store'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b04efa4f848f9d6db17999c40bac098e8b72b44f"},"cell_type":"code","source":"plot_scores(test.assign(predict_y = pred_y), ['Date', 'Dept'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1809b25e6d3ccb2a95f78d051205170c7ba3b4dd"},"cell_type":"code","source":"\nplot_scores(test.assign(predict_y=pred_y), ['Date'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"376470766501051a730eeac33476ff687b7f7575"},"cell_type":"markdown","source":"Видно, что прогноз улавливает структурy пиков и провалов!\n\nНа графиках с детализацией также видно, что модель хорошо предсказывает будущее."},{"metadata":{"trusted":true,"_uuid":"d17029158f33b020ff9cb635243b86f37e49876f"},"cell_type":"code","source":"plot_scores(test.assign(predict_y=pred_y)[test.Store == 20], ['Date', 'Store'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"140463afca1e2456f97c4e5e12e9c063bd1f2550"},"cell_type":"markdown","source":"### Идеи по улучшению модели:\nПроанализировать влияние признаков на модель, найти комбинацию признаков, которая давала бы наиболее устойчивый результат.\n\nПроанализировать остатки на наличие в них структуры (автокорреляции, смещенности), и в случае нахождения постараться от неё избавиться.\n\nПопробовать какие-нибудь более сложные модели (xgboost, catboost), а также какие-нибудь архитектуры нейронных сетей.\n\nКроме того, можно было бы объединить результаты нескольких моделей при помощи стекинга (например даже тех двух моделей, которые у меня обучены на разных массивах данных)\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}