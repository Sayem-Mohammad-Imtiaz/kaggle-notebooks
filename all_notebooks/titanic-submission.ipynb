{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('../input/titanic/test.csv')\ntrain = pd.read_csv('../input/titanic/train.csv')\ngender_submission = pd.read_csv('../input/titanic/gender_submission.csv')\nprint('Datasets Loaded')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gender_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears a few rows in the Age and Class columns are missing as well as a couple in Embarked. We can see the missing values in the data set using the missingno.matrix() function.","metadata":{}},{"cell_type":"code","source":"missingno.matrix(train, figsize = (30,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"forming bins:","metadata":{}},{"cell_type":"code","source":"df_bin = pd.DataFrame() #for discretized continuous variables\ndf_con = pd.DataFrame() #for continous variables","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"first lets see how many people survived:","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,1))\nsns.countplot(y='Survived', data=train);\nprint(train.Survived.value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"549 to 342, not great odds for our prospective survivors.","metadata":{}},{"cell_type":"code","source":"#we can add this to our subset dataframes\ndf_bin['Survived'] = train['Survived']\ndf_con['Survived'] = train['Survived']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's look at another feature : Pclass\n\nthe ticket class of the passenger","metadata":{}},{"cell_type":"code","source":"#plotting the distribution\nsns.distplot(train.Pclass)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"if we recall from the missingno function, there were no missing values here. So we can go ahead and add them to our sub dataframes.","metadata":{}},{"cell_type":"code","source":"df_bin['Pclass'] = train['Pclass']\ndf_con['Pclass'] = train['Pclass']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, the Name feature is next","metadata":{}},{"cell_type":"code","source":"train.Name.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"seems like now two names are repeating which is good. We could probably clean up the column by removing the title ex: 'Mr.' 'Mrs' 'Master'","metadata":{}},{"cell_type":"markdown","source":"Next we will look at the break down of the Sex feature:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(y='Sex', data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"quite a higher value of males to females, since we know there are no missing values here we can add it to our subset.\n","metadata":{}},{"cell_type":"code","source":"df_bin['Sex'] = train['Sex']\n#because it's a binary value, lets add it as either 1 or 0\ndf_bin['Sex'] = np.where(df_bin['Sex'] == 'female', 1,0)\n\ndf_con['Sex'] = train['Sex']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because 'Sex' and 'Survival' are both binary, we can easily compare the two:","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(10,10))\nsns.distplot(df_bin.loc[df_bin['Survived'] == 1]['Sex'], kde_kws={'label': 'Survived'});\nsns.distplot(df_bin.loc[df_bin['Survived'] == 0]['Sex'], kde_kws={'label': 'Did not survive'});","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"this chart shows 0 for male and 1 for female. with the yellow bar showing deaths and the blue bar showing survivals. Clearly females had a significantly higher survival rate than males.","metadata":{}},{"cell_type":"markdown","source":"Ok we will move on the the Age feature\nwe can recall there were quite a fiew missing values in age. We have the option to either average out the age or cut them out. I've chosen to fill the blanks with the average:","metadata":{}},{"cell_type":"code","source":"#first find the average\ntrain['Age'].mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the mean age is roughly 29.70, we can fill this into the missing cells and add it to our subset","metadata":{}},{"cell_type":"code","source":"train['Age'].fillna(29.70, inplace = True)\ntrain['Age']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bin['Age'] = pd.cut(train['Age'], 10) #this will bucket our bin into different age groups\ndf_con['Age'] = train['Age'] #non-bucketed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On to the SibSp feature, which is a measure of how many siblins/spouses the passenger has abourd the Titanic:","metadata":{}},{"cell_type":"code","source":"train.SibSp.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No missing values so we can add it to our sub dataframe","metadata":{}},{"cell_type":"code","source":"df_bin['SibSp'] = train['SibSp']\ndf_con['SibSp'] = train['SibSp']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#setting up visualization function for ease of use.\ndef plot_count_dist(data, bin_df, label_column, target_column, figsize=(20,5), use_bin_df=False):\n\n    if use_bin_df:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1,2,1)\n        sns.countplot(y=target_column, data=bin_df);\n        plt.subplot(1,2,2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column],\n                    kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column],\n                    kde_kws={\"label\": \"Did not survive\"});\n    else:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1,2,1)\n        sns.countplot(y=target_column, data=bin_df);\n        plt.subplot(1,2,2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column],\n                    kde_kws={\"label\": \"Survived\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column],\n                    kde_kws={\"label\": \"Did not survive\"});","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can now easily visualize our Sibsp feature compared to survivability","metadata":{}},{"cell_type":"code","source":"plot_count_dist(train,\n               bin_df=df_bin,\n               label_column='Survived',\n               target_column='SibSp',\n               figsize=(20,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see on the right graph, if you have only 1 sibling or spouse, your survivability greatly exceeds the fatality rate.","metadata":{}},{"cell_type":"markdown","source":"Next we will look at the feature Parch: which is the number of parents/childeren the passenger has aboard the titanic, this is similar to SibSp so the analysis will be similar.","metadata":{}},{"cell_type":"code","source":"#add to subset\ndf_bin['Parch'] = train['Parch']\ndf_con['Parch'] = train['Parch']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_count_dist(train,\n               bin_df=df_bin,\n               label_column='Survived',\n               target_column='Parch',\n               figsize=(20,10))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that having 1-2 children or parents greatly increases your survivability","metadata":{}},{"cell_type":"markdown","source":"Now we will look at the Ticket feature: the passangers ticket number","metadata":{}},{"cell_type":"code","source":"sns.countplot(y='Ticket', data=train);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"not an ideal depiction, how else can we look at it.","metadata":{}},{"cell_type":"code","source":"#how many kinds of ticket were there?\ntrain.Ticket.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"681 different types with a difficult pattern to determine. There may be a way to reduce this down, but for now it's not going to be usable for much.","metadata":{}},{"cell_type":"markdown","source":"Let's continue to the Fare feature: the price of the ticket.","metadata":{}},{"cell_type":"code","source":"#sns.countplot(y='Fare',data=train);\ntrain.Fare.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fare has 248 different values but since they are quantitative we can still use it in by cutting it into bins.","metadata":{}},{"cell_type":"code","source":"#add to subset in bins\ndf_bin['Fare'] = pd.cut(train['Fare'], bins=5) #discretised 'cut into bins'\ndf_con['Fare'] = train['Fare'] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_bin.Fare.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the values have been catagorized into 5 bins, each incremint is roughly $100. there also appears to be a value of '-50.' an outlier that we may wish to remove.","metadata":{}},{"cell_type":"code","source":"plot_count_dist(data = train,\n               bin_df = df_bin,\n               label_column='Survived',\n               target_column='Fare',\n               figsize=(20,5),\n               use_bin_df=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see having a cheaper ticket greatly reduced your chances of survival.","metadata":{}},{"cell_type":"markdown","source":"feature Cabin: the passengers Cabin number\nthis feature had quite a significant amount of missing values, since this is an innitial EDA we aren't going to try and use it now and move on.","metadata":{}},{"cell_type":"markdown","source":"Feature: Embarked\n    the port where the passenger boarded the titanic\n    key: C= Cherbourg, Q = Queenstown, S = Southampton\n    \n  this feature had a couple missing values but is overall fine to use.","metadata":{}},{"cell_type":"code","source":"sns.countplot(y='Embarked', data=train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Southhampton was clearly the most common embarking point for passangers","metadata":{}},{"cell_type":"markdown","source":"As for the 2 missing values, we can probably assume they are from Southampton. However as 2 values will likely not skew our predictions much we will just remove them.","metadata":{}},{"cell_type":"code","source":"#add to subset\ndf_bin['Embarked'] = train['Embarked']\ndf_con['Embarked'] = train['Embarked']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove the NAN values based on the Embarked feature\nprint(len(df_con))\ndf_con = df_con.dropna(subset=['Embarked'])\ndf_bin = df_bin.dropna(subset=['Embarked'])\nprint(len(df_con))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"this has removed the two rows. With that we have our two cleaned sub dataframes:","metadata":{}},{"cell_type":"code","source":"df_bin.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_con.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Encoding:\nNow we have our two sub dataframes ready, we can encode the features so they're ready to be used with our machine learning models.\nWe will encode our binned dataframe (df_bin) with one-hot encoding and our continuous datafram (df_con) with the label encoding function from sklearn.","metadata":{}},{"cell_type":"code","source":"#one-hot encode binned variables\none_hot_cols = df_bin.columns.tolist()\none_hot_cols.remove('Survived')\ndf_bin_enc = pd.get_dummies(df_bin,columns=one_hot_cols)\n\ndf_bin_enc.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\"one hot encoding gives it a 0 for what it isnt and a 1 for what it is\"","metadata":{}},{"cell_type":"code","source":"# One hot encode the categorical columns\ndf_embarked_one_hot = pd.get_dummies(df_con['Embarked'], \n                                     prefix='embarked')\n\ndf_sex_one_hot = pd.get_dummies(df_con['Sex'], \n                                prefix='sex')\n\ndf_plcass_one_hot = pd.get_dummies(df_con['Pclass'], \n                                   prefix='pclass')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine the one hot encoded columns with df_con_enc\ndf_con_enc = pd.concat([df_con, \n                        df_embarked_one_hot, \n                        df_sex_one_hot, \n                        df_plcass_one_hot], axis=1)\n\n# Drop the original categorical columns (because now they've been one hot encoded)\ndf_con_enc = df_con_enc.drop(['Pclass', 'Sex', 'Embarked'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_con_enc.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building Machine Learning Models:\n\nnow our data has been manipulated and converted into numbers, we can run a series of different machine learning algorithms over it to find which yield the best results.","metadata":{}},{"cell_type":"code","source":"#select the datafram we want to use first for predictions\nselected_df = df_con_enc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#split the dataframe into data and labels\nX_train = selected_df.drop('Survived', axis=1) #taking the selected_df, dropping survived and using the remaining variables\ny_train = selected_df.Survived #taking the survived variable","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shape of the data (without labels)\nX_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shape of the data (with labels)\ny_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define a function to fit machine learning algorithms:\nSince many of the algorithms we will use are from the sklearn library, they all take similar (practically the same) inputs and produce similar outputs.\n\nTo prevent writing code multiple times, we will functionise fitting the model and returning the accuracy scores.","metadata":{}},{"cell_type":"code","source":"# Function that runs the requested algorithm and returns the accuracy metrics\ndef fit_ml_algo(algo, X_train, y_train, cv):\n    \n    # One Pass\n    model = algo.fit(X_train, y_train)\n    acc = round(model.score(X_train, y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n    \n    return train_pred, acc, acc_cv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistic Regression","metadata":{}},{"cell_type":"code","source":"# Logistic Regression\nstart_time = time.time()\ntrain_pred_log, acc_log, acc_cv_log = fit_ml_algo(LogisticRegression(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\nlog_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=log_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"K-Nearest Neighbors","metadata":{}},{"cell_type":"code","source":"# k-Nearest Neighbours\nstart_time = time.time()\ntrain_pred_knn, acc_knn, acc_cv_knn = fit_ml_algo(KNeighborsClassifier(), \n                                                  X_train, \n                                                  y_train, \n                                                  10)\nknn_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=knn_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gaurssian Naive Bayes","metadata":{}},{"cell_type":"code","source":"# Gaussian Naive Bayes\nstart_time = time.time()\ntrain_pred_gaussian, acc_gaussian, acc_cv_gaussian = fit_ml_algo(GaussianNB(), \n                                                                      X_train, \n                                                                      y_train, \n                                                                           10)\ngaussian_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gaussian)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gaussian)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gaussian_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nLinear Support Vector Machines (SVC)","metadata":{}},{"cell_type":"code","source":"# Linear SVC\nstart_time = time.time()\ntrain_pred_svc, acc_linear_svc, acc_cv_linear_svc = fit_ml_algo(LinearSVC(),\n                                                                X_train, \n                                                                y_train, \n                                                                10)\nlinear_svc_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_linear_svc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_linear_svc)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=linear_svc_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Stochastic Gradient Descent","metadata":{}},{"cell_type":"code","source":"# Stochastic Gradient Descent\nstart_time = time.time()\ntrain_pred_sgd, acc_sgd, acc_cv_sgd = fit_ml_algo(SGDClassifier(), \n                                                  X_train, \n                                                  y_train,\n                                                  10)\nsgd_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_sgd)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgd)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=sgd_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decision Tree Classifier","metadata":{}},{"cell_type":"code","source":"# Decision Tree Classifier\nstart_time = time.time()\ntrain_pred_dt, acc_dt, acc_cv_dt = fit_ml_algo(DecisionTreeClassifier(), \n                                                                X_train, \n                                                                y_train,\n                                                                10)\ndt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_dt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_dt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=dt_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gradient Boost Trees","metadata":{}},{"cell_type":"code","source":"# Gradient Boosting Trees\nstart_time = time.time()\ntrain_pred_gbt, acc_gbt, acc_cv_gbt = fit_ml_algo(GradientBoostingClassifier(), \n                                                                       X_train, \n                                                                       y_train,\n                                                                       10)\ngbt_time = (time.time() - start_time)\nprint(\"Accuracy: %s\" % acc_gbt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gbt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gbt_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"CatBoost Algorithm","metadata":{}},{"cell_type":"code","source":"#define the categorical features for the CatBoost model\ncat_features = np.where(X_train.dtypes != np.float)[0]\ncat_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#use the CatBoost Pool() function to pool together the training data and categorical feature labels\ntrain_pool = Pool(X_train,\n                 y_train,\n                 cat_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CatBoost model definition\ncatboost_model = CatBoostClassifier(iterations=1000,\n                                   custom_loss=['Accuracy'],\n                                   loss_function='Logloss')\n\n#Fit CatBoost model\ncatboost_model.fit(train_pool,\n                  plot=True)\n\n#CatBoost accuracy\nacc_catboost = round(catboost_model.score(X_train, y_train) * 100, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How long will this take?\nstart_time = time.time()\n\n# Set params for cross-validation as same as initial model\ncv_params = catboost_model.get_params()\n\n# Run the cross-validation for 10-folds (same as the other models)\ncv_data = cv(train_pool,\n             cv_params,\n             fold_count=10,\n             plot=True)\n\n# How long did it take?\ncatboost_time = (time.time() - start_time)\n\n# CatBoost CV results save into a dataframe (cv_data), let's withdraw the maximum accuracy score\nacc_cv_catboost = round(np.max(cv_data['test-Accuracy-mean']) * 100, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print out the CatBoost model metrics\nprint(\"---CatBoost Metrics---\")\nprint(\"Accuracy: {}\".format(acc_catboost))\nprint(\"Accuracy cross-validation 10-Fold: {}\".format(acc_cv_catboost))\nprint(\"Running Time: {}\".format(datetime.timedelta(seconds=catboost_time)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results:\nwhich model had the best cross-validation accuracy?","metadata":{}},{"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_knn, \n        acc_log,  \n        acc_gaussian, \n        acc_sgd, \n        acc_linear_svc, \n        acc_dt,\n        acc_gbt,\n        acc_catboost\n    ]})\nprint(\"---Reuglar Accuracy Scores---\")\nmodels.sort_values(by='Score', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Trees',\n              'CatBoost'],\n    'Score': [\n        acc_cv_knn, \n        acc_cv_log,      \n        acc_cv_gaussian, \n        acc_cv_sgd, \n        acc_cv_linear_svc, \n        acc_cv_dt,\n        acc_cv_gbt,\n        acc_cv_catboost\n    ]})\nprint('---Cross-validation Accuracy Scores---')\ncv_models.sort_values(by='Score', ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that Gradient Boosting trees slightly edges out CatBoost in accuracy\nWe'll pay more attention to the cross-validation figure.\n\nCross-validation is more robust than just the .fit() models as it does multiple passes over the data instead of one.\n\nBecause the Gradiant boosting tree model got the best results, we'll use it for the next steps.","metadata":{}},{"cell_type":"markdown","source":"Feature Importance:\nwhich features of the best model were most important for making predictions?","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def feature_importance(model, data):\n    \n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc [-30:]\n    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20,10))\n    return fea_imp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using catboost, not sure how to use gt\nfeature_importance(catboost_model, X_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see here that the greatest factors deciding survivability would be sex as well as having a child/parent as well as age.","metadata":{}},{"cell_type":"markdown","source":"Submission!!:","metadata":{}},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One hot encode the columns in the test data frame (like X_train)\n\ntest_embarked_one_hot = pd.get_dummies(test['Embarked'], \n                                       prefix='embarked')\n\ntest_sex_one_hot = pd.get_dummies(test['Sex'], \n                                prefix='sex')\n\ntest_plcass_one_hot = pd.get_dummies(test['Pclass'], \n                                   prefix='pclass')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine the test one hot encoded columns with test\ntest = pd.concat([test, \n                  test_embarked_one_hot, \n                  test_sex_one_hot, \n                  test_plcass_one_hot], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a list of columns to be used for the predictions\nwanted_test_columns = X_train.columns\nwanted_test_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a prediction using the CatBoost model on the wanted columns\npredictions = catboost_model.predict(test[wanted_test_columns])\npredictions[:20]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a new dataframe we can submit\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = predictions.astype(int)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# our submission must have 418 rows\nsubmission.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert submission dataframe to a csv for submission to kaggle\nsubmission.to_csv('./df.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}