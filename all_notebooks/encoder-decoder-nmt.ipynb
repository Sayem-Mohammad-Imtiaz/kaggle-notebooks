{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Basic utilities needed in the code\n\nimport torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    \"\"\"\n    This function translates the input german sentence to the english sentence.\n    German sentence --> German Vector --> Encoder --> context vector --> Decoder --> English Vector --> English Sentence\n\n    :param model: the sequence-to-sequnce model\n    :param sentence: the input \"german\" sentence\n    :param german: the german Field object\n    :param english : the english Field object\n    :param device: cuda / cpu\n    :param max_length : maximum length of the translated sentence\n    \"\"\"\n\n    spacy_german = spacy.load(\"de\")\n\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_german(sentence)]\n\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # insert the start and end sequence\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    text_to_indicies = [german.vocab.stoi[token] for token in tokens]\n\n    # (N, ) --> (1 X N)\n    sentence_tensor = torch.LongTensor(text_to_indicies).unsqueeze(0).to(device)\n\n    # Retrieve the hidden_state and cell_state from the encoder\n    with torch.no_grad():\n        hidden_state, cell_state = model.Encoder_LSTM(sentence_tensor)\n\n    # start the decoding part using start sequence and the (hidden_state, cell_state)\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden_state, cell_state = model.Decoder_LSTM(previous_word, hidden_state, cell_state)\n\n            # shape received : 1 X 1 X |Eng_Vocab|; squeeze it\n            # output = output.squeeze(0)\n\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model stops predicting if it predicts <eos> token (index)\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    # We have the indicies of the translated sentence in english\n    # Now, we will predict the sentence\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    return translated_sentence[1:]\n\ndef bleu(data, model, german, english, device):\n    \"\"\"\n    *** reference : https://www.youtube.com/watch?v=DejHQYAGb7Q ***\n    :param data: the batch containing german and english sentences\n    :param model: the model\n    :param german: the german Field object\n    :param english: the english Field object\n    :param device: cuda / cpu\n    \"\"\"\n\n    targets = []\n    outputs = []\n\n    for example in data:\n        ger_sent = vars(example)[\"ger_sent\"]\n        eng_sent = vars(example)[\"eng_sent\"]\n        \n        prediction = translate_sentence(model, ger_sent, german, english, device)\n\n        # remove the <eos> token from the end\n        prediction = prediction[:-1]\n\n        targets.append([eng_sent])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-22T20:44:57.305926Z","iopub.execute_input":"2021-06-22T20:44:57.306329Z","iopub.status.idle":"2021-06-22T20:44:57.322328Z","shell.execute_reply.started":"2021-06-22T20:44:57.306289Z","shell.execute_reply":"2021-06-22T20:44:57.320724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport random\n\n# ---------------------------- ENCODER ----------------------------\nclass Encoder(nn.Module):\n\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, drop_prob):\n        \"\"\"\n        :param input_size: the size of the input sequence\n        :param embedding_size: the embedding dimension\n        :param hidden_size: the hidden dimension used in the LSTM model\n        :param num_layers: number of layers in the LSTM model\n        :param drop_prob: the probability of dropout\n        \"\"\"\n\n        # self.param_dict = {\n        #     'input_size' : input_size,\n        #     'embedding_size' : embedding_size,\n        #     'hidden_size' : hidden_size,\n        #     'num_layers' : num_layers,\n        #     'drop_prob' : drop_prob\n        # }\n\n        super(Encoder, self).__init__()\n\n        self.dropout = nn.Dropout(drop_prob)  # for Regularization\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        \n        # the rnn cell\n        self.rnn = nn.LSTM(input_size = embedding_size,\n                        hidden_size = hidden_size,\n                        num_layers = num_layers,\n                        dropout=drop_prob,\n                        batch_first=True\n        )\n\n    def forward(self, x):\n        \"\"\"\n        :param x: the vector form of the sentence \n                  (containing the indicies mapped in the vocab)\n        \"\"\"\n\n        # pass the data\n        # N X T --> N X T X D\n        x = self.dropout(self.embedding(x))\n\n        output, (hidden_state, cell_state) = self.rnn(x)\n\n        # return the context vectors\n        # their shape : L X N X H (num_layers X batch_size X hidden_size)\n        return hidden_state, cell_state\n\n\n\n\n# ---------------------------- DECODER ----------------------------\nclass Decoder(nn.Module):\n\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, drop_prob, output_size):\n        \"\"\"\n        :param input_size: the size of the input sequence\n        :param embedding_size: the embedding dimension\n        :param hidden_size: the hidden dimension used in the LSTM model\n        :param num_layers: number of layers in the LSTM model\n        :param drop_prob: the probability of dropout\n        :param output_size: the output size of the linear layer after the decoding\n        \"\"\"\n\n        # self.param_dict = {\n        #     'input_size' : input_size,\n        #     'embedding_size' : embedding_size,\n        #     'hidden_size' : hidden_size,\n        #     'num_layers' : num_layers,\n        #     'drop_prob' : drop_prob,\n        #     'output_size' : output_size\n        # }\n\n        super(Decoder, self).__init__()\n\n        self.dropout = nn.Dropout(drop_prob)  # for Regularization\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n\n        self.rnn = nn.LSTM(input_size=embedding_size,\n                            hidden_size=hidden_size,\n                            num_layers=num_layers,\n                            dropout=drop_prob,\n                            # batch_first=True\n        )\n\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, hidden_state, cell_state):\n\n        # unsqueeze x\n        # shape becomes : 1 X N\n        x = x.unsqueeze(0)\n\n        # 1 X N --> 1 X N X D\n        x = self.dropout(self.embedding(x))\n\n        # shape of outputs : 1 X N X H (1 X batch_size X Hidden_size)\n        # shape of hidden and cell states : L X N X H\n        outputs, (hidden_state, cell_state) = self.rnn(x, (hidden_state, cell_state))\n\n        # 1 X N X H --> 1 X N X output_size\n        predictions = self.fc(outputs)\n\n        # 1 X N X output_size --> N X output_size\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden_state, cell_state\n\n\n\n\n# ---------------------------- SEQUENCE-TO-SEQUENCE ----------------------------\nclass Seq2Seq(nn.Module):\n\n    def __init__(self, Encoder_LSTM, Decoder_LSTM):\n        \"\"\"\n        :param Encoder_LSTM: the encoder part for the Seq2Seq model\n        :param Decoder_LSTM: the decoder part for the Seq2Seq model\n        \"\"\"\n\n        super(Seq2Seq, self).__init__()\n        self.Encoder_LSTM = Encoder_LSTM\n        self.Decoder_LSTM = Decoder_LSTM\n\n    def forward(self, source, target, eng_vocab_size, tfr=0.5):\n        \"\"\"\n        :param source: padded sentences in German\n                       shape : [(sentence length German + some padding), #Sentences]\n        :param target: padded sentences in English\n                       shape : [(sentence length English + some padding), #Sentences]\n        :param eng_vocab_size : size of the english vocab\n        :param tfr: teach force ratio\n        \"\"\"\n\n        # # Convert it into Batch Size X Sequence Length\n        # target = target.permute(1, 0)\n\n        batch_size = source.shape[0]\n        target_len = target.shape[0]\n\n        outputs = torch.zeros(target_len, batch_size, eng_vocab_size).to(device)\n\n        # retaining the context vector from the encoder\n        hidden_state, cell_state = self.Encoder_LSTM(source)\n\n        x = target[0]\n\n        for i in range(1, target_len):\n\n            # output : batch_size X |Eng_Vocab_Size|\n            output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n\n            outputs[i] = output\n\n            best_guess = output.argmax(1)  # the most suitable word embedding\n\n            # Teach force ratio\n            # Either pass the next correct word from the dataset\n            # or use the predicted word\n            x = target[i] if random.random() < tfr else best_guess\n\n        return outputs\n\n\nif __name__ == '__main__':\n\n    # ..................... Some testing code .....................\n\n    # for encoder\n    input_size_encoder = 5000  # vocab size\n    encoder_embedding_size = 300\n    hidden_size = 1024\n    num_layers = 2\n    encoder_dropout = float(0.5)\n    \n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    encoder_lstm = Encoder(input_size_encoder, encoder_embedding_size, \n                            hidden_size, num_layers, encoder_dropout).to(device)\n\n    # print(encoder_lstm)\n\n    # for decoder\n    input_size_decoder = 4500\n    decoder_embedding_size = 300\n    hidden_size = 1024\n    num_layers = 2\n    decoder_dropout = float(0.5)\n    output_size = 4500\n    \n    decoder_lstm = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, \n                            num_layers, decoder_dropout, output_size).to(device)\n\n    # print(decoder_lstm)\n\n    model = Seq2Seq(encoder_lstm, decoder_lstm)\n    print(model)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:44:57.619246Z","iopub.execute_input":"2021-06-22T20:44:57.619613Z","iopub.status.idle":"2021-06-22T20:44:57.954065Z","shell.execute_reply.started":"2021-06-22T20:44:57.61958Z","shell.execute_reply":"2021-06-22T20:44:57.952824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def testing_Iterators(train_iterator, test_iterator, GERMAN_VOCAB, ENGLISH_VOCAB):\n    \"\"\"\n    This function just prints the batches\n\n    :param train_iterator: iterator for training\n    :param test_iterator: iterator for testing\n    :param GERMAN_VOCAB: the German vocab\n    :param ENGLISH_VOCAB: the English vocab\n    \n    \"\"\"\n    for data in train_iterator:\n        # print(f\"Length : {data.ger_sent.shape}\")  # \"German :\", *data.ger_sent, \n        # print(f\"Length : {data.eng_sent.shape}\")  # \"English :\", *data.eng_sent, \n        \n        print(\"-------------- GERMAN SENTENCES ------------\")\n        print()\n        temp = data.ger_sent.permute(1, 0)\n        for ele in temp:\n            for num in ele:\n                print(GERMAN_VOCAB.itos[num.item()], end=\" \")\n\n            print()\n\n        print()\n\n        print(\"-------------- ENGLISH SENTENCES ------------\")\n        print()\n        temp = data.eng_sent.permute(1, 0)\n        for ele in temp:\n            for num in ele:\n                print(ENGLISH_VOCAB.itos[num.item()], end=\" \")\n\n            print()\n\n        print()\n        break\n\n    for data in test_iterator:\n        # print(f\"Length : {data.ger_sent.shape}\")  # \"German :\", *data.ger_sent, \n        # print(f\"Length : {data.eng_sent.shape}\")  # \"English :\", *data.eng_sent, \n        \n        print(\"-------------- GERMAN SENTENCES ------------\")\n        print()\n        temp = data.ger_sent.permute(1, 0)\n        for ele in temp:\n            for num in ele:\n                print(GERMAN_VOCAB.itos[num.item()], end=\" \")\n\n            print()\n\n        print()\n\n        print(\"-------------- ENGLISH SENTENCES ------------\")\n        print()\n        temp = data.eng_sent.permute(1, 0)\n        for ele in temp:\n            for num in ele:\n                print(ENGLISH_VOCAB.itos[num.item()], end=\" \")\n\n            print()\n\n        print()\n        break\n","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:44:58.93006Z","iopub.execute_input":"2021-06-22T20:44:58.930429Z","iopub.status.idle":"2021-06-22T20:44:58.944282Z","shell.execute_reply.started":"2021-06-22T20:44:58.930399Z","shell.execute_reply":"2021-06-22T20:44:58.943237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# declare constants here...\nlearning_rate = 0.003\nepochs = 100\ntrain_batch_size = 128\ntest_batch_size = 256","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:45:00.22809Z","iopub.execute_input":"2021-06-22T20:45:00.228633Z","iopub.status.idle":"2021-06-22T20:45:00.236112Z","shell.execute_reply.started":"2021-06-22T20:45:00.228591Z","shell.execute_reply":"2021-06-22T20:45:00.233872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef train(data_loader, model, optimizer, criterion, english_vocab_size, device):\n    \"\"\"\n    This is the main training function that trains the model and\n    returns training loss\n\n    :param data_loader: this is the torch data loader\n    :param model: model (encoder - decoder model)\n    :param optimizer: torch optimizer, e.g. adam, sgd, etc.\n    :param criterion: loss function\n    :param english_vocab_size: size of the english vocabulary\n    :param device: this can be \"cuda\" or \"cpu\"\n    \"\"\"\n\n    # set the model to training mode\n    model.train()\n\n    batch_loss = 0.0\n    batches = 0\n    for data in data_loader:\n\n        input = data.ger_sent.to(device)\n        target = data.eng_sent.to(device)\n\n        input = input.permute(1, 0)\n        \n        optimizer.zero_grad()\n\n        # pass the input and target for model's forward method\n        output = model(input, target, english_vocab_size)\n\n        output = output.permute(1, 0, 2)\n\n        # print(output.shape)\n\n        output = output[1:].reshape(-1, output.shape[2])\n\n        target = target.permute(1, 0)\n        target = target[1:].reshape(-1)\n\n        # calculate the loss\n        loss = criterion(output, target)\n\n        # back-prop\n        loss.backward()\n\n        # clip the gradient value if it exceeds 1 => called NORM clipping  (https://www.youtube.com/watch?v=_-CZr06R5CQ)\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # update the weight values\n        optimizer.step()\n\n        batches += 1.0\n        batch_loss += loss.item()\n\n    return batch_loss/batches\n\n\ndef evaluate(data_loader, model, criterion, device):\n    \"\"\"\n    This function is used for returning loss\n\n    :param data_loader: this is the torch data loader\n    :param model: model (encoder - decoder model)\n    :param criterion: loss function\n    :param device: this can be \"cuda\" or \"cpu\"\n    \"\"\"\n\n    batch_loss = 0.0\n    batches = 0\n\n    # put the model in evaluation mode\n    model.eval()\n\n    with torch.no_grad():\n\n        for data in data_loader:\n\n            input = data.ger_sent.to(device)\n            target = data.eng_sent.to(device)\n\n            # pass the input and target for model's forward method\n            output = model(input, target, eng_vocab_size)\n\n            loss = criterion(output, target)\n\n            batches += 1.0\n            batch_loss += loss.item()\n\n    return batch_loss/batches\n","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:45:03.222466Z","iopub.execute_input":"2021-06-22T20:45:03.222825Z","iopub.status.idle":"2021-06-22T20:45:03.23578Z","shell.execute_reply.started":"2021-06-22T20:45:03.222793Z","shell.execute_reply":"2021-06-22T20:45:03.234231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download en\n!python -m spacy download de","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:27:51.262453Z","iopub.execute_input":"2021-06-22T20:27:51.262835Z","iopub.status.idle":"2021-06-22T20:27:51.26914Z","shell.execute_reply.started":"2021-06-22T20:27:51.262804Z","shell.execute_reply":"2021-06-22T20:27:51.26757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def checkpoint_and_save(model, best_loss, epoch, optimizer, epoch_loss):\n    print('saving')\n    print()\n    state = {'model': model,'best_loss': best_loss,'epoch': epoch,'rng_state': torch.get_rng_state(), 'optimizer': optimizer.state_dict(),}\n    torch.save(state, '/kaggle/working/checkpoint-NMT-BEST.pth')\n    torch.save(model.state_dict(),'/kaggle/working/checkpoint-NMT-BEST-SD.pth')","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:49:56.993735Z","iopub.execute_input":"2021-06-22T20:49:56.994095Z","iopub.status.idle":"2021-06-22T20:49:57.003737Z","shell.execute_reply.started":"2021-06-22T20:49:56.994064Z","shell.execute_reply":"2021-06-22T20:49:57.002098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchtext\nimport spacy\nfrom torchtext.data.metrics import bleu_score\nfrom torchtext.data import Field, TabularDataset, BucketIterator\nimport torch.optim as optim\nimport warnings\nwarnings.simplefilter('ignore')\n\n\ndef tokenize_german(text):\n    \"\"\"\n    tokenizer for German language\n    \"\"\"\n    return [token.text for token in spacy_german.tokenizer(text)]\n\ndef tokenize_english(text):\n    \"\"\"\n    tokenizer for English language\n    \"\"\"\n    return [token.text for token in spacy_english.tokenizer(text)]\n\n\nif __name__ == '__main__':\n\n    # tokenizers for German and English\n    spacy_german = spacy.load(\"de\")\n    spacy_english = spacy.load(\"en\")\n\n    # Field Object for German\n    german = Field(tokenize=tokenize_german,\n                    lower=True,\n                    init_token=\"<sos>\",\n                    eos_token=\"<eos>\"\n    )\n\n    # Field Object for English\n    english = Field(tokenize=tokenize_english,\n                    lower=True,\n                    init_token=\"<sos>\",\n                    eos_token=\"<eos>\"\n    )\n\n    # dataset object\n    dataset = TabularDataset(path=\"../input/german-to-english/dataset.csv\",\n                            format='csv',\n                            skip_header=True,\n                            fields=[('ger_sent', german), ('eng_sent', english)]\n    )\n\n    # 80% training\n    train_dataset, test_dataset = dataset.split(split_ratio=0.80)\n\n    # BUILDING THE VOCAB\n    german.build_vocab(train_dataset, max_size=10000, min_freq=3)\n    english.build_vocab(train_dataset, max_size=10000, min_freq=3)\n\n    GERMAN_VOCAB = german.vocab\n    ENGLISH_VOCAB = english.vocab\n\n    print(f\"German Vocab Size : {len(GERMAN_VOCAB)}\")\n    print(f\"English Vocab Size : {len(ENGLISH_VOCAB)}\")\n\n    # set up the device to cuda\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    TRAIN_BATCH_SIZE = train_batch_size\n    TEST_BATCH_SIZE = test_batch_size\n\n    # Iterators\n    train_iterator, test_iterator = BucketIterator.splits(\n        (train_dataset, test_dataset),\n        batch_sizes=(TRAIN_BATCH_SIZE,TEST_BATCH_SIZE),\n        sort_within_batch = True,\n        sort_key=lambda x: len(x.ger_sent),\n        device=device\n    )\n\n    # if we wanna explore the data in train and test iterators\n    # use this function\n    testing_Iterators(train_iterator, test_iterator, GERMAN_VOCAB, ENGLISH_VOCAB)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:46:48.971765Z","iopub.execute_input":"2021-06-22T20:46:48.972194Z","iopub.status.idle":"2021-06-22T20:47:02.161405Z","shell.execute_reply.started":"2021-06-22T20:46:48.972164Z","shell.execute_reply":"2021-06-22T20:47:02.160492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's create the model\n# ENCODER : \ninput_size_encoder = len(GERMAN_VOCAB)  # vocab size\nencoder_embedding_size = 300\nhidden_size = 1024\nnum_layers = 2\nencoder_dropout = float(0.5)\n\nencoder_lstm = Encoder(input_size_encoder, encoder_embedding_size, \n                        hidden_size, num_layers, encoder_dropout).to(device)\n\n# DECODER : \ninput_size_decoder = len(ENGLISH_VOCAB)\ndecoder_embedding_size = 300\nhidden_size = 1024\nnum_layers = 2\ndecoder_dropout = float(0.5)\noutput_size = len(ENGLISH_VOCAB)\n\ndecoder_lstm = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, \n                        num_layers, decoder_dropout, output_size).to(device)\n\n\nmy_model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n\n# Let's train the model\nprint(\"Model Training started :)\")\n\nEPOCHS = epochs\nlearning_rate = learning_rate\n\nepoch_loss = 0.0\nbest_loss = 10**7\nbest_epoch = -1\noptimizer = optim.Adam(my_model.parameters(), lr=learning_rate)\npad_idx = ENGLISH_VOCAB.stoi[\"<pad>\"]\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\nearly_stopping_counter = 0\nprint(my_model, end=\"\\n\")\n\n# for checking the model at every step\nsample_sentence = \"ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster\"\n\ntrain_losses = []\ntest_bleu_scores = []\nprint(translate_sentence(my_model, sample_sentence, german, english, device))\nfor epoch in range(EPOCHS):\n\n    epoch_loss = train(train_iterator, my_model, optimizer, criterion, len(ENGLISH_VOCAB), device)\n\n    # Append the training loss\n    train_losses.append(epoch_loss)\n    print(f\"Epoch : {epoch} ; Epoch Loss : {epoch_loss}\")\n\n    # print the bleu bleu score for testing # update to 1:100\n    print(f\"Testing Bleu Score : {bleu(test_dataset[1:100], my_model, german, english, device)}\")\n\n    if epoch_loss < best_loss:\n        best_loss = epoch_loss\n        best_epoch = epoch\n        checkpoint_and_save(my_model, best_loss, epoch, optimizer, epoch_loss)\n\n    else:\n        early_stopping_counter += 1\n\n    if early_stopping_counter > 5:\n        print(\"Early Stopping...\")\n        break\n\n    print(translate_sentence(my_model, sample_sentence, german, english, device))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:52:08.029415Z","iopub.execute_input":"2021-06-22T20:52:08.029842Z","iopub.status.idle":"2021-06-22T20:52:40.331257Z","shell.execute_reply.started":"2021-06-22T20:52:08.02981Z","shell.execute_reply":"2021-06-22T20:52:40.330072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Loading the model","metadata":{"execution":{"iopub.status.busy":"2021-06-22T20:58:58.974397Z","iopub.execute_input":"2021-06-22T20:58:58.974811Z","iopub.status.idle":"2021-06-22T20:58:58.981858Z","shell.execute_reply.started":"2021-06-22T20:58:58.974767Z","shell.execute_reply":"2021-06-22T20:58:58.980113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chk = torch.load(\"./checkpoint-NMT-BEST.pth\")\nmm = chk['model']\n\nsd = torch.load(\"./checkpoint-NMT-BEST-SD.pth\")\nmm.load_state_dict(sd)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T21:00:01.061187Z","iopub.execute_input":"2021-06-22T21:00:01.061564Z","iopub.status.idle":"2021-06-22T21:00:01.366722Z","shell.execute_reply.started":"2021-06-22T21:00:01.061504Z","shell.execute_reply":"2021-06-22T21:00:01.365434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(translate_sentence(mm, sample_sentence, german, english, device))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T21:00:02.789507Z","iopub.execute_input":"2021-06-22T21:00:02.790014Z","iopub.status.idle":"2021-06-22T21:00:04.656289Z","shell.execute_reply.started":"2021-06-22T21:00:02.789983Z","shell.execute_reply":"2021-06-22T21:00:04.655004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}