{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Detecting Clickbait Headlines in Indonesia\n\nIn this notebook, we will try to predict whether a headline in Indonesian News is a clickbait or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/judul-artikel-online-dengan-label-clickbait/primary-dataset.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add a new column called `length`, which is simply the number of words on a text."},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"length\"] = [len(text.split()) for text in data.text]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Exploratory Data Analysis\n\nFirst let's see the distribution around the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(data.index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 3237 texts/headlines in this data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.label.value_counts().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are less clickbait contents/headlines than non-clickbait one, but it's closer to balance."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.kdeplot(data.sort_values(by=\"length\", ascending=False).length, shade=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The length of the headlines has a normal distribution."},{"metadata":{},"cell_type":"markdown","source":"## 2. Text Pre-processing\n\nBefore building our NLP model, we have to clean the text first through some steps.\n\n1. Lower-casing\n2. Remove numbers and punctuations\n3. Remove stopwords\n4. Tokenizing"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import word_tokenize\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords_id = pd.read_csv(\"/kaggle/input/indonesian-stoplist/stopwordbahasa.csv\", header=None)\nstopwords_id.columns = [\"Words\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(data):\n    token = word_tokenize(data)\n    token = [text.lower() for text in token]\n    token = [text for text in token if text.isalpha()]\n    token = [text for text in token if not stopwords_id.Words.eq(text).any()]\n    return token","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(data):\n    token = preprocess(data)\n    words = token[0]\n    for num in range(1,len(token)):\n        words = words + (\" \" + token[num])\n    return words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"text_clean\"] = [clean_text(text) for text in data.text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vec = CountVectorizer(ngram_range=(1,2), min_df = 2)\ntoken = count_vec.fit_transform(data.text_clean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 4599 terms which appear at least in 2 documents, and these terms could be a single word or two words (bigram)."},{"metadata":{},"cell_type":"markdown","source":"## 3. Most Frequent Terms\n\nAfter tokenizing the text, we can now see which terms appear the most."},{"metadata":{"trusted":true},"cell_type":"code","source":"def most_freq_terms(min_len, max_len):\n    most_freq_vec = CountVectorizer(ngram_range=(min_len,max_len))\n    most_freq_mat = most_freq_vec.fit_transform(data.text_clean)\n    terms = most_freq_vec.get_feature_names()\n    freq = most_freq_mat.toarray().sum(axis=0)\n    df = pd.DataFrame(freq, terms)\n    df.columns = [\"Terms\"]\n    df = df.sort_values(by = \"Terms\", ascending = False)\n    df.head(10).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_freq_terms(1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_freq_terms(2,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Terms like **new normal**, **virus corona**, and **pandemi** appear a lot since the data was scraped from the last few months."},{"metadata":{},"cell_type":"markdown","source":"## 4. Modelling : Naive-Bayes\n\nWe can now fit the data into the model. First we will try a simple model aka Naive-Bayes."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, test_x, train_y, test_y = train_test_split(token, data.label, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Size :\", train_x.shape)\nprint(\"Test Size :\", test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = MultinomialNB()\nnb.fit(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb.score(train_x, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb.score(test_x, test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"87.6% Accuracy for train dataset and only 71,7% for test dataset. Not really a good performance."},{"metadata":{},"cell_type":"markdown","source":"## 5. Modelling : Neural Networks\n\nIn most cases, for NLP, you'd prefer to use neural networks rather than models like Naive-Bayes simply because it is a much better algorithm. First we have to do pad sequencing, which is a step of giving index to each words (sequencing), and then normalizing the text length by using padding method."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(data.text, data.label, test_size = 0.2, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_SIZE = 2000\nMAX_LEN = 50\ntkz = Tokenizer(num_words=VOCAB_SIZE)\ntkz.fit_on_texts(x_train)\nsequences = tkz.texts_to_sequences(x_train)\nsequences = sequence.pad_sequences(sequences, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we set the vocabulary size to 2000 and maximum sequence/length for each text to be 50 words."},{"metadata":{},"cell_type":"markdown","source":"Then we can simply fit our RNN model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.random import set_seed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(42)\nset_seed(42)\nmodel = Sequential()\nmodel.add(Embedding(VOCAB_SIZE, 50, input_length = MAX_LEN))\nmodel.add(LSTM(64))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(sequences, y_train, batch_size=128, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences_test = tkz.texts_to_sequences(x_test)\nsequences_test = sequence.pad_sequences(sequences_test, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(sequences_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even a simple LSTM model (3 layers and 10 epochs) has already done well. With a small data like this, 76% is a decent accuracy. Surely we can still improve this model by adding more layers, using different batch size, do more epochs, etc."},{"metadata":{},"cell_type":"markdown","source":"## 6. Summary\n\n1. There are a lot of clickbait headlines in Indonesia\n2. The last few months, the most occuring terms in Indonesian headlines are virus corona, new normal, and pandemi\n3. We can predict a headline is clickbait or not with a 76% accuracy."},{"metadata":{},"cell_type":"markdown","source":"## Demo\n\nYou can copy this code snippet for interactive demo"},{"metadata":{"trusted":true},"cell_type":"code","source":"def interactive(title):\n    print(\"Headline :\",title)\n    title = tkz.texts_to_sequences([title])\n    title = sequence.pad_sequences(title, maxlen=MAX_LEN)\n    label = model.predict_classes(title)\n    if label[0][0]==0:\n        print(\"This news is not a clickbait\")\n    else:\n        print(\"This news is a clickbait\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interactive(\"Berikut 5 fakta mengenai Enzy Storia, nomor 4 sempat menuai kontroversi\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interactive(\"Kevin De Bruyne cetak 2 gol ke gawang Arsenal\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}