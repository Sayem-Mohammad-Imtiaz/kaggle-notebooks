{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom pandas_profiling import ProfileReport\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\nsample_submission = pd.read_csv(\"../input/house-prices-dataset/sample_submission.csv\")\ntest = pd.read_csv(\"../input/house-prices-dataset/test.csv\")\ntrain = pd.read_csv(\"../input/house-prices-dataset/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Separating target variable from train dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"target=pd.DataFrame(train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=train.drop(['SalePrice'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets have a look into avaialble features*"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*There are many categorical as well as numerical features in the dataset*"},{"metadata":{},"cell_type":"markdown","source":"*Lets combine train and test datasets so that we can do data processing easily in one shot*"},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df=train_df.append(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> *Identifying list and number of numerical and categorical*\n* numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in comb_df.columns:\n    if(comb_df[col].dtypes!='object'):\n        num_features.append(col)\n    else:\n        cat_features.append(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*List of numerical features*"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total numerical features\",len(num_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(num_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*List of categorical features*"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Total number of ctegorical features\",len(cat_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(cat_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Considering huge number of features we have, we can think of reducing/dropping columns which are of not much use. Starting point can be columns which have most of missing values.We can drop columns where missing data percentage is more than 50%*.\nLets use pandas profiling to get all the basic details."},{"metadata":{"trusted":true},"cell_type":"code","source":"ProfileReport(comb_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*If you observe the count of categorical and numerical fetures in above report, it doesn't match with our analysis.Numerical count is 34 whereas ours was 37. I guess there are 3 such variables which have been incorrectly identified as numeric.Lets identify them"},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df[num_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*It seems MSSubClass , OverallQual , OverallCond are more of object data types not numrical.Lets convert them into object*"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ['MSSubClass','OverallQual','OverallCond']:\n    comb_df[col]=comb_df[col].astype('object')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Lets findout new list of numeric and categorical features*"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in comb_df.columns:\n    if(comb_df[col].dtypes!='object'):\n        num_features.append(col)\n    else:\n        cat_features.append(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(num_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(cat_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Now counts match with profile report*"},{"metadata":{},"cell_type":"markdown","source":"*Lets take help of profile report to identify the features having missing values more than 50%.We can drop these columns from our data set.Also we can drop ID column, doesnt seem to have any contribution*"},{"metadata":{"trusted":true},"cell_type":"code","source":"col_drop=['Id','Alley','Fence','MiscFeature','PoolQC']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df=comb_df.drop(col_drop,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's use Profile Report to identify few more columns which we can get rid of thus reducing the feature number**\n* 3SsnPorch is filled with almost one value i.e. there is no variance in terms of values and one value has dominated - drop\n* Condition2 is filled with almost one value i.e. there is no variance in terms of values and one value has dominated - drop\n* LowQualFinSF is filled with almost one value i.e. there is no variance in terms of values and one value has dominated - drop\n* MiscVal is filled with almost one value i.e. there is no variance in terms of values and one value has dominated - drop\n* PoolArea\n* Utilities\n* We can insert a new column called Age which will be difference between YearBuilt and YearSold. Age can have an impact on selling price. We can drop the other 3 columns -YearBuilt / YrSold / YearRemodAdd\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df['Age']=comb_df['YrSold']-comb_df['YearBuilt']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df=comb_df.drop(['3SsnPorch','Condition2','LowQualFinSF','MiscVal','PoolArea','Utilities','YearBuilt','YrSold','YearRemodAdd'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have managed to bring down the feature number from 81 to 67**"},{"metadata":{},"cell_type":"markdown","source":"**Let's impute the missing values in our dataset.We will use Iterative Imputer for numerical features.For categorical, we can update NaNs as \"Unknown\"**"},{"metadata":{},"cell_type":"markdown","source":"> Before that, lets refresh our latest list of numerical and categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features=[]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in comb_df.columns:\n    if(comb_df[col].dtypes!='object'):\n        num_features.append(col)\n    else:\n        cat_features.append(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(num_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(cat_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Impute missing values in numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df_num=comb_df[num_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer=IterativeImputer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df_num_imp=pd.DataFrame(imputer.fit_transform(comb_df_num))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df_num_imp.columns=comb_df_num.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df_num_imp.index=comb_df_num.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Imputing Missing Values in Categorical Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df_cat=comb_df[cat_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_df_cat=comb_df_cat.fillna('Unknown')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Let's LabelEncode our categorical variables in order to use them during model implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"le=LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in comb_df_cat.columns:\n    comb_df_cat[col]=le.fit_transform(comb_df_cat[col])\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Creating one dataset by concatenating imputed numerical and categorical features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_new=pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_new=pd.concat([comb_df_cat,comb_df_num_imp],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_new.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets scale our dataset**\n* Its not required to scale our target variable.Please refer to below discussion:\nhttps://stats.stackexchange.com/questions/111467/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler=StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_new_scaled=pd.DataFrame(scaler.fit_transform(comb_new))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We had merged our Train and Test Dataset at the begining of this notebook in order to apply all preprocessing steps to both train and test datasets.As we know prepare for model building, let's split the dataset.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_new_scaled_train=comb_new_scaled.iloc[:1460,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Renaming the scaled train dataset to X and target variable to y for clarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"X=comb_new_scaled_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y=target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_new_scaled_test=comb_new_scaled.iloc[1460:,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Above scaled test data will be used to predict and submit our result.Renaming it to test_data"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data=comb_new_scaled_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets use our X and y datasets to create training and vaidation datasets for model implementation and accuracy tests**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We will be using XGBRegressor and LGBMRegressor for predicting house prices**"},{"metadata":{},"cell_type":"markdown","source":"> Lets start with XGBRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB = XGBRegressor(max_depth=3,learning_rate=0.1,n_estimators=1000,reg_alpha=0.001,reg_lambda=0.000001,n_jobs=-1,min_child_weight=3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Let's check the XGB model performance on our train and test data set"},{"metadata":{},"cell_type":"markdown","source":"> train score"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(XGB.score(X_train,y_train))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> test score"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(XGB.score(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> The difference between train and test score suggest that there is variance in our model.We might need more hyperparameter tuning. I will work on this in few days."},{"metadata":{},"cell_type":"markdown","source":"**Predicting house price for test_data **"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = pd.DataFrame( XGB.predict(test_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I will work on Parameter Tuning and LightGBM part**\nThank you."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}