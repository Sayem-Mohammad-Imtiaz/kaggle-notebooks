{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#libraries for getting data and extracting\nimport os\nimport urllib.request\nimport tarfile\nimport json\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n\n#libraries for text preprocessing\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nnltk.download('wordnet') \nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n#libraries for keyword extraction with tf-idf\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom scipy.sparse import coo_matrix\n\n#libraries for reading and writing files\nimport pickle\n\n#libraries for BM25\n!pip install rank_bm25\nfrom rank_bm25 import BM25Okapi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getData():\n    urls = ['https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/comm_use_subset.tar.gz', 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/noncomm_use_subset.tar.gz', 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/custom_license.tar.gz', 'https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-03-27/biorxiv_medrxiv.tar.gz']\n\n    # Create data directory\n    try:\n        os.mkdir('./data')\n        print('Directory created')\n    except FileExistsError:\n        print('Directory already exists')\n\n    #Download all files\n    for i in range(len(urls)):\n        urllib.request.urlretrieve(urls[i], './data/file'+str(i)+'.tar.gz')\n        print('Downloaded file '+str(i+1)+'/'+str(len(urls)))\n        tar = tarfile.open('./data/file'+str(i)+'.tar.gz')\n        tar.extractall('./data')\n        tar.close()\n        print('Extracted file '+str(i+1)+'/'+str(len(urls)))\n        os.remove('./data/file'+str(i)+'.tar.gz')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(text):\n    #define stopwords\n    stop_words = set(stopwords.words(\"english\"))\n    #Remove punctuations\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    #Convert to lowercase\n    text = text.lower()\n    #remove tags\n    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    ##Convert to list from string\n    text = text.split()\n    ##Stemming\n    ps=PorterStemmer()\n    text = [ps.stem(word) for word in text if not word in stop_words]\n    #Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  stop_words] \n    text = \" \".join(text) \n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract():\n    #create our collection locally in the data folder\n    \n    #creating our initial datastructure\n    x = {'paper_id':[], 'title':[], 'abstract': []}\n    \n    #Iterate through all files in the data directory\n    for subdir, dirs, files in os.walk('./data'):\n        for file in tqdm(files):\n            with open(os.path.join(subdir, file)) as f:\n                data = json.load(f)\n                \n               #Append paper ID to list\n                x['paper_id'].append(data['paper_id'])\n               #Append article title to list & preprocess the text\n                x['title'].append((data['metadata']['title']))\n                \n                #Append abstract text content values only to abstract list & preprocess the text\n                abstract = \"\"\n                for paragraph in data['abstract']:\n                    abstract += paragraph['text']\n                    abstract += '\\n'\n                #if json file no abstract in file, set the body text as the abstract (happens rarely, but often enough that this edge case matters)\n                if abstract == \"\": \n                    for paragraph in data['body_text']:\n                        abstract += paragraph['text']\n                        abstract += '\\n'\n                x['abstract'].append(preprocess(abstract))\n                \n    #Create Pandas dataframe & write to pickle file\n    df = pd.DataFrame.from_dict(x, orient='index')\n    df = df.transpose()\n    pickle.dump( df, open( \"full_data_processed_FINAL.p\", \"wb\" ) )\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_topn_from_vector(feature_names, sorted_items, topN):\n    #use only topn items from vector\n    sorted_items = sorted_items[:topN]\n \n    score_vals = []\n    feature_vals = []\n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getAbstractKeywords(entry, cv, X, tfidf_transformer, feature_names, topN):\n    abstract = entry['abstract']\n    \n    #first check that abstract is full\n    if type(abstract) == float:\n        return []\n \n    #generate tf-idf for the given document\n    tf_idf_vector=tfidf_transformer.transform(cv.transform([abstract])) \n    #sort the tf-idf vectors by descending order of scores\n    sorted_items=sort_coo(tf_idf_vector.tocoo())\n    #extract only the topN # items\n    keywords_dict=extract_topn_from_vector(feature_names,sorted_items,topN)\n    #just want words themselves, so only need keys of the dictionary\n    keywords = list(keywords_dict.keys()) \n     \n    return keywords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getTitleKeywords(entry):\n    title = entry['title']  \n    title = preprocess(title)\n    #first check that the title of that entry is full\n    if type(title) == float:\n        return []\n    \n    keywords_title = title.split(' ')\n    return keywords_title","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getFinalKeywords(entry, cv, X, tfidf_trans, feature_names, topN):\n    #get keywords from abstract and title\n    fromAbstract = getAbstractKeywords(entry, cv, X, tfidf_trans, feature_names, topN)\n    fromTitle = getTitleKeywords(entry)\n    #concatenate two lists\n    finalKeywords = fromAbstract + fromTitle\n    #convert to set and then back to list to ensure there are no duplicates in list\n    final_no_duplicates = list(set(finalKeywords))\n    return final_no_duplicates","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getCorpus(articlesDf):\n    #creating a new dataframe, abstractDf, of just the abstracts, so that we don't modify the original dataframe, articlesDf\n    abstractDf = pd.DataFrame(columns = ['abstract'])\n    #filling abstractDf with the abstract column from articlesDf\n    abstractDf['abstract'] = articlesDf['abstract']\n    #converting column of dataframe to a list\n    corpus = abstractDf['abstract'].to_list()\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def addKeywords(df, topN, makeFile, fileName):\n    #defining stopwords\n    stop_words = set(stopwords.words(\"english\"))\n\n    #creating following variables that are needed for keyword extract from abstract, using tf-idf methodology,\n    #all input in getFinalKewords method\n    corpus = getCorpus(df)\n    cv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=1000, ngram_range=(1,1))    \n    X=cv.fit_transform(corpus)\n    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n    tfidf_transformer.fit(X)\n    feature_names=cv.get_feature_names()\n    \n    #adding keywords article to dataframe\n    df = df.reindex(columns = ['paper_id', 'title', 'abstract','keywords'])                \n    #getting keywords for each entry in article dataframe -- using apply to be more efficient\n    df['keywords'] = df.apply(lambda row: getFinalKeywords(row, cv, X, tfidf_transformer, feature_names, topN), axis=1)\n\n    #make pickle file depending on user input\n    if makeFile == True:\n        pickle.dump( df, open( fileName, \"wb\" ) )\n    return df  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def createInvertedIndices(df):\n    numEntries = df.shape[0]\n    invertInd = {}\n    \n    for i in range (numEntries):\n        entry = df.iloc[i]\n        paper_id = entry['paper_id']    \n        keywords = entry['keywords']\n        for k in keywords:\n            if k not in invertInd:\n                invertInd[k] = []\n                invertInd[k].append(paper_id)\n            else:\n                invertInd[k].append(paper_id)\n    return invertInd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def organize():\n    df_without_keywords = pickle.load(open(\"full_data_processed_FINAL.p\", \"rb\"))\n    df_with_keywords = addKeywords(df_without_keywords, 10, False, \"full_data_withKeywords_FINAL.p\")\n    invertedIndices = createInvertedIndices(df_with_keywords)\n    pickle.dump( invertedIndices, open( \"invertedIndices_FINAL.p\", \"wb\" ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getPotentialArticleSubset(query):\n    #load in inverted indices\n    invertedIndices = pickle.load(open(\"invertedIndices_FINAL.p\", \"rb\"))\n    \n    #preprocess query and split into individual terms\n    query = preprocess(query)\n    queryTerms = query.split(' ')\n    \n    potentialArticles = []\n    #concatenate list of potential articles by looping through potential articles for each word in query\n    for word in queryTerms:\n        if word in invertedIndices: #so if someone types in nonsensical query term that's not in invertedIndices, still won't break!\n            someArticles = invertedIndices[word]\n            potentialArticles = potentialArticles + someArticles\n            \n    #convert to set then back to list so there are no repeat articles\n    potentialArticles = list(set(potentialArticles))\n    return potentialArticles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bm25(articles, df_dic, title_w, abstract_w, query):\n    corpus_title = []\n    corpus_abstract = []\n    \n    for article in articles:\n        arr = df_dic.get(article)\n        #title\n        if type(arr[0]) != float:\n            preprocessedTitle = preprocess(arr[0])\n            corpus_title.append(preprocessedTitle)\n        else:\n            corpus_title.append(\" \")\n        \n        #abstract\n        if type(arr[1]) != float:\n            preprocessedAbst = preprocess(arr[1])\n            corpus_abstract.append(preprocessedAbst)\n        else:\n            corpus_abstract.append(\" \")\n            \n    query = preprocess(query)\n    \n    tokenized_query = query.split(\" \")\n    \n    tokenized_corpus_title = [doc.split(\" \") for doc in corpus_title]\n    tokenized_corpus_abstract = [doc.split(\" \") for doc in corpus_abstract]\n    \n    #running bm25 on titles\n    bm25_title = BM25Okapi(tokenized_corpus_title)\n    doc_scores_titles = bm25_title.get_scores(tokenized_query)\n    #weighting array\n    doc_scores_titles = np.array(doc_scores_titles)\n    doc_scores_titles = doc_scores_titles**title_w\n    \n    #running bm25 on abstracts\n    bm25_abstract = BM25Okapi(tokenized_corpus_abstract)\n    doc_scores_abstracts = bm25_abstract.get_scores(tokenized_query)\n    #weighting\n    doc_scores_abstracts = np.array(doc_scores_abstracts)\n    doc_scores_abstracts = doc_scores_abstracts ** abstract_w\n    \n    #summing up the two different scores\n    doc_scores = np.add(doc_scores_abstracts,doc_scores_titles)\n    \n    #creating a dictionary with the scores\n    score_dict = dict(zip(articles, doc_scores))\n    \n    #creating list of ranked documents high to low\n    doc_ranking = sorted(score_dict, key=score_dict.get, reverse = True)\n    \n    #get top 100\n    doc_ranking = doc_ranking[0:100]\n    \n    for i in range(len(doc_ranking)):\n        dic_entry = df_dic.get(doc_ranking[i])\n        doc_ranking[i] = dic_entry[0]\n    \n    return doc_ranking","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def retrieve(queries):\n    #performing information retrieval\n    df_without_keywords = pickle.load(open(\"full_data_processed_FINAL.p\", \"rb\"))\n    df_dic = df_without_keywords.set_index('paper_id').T.to_dict('list')\n    results = []\n    for q in queries:\n        articles = getPotentialArticleSubset(q)\n        result = bm25(articles,df_dic,1,2,q)\n        results.append(result)\n\n    #Output results\n    for query in range(len(results)):\n        for rank in range(len(results[query])):\n            print(str(query+1)+'\\t'+str(rank+1)+'\\t'+str(results[query][rank]))\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"getData()\nextract()\norganize()\nq = ['coronavirus origin',\n'coronavirus response to weather changes',\n'coronavirus immunity']\nretrieve(q)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}