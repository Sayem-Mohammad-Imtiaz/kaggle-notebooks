{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"! nvidia-smi","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Imports\nThe following packages are imported:\n- tensorflow\n- matplotlib\n- numpy\n- IPython\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import wandb\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nimport IPython","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data\n- data_dir is the path to the images and the `results.csv`\n- image_dir is the path exculsively to the images\n- csv_file is the path to the `results.csv` file"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = '../input/flickr-image-dataset/flickr30k_images'\nimage_dir = f'{data_dir}/flickr30k_images'\ncsv_file = f'{data_dir}/results.csv'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we read the csv file as a dataframe and make some observations from it.\nFor a quick EDA we are going to \n- check the shape of the dataframe\n- check the names of the columns\n- find out the unique image names there are"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(csv_file, delimiter='|')\n\nprint(f'[INFO] The shape of dataframe: {df.shape}')\nprint(f'[INFO] The columns in the dataframe: {df.columns}')\nprint(f'[INFO] Unique image names: {len(pd.unique(df[\"image_name\"]))}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A quick observation here is to see that the dataframe has `158915` elements but only `31783` image names. This means that there is a duplicacy involved. On further inspection we will see that each image has 5 unique captions attached to it ($31783\\times 5=158915$)\n\nWhile looking into the dataframe I found out that `19999` had some messed up entries. This has led me to manually change the entries in that row."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns = ['image_name', 'comment_number', 'comment']\ndel df['comment_number']\n\n# Under scrutiny I had found that 19999 had a messed up entry\ndf['comment'][19999] = ' A dog runs across the grass .'\n\n# Image names now correspond to the absolute position\ndf['image_name'] = image_dir+'/'+df['image_name']\n\n# <start> comment <end>\ndf['comment'] = \"<start> \"+df['comment']+\" <end>\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shuffle the dataframe\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SIZE = len(df)\n\n# train_size = int(0.7* SIZE) \n# val_size = int(0.1* SIZE)\n# test_size = int(0.2* SIZE)\n\n# train_size, val_size, test_size\n\ntrain_size = 60_000 \nval_size = 10_000\ntest_size = 20_000","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting the dataframe accordingly"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = df.iloc[:train_size,:]\nval_df = df.iloc[train_size:train_size+val_size,:]\ntest_df = df.iloc[train_size+val_size:train_size+val_size+test_size,:]\n\ntrain_df.shape, val_df.shape, test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Enter different indices.\nindex = 200\n\nimage_name = train_df['image_name'][index]\ncomment = train_df['comment'][index]\n\nprint(comment)\n\nIPython.display.Image(filename=image_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Handling\n- Defined the size of the vocab which is `5000`.\n- Initialized the Tokenizer class.\n    - Standardized (all to lower case)\n    - Filters the punctuations\n    - Splits the text\n    - Creates the vocabulary (`<start>, <end> and <unk>` is defined)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Choose the top 10000 words from the vocabulary\ntop_k = 10000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we fit the `tokenizer` object on the captions. This helps in the updation of the vocab that the `tokenizer` object might have.\n\nIn the first iteration the vocabulary does not start from `0`. Both the dictionaries have 1 as the key or value."},{"metadata":{"trusted":true},"cell_type":"code","source":"# build the vocabulary\ntokenizer.fit_on_texts(train_df['comment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This is a sanity check function\ndef check_vocab(word):\n    i = tokenizer.word_index[word]\n    print(f\"The index of the word: {i}\")\n    print(f\"Index {i} is word {tokenizer.index_word[i]}\")\n    \ncheck_vocab(\"pajama\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we are padding the sentences so that each of the sentences are of the same length."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(train_df['comment'])\nval_seqs = tokenizer.texts_to_sequences(val_df['comment'])\ntest_seqs = tokenizer.texts_to_sequences(test_df['comment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ntrain_cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\nval_cap_vector = tf.keras.preprocessing.sequence.pad_sequences(val_seqs, padding='post')\ntest_cap_vector = tf.keras.preprocessing.sequence.pad_sequences(test_seqs, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Caption vector\ntrain_cap_vector.shape, val_cap_vector.shape, test_cap_vector.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cap_ds = tf.data.Dataset.from_tensor_slices(train_cap_vector)\nval_cap_ds = tf.data.Dataset.from_tensor_slices(val_cap_vector)\ntest_cap_ds = tf.data.Dataset.from_tensor_slices(test_cap_vector)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Handling\n- Load the image\n- decode jpeg\n- resize\n- standardize"},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef load_img(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (299, 299))\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_img_name = train_df['image_name'].values\nval_img_name = val_df['image_name'].values\ntest_img_name = test_df['image_name'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_img_ds = tf.data.Dataset.from_tensor_slices(train_img_name).map(load_img)\nval_img_ds = tf.data.Dataset.from_tensor_slices(val_img_name).map(load_img)\ntest_img_ds = tf.data.Dataset.from_tensor_slices(test_img_name).map(load_img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Joint data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prefecth and batch the dataset\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 512\n\ntrain_ds = tf.data.Dataset.zip((train_img_ds, train_cap_ds)).batch(BATCH_SIZE,drop_remainder=True).prefetch(buffer_size=AUTOTUNE)\nval_ds = tf.data.Dataset.zip((val_img_ds, val_cap_ds)).batch(BATCH_SIZE,drop_remainder=True).prefetch(buffer_size=AUTOTUNE)\ntest_ds = tf.data.Dataset.zip((test_img_ds, test_cap_ds)).batch(BATCH_SIZE,drop_remainder=True).prefetch(buffer_size=AUTOTUNE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sanity check for the division of datasets"},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some global variables\nEMBEDDING_DIM = 512\nVOCAB_SIZE = top_k+1\nUNITS = 256\nKERNEL = 64\nFEATURES = 2048","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CNN_Encoder(tf.keras.Model):\n    \n    def __init__(self, embedding_dim, batch_size):\n        super(CNN_Encoder, self).__init__()\n        self.batch_size = batch_size\n        self.embedding_dim = embedding_dim\n        \n    def build(self, input_shape):\n        self.image_model = tf.keras.applications.InceptionV3(include_top=False,\n                                                weights='imagenet')\n        self.new_input = self.image_model.input\n        self.hidden_layer = self.image_model.layers[-1].output\n        self.image_features_extract_model = tf.keras.Model(self.new_input, self.hidden_layer)\n        self.image_features_extract_model.trainable = False\n        \n        self.reshape = tf.keras.layers.Reshape(target_shape=(KERNEL,FEATURES))\n        self.fc = Dense(units=self.embedding_dim,\n                        activation='relu')\n        \n    def call(self, x):\n        x = self.image_features_extract_model(x)\n        x = self.reshape(x)\n        x = self.fc(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the encoder\nencoder = CNN_Encoder(EMBEDDING_DIM, BATCH_SIZE)\nfor image, caption in train_ds.take(1):\n    features = encoder(image)\n    print(f\"ENCODER OUTPUT: {features.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BahdanauAttention(tf.keras.Model):\n    def __init__(self, units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, annotations, hidden):\n        #                                           64          64  256\n        # annotations(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n        #                  64          512\n        # hidden shape == (batch_size, units)\n        #                                 64          1  512\n        # hidden_with_time_axis shape == (batch_size, 1, units)\n        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n        #                                  64          64  512\n        # attention_hidden_layer shape == (batch_size, 64, units)\n        attention_hidden_layer = (tf.nn.tanh(self.W1(annotations) +\n                                             self.W2(hidden_with_time_axis)))\n        #                 64          64  1\n        # score shape == (batch_size, 64, 1)\n        # This gives you an unnormalized score for each image feature.\n        score = self.V(attention_hidden_layer)\n        #                             64          64  1\n        # attention_weights shape == (batch_size, 64, 1)\n        attention_weights = tf.nn.softmax(score, axis=1)\n        #                          64          64  256\n        # context_vector shape == (batch_size, 64, embedding_dim)\n        #                                    64          256\n        # context_vector shape after sum == (batch_size, embedding_dim)\n        context_vector = attention_weights * annotations\n        context_vector = tf.reduce_sum(context_vector, axis=1) # thinking: average?\n\n        return context_vector, attention_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n    def __init__(self, embedding_dim, units, vocab_size, batch_size):\n        super(RNN_Decoder, self).__init__()\n        self.batch_size = batch_size\n        self.units = units\n\n        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n        self.gru = tf.keras.layers.GRU(self.units,\n                                       recurrent_initializer='glorot_uniform')\n        # self.fc1 = tf.keras.layers.Dense(self.units)\n        self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n        self.attention = BahdanauAttention(self.units)\n\n    def call(self, x, annotations, hidden):\n        # defining attention as a separate model\n        #                          64     256\n        # context_vector shape == (batch, embedding_shape)\n        #                             64     64  1\n        # attention_weights shape == (batch, 64, 1)\n        context_vector, attention_weights = self.attention(annotations, hidden)\n        #                                            64           1  256\n        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n        x = self.embedding(x)\n        #                                 64          1  256+256\n        # x shape after concatenation == (batch_size, 1, embedding_dim + embedding_shape)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n        # passing the concatenated vector to the GRU\n        output = self.gru(x)\n        state = output\n        #           64          1  512\n        # shape == (batch_size, 1, units)\n        x = self.fc2(output)\n        # #            64               512\n        # # x shape == (batch_size * 1, units)\n        # x = tf.reshape(x, (-1, x.shape[2]))\n        # #                  64              vocab\n        # # output shape == (batch_size * 1, vocab)\n        # x = self.fc2(x)\n\n        return x, output, attention_weights\n\n    def reset_state(self):\n        return tf.zeros((self.batch_size, self.units))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the decoder\nencoder = CNN_Encoder(EMBEDDING_DIM, BATCH_SIZE)\ndecoder = RNN_Decoder(EMBEDDING_DIM, UNITS, VOCAB_SIZE, BATCH_SIZE)\n\nfor image, caption in train_ds.take(1):\n    features = encoder(image)\n    print(f\"ENCODER OUTPUT: {features.shape}\")\n    hidden = decoder.reset_state()\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * caption.shape[0], 1)\n    predictions, hidden, attn_weights = decoder(dec_input, features, hidden)\n    print(f\"PREDICTION: {predictions.shape}\")\n    print(f\"HIDDEN: {hidden.shape}\")\n    print(f\"ATTENTION: {attn_weights.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Wrapping the Gradient Tape in Model Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"for image, caption in train_ds.take(1):\n    print(image.shape)\n    print(caption.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Image_Caption_Gen(tf.keras.Model):\n    def __init__(self, encoder, decoder):\n        super(Image_Caption_Gen, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def train_step(self, data):\n        img_tensor, target = data\n        \n        loss = 0\n        \n        # initializing the hidden state for each batch\n        # because the captions are not related from image to image\n        hidden = self.decoder.reset_state()\n        \n        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n        \n        with tf.GradientTape() as tape:\n            features = self.encoder(img_tensor)\n            \n            for i in range(1, target.shape[1]):\n                # passing the features through the decoder\n                predictions, hidden, _ = self.decoder(dec_input, features, hidden)\n                \n                loss += loss_function(target[:, i], predictions)\n                \n                # using teacher forcing\n                dec_input = tf.expand_dims(target[:, i], 1)\n                \n        total_loss = (loss / int(target.shape[1]))\n        trainable_variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n        gradients = tape.gradient(loss, trainable_variables)\n        optimizer.apply_gradients(zip(gradients, trainable_variables))\n        return {\"custom_loss\": total_loss}\n    \n    def test_step(self, data):\n        img_tensor, target = data\n        \n        loss = 0\n        \n        # initializing the hidden state for each batch\n        # because the captions are not related from image to image\n        hidden = self.decoder.reset_state()\n        \n        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n        \n        features = self.encoder(img_tensor)\n            \n        for i in range(1, target.shape[1]):\n            # passing the features through the decoder\n            predictions, hidden, _ = self.decoder(dec_input, features, hidden)\n\n            loss += loss_function(target[:, i], predictions)\n\n            # using teacher forcing\n            dec_input = tf.expand_dims(target[:, i], 1)\n                \n        total_loss = (loss / int(target.shape[1]))\n        return {\"custom_loss\": total_loss}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use `Adam` as the optimizer.\n\nThe loss is `SparseCategoricalCrossentropy`, because here it would be inefficient to use one-hot-encoders are the ground truth. We will also use mask to help mask the `<pad>` so that we do not let the sequence model learn to overfit on the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Early Stopping to prevent overfitting\nes = tf.keras.callbacks.EarlyStopping(monitor=\"val_custom_loss\", patience=5, verbose=2, restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS=100\n# Test the decoder\nencoder = CNN_Encoder(EMBEDDING_DIM, BATCH_SIZE)\ndecoder = RNN_Decoder(EMBEDDING_DIM, UNITS, VOCAB_SIZE, BATCH_SIZE)\n\noptimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n\nmain_model = Image_Caption_Gen(encoder, decoder)\nmain_model.compile(loss=loss_function, optimizer=optimizer)\n\nhistory = main_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    callbacks = [es],\n    epochs=EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_test_loss = main_model.evaluate(test_ds)\nprint(f'[INFO] Test Loss: {custom_test_loss}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install wandb -qqq\nimport wandb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run = wandb.init(entity=\"authors\",\n                 project=\"under-attention\",\n                 group=\"Show_Attend_Tell\",\n                 name=\"SAT-Baseline\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history.history[\"custom_loss\"], label=\"train_loss\")\nplt.plot(history.history[\"val_custom_loss\"], label=\"val_loss\")\nplt.title(\"Loss vs. Epoch\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\")\nplt.legend(loc=\"lower left\")\n\nplt.savefig(\"loss.png\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"run.log({\"Loss\":wandb.Image(\"loss.png\")})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the weights of the model for better reproducibility\nmain_model.encoder.save_weights(\"encoder.h5\")\nmain_model.decoder.save_weights(\"decoder.h5\")\n\nartifact = wandb.Artifact('model-weights', type='model')\n\n# Add a file to the artifact's contents\nartifact.add_file('encoder.h5')\nartifact.add_file('decoder.h5')\n\n# Save the artifact version to W&B and mark it as the output of this run\nrun.log_artifact(artifact)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Catpions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test the decoder\nencoder = CNN_Encoder(EMBEDDING_DIM, 1)\ndecoder = RNN_Decoder(EMBEDDING_DIM, UNITS, VOCAB_SIZE, 1)\n\nfor image, caption in train_ds.take(1):\n    features = encoder(tf.expand_dims(image[1],0))\n    print(f\"ENCODER OUTPUT: {features.shape}\")\n    hidden = decoder.reset_state()\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 1)\n    predictions, hidden, attn_weights = decoder(dec_input, features, hidden)\n    print(f\"PREDICTION: {predictions.shape}\")\n    print(f\"HIDDEN: {hidden.shape}\")\n    print(f\"ATTENTION: {attn_weights.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder.load_weights(\"encoder.h5\")\ndecoder.load_weights(\"decoder.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(image):\n    #                          max_length  64\n    attention_plot = np.zeros((72, KERNEL))\n\n    hidden = decoder.reset_state()\n\n    img = tf.expand_dims(load_img(image), 0)\n    features = encoder(img)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(72):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_attention(image, result, attention_plot):\n    temp_image = np.array(Image.open(image))\n\n    fig = plt.figure(figsize=(10, 10))\n\n    len_result = len(result)\n    for l in range(len_result):\n        temp_att = np.resize(attention_plot[l], (8, 8))\n        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n        ax.set_title(result[l])\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_url = 'https://tensorflow.org/images/surf.jpg'\nimage_extension = image_url[-4:]\nimage_path = tf.keras.utils.get_file('image'+image_extension,\n                                     origin=image_url)\n\nresult, attention_plot = evaluate(image_path)\nprint ('Prediction Caption:', ' '.join(result))\nplot_attention(image_path, result, attention_plot)\n# opening the image\nImage.open(image_path)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}