{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dat=pd.read_csv(\"../input/company-bankruptcy-prediction/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"t=[]\nfor i in range(len(dat)):     #Implementing id column which will help us during merging\n    t.append(i)\n\ndat['id']=t\ndat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nn,bins,patches=plt.hist(x=dat['Bankrupt?'],bins='auto',color='#0504aa',alpha=0.7, rwidth=0.85)\nplt.grid(axis='y',alpha=0.75)\nplt.xlabel(\"value\")\nplt.ylabel(\"frequency\")\nplt.title(\"Class imbalance check!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\ncnt_pos=0;cnt_neg=0\nfor i in dat['Bankrupt?']:\n    if i:\n        cnt_pos+=1\n    else:\n        cnt_neg+=1\nspw = math.sqrt(cnt_pos/cnt_neg) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of the dataset: {0}\".format(dat.shape))\ndat.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"## Checking NAs ##\\n\")\ndat.isna().sum()   #High dimensional dataset, so use for loop\nfor i in dat.columns:\n    print(\"NA count for *{0}* is {1}\".format(i,dat[dat[i].isnull()].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**EDA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"## Checking correlation ##\\n\")\n\nimport seaborn as sns \nsns.set_theme(style=\"white\")\ncorr=dat.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(50, 20))\ncmap = sns.diverging_palette(200, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot = dat.boxplot(grid=False, column=[' ROA(C) before interest and depreciation before interest', \n                              ' Tax rate (A)', ' Net Income to Total Assets'],rot=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dat[['Bankrupt?','id']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outlier Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\ny=dat[['Bankrupt?','id']]; y=y.set_index('id')\nX=dat.iloc[:,1:];X=X.set_index('id')\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.33, random_state=1)\nprint(X_train.shape, y_train.shape)\nee = EllipticEnvelope(contamination=0.01)  #Clean the outliers using EllipticEnvelope\nyhat = ee.fit_predict(X_train)\nmask = yhat != -1\nX_train, y_train = X_train.loc[mask, :], y_train.loc[mask]\nprint(X_train.shape, y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dat=X_train.merge(y_train,how='inner',on='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dat2=X_test.merge(y_test,how='inner',on='id')\nnew_dat2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dat3=pd.concat([new_dat,new_dat2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dat3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n,bins,patches=plt.hist(x=new_dat3[' Net Income Flag'],bins=[0.5,0.75,0.90,1,1.25,1.5],color='#0504aa',alpha=0.7, rwidth=0.85)\nplt.grid(axis='y',alpha=0.75)\nplt.xlabel(\"value\")\nplt.ylabel(\"frequency\")\nplt.title(\"What is Net Income Flag\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dat3[' Net Income Flag'].unique()  #So we can drop it!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dat3.drop([' Net Income Flag'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(new_dat3.shape[1]):\n    x_mean=new_dat3.iloc[:,i].mean()\n    y_std=new_dat3.iloc[:,i].std()\n    print(\"For column {0}-> Mean={1}, Std={2}\".format(i,x_mean,y_std))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n,bins,patches=plt.hist(x=(new_dat3.iloc[:,30]),color='#0504aa',alpha=0.7, rwidth=0.85)\nplt.grid(axis='y',alpha=0.75)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"count=0\nfor i in new_dat3.std()<0.01:\n    if i:\n        count+=1\nprint(count)   #Columns which have less than 0.01 stdeviation (won't actually help model to predict!)\n\ncount2=0\nfor i in new_dat3.std()<0.015:\n    if i:\n        count2+=1\nprint(count2) #Columns which have less than 0.015 stdeviation (difference seems interesting)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dat4=new_dat3.loc[:,new_dat3.std()>0.01]\nnew_dat4.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(new_dat4.shape[1]):\n    x_mean=new_dat4.iloc[:,i].mean()\n    y_std=new_dat4.iloc[:,i].std()\n    print(\"For column {0}-> Mean={1}, Std={2}\".format(i,x_mean,y_std))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sc_set=[10,11,13,20,28,29,32,44,45,46,47,49,51,56,57,61,65,69,70,71,72,83]  #To scale down these columns\n\nfor i in sc_set:\n    dum_t=new_dat4.iloc[:,i]\n    dum=[];\n    for k in dum_t:\n        dum.append((k-dum_t.min())/(dum_t.max()-dum_t.min()))   #You can use other than min-max scaling\n                                                                #But be careful while using log transformation\n    \n    new_dat4.iloc[:,i]=dum\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(new_dat4.shape[1]):\n    x_mean=new_dat4.iloc[:,i].mean()\n    y_std=new_dat4.iloc[:,i].std()\n    print(\"For column {0}-> Mean={1}, Std={2}\".format(i,x_mean,y_std))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dat4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Min-max scaling may have misinterpreted column 83(Total assets to GNP price) (maybe there were outliers), check!*"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dat4=new_dat4.sort_values(by='id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(new_dat4[' Total assets to GNP price'], bins=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dat4=new_dat4.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_dat4.drop(['id'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize\nfrom scipy import special\nfrom sklearn import metrics\nfrom sklearn import model_selection\n\nclass FocalLoss:\n\n    def __init__(self, gamma, alpha=None):\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def at(self, y):\n        if self.alpha is None:\n            return np.ones_like(y)\n        return np.where(y, self.alpha, 1 - self.alpha)\n\n    def pt(self, y, p):\n        p = np.clip(p, 1e-15, 1 - 1e-15)\n        return np.where(y, p, 1 - p)\n\n    def __call__(self, y_true, y_pred):\n        at = self.at(y_true)\n        pt = self.pt(y_true, y_pred)\n        return -at * (1 - pt) ** self.gamma * np.log(pt)\n\n    def grad(self, y_true, y_pred):\n        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n        at = self.at(y_true)\n        pt = self.pt(y_true, y_pred)\n        g = self.gamma\n        return at * y * (1 - pt) ** g * (g * pt * np.log(pt) + pt - 1)\n\n    def hess(self, y_true, y_pred):\n        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n        at = self.at(y_true)\n        pt = self.pt(y_true, y_pred)\n        g = self.gamma\n\n        u = at * y * (1 - pt) ** g\n        du = -at * y * g * (1 - pt) ** (g - 1)\n        v = g * pt * np.log(pt) + pt - 1\n        dv = g * np.log(pt) + g + 1\n\n        return (du * v + u * dv) * y * (pt * (1 - pt))\n\n    def init_score(self, y_true):\n        res = optimize.minimize_scalar(\n            lambda p: self(y_true, p).sum(),\n            bounds=(0, 1),\n            method='bounded'\n        )\n        p = res.x\n        log_odds = np.log(p / (1 - p))\n        return log_odds\n\n    def lgb_obj(self, preds, train_data):\n        y = train_data.get_label()\n        p = special.expit(preds)\n        return self.grad(y, p), self.hess(y, p)\n\n    def lgb_eval(self, preds, train_data):\n        y = train_data.get_label()\n        p = special.expit(preds)\n        is_higher_better = False\n        return 'focal_loss', self(y, p).mean(), is_higher_better","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import model_selection\ny=new_dat4[['Bankrupt?']]\ny=y['Bankrupt?']\nX=new_dat4.iloc[:,0:91]\nfl = FocalLoss(alpha=None, gamma=0)\nX_train, X_test, y_train, y_test=model_selection.train_test_split(X, y, random_state=1)\n\nX_fit, X_val, y_fit, y_val = model_selection.train_test_split(\n    X_train, y_train,\n    random_state=42\n)\n\nfit = lightgbm.Dataset(\n    X_fit, y_fit,\n    init_score=np.full_like(y_fit, fl.init_score(y_fit), dtype=float)\n)\n    \nval = lightgbm.Dataset(\n    X_val, y_val,\n    init_score=np.full_like(y_val, fl.init_score(y_fit), dtype=float),\n    reference=fit\n)\n\nmodel = lightgbm.train(\n    params={'learning_rate': 0.01},\n    train_set=fit,\n    num_boost_round=10000,\n    valid_sets=(fit, val),\n    valid_names=('fit', 'val'),\n    early_stopping_rounds=20,\n    verbose_eval=100,\n    fobj=fl.lgb_obj,\n    feval=fl.lgb_eval\n)\n\ny_pred = special.expit(fl.init_score(y_fit) + model.predict(X_test))\n\nprint()\nprint(f\"Test's ROC AUC: {metrics.roc_auc_score(y_test, y_pred):.5f}\")\nprint(f\"Test's logloss: {metrics.log_loss(y_test, y_pred):.5f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tpot import TPOTClassifier\nfrom tpot.config import classifier_config_dict\nfrom sklearn.pipeline import make_pipeline\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfor csbt in [0.9,1]:\n    for mcw in [1,2]:\n        for lr in [0.4]:\n            for n in [120,200,300]:\n                for spw in [8,10,12]:\n                    xgb = XGBClassifier(scale_pos_weight = spw, n_estimators= n, colsample_bytree=csbt\n                                        ,eval_metric = \"logloss\",use_label_encoder=False)\n                    #xgb=xgb.fit(X_train,y_train)\n                    kf = KFold(shuffle=True, n_splits=5)\n                    print(\"mcwght: \",mcw ,\" l_r: \", lr  , \" n: \", n, \" spw: \", \n                          spw,\" csbt: \", csbt , \" auc: \",\n                          cross_val_score(xgb, X_test, y_test, cv=kf, scoring=\"roc_auc\").mean())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**/XGB Confusion matrix with the optimized parameters\\**\n*scale_pos_weight=1,n_estimators=300,colsample_bytree=0.9,learning_rate=0.1,min_child_weight = 1*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix \nxgb=XGBClassifier(scale_pos_weight=1,n_estimators=300,colsample_bytree=0.9,use_label_encoder=False,\n                  eval_metric = \"logloss\",learning_rate=0.1,min_child_weight = 1)\nxgb=xgb.fit(X_train,y_train)\n\nplot_confusion_matrix(xgb,X_test,y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}