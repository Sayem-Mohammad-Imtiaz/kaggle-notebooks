{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.utils import shuffle\nimport pandas as pd\nimport pickle\nfrom matplotlib.pyplot import MultipleLocator","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:51:34.58953Z","iopub.execute_input":"2021-07-02T08:51:34.589977Z","iopub.status.idle":"2021-07-02T08:51:41.746043Z","shell.execute_reply.started":"2021-07-02T08:51:34.589877Z","shell.execute_reply":"2021-07-02T08:51:41.745121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\n# This is the TPU initialization code that has to be at the beginning.\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:51:41.750013Z","iopub.execute_input":"2021-07-02T08:51:41.750427Z","iopub.status.idle":"2021-07-02T08:51:47.243724Z","shell.execute_reply.started":"2021-07-02T08:51:41.750399Z","shell.execute_reply":"2021-07-02T08:51:47.242838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.TPUStrategy(resolver)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:51:47.245334Z","iopub.execute_input":"2021-07-02T08:51:47.245623Z","iopub.status.idle":"2021-07-02T08:51:47.254708Z","shell.execute_reply.started":"2021-07-02T08:51:47.245596Z","shell.execute_reply":"2021-07-02T08:51:47.253398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"读入数据\"\"\"\ndata_A = pd.read_csv(r'../input/datasets-ymz/train_0.csv')\ndata_B = pd.read_csv(r'../input/datasets-ymz/test_0.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:51:47.256249Z","iopub.execute_input":"2021-07-02T08:51:47.256745Z","iopub.status.idle":"2021-07-02T08:53:45.430813Z","shell.execute_reply.started":"2021-07-02T08:51:47.256715Z","shell.execute_reply":"2021-07-02T08:53:45.429588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = data_A.iloc[:105000,:]\nprint(len(train_data))\n\nvalidation_data = data_A.iloc[105000:,:]\nprint(len(validation_data))\n\ntest_data = data_B.iloc[:,:]\nprint(len(test_data))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.436033Z","iopub.execute_input":"2021-07-02T08:53:45.436509Z","iopub.status.idle":"2021-07-02T08:53:45.446039Z","shell.execute_reply.started":"2021-07-02T08:53:45.436454Z","shell.execute_reply":"2021-07-02T08:53:45.44508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"加载数据集train_data\"\"\"\nTrain_label_Sbp = train_data.iloc[:,1800]\nTrain_label_Sbp = Train_label_Sbp.values\n\nTrain_label_Dbp = train_data.iloc[:,1801]\nTrain_label_Dbp = Train_label_Dbp.values\n\nTrain_Dev_One = train_data.iloc[:,600:1200]\nTrain_Dev_One = Train_Dev_One.values\n\nTrain_Dev_Two = train_data.iloc[:,1200:1800]\nTrain_Dev_Two = Train_Dev_Two.values\n\nTrain_Data = train_data.iloc[:,:600]\nTrain_Data = Train_Data.values","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.468736Z","iopub.execute_input":"2021-07-02T08:53:45.469043Z","iopub.status.idle":"2021-07-02T08:53:45.484324Z","shell.execute_reply.started":"2021-07-02T08:53:45.469013Z","shell.execute_reply":"2021-07-02T08:53:45.483124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"加载数据集validation_data\"\"\"\nValidation_label_Sbp = validation_data.iloc[:,1800]\nValidation_label_Sbp = Validation_label_Sbp.values\n\nValidation_label_Dbp = validation_data.iloc[:,1801]\nValidation_label_Dbp = Validation_label_Dbp.values\n\nValidation_Dev_One = validation_data.iloc[:,600:1200]\nValidation_Dev_One = Validation_Dev_One.values\n\nValidation_Dev_Two = validation_data.iloc[:,1200:1800]\nValidation_Dev_Two = Validation_Dev_Two.values\n\nValidation_Data = validation_data.iloc[:,:600]\nValidation_Data = Validation_Data.values","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.517119Z","iopub.execute_input":"2021-07-02T08:53:45.51757Z","iopub.status.idle":"2021-07-02T08:53:45.529514Z","shell.execute_reply.started":"2021-07-02T08:53:45.517523Z","shell.execute_reply":"2021-07-02T08:53:45.52879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"数据格式、尺寸\"\"\"\nprint(\"data_information:\")\nprint(Train_Data.shape)\nprint(Train_Dev_One.shape)\nprint(Train_Dev_Two.shape)\nprint(Train_label_Sbp.shape)\nprint(Train_label_Dbp.shape)\n\nprint(\"data_information:\")\nprint(Validation_Data.shape)\nprint(Validation_Dev_One.shape)\nprint(Validation_Dev_Two.shape)\nprint(Validation_label_Sbp.shape)\nprint(Validation_label_Dbp.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.530441Z","iopub.execute_input":"2021-07-02T08:53:45.530687Z","iopub.status.idle":"2021-07-02T08:53:45.548438Z","shell.execute_reply.started":"2021-07-02T08:53:45.530662Z","shell.execute_reply":"2021-07-02T08:53:45.547045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"注意力机制模块\"\"\"\n#---------------------------------------------------------------------------------------\n#---------------------------------------------------------------------------------------\n# 注意力机制：Relu6方式\n# 激活函数 relu6\ndef relu6(x):\n    return tf.keras.activations.relu(x, max_value=6)\n#   利用relu函数乘上x模拟sigmoid\ndef hard_swish(x):\n    return x * tf.keras.activations.relu(x + 3.0, max_value=6.0) / 6.0\n\n#---------------------------------------#\n#   通道注意力机制单元\n#   利用两次全连接算出每个通道的比重\n#   可以连接在任意特征层后面\n#---------------------------------------#\n\ndef squeeze(inputs):\n    input_channels = int(inputs.shape[-1])\n    \n    x = layers.GlobalAveragePooling1D()(inputs)\n\n    x = layers.Dense(int(input_channels/4))(x)\n    x = relu6(x)\n\n    x = layers.Dense(input_channels)(x)\n    x = hard_swish(x)\n\n    x = layers.Reshape((1, input_channels))(x)\n    #print(x)\n    #print(inputs)\n    x = layers.Multiply()([inputs, x])\n    return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=32\ndef create_model():\n    inputs = keras.Input(shape=(256,1))\n\n    conv1 = layers.Conv1D(x*2,3,padding='same')(inputs)\n    conv1 = layers.BatchNormalization()(conv1)\n    conv1 = layers.Activation(tf.nn.relu)(conv1)\n\n    conv1 = layers.Conv1D(x*2,3,padding='same')(conv1)\n    conv1 = layers.BatchNormalization()(conv1)\n    conv1 = layers.Activation(tf.nn.relu)(conv1)\n\n    pool1 = layers.AveragePooling1D(pool_size=2)(conv1)\n    \n    conv2 = layers.Conv1D(x*4,3,padding='same')(pool1)\n    conv2 = layers.BatchNormalization()(conv2)\n    conv2 = layers.Activation(tf.nn.relu)(conv2)\n\n    conv2 = layers.Conv1D(x*4,3,padding='same')(conv2)\n    conv2 = layers.BatchNormalization()(conv2)\n    conv2 = layers.Activation(tf.nn.relu)(conv2)\n\n    pool2 = layers.AveragePooling1D(pool_size=2)(conv2)\n    \n    \n    conv3 = layers.Conv1D(x*8,3,padding='same')(pool2)\n    conv3 = layers.BatchNormalization()(conv3)\n    conv3 = layers.Activation(tf.nn.relu)(conv3)\n\n    conv3 = layers.Conv1D(x*8,3,padding='same')(conv3)\n    conv3 = layers.BatchNormalization()(conv3)\n    conv3 = layers.Activation(tf.nn.relu)(conv3)\n\n    pool3 = layers.AveragePooling1D(pool_size=2)(conv3)\n    \n    \n    conv4 = layers.Conv1D(x*16,3,padding='same')(pool3)\n    conv4 = layers.BatchNormalization()(conv4)\n    conv4 = layers.Activation(tf.nn.relu)(conv4)\n\n    conv4 = layers.Conv1D(x*16,3,padding='same')(conv4)\n    conv4 = layers.BatchNormalization()(conv4)\n    conv4 = layers.Activation(tf.nn.relu)(conv4)\n\n    pool4 = layers.AveragePooling1D(pool_size=2)(conv4)\n    \n\n    \n    conv5 = layers.Conv1D(x*32,3,padding='same')(pool4)\n    conv5 = layers.BatchNormalization()(conv5)\n    conv5 = layers.Activation(tf.nn.relu)(conv5)\n\n    conv5 = layers.Conv1D(x*32,3,padding='same')(conv5)\n    conv5 = layers.BatchNormalization()(conv5)\n    conv5 = layers.Activation(tf.nn.relu)(conv5)\n    \n    conv5 = layers.UpSampling1D(size=2)(conv5)\n    conv5 = layers.ZeroPadding1D((0,1))(conv5)\n    \n    conv5 = layers.Conv1D(x*16,2,padding='same')(conv5)\n    \n    conv4 = squeeze(conv4)\n    \n    up6 = layers.Concatenate(axis=2)([conv5,conv4])\n    \n    conv6 = layers.Conv1D(x*16,3,padding='same')(up6)\n    conv6 = layers.BatchNormalization()(conv6)\n    conv6 = layers.Activation(tf.nn.relu)(conv6)\n\n    conv6 = layers.Conv1D(x*16,3,padding='same')(conv6)\n    conv6 = layers.BatchNormalization()(conv6)\n    conv6 = layers.Activation(tf.nn.relu)(conv6)\n    \n\n    conv6 = layers.UpSampling1D(size=2)(conv6)\n    \n    conv6 = layers.Conv1D(x*8,2,padding='same')(conv6)\n    conv6 = layers.BatchNormalization()(conv6)\n    conv6 = layers.Activation(tf.nn.relu)(conv6)\n    \n    conv3 = squeeze(conv3)\n    \n    up7 = layers.Concatenate(axis=2)([conv6,conv3])\n\n    conv7 = layers.Conv1D(x*8,3,padding='same')(up7)\n    conv7 = layers.BatchNormalization()(conv7)\n    conv7 = layers.Activation(tf.nn.relu)(conv7)\n\n    conv7 = layers.Conv1D(x*8,3,padding='same')(conv7)\n    conv7 = layers.BatchNormalization()(conv7)\n    conv7 = layers.Activation(tf.nn.relu)(conv7)\n    \n    ####\n    \n    conv7 = layers.UpSampling1D(size=2)(conv7)\n    \n    conv7 = layers.Conv1D(x*4,2,padding='same')(conv7)\n    conv7 = layers.BatchNormalization()(conv7)\n    conv7 = layers.Activation(tf.nn.relu)(conv7)\n    \n    conv2 = squeeze(conv2)\n    \n    up8 = layers.Concatenate(axis=2)([conv7,conv2])\n\n    conv8 = layers.Conv1D(x*4,3,padding='same')(up8)\n    conv8 = layers.BatchNormalization()(conv8)\n    conv8 = layers.Activation(tf.nn.relu)(conv8)\n\n    conv8 = layers.Conv1D(x*4,3,padding='same')(conv8)\n    conv8 = layers.BatchNormalization()(conv8)\n    conv8 = layers.Activation(tf.nn.relu)(conv8)\n    \n    ####\n    \n    conv8 = layers.UpSampling1D(size=2)(conv8)\n    \n    conv8 = layers.Conv1D(x*2,2,padding='same')(conv8)\n    conv8 = layers.BatchNormalization()(conv8)\n    conv8 = layers.Activation(tf.nn.relu)(conv8)\n    \n    conv1 = squeeze(conv1)\n\n    up9 = layers.Concatenate(axis=2)([conv8,conv1])\n\n    conv9 = layers.Conv1D(x*2,3,padding='same')(up9)\n    conv9 = layers.BatchNormalization()(conv9)\n    conv9 = layers.Activation(tf.nn.relu)(conv9)\n\n    conv9 = layers.Conv1D(x*2,3,padding='same')(conv9)\n    conv9 = layers.BatchNormalization()(conv9)\n    conv9 = layers.Activation(tf.nn.relu)(conv9)\n\n    \n    com_layer = layers.GlobalAveragePooling1D()(conv9)\n    \n    \n    outputs_sbp = layers.Dense(1,name='Sbp')(com_layer)\n    outputs_dbp = layers.Dense(1,name='Dbp')(com_layer)\n\n    model = keras.Model(inputs=inputs,outputs=[outputs_sbp,outputs_dbp])\n    \n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"自定义评价指标模块\"\"\"\ndef standard_deviation(y_true, y_pred):\n    u = keras.backend.mean(abs(y_pred-y_true))\n    return keras.backend.sqrt(keras.backend.mean(keras.backend.square(abs(y_pred-y_true) - u)))","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.650139Z","iopub.execute_input":"2021-07-02T08:53:45.650941Z","iopub.status.idle":"2021-07-02T08:53:45.662721Z","shell.execute_reply.started":"2021-07-02T08:53:45.650893Z","shell.execute_reply":"2021-07-02T08:53:45.661815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"回调函数\"\"\"\n#保存迭代周期内最好的模型\ncheckpoint_filepath = r'./model_struction.h5'\nSave_epochs = 100 #迭代多少层保存一次模型\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    # save_weights_only=True,\n    monitor='val_Sbp_mean_absolute_error',\n    mode='min',\n    save_best_only=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.66397Z","iopub.execute_input":"2021-07-02T08:53:45.664426Z","iopub.status.idle":"2021-07-02T08:53:45.674472Z","shell.execute_reply.started":"2021-07-02T08:53:45.664387Z","shell.execute_reply":"2021-07-02T08:53:45.673448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"建立模型\"\"\"\nwith strategy.scope():\n    model = create_model()\n    \n    model.compile(loss={'Sbp':\"mse\",'Dbp':\"mse\"}, optimizer=keras.optimizers.Adam(lr=0.0001),metrics=[tf.keras.metrics.MeanAbsoluteError(),standard_deviation])","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:45.676005Z","iopub.execute_input":"2021-07-02T08:53:45.676286Z","iopub.status.idle":"2021-07-02T08:53:55.87603Z","shell.execute_reply.started":"2021-07-02T08:53:45.67626Z","shell.execute_reply":"2021-07-02T08:53:55.875256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:55.877354Z","iopub.execute_input":"2021-07-02T08:53:55.877922Z","iopub.status.idle":"2021-07-02T08:53:55.977215Z","shell.execute_reply.started":"2021-07-02T08:53:55.87788Z","shell.execute_reply":"2021-07-02T08:53:55.976432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"保存模型结构图片\"\"\"\ntf.keras.utils.plot_model(model, to_file=r'./model_graph.png', show_shapes=True, show_layer_names=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-02T08:53:55.9782Z","iopub.execute_input":"2021-07-02T08:53:55.978454Z","iopub.status.idle":"2021-07-02T08:54:00.654169Z","shell.execute_reply.started":"2021-07-02T08:53:55.978429Z","shell.execute_reply":"2021-07-02T08:54:00.652628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit({'inputs_1':Train_Data},{'Sbp':Train_label_Sbp,'Dbp':Train_label_Dbp},\n                    batch_size=128*8,\n                    epochs=500,\n                    callbacks=model_checkpoint_callback,\n                    validation_data=({'inputs_1':Validation_Data},{'Sbp':Validation_label_Sbp,'Dbp':Validation_label_Dbp})\n                    )","metadata":{"execution":{"iopub.status.busy":"2021-07-02T06:44:15.513697Z","iopub.status.idle":"2021-07-02T06:44:15.514157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(r'./last_model.h5.pickle', 'wb') as file_pi:\n \tpickle.dump(history.history, file_pi)","metadata":{"execution":{"iopub.status.busy":"2021-07-02T06:44:15.514942Z","iopub.status.idle":"2021-07-02T06:44:15.515351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}