{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center> BANK CUSTOMER CHURN PREDICTION </center></h1><br>\n<b>Customer churn </b> can be defined as the customer terminates any relationship with a company that provides services either online or offline. Churn prediction can be referred to as the prediction of customers who are likely to cancel a subscription, product or service. <br>\n<br>\n<h3><b>:::::Importing following libraries:::::</b></h3>\n<ol>\n    <li><b>Pandas::</b> For the data manipulation and analysis.</li>\n    <li><b>Matplotlib::</b> For the data visualization.</li>\n    <li><b>Keras::</b> For building the neural network built on the Tensorflow backend.</li>\n    <li><b>Warning::</b> For dealing with the warnings coming while execution of the lines of code.</li>\n</ol>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport warnings\nimport os\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>::::: Importing dataset:::::</b></h3><br>\nThe data is imported using the pandas (alias name pd) pre-defined function read_csv() as our data file format is csv (comma-seprated values) in the dataset variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.read_csv('../input/churn-modelling/Churn_Modelling.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::The snapshot of the imported data:::::</b></h3><br>\nAs the dataset variable of DATA_FRAME type so we can use the head() function to show the top 5 rows/tuples of the whole dataset.<br>\n<h3><b>:::::The featuers and Class_Label of the data:::::</b></h3><br>\n<ol>\n    <li><b>RowNumber:</b> The index number of the row.</li>\n    <li><b>CusomerId:</b> The customer ID. </li>\n    <li><b>Surname:</b> The last name of the customer.</li>\n    <li><b>CreditScore:</b> The credit score given by the bank.</li>\n    <li><b>Geography:</b> Country that customer belongs.\\begin{equation}\nGeography \\: \\: \\epsilon \\: \\:  R^{\\{France,Germany,Spain\\}}\n\\end{equation}</li>\n    <li><b>Gender:</b> The gender of the customer. \\begin{equation}\nGender \\: \\: \\epsilon \\: \\:  R^{\\{Male\\:,\\:Female\\}}\n\\end{equation}</li>\n    <li><b>Age:</b>The age of the customer.</li>\n    <li><b>Tenure:</b>Number of years customer is with the bank.</li>\n    <li><b>Balance:</b>The current balance of the account.</li>\n    <li><b>NumOfProducts:</b>The number of the products taken by the customer.\\begin{equation}\nNumOfProducts \\: \\: \\epsilon \\: \\:  R^{\\{1\\:,\\:2\\:,\\:3\\:,\\:4\\:\\}}\n\\end{equation}</li>\n    <li><b>HasCrCard:</b> Is customer owing a credit card or not. \\begin{equation}\nHasCrCard \\: \\: \\epsilon \\: \\:  R^{\\{\\:0\\: = \\:No\\:,\\: 1\\: =\\: Yes\\:\\}}\n\\end{equation}</li>\n    <li><b>IsActiveMember:</b>Is customer is active or not.\\begin{equation}\nIsActiveMember \\: \\: \\epsilon \\: \\:  R^{\\{\\:0\\: = \\:No\\:,\\: 1\\: =\\: Yes\\:\\}}\n\\end{equation}</li>\n    <li><b>EstimatedSalary:</b>The annual salary of the customers.</li>\n    <li><b>Exited:</b>The <b>CLASS LABEL</b> whether customer still with bank or not.\\begin{equation}\nExited \\: \\: \\epsilon \\: \\:  R^{\\{\\:0\\: = \\:No\\:,\\: 1\\: =\\: Yes\\:\\}}\n\\end{equation}</li>\n</ol>"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::Dropping the unsignificant featuers:::::</b></h3><br>\nThe function dataset_name.drop([\"LIST_OF_FEATUERS\"], axis = 0/1) will drop the columns (when axis=1) and rows (when axis=0).<br><br>\n<b>The data divison into the dataset featuers and class label.</b>"},{"metadata":{"trusted":false},"cell_type":"code","source":"X = dataset.iloc[:, 3:13].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y = dataset.iloc[:, 13].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(X[1])\nprint(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::The shape of the feature dataset:::::</b></h3>"},{"metadata":{"trusted":false},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::The shape of the class label dataset:::::</b></h3>"},{"metadata":{"trusted":false},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::The conversion of the categorical features into numerical using the One-Hot coding Technique.:::::</b></h3><br>\nThe conversion of the categorical featuers into numerical featuers using the one-hot encoding technique in which each unique value in the feature will be converted into a seperate column."},{"metadata":{"trusted":false},"cell_type":"code","source":"labelencoder_X_1 = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"labelencoder_X_2 = LabelEncoder()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"onehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\nX = X[:, 1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3<<b>:::::The data division into training and testing:::::</b></h3><br>\nThe data is divided into training and testing into 80-20 ratio."},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train[1:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train[1:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>::::: The Keras Sequential Model:::::</b></h3><br>\nThe Sequential model is a linear stack of layers."},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier = Sequential()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>::::: Initializing the weights and bias for the neural network model:::::</b></h3><br>\nWe have taken the initial vector values from normal/gaussian distribution (mean = 0 and standard deviation = 0.05) for the neural network weights and bias."},{"metadata":{"trusted":false},"cell_type":"code","source":"initializers = keras.initializers.RandomNormal(mean=0.0, stddev=1.0, seed=None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::The Input Layer:::::</b></h3>\n<ol>\n    <li><b>The Neurons:</b> 11</li>\n    <li><b>The weight matrix initial value:</b> Normal Distribution</li>\n    <li><b>The Bias initial values:</b> Normal Distribution</li>\n    <li><b>Activation Function:</b> Rectified Linear Unit (ReLU)</li>\n</ol>"},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier.add(Dense(units = 11, kernel_initializer=initializers ,bias_initializer=initializers, activation = 'relu'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier.add(Dropout(0.5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::The Hidden Layer (L1):::::</b></h3>\n<ol>\n    <li><b>Neurons:</b> 8</li>\n    <li><b>The weight matrix initial value:</b> Normal Distribution</li>\n    <li><b>The Bias initial values:</b> Normal Distribution</li>\n    <li><b>Activation Function:</b> Rectified Linear Unit (ReLU)</li>\n</ol>"},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier.add(Dense( units = 8, kernel_initializer=initializers ,bias_initializer=initializers, activation = 'relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::The Hidden Layer (L2):::::</b></h3>\n<ol>\n    <li><b>Neurons:</b> 6</li>\n    <li><b>The weight matrix initial value:</b> Normal Distribution</li>\n    <li><b>The Bias initial values:</b> Normal Distribution</li>\n    <li><b>Activation Function:</b> Rectified Linear Unit (ReLU)</li>\n</ol>"},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier.add(Dense(units = 6, kernel_initializer=initializers ,bias_initializer=initializers, activation = 'relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::The Hidden Layer (L3):::::</b></h3>\n<ol>\n    <li><b>Neurons:</b> 4</li>\n    <li><b>The weight matrix initial value:</b> Normal Distribution</li>\n    <li><b>The Bias initial values:</b> Normal Distribution</li>\n    <li><b>Activation Function:</b> Rectified Linear Unit (ReLU)</li>\n</ol>"},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier.add(Dense(units = 4, kernel_initializer=initializers ,bias_initializer=initializers, activation = 'relu'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::The Output Layer:::::</b></h3>\n<ol>\n    <li><b>Neurons:</b> 1</li>\n    <li><b>The weight matrix initial value:</b> Normal Distribution</li>\n    <li><b>The Bias initial values:</b> Normal Distribution</li>\n    <li><b>Activation Function:</b> Sigmoid</li>\n</ol>"},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier.add(Dense(units = 1, kernel_initializer=initializers ,bias_initializer=initializers, activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <h3><b>:::::The Weight's Optimizer:::::</b></h3><br>\n Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.<br>\n The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.<br>\n<ol>\n    <li><b>Learning Rate (0.001):</b> The proportion that weights are updated (e.g. 0.001). Larger values (e.g. 0.3) results in faster initial learning before the rate is updated. Smaller values (e.g. 1.0E-5) slow learning right down during training</li>\n    <li><b>beta_1(0.9):</b> The exponential decay rate for the first moment estimates (e.g. 0.9).</li>\n    <li><b>beta_2(0.999):</b> The exponential decay rate for the second-moment estimates (e.g. 0.999). This value should be set close to 1.0 on problems with a sparse gradient (e.g. NLP and computer vision problems).</li>\n</ol>"},{"metadata":{"trusted":false},"cell_type":"code","source":"opti = keras.optimizers.Adam(lr = 0.0001, beta_1 = 0.9, beta_2 = 0.999, amsgrad=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::The Fiting Of The Neural Network:::::</b></h3><br>\nThis method of the keras library will fit the all the layers of the neural network.\n<ol>\n    <li><b>Optimizer:</b> Adam Optimizer</li>\n    <li><b>Loss Function:</b> Binary Cross Entropy</li>\n    <li><b>Metrics:</b> Accuracy</li>\n</ol>"},{"metadata":{"trusted":false},"cell_type":"code","source":"classifier.compile(optimizer = opti, loss = 'binary_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::Training of the Neural Network</b></h3>\n<ol>\n    <li><b>The Batch Size:</b> 128</li>\n    <li><b>Number of Epochs:</b>10000</li>\n</ol>"},{"metadata":{"trusted":false},"cell_type":"code","source":"history = classifier.fit(X_train, y_train, validation_split=0.10, batch_size = 32, epochs = 10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::The Testing of the Neural Network:::::</b></h3><br>\nWe are testing the model with 2000 testing sample extracted randomly from the dataset during the spliting of dataset."},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>:::::The Evaluation of the Model:::::</b></h3><br>\nAs, we are doing the binary classification so we have used the confusion matrix for the evaluation.<br>\nA <b>confusion matrix</b>, in predictive analytics, is a two-by-two table that tells us the rate of false positives, false negatives, true positives and true negatives for a test or predictor. "},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3><b>::::Various Measures Calculated using the Confusion Matrix:::::</b></h3>\n<ol>\n    <li><b>Accuracy</b></li>\n    <li><b>Recall:</b> Recall can be defined as the ratio of the total number of correctly classified positive examples divide to the total number of positive examples. High Recall indicates the class is correctly recognized (small number of FN).</li>\n    <li><b>Precision:</b> Precision is calculated by the division of the total number of correctly classified positive examples by the total number of predicted positive examples. High Precision indicates an example labeled as positive is indeed positive (small number of FP).</li>\n    <li><b>F-Measure:</b> F-measure which uses Harmonic Mean in place of Arithmetic Mean as it punishes the extreme values more. The F-Measure will always be nearer to the smaller value of Precision or Recall.</li>\n</ol>"},{"metadata":{"trusted":false},"cell_type":"code","source":"total_test_sample = 2000\nAccuracy = ((cm[0][0]+cm[1][1])/total_test_sample)*100\nRecall = (cm[0][0]/(cm[0][0]+cm[1][0]))*100\nPrecision = (cm[0][0]/(cm[0][0]+cm[0][1]))*100","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Recall_1 = (cm[0][0]/(cm[0][0]+cm[1][0]))\nPrecision_1 = (cm[0][0]/(cm[0][0]+cm[0][1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"F = (Recall_1+Precision_1)/(2*Recall_1*Precision_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"********** CONFUSION MATRIX MEASURES**********\")\nprint(\"The accuracy is:::::\",Accuracy,\"%\")\nprint(\"\\n\")\nprint(\"**********************************************\")\nprint(\"The Recall is:::::\", Recall,\"%\")\nprint(\"\\n\")\nprint(\"**********************************************\")\nprint(\"The Precision is:::::\",Precision,\"%\")\nprint(\"\\n\")\nprint(\"**********************************************\")\nprint(\"The F-Measure is:::::\",F)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}