{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting Red Wine Quality\n\nIn this project we shall attempt to understand which chemical features of red wine are the most important factors which contribute to the quality of red wine. \n\nThe dataset used within this project is the 'Red Wine Quality' dataset uploaded to Kaggle by UCI Machine Learning.\n\nThroughout the duration of the project, we shall focus on the following tasks:\n\n0. Package and Data Imports\n1. Exploratory Data Analysis and Visualisation\n2. Data Preprocessing\n3. Model Creation and Analysis\n    1. 'Good' vs 'Bad' Wine predictions\n    2. Predicting Quality Ratings\n    \n## 0: Package and Data Imports\n\nLet us first import the data visualisation libraries that will be needed for the next section.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We shall now import the data into a dataframe called 'df'.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1: Exploratory Data Analysis and Visualisation\n\nIn this section we shall attempt to understand the relationship between our data features and the target variable. We shall first check some basic information regarding the dataframe we have created.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we have a total of 11 features that can be used to predict the value of target variable \"quality\". ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also notice that each feature is in a numerical format and that we have a total of 1599 different wines stored within our dataset.\n\n### 1.1: Impact of Features on our Target Variable\n\nIn this section we shall attempt to understand how each of our features in turn impacts the quality rating of wine. \n\n#### 1.1.1: Fixed Acidity","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').mean()['fixed acidity'].plot(kind='bar')\nplt.ylabel('Mean Fixed Acidity')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot above shows the average fixed acidity number for each wine grouped by the quality rating it has been given. We can see that the value of this feature does not significantly change with the quality rating. As a result, we can determine that this features is not a useful predictor of the target variable.\n\n#### 1.1.2: Volatile Acidity","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.lmplot(x='volatile acidity',y='quality',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').mean()['volatile acidity'].plot(kind='bar')\nplt.ylabel('Mean Volatile Acidity')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the two plots shown above, we can clearly see a strong negative relationship between the quality of wine and its volatile acidity level. Hence, we can deduce that this feature is a significant factor in the prediction of our target variable.\n\n#### 1.1.3: Citric Acid","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').mean()['citric acid'].plot(kind='bar')\nplt.ylabel('Mean Citric Acid Level')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly visualise a strong, postive relationship between the citric acid level and the wine's quality. This feature is an important factor in predicting the quality rating.\n\n#### 1.1.4: Residual Sugar","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').mean()['residual sugar'].plot(kind='bar')\nplt.ylabel('Mean Residual Sugar Level')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot above shows no clear linear relationship between the quality of wine and the level of residual sugar contained within it.\n\n#### 1.1.5: Chlorides","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').mean()['chlorides'].plot(kind='bar')\nplt.ylabel('Mean Chloride Level')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above plot, we can determine that, on average, a low chloride level leads to a higher quality of wine. This feature is an important factor when predicting the quality of wine.\n\n#### 1.1.6: Free Sulfur Dioxide","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').mean()['free sulfur dioxide'].plot(kind='bar')\nplt.ylabel('Mean Free Sulfur Dioxide')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(x='free sulfur dioxide',y='quality',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is difficult to determine whether this feature has a significant impact on our target variable. For wines with a quality of above 5, as the level of free sulfur dioxide decreases, the quality of wine increases. However, for wines rated as either a 3 or 4, the reverse of this statement is true. \n\nWe shall leave this variable within our dataset as it appears like there is some hidden information within this feature.\n\n#### 1.1.7: Total Sulfur Dioxide","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').mean()['total sulfur dioxide'].plot(kind='bar')\nplt.ylabel('Mean Total Sulfur Dioxide Level')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As with free sulfur dioxide above, it is difficult to determine whether this feature is an important factor. Further investigation on the relatonship between the \"free sulfur dioxide\" and \"total sulfur dioxide\" shall be done in section 1.2\n\n#### 1.1.8: Density","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').mean()['density'].plot(kind='bar')\nplt.ylabel('Mean Density')\nplt.ylim(0.99,1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that, despite a negative relationship shown in the graph, there is no clear relationship between the density of the wine and its quality. The range of densities contained within our dataset is so small that any relationship would be insignificant. As a result, we can assume that there is no relationship between these two variables.\n\n#### 1.1.9: pH","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').mean()['pH'].plot(kind='bar')\nplt.ylabel('Mean pH Level')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As with density above, there is no significant relationshio between the pH level of a wine and its quality.\n\n#### 1.1.10: Sulphates","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').mean()['sulphates'].plot(kind='bar')\nplt.ylabel('Mean Sulphate Level')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot of the mean sulphate level by quality class shows a clear, positive relationship between the two variables. This feature is an important factor in predicting the quality of wine.\n\n#### 1.1.11: Alcohol","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').mean()['alcohol'].plot(kind='bar')\nplt.ylabel('Mean Alcohol Level')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again, we can see a clear relationship between these variables.\n\nIn summary, we have found that there seem to be 5 main factors that influence the quality of wine. These are:\n\n - Alcohol\n - Sulphates\n - Chlorides\n - Citric Acid\n - Volatile Acidity\n \nLet us produce a plot of the correlation values of all our features to our target variable to confirm these findings.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"series = pd.Series(df.corr()['quality'])\nseries.drop('quality').plot(kind='bar')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that the 5 columns mentioned above seem to have a high correlation value, in absolute terms. However, it also seems that the density and total sulfur dioxide features are also significantly correlated to our target variable. \n\n### 1.2: Feature Relationships\n\nNow that we have determined which features have an impact on our target variable, we shall now begin to investigate the relationships between our features. We shall do this by producing a correlation heatmap.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.heatmap(df.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The heatmap above shows the correlation values between each of the features. We shall now investigate values that have a large absolute value.\n\n#### 1.2.1: Fixed Acidity v Citric Acid","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(x='fixed acidity',y='citric acid',hue='quality',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first thing to note is that the level of fixed acidity within a wine is always greater than the level of citric acid. This is because citric acid is one of the types of acid considered in the fixed acidity value. We notice a clear positive relationship between these two features, which is why the value of 0.67 in the correlation matrix appears. \n\nLet us investigate whether the percentage of the citric acid has an effect on the quality of wine. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df['citric acid percentage'] = df['citric acid'] / (df['citric acid'] + df['fixed acidity'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').mean()['citric acid percentage'].plot(kind='bar')\nplt.ylabel('Mean Citric Acid Percentage')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly see that the as the average citric acid percentage of the wine increases, the quality score associated to the wine also increases. This is due to the fact that acids impart the fundamental features of a wines taste. We have found another useful characteristic of our target variable. \n\n#### 1.2.2: Fixed Acidity v Density","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(x='fixed acidity',y='density',hue='quality',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once again we can see a clear positive relationship between these two variables.\n\n#### 1.2.3: Fixed Acidity v pH","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(x='fixed acidity',y='pH',hue='quality',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, we see a clear negative relationshio between these variables. This intuitively makes sense, since pH is a direct measure of how acidic or alkaline a substance is. The more acidic the wine is, the lower the pH value is because of the scale that is used when measuring pH levels. A substance with 0pH is extremelty acidic, 7pH is neutral and 13pH is extremely alkaline. \n\n#### 1.2.4: Volatile Acidity v Citric Acid","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.scatterplot(x='volatile acidity',y='citric acid',hue='quality',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The negative relationship between these variables is clear. Volatile acidity is commonly used as a measure of wine spoilage, and manufactures aim to keep the level of volatile acidity negligible. This may be the reason why we see more high quality wines to the top left of the scatter plot above. This is because in this region the citric acid levels are high, which contribute to the taste of the wine, and the volatile acidity levels are low, which reduce the spoilage. \n\n#### 1.2.5: Citric Acid v pH","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(x='citric acid',y='pH',hue='quality',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The relationship between these two variables is clearly negative for the same reasons as the relationship between fixed acidity and pH.\n\n#### 1.2.6: Residual Sugar v Density","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(x='residual sugar',y='density',hue='quality',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The positive relationship between these two variables is difficult to be seen since the correlation between them is only 0.37. \n\n#### 1.2.7: Chlorides v Sulphates","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(x='chlorides',y='sulphates',hue='quality',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see a slightly positive linear relationship between these two variables. We notice that the majority of the high quality wines are plotted in the left hand part of this graph. This is due to the negative relationship that chlorides have with the wine quality.\n\n#### 1.2.8: Free Sulfur Dioxide v Total Sulfur Dioxide","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(x='free sulfur dioxide',y='total sulfur dioxide',hue='quality',data=df)\nplt.plot(range(0,71),range(0,71))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The first thing to notice is that the level of free sulfur dioxide is always less than or equal to the total sulfir dioxide level, which makes intuitive sense. Let us investigate whether the percentage of free sulfur dixoide impacts the quality of wine.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df['free sulfur dioxide percentage'] = df['free sulfur dioxide'] / (df['free sulfur dioxide'] + df['total sulfur dioxide'])\ndf.groupby('quality').mean()['free sulfur dioxide percentage'].plot(kind='bar')\nplt.ylabel('Mean Free Sulfur Dioxide Percentage')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that for each quality rating there is a differet average free sulfur dioxide percentage, but there does not exist a clear relationship between these variables. Let us check this by calculating the correlation between these features.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.corr()['quality']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our correlation value has been shown to be equal to approximately 0.2, which suggests that there is enough of a relationship to consider this variable for predicting wine quality. \n\n#### 1.2.9: Alcohol v Density","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.scatterplot(x='alcohol',y='density',hue='quality',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see a clear negative relationship between these two variables.\n\n## 2: Data Preprocessing\n\nIn this section we shall manipulate our dataset for use in the machine learning algorithms that we shall implement in section 3. \n\nFrom the analysis we perform in section 1, we identified 5 key variables in predicting wine quality. These were alcohol, sulphates, chlorides, citric acid and volatile acidity. We also created two new variables, citric acid percentage and free sulfur dioxide percentage, which we also found to be useful. \n\nWe must now remove any variables which we find are not useful or variables that may cause multicolinearity issues. As a result, we shall remove the columns free sulfur dioxide and fixed acidity.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop(['free sulfur dioxide','fixed acidity'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now check the distribution of our target class by producing a countplot.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(x='quality',data=df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that we have an extremely unbalanced dataset, which may lead to issues when implementing our algorithms. Let us see how unbalanced the dataset really is.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df.groupby('quality').count()['pH'] * 100 /len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that over 80% of our data points belong to either quality rating 5 or 6. However, if we reclassify our points as \"good\" if they have a quality rating of at least 6 and \"bad\" otherwise, then we should achieve a much more balanced dataset. Let us create a new target variable called \"new_rating\" that contains a 1 if the wine is \"good\" and a 0 if the wine is \"bad\".","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df['new_rating'] = df['quality'].apply(lambda x: 1 if x >= 6 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check the split that this has produced.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(x='new_rating',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['new_rating'].value_counts() * 100 / len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now clearly see that we have a much more balanced dataset as a result of the new rating system we have implemented.\n\nWe are now able to split our data into training and test sets for use in our machine learning algorithms. ","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"X = df.drop(['quality','new_rating'],axis=1)\ny = df['new_rating']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3: Model Creation and Analysis\n\nIn this section we shall create and analyse machine learning models in order to predict the quality of wine. \n\n### 3.1: \"Good\" vs \"Bad\" Predictions\n\nIn this section, we shall make use of the \"new_rating\" feature created above to predict whether a wine is either \"good\" or \"bad\". We shall implement a range of machine learning algorithms and attempt to determine which method produces the most accuracy. In this case, we are considering a binary classification problem. Let us begin implementing and analysing the following algorithms:\n\n1. Logistic Regression (75%)\n2. Decision Tree (74%)\n3. Random Forest (80%)\n4. Support Vector Machines (77%)\n5. XGBoost (78%)\n\n#### 3.1.1: Logistic Regression\n\nWe first need to import the Logistic Regression model from scikit-learn. We will then fit the model using our training data, followed by creating predictions using our testing data.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression(max_iter=1500)\nlogmodel.fit(X_train, y_train)\nlogmodelpreds = logmodel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now produce a confusion matrix and a classification report to determine the accuracy of our model.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test,logmodelpreds))\nprint(\"-\" * 50)\nprint(\"Classification Report: \")\nprint(classification_report(y_test,logmodelpreds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we are attempt to solve a binary classification problem, the key metric to consider is accuracy. In this case, our logistic regression model achieved 75% when predicting wines from the testing set. \n\nThe process we shall undertake for the implementation of each model type will be similar to the process we have just undertaken for our logistic regression model. Let us now work through each model in turn and find out its accuracy.\n\n#### 3.1.2: Decision Tree","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\ntree.fit(X_train, y_train)\ntreepreds = tree.predict(X_test)\n\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test,treepreds))\nprint(\"-\" * 50)\nprint(\"Classification Report: \")\nprint(classification_report(y_test,treepreds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the decision tree model implemented achieved 74% accuracy.\n\n#### 3.1.3: Random Forest","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators=100)\nforest.fit(X_train, y_train)\nforestpreds = forest.predict(X_test)\n\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test,forestpreds))\nprint(\"-\" * 50)\nprint(\"Classification Report: \")\nprint(classification_report(y_test,forestpreds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our random forest classifier achieved 80% accuracy.\n\n#### 3.1.4: Support Vector Machines","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(X_train, y_train)\nsvcpreds = svc.predict(X_test)\n\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test,svcpreds))\nprint(\"-\" * 50)\nprint(\"Classification Report: \")\nprint(classification_report(y_test,svcpreds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our support vector machine classifier only achieved 58% accuracy. However, we can use a grid search to attempt to alter the model parameters and achieve a higher accuracy.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} \nfrom sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\ngrid.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"grid.best_estimator_\ngridpreds = grid.predict(X_test)\n\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test,gridpreds))\nprint(\"-\" * 50)\nprint(\"Classification Report: \")\nprint(classification_report(y_test,gridpreds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Despite tuning the parameters, we were only able to achieve a 77% accuracy using Support Vector Machines.\n\n#### 3.1.5: XGBoost","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)\nxgbpreds = xgb.predict(X_test)\n\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test,xgbpreds))\nprint(\"-\" * 50)\nprint(\"Classification Report: \")\nprint(classification_report(y_test,xgbpreds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our XGBoost model achieved 78% accuracy.\n\nIn summary, each of our models achieved at least 70% accuracy. The reason for this relatively low score may be due to the fact that there were lots of cases that were located close to boundary between \"good\" and \"bad\", since about 40% of our datapoints belonged to both quality rating 5 or 6. However, our random forest model managed to achieve a much better 80% accuracy. \n\n### 3.2 Predicting Quality Rating\n\nIn this section we shall attempt to solve our original problem of predicting the quality rating of wine given its chemical features. Note that our dataset was extremely unbalanced, which was the major reason for considering the \"good\" v \"bad\" problem above. In order to predict exact quality ratings, we will need to balance our dataset. We can do this using a technique known as SMOTE, which will synthetically produce more samples of the under represented classes. Let us apply this method and then check the number of each wine within eavh quality rating. \n\nFirst, we shall remove the \"new_rating\" target column since it is no longer needed.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.drop('new_rating',axis=1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us reorder the columns of order dataframe so that our target variable shows in the last column.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df[['volatile acidity','citric acid','residual sugar','chlorides','total sulfur dioxide','density','pH','sulphates','alcohol','citric acid percentage','free sulfur dioxide percentage','quality']]\ndf.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now implement the SMOTE technique to oversample our dataset which will result in perfectly balanced classes.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"data = df.values\nX = data[:, :-1]\ny = data[:, -1]\nX_columns = df.columns[:-1]\ny_columns = df.columns[-1]\n\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX, y = oversample.fit_sample(X, y)\n\nX_sampled = pd.DataFrame(X, columns=X_columns)\ny_sampled = pd.DataFrame(y, columns=[y_columns])\ndf = pd.concat([X_sampled,y_sampled],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check that the technique worked and that we have balanced classes.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.countplot(x='quality',data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['quality'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot and series above show that we now posses 681 wines for each quality rating, meaning that we have a perfectly balanced dataset as required. Let us now move on to create training and testing sets for this data.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"X = df.drop('quality',axis=1)\ny = df['quality']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have oversampled and split our data into training and test sets, we are now in a position to begin implementing machine learning models to predict the quality of wine.\n\n#### 3.2.1: Logistic Regression","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"logmodel = LogisticRegression(max_iter=5000)\nlogmodel.fit(X_train, y_train)\nlogmodelpreds = logmodel.predict(X_test)\n\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test,logmodelpreds))\nprint(\"-\" * 50)\nprint(\"Classification Report: \")\nprint(classification_report(y_test,logmodelpreds))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our logistic regression model achieved 56% accuracy.\n\n#### 3.2.2 Decision Tree","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"tree = DecisionTreeClassifier()\ntree.fit(X_train, y_train)\ntreepreds = tree.predict(X_test)\n\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test,treepreds))\nprint(\"-\" * 50)\nprint(\"Classification Report: \")\nprint(classification_report(y_test,treepreds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have managed to significantly improve the accuracy of predictions to 77% by using a decision tree.\n\n#### 3.2.3: Random Forest","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"forest = RandomForestClassifier(n_estimators=100)\nforest.fit(X_train, y_train)\nforestpreds = forest.predict(X_test)\n\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test,forestpreds))\nprint(\"-\" * 50)\nprint(\"Classification Report: \")\nprint(classification_report(y_test,forestpreds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our random forest model has performed the best out of the models created so far, achieving 85% accuracy on our test data.\n\n#### 3.2.4: Support Vector Machines","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"svc = SVC()\nsvc.fit(X_train, y_train)\nsvcpreds = svc.predict(X_test)\n\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test,svcpreds))\nprint(\"-\" * 50)\nprint(\"Classification Report: \")\nprint(classification_report(y_test,svcpreds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our support vector machine achieves only 37% accuracy. Let us use a grid search to attempt to improve this accuracy.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} \nfrom sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)\ngrid.fit(X_train,y_train)\n\ngrid.best_estimator_\ngridpreds = grid.predict(X_test)\n\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test,gridpreds))\nprint(\"-\" * 50)\nprint(\"Classification Report: \")\nprint(classification_report(y_test,gridpreds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our grid search has increased the accuracy we were able to achieve from 37% to 82%.\n\n#### 3.2.5: XGBoost","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"xgb = XGBClassifier()\nxgb.fit(X_train, y_train)\nxgbpreds = xgb.predict(X_test)\n\nprint(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test,xgbpreds))\nprint(\"-\" * 50)\nprint(\"Classification Report: \")\nprint(classification_report(y_test,xgbpreds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our XGBoost model achieved a 75% accuracy.","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}