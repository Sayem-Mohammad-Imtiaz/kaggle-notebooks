{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network.It utilizes a hierarchical level of artificial neural networks to carry out the process of machine learning. The artificial neural networks are built like the human brain, with neuron nodes connected together like a web.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://i1.wp.com/gereshes.com/wp-content/uploads/2019/09/trajNoFlashLowFid.gif?resize=1089%2C548&ssl=1)\n### -------------- Neural Network Optimal Control in Astrodynamics (Sounds Cool) ------------------\n                              ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**The term “deep” usually refers to the number of hidden layers in the neural network. Traditional neural networks only contain 2-3 hidden layers, while deep networks can have much higher hidden layers.Deep learning models are trained by using large sets of labeled data and neural network architectures that learn features directly from the data without the need for manual feature extraction.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Neural Networks","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://static.wixstatic.com/media/bd3071_7dfa208b4f3c4283acda8703cf76bcdf~mv2.gif)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://articles.adxy.in/wp-content/uploads/2019/08/similarities-between-biologicalneuron-artificialneuron.gif)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## ------------------------ Let's get started -----------------------------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1) Basic Image Classification with Fashion MNIST","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://peltarion.com/static/fashion-mnist_long.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fashion_mnist = keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Explore the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_images.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Preprocess the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255\nplt.figure()\nplt.imshow(train_images[0])\nplt.colorbar()\nplt.grid(False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_images = train_images / 255.0\n\ntest_images = test_images / 255.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's display the first 25 images from the training set and display the class name below each image\nplt.figure(figsize=(12,12))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n    plt.xlabel(class_names[train_labels[i]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Build the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(10)\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Compile the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Train the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train_images, train_labels, epochs=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Evaluate accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n\nprint('\\nTest accuracy:', test_acc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It turns out that the accuracy on the test dataset is a little less than the accuracy on the training dataset. This gap between training accuracy and test accuracy represents overfitting. Overfitting happens when a machine learning model performs worse on new, previously unseen inputs than it does on the training data. An overfitted model \"memorizes\" the noise and details in the training dataset to a point where it negatively impacts the performance of the model on the new data.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2) Binary Text classification with IMDB ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://i.redd.it/1jofyu3ugsiz.jpg)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom keras.datasets import imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Since we restricted ourselves to the top 10,000 most frequent words, no word index will exceed 10,000\nmax([max(sequence) for sequence in train_data])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Preparing the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**We cannot feed lists of integers into a neural network. We have to turn our lists into tensors.We could one-hot-encode our lists to turn them into vectors of 0s and 1s. Concretely, this would mean for instance turning the sequence. [3,5] into a 10,000-dimensional vector that would be all-zeros except for indices 3 and 5, which would be ones. Then we could use as first layer in our network a Dense layer, capable of handling floating point vector data**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport numpy as np\n\ndef vectorize_sequences(sequences, dimension=10000):\n    # Create an all-zero matrix of shape (len(sequences), dimension)\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1  #set specific indices of results[i] to 1s\n    return results\n\n# Our vectorized training data\nx_train = vectorize_sequences(train_data)\n# Our vectorized test data\nx_test = vectorize_sequences(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our vectorized labels\ny_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Building our network","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Our input data is simply vectors, and our labels are scalars (1s and 0s): this is the easiest setup you will ever encounter. A type of network that performs well on such a problem would be a simple stack of fully-connected (Dense) layers with relu activations.we need to pick a loss function and an optimizer. Since we are facing a binary classification problem and the output of our network is a probability (we end our network with a single-unit layer with a sigmoid activation), is it best to use the binary_crossentropy loss**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import optimizers\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import losses\nfrom keras import metrics\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.001),\n              loss=losses.binary_crossentropy,\n              metrics=[metrics.binary_accuracy])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Validating our approach","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_val = x_train[:10000]\npartial_x_train = x_train[10000:]\n\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_dict = history.history\nhistory_dict.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**It contains 4 entries: one per metric that was being monitored, during training and during validation. Let's use Matplotlib to plot the training and validation loss side by side, as well as the training and validation accuracy**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nacc = history.history['loss']\nval_acc = history.history['binary_accuracy']\nloss = history.history['val_loss']\nval_loss = history.history['val_binary_accuracy']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.clf()   # clear figure\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)\nresults = model.evaluate(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Our fairly naive approach achieves an accuracy of 88%**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Convolutional neural network","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://static.wixstatic.com/media/bd3071_7dfa208b4f3c4283acda8703cf76bcdf~mv2.gif)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/3288/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/2560/1*ciDgQEjViWLnCbmX-EeSrA.gif)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://i.stack.imgur.com/ptDPZ.gif)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3) Image classification with Augmentation (Cats vs Dogs)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/2560/1*NgPy8YAskOp51S9gfsY0Ug.jpeg)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n\npath_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n\nPATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#After extracting its contents, assign variables with the proper file path for the training and validation set\ntrain_dir = os.path.join(PATH, 'train')\nvalidation_dir = os.path.join(PATH, 'validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\ntrain_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\nvalidation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\nvalidation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Understand the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_cats_tr = len(os.listdir(train_cats_dir))\nnum_dogs_tr = len(os.listdir(train_dogs_dir))\n\nnum_cats_val = len(os.listdir(validation_cats_dir))\nnum_dogs_val = len(os.listdir(validation_dogs_dir))\n\ntotal_train = num_cats_tr + num_dogs_tr\ntotal_val = num_cats_val + num_dogs_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('total training cat images:', num_cats_tr)\nprint('total training dog images:', num_dogs_tr)\n\nprint('total validation cat images:', num_cats_val)\nprint('total validation dog images:', num_dogs_val)\nprint(\"--\")\nprint(\"Total training images:\", total_train)\nprint(\"Total validation images:\", total_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Setting up variables to use while pre-processing the dataset and training the network\nbatch_size = 128\nepochs = 15\nIMG_HEIGHT = 150\nIMG_WIDTH = 150","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Data preparation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data\nvalidation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n                                                           directory=train_dir,\n                                                           shuffle=True,\n                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                           class_mode='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,\n                                                              directory=validation_dir,\n                                                              target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                              class_mode='binary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Create the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential([\n    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n    MaxPooling2D(),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dense(1)\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Compile the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Train the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(\n    train_data_gen,\n    steps_per_epoch=total_train // batch_size,\n    epochs=epochs,\n    validation_data=val_data_gen,\n    validation_steps=total_val // batch_size\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**In the plots above, the training accuracy is increasing linearly over time, whereas validation accuracy stalls around 70% in the training process. Also, the difference in accuracy between training and validation accuracy is noticeable a sign of overfitting.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Data augmentation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Data augmentation takes the approach of generating more training data from existing training samples by augmenting the samples using random transformations that yield believable-looking images. The goal is the model will never see the exact same picture twice during training. This helps expose the model to more aspects of the data and generalize better.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Apply horizontal flip","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n                                               directory=train_dir,\n                                               shuffle=True,\n                                               target_size=(IMG_HEIGHT, IMG_WIDTH))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Take one sample image from the training examples and repeat it five times so that the augmentation is applied to the same image five times**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"augmented_images = [train_data_gen[0][0][0] for i in range(5)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Randomly rotate the image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_gen = ImageDataGenerator(rescale=1./255, rotation_range=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n                                               directory=train_dir,\n                                               shuffle=True,\n                                               target_size=(IMG_HEIGHT, IMG_WIDTH))\n\naugmented_images = [train_data_gen[0][0][0] for i in range(5)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Apply zoom augmentation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# zoom_range from 0 - 1 where 1 = 100%.\nimage_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.5) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n                                               directory=train_dir,\n                                               shuffle=True,\n                                               target_size=(IMG_HEIGHT, IMG_WIDTH))\n\naugmented_images = [train_data_gen[0][0][0] for i in range(5)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Put it all together","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_gen_train = ImageDataGenerator(\n                    rescale=1./255,\n                    rotation_range=45,\n                    width_shift_range=.15,\n                    height_shift_range=.15,\n                    horizontal_flip=True,\n                    zoom_range=0.5\n                    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen = image_gen_train.flow_from_directory(batch_size=batch_size,\n                                                     directory=train_dir,\n                                                     shuffle=True,\n                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                     class_mode='binary')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"augmented_images = [train_data_gen[0][0][0] for i in range(5)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Create validation data generator","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"image_gen_val = ImageDataGenerator(rescale=1./255)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n                                                 directory=validation_dir,\n                                                 target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                 class_mode='binary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Dropout","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Another technique to reduce overfitting is to introduce dropout to the network. It is a form of regularization that forces the weights in the network to take only small values, which makes the distribution of weight values more regular and the network can reduce overfitting on small training examples.When we apply dropout to a layer it randomly drops out (set to zero) number of output units from the applied layer during the training process. Dropout takes a fractional number as its input value, in the form such as 0.1, 0.2, 0.4, etc. This means dropping out 10%, 20% or 40% of the output units randomly from the applied layer**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_new = Sequential([\n    Conv2D(16, 3, padding='same', activation='relu', \n           input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Dropout(0.2),\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dense(1)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_new.compile(optimizer='adam',\n                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                  metrics=['accuracy'])\n\nmodel_new.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Train the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model_new.fit_generator(\n    train_data_gen,\n    steps_per_epoch=total_train // batch_size,\n    epochs=epochs,\n    validation_data=val_data_gen,\n    validation_steps=total_val // batch_size\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recurrent Neural Network","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://static.wixstatic.com/media/bd3071_7dfa208b4f3c4283acda8703cf76bcdf~mv2.gif)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://corochann.com/wp-content/uploads/2017/05/rnn1_graph-800x460.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://hackernoon.com/hn-images/1*_mM83sFLjzKt8cRB439Y3Q.gif)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/1928/1*xn5kA92_J5KLaKcP7BMRLA.gif)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://lh3.googleusercontent.com/proxy/qIQXDsuZv8v07mLanR7K0fiIwur-OWZHV64T0EXt8dZ5PXi88bsGKcQxit5coR3eONbbi5hSF3cA9wJy1bzbjVNxpJSjIMuZpRLuRZt8KzbKRkJ8e2OHSZJRphJGxUlI)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 4) Predict Next Character in the Sequence with Shakespeare","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/3534/0*yK_SN7M6SG-ovDwi.jpg)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nimport numpy as np\nimport os\nimport time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Download the Shakespeare dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-  Read the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read, then decode for py2 compat.\ntext = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n# length of text is the number of characters in it\nprint ('Length of text: {} characters'.format(len(text)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take a look at the first 250 characters in text\nprint(text[:250])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The unique characters in the file\nvocab = sorted(set(text))\nprint ('{} unique characters'.format(len(vocab)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Process the text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a mapping from unique characters to indices\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\n\ntext_as_int = np.array([char2idx[c] for c in text])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('{')\nfor char,_ in zip(char2idx, range(20)):\n    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\nprint('  ...\\n}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show how the first 13 characters from the text are mapped to integers\nprint ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The prediction task","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Create training examples and targets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# The maximum length sentence we want for a single input in characters\nseq_length = 100\nexamples_per_epoch = len(text)//(seq_length+1)\n\n# Create training examples / targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nfor i in char_dataset.take(5):\n  print(idx2char[i.numpy()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n\nfor item in sequences.take(5):\n  print(repr(''.join(idx2char[item.numpy()])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Create training batches","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\ndataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Build The Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nrnn_units = 1024","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n  return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(\n  vocab_size = len(vocab),\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Try the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for input_example_batch, target_example_batch in dataset.take(1):\n  example_batch_predictions = model(input_example_batch)\n  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"-  Try it for the first example in the batch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sampled_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\nprint()\nprint(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Train the model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n\nexample_batch_loss  = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss=loss)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Configure checkpoints","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Directory where the checkpoints will be saved\ncheckpoint_dir = './training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Execute the training","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Generate text","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS=10\n\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.train.latest_checkpoint(checkpoint_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The prediction loop","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The following code block generates the text\n\n- It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate\n- Get the prediction distribution of the next character using the start string and the RNN state\n- Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model\n- The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_text(model, start_string):\n  # Evaluation step (generating text using the learned model)\n\n  # Number of characters to generate\n  num_generate = 1000\n\n  # Converting our start string to numbers (vectorizing)\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n  temperature = 1.0\n\n  # Here batch size == 1\n  model.reset_states()\n  for i in range(num_generate):\n      predictions = model(input_eval)\n      # remove the batch dimension\n      predictions = tf.squeeze(predictions, 0)\n\n      # using a categorical distribution to predict the character returned by the model\n      predictions = predictions / temperature\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n      # We pass the predicted character as the next input to the model\n      # along with the previous hidden state\n      input_eval = tf.expand_dims([predicted_id], 0)\n\n      text_generated.append(idx2char[predicted_id])\n\n  return (start_string + ''.join(text_generated))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(generate_text(model, start_string=u\"ROMEO: \"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### --------------------------If You Found this helpful, Do Upvote Please----------------------------------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://media2.giphy.com/media/l3q2FnW3yZRJVZH2g/giphy.gif)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}