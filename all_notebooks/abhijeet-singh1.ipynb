{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndata =  pd.read_csv(\"../input/pump-sensor-data/sensor.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['Unnamed: 0', 'timestamp','sensor_00','sensor_15','sensor_50','sensor_51'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nprint(data.plot(subplots =True, sharex = True, figsize = (20,50)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['machine_status'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nconditions = [(data['machine_status'] =='NORMAL'), (data['machine_status'] =='BROKEN'), (data['machine_status'] =='RECOVERING')]\nchoices = [1, 0, 0.5]\ndata['Operation'] = np.select(conditions, choices, default=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ndata.plot(subplots =True, sharex = True, figsize = (20,50))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set 0: sensors 4, 6, 7, 8, 9"},{"metadata":{"trusted":true},"cell_type":"code","source":"df0 = pd.DataFrame(data, columns=['Operation','sensor_04', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set 1: sensors 1, 4, 10, 14, 19, 25, 34, 38"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = pd.DataFrame(data, columns=['Operation','sensor_01', 'sensor_04', 'sensor_10', 'sensor_14', 'sensor_19', 'sensor_25'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set 2: sensors 2, 5, 11, 16, 20, 26, 39"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = pd.DataFrame(data, columns = ['Operation','sensor_02', 'sensor_05', 'sensor_11', 'sensor_16', 'sensor_20', 'sensor_26'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set 3: sensors 3, 6, 12, 17, 21, 28, 40"},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = pd.DataFrame(data, columns = ['Operation','sensor_03', 'sensor_06', 'sensor_12', 'sensor_17', 'sensor_21', 'sensor_28'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df0.plot(subplots =True, sharex = True, figsize = (20,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df0\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    dff = pd.DataFrame(data)\n    cols, names = list(), list()\n    for i in range(n_in, 0, -1):\n        cols.append(dff.shift(-i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    for i in range(0, n_out):\n        cols.append(dff.shift(-i))\n        if i==0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1)) for j in range(n_vars)]        \n        agg = pd.concat(cols, axis=1)\n        agg.columns = names\n        if dropnan:\n            agg.dropna(inplace=True)\n        return agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nvalues = df.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled, 1, 1)\nr = list(range(df.shape[1]+1, 2*df.shape[1]))\nreframed.drop(reframed.columns[r], axis=1, inplace=True)\nreframed.head()\n\n# Data spliting into train and test data series.\nvalues = reframed.values\nn_train_time = 50000\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\ntrain_x, train_y = train[:, :-1], train[:, -1]\ntest_x, test_y = test[:, :-1], test[:, -1]\ntrain_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\ntest_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_x.shape[1], train_x.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n# Network fitting\nhistory = model.fit(train_x, train_y, epochs=50, batch_size=70, validation_data=(test_x, test_y), verbose=2, shuffle=False)\n\n# Loss history plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\nsize = df.shape[1]\n\n# Prediction test\nyhat = model.predict(test_x)\ntest_x = test_x.reshape((test_x.shape[0], size))\n\n# invert scaling for prediction\ninv_yhat = np.concatenate((yhat, test_x[:, 1-size:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_x[:, 1-size:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e = 100 - round(sum(abs(inv_y[:]-inv_yhat[:]))/len(inv_y[:])*100,2)\naa=[x for x in range(160000)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[:160000], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:160000], 'r', label=\"prediction with precision of {} %\".format(e))\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time', size=15)\nplt.legend(fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df2\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df2.plot(subplots =True, sharex = True, figsize = (20,20))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    dff = pd.DataFrame(data)\n    cols, names = list(), list()\n    for i in range(n_in, 0, -1):\n        cols.append(dff.shift(-i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    for i in range(0, n_out):\n        cols.append(dff.shift(-i))\n        if i==0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1)) for j in range(n_vars)]        \n        agg = pd.concat(cols, axis=1)\n        agg.columns = names\n        if dropnan:\n            agg.dropna(inplace=True)\n        return agg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nvalues = df.values\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nreframed = series_to_supervised(scaled, 1, 1)\nr = list(range(df.shape[1]+1, 2*df.shape[1]))\nreframed.drop(reframed.columns[r], axis=1, inplace=True)\nreframed.head()\n\n# Data spliting into train and test data series.\nvalues = reframed.values\nn_train_time = 50000\ntrain = values[:n_train_time, :]\ntest = values[n_train_time:, :]\ntrain_x, train_y = train[:, :-1], train[:, -1]\ntest_x, test_y = test[:, :-1], test[:, -1]\ntrain_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\ntest_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_shape=(train_x.shape[1], train_x.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\n# Network fitting\nhistory = model.fit(train_x, train_y, epochs=50, batch_size=70, validation_data=(test_x, test_y), verbose=2, shuffle=False)\n\n# Loss history plot\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\nsize = df.shape[1]\n\n# Prediction test\nyhat = model.predict(test_x)\ntest_x = test_x.reshape((test_x.shape[0], size))\n\n# invert scaling for prediction\ninv_yhat = np.concatenate((yhat, test_x[:, 1-size:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_x[:, 1-size:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\nprint('Test RMSE: %.3f' % rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e = 100 - round(sum(abs(inv_y[:]-inv_yhat[:]))/len(inv_y[:])*100,2)\naa=[x for x in range(170000)]\nplt.figure(figsize=(25,10)) \nplt.plot(aa, inv_y[:170000], marker='.', label=\"actual\")\nplt.plot(aa, inv_yhat[:170000], 'r', label=\"prediction with precision of {} %\".format(e))\nplt.ylabel(df.columns[0], size=15)\nplt.xlabel('Time', size=15)\nplt.legend(fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = data.corr()\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\ncorr80 = corr[abs(corr)> 0.8]\nsns.heatmap(corr80)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotPerColumnDistribution(df, 10, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}