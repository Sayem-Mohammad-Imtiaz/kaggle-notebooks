{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning and Artificial Neural Network approach for COVID-19 Early Detection From Audio Recording | Part 1\n\nby Nasrul Hakim\n\n![https://miro.medium.com/max/1400/1*hJ7Hl9k1m0tqOFLh6mH2vA.jpeg](https://miro.medium.com/max/1400/1*hJ7Hl9k1m0tqOFLh6mH2vA.jpeg)\n\n## Introduction\n\nThe SARS-CoV-2-caused novel coronavirus disease 2019 (COVID-19) pandemic remains a critical and urgent threat to global health. While advancements in testing have made these tools more widely available in recent months, there is still a need for low-cost, quick, and scalable COVID-19 screening technology.\nIn the context of the COVID-19 pandemic, considerable research activity has emerged to use respiratory sounds (e.g., coughs, breathing, and voice) as primary sources of information.\n\nCOVID-19 is a respiratory condition that affects breathing and voice, causing symptoms such as a dry cough, sore throat, an overly breathy voice, and typical breathing pattern. These are all symptoms that can distinguish patients' voices, resulting in recognisable voice signatures and allowing the training of algorithms to predict the presence of a SARS-COV-2 infection or as a tool to grade the disease's severity. The findings of Cambridge University (Area Under the ROC Curve, AUC = 80%) and MIT scientists (AUC = 97 percent, based on cough recordings only) on vocal biomarkers to aid in the diagnosis of COVID-19 are promising.\n\n\n## The COUGHVID crowdsourcing dataset\n\nCough audio signal classification has been successfully used to diagnose a variety of respiratory conditions, and there has been significant interest in leveraging Machine Learning (ML) to provide widespread COVID-19 screening. The COUGHVID dataset provides over 25,000 crowdsourced cough recordings representing a wide range of participant ages, genders, geographic locations, and COVID-19 statuses. Four experienced physicians labeled more than 2,800 recordings to diagnose medical abnormalities present in the coughs, contributing one of the largest expert-labeled cough datasets in existence that can be used for a plethora of cough audio classification tasks. \n\n### Recording quality estimation\nEvery user who uploaded their cough sound to the COUGHVID dataset presumably used a different device, potentially introducing a variation in recording quality due to the different recording hardware and software of each device. Furthermore, the recordings were captured at various locations around the world with non-constant degrees of background noise. In order to assist users of the COUGHVID dataset in estimating the quality of each signal, open-sourced code to estimate the Signal-to-Noise Ratio (SNR) of each cough recording is provided.\n\n#### Reference\nOrlandic, L., Teijeiro, T. & Atienza, D. <br>\nThe COUGHVID crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms. Sci Data 8, 156 (2021). <br>\nhttps://doi.org/10.1038/s41597-021-00937-4<br>\nhttps://www.nature.com/articles/s41597-021-00937-4<br>\nhttps://c4science.ch/diffusion/10770/\n\n## Data Analysis","metadata":{"_kg_hide-input":false}},{"cell_type":"code","source":"# data manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\n\nimport seaborn as sns\nsns.set_palette(sns.color_palette(\"GnBu_r\"))\nimport matplotlib.pyplot as plt\nfrom IPython import display\n\n# set variables\nROOT = '../input/coughvid-wav/public_dataset/'\n\n# load coughvid meta\ndata_raw = pd.read_csv(ROOT+'metadata_compiled.csv')\ndata_raw.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:38:35.828342Z","iopub.execute_input":"2021-09-02T09:38:35.829007Z","iopub.status.idle":"2021-09-02T09:38:38.62024Z","shell.execute_reply.started":"2021-09-02T09:38:35.828911Z","shell.execute_reply":"2021-09-02T09:38:38.619274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_full = data_raw.fillna('unknown')\nf, axs = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw=dict(width_ratios=[4, 4]))\nsns.countplot(data=data_full, x=\"gender\", ax=axs[0])\nsns.countplot(data=data_full, x=\"status\", ax=axs[1])\nf.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:38:38.622005Z","iopub.execute_input":"2021-09-02T09:38:38.622456Z","iopub.status.idle":"2021-09-02T09:38:39.206335Z","shell.execute_reply.started":"2021-09-02T09:38:38.622411Z","shell.execute_reply":"2021-09-02T09:38:39.205082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axs = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw=dict(width_ratios=[4, 4]))\nsns.countplot(data=data_full, x=\"respiratory_condition\", ax=axs[0])\nsns.countplot(data=data_full, x=\"fever_muscle_pain\", ax=axs[1])\nf.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:39:13.165485Z","iopub.execute_input":"2021-09-02T09:39:13.165881Z","iopub.status.idle":"2021-09-02T09:39:13.571507Z","shell.execute_reply.started":"2021-09-02T09:39:13.165849Z","shell.execute_reply":"2021-09-02T09:39:13.570532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(data_full, x=\"cough_detected\", bins=10, height=4, aspect=3)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:39:13.931015Z","iopub.execute_input":"2021-09-02T09:39:13.931553Z","iopub.status.idle":"2021-09-02T09:39:14.250532Z","shell.execute_reply.started":"2021-09-02T09:39:13.931503Z","shell.execute_reply":"2021-09-02T09:39:14.249536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Expert annotation\nFour expert physician assisted with the data quality by revising 320 recordings each, selecting one of the predefined options to each of the following 10 items:\n* **Quality**: Good; Ok; Poor; No cough present.\n* **Type of cough**: Wet (productive); Dry; Can’t tell.\n* **Audible dyspnea**: Checkbox.\n* **Audible wheezing**: Checkbox.\n* **Audible stridor**: Checkbox.\n* **Audible choking**: Checkbox.\n* **Audible nasal congestion**: Checkbox.\n* **Nothing specific**: Checkbox.\n* **Impression**: I think this patient has…: An upper respiratory tract infection; A lower respiratory tract infection; Obstructive lung disease (Asthma, COPD, …); COVID-19; Nothing (healthy cough).\n* **Impression**: the cough is probably…: Pseudocough/Healthy cough (from a healthy person); Mild (from a sick person); Severe (from a sick person); Can’t tell.","metadata":{}},{"cell_type":"code","source":"def split_by_physicians(df):\n    column_names = ['uuid', 'datetime', 'cough_detected', 'SNR', 'latitude', 'longitude', \n                    'age', 'gender', 'respiratory_condition', 'fever_muscle_pain', 'status', \n                    'quality', 'cough_type', 'dyspnea', 'wheezing', 'stridor', 'choking', \n                    'congestion', 'nothing', 'diagnosis', 'severity' ]\n    physician_01 = df.iloc[:, 0:21]\n    physician_01 = physician_01[physician_01.quality_1.notna()].reset_index(drop=True)\n    physician_01.columns = column_names\n    physician_01['physician'] = 'P01'\n\n    physician_02 = pd.concat([df.iloc[:, 0:11], df.iloc[:, 21:31]], axis=1)\n    physician_02 = physician_02[physician_02.quality_2.notna()].reset_index(drop=True)\n    physician_02.columns = column_names\n    physician_02['physician'] = 'P02'\n\n    physician_03 = pd.concat([df.iloc[:, 0:11], df.iloc[:, 31:41]], axis=1)\n    physician_03 = physician_03[physician_03.quality_3.notna()].reset_index(drop=True)\n    physician_03.columns = column_names\n    physician_03['physician'] = 'P03'\n\n    physician_04 = pd.concat([df.iloc[:, 0:11], df.iloc[:, 41:51]], axis=1)\n    physician_04 = physician_04[physician_04.quality_4.notna()].reset_index(drop=True)\n    physician_04.columns = column_names\n    physician_04['physician'] = 'P04'\n    return physician_01, physician_02, physician_03, physician_04","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-02T09:39:15.515198Z","iopub.execute_input":"2021-09-02T09:39:15.515647Z","iopub.status.idle":"2021-09-02T09:39:15.526868Z","shell.execute_reply.started":"2021-09-02T09:39:15.515568Z","shell.execute_reply":"2021-09-02T09:39:15.525695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"physician_01, physician_02, physician_03, physician_04 = split_by_physicians(data_raw)\nannotated_df = pd.concat([physician_01,physician_02,physician_03,physician_04]).reset_index(drop=True)  \nannotated_df = annotated_df.fillna('unknown')\nannotated_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:39:16.234013Z","iopub.execute_input":"2021-09-02T09:39:16.234392Z","iopub.status.idle":"2021-09-02T09:39:16.375712Z","shell.execute_reply.started":"2021-09-02T09:39:16.234362Z","shell.execute_reply":"2021-09-02T09:39:16.374715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axs = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw=dict(width_ratios=[10, 4]))\nsns.countplot(data=annotated_df, x=\"status\", hue='physician', ax=axs[0])\nsns.countplot(data=annotated_df, x=\"physician\", ax=axs[1])\nf.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:39:17.097345Z","iopub.execute_input":"2021-09-02T09:39:17.09768Z","iopub.status.idle":"2021-09-02T09:39:17.596231Z","shell.execute_reply.started":"2021-09-02T09:39:17.097652Z","shell.execute_reply":"2021-09-02T09:39:17.595253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axs = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw=dict(width_ratios=[10, 4]))\nsns.countplot(data=annotated_df, x=\"diagnosis\", ax=axs[0])\nsns.countplot(data=annotated_df, x=\"severity\", ax=axs[1])\nf.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:39:18.113913Z","iopub.execute_input":"2021-09-02T09:39:18.114276Z","iopub.status.idle":"2021-09-02T09:39:18.501115Z","shell.execute_reply.started":"2021-09-02T09:39:18.114246Z","shell.execute_reply":"2021-09-02T09:39:18.499934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axs = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw=dict(width_ratios=[4, 4]))\nsns.countplot(data=annotated_df, x=\"cough_type\", ax=axs[0])\nsns.countplot(data=annotated_df, x=\"quality\", ax=axs[1])\nf.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:39:18.682295Z","iopub.execute_input":"2021-09-02T09:39:18.682652Z","iopub.status.idle":"2021-09-02T09:39:19.042344Z","shell.execute_reply.started":"2021-09-02T09:39:18.682615Z","shell.execute_reply":"2021-09-02T09:39:19.041413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axs = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw=dict(width_ratios=[4, 4]))\nsns.countplot(data=annotated_df, x=\"dyspnea\", ax=axs[0])\nsns.countplot(data=annotated_df, x=\"wheezing\", ax=axs[1])\nf.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:39:19.210714Z","iopub.execute_input":"2021-09-02T09:39:19.211135Z","iopub.status.idle":"2021-09-02T09:39:19.515507Z","shell.execute_reply.started":"2021-09-02T09:39:19.211099Z","shell.execute_reply":"2021-09-02T09:39:19.514558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axs = plt.subplots(1, 2, figsize=(12, 4), gridspec_kw=dict(width_ratios=[4, 4]))\nsns.countplot(data=annotated_df, x=\"choking\", ax=axs[0])\nsns.countplot(data=annotated_df, x=\"congestion\", ax=axs[1])\nf.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:39:19.893021Z","iopub.execute_input":"2021-09-02T09:39:19.89351Z","iopub.status.idle":"2021-09-02T09:39:20.329851Z","shell.execute_reply.started":"2021-09-02T09:39:19.893479Z","shell.execute_reply":"2021-09-02T09:39:20.328638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Audio data examples","metadata":{}},{"cell_type":"code","source":"healthy_sample = ROOT+data_raw[data_raw.uuid == 'a28776b5-b876-47ac-8973-9ca280156608'].uuid.values[0]+'.wav'\ncovid_sample = ROOT+data_raw[data_raw.uuid == 'ffbca476-8b35-4797-bc8a-b7f0a2f24b55'].uuid.values[0]+'.wav'\nsymptomatic_sample = ROOT+data_raw[data_raw.uuid == '6d8fcfb2-7aff-4143-a319-99d568035655'].uuid.values[0]+'.wav'","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-02T09:39:23.043771Z","iopub.execute_input":"2021-09-02T09:39:23.044133Z","iopub.status.idle":"2021-09-02T09:39:23.066461Z","shell.execute_reply.started":"2021-09-02T09:39:23.044102Z","shell.execute_reply":"2021-09-02T09:39:23.065492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"healthy_audio, fs = librosa.load(healthy_sample, mono=True)\ncovid_audio, fs = librosa.load(covid_sample, mono=True)\nsymptomatic_audio, fs = librosa.load(symptomatic_sample, mono=True)\n\nfig, (ax, ax1, ax2) = plt.subplots(nrows=3, figsize=(12,6))\nax.set(title='Sample Audio')\nlibrosa.display.waveshow(healthy_audio, sr=fs, ax=ax, label='Healthy')\nlibrosa.display.waveshow(covid_audio, sr=fs, ax=ax1, label='Covid')\nlibrosa.display.waveshow(symptomatic_audio, sr=fs,  ax=ax2, label='Symptomatic')\nax.axis('off')\nax.label_outer()\nax.legend()\nax1.axis('off')\nax1.label_outer()\nax1.legend()\nax2.axis('off')\nax2.label_outer()\nax2.legend()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-09-02T09:39:24.386518Z","iopub.execute_input":"2021-09-02T09:39:24.386902Z","iopub.status.idle":"2021-09-02T09:39:27.205651Z","shell.execute_reply.started":"2021-09-02T09:39:24.386867Z","shell.execute_reply":"2021-09-02T09:39:27.204556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# healthy sample\ndisplay.Audio(healthy_audio, rate=fs)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:40:08.588849Z","iopub.execute_input":"2021-09-02T09:40:08.589209Z","iopub.status.idle":"2021-09-02T09:40:08.602888Z","shell.execute_reply.started":"2021-09-02T09:40:08.589169Z","shell.execute_reply":"2021-09-02T09:40:08.601995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# covid sample\ndisplay.Audio(covid_audio, rate=fs)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:40:13.170356Z","iopub.execute_input":"2021-09-02T09:40:13.17069Z","iopub.status.idle":"2021-09-02T09:40:13.196012Z","shell.execute_reply.started":"2021-09-02T09:40:13.170661Z","shell.execute_reply":"2021-09-02T09:40:13.195059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# symptomatic sample\ndisplay.Audio(symptomatic_audio, rate=fs)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:40:35.657633Z","iopub.execute_input":"2021-09-02T09:40:35.658021Z","iopub.status.idle":"2021-09-02T09:40:35.673439Z","shell.execute_reply.started":"2021-09-02T09:40:35.657989Z","shell.execute_reply":"2021-09-02T09:40:35.67243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre-processing and cough segmentation","metadata":{}},{"cell_type":"code","source":"from scipy import signal\nfrom scipy.io import wavfile\nfrom scipy.signal import butter,filtfilt\nfrom scipy.signal import cwt\nfrom scipy.signal import hilbert\nfrom scipy.signal import resample\nfrom scipy.signal import decimate\nfrom scipy.signal import spectrogram\nfrom scipy.signal.windows import get_window\n\ndef preprocess_cough(x,fs, cutoff = 6000, normalize = True, filter_ = True, downsample = True):\n    \n    # Normalize, lowpass filter, and downsample cough samples in a given data folder \n    fs_downsample = cutoff*2\n    \n    #Preprocess Data\n    if len(x.shape)>1:\n        x = np.mean(x,axis=1)                          # Convert to mono\n    if normalize:\n        x = x/(np.max(np.abs(x))+1e-17)                # Norm to range between -1 to 1\n    if filter_:\n        b, a = butter(4, fs_downsample/fs, btype='lowpass') # 4th order butter lowpass filter\n        x = filtfilt(b, a, x)\n    if downsample:\n        x = signal.decimate(x, int(fs/fs_downsample)) # Downsample for anti-aliasing\n    \n    fs_new = fs_downsample\n\n    return np.float32(x), fs_new\n\ndef segment_cough(x,fs, cough_padding=0.2,min_cough_len=0.2, th_l_multiplier = 0.1, th_h_multiplier = 2):\n    #Preprocess the data by segmenting each file into individual coughs using a hysteresis comparator on the signal power\n                \n    cough_mask = np.array([False]*len(x))\n    \n    #Define hysteresis thresholds\n    rms = np.sqrt(np.mean(np.square(x)))\n    seg_th_l = th_l_multiplier * rms\n    seg_th_h =  th_h_multiplier*rms\n\n    #Segment coughs\n    coughSegments = []\n    padding = round(fs*cough_padding)\n    min_cough_samples = round(fs*min_cough_len)\n    cough_start = 0\n    cough_end = 0\n    cough_in_progress = False\n    tolerance = round(0.01*fs)\n    below_th_counter = 0\n    \n    for i, sample in enumerate(x**2):\n        if cough_in_progress:\n            if sample<seg_th_l:\n                below_th_counter += 1\n                if below_th_counter > tolerance:\n                    cough_end = i+padding if (i+padding < len(x)) else len(x)-1\n                    cough_in_progress = False\n                    if (cough_end+1-cough_start-2*padding>min_cough_samples):\n                        coughSegments.append(x[cough_start:cough_end+1])\n                        cough_mask[cough_start:cough_end+1] = True\n            elif i == (len(x)-1):\n                cough_end=i\n                cough_in_progress = False\n                if (cough_end+1-cough_start-2*padding>min_cough_samples):\n                    coughSegments.append(x[cough_start:cough_end+1])\n            else:\n                below_th_counter = 0\n        else:\n            if sample>seg_th_h:\n                cough_start = i-padding if (i-padding >=0) else 0\n                cough_in_progress = True\n    \n    return coughSegments, cough_mask","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-09-02T09:40:38.02809Z","iopub.execute_input":"2021-09-02T09:40:38.028466Z","iopub.status.idle":"2021-09-02T09:40:38.106615Z","shell.execute_reply.started":"2021-09-02T09:40:38.02843Z","shell.execute_reply":"2021-09-02T09:40:38.105542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess (Normalize, lowpass filter, and downsample cough samples)\nprocessed_audio, sample_rate = preprocess_cough(healthy_audio, fs)\n\n# Segment each audio into individual coughs using a hysteresis comparator on the signal power\ncough_segments, cough_mask = segment_cough(processed_audio, sample_rate, min_cough_len=0.1, cough_padding=0.1, th_l_multiplier = 0.1, th_h_multiplier = 2)\n\nfig = plt.figure(figsize=(12,4))\nplt.plot(processed_audio)\nplt.plot(cough_mask)\nplt.title(\"Segmentation Output\")\nplt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:40:38.80098Z","iopub.execute_input":"2021-09-02T09:40:38.801347Z","iopub.status.idle":"2021-09-02T09:40:39.271267Z","shell.execute_reply.started":"2021-09-02T09:40:38.80131Z","shell.execute_reply":"2021-09-02T09:40:39.269983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(len(cough_segments),1, figsize=(12,8))\nfor i in range(0,len(cough_segments)):\n    axs[i].plot(cough_segments[i])\n    axs[i].set_title(\"Cough segment \" + str(i))\n    axs[i].axis('off')","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:40:41.775333Z","iopub.execute_input":"2021-09-02T09:40:41.775723Z","iopub.status.idle":"2021-09-02T09:40:42.205613Z","shell.execute_reply.started":"2021-09-02T09:40:41.775688Z","shell.execute_reply":"2021-09-02T09:40:42.20448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first segment healthy sample\ndisplay.Audio(cough_segments[0], rate=fs)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:41:50.432507Z","iopub.execute_input":"2021-09-02T09:41:50.432978Z","iopub.status.idle":"2021-09-02T09:41:50.44117Z","shell.execute_reply.started":"2021-09-02T09:41:50.43294Z","shell.execute_reply":"2021-09-02T09:41:50.440236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Audio Data Features Extraction\n\nSound is represented in the form of an audio signal having parameters such as frequency, bandwidth, decibel, etc. A typical audio signal can be expressed as a function of Amplitude and Time.\n\nBecause the audio data presented cannot be interpreted directly by the models, feature extraction is utilised to convert it to a comprehensible format. It is a process that explains most of the data but in an understandable way. Commonly used features or representations that are directly fed into neural network architectures are spectrograms, mel-spectrograms, and Mel-Frequency Cepstral Coefficients (MFCCs).\n\n### Spectrogram\nA spectrogram is a graphical representation of the signal strength, or \"loudness,\" of a signal over time at various frequencies present in a waveform. Not only can one see whether there is more or less energy at a particular frequency, such as 2 Hz vs 10 Hz, but also how the energy levels vary over time. \n\n### Short-Time Fourier Transform\nThe Short-Time Fourier Transform (STFT) is an immensely useful tool for processing audio signals. It defines a particularly useful class of time-frequency distributions that specify the complex amplitude versus time and frequency characteristics of any signal. \n\nIt is obtained by applying the Short-Time Fourier Transform (STFT) on the signal. In the simplest of terms, the STFT of a signal is calculated by applying the Fast Fourier Transform (FFT) locally on small-time segments of the signal. Typically, a spectrogram is depicted as a heat map, that is, an image with the intensity indicated by varying the colour or brightness.","metadata":{}},{"cell_type":"code","source":"# Short Term Fourier Transform\naudio_data = cough_segments[1]\nstft = np.abs(librosa.stft(audio_data))\nDstft = librosa.amplitude_to_db(stft, ref=np.max)\nax = librosa.display.specshow(Dstft)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:42:45.638504Z","iopub.execute_input":"2021-09-02T09:42:45.638896Z","iopub.status.idle":"2021-09-02T09:42:45.717481Z","shell.execute_reply.started":"2021-09-02T09:42:45.638861Z","shell.execute_reply":"2021-09-02T09:42:45.716572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mel-spectrogram\nHumans perceive sound logarithmically. We are more sensitive to differences in lower frequencies than to differences in higher frequencies. For example, while we can easily distinguish between 500 and 1000 Hz, we will struggle to distinguish between 10,000 and 10,500 Hz, even if the distance between the two pairs is the same. As a result, the mel scale was created. It is a logarithmic scale based on the axiom that equal distances on the scale correspond to the same perceived distance.","metadata":{}},{"cell_type":"code","source":"# compute a mel-scaled spectrogram.\nmel = librosa.feature.melspectrogram(audio_data, sr=fs)\nDmel = librosa.amplitude_to_db(mel, ref=np.max)\nax = librosa.display.specshow(Dmel)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:42:47.942672Z","iopub.execute_input":"2021-09-02T09:42:47.943162Z","iopub.status.idle":"2021-09-02T09:42:48.101094Z","shell.execute_reply.started":"2021-09-02T09:42:47.943114Z","shell.execute_reply":"2021-09-02T09:42:48.100341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mel-Frequency Cepstral Coefficients\nThe Mel-Frequency Cepstrum Coefficients (MFCCs) are the coefficients that comprise the mel-frequency cepstrum. The cepstrum of a signal contains information about the rate of change in its spectral bands. \n\nA cepstrum is essentially a spectrum of the log of the time signal's spectrum. The resulting spectrum is neither in the frequency domain nor in the time domain and thus has been dubbed the quefrency domain. The cepstrum communicates the various values that contribute to the formation of a sound's formants (a characteristic component of the quality of a speech sound) and timbre.","metadata":{}},{"cell_type":"code","source":"# mel-frequency cepstral coefficients (MFCCs)\nmfcc = librosa.feature.mfcc(y=audio_data, sr=fs, n_mfcc=40)\nax = librosa.display.specshow(mfcc)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:42:48.988838Z","iopub.execute_input":"2021-09-02T09:42:48.98931Z","iopub.status.idle":"2021-09-02T09:42:49.092267Z","shell.execute_reply.started":"2021-09-02T09:42:48.989278Z","shell.execute_reply":"2021-09-02T09:42:49.091234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Chroma Features\nChroma features are a highly effective method of representing music audio in which we use a 12-element representation of spectral energy called a chroma vector in which each of the 12 bins represents one of the twelve equal-tempered pitch classes found in western-style music (semitone spacing). It can be calculated from the input sound signal's logarithmic short-time Fourier transform, also known as a chromatogram or pitch class profile.","metadata":{}},{"cell_type":"code","source":"# compute a chromagram from a waveform or power spectrogram.\nchroma = librosa.feature.chroma_stft(S=stft, sr=fs)\nax = librosa.display.specshow(chroma)","metadata":{"execution":{"iopub.status.busy":"2021-09-02T09:42:50.112179Z","iopub.execute_input":"2021-09-02T09:42:50.112522Z","iopub.status.idle":"2021-09-02T09:42:50.176002Z","shell.execute_reply.started":"2021-09-02T09:42:50.112495Z","shell.execute_reply":"2021-09-02T09:42:50.174985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Part 2 | Machine Learning Algorithm Comparison\n\nhttps://www.kaggle.com/nasrulhakim86/covid-19-screening-from-audio-part-2?scriptVersionId=73794324","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}